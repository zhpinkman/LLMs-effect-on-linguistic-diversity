{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy, levene\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import collections as coll\n",
    "import math\n",
    "import spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_liwc_features = [\n",
    "    \"Analytic\",\n",
    "    \"Clout\",\n",
    "    \"Authentic\",\n",
    "    \"Tone\",\n",
    "    \"WPS\",\n",
    "    \"BigWords\",\n",
    "    \"Dic\",\n",
    "    \"Linguistic\",\n",
    "    \"function\",\n",
    "    \"pronoun\",\n",
    "    \"ppron\",\n",
    "    \"i\",\n",
    "    \"we\",\n",
    "    \"you\",\n",
    "    \"shehe\",\n",
    "    \"they\",\n",
    "    \"ipron\",\n",
    "    \"det\",\n",
    "    \"article\",\n",
    "    \"number\",\n",
    "    \"prep\",\n",
    "    \"auxverb\",\n",
    "    \"adverb\",\n",
    "    \"conj\",\n",
    "    \"negate\",\n",
    "    \"verb\",\n",
    "    \"adj\",\n",
    "    \"quantity\",\n",
    "    \"Drives\",\n",
    "    \"affiliation\",\n",
    "    \"achieve\",\n",
    "    \"power\",\n",
    "    \"Cognition\",\n",
    "    \"allnone\",\n",
    "    \"cogproc\",\n",
    "    \"insight\",\n",
    "    \"cause\",\n",
    "    \"discrep\",\n",
    "    \"tentat\",\n",
    "    \"certitude\",\n",
    "    \"differ\",\n",
    "    \"memory\",\n",
    "    \"Affect\",\n",
    "    \"tone_pos\",\n",
    "    \"tone_neg\",\n",
    "    \"emotion\",\n",
    "    \"emo_pos\",\n",
    "    \"emo_neg\",\n",
    "    \"emo_anx\",\n",
    "    \"emo_anger\",\n",
    "    \"emo_sad\",\n",
    "    \"swear\",\n",
    "    \"Social\",\n",
    "    \"socbehav\",\n",
    "    \"prosocial\",\n",
    "    \"polite\",\n",
    "    \"conflict\",\n",
    "    \"moral\",\n",
    "    \"comm\",\n",
    "    \"socrefs\",\n",
    "    \"family\",\n",
    "    \"friend\",\n",
    "    \"female\",\n",
    "    \"male\",\n",
    "    \"Culture\",\n",
    "    \"politic\",\n",
    "    \"ethnicity\",\n",
    "    \"tech\",\n",
    "    \"Lifestyle\",\n",
    "    \"leisure\",\n",
    "    \"home\",\n",
    "    \"work\",\n",
    "    \"money\",\n",
    "    \"relig\",\n",
    "    \"Physical\",\n",
    "    \"health\",\n",
    "    \"illness\",\n",
    "    \"wellness\",\n",
    "    \"mental\",\n",
    "    \"substances\",\n",
    "    \"sexual\",\n",
    "    \"food\",\n",
    "    \"death\",\n",
    "    \"need\",\n",
    "    \"want\",\n",
    "    \"acquire\",\n",
    "    \"lack\",\n",
    "    \"fulfill\",\n",
    "    \"fatigue\",\n",
    "    \"reward\",\n",
    "    \"risk\",\n",
    "    \"curiosity\",\n",
    "    \"allure\",\n",
    "    \"Perception\",\n",
    "    \"attention\",\n",
    "    \"motion\",\n",
    "    \"space\",\n",
    "    \"visual\",\n",
    "    \"auditory\",\n",
    "    \"feeling\",\n",
    "    \"time\",\n",
    "    \"focuspast\",\n",
    "    \"focuspresent\",\n",
    "    \"focusfuture\",\n",
    "    \"Conversation\",\n",
    "    \"netspeak\",\n",
    "    \"assent\",\n",
    "    \"nonflu\",\n",
    "    \"filler\",\n",
    "    \"AllPunc\",\n",
    "    \"Period\",\n",
    "    \"Comma\",\n",
    "    \"QMark\",\n",
    "    \"Exclam\",\n",
    "    \"Apostro\",\n",
    "    \"OtherP\",\n",
    "    \"Emoji\",\n",
    "]\n",
    "\n",
    "complexity_features = [\n",
    "    \"avg_dependency_link_length\",\n",
    "    \"type_token_ratio\",\n",
    "    \"hapax_legemena\",\n",
    "    \"shannon_entropy\",\n",
    "    \"simpsons_index\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TO_DATASET_NAME = {\n",
    "    \"essays\": \"Essays\",\n",
    "    \"wassa\": \"Empathetic.\",\n",
    "    \"facebook\": \"YourMorals\",\n",
    "    \"political\": \"Congress\",\n",
    "}\n",
    "\n",
    "BASELINES = {\n",
    "    \"CON\": 0.5110732538330494,\n",
    "    \"EXT\": 0.5157580919931857,\n",
    "    \"AGR\": 0.530664395229983,\n",
    "    \"NEU\": 0.5008517887563884,\n",
    "    \"OPN\": 0.5161839863713799,\n",
    "    \".care\": 0.5070035704476792,\n",
    "    \".purity\": 0.5358418017028289,\n",
    "    \".fairness\": 0.5281516067014557,\n",
    "    \".loyalty\": 0.5034331227684702,\n",
    "    \".authority\": 0.5339192529524857,\n",
    "    \".iri.concern\": 0.5724331926863573,\n",
    "    \".iri.distress\": 0.5175808720112518,\n",
    "    \".iri.fantasy\": 0.5471167369901547,\n",
    "    \".iri.perspective\": 0.5274261603375527,\n",
    "    \"gender\": 0.5,\n",
    "    \"cohort\": 0.25,\n",
    "    \"party\": 0.5,\n",
    "}\n",
    "\n",
    "\n",
    "FROM_CURRENT_TO_ORDERED_AGE_GROUP_INDICES = {\n",
    "    0: 3,\n",
    "    1: 1,\n",
    "    2: 0,\n",
    "    3: 2,\n",
    "}\n",
    "\n",
    "\n",
    "DATASET_TO_BASELINES = {\n",
    "    \"essays\": np.mean([BASELINES[x] for x in [\"CON\", \"EXT\", \"AGR\", \"NEU\", \"OPN\"]]),\n",
    "    \"wassa\": np.mean(\n",
    "        [\n",
    "            BASELINES[x]\n",
    "            for x in [\n",
    "                \".iri.concern\",\n",
    "                \".iri.distress\",\n",
    "                \".iri.fantasy\",\n",
    "                \".iri.perspective\",\n",
    "            ]\n",
    "        ]\n",
    "    ),\n",
    "    \"wassa_individual\": np.mean(\n",
    "        [\n",
    "            BASELINES[x]\n",
    "            for x in [\n",
    "                \".iri.concern\",\n",
    "                \".iri.distress\",\n",
    "                \".iri.fantasy\",\n",
    "                \".iri.perspective\",\n",
    "            ]\n",
    "        ]\n",
    "    ),\n",
    "    \"facebook\": np.mean(\n",
    "        [\n",
    "            BASELINES[x]\n",
    "            for x in [\".care\", \".purity\", \".fairness\", \".loyalty\", \".authority\"]\n",
    "        ]\n",
    "    ),\n",
    "    \"party\": BASELINES[\"party\"],\n",
    "    \"gender\": BASELINES[\"gender\"],\n",
    "    \"cohort\": BASELINES[\"cohort\"],\n",
    "}\n",
    "\n",
    "REWRITTEN_TYPE_TO_SHORT = {\n",
    "    \"syntax_grammar\": \"SG\",\n",
    "    \"rephrase\": \"R\",\n",
    "}\n",
    "\n",
    "LLM_TO_NAME = {\n",
    "    \"original\": \"Original\",\n",
    "    \"gpt\": \"GPT3.5\",\n",
    "    \"llama\": \"Llama 2\",\n",
    "    \"gemini\": \"Gemini\",\n",
    "}\n",
    "\n",
    "LABELS_TO_NAME = {\n",
    "    \".iri.concern\": \"Concern\",\n",
    "    \".iri.distress\": \"Distress\",\n",
    "    \".iri.perspective\": \"Perspective\",\n",
    "    \".iri.fantasy\": \"Fantasy\",\n",
    "    \".authority\": \"Authority\",\n",
    "    \".care\": \"Care\",\n",
    "    \".fairness\": \"Fairness\",\n",
    "    \".loyalty\": \"Loyalty\",\n",
    "    \".purity\": \"Purity\",\n",
    "    \"CON\": \"CON\",\n",
    "    \"NEU\": \"NEU\",\n",
    "    \"EXT\": \"EXT\",\n",
    "    \"AGR\": \"AGR\",\n",
    "    \"OPN\": \"OPN\",\n",
    "    \"gender\": \"Gender\",\n",
    "    \"cohort\": \"Age group\",\n",
    "    \"party\": \"Party\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "personality_columns = [\n",
    "    \"nrc.negative\",\n",
    "    \"emo_pos\",\n",
    "    \"affect\",\n",
    "    \"nrc.positive\",\n",
    "    \"affiliation\",\n",
    "    \"emotion\",\n",
    "    \"nrc.joy\",\n",
    "    \"social\",\n",
    "    \"emo_sad\",\n",
    "    \"nrc.anticipation\",\n",
    "    \"nrc.anger\",\n",
    "    \"pronoun\",\n",
    "    \"i\",\n",
    "    \"emo_anger\",\n",
    "    \"swear\",\n",
    "    \"bigwords\",\n",
    "    \"emo_neg\",\n",
    "    \"nrc.disgust\",\n",
    "    \"nrc.sadness\",\n",
    "    \"nrc.trust\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_columns = [\n",
    "    \"we\",\n",
    "    \"socrefs\",\n",
    "    \"pronoun\",\n",
    "    \"friend\",\n",
    "    \"mfd.authority.virtue\",\n",
    "    \"mfd.care.virtue\",\n",
    "    \"affect\",\n",
    "    \"i\",\n",
    "    \"mfd.authority.vice\",\n",
    "    \"mfd.purity.vice\",\n",
    "    \"affiliation\",\n",
    "    \"prosocial\",\n",
    "    \"family\",\n",
    "    \"religion\",\n",
    "    \"mfd.purity.virtue\",\n",
    "    \"you\",\n",
    "    \"social\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wassa_columns = [\n",
    "    \"affect\",\n",
    "    \"differ\",\n",
    "    \"we\",\n",
    "    \"she,he\",\n",
    "    \"pronoun\",\n",
    "    \"emo_neg\",\n",
    "    \"tentat\",\n",
    "    \"empathy.low\",\n",
    "    \"pronoun\",\n",
    "    \"cogproc\",\n",
    "    \"distress.low\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "political_columns = [\n",
    "    \"mfd.authority.virtue\",\n",
    "    \"mfd.authority.vice\",\n",
    "    \"mfd.loyalty.virtue\",\n",
    "    \"mfd.loyalty.vice\",\n",
    "    \"mfd.fairness.virtue\",\n",
    "    \"mfd.fairness.vice\",\n",
    "    \"emo_anx\",\n",
    "    \"adverb\",\n",
    "    \"conj\",\n",
    "    \"emo_neg\",\n",
    "    \"emo_anger\",\n",
    "    \"we\",\n",
    "    \"relig\",\n",
    "    \"swear\",\n",
    "    \"i\",\n",
    "    \"cogproc\",\n",
    "    \"emo_pos\",\n",
    "    \"certitude\",\n",
    "]\n",
    "\n",
    "gender_columns = [\n",
    "    \"article\",\n",
    "    \"social\",\n",
    "    \"emo_anx\",\n",
    "    \"pronoun\",\n",
    "    \"i\",\n",
    "    \"emo_pos\",\n",
    "    \"emo_neg\",\n",
    "    \"affect\",\n",
    "    \"tentat\",\n",
    "    \"motion\",\n",
    "    \"swear\",\n",
    "    \"quant\",\n",
    "    \"number\",\n",
    "    \"space\",\n",
    "    \"cogproc\",\n",
    "]\n",
    "\n",
    "age_columns = [\n",
    "    \"we\",\n",
    "    \"cogproc\",\n",
    "    \"prep\",\n",
    "    \"article\",\n",
    "    \"social\",\n",
    "    \"focusfuture\",\n",
    "    \"focuspast\",\n",
    "    \"emo_neg\",\n",
    "    \"emo_pos\",\n",
    "    \"i\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def RemoveSpecialCHs(text):\n",
    "    text = word_tokenize(text)\n",
    "    st = [\n",
    "        \",\",\n",
    "        \".\",\n",
    "        \"'\",\n",
    "        \"!\",\n",
    "        '\"',\n",
    "        \"#\",\n",
    "        \"$\",\n",
    "        \"%\",\n",
    "        \"&\",\n",
    "        \"(\",\n",
    "        \")\",\n",
    "        \"*\",\n",
    "        \"+\",\n",
    "        \"-\",\n",
    "        \".\",\n",
    "        \"/\",\n",
    "        \":\",\n",
    "        \";\",\n",
    "        \"<\",\n",
    "        \"=\",\n",
    "        \">\",\n",
    "        \"?\",\n",
    "        \"@\",\n",
    "        \"[\",\n",
    "        \"\\\\\",\n",
    "        \"]\",\n",
    "        \"^\",\n",
    "        \"_\",\n",
    "        \"`\",\n",
    "        \"{\",\n",
    "        \"|\",\n",
    "        \"}\",\n",
    "        \"~\",\n",
    "        \"\\t\",\n",
    "        \"\\n\",\n",
    "    ]\n",
    "\n",
    "    words = [word for word in text if word not in st]\n",
    "    return words\n",
    "\n",
    "\n",
    "def compute_avg_dependency_link_length(text):\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        link_lengths = []\n",
    "        for sent in doc.sents:\n",
    "            sent_link_lengths = []\n",
    "            for token in sent:\n",
    "                if token.dep_ != \"ROOT\":\n",
    "                    head = token.head\n",
    "                    sent_link_lengths.append(abs(head.i - token.i))\n",
    "            if sent_link_lengths:  # Only append if sentence had any links\n",
    "                link_lengths.append(np.mean(sent_link_lengths))\n",
    "        return np.mean(link_lengths)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def typeTokenRatio(text):\n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "        return len(set(words)) / len(words)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def hapaxLegemena(text):\n",
    "    try:\n",
    "        words = RemoveSpecialCHs(text)\n",
    "        V1 = 0\n",
    "        # dictionary comprehension . har word kay against value 0 kardi\n",
    "        freqs = {key: 0 for key in words}\n",
    "        for word in words:\n",
    "            freqs[word] += 1\n",
    "        for word in freqs:\n",
    "            if freqs[word] == 1:\n",
    "                V1 += 1\n",
    "        N = len(words)\n",
    "        V = float(len(set(words)))\n",
    "        R = 100 * math.log(N) / max(1, (1 - (V1 / V)))\n",
    "        h = V1 / N\n",
    "        return R, h\n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "\n",
    "def ShannonEntropy(text):\n",
    "    try:\n",
    "        words = RemoveSpecialCHs(text)\n",
    "        lenght = len(words)\n",
    "        freqs = coll.Counter()\n",
    "        freqs.update(words)\n",
    "        arr = np.array(list(freqs.values()))\n",
    "        distribution = 1.0 * arr\n",
    "        distribution /= max(1, lenght)\n",
    "        import scipy as sc\n",
    "\n",
    "        H = sc.stats.entropy(distribution, base=2)\n",
    "        # H = sum([(i/lenght)*math.log(i/lenght,math.e) for i in freqs.values()])\n",
    "        return H\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def SimpsonsIndex(text):\n",
    "    try:\n",
    "        words = RemoveSpecialCHs(text)\n",
    "        freqs = coll.Counter()\n",
    "        freqs.update(words)\n",
    "        N = len(words)\n",
    "        n = sum([1.0 * i * (i - 1) for i in freqs.values()])\n",
    "        D = 1 - (n / (N * (N - 1)))\n",
    "        return D\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_complexity_features(df):\n",
    "    df[\"avg_dependency_link_length\"] = [\n",
    "        compute_avg_dependency_link_length(text)\n",
    "        for text in tqdm(df[\"text\"], leave=False)\n",
    "    ]\n",
    "\n",
    "    df[\"type_token_ratio\"] = [\n",
    "        typeTokenRatio(text) for text in tqdm(df[\"text\"], leave=False)\n",
    "    ]\n",
    "    df[\"hapax_legemena\"] = [\n",
    "        hapaxLegemena(text)[1] for text in tqdm(df[\"text\"], leave=False)\n",
    "    ]\n",
    "    df[\"shannon_entropy\"] = [\n",
    "        ShannonEntropy(text) for text in tqdm(df[\"text\"], leave=False)\n",
    "    ]\n",
    "    df[\"simpsons_index\"] = [\n",
    "        SimpsonsIndex(text) for text in tqdm(df[\"text\"], leave=False)\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_complexity_features_to_dataset(dataset):\n",
    "    dataset.set_original_data(add_complexity_features(dataset.get_original_data()))\n",
    "    for llm in tqdm([\"gpt\", \"llama\", \"gemini\"], leave=False):\n",
    "        for mode in tqdm([\"syntax_grammar\", \"rephrase\"], leave=False):\n",
    "            dataset.set_rewritten_data(\n",
    "                mode,\n",
    "                llm,\n",
    "                add_complexity_features(dataset.get_rewritten_data(mode, llm)),\n",
    "            )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def does_dataset_have_complexity_features(dataset):\n",
    "    return all(\n",
    "        [\n",
    "            feature in dataset.get_original_data().columns\n",
    "            for feature in complexity_features\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self) -> None:\n",
    "        self.filtered_columns = all_liwc_features + complexity_features\n",
    "        self.rewritten_datasets = {}\n",
    "\n",
    "    def get_original_data(self):\n",
    "        return self.original_data\n",
    "\n",
    "    def set_original_data(self, data):\n",
    "        self.original_data = data\n",
    "        data.to_csv(self.original_data_path, index=False)\n",
    "\n",
    "    def get_rewritten_data(self, n, m):\n",
    "        if self.rewritten_datasets.get((n, m)) is None:\n",
    "            self.rewritten_datasets[(n, m)] = pd.read_csv(\n",
    "                self.rewritten_data_path.format(n, m)\n",
    "            )\n",
    "        return self.rewritten_datasets[(n, m)]\n",
    "\n",
    "    def set_rewritten_data(self, n, m, data):\n",
    "        self.rewritten_datasets[(n, m)] = data\n",
    "        data.to_csv(self.rewritten_data_path.format(n, m), index=False)\n",
    "\n",
    "\n",
    "class Essays(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.original_data_path = \"essays/with_dictionaries/LIWC-22 Results - essays_anon_full - LIWC Analysis.csv\"\n",
    "        self.rewritten_data_path = \"essays/with_dictionaries/LIWC-22 Results - essays_rewritten_{}_{} - LIWC Analysis.csv\"\n",
    "        self.original_data = pd.read_csv(self.original_data_path)\n",
    "        self.id_column = \"#AUTHID\"\n",
    "        self.filtered_columns = self.filtered_columns + [\n",
    "            col for col in self.original_data.columns if \"nrc.\" in col\n",
    "        ]\n",
    "        self.name = \"essays\"\n",
    "\n",
    "    def get_original_data(self):\n",
    "        # call the parent class method to get the original data\n",
    "        return super().get_original_data()\n",
    "\n",
    "    def get_rewritten_data(self, n, m):\n",
    "        return super().get_rewritten_data(n, m)\n",
    "\n",
    "\n",
    "class Wassa(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.original_data_path = (\n",
    "            \"wassa/with_dictionaries/LIWC-22 Results - clean_wassa - LIWC Analysis.csv\"\n",
    "        )\n",
    "        self.rewritten_data_path = \"wassa/with_dictionaries/LIWC-22 Results - wassa_rewritten_{}_{} - LIWC Analysis.csv\"\n",
    "        self.original_data = pd.read_csv(self.original_data_path)\n",
    "        self.id_column = \"id\"\n",
    "        self.filtered_columns = self.filtered_columns + [\n",
    "            col\n",
    "            for col in self.original_data.columns\n",
    "            if col.split(\".\") in [\"distress\", \"empathy\"]\n",
    "        ]\n",
    "        self.name = \"wassa\"\n",
    "\n",
    "    def get_original_data(self):\n",
    "        return super().get_original_data()\n",
    "\n",
    "    def get_rewritten_data(self, n, m):\n",
    "        return super().get_rewritten_data(n, m)\n",
    "\n",
    "\n",
    "class Political(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.original_data_path = \"political/with_dictionaries_both/LIWC-22 Results - clean_data_agg - LIWC Analysis.csv\"\n",
    "        self.rewritten_data_path = \"political/with_dictionaries_both/LIWC-22 Results - political_rewritten_{}_{} - LIWC Analysis.csv\"\n",
    "        self.original_data = pd.read_csv(self.original_data_path)\n",
    "        self.id_column = \"speakerid\"\n",
    "        self.name = \"political\"\n",
    "\n",
    "    def get_original_data(self):\n",
    "        return super().get_original_data()\n",
    "\n",
    "    def get_rewritten_data(self, n, m):\n",
    "        return super().get_rewritten_data(n, m)\n",
    "\n",
    "\n",
    "class Facebook(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.original_data_path = \"facebook/with_dictionaries/LIWC-22 Results - full_dataset_clean - LIWC Analysis.csv\"\n",
    "        self.rewritten_data_path = \"facebook/with_dictionaries/LIWC-22 Results - facebook_rewritten_{}_{} - LIWC Analysis.csv\"\n",
    "        self.original_data = pd.read_csv(self.original_data_path)\n",
    "        self.id_column = \"subject_id\"\n",
    "        self.filtered_columns = self.filtered_columns + [\n",
    "            col for col in self.original_data.columns if \"mfd.\" in col\n",
    "        ]\n",
    "        self.name = \"facebook\"\n",
    "\n",
    "    def get_original_data(self):\n",
    "        return super().get_original_data()\n",
    "\n",
    "    def get_rewritten_data(self, n, m):\n",
    "        return super().get_rewritten_data(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = Essays()\n",
    "wassa = Wassa()\n",
    "political = Political()\n",
    "facebook = Facebook()\n",
    "\n",
    "if not does_dataset_have_complexity_features(essays):\n",
    "    print(\"Processing Essays dataset with the complexity features\")\n",
    "    essays = add_complexity_features_to_dataset(essays)\n",
    "if not does_dataset_have_complexity_features(wassa):\n",
    "    print(\"Processing Wassa dataset with the complexity features\")\n",
    "    wassa = add_complexity_features_to_dataset(wassa)\n",
    "if not does_dataset_have_complexity_features(political):\n",
    "    print(\"Processing Political dataset with the complexity features\")\n",
    "    political = add_complexity_features_to_dataset(political)\n",
    "if not does_dataset_have_complexity_features(facebook):\n",
    "    print(\"Processing Facebook dataset with the complexity features\")\n",
    "    facebook = add_complexity_features_to_dataset(facebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_features_and_entropies():\n",
    "    llms = []\n",
    "    modes = []\n",
    "    cols = []\n",
    "    # entropy_values = []\n",
    "    distribution_values = []\n",
    "    index_values = []\n",
    "    datasets = []\n",
    "    dataset_original_or_not = []\n",
    "    for dataset in tqdm([essays, wassa, political, facebook], leave=False):\n",
    "        for llm in tqdm([\"gpt\", \"llama\", \"gemini\"], leave=False):\n",
    "            for mode in tqdm([\"syntax_grammar\", \"rephrase\"], leave=False):\n",
    "                for col in tqdm(dataset.filtered_columns, leave=False):\n",
    "\n",
    "                    original_data = dataset.get_original_data()\n",
    "                    rewritten_data = dataset.get_rewritten_data(mode, llm)\n",
    "\n",
    "                    original_data = original_data[original_data[col].notna()]\n",
    "                    rewritten_data = rewritten_data[rewritten_data[col].notna()]\n",
    "\n",
    "                    shared_ids = list(\n",
    "                        set.intersection(\n",
    "                            set(original_data[dataset.id_column].tolist()),\n",
    "                            set(rewritten_data[dataset.id_column].tolist()),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    original_data = original_data[\n",
    "                        original_data[dataset.id_column].isin(shared_ids)\n",
    "                    ]\n",
    "                    rewritten_data = rewritten_data[\n",
    "                        rewritten_data[dataset.id_column].isin(shared_ids)\n",
    "                    ]\n",
    "\n",
    "                    original_data = original_data.set_index(\n",
    "                        dataset.id_column\n",
    "                    ).sort_index()[col]\n",
    "\n",
    "                    rewritten_data = rewritten_data.set_index(\n",
    "                        dataset.id_column\n",
    "                    ).sort_index()[col]\n",
    "\n",
    "                    shared_ids = list(\n",
    "                        set.intersection(\n",
    "                            set(original_data.index.tolist()),\n",
    "                            set(rewritten_data.index.tolist()),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    original_data_normalized = original_data\n",
    "                    rewritten_data_normalized = rewritten_data\n",
    "\n",
    "                    for is_original in [True, False]:\n",
    "                        dataset_original_or_not.append(is_original)\n",
    "                        if is_original:\n",
    "\n",
    "                            distribution_values.append(\n",
    "                                original_data_normalized.tolist()\n",
    "                            )\n",
    "                        else:\n",
    "\n",
    "                            distribution_values.append(\n",
    "                                rewritten_data_normalized.tolist()\n",
    "                            )\n",
    "                        index_values.append(shared_ids)\n",
    "                        llms.append(llm)\n",
    "                        modes.append(mode)\n",
    "                        cols.append(col)\n",
    "                        datasets.append(\n",
    "                            \"essays\"\n",
    "                            if isinstance(dataset, Essays)\n",
    "                            else (\n",
    "                                \"wassa\"\n",
    "                                if isinstance(dataset, Wassa)\n",
    "                                else (\n",
    "                                    \"political\"\n",
    "                                    if isinstance(dataset, Political)\n",
    "                                    else \"facebook\"\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "    return (\n",
    "        llms,\n",
    "        modes,\n",
    "        cols,\n",
    "        datasets,\n",
    "        dataset_original_or_not,\n",
    "        # entropy_values,\n",
    "        distribution_values,\n",
    "        index_values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    llms,\n",
    "    modes,\n",
    "    cols,\n",
    "    datasets,\n",
    "    dataset_original_or_not,\n",
    "    # entropy_values,\n",
    "    distribution_values,\n",
    "    index_values,\n",
    ") = get_agg_features_and_entropies()\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"llm\": llms,\n",
    "        \"mode\": modes,\n",
    "        \"col\": cols,\n",
    "        \"dataset\": datasets,\n",
    "        \"is_original\": dataset_original_or_not,\n",
    "        # \"entropy\": entropy_values,\n",
    "        \"distribution\": distribution_values,\n",
    "        \"index\": index_values,\n",
    "    }\n",
    ")\n",
    "df[\"col\"] = df[\"col\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['avg_dependency_link_length',\n",
       " 'type_token_ratio',\n",
       " 'hapax_legemena',\n",
       " 'shannon_entropy',\n",
       " 'simpsons_index']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexity_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"col\"].isin(complexity_features)]\n",
    "\n",
    "filtered_llms = []\n",
    "filtered_modes = []\n",
    "filtered_datasets = []\n",
    "filtered_is_originals = []\n",
    "mean_values = []\n",
    "avg_dep_values = []\n",
    "type_token_values = []\n",
    "hapax_values = []\n",
    "shannon_values = []\n",
    "simpsons_values = []\n",
    "\n",
    "for llm in df[\"llm\"].unique():\n",
    "    for mode in df[\"mode\"].unique():\n",
    "        for dataset in df[\"dataset\"].unique():\n",
    "            for is_original in [True, False]:\n",
    "\n",
    "                sub_df = df[\n",
    "                    (df[\"llm\"] == llm)\n",
    "                    & (df[\"mode\"] == mode)\n",
    "                    & (df[\"dataset\"] == dataset)\n",
    "                    & (df[\"is_original\"] == is_original)\n",
    "                ]\n",
    "                avg_dep_subdf = pd.DataFrame(\n",
    "                    {\n",
    "                        \"value_avg_dep\": sub_df[\n",
    "                            sub_df[\"col\"] == \"avg_dependency_link_length\"\n",
    "                        ][\"distribution\"].values[0],\n",
    "                        \"index\": sub_df[sub_df[\"col\"] == \"avg_dependency_link_length\"][\n",
    "                            \"index\"\n",
    "                        ].values[0],\n",
    "                    }\n",
    "                )\n",
    "                type_token_subdf = pd.DataFrame(\n",
    "                    {\n",
    "                        \"value_type_token\": sub_df[sub_df[\"col\"] == \"type_token_ratio\"][\n",
    "                            \"distribution\"\n",
    "                        ].values[0],\n",
    "                        \"index\": sub_df[sub_df[\"col\"] == \"type_token_ratio\"][\n",
    "                            \"index\"\n",
    "                        ].values[0],\n",
    "                    }\n",
    "                )\n",
    "                hapax_subdf = pd.DataFrame(\n",
    "                    {\n",
    "                        \"value_hapax\": sub_df[sub_df[\"col\"] == \"hapax_legemena\"][\n",
    "                            \"distribution\"\n",
    "                        ].values[0],\n",
    "                        \"index\": sub_df[sub_df[\"col\"] == \"hapax_legemena\"][\n",
    "                            \"index\"\n",
    "                        ].values[0],\n",
    "                    }\n",
    "                )\n",
    "                shannon_subdf = pd.DataFrame(\n",
    "                    {\n",
    "                        \"value_shannon\": sub_df[sub_df[\"col\"] == \"shannon_entropy\"][\n",
    "                            \"distribution\"\n",
    "                        ].values[0],\n",
    "                        \"index\": sub_df[sub_df[\"col\"] == \"shannon_entropy\"][\n",
    "                            \"index\"\n",
    "                        ].values[0],\n",
    "                    }\n",
    "                )\n",
    "                simpsons_subdf = pd.DataFrame(\n",
    "                    {\n",
    "                        \"value_simpsons\": sub_df[sub_df[\"col\"] == \"simpsons_index\"][\n",
    "                            \"distribution\"\n",
    "                        ].values[0],\n",
    "                        \"index\": sub_df[sub_df[\"col\"] == \"simpsons_index\"][\n",
    "                            \"index\"\n",
    "                        ].values[0],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                merged_df = (\n",
    "                    pd.merge(\n",
    "                        avg_dep_subdf,\n",
    "                        type_token_subdf,\n",
    "                        on=\"index\",\n",
    "                    )\n",
    "                    .merge(hapax_subdf, on=\"index\")\n",
    "                    .merge(\n",
    "                        shannon_subdf,\n",
    "                        on=\"index\",\n",
    "                    )\n",
    "                    .merge(\n",
    "                        simpsons_subdf,\n",
    "                        on=\"index\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # get the mean of all the value_* columns normalized values\n",
    "                from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                value_avg_dep_normalized = scaler.fit_transform(\n",
    "                    merged_df[\"value_avg_dep\"].values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "                scaler = StandardScaler()\n",
    "                value_type_token_normalized = scaler.fit_transform(\n",
    "                    merged_df[\"value_type_token\"].values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "                scaler = StandardScaler()\n",
    "                value_hapax_normalized = scaler.fit_transform(\n",
    "                    merged_df[\"value_hapax\"].values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "                scaler = StandardScaler()\n",
    "                value_shannon_normalized = scaler.fit_transform(\n",
    "                    merged_df[\"value_shannon\"].values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "                scaler = StandardScaler()\n",
    "                value_simpsons_normalized = scaler.fit_transform(\n",
    "                    merged_df[\"value_simpsons\"].values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "                merged_df[\"value\"] = np.mean(\n",
    "                    [\n",
    "                        value_avg_dep_normalized,\n",
    "                        value_type_token_normalized,\n",
    "                        value_hapax_normalized,\n",
    "                        value_shannon_normalized,\n",
    "                        value_simpsons_normalized,\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "                mean_values.append(merged_df[\"value\"].values)\n",
    "                avg_dep_values.append(merged_df[\"value_avg_dep\"].values)\n",
    "                type_token_values.append(merged_df[\"value_type_token\"].values)\n",
    "                hapax_values.append(merged_df[\"value_hapax\"].values)\n",
    "                shannon_values.append(merged_df[\"value_shannon\"].values)\n",
    "                simpsons_values.append(merged_df[\"value_simpsons\"].values)\n",
    "                filtered_llms.append(llm)\n",
    "                filtered_modes.append(mode)\n",
    "                filtered_datasets.append(dataset)\n",
    "                filtered_is_originals.append(is_original)\n",
    "\n",
    "filtered_df = pd.DataFrame(\n",
    "    {\n",
    "        \"llm\": filtered_llms,\n",
    "        \"mode\": filtered_modes,\n",
    "        \"dataset\": filtered_datasets,\n",
    "        \"is_original\": filtered_is_originals,\n",
    "        \"mean\": mean_values,\n",
    "        \"avg_dep\": avg_dep_values,\n",
    "        \"type_token\": type_token_values,\n",
    "        \"hapax\": hapax_values,\n",
    "        \"shannon\": shannon_values,\n",
    "        \"simpsons\": simpsons_values,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "analysis for:  mean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>significance</th>\n",
       "      <th>statistic</th>\n",
       "      <th>significance</th>\n",
       "      <th>statistic</th>\n",
       "      <th>significance</th>\n",
       "      <th>statistic</th>\n",
       "      <th>significance</th>\n",
       "      <th>statistic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>essays</th>\n",
       "      <th>essays</th>\n",
       "      <th>facebook</th>\n",
       "      <th>facebook</th>\n",
       "      <th>political</th>\n",
       "      <th>political</th>\n",
       "      <th>wassa</th>\n",
       "      <th>wassa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm</th>\n",
       "      <th>mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gemini</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0151 -&gt; 0.0112 (0.0)*</td>\n",
       "      <td>39.233</td>\n",
       "      <td>0.0092 -&gt; 0.0074 (0.0)*</td>\n",
       "      <td>119.808</td>\n",
       "      <td>0.0197 -&gt; 0.0083 (0.0)*</td>\n",
       "      <td>131.365</td>\n",
       "      <td>0.0079 -&gt; 0.0071 (0.958)</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0151 -&gt; 0.0089 (0.0)*</td>\n",
       "      <td>103.269</td>\n",
       "      <td>0.0092 -&gt; 0.0094 (0.0)*</td>\n",
       "      <td>36.435</td>\n",
       "      <td>0.0197 -&gt; 0.0123 (0.0)*</td>\n",
       "      <td>37.274</td>\n",
       "      <td>0.0078 -&gt; 0.0075 (0.826)</td>\n",
       "      <td>0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gpt</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0151 -&gt; 0.0152 (0.216)</td>\n",
       "      <td>1.530</td>\n",
       "      <td>0.0092 -&gt; 0.006 (0.0)*</td>\n",
       "      <td>93.474</td>\n",
       "      <td>0.0197 -&gt; 0.0108 (0.0)*</td>\n",
       "      <td>65.211</td>\n",
       "      <td>0.008 -&gt; 0.0052 (0.164)</td>\n",
       "      <td>1.959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0151 -&gt; 0.0153 (0.17)</td>\n",
       "      <td>1.880</td>\n",
       "      <td>0.0092 -&gt; 0.0075 (0.0)*</td>\n",
       "      <td>63.159</td>\n",
       "      <td>0.0197 -&gt; 0.0118 (0.0)*</td>\n",
       "      <td>47.236</td>\n",
       "      <td>0.008 -&gt; 0.0069 (0.407)</td>\n",
       "      <td>0.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">llama</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.015 -&gt; 0.0159 (0.63)</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0083 -&gt; 0.0073 (0.13)</td>\n",
       "      <td>2.299</td>\n",
       "      <td>0.0197 -&gt; 0.0138 (0.0)*</td>\n",
       "      <td>62.600</td>\n",
       "      <td>0.008 -&gt; 0.0062 (0.532)</td>\n",
       "      <td>0.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.015 -&gt; 0.0124 (0.0)*</td>\n",
       "      <td>127.087</td>\n",
       "      <td>0.0082 -&gt; 0.007 (0.004)</td>\n",
       "      <td>8.395</td>\n",
       "      <td>0.0197 -&gt; 0.0152 (0.0)*</td>\n",
       "      <td>36.412</td>\n",
       "      <td>0.008 -&gt; 0.0105 (0.372)</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   significance statistic  \\\n",
       "dataset                                  essays    essays   \n",
       "llm    mode                                                 \n",
       "gemini rephrase         0.0151 -> 0.0112 (0.0)*    39.233   \n",
       "       syntax_grammar   0.0151 -> 0.0089 (0.0)*   103.269   \n",
       "gpt    rephrase        0.0151 -> 0.0152 (0.216)     1.530   \n",
       "       syntax_grammar   0.0151 -> 0.0153 (0.17)     1.880   \n",
       "llama  rephrase          0.015 -> 0.0159 (0.63)     0.232   \n",
       "       syntax_grammar    0.015 -> 0.0124 (0.0)*   127.087   \n",
       "\n",
       "                                  significance statistic  \\\n",
       "dataset                               facebook  facebook   \n",
       "llm    mode                                                \n",
       "gemini rephrase        0.0092 -> 0.0074 (0.0)*   119.808   \n",
       "       syntax_grammar  0.0092 -> 0.0094 (0.0)*    36.435   \n",
       "gpt    rephrase         0.0092 -> 0.006 (0.0)*    93.474   \n",
       "       syntax_grammar  0.0092 -> 0.0075 (0.0)*    63.159   \n",
       "llama  rephrase        0.0083 -> 0.0073 (0.13)     2.299   \n",
       "       syntax_grammar  0.0082 -> 0.007 (0.004)     8.395   \n",
       "\n",
       "                                  significance statistic  \\\n",
       "dataset                              political political   \n",
       "llm    mode                                                \n",
       "gemini rephrase        0.0197 -> 0.0083 (0.0)*   131.365   \n",
       "       syntax_grammar  0.0197 -> 0.0123 (0.0)*    37.274   \n",
       "gpt    rephrase        0.0197 -> 0.0108 (0.0)*    65.211   \n",
       "       syntax_grammar  0.0197 -> 0.0118 (0.0)*    47.236   \n",
       "llama  rephrase        0.0197 -> 0.0138 (0.0)*    62.600   \n",
       "       syntax_grammar  0.0197 -> 0.0152 (0.0)*    36.412   \n",
       "\n",
       "                                   significance statistic  \n",
       "dataset                                   wassa     wassa  \n",
       "llm    mode                                                \n",
       "gemini rephrase        0.0079 -> 0.0071 (0.958)     0.003  \n",
       "       syntax_grammar  0.0078 -> 0.0075 (0.826)     0.048  \n",
       "gpt    rephrase         0.008 -> 0.0052 (0.164)     1.959  \n",
       "       syntax_grammar   0.008 -> 0.0069 (0.407)     0.693  \n",
       "llama  rephrase         0.008 -> 0.0062 (0.532)     0.393  \n",
       "       syntax_grammar   0.008 -> 0.0105 (0.372)     0.802  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrlrlrlr}\n",
      "\\toprule\n",
      " &  & significance & statistic & significance & statistic & significance & statistic & significance & statistic \\\\\n",
      " & dataset & essays & essays & facebook & facebook & political & political & wassa & wassa \\\\\n",
      "llm & mode &  &  &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{2}{*}{gemini} & rephrase & 0.0151 -> 0.0112 (0.0)* & 39.233000 & 0.0092 -> 0.0074 (0.0)* & 119.808000 & 0.0197 -> 0.0083 (0.0)* & 131.365000 & 0.0079 -> 0.0071 (0.958) & 0.003000 \\\\\n",
      " & syntax_grammar & 0.0151 -> 0.0089 (0.0)* & 103.269000 & 0.0092 -> 0.0094 (0.0)* & 36.435000 & 0.0197 -> 0.0123 (0.0)* & 37.274000 & 0.0078 -> 0.0075 (0.826) & 0.048000 \\\\\n",
      "\\cline{1-10}\n",
      "\\multirow[t]{2}{*}{gpt} & rephrase & 0.0151 -> 0.0152 (0.216) & 1.530000 & 0.0092 -> 0.006 (0.0)* & 93.474000 & 0.0197 -> 0.0108 (0.0)* & 65.211000 & 0.008 -> 0.0052 (0.164) & 1.959000 \\\\\n",
      " & syntax_grammar & 0.0151 -> 0.0153 (0.17) & 1.880000 & 0.0092 -> 0.0075 (0.0)* & 63.159000 & 0.0197 -> 0.0118 (0.0)* & 47.236000 & 0.008 -> 0.0069 (0.407) & 0.693000 \\\\\n",
      "\\cline{1-10}\n",
      "\\multirow[t]{2}{*}{llama} & rephrase & 0.015 -> 0.0159 (0.63) & 0.232000 & 0.0083 -> 0.0073 (0.13) & 2.299000 & 0.0197 -> 0.0138 (0.0)* & 62.600000 & 0.008 -> 0.0062 (0.532) & 0.393000 \\\\\n",
      " & syntax_grammar & 0.015 -> 0.0124 (0.0)* & 127.087000 & 0.0082 -> 0.007 (0.004) & 8.395000 & 0.0197 -> 0.0152 (0.0)* & 36.412000 & 0.008 -> 0.0105 (0.372) & 0.802000 \\\\\n",
      "\\cline{1-10}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "\n",
    "for col in [\n",
    "    \"mean\",\n",
    "]:\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"analysis for: \", col)\n",
    "\n",
    "    final_datasets = []\n",
    "    final_llms = []\n",
    "    final_modes = []\n",
    "    final_significances = []\n",
    "    final_stats = []\n",
    "\n",
    "    for dataset in filtered_df[\"dataset\"].unique():\n",
    "        for llm in filtered_df[\"llm\"].unique():\n",
    "            for mode in filtered_df[\"mode\"].unique():\n",
    "                final_datasets.append(dataset)\n",
    "                final_llms.append(llm)\n",
    "                final_modes.append(mode)\n",
    "                original_values = filtered_df[\n",
    "                    (filtered_df[\"dataset\"] == dataset)\n",
    "                    & (filtered_df[\"llm\"] == llm)\n",
    "                    & (filtered_df[\"mode\"] == mode)\n",
    "                    & (filtered_df[\"is_original\"] == True)\n",
    "                ][col].values[0]\n",
    "                rewritten_values = filtered_df[\n",
    "                    (filtered_df[\"dataset\"] == dataset)\n",
    "                    & (filtered_df[\"llm\"] == llm)\n",
    "                    & (filtered_df[\"mode\"] == mode)\n",
    "                    & (filtered_df[\"is_original\"] == False)\n",
    "                ][col].values[0]\n",
    "                stats, p = levene(original_values, rewritten_values)\n",
    "                p = np.round(p, 3)\n",
    "                stats = np.round(stats, 3)\n",
    "                final_stats.append(stats)\n",
    "                original_vars = np.round(np.var(original_values), 4)\n",
    "                rewritten_vars = np.round(np.var(rewritten_values), 4)\n",
    "                if p <= 0.001:\n",
    "                    final_significances.append(\n",
    "                        f\"{original_vars} -> {rewritten_vars} ({p})*\"\n",
    "                    )\n",
    "                else:\n",
    "                    final_significances.append(\n",
    "                        f\"{original_vars} -> {rewritten_vars} ({p})\"\n",
    "                    )\n",
    "\n",
    "    final_df = pd.DataFrame(\n",
    "        {\n",
    "            \"dataset\": final_datasets,\n",
    "            \"llm\": final_llms,\n",
    "            \"mode\": final_modes,\n",
    "            \"significance\": final_significances,\n",
    "            \"statistic\": final_stats,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    display(\n",
    "        final_df.pivot_table(\n",
    "            index=[\"llm\", \"mode\"],\n",
    "            columns=\"dataset\",\n",
    "            values=[\"significance\", \"statistic\"],\n",
    "            aggfunc=lambda x: x,\n",
    "        ).reindex(\n",
    "            columns=[\n",
    "                (\"significance\", \"essays\"),\n",
    "                (\"statistic\", \"essays\"),\n",
    "                (\"significance\", \"facebook\"),\n",
    "                (\"statistic\", \"facebook\"),\n",
    "                (\"significance\", \"political\"),\n",
    "                (\"statistic\", \"political\"),\n",
    "                (\"significance\", \"wassa\"),\n",
    "                (\"statistic\", \"wassa\"),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        final_df.pivot_table(\n",
    "            index=[\"llm\", \"mode\"],\n",
    "            columns=\"dataset\",\n",
    "            values=[\"significance\", \"statistic\"],\n",
    "            aggfunc=lambda x: x,\n",
    "        )\n",
    "        .reindex(\n",
    "            columns=[\n",
    "                (\"significance\", \"essays\"),\n",
    "                (\"statistic\", \"essays\"),\n",
    "                (\"significance\", \"facebook\"),\n",
    "                (\"statistic\", \"facebook\"),\n",
    "                (\"significance\", \"political\"),\n",
    "                (\"statistic\", \"political\"),\n",
    "                (\"significance\", \"wassa\"),\n",
    "                (\"statistic\", \"wassa\"),\n",
    "            ]\n",
    "        )\n",
    "        .to_latex()\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
