library(dplyr)
library(ggplot2)
library(tidyr)
library(tseries)
library(vars)
library(readr)
library(lmtest)
data_sources = c("papers", "reddit", "news")
for (data_source_index in 1:length(data_sources)) {
data_source = data_sources[data_source_index]
data = read_csv(paste(
"data",
"/",
data_source,
"/",
data_source,
"_final_features.csv",
sep = ""
))
colnames = c(
"complexity"
)
lags_list = c()
p_values = c()
p_values_reverse = c()
col_values = c()
for (col in colnames) {
data$date = as.Date(data$date)
# sort the data based on date
data = data %>% arrange(date)
# plot the complexity over the date
data %>% ggplot(aes(x = date, y = col)) +
geom_line() +
labs(title = "Feature over time", x = "Date", y = "Complexity")
data %>% ggplot(aes(x = date, y = ai_written)) +
geom_line() +
labs(title = "AI written over time", x = "Date", y = "AI written")
# check that the data would be stationary using the adf test
adf.test(data[[col]], alternative = "stationary")
# check the autocorrelation and partial autocorrelation
# acf(data$complexity)
# pacf(data$complexity) # as far as the result goes, we only have to include the first lag
# making the data stationary
adf.test(diff(data[[col]]), alternative = "stationary")
adf.test(diff(data$ai_written), alternative = "stationary")
# plot the differenced version of both complexity and ai_written
diff_complexity = diff(data[[col]])
diff_ai_written = diff(data$ai_written)
new_date = data$date[-1]
data_diff = data.frame(date = new_date, diff_complexity, diff_ai_written)
# data_diff %>%
# ggplot(aes(x = date, y = diff_complexity)) +
# geom_line() +
# labs(title = "Differenced Feature over time", x = "Date", y = "Differenced Feature")
# data_diff %>%
# ggplot(aes(x = date, y = diff_ai_written)) +
# geom_line() +
# labs(title = "Differenced AI written over time", x = "Date", y = "Differenced AI written")
# check the correlation of the diff_ai_written and diff_complexity separately and also together
# remove the na values before checking the correlation
# acf(data_diff$diff_complexity)
# pacf(data_diff$diff_complexity)
# acf(data_diff$diff_ai_written)
# pacf(data_diff$diff_ai_written)
# it looks like that we only have lag 1 ai_written but not for complexity
# now let's check the correlation between the two using cross correlation, whether basically ai_written is granger causing complexity
# add the feature to the label of the plot for ccf(data_diff$diff_ai_written, data_diff$diff_complexity)
# cc = ccf(data_diff$diff_ai_written, data_diff$diff_complexity)
# add the data source too
# plot(cc, main = paste("Cross-correlation between AI written and", col, "for", data_source))
# print(ccf(data_diff$diff_ai_written, data_diff$diff_complexity))
# now let's do grangertest
# for multiple lags and store the results all in a dataframe to check in aggregate (for the first 20 lags)
for (i in 1:20) {
granger_result = grangertest(
data_diff$diff_complexity ~ data_diff$diff_ai_written,
order = i,
data = data_diff
)
lags_list = c(lags_list, i)
p_values = c(p_values, granger_result$`Pr(>F)`[[2]])
granger_result_2 = grangertest(
data_diff$diff_ai_written ~ data_diff$diff_complexity,
order = i,
data = data_diff
)
p_values_reverse = c(p_values_reverse, granger_result_2$`Pr(>F)`[[2]])
col_values = c(col_values, col)
}
}
granger_test_results = data.frame(lags_list, p_values, p_values_reverse, col_values)
# save the results in the granger test resutls {data_source}_granger_test_results.csv
write_csv(granger_test_results, paste(
"data",
"/",
data_source,
"/",
data_source,
"_granger_test_results.csv",
sep = ""
))
}
# Load required libraries
library(tidyverse)
library(nlme)        # For GLS models
library(broom)       # For model summaries
library(ggplot2)     # Visualization
library(tseries)
# =====================
# Step 1: Load the Data
# =====================
## go over all the data sources and for each, read the file "data_sources/data_sources_prefixes_pca.csv"
# Function to fit GLS for a given column
fit_gls_papers <- function(column_name) {
formula <- as.formula(
paste(
column_name,
"~ time + ONSET_chatgpt + POST_chatgpt"
)
)
# Fit GLS model with AR(1) correlation structure
model <- gls(formula,
data = data,
correlation = corAR1(form = ~ as.numeric(time)))
return(model)
}
fit_gls_reddit <- function(column_name) {
formula <- as.formula(
paste(
column_name,
"~ time + ONSET_chatgpt + POST_chatgpt"
)
)
# Fit GLS model with AR(1) correlation structure
model <- gls(formula,
data = data,
correlation = corAR1(form = ~ as.numeric(time)))
return(model)
}
fit_gls_news <- function(column_name) {
formula <- as.formula(
paste(
column_name,
"~ time + ONSET_chatgpt + POST_chatgpt"
)
)
# Fit GLS model with AR(1) correlation structure
model <- gls(formula,
data = data,
correlation = corAR1(form = ~ as.numeric(time)))
return(model)
}
extract_gls_results <- function(model, column_name, conf_level = 0.95) {
summary_model <- summary(model)
coefficients <- summary_model$tTable
confidence_intervals <- confint(model, level = conf_level)  # Get confidence intervals
# Convert coefficients to a data frame
results_df <- as.data.frame(coefficients) %>%
rownames_to_column(var = "term") %>%
mutate(
PC = column_name,
estimate = Value,
std.error = Std.Error,
statistic = `t-value`,
p.value = `p-value`,
conf.low = confidence_intervals[, 1],  # Lower bound of CI
conf.high = confidence_intervals[, 2]  # Upper bound of CI
) %>%
select(PC, term, estimate, std.error, statistic, p.value, conf.low, conf.high)
return(results_df)
}
# ==========================
# Step 5: Assumption Checks
# ==========================
# Function to check residual autocorrelation
check_residuals <- function(model, column_name) {
residuals <- residuals(model, type = "normalized")
# Perform Durbin-Watson Test
dw_test <- nlme::corAR1(form = ~ as.numeric(time))
cat("### Assumption Checks for:", column_name, "###\n")
print(summary(dw_test))
}
visualize_gls <- function(column_name, model) {
ggplot(data, aes(x = time, y = .data[[column_name]])) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", color = "blue") +
labs(
title = paste("GLS Model for", column_name),
x = "Time",
y = column_name
) +
theme_minimal()
}
data_sources = c("papers", "reddit", "news")
for (data_source_index in 1:length(data_sources)) {
data_source = data_sources[data_source_index]
print(data_source)
data = read_csv(paste(
"data",
"/",
data_source,
"/",
data_source,
"_final_features.csv",
sep = ""
))
if (data_source == "news") {
data <- data %>%
mutate(
time = as.Date(date),
ONSET_chatgpt = ifelse(time >= as.Date("2022-11-01"), 1, 0),
POST_chatgpt = ifelse(ONSET_chatgpt == 1, as.numeric(time - as.Date("2022-11-01")), 0),
ONSET_gpt4 = ifelse(time >= as.Date("2023-03-14"), 1, 0),
POST_gpt4 = ifelse(ONSET_gpt4 == 1, as.numeric(time - as.Date("2023-03-14")), 0),
ONSET_claude = ifelse(time >= as.Date("2023-07-11"), 1, 0),
POST_claude = ifelse(ONSET_claude == 1, as.numeric(time - as.Date("2023-07-11")), 0),
)
} else {
data <- data %>%
mutate(
time = as.Date(date),
ONSET_chatgpt = ifelse(time >= as.Date("2022-11-01"), 1, 0),
POST_chatgpt = ifelse(ONSET_chatgpt == 1, as.numeric(time - as.Date("2022-11-01")), 0),
ONSET_gpt4 = ifelse(time >= as.Date("2023-03-14"), 1, 0),
POST_gpt4 = ifelse(ONSET_gpt4 == 1, as.numeric(time - as.Date("2023-03-14")), 0),
ONSET_gpt4o = ifelse(time >= as.Date("2024-05-13"), 1, 0),
POST_gpt4o = ifelse(ONSET_gpt4o == 1, as.numeric(time - as.Date("2024-05-13")), 0),
ONSET_claude = ifelse(time >= as.Date("2023-07-11"), 1, 0),
POST_claude = ifelse(ONSET_claude == 1, as.numeric(time - as.Date("2023-07-11")), 0),
ONSET_gemini = ifelse(time >= as.Date("2023-12-06"), 1, 0),
POST_gemini = ifelse(ONSET_gemini == 1, as.numeric(time - as.Date("2023-12-06")), 0)
)
}
pacf(data$complexity)
# filter the data to contain only the data after 2018
# Select only PC columns
pc_columns <- array(c("complexity"), dim = 1)
if (data_source == "papers") {
gls_models <- map(pc_columns, fit_gls_papers)
} else if (data_source == "reddit") {
gls_models <- map(pc_columns, fit_gls_reddit)
} else {
gls_models <- map(pc_columns, fit_gls_news)
}
gls_results <- map_df(seq_along(pc_columns),
~ extract_gls_results(gls_models[[.x]], pc_columns[.x]))
# Filter for significant results
significant_gls_results <- gls_results
# Run assumption checks for significant models
significant_pcs <- unique(significant_gls_results$PC)
significant_gls_models <- gls_models[pc_columns %in% significant_pcs]
map2(significant_gls_models,
significant_pcs,
~ check_residuals(.x, .y))
# ===========================
# Step 7: Export Results
# ===========================
write_csv(
significant_gls_results,
paste(
"data",
"/",
data_source,
"/",
data_source,
"_",
"significant_results_final_features.csv",
sep = ""
)
)
}
setwd("~/Desktop/LLMs-effect-on-linguistic-diversity/linguistic_diversity_analysis")
# Load required libraries
library(tidyverse)
library(nlme)        # For GLS models
library(broom)       # For model summaries
library(ggplot2)     # Visualization
library(tseries)
# =====================
# Step 1: Load the Data
# =====================
## go over all the data sources and for each, read the file "data_sources/data_sources_prefixes_pca.csv"
# Function to fit GLS for a given column
fit_gls_papers <- function(column_name) {
formula <- as.formula(
paste(
column_name,
"~ time + ONSET_chatgpt + POST_chatgpt"
)
)
# Fit GLS model with AR(1) correlation structure
model <- gls(formula,
data = data,
correlation = corAR1(form = ~ as.numeric(time)))
return(model)
}
fit_gls_reddit <- function(column_name) {
formula <- as.formula(
paste(
column_name,
"~ time + ONSET_chatgpt + POST_chatgpt"
)
)
# Fit GLS model with AR(1) correlation structure
model <- gls(formula,
data = data,
correlation = corAR1(form = ~ as.numeric(time)))
return(model)
}
fit_gls_news <- function(column_name) {
formula <- as.formula(
paste(
column_name,
"~ time + ONSET_chatgpt + POST_chatgpt"
)
)
# Fit GLS model with AR(1) correlation structure
model <- gls(formula,
data = data,
correlation = corAR1(form = ~ as.numeric(time)))
return(model)
}
extract_gls_results <- function(model, column_name, conf_level = 0.95) {
summary_model <- summary(model)
coefficients <- summary_model$tTable
confidence_intervals <- confint(model, level = conf_level)  # Get confidence intervals
# Convert coefficients to a data frame
results_df <- as.data.frame(coefficients) %>%
rownames_to_column(var = "term") %>%
mutate(
PC = column_name,
estimate = Value,
std.error = Std.Error,
statistic = `t-value`,
p.value = `p-value`,
conf.low = confidence_intervals[, 1],  # Lower bound of CI
conf.high = confidence_intervals[, 2]  # Upper bound of CI
) %>%
select(PC, term, estimate, std.error, statistic, p.value, conf.low, conf.high)
return(results_df)
}
# ==========================
# Step 5: Assumption Checks
# ==========================
# Function to check residual autocorrelation
check_residuals <- function(model, column_name) {
residuals <- residuals(model, type = "normalized")
# Perform Durbin-Watson Test
dw_test <- nlme::corAR1(form = ~ as.numeric(time))
cat("### Assumption Checks for:", column_name, "###\n")
print(summary(dw_test))
}
visualize_gls <- function(column_name, model) {
ggplot(data, aes(x = time, y = .data[[column_name]])) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", color = "blue") +
labs(
title = paste("GLS Model for", column_name),
x = "Time",
y = column_name
) +
theme_minimal()
}
data_sources = c("papers", "reddit", "news")
for (data_source_index in 1:length(data_sources)) {
data_source = data_sources[data_source_index]
print(data_source)
data = read_csv(paste(
"data",
"/",
data_source,
"/",
data_source,
"_final_features.csv",
sep = ""
))
if (data_source == "news") {
data <- data %>%
mutate(
time = as.Date(date),
ONSET_chatgpt = ifelse(time >= as.Date("2022-11-01"), 1, 0),
POST_chatgpt = ifelse(ONSET_chatgpt == 1, as.numeric(time - as.Date("2022-11-01")), 0),
ONSET_gpt4 = ifelse(time >= as.Date("2023-03-14"), 1, 0),
POST_gpt4 = ifelse(ONSET_gpt4 == 1, as.numeric(time - as.Date("2023-03-14")), 0),
ONSET_claude = ifelse(time >= as.Date("2023-07-11"), 1, 0),
POST_claude = ifelse(ONSET_claude == 1, as.numeric(time - as.Date("2023-07-11")), 0),
)
} else {
data <- data %>%
mutate(
time = as.Date(date),
ONSET_chatgpt = ifelse(time >= as.Date("2022-11-01"), 1, 0),
POST_chatgpt = ifelse(ONSET_chatgpt == 1, as.numeric(time - as.Date("2022-11-01")), 0),
ONSET_gpt4 = ifelse(time >= as.Date("2023-03-14"), 1, 0),
POST_gpt4 = ifelse(ONSET_gpt4 == 1, as.numeric(time - as.Date("2023-03-14")), 0),
ONSET_gpt4o = ifelse(time >= as.Date("2024-05-13"), 1, 0),
POST_gpt4o = ifelse(ONSET_gpt4o == 1, as.numeric(time - as.Date("2024-05-13")), 0),
ONSET_claude = ifelse(time >= as.Date("2023-07-11"), 1, 0),
POST_claude = ifelse(ONSET_claude == 1, as.numeric(time - as.Date("2023-07-11")), 0),
ONSET_gemini = ifelse(time >= as.Date("2023-12-06"), 1, 0),
POST_gemini = ifelse(ONSET_gemini == 1, as.numeric(time - as.Date("2023-12-06")), 0)
)
}
pacf(data$complexity)
# filter the data to contain only the data after 2018
# Select only PC columns
pc_columns <- array(c("complexity"), dim = 1)
if (data_source == "papers") {
gls_models <- map(pc_columns, fit_gls_papers)
} else if (data_source == "reddit") {
gls_models <- map(pc_columns, fit_gls_reddit)
} else {
gls_models <- map(pc_columns, fit_gls_news)
}
gls_results <- map_df(seq_along(pc_columns),
~ extract_gls_results(gls_models[[.x]], pc_columns[.x]))
# Filter for significant results
significant_gls_results <- gls_results
# Run assumption checks for significant models
significant_pcs <- unique(significant_gls_results$PC)
significant_gls_models <- gls_models[pc_columns %in% significant_pcs]
map2(significant_gls_models,
significant_pcs,
~ check_residuals(.x, .y))
# ===========================
# Step 7: Export Results
# ===========================
write_csv(
significant_gls_results,
paste(
"data",
"/",
data_source,
"/",
data_source,
"_",
"significant_results_final_features.csv",
sep = ""
)
)
}
rm(list = ls())
