{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy, levene\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import collections as coll\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def RemoveSpecialCHs(text):\n",
    "    text = word_tokenize(text)\n",
    "    st = [\n",
    "        \",\",\n",
    "        \".\",\n",
    "        \"'\",\n",
    "        \"!\",\n",
    "        '\"',\n",
    "        \"#\",\n",
    "        \"$\",\n",
    "        \"%\",\n",
    "        \"&\",\n",
    "        \"(\",\n",
    "        \")\",\n",
    "        \"*\",\n",
    "        \"+\",\n",
    "        \"-\",\n",
    "        \".\",\n",
    "        \"/\",\n",
    "        \":\",\n",
    "        \";\",\n",
    "        \"<\",\n",
    "        \"=\",\n",
    "        \">\",\n",
    "        \"?\",\n",
    "        \"@\",\n",
    "        \"[\",\n",
    "        \"\\\\\",\n",
    "        \"]\",\n",
    "        \"^\",\n",
    "        \"_\",\n",
    "        \"`\",\n",
    "        \"{\",\n",
    "        \"|\",\n",
    "        \"}\",\n",
    "        \"~\",\n",
    "        \"\\t\",\n",
    "        \"\\n\",\n",
    "    ]\n",
    "\n",
    "    words = [word for word in text if word not in st]\n",
    "    return words\n",
    "\n",
    "\n",
    "def compute_avg_dependency_link_length(text):\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        link_lengths = []\n",
    "        for sent in doc.sents:\n",
    "            sent_link_lengths = []\n",
    "            for token in sent:\n",
    "                if token.dep_ != \"ROOT\":\n",
    "                    head = token.head\n",
    "                    sent_link_lengths.append(abs(head.i - token.i))\n",
    "            if sent_link_lengths:  # Only append if sentence had any links\n",
    "                link_lengths.append(np.mean(sent_link_lengths))\n",
    "        return np.mean(link_lengths)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def typeTokenRatio(text):\n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "        return len(set(words)) / len(words)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def hapaxLegemena(text):\n",
    "    try:\n",
    "        words = RemoveSpecialCHs(text)\n",
    "        V1 = 0\n",
    "        # dictionary comprehension . har word kay against value 0 kardi\n",
    "        freqs = {key: 0 for key in words}\n",
    "        for word in words:\n",
    "            freqs[word] += 1\n",
    "        for word in freqs:\n",
    "            if freqs[word] == 1:\n",
    "                V1 += 1\n",
    "        N = len(words)\n",
    "        V = float(len(set(words)))\n",
    "        R = 100 * math.log(N) / max(1, (1 - (V1 / V)))\n",
    "        h = V1 / N\n",
    "        return R, h\n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "\n",
    "def ShannonEntropy(text):\n",
    "    try:\n",
    "        words = RemoveSpecialCHs(text)\n",
    "        lenght = len(words)\n",
    "        freqs = coll.Counter()\n",
    "        freqs.update(words)\n",
    "        arr = np.array(list(freqs.values()))\n",
    "        distribution = 1.0 * arr\n",
    "        distribution /= max(1, lenght)\n",
    "        import scipy as sc\n",
    "\n",
    "        H = sc.stats.entropy(distribution, base=2)\n",
    "        # H = sum([(i/lenght)*math.log(i/lenght,math.e) for i in freqs.values()])\n",
    "        return H\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def SimpsonsIndex(text):\n",
    "    try:\n",
    "        words = RemoveSpecialCHs(text)\n",
    "        freqs = coll.Counter()\n",
    "        freqs.update(words)\n",
    "        N = len(words)\n",
    "        n = sum([1.0 * i * (i - 1) for i in freqs.values()])\n",
    "        D = 1 - (n / (N * (N - 1)))\n",
    "        return D\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_features = [\n",
    "    \"avg_dependency_link_length\",\n",
    "    \"type_token_ratio\",\n",
    "    \"hapax_legemena\",\n",
    "    \"shannon_entropy\",\n",
    "    \"simpsons_index\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_complexity_features(df, text_column):\n",
    "    df[f\"{text_column}_avg_dependency_link_length\"] = [\n",
    "        compute_avg_dependency_link_length(text)\n",
    "        for text in tqdm(df[text_column], leave=False)\n",
    "    ]\n",
    "\n",
    "    df[f\"{text_column}_type_token_ratio\"] = [\n",
    "        typeTokenRatio(text) for text in tqdm(df[text_column], leave=False)\n",
    "    ]\n",
    "    df[f\"{text_column}_hapax_legemena\"] = [\n",
    "        hapaxLegemena(text)[1] for text in tqdm(df[text_column], leave=False)\n",
    "    ]\n",
    "    df[f\"{text_column}_shannon_entropy\"] = [\n",
    "        ShannonEntropy(text) for text in tqdm(df[text_column], leave=False)\n",
    "    ]\n",
    "    df[f\"{text_column}_simpsons_index\"] = [\n",
    "        SimpsonsIndex(text) for text in tqdm(df[text_column], leave=False)\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_complexity_features_for_all_texts(df):\n",
    "    df = add_complexity_features(df, \"original_text\")\n",
    "    df = add_complexity_features(df, \"rewritten_text\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    all_data = None\n",
    "    for llm in [\"gpt\", \"gemini\", \"llama\"]:\n",
    "        for dataset_name in [\"papers\", \"news\", \"reddit\"]:\n",
    "            for rewrite_type in [\"syntax_grammar\", \"rephrase\"]:  # ,\n",
    "                log_file_name = f\"{dataset_name}_rewritten_{rewrite_type}_{llm}.json\"\n",
    "                with open(os.path.join(dataset_name, log_file_name)) as f:\n",
    "                    data = pd.read_json(f, orient=\"index\")\n",
    "                    data[\"index\"] = data.index\n",
    "                    data[\"dataset\"] = dataset_name\n",
    "                    data[\"rewrite_type\"] = rewrite_type\n",
    "                    data[\"llm\"] = llm\n",
    "                    data.reset_index(drop=True, inplace=True)\n",
    "                    if all_data is None:\n",
    "                        all_data = data\n",
    "                    else:\n",
    "                        all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/_dcfmqnx4mxbhf22rbww8kwc0000gp/T/ipykernel_81388/1419015784.py:12: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  papers_data = pd.read_csv(\"papers/cl_cv_papers.csv\")\n"
     ]
    }
   ],
   "source": [
    "reddit_data = pd.read_csv(\"reddit/filtered_comments.csv\")\n",
    "# text columns is \"body\" and the id column is \"name\"\n",
    "reddit_data[\"id\"] = reddit_data[\"name\"]\n",
    "reddit_data[\"text\"] = reddit_data[\"body\"]\n",
    "reddit_data[\"id\"] = reddit_data[\"id\"].astype(str)\n",
    "\n",
    "news_data = pd.read_csv(\"news/news_data.csv\")\n",
    "# text column is \"text\", and since there's no id column, we'll use the index\n",
    "news_data[\"id\"] = news_data.index\n",
    "news_data[\"id\"] = news_data[\"id\"].astype(str)\n",
    "\n",
    "papers_data = pd.read_csv(\"papers/cl_cv_papers.csv\")\n",
    "papers_data[\"final_date\"] = pd.to_datetime(papers_data[\"update_date\"])\n",
    "papers_data[\"year\"] = papers_data[\"final_date\"].dt.year\n",
    "papers_data[\"month\"] = papers_data[\"final_date\"].dt.month\n",
    "# text column is \"abstract\", and the id column is \"id\"\n",
    "papers_data[\"text\"] = papers_data[\"abstract\"]\n",
    "papers_data[\"id\"] = papers_data[\"id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_indices_to_years = {}\n",
    "for data in [reddit_data, news_data, papers_data]:\n",
    "    all_data_indices_to_years.update(dict(zip(data[\"id\"], data[\"year\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = load_data()\n",
    "all_data = add_complexity_features_for_all_texts(all_data)\n",
    "all_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"id\"] = all_data[\"index\"].astype(str)\n",
    "all_data[\"year\"] = all_data[\"id\"].map(all_data_indices_to_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_features_and_entropies(df):\n",
    "    llms = []\n",
    "    modes = []\n",
    "    cols = []\n",
    "    datasets = []\n",
    "    distribution_values = []\n",
    "    dataset_original_or_not = []\n",
    "    all_indices = []\n",
    "    for dataset in tqdm(df[\"dataset\"].unique(), leave=False):\n",
    "        for llm in tqdm(df[df[\"dataset\"] == dataset][\"llm\"].unique(), leave=False):\n",
    "            for mode in tqdm(\n",
    "                df[(df[\"dataset\"] == dataset) & (df[\"llm\"] == llm)][\n",
    "                    \"rewrite_type\"\n",
    "                ].unique(),\n",
    "                leave=False,\n",
    "            ):\n",
    "                for col in tqdm(complexity_features, leave=False):\n",
    "                    for prefix in [\"original_text\", \"rewritten_text\"]:\n",
    "                        data = df[\n",
    "                            (df[\"dataset\"] == dataset)\n",
    "                            & (df[\"llm\"] == llm)\n",
    "                            & (df[\"rewrite_type\"] == mode)\n",
    "                        ]\n",
    "                        data = data[data[f\"{prefix}_{col}\"].notna()]\n",
    "\n",
    "                        # make the year would be before 2022\n",
    "                        data = data[data[\"year\"] < 2023]\n",
    "\n",
    "                        values = data[f\"{prefix}_{col}\"].tolist()\n",
    "                        indices = data[\"index\"].tolist()\n",
    "\n",
    "                        # scaler = StandardScaler()\n",
    "                        # normalized_values = scaler.fit_transform(\n",
    "                        #     np.array(values).reshape(-1, 1)\n",
    "                        # ).flatten()\n",
    "                        normalized_values = values\n",
    "\n",
    "                        llms.append(llm)\n",
    "                        modes.append(mode)\n",
    "                        cols.append(col)\n",
    "                        datasets.append(dataset)\n",
    "                        distribution_values.append(normalized_values)\n",
    "                        dataset_original_or_not.append(prefix)\n",
    "                        all_indices.append(indices)\n",
    "\n",
    "    return (\n",
    "        llms,\n",
    "        modes,\n",
    "        cols,\n",
    "        datasets,\n",
    "        dataset_original_or_not,\n",
    "        distribution_values,\n",
    "        all_indices,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    llms,\n",
    "    modes,\n",
    "    cols,\n",
    "    datasets,\n",
    "    dataset_original_or_not,\n",
    "    distribution_values,\n",
    "    all_indices,\n",
    ") = get_agg_features_and_entropies(all_data)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"llm\": llms,\n",
    "        \"mode\": modes,\n",
    "        \"col\": cols,\n",
    "        \"dataset\": datasets,\n",
    "        \"is_original\": dataset_original_or_not,\n",
    "        \"distribution\": distribution_values,\n",
    "        \"index\": all_indices,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"col\"].isin(complexity_features)]\n",
    "\n",
    "filtered_llms = []\n",
    "filtered_modes = []\n",
    "filtered_datasets = []\n",
    "filtered_is_originals = []\n",
    "mean_values = []\n",
    "avg_dep_values = []\n",
    "type_token_values = []\n",
    "hapax_values = []\n",
    "shannon_values = []\n",
    "simpsons_values = []\n",
    "\n",
    "\n",
    "for llm in df[\"llm\"].unique():\n",
    "    for mode in df[\"mode\"].unique():\n",
    "        for dataset in df[\"dataset\"].unique():\n",
    "            for is_original in df[\"is_original\"].unique():\n",
    "\n",
    "                sub_df = df[\n",
    "                    (df[\"llm\"] == llm)\n",
    "                    & (df[\"mode\"] == mode)\n",
    "                    & (df[\"dataset\"] == dataset)\n",
    "                    & (df[\"is_original\"] == is_original)\n",
    "                ]\n",
    "                avg_dep_subdf = pd.DataFrame(\n",
    "                    {\n",
    "                        \"value_avg_dep\": sub_df[\n",
    "                            sub_df[\"col\"] == \"avg_dependency_link_length\"\n",
    "                        ][\"distribution\"].values[0],\n",
    "                        \"index\": sub_df[sub_df[\"col\"] == \"avg_dependency_link_length\"][\n",
    "                            \"index\"\n",
    "                        ].values[0],\n",
    "                    }\n",
    "                )\n",
    "                type_token_subdf = pd.DataFrame(\n",
    "                    {\n",
    "                        \"value_type_token\": sub_df[sub_df[\"col\"] == \"type_token_ratio\"][\n",
    "                            \"distribution\"\n",
    "                        ].values[0],\n",
    "                        \"index\": sub_df[sub_df[\"col\"] == \"type_token_ratio\"][\n",
    "                            \"index\"\n",
    "                        ].values[0],\n",
    "                    }\n",
    "                )\n",
    "                hapax_subdf = pd.DataFrame(\n",
    "                    {\n",
    "                        \"value_hapax\": sub_df[sub_df[\"col\"] == \"hapax_legemena\"][\n",
    "                            \"distribution\"\n",
    "                        ].values[0],\n",
    "                        \"index\": sub_df[sub_df[\"col\"] == \"hapax_legemena\"][\n",
    "                            \"index\"\n",
    "                        ].values[0],\n",
    "                    }\n",
    "                )\n",
    "                shannon_subdf = pd.DataFrame(\n",
    "                    {\n",
    "                        \"value_shannon\": sub_df[sub_df[\"col\"] == \"shannon_entropy\"][\n",
    "                            \"distribution\"\n",
    "                        ].values[0],\n",
    "                        \"index\": sub_df[sub_df[\"col\"] == \"shannon_entropy\"][\n",
    "                            \"index\"\n",
    "                        ].values[0],\n",
    "                    }\n",
    "                )\n",
    "                simpsons_subdf = pd.DataFrame(\n",
    "                    {\n",
    "                        \"value_simpsons\": sub_df[sub_df[\"col\"] == \"simpsons_index\"][\n",
    "                            \"distribution\"\n",
    "                        ].values[0],\n",
    "                        \"index\": sub_df[sub_df[\"col\"] == \"simpsons_index\"][\n",
    "                            \"index\"\n",
    "                        ].values[0],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                merged_df = (\n",
    "                    pd.merge(\n",
    "                        avg_dep_subdf,\n",
    "                        type_token_subdf,\n",
    "                        on=\"index\",\n",
    "                    )\n",
    "                    .merge(hapax_subdf, on=\"index\")\n",
    "                    .merge(\n",
    "                        shannon_subdf,\n",
    "                        on=\"index\",\n",
    "                    )\n",
    "                    .merge(\n",
    "                        simpsons_subdf,\n",
    "                        on=\"index\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                value_avg_dep_normalized = scaler.fit_transform(\n",
    "                    merged_df[\"value_avg_dep\"].values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "                scaler = StandardScaler()\n",
    "                value_type_token_normalized = scaler.fit_transform(\n",
    "                    merged_df[\"value_type_token\"].values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "                scaler = StandardScaler()\n",
    "                value_hapax_normalized = scaler.fit_transform(\n",
    "                    merged_df[\"value_hapax\"].values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "                scaler = StandardScaler()\n",
    "                value_shannon_normalized = scaler.fit_transform(\n",
    "                    merged_df[\"value_shannon\"].values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "                scaler = StandardScaler()\n",
    "                value_simpsons_normalized = scaler.fit_transform(\n",
    "                    merged_df[\"value_simpsons\"].values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "\n",
    "                # # get the mean of all the value_* columns\n",
    "                merged_df[\"value\"] = np.mean(\n",
    "                    [\n",
    "                        merged_df[\"value_avg_dep\"],\n",
    "                        merged_df[\"value_type_token\"],\n",
    "                        merged_df[\"value_hapax\"],\n",
    "                        merged_df[\"value_shannon\"],\n",
    "                        merged_df[\"value_simpsons\"],\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "                mean_values.append(merged_df[\"value\"].values)\n",
    "                avg_dep_values.append(merged_df[\"value_avg_dep\"].values)\n",
    "                type_token_values.append(merged_df[\"value_type_token\"].values)\n",
    "                hapax_values.append(merged_df[\"value_hapax\"].values)\n",
    "                shannon_values.append(merged_df[\"value_shannon\"].values)\n",
    "                simpsons_values.append(merged_df[\"value_simpsons\"].values)\n",
    "                filtered_llms.append(llm)\n",
    "                filtered_modes.append(mode)\n",
    "                filtered_datasets.append(dataset)\n",
    "                filtered_is_originals.append(is_original)\n",
    "\n",
    "filtered_df = pd.DataFrame(\n",
    "    {\n",
    "        \"llm\": filtered_llms,\n",
    "        \"mode\": filtered_modes,\n",
    "        \"dataset\": filtered_datasets,\n",
    "        \"is_original\": filtered_is_originals,\n",
    "        \"mean\": mean_values,\n",
    "        \"avg_dep\": avg_dep_values,\n",
    "        \"type_token\": type_token_values,\n",
    "        \"hapax\": hapax_values,\n",
    "        \"shannon\": shannon_values,\n",
    "        \"simpsons\": simpsons_values,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "analysis for:  mean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">significance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">statistics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>news</th>\n",
       "      <th>papers</th>\n",
       "      <th>reddit</th>\n",
       "      <th>news</th>\n",
       "      <th>papers</th>\n",
       "      <th>reddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm</th>\n",
       "      <th>mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gemini</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0279 -&gt; 0.0289 (0.6599)</td>\n",
       "      <td>0.009 -&gt; 0.0071 (0.0247)*</td>\n",
       "      <td>0.0091 -&gt; 0.0076 (0.1457)</td>\n",
       "      <td>0.194</td>\n",
       "      <td>5.062</td>\n",
       "      <td>2.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0279 -&gt; 0.1457 (0.2455)</td>\n",
       "      <td>0.009 -&gt; 0.0074 (0.034)*</td>\n",
       "      <td>0.0091 -&gt; 0.0069 (0.0207)*</td>\n",
       "      <td>1.356</td>\n",
       "      <td>4.507</td>\n",
       "      <td>5.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gpt</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0279 -&gt; 0.0227 (0.4912)</td>\n",
       "      <td>0.009 -&gt; 0.007 (0.0251)*</td>\n",
       "      <td>0.0091 -&gt; 0.0063 (0.0149)*</td>\n",
       "      <td>0.476</td>\n",
       "      <td>5.031</td>\n",
       "      <td>5.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0279 -&gt; 0.0288 (0.8927)</td>\n",
       "      <td>0.009 -&gt; 0.0071 (0.0124)*</td>\n",
       "      <td>0.0091 -&gt; 0.0068 (0.0179)*</td>\n",
       "      <td>0.018</td>\n",
       "      <td>6.276</td>\n",
       "      <td>5.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">llama</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0279 -&gt; 0.0095 (0.0074)*</td>\n",
       "      <td>0.009 -&gt; 0.0064 (0.0006)*</td>\n",
       "      <td>0.0091 -&gt; 0.0045 (0.0)*</td>\n",
       "      <td>7.322</td>\n",
       "      <td>12.007</td>\n",
       "      <td>19.387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0279 -&gt; 0.0226 (0.6469)</td>\n",
       "      <td>0.009 -&gt; 0.0111 (0.0664)</td>\n",
       "      <td>0.0091 -&gt; 0.0069 (0.0476)*</td>\n",
       "      <td>0.211</td>\n",
       "      <td>3.377</td>\n",
       "      <td>3.952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     significance                             \\\n",
       "dataset                                      news                     papers   \n",
       "llm    mode                                                                    \n",
       "gemini rephrase         0.0279 -> 0.0289 (0.6599)  0.009 -> 0.0071 (0.0247)*   \n",
       "       syntax_grammar   0.0279 -> 0.1457 (0.2455)   0.009 -> 0.0074 (0.034)*   \n",
       "gpt    rephrase         0.0279 -> 0.0227 (0.4912)   0.009 -> 0.007 (0.0251)*   \n",
       "       syntax_grammar   0.0279 -> 0.0288 (0.8927)  0.009 -> 0.0071 (0.0124)*   \n",
       "llama  rephrase        0.0279 -> 0.0095 (0.0074)*  0.009 -> 0.0064 (0.0006)*   \n",
       "       syntax_grammar   0.0279 -> 0.0226 (0.6469)   0.009 -> 0.0111 (0.0664)   \n",
       "\n",
       "                                                  statistics                  \n",
       "dataset                                    reddit       news  papers  reddit  \n",
       "llm    mode                                                                   \n",
       "gemini rephrase         0.0091 -> 0.0076 (0.1457)      0.194   5.062   2.126  \n",
       "       syntax_grammar  0.0091 -> 0.0069 (0.0207)*      1.356   4.507   5.399  \n",
       "gpt    rephrase        0.0091 -> 0.0063 (0.0149)*      0.476   5.031   5.978  \n",
       "       syntax_grammar  0.0091 -> 0.0068 (0.0179)*      0.018   6.276   5.652  \n",
       "llama  rephrase           0.0091 -> 0.0045 (0.0)*      7.322  12.007  19.387  \n",
       "       syntax_grammar  0.0091 -> 0.0069 (0.0476)*      0.211   3.377   3.952  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllrrr}\n",
      "\\toprule\n",
      " &  & \\multicolumn{3}{r}{significance} & \\multicolumn{3}{r}{statistics} \\\\\n",
      " & dataset & news & papers & reddit & news & papers & reddit \\\\\n",
      "llm & mode &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{2}{*}{gemini} & rephrase & 0.0279 -> 0.0289 (0.6599) & 0.009 -> 0.0071 (0.0247)* & 0.0091 -> 0.0076 (0.1457) & 0.194000 & 5.062000 & 2.126000 \\\\\n",
      " & syntax_grammar & 0.0279 -> 0.1457 (0.2455) & 0.009 -> 0.0074 (0.034)* & 0.0091 -> 0.0069 (0.0207)* & 1.356000 & 4.507000 & 5.399000 \\\\\n",
      "\\cline{1-8}\n",
      "\\multirow[t]{2}{*}{gpt} & rephrase & 0.0279 -> 0.0227 (0.4912) & 0.009 -> 0.007 (0.0251)* & 0.0091 -> 0.0063 (0.0149)* & 0.476000 & 5.031000 & 5.978000 \\\\\n",
      " & syntax_grammar & 0.0279 -> 0.0288 (0.8927) & 0.009 -> 0.0071 (0.0124)* & 0.0091 -> 0.0068 (0.0179)* & 0.018000 & 6.276000 & 5.652000 \\\\\n",
      "\\cline{1-8}\n",
      "\\multirow[t]{2}{*}{llama} & rephrase & 0.0279 -> 0.0095 (0.0074)* & 0.009 -> 0.0064 (0.0006)* & 0.0091 -> 0.0045 (0.0)* & 7.322000 & 12.007000 & 19.387000 \\\\\n",
      " & syntax_grammar & 0.0279 -> 0.0226 (0.6469) & 0.009 -> 0.0111 (0.0664) & 0.0091 -> 0.0069 (0.0476)* & 0.211000 & 3.377000 & 3.952000 \\\\\n",
      "\\cline{1-8}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "---------------------------------\n",
      "analysis for:  avg_dep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">significance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">statistics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>news</th>\n",
       "      <th>papers</th>\n",
       "      <th>reddit</th>\n",
       "      <th>news</th>\n",
       "      <th>papers</th>\n",
       "      <th>reddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm</th>\n",
       "      <th>mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gemini</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.1245 -&gt; 0.2501 (0.0538)</td>\n",
       "      <td>0.0984 -&gt; 0.1044 (0.3547)</td>\n",
       "      <td>0.0792 -&gt; 0.1083 (0.5782)</td>\n",
       "      <td>3.764</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.1245 -&gt; 3.1575 (0.1851)</td>\n",
       "      <td>0.0984 -&gt; 0.0905 (0.3449)</td>\n",
       "      <td>0.0792 -&gt; 0.0475 (0.0005)*</td>\n",
       "      <td>1.768</td>\n",
       "      <td>0.893</td>\n",
       "      <td>12.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gpt</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.1245 -&gt; 0.255 (0.514)</td>\n",
       "      <td>0.0984 -&gt; 0.0829 (0.1703)</td>\n",
       "      <td>0.0792 -&gt; 0.0596 (0.0196)*</td>\n",
       "      <td>0.428</td>\n",
       "      <td>1.883</td>\n",
       "      <td>5.498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.1245 -&gt; 0.2245 (0.4283)</td>\n",
       "      <td>0.0984 -&gt; 0.0784 (0.0105)*</td>\n",
       "      <td>0.0792 -&gt; 0.0569 (0.0262)*</td>\n",
       "      <td>0.630</td>\n",
       "      <td>6.578</td>\n",
       "      <td>4.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">llama</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.1245 -&gt; 0.1087 (0.7688)</td>\n",
       "      <td>0.0984 -&gt; 0.1277 (0.0139)**</td>\n",
       "      <td>0.0796 -&gt; 0.0519 (0.0036)*</td>\n",
       "      <td>0.087</td>\n",
       "      <td>6.073</td>\n",
       "      <td>8.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.1245 -&gt; 0.1332 (0.7941)</td>\n",
       "      <td>0.0984 -&gt; 0.0838 (0.1566)</td>\n",
       "      <td>0.0792 -&gt; 0.0647 (0.0822)</td>\n",
       "      <td>0.068</td>\n",
       "      <td>2.009</td>\n",
       "      <td>3.037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    significance                               \\\n",
       "dataset                                     news                       papers   \n",
       "llm    mode                                                                     \n",
       "gemini rephrase        0.1245 -> 0.2501 (0.0538)    0.0984 -> 0.1044 (0.3547)   \n",
       "       syntax_grammar  0.1245 -> 3.1575 (0.1851)    0.0984 -> 0.0905 (0.3449)   \n",
       "gpt    rephrase          0.1245 -> 0.255 (0.514)    0.0984 -> 0.0829 (0.1703)   \n",
       "       syntax_grammar  0.1245 -> 0.2245 (0.4283)   0.0984 -> 0.0784 (0.0105)*   \n",
       "llama  rephrase        0.1245 -> 0.1087 (0.7688)  0.0984 -> 0.1277 (0.0139)**   \n",
       "       syntax_grammar  0.1245 -> 0.1332 (0.7941)    0.0984 -> 0.0838 (0.1566)   \n",
       "\n",
       "                                                  statistics                 \n",
       "dataset                                    reddit       news papers  reddit  \n",
       "llm    mode                                                                  \n",
       "gemini rephrase         0.0792 -> 0.1083 (0.5782)      3.764  0.857   0.310  \n",
       "       syntax_grammar  0.0792 -> 0.0475 (0.0005)*      1.768  0.893  12.365  \n",
       "gpt    rephrase        0.0792 -> 0.0596 (0.0196)*      0.428  1.883   5.498  \n",
       "       syntax_grammar  0.0792 -> 0.0569 (0.0262)*      0.630  6.578   4.980  \n",
       "llama  rephrase        0.0796 -> 0.0519 (0.0036)*      0.087  6.073   8.564  \n",
       "       syntax_grammar   0.0792 -> 0.0647 (0.0822)      0.068  2.009   3.037  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllrrr}\n",
      "\\toprule\n",
      " &  & \\multicolumn{3}{r}{significance} & \\multicolumn{3}{r}{statistics} \\\\\n",
      " & dataset & news & papers & reddit & news & papers & reddit \\\\\n",
      "llm & mode &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{2}{*}{gemini} & rephrase & 0.1245 -> 0.2501 (0.0538) & 0.0984 -> 0.1044 (0.3547) & 0.0792 -> 0.1083 (0.5782) & 3.764000 & 0.857000 & 0.310000 \\\\\n",
      " & syntax_grammar & 0.1245 -> 3.1575 (0.1851) & 0.0984 -> 0.0905 (0.3449) & 0.0792 -> 0.0475 (0.0005)* & 1.768000 & 0.893000 & 12.365000 \\\\\n",
      "\\cline{1-8}\n",
      "\\multirow[t]{2}{*}{gpt} & rephrase & 0.1245 -> 0.255 (0.514) & 0.0984 -> 0.0829 (0.1703) & 0.0792 -> 0.0596 (0.0196)* & 0.428000 & 1.883000 & 5.498000 \\\\\n",
      " & syntax_grammar & 0.1245 -> 0.2245 (0.4283) & 0.0984 -> 0.0784 (0.0105)* & 0.0792 -> 0.0569 (0.0262)* & 0.630000 & 6.578000 & 4.980000 \\\\\n",
      "\\cline{1-8}\n",
      "\\multirow[t]{2}{*}{llama} & rephrase & 0.1245 -> 0.1087 (0.7688) & 0.0984 -> 0.1277 (0.0139)** & 0.0796 -> 0.0519 (0.0036)* & 0.087000 & 6.073000 & 8.564000 \\\\\n",
      " & syntax_grammar & 0.1245 -> 0.1332 (0.7941) & 0.0984 -> 0.0838 (0.1566) & 0.0792 -> 0.0647 (0.0822) & 0.068000 & 2.009000 & 3.037000 \\\\\n",
      "\\cline{1-8}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "---------------------------------\n",
      "analysis for:  type_token\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">significance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">statistics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>news</th>\n",
       "      <th>papers</th>\n",
       "      <th>reddit</th>\n",
       "      <th>news</th>\n",
       "      <th>papers</th>\n",
       "      <th>reddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm</th>\n",
       "      <th>mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gemini</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0168 -&gt; 0.0096 (0.0091)*</td>\n",
       "      <td>0.0034 -&gt; 0.0025 (0.0027)*</td>\n",
       "      <td>0.0053 -&gt; 0.003 (0.0)*</td>\n",
       "      <td>6.935</td>\n",
       "      <td>9.060</td>\n",
       "      <td>20.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0168 -&gt; 0.0116 (0.0538)</td>\n",
       "      <td>0.0034 -&gt; 0.0028 (0.1086)</td>\n",
       "      <td>0.0053 -&gt; 0.0047 (0.1974)</td>\n",
       "      <td>3.764</td>\n",
       "      <td>2.579</td>\n",
       "      <td>1.668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gpt</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0168 -&gt; 0.008 (0.0015)*</td>\n",
       "      <td>0.0034 -&gt; 0.003 (0.2355)</td>\n",
       "      <td>0.0053 -&gt; 0.0042 (0.006)*</td>\n",
       "      <td>10.311</td>\n",
       "      <td>1.409</td>\n",
       "      <td>7.637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0168 -&gt; 0.0107 (0.0214)*</td>\n",
       "      <td>0.0034 -&gt; 0.0027 (0.019)*</td>\n",
       "      <td>0.0053 -&gt; 0.0036 (0.0001)*</td>\n",
       "      <td>5.375</td>\n",
       "      <td>5.521</td>\n",
       "      <td>14.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">llama</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0168 -&gt; 0.0083 (0.0013)*</td>\n",
       "      <td>0.0034 -&gt; 0.0027 (0.088)</td>\n",
       "      <td>0.0053 -&gt; 0.0032 (0.0001)*</td>\n",
       "      <td>10.604</td>\n",
       "      <td>2.917</td>\n",
       "      <td>14.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0168 -&gt; 0.0089 (0.0106)*</td>\n",
       "      <td>0.0034 -&gt; 0.0096 (0.0)**</td>\n",
       "      <td>0.0053 -&gt; 0.005 (0.524)</td>\n",
       "      <td>6.651</td>\n",
       "      <td>109.819</td>\n",
       "      <td>0.407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     significance                              \\\n",
       "dataset                                      news                      papers   \n",
       "llm    mode                                                                     \n",
       "gemini rephrase        0.0168 -> 0.0096 (0.0091)*  0.0034 -> 0.0025 (0.0027)*   \n",
       "       syntax_grammar   0.0168 -> 0.0116 (0.0538)   0.0034 -> 0.0028 (0.1086)   \n",
       "gpt    rephrase         0.0168 -> 0.008 (0.0015)*    0.0034 -> 0.003 (0.2355)   \n",
       "       syntax_grammar  0.0168 -> 0.0107 (0.0214)*   0.0034 -> 0.0027 (0.019)*   \n",
       "llama  rephrase        0.0168 -> 0.0083 (0.0013)*    0.0034 -> 0.0027 (0.088)   \n",
       "       syntax_grammar  0.0168 -> 0.0089 (0.0106)*    0.0034 -> 0.0096 (0.0)**   \n",
       "\n",
       "                                                  statistics                   \n",
       "dataset                                    reddit       news   papers  reddit  \n",
       "llm    mode                                                                    \n",
       "gemini rephrase            0.0053 -> 0.003 (0.0)*      6.935    9.060  20.162  \n",
       "       syntax_grammar   0.0053 -> 0.0047 (0.1974)      3.764    2.579   1.668  \n",
       "gpt    rephrase         0.0053 -> 0.0042 (0.006)*     10.311    1.409   7.637  \n",
       "       syntax_grammar  0.0053 -> 0.0036 (0.0001)*      5.375    5.521  14.680  \n",
       "llama  rephrase        0.0053 -> 0.0032 (0.0001)*     10.604    2.917  14.844  \n",
       "       syntax_grammar     0.0053 -> 0.005 (0.524)      6.651  109.819   0.407  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllrrr}\n",
      "\\toprule\n",
      " &  & \\multicolumn{3}{r}{significance} & \\multicolumn{3}{r}{statistics} \\\\\n",
      " & dataset & news & papers & reddit & news & papers & reddit \\\\\n",
      "llm & mode &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{2}{*}{gemini} & rephrase & 0.0168 -> 0.0096 (0.0091)* & 0.0034 -> 0.0025 (0.0027)* & 0.0053 -> 0.003 (0.0)* & 6.935000 & 9.060000 & 20.162000 \\\\\n",
      " & syntax_grammar & 0.0168 -> 0.0116 (0.0538) & 0.0034 -> 0.0028 (0.1086) & 0.0053 -> 0.0047 (0.1974) & 3.764000 & 2.579000 & 1.668000 \\\\\n",
      "\\cline{1-8}\n",
      "\\multirow[t]{2}{*}{gpt} & rephrase & 0.0168 -> 0.008 (0.0015)* & 0.0034 -> 0.003 (0.2355) & 0.0053 -> 0.0042 (0.006)* & 10.311000 & 1.409000 & 7.637000 \\\\\n",
      " & syntax_grammar & 0.0168 -> 0.0107 (0.0214)* & 0.0034 -> 0.0027 (0.019)* & 0.0053 -> 0.0036 (0.0001)* & 5.375000 & 5.521000 & 14.680000 \\\\\n",
      "\\cline{1-8}\n",
      "\\multirow[t]{2}{*}{llama} & rephrase & 0.0168 -> 0.0083 (0.0013)* & 0.0034 -> 0.0027 (0.088) & 0.0053 -> 0.0032 (0.0001)* & 10.604000 & 2.917000 & 14.844000 \\\\\n",
      " & syntax_grammar & 0.0168 -> 0.0089 (0.0106)* & 0.0034 -> 0.0096 (0.0)** & 0.0053 -> 0.005 (0.524) & 6.651000 & 109.819000 & 0.407000 \\\\\n",
      "\\cline{1-8}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "---------------------------------\n",
      "analysis for:  hapax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">significance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">statistics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>news</th>\n",
       "      <th>papers</th>\n",
       "      <th>reddit</th>\n",
       "      <th>news</th>\n",
       "      <th>papers</th>\n",
       "      <th>reddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm</th>\n",
       "      <th>mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gemini</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0219 -&gt; 0.0107 (0.0068)*</td>\n",
       "      <td>0.0055 -&gt; 0.0057 (0.6838)</td>\n",
       "      <td>0.0058 -&gt; 0.0044 (0.0333)*</td>\n",
       "      <td>7.486</td>\n",
       "      <td>0.166</td>\n",
       "      <td>4.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0219 -&gt; 0.0124 (0.0335)*</td>\n",
       "      <td>0.0055 -&gt; 0.0054 (0.6571)</td>\n",
       "      <td>0.0058 -&gt; 0.0064 (0.6129)</td>\n",
       "      <td>4.584</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gpt</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0219 -&gt; 0.0116 (0.0113)*</td>\n",
       "      <td>0.0055 -&gt; 0.0057 (0.8879)</td>\n",
       "      <td>0.0058 -&gt; 0.0054 (0.1782)</td>\n",
       "      <td>6.534</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0219 -&gt; 0.0146 (0.0473)*</td>\n",
       "      <td>0.0055 -&gt; 0.0044 (0.0169)*</td>\n",
       "      <td>0.0058 -&gt; 0.0049 (0.0357)*</td>\n",
       "      <td>3.982</td>\n",
       "      <td>5.730</td>\n",
       "      <td>4.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">llama</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0219 -&gt; 0.0122 (0.0169)*</td>\n",
       "      <td>0.0055 -&gt; 0.0059 (0.419)</td>\n",
       "      <td>0.0058 -&gt; 0.0048 (0.1692)</td>\n",
       "      <td>5.800</td>\n",
       "      <td>0.654</td>\n",
       "      <td>1.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0219 -&gt; 0.0149 (0.1279)</td>\n",
       "      <td>0.0055 -&gt; 0.0155 (0.0)**</td>\n",
       "      <td>0.0058 -&gt; 0.0061 (0.7886)</td>\n",
       "      <td>2.337</td>\n",
       "      <td>110.674</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     significance                              \\\n",
       "dataset                                      news                      papers   \n",
       "llm    mode                                                                     \n",
       "gemini rephrase        0.0219 -> 0.0107 (0.0068)*   0.0055 -> 0.0057 (0.6838)   \n",
       "       syntax_grammar  0.0219 -> 0.0124 (0.0335)*   0.0055 -> 0.0054 (0.6571)   \n",
       "gpt    rephrase        0.0219 -> 0.0116 (0.0113)*   0.0055 -> 0.0057 (0.8879)   \n",
       "       syntax_grammar  0.0219 -> 0.0146 (0.0473)*  0.0055 -> 0.0044 (0.0169)*   \n",
       "llama  rephrase        0.0219 -> 0.0122 (0.0169)*    0.0055 -> 0.0059 (0.419)   \n",
       "       syntax_grammar   0.0219 -> 0.0149 (0.1279)    0.0055 -> 0.0155 (0.0)**   \n",
       "\n",
       "                                                  statistics                  \n",
       "dataset                                    reddit       news   papers reddit  \n",
       "llm    mode                                                                   \n",
       "gemini rephrase        0.0058 -> 0.0044 (0.0333)*      7.486    0.166  4.564  \n",
       "       syntax_grammar   0.0058 -> 0.0064 (0.6129)      4.584    0.197  0.256  \n",
       "gpt    rephrase         0.0058 -> 0.0054 (0.1782)      6.534    0.020  1.819  \n",
       "       syntax_grammar  0.0058 -> 0.0049 (0.0357)*      3.982    5.730  4.443  \n",
       "llama  rephrase         0.0058 -> 0.0048 (0.1692)      5.800    0.654  1.898  \n",
       "       syntax_grammar   0.0058 -> 0.0061 (0.7886)      2.337  110.674  0.072  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllrrr}\n",
      "\\toprule\n",
      " &  & \\multicolumn{3}{r}{significance} & \\multicolumn{3}{r}{statistics} \\\\\n",
      " & dataset & news & papers & reddit & news & papers & reddit \\\\\n",
      "llm & mode &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{2}{*}{gemini} & rephrase & 0.0219 -> 0.0107 (0.0068)* & 0.0055 -> 0.0057 (0.6838) & 0.0058 -> 0.0044 (0.0333)* & 7.486000 & 0.166000 & 4.564000 \\\\\n",
      " & syntax_grammar & 0.0219 -> 0.0124 (0.0335)* & 0.0055 -> 0.0054 (0.6571) & 0.0058 -> 0.0064 (0.6129) & 4.584000 & 0.197000 & 0.256000 \\\\\n",
      "\\cline{1-8}\n",
      "\\multirow[t]{2}{*}{gpt} & rephrase & 0.0219 -> 0.0116 (0.0113)* & 0.0055 -> 0.0057 (0.8879) & 0.0058 -> 0.0054 (0.1782) & 6.534000 & 0.020000 & 1.819000 \\\\\n",
      " & syntax_grammar & 0.0219 -> 0.0146 (0.0473)* & 0.0055 -> 0.0044 (0.0169)* & 0.0058 -> 0.0049 (0.0357)* & 3.982000 & 5.730000 & 4.443000 \\\\\n",
      "\\cline{1-8}\n",
      "\\multirow[t]{2}{*}{llama} & rephrase & 0.0219 -> 0.0122 (0.0169)* & 0.0055 -> 0.0059 (0.419) & 0.0058 -> 0.0048 (0.1692) & 5.800000 & 0.654000 & 1.898000 \\\\\n",
      " & syntax_grammar & 0.0219 -> 0.0149 (0.1279) & 0.0055 -> 0.0155 (0.0)** & 0.0058 -> 0.0061 (0.7886) & 2.337000 & 110.674000 & 0.072000 \\\\\n",
      "\\cline{1-8}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "---------------------------------\n",
      "analysis for:  shannon\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">significance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">statistics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>news</th>\n",
       "      <th>papers</th>\n",
       "      <th>reddit</th>\n",
       "      <th>news</th>\n",
       "      <th>papers</th>\n",
       "      <th>reddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm</th>\n",
       "      <th>mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gemini</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.7819 -&gt; 0.4592 (0.0854)</td>\n",
       "      <td>0.0907 -&gt; 0.0784 (0.1474)</td>\n",
       "      <td>0.177 -&gt; 0.1612 (0.3516)</td>\n",
       "      <td>2.989</td>\n",
       "      <td>2.102</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.7819 -&gt; 0.6344 (0.4731)</td>\n",
       "      <td>0.0907 -&gt; 0.0945 (0.6207)</td>\n",
       "      <td>0.177 -&gt; 0.1698 (0.9084)</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gpt</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.7819 -&gt; 0.3637 (0.0071)*</td>\n",
       "      <td>0.0907 -&gt; 0.0824 (0.4162)</td>\n",
       "      <td>0.177 -&gt; 0.1965 (0.8433)</td>\n",
       "      <td>7.406</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.7819 -&gt; 0.5653 (0.2067)</td>\n",
       "      <td>0.0907 -&gt; 0.0895 (0.8159)</td>\n",
       "      <td>0.177 -&gt; 0.1251 (0.0029)*</td>\n",
       "      <td>1.605</td>\n",
       "      <td>0.054</td>\n",
       "      <td>9.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">llama</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.7819 -&gt; 0.2998 (0.0009)*</td>\n",
       "      <td>0.0907 -&gt; 0.078 (0.2634)</td>\n",
       "      <td>0.1755 -&gt; 0.0937 (0.0)*</td>\n",
       "      <td>11.274</td>\n",
       "      <td>1.252</td>\n",
       "      <td>22.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.7819 -&gt; 0.5083 (0.1573)</td>\n",
       "      <td>0.0907 -&gt; 0.0754 (0.1146)</td>\n",
       "      <td>0.177 -&gt; 0.1486 (0.1654)</td>\n",
       "      <td>2.015</td>\n",
       "      <td>2.494</td>\n",
       "      <td>1.931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     significance                             \\\n",
       "dataset                                      news                     papers   \n",
       "llm    mode                                                                    \n",
       "gemini rephrase         0.7819 -> 0.4592 (0.0854)  0.0907 -> 0.0784 (0.1474)   \n",
       "       syntax_grammar   0.7819 -> 0.6344 (0.4731)  0.0907 -> 0.0945 (0.6207)   \n",
       "gpt    rephrase        0.7819 -> 0.3637 (0.0071)*  0.0907 -> 0.0824 (0.4162)   \n",
       "       syntax_grammar   0.7819 -> 0.5653 (0.2067)  0.0907 -> 0.0895 (0.8159)   \n",
       "llama  rephrase        0.7819 -> 0.2998 (0.0009)*   0.0907 -> 0.078 (0.2634)   \n",
       "       syntax_grammar   0.7819 -> 0.5083 (0.1573)  0.0907 -> 0.0754 (0.1146)   \n",
       "\n",
       "                                                 statistics                 \n",
       "dataset                                   reddit       news papers  reddit  \n",
       "llm    mode                                                                 \n",
       "gemini rephrase         0.177 -> 0.1612 (0.3516)      2.989  2.102   0.870  \n",
       "       syntax_grammar   0.177 -> 0.1698 (0.9084)      0.517  0.245   0.013  \n",
       "gpt    rephrase         0.177 -> 0.1965 (0.8433)      7.406  0.662   0.039  \n",
       "       syntax_grammar  0.177 -> 0.1251 (0.0029)*      1.605  0.054   9.007  \n",
       "llama  rephrase          0.1755 -> 0.0937 (0.0)*     11.274  1.252  22.530  \n",
       "       syntax_grammar   0.177 -> 0.1486 (0.1654)      2.015  2.494   1.931  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllrrr}\n",
      "\\toprule\n",
      " &  & \\multicolumn{3}{r}{significance} & \\multicolumn{3}{r}{statistics} \\\\\n",
      " & dataset & news & papers & reddit & news & papers & reddit \\\\\n",
      "llm & mode &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{2}{*}{gemini} & rephrase & 0.7819 -> 0.4592 (0.0854) & 0.0907 -> 0.0784 (0.1474) & 0.177 -> 0.1612 (0.3516) & 2.989000 & 2.102000 & 0.870000 \\\\\n",
      " & syntax_grammar & 0.7819 -> 0.6344 (0.4731) & 0.0907 -> 0.0945 (0.6207) & 0.177 -> 0.1698 (0.9084) & 0.517000 & 0.245000 & 0.013000 \\\\\n",
      "\\cline{1-8}\n",
      "\\multirow[t]{2}{*}{gpt} & rephrase & 0.7819 -> 0.3637 (0.0071)* & 0.0907 -> 0.0824 (0.4162) & 0.177 -> 0.1965 (0.8433) & 7.406000 & 0.662000 & 0.039000 \\\\\n",
      " & syntax_grammar & 0.7819 -> 0.5653 (0.2067) & 0.0907 -> 0.0895 (0.8159) & 0.177 -> 0.1251 (0.0029)* & 1.605000 & 0.054000 & 9.007000 \\\\\n",
      "\\cline{1-8}\n",
      "\\multirow[t]{2}{*}{llama} & rephrase & 0.7819 -> 0.2998 (0.0009)* & 0.0907 -> 0.078 (0.2634) & 0.1755 -> 0.0937 (0.0)* & 11.274000 & 1.252000 & 22.530000 \\\\\n",
      " & syntax_grammar & 0.7819 -> 0.5083 (0.1573) & 0.0907 -> 0.0754 (0.1146) & 0.177 -> 0.1486 (0.1654) & 2.015000 & 2.494000 & 1.931000 \\\\\n",
      "\\cline{1-8}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "---------------------------------\n",
      "analysis for:  simpsons\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">significance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">statistics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>news</th>\n",
       "      <th>papers</th>\n",
       "      <th>reddit</th>\n",
       "      <th>news</th>\n",
       "      <th>papers</th>\n",
       "      <th>reddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm</th>\n",
       "      <th>mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gemini</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0 -&gt; 0.0 (0.8893)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0488)*</td>\n",
       "      <td>0.019</td>\n",
       "      <td>40.053</td>\n",
       "      <td>3.908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0 -&gt; 0.0 (0.5354)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.1106)</td>\n",
       "      <td>0.385</td>\n",
       "      <td>56.825</td>\n",
       "      <td>2.557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gpt</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0 -&gt; 0.0 (0.3797)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0002)*</td>\n",
       "      <td>0.775</td>\n",
       "      <td>29.359</td>\n",
       "      <td>14.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0 -&gt; 0.0 (0.8532)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0003)*</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0385)*</td>\n",
       "      <td>0.034</td>\n",
       "      <td>13.437</td>\n",
       "      <td>4.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">llama</th>\n",
       "      <th>rephrase</th>\n",
       "      <td>0.0 -&gt; 0.0 (0.9724)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0008)*</td>\n",
       "      <td>0.001</td>\n",
       "      <td>23.603</td>\n",
       "      <td>11.423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax_grammar</th>\n",
       "      <td>0.0 -&gt; 0.0 (0.5851)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.6706)</td>\n",
       "      <td>0.299</td>\n",
       "      <td>35.498</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              significance                        \\\n",
       "dataset                               news                papers   \n",
       "llm    mode                                                        \n",
       "gemini rephrase        0.0 -> 0.0 (0.8893)     0.0 -> 0.0 (0.0)*   \n",
       "       syntax_grammar  0.0 -> 0.0 (0.5354)     0.0 -> 0.0 (0.0)*   \n",
       "gpt    rephrase        0.0 -> 0.0 (0.3797)     0.0 -> 0.0 (0.0)*   \n",
       "       syntax_grammar  0.0 -> 0.0 (0.8532)  0.0 -> 0.0 (0.0003)*   \n",
       "llama  rephrase        0.0 -> 0.0 (0.9724)     0.0 -> 0.0 (0.0)*   \n",
       "       syntax_grammar  0.0 -> 0.0 (0.5851)     0.0 -> 0.0 (0.0)*   \n",
       "\n",
       "                                            statistics                  \n",
       "dataset                              reddit       news  papers  reddit  \n",
       "llm    mode                                                             \n",
       "gemini rephrase        0.0 -> 0.0 (0.0488)*      0.019  40.053   3.908  \n",
       "       syntax_grammar   0.0 -> 0.0 (0.1106)      0.385  56.825   2.557  \n",
       "gpt    rephrase        0.0 -> 0.0 (0.0002)*      0.775  29.359  14.603  \n",
       "       syntax_grammar  0.0 -> 0.0 (0.0385)*      0.034  13.437   4.314  \n",
       "llama  rephrase        0.0 -> 0.0 (0.0008)*      0.001  23.603  11.423  \n",
       "       syntax_grammar   0.0 -> 0.0 (0.6706)      0.299  35.498   0.181  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllrrr}\n",
      "\\toprule\n",
      " &  & \\multicolumn{3}{r}{significance} & \\multicolumn{3}{r}{statistics} \\\\\n",
      " & dataset & news & papers & reddit & news & papers & reddit \\\\\n",
      "llm & mode &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{2}{*}{gemini} & rephrase & 0.0 -> 0.0 (0.8893) & 0.0 -> 0.0 (0.0)* & 0.0 -> 0.0 (0.0488)* & 0.019000 & 40.053000 & 3.908000 \\\\\n",
      " & syntax_grammar & 0.0 -> 0.0 (0.5354) & 0.0 -> 0.0 (0.0)* & 0.0 -> 0.0 (0.1106) & 0.385000 & 56.825000 & 2.557000 \\\\\n",
      "\\cline{1-8}\n",
      "\\multirow[t]{2}{*}{gpt} & rephrase & 0.0 -> 0.0 (0.3797) & 0.0 -> 0.0 (0.0)* & 0.0 -> 0.0 (0.0002)* & 0.775000 & 29.359000 & 14.603000 \\\\\n",
      " & syntax_grammar & 0.0 -> 0.0 (0.8532) & 0.0 -> 0.0 (0.0003)* & 0.0 -> 0.0 (0.0385)* & 0.034000 & 13.437000 & 4.314000 \\\\\n",
      "\\cline{1-8}\n",
      "\\multirow[t]{2}{*}{llama} & rephrase & 0.0 -> 0.0 (0.9724) & 0.0 -> 0.0 (0.0)* & 0.0 -> 0.0 (0.0008)* & 0.001000 & 23.603000 & 11.423000 \\\\\n",
      " & syntax_grammar & 0.0 -> 0.0 (0.5851) & 0.0 -> 0.0 (0.0)* & 0.0 -> 0.0 (0.6706) & 0.299000 & 35.498000 & 0.181000 \\\\\n",
      "\\cline{1-8}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "\n",
    "for col in [\"mean\", \"avg_dep\", \"type_token\", \"hapax\", \"shannon\", \"simpsons\"]:\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"analysis for: \", col)\n",
    "\n",
    "    final_datasets = []\n",
    "    final_llms = []\n",
    "    final_modes = []\n",
    "    final_significances = []\n",
    "    final_statistics = []\n",
    "\n",
    "    for dataset in filtered_df[\"dataset\"].unique():\n",
    "        for llm in filtered_df[\"llm\"].unique():\n",
    "            for mode in filtered_df[\"mode\"].unique():\n",
    "                final_datasets.append(dataset)\n",
    "                final_llms.append(llm)\n",
    "                final_modes.append(mode)\n",
    "                original_values = filtered_df[\n",
    "                    (filtered_df[\"dataset\"] == dataset)\n",
    "                    & (filtered_df[\"llm\"] == llm)\n",
    "                    & (filtered_df[\"mode\"] == mode)\n",
    "                    & (filtered_df[\"is_original\"] == \"original_text\")\n",
    "                ][col].values[0]\n",
    "                rewritten_values = filtered_df[\n",
    "                    (filtered_df[\"dataset\"] == dataset)\n",
    "                    & (filtered_df[\"llm\"] == llm)\n",
    "                    & (filtered_df[\"mode\"] == mode)\n",
    "                    & (filtered_df[\"is_original\"] == \"rewritten_text\")\n",
    "                ][col].values[0]\n",
    "                stats, p = levene(original_values, rewritten_values)\n",
    "                p = np.round(p, 4)\n",
    "                stats = np.round(stats, 3)\n",
    "                final_statistics.append(stats)\n",
    "                original_vars = np.round(np.var(original_values), 4)\n",
    "                rewritten_vars = np.round(np.var(rewritten_values), 4)\n",
    "                if p <= 0.05:\n",
    "                    if original_vars >= rewritten_vars:\n",
    "                        final_significances.append(\n",
    "                            f\"{original_vars} -> {rewritten_vars} ({p})*\"\n",
    "                        )\n",
    "                    elif original_vars < rewritten_vars:\n",
    "                        final_significances.append(\n",
    "                            f\"{original_vars} -> {rewritten_vars} ({p})**\"\n",
    "                        )\n",
    "                else:\n",
    "                    final_significances.append(\n",
    "                        f\"{original_vars} -> {rewritten_vars} ({p})\"\n",
    "                    )\n",
    "\n",
    "    final_df = pd.DataFrame(\n",
    "        {\n",
    "            \"dataset\": final_datasets,\n",
    "            \"llm\": final_llms,\n",
    "            \"mode\": final_modes,\n",
    "            \"significance\": final_significances,\n",
    "            \"statistics\": final_statistics,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    display(\n",
    "        final_df.pivot_table(\n",
    "            index=[\"llm\", \"mode\"],\n",
    "            columns=\"dataset\",\n",
    "            values=[\"significance\", \"statistics\"],\n",
    "            aggfunc=lambda x: x,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        final_df.pivot_table(\n",
    "            index=[\"llm\", \"mode\"],\n",
    "            columns=\"dataset\",\n",
    "            values=[\"significance\", \"statistics\"],\n",
    "            aggfunc=lambda x: x,\n",
    "        ).to_latex()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_shortened_names = {\n",
    "    \"simpsons\": \"S\",\n",
    "    \"shannon\": \"N\",\n",
    "    \"hapax\": \"H\",\n",
    "    \"type_token\": \"T\",\n",
    "    \"avg_dep\": \"A\",\n",
    "    \"mean\": \"G\",\n",
    "}\n",
    "\n",
    "llms_to_correct_names = {\n",
    "    \"gemini\": \"Gemini\",\n",
    "    \"gpt\": \"GPT-3\",\n",
    "    \"llama\": \"Llama 3\",\n",
    "}\n",
    "\n",
    "datasets_to_correct_names = {\n",
    "    \"news\": \"News\",\n",
    "    \"papers\": \"Papers\",\n",
    "    \"reddit\": \"Reddit\",\n",
    "}\n",
    "\n",
    "modes_to_correct_names = {\n",
    "    \"rephrase\": \"R\",\n",
    "    \"syntax_grammar\": \"SG\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th colspan=\"6\" halign=\"left\">News</th>\n",
       "      <th colspan=\"6\" halign=\"left\">Papers</th>\n",
       "      <th colspan=\"6\" halign=\"left\">Reddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "      <th>A</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>N</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>A</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>N</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>A</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>N</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm</th>\n",
       "      <th>mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">GPT-3</th>\n",
       "      <th>R</th>\n",
       "      <td>0.1245 -&gt; 0.255 (0.51)</td>\n",
       "      <td>0.0279 -&gt; 0.0227 (0.49)</td>\n",
       "      <td>0.0219 -&gt; 0.0116 (0.01)*</td>\n",
       "      <td>0.7819 -&gt; 0.3637 (0.01)*</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.38)</td>\n",
       "      <td>0.0168 -&gt; 0.008 (0.0)*</td>\n",
       "      <td>0.0984 -&gt; 0.0829 (0.17)</td>\n",
       "      <td>0.009 -&gt; 0.007 (0.03)*</td>\n",
       "      <td>0.0055 -&gt; 0.0057 (0.89)</td>\n",
       "      <td>0.0907 -&gt; 0.0824 (0.42)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0034 -&gt; 0.003 (0.24)</td>\n",
       "      <td>0.0792 -&gt; 0.0596 (0.02)*</td>\n",
       "      <td>0.0091 -&gt; 0.0063 (0.01)*</td>\n",
       "      <td>0.0058 -&gt; 0.0054 (0.18)</td>\n",
       "      <td>0.177 -&gt; 0.1965 (0.84)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0053 -&gt; 0.0042 (0.01)*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SG</th>\n",
       "      <td>0.1245 -&gt; 0.2245 (0.43)</td>\n",
       "      <td>0.0279 -&gt; 0.0288 (0.89)</td>\n",
       "      <td>0.0219 -&gt; 0.0146 (0.05)*</td>\n",
       "      <td>0.7819 -&gt; 0.5653 (0.21)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.85)</td>\n",
       "      <td>0.0168 -&gt; 0.0107 (0.02)*</td>\n",
       "      <td>0.0984 -&gt; 0.0784 (0.01)*</td>\n",
       "      <td>0.009 -&gt; 0.0071 (0.01)*</td>\n",
       "      <td>0.0055 -&gt; 0.0044 (0.02)*</td>\n",
       "      <td>0.0907 -&gt; 0.0895 (0.82)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0034 -&gt; 0.0027 (0.02)*</td>\n",
       "      <td>0.0792 -&gt; 0.0569 (0.03)*</td>\n",
       "      <td>0.0091 -&gt; 0.0068 (0.02)*</td>\n",
       "      <td>0.0058 -&gt; 0.0049 (0.04)*</td>\n",
       "      <td>0.177 -&gt; 0.1251 (0.0)*</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.04)*</td>\n",
       "      <td>0.0053 -&gt; 0.0036 (0.0)*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Gemini</th>\n",
       "      <th>R</th>\n",
       "      <td>0.1245 -&gt; 0.2501 (0.05)**</td>\n",
       "      <td>0.0279 -&gt; 0.0289 (0.66)</td>\n",
       "      <td>0.0219 -&gt; 0.0107 (0.01)*</td>\n",
       "      <td>0.7819 -&gt; 0.4592 (0.09)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.89)</td>\n",
       "      <td>0.0168 -&gt; 0.0096 (0.01)*</td>\n",
       "      <td>0.0984 -&gt; 0.1044 (0.35)</td>\n",
       "      <td>0.009 -&gt; 0.0071 (0.02)*</td>\n",
       "      <td>0.0055 -&gt; 0.0057 (0.68)</td>\n",
       "      <td>0.0907 -&gt; 0.0784 (0.15)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0034 -&gt; 0.0025 (0.0)*</td>\n",
       "      <td>0.0792 -&gt; 0.1083 (0.58)</td>\n",
       "      <td>0.0091 -&gt; 0.0076 (0.15)</td>\n",
       "      <td>0.0058 -&gt; 0.0044 (0.03)*</td>\n",
       "      <td>0.177 -&gt; 0.1612 (0.35)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.05)*</td>\n",
       "      <td>0.0053 -&gt; 0.003 (0.0)*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SG</th>\n",
       "      <td>0.1245 -&gt; 3.1575 (0.19)</td>\n",
       "      <td>0.0279 -&gt; 0.1457 (0.25)</td>\n",
       "      <td>0.0219 -&gt; 0.0124 (0.03)*</td>\n",
       "      <td>0.7819 -&gt; 0.6344 (0.47)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.54)</td>\n",
       "      <td>0.0168 -&gt; 0.0116 (0.05)*</td>\n",
       "      <td>0.0984 -&gt; 0.0905 (0.34)</td>\n",
       "      <td>0.009 -&gt; 0.0074 (0.03)*</td>\n",
       "      <td>0.0055 -&gt; 0.0054 (0.66)</td>\n",
       "      <td>0.0907 -&gt; 0.0945 (0.62)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0034 -&gt; 0.0028 (0.11)</td>\n",
       "      <td>0.0792 -&gt; 0.0475 (0.0)*</td>\n",
       "      <td>0.0091 -&gt; 0.0069 (0.02)*</td>\n",
       "      <td>0.0058 -&gt; 0.0064 (0.61)</td>\n",
       "      <td>0.177 -&gt; 0.1698 (0.91)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.11)</td>\n",
       "      <td>0.0053 -&gt; 0.0047 (0.2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Llama 3</th>\n",
       "      <th>R</th>\n",
       "      <td>0.1245 -&gt; 0.1087 (0.77)</td>\n",
       "      <td>0.0279 -&gt; 0.0095 (0.01)*</td>\n",
       "      <td>0.0219 -&gt; 0.0122 (0.02)*</td>\n",
       "      <td>0.7819 -&gt; 0.2998 (0.0)*</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.97)</td>\n",
       "      <td>0.0168 -&gt; 0.0083 (0.0)*</td>\n",
       "      <td>0.0984 -&gt; 0.1277 (0.01)**</td>\n",
       "      <td>0.009 -&gt; 0.0064 (0.0)*</td>\n",
       "      <td>0.0055 -&gt; 0.0059 (0.42)</td>\n",
       "      <td>0.0907 -&gt; 0.078 (0.26)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0034 -&gt; 0.0027 (0.09)</td>\n",
       "      <td>0.0796 -&gt; 0.0519 (0.0)*</td>\n",
       "      <td>0.0091 -&gt; 0.0045 (0.0)*</td>\n",
       "      <td>0.0058 -&gt; 0.0048 (0.17)</td>\n",
       "      <td>0.1755 -&gt; 0.0937 (0.0)*</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0053 -&gt; 0.0032 (0.0)*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SG</th>\n",
       "      <td>0.1245 -&gt; 0.1332 (0.79)</td>\n",
       "      <td>0.0279 -&gt; 0.0226 (0.65)</td>\n",
       "      <td>0.0219 -&gt; 0.0149 (0.13)</td>\n",
       "      <td>0.7819 -&gt; 0.5083 (0.16)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.59)</td>\n",
       "      <td>0.0168 -&gt; 0.0089 (0.01)*</td>\n",
       "      <td>0.0984 -&gt; 0.0838 (0.16)</td>\n",
       "      <td>0.009 -&gt; 0.0111 (0.07)</td>\n",
       "      <td>0.0055 -&gt; 0.0155 (0.0)**</td>\n",
       "      <td>0.0907 -&gt; 0.0754 (0.11)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.0)*</td>\n",
       "      <td>0.0034 -&gt; 0.0096 (0.0)**</td>\n",
       "      <td>0.0792 -&gt; 0.0647 (0.08)</td>\n",
       "      <td>0.0091 -&gt; 0.0069 (0.05)*</td>\n",
       "      <td>0.0058 -&gt; 0.0061 (0.79)</td>\n",
       "      <td>0.177 -&gt; 0.1486 (0.17)</td>\n",
       "      <td>0.0 -&gt; 0.0 (0.67)</td>\n",
       "      <td>0.0053 -&gt; 0.005 (0.52)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset                            News                            \\\n",
       "col                                   A                         G   \n",
       "llm     mode                                                        \n",
       "GPT-3   R        0.1245 -> 0.255 (0.51)   0.0279 -> 0.0227 (0.49)   \n",
       "        SG      0.1245 -> 0.2245 (0.43)   0.0279 -> 0.0288 (0.89)   \n",
       "Gemini  R     0.1245 -> 0.2501 (0.05)**   0.0279 -> 0.0289 (0.66)   \n",
       "        SG      0.1245 -> 3.1575 (0.19)   0.0279 -> 0.1457 (0.25)   \n",
       "Llama 3 R       0.1245 -> 0.1087 (0.77)  0.0279 -> 0.0095 (0.01)*   \n",
       "        SG      0.1245 -> 0.1332 (0.79)   0.0279 -> 0.0226 (0.65)   \n",
       "\n",
       "dataset                                                           \\\n",
       "col                                  H                         N   \n",
       "llm     mode                                                       \n",
       "GPT-3   R     0.0219 -> 0.0116 (0.01)*  0.7819 -> 0.3637 (0.01)*   \n",
       "        SG    0.0219 -> 0.0146 (0.05)*   0.7819 -> 0.5653 (0.21)   \n",
       "Gemini  R     0.0219 -> 0.0107 (0.01)*   0.7819 -> 0.4592 (0.09)   \n",
       "        SG    0.0219 -> 0.0124 (0.03)*   0.7819 -> 0.6344 (0.47)   \n",
       "Llama 3 R     0.0219 -> 0.0122 (0.02)*   0.7819 -> 0.2998 (0.0)*   \n",
       "        SG     0.0219 -> 0.0149 (0.13)   0.7819 -> 0.5083 (0.16)   \n",
       "\n",
       "dataset                                                    \\\n",
       "col                           S                         T   \n",
       "llm     mode                                                \n",
       "GPT-3   R     0.0 -> 0.0 (0.38)    0.0168 -> 0.008 (0.0)*   \n",
       "        SG    0.0 -> 0.0 (0.85)  0.0168 -> 0.0107 (0.02)*   \n",
       "Gemini  R     0.0 -> 0.0 (0.89)  0.0168 -> 0.0096 (0.01)*   \n",
       "        SG    0.0 -> 0.0 (0.54)  0.0168 -> 0.0116 (0.05)*   \n",
       "Llama 3 R     0.0 -> 0.0 (0.97)   0.0168 -> 0.0083 (0.0)*   \n",
       "        SG    0.0 -> 0.0 (0.59)  0.0168 -> 0.0089 (0.01)*   \n",
       "\n",
       "dataset                          Papers                           \\\n",
       "col                                   A                        G   \n",
       "llm     mode                                                       \n",
       "GPT-3   R       0.0984 -> 0.0829 (0.17)   0.009 -> 0.007 (0.03)*   \n",
       "        SG     0.0984 -> 0.0784 (0.01)*  0.009 -> 0.0071 (0.01)*   \n",
       "Gemini  R       0.0984 -> 0.1044 (0.35)  0.009 -> 0.0071 (0.02)*   \n",
       "        SG      0.0984 -> 0.0905 (0.34)  0.009 -> 0.0074 (0.03)*   \n",
       "Llama 3 R     0.0984 -> 0.1277 (0.01)**   0.009 -> 0.0064 (0.0)*   \n",
       "        SG      0.0984 -> 0.0838 (0.16)   0.009 -> 0.0111 (0.07)   \n",
       "\n",
       "dataset                                                          \\\n",
       "col                                  H                        N   \n",
       "llm     mode                                                      \n",
       "GPT-3   R      0.0055 -> 0.0057 (0.89)  0.0907 -> 0.0824 (0.42)   \n",
       "        SG    0.0055 -> 0.0044 (0.02)*  0.0907 -> 0.0895 (0.82)   \n",
       "Gemini  R      0.0055 -> 0.0057 (0.68)  0.0907 -> 0.0784 (0.15)   \n",
       "        SG     0.0055 -> 0.0054 (0.66)  0.0907 -> 0.0945 (0.62)   \n",
       "Llama 3 R      0.0055 -> 0.0059 (0.42)   0.0907 -> 0.078 (0.26)   \n",
       "        SG    0.0055 -> 0.0155 (0.0)**  0.0907 -> 0.0754 (0.11)   \n",
       "\n",
       "dataset                                                    \\\n",
       "col                           S                         T   \n",
       "llm     mode                                                \n",
       "GPT-3   R     0.0 -> 0.0 (0.0)*    0.0034 -> 0.003 (0.24)   \n",
       "        SG    0.0 -> 0.0 (0.0)*  0.0034 -> 0.0027 (0.02)*   \n",
       "Gemini  R     0.0 -> 0.0 (0.0)*   0.0034 -> 0.0025 (0.0)*   \n",
       "        SG    0.0 -> 0.0 (0.0)*   0.0034 -> 0.0028 (0.11)   \n",
       "Llama 3 R     0.0 -> 0.0 (0.0)*   0.0034 -> 0.0027 (0.09)   \n",
       "        SG    0.0 -> 0.0 (0.0)*  0.0034 -> 0.0096 (0.0)**   \n",
       "\n",
       "dataset                         Reddit                            \\\n",
       "col                                  A                         G   \n",
       "llm     mode                                                       \n",
       "GPT-3   R     0.0792 -> 0.0596 (0.02)*  0.0091 -> 0.0063 (0.01)*   \n",
       "        SG    0.0792 -> 0.0569 (0.03)*  0.0091 -> 0.0068 (0.02)*   \n",
       "Gemini  R      0.0792 -> 0.1083 (0.58)   0.0091 -> 0.0076 (0.15)   \n",
       "        SG     0.0792 -> 0.0475 (0.0)*  0.0091 -> 0.0069 (0.02)*   \n",
       "Llama 3 R      0.0796 -> 0.0519 (0.0)*   0.0091 -> 0.0045 (0.0)*   \n",
       "        SG     0.0792 -> 0.0647 (0.08)  0.0091 -> 0.0069 (0.05)*   \n",
       "\n",
       "dataset                                                          \\\n",
       "col                                  H                        N   \n",
       "llm     mode                                                      \n",
       "GPT-3   R      0.0058 -> 0.0054 (0.18)   0.177 -> 0.1965 (0.84)   \n",
       "        SG    0.0058 -> 0.0049 (0.04)*   0.177 -> 0.1251 (0.0)*   \n",
       "Gemini  R     0.0058 -> 0.0044 (0.03)*   0.177 -> 0.1612 (0.35)   \n",
       "        SG     0.0058 -> 0.0064 (0.61)   0.177 -> 0.1698 (0.91)   \n",
       "Llama 3 R      0.0058 -> 0.0048 (0.17)  0.1755 -> 0.0937 (0.0)*   \n",
       "        SG     0.0058 -> 0.0061 (0.79)   0.177 -> 0.1486 (0.17)   \n",
       "\n",
       "dataset                                                     \n",
       "col                            S                         T  \n",
       "llm     mode                                                \n",
       "GPT-3   R      0.0 -> 0.0 (0.0)*  0.0053 -> 0.0042 (0.01)*  \n",
       "        SG    0.0 -> 0.0 (0.04)*   0.0053 -> 0.0036 (0.0)*  \n",
       "Gemini  R     0.0 -> 0.0 (0.05)*    0.0053 -> 0.003 (0.0)*  \n",
       "        SG     0.0 -> 0.0 (0.11)    0.0053 -> 0.0047 (0.2)  \n",
       "Llama 3 R      0.0 -> 0.0 (0.0)*   0.0053 -> 0.0032 (0.0)*  \n",
       "        SG     0.0 -> 0.0 (0.67)    0.0053 -> 0.005 (0.52)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "final_datasets = []\n",
    "final_llms = []\n",
    "final_modes = []\n",
    "final_significances = []\n",
    "final_cols = []\n",
    "\n",
    "for col in [\"mean\", \"avg_dep\", \"type_token\", \"hapax\", \"shannon\", \"simpsons\"]:\n",
    "    for dataset in filtered_df[\"dataset\"].unique():\n",
    "        for llm in filtered_df[\"llm\"].unique():\n",
    "            for mode in filtered_df[\"mode\"].unique():\n",
    "                final_datasets.append(datasets_to_correct_names[dataset])\n",
    "                final_llms.append(llms_to_correct_names[llm])\n",
    "                final_modes.append(modes_to_correct_names[mode])\n",
    "                final_cols.append(cols_to_shortened_names[col])\n",
    "                original_values = filtered_df[\n",
    "                    (filtered_df[\"dataset\"] == dataset)\n",
    "                    & (filtered_df[\"llm\"] == llm)\n",
    "                    & (filtered_df[\"mode\"] == mode)\n",
    "                    & (filtered_df[\"is_original\"] == \"original_text\")\n",
    "                ][col].values[0]\n",
    "                rewritten_values = filtered_df[\n",
    "                    (filtered_df[\"dataset\"] == dataset)\n",
    "                    & (filtered_df[\"llm\"] == llm)\n",
    "                    & (filtered_df[\"mode\"] == mode)\n",
    "                    & (filtered_df[\"is_original\"] == \"rewritten_text\")\n",
    "                ][col].values[0]\n",
    "                stats, p = levene(original_values, rewritten_values)\n",
    "                p = np.round(p, 2)\n",
    "                original_vars = np.round(np.var(original_values), 4)\n",
    "                rewritten_vars = np.round(np.var(rewritten_values), 4)\n",
    "                if p <= 0.05:\n",
    "                    if original_vars >= rewritten_vars:\n",
    "                        final_significances.append(\n",
    "                            f\"{original_vars} -> {rewritten_vars} ({p})*\"\n",
    "                        )\n",
    "                    else:\n",
    "                        final_significances.append(\n",
    "                            f\"{original_vars} -> {rewritten_vars} ({p})**\"\n",
    "                        )\n",
    "                else:\n",
    "                    final_significances.append(\n",
    "                        f\"{original_vars} -> {rewritten_vars} ({p})\"\n",
    "                    )\n",
    "\n",
    "final_df = pd.DataFrame(\n",
    "    {\n",
    "        \"dataset\": final_datasets,\n",
    "        \"llm\": final_llms,\n",
    "        \"mode\": final_modes,\n",
    "        \"significance\": final_significances,\n",
    "        \"col\": final_cols,\n",
    "    }\n",
    ")\n",
    "\n",
    "display(\n",
    "    final_df.pivot_table(\n",
    "        index=[\"llm\", \"mode\"],\n",
    "        columns=[\"dataset\", \"col\"],\n",
    "        values=\"significance\",\n",
    "        aggfunc=lambda x: x,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the second level of the agg_table in columsn, sort them in the order of AGG, AD, TT, H, SH, S\n",
    "agg_table = final_df.pivot_table(\n",
    "    index=[\"llm\", \"mode\"],\n",
    "    columns=[\"dataset\", \"col\"],\n",
    "    values=\"significance\",\n",
    "    aggfunc=lambda x: x,\n",
    ")\n",
    "\n",
    "agg_table = agg_table[\n",
    "    [\n",
    "        (\"News\", \"G\"),\n",
    "        (\"News\", \"A\"),\n",
    "        (\"News\", \"T\"),\n",
    "        (\"News\", \"H\"),\n",
    "        (\"News\", \"N\"),\n",
    "        (\"News\", \"S\"),\n",
    "        (\"Papers\", \"G\"),\n",
    "        (\"Papers\", \"A\"),\n",
    "        (\"Papers\", \"T\"),\n",
    "        (\"Papers\", \"H\"),\n",
    "        (\"Papers\", \"N\"),\n",
    "        (\"Papers\", \"S\"),\n",
    "        (\"Reddit\", \"G\"),\n",
    "        (\"Reddit\", \"A\"),\n",
    "        (\"Reddit\", \"T\"),\n",
    "        (\"Reddit\", \"H\"),\n",
    "        (\"Reddit\", \"N\"),\n",
    "        (\"Reddit\", \"S\"),\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/_dcfmqnx4mxbhf22rbww8kwc0000gp/T/ipykernel_81388/3574941191.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dataset_data.applymap(lambda x: 2 if \"**\" in x else (1 if \"*\" in x else 0)),\n",
      "/var/folders/s6/_dcfmqnx4mxbhf22rbww8kwc0000gp/T/ipykernel_81388/3574941191.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  annot=dataset_data.applymap(\n",
      "/var/folders/s6/_dcfmqnx4mxbhf22rbww8kwc0000gp/T/ipykernel_81388/3574941191.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dataset_data.applymap(lambda x: 2 if \"**\" in x else (1 if \"*\" in x else 0)),\n",
      "/var/folders/s6/_dcfmqnx4mxbhf22rbww8kwc0000gp/T/ipykernel_81388/3574941191.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  annot=dataset_data.applymap(\n",
      "/var/folders/s6/_dcfmqnx4mxbhf22rbww8kwc0000gp/T/ipykernel_81388/3574941191.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dataset_data.applymap(lambda x: 2 if \"**\" in x else (1 if \"*\" in x else 0)),\n",
      "/var/folders/s6/_dcfmqnx4mxbhf22rbww8kwc0000gp/T/ipykernel_81388/3574941191.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  annot=dataset_data.applymap(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create three subplots, one for each dataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 4), sharey=True)\n",
    "datasets = [\"News\", \"Papers\", \"Reddit\"]\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "for dataset, ax in zip(datasets, axes):\n",
    "    # Get data for this dataset\n",
    "    dataset_data = agg_table[\n",
    "        [\n",
    "            (dataset, \"G\"),\n",
    "            (dataset, \"A\"),\n",
    "            (dataset, \"T\"),\n",
    "            (dataset, \"H\"),\n",
    "            (dataset, \"N\"),\n",
    "            (dataset, \"S\"),\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        dataset_data.applymap(lambda x: 2 if \"**\" in x else (1 if \"*\" in x else 0)),\n",
    "        cmap=sns.color_palette([\"lightgrey\", \"salmon\", \"skyblue\"]),\n",
    "        annot=dataset_data.applymap(\n",
    "            lambda x: \"\" if \"**\" in x else (\"\" if \"*\" in x else \"\")\n",
    "        ),\n",
    "        fmt=\"\",\n",
    "        cbar=False,\n",
    "        linewidths=1,\n",
    "        linecolor=\"black\",\n",
    "        annot_kws={\"color\": \"black\"},\n",
    "        center=1,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Set x-axis labels\n",
    "    ax.set_xticks(np.arange(len(dataset_data.columns)) + 0.5)\n",
    "    ax.set_xticklabels([col[1] for col in dataset_data.columns], rotation=0)\n",
    "    if ax == ax1:\n",
    "        ax.set_ylabel(\"LLM / Mode\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "\n",
    "    # Set y-axis labels only for first subplot\n",
    "\n",
    "    # Add first level of row labels\n",
    "    ax.set_yticks(\n",
    "        np.arange(len(dataset_data.index.levels[0].unique()))\n",
    "        * len(dataset_data.index.levels[1].unique())\n",
    "        + (len(dataset_data.index.levels[1].unique()) - 1) / 2\n",
    "        + 0.2,\n",
    "        minor=False,\n",
    "    )\n",
    "    ax.set_yticklabels(\n",
    "        dataset_data.index.levels[0].unique(), minor=False, rotation=90, x=-0.05\n",
    "    )\n",
    "\n",
    "    # Add second level of row labels\n",
    "    ax.set_yticks(np.arange(len(dataset_data.index)) + 0.4, minor=True)\n",
    "    ax.set_yticklabels([idx[1] for idx in dataset_data.index], minor=True)\n",
    "\n",
    "    # Set title and labels\n",
    "    ax.set_title(dataset)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "fig.supxlabel(\"Complexity Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"study_2_results_all_features.png\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
