{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities_based_on_features(features_df, year_col, month_col, day_col):\n",
    "\n",
    "    # Add week number to features_df\n",
    "    features_df[\"date\"] = pd.to_datetime(\n",
    "        features_df[year_col].astype(int).astype(str)\n",
    "        + \"-\"\n",
    "        + features_df[month_col].astype(int).astype(str)\n",
    "        + \"-\"\n",
    "        + features_df[day_col].astype(int).astype(str)\n",
    "    )\n",
    "    features_df[\"week\"] = features_df[\"date\"].dt.isocalendar().week\n",
    "\n",
    "    grouped = features_df.groupby([year_col, \"week\"])\n",
    "    features_columns = [\n",
    "        col\n",
    "        for col in features_df.columns\n",
    "        if (col.startswith(\"voc_\") or col.startswith(\"lex_\") or col.startswith(\"liwc_\"))\n",
    "    ]\n",
    "\n",
    "    weeks = []\n",
    "    years = []\n",
    "    variances = defaultdict(list)\n",
    "    means = defaultdict(list)\n",
    "\n",
    "    for name, group in grouped:\n",
    "        year, week = name\n",
    "        for col in features_columns:\n",
    "            try:\n",
    "                variances[col].append(group[col].var())\n",
    "                means[col].append(group[col].mean())\n",
    "            except:\n",
    "                variances[col].append(np.nan)\n",
    "                means[col].append(np.nan)\n",
    "        weeks.append(week)\n",
    "        years.append(year)\n",
    "\n",
    "    variances_df = pd.DataFrame({\"year\": years, \"week\": weeks})\n",
    "    for col in features_columns:\n",
    "        variances_df[f\"similarity_{col}\"] = variances[col]\n",
    "        variances_df[f\"mean_{col}\"] = means[col]\n",
    "\n",
    "    return variances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_similarities(data):\n",
    "    data[\"year\"] = data[\"year\"].astype(float).astype(int)\n",
    "    data[\"week\"] = data[\"week\"].astype(float).astype(int)\n",
    "\n",
    "    # only keep the rows with year >= 2018\n",
    "    data = data[data[\"year\"] >= 2018]\n",
    "\n",
    "    # sort the data by year and month\n",
    "    # Fill NaN values with mean of each column\n",
    "    data = data.fillna(data.mean())\n",
    "    data = data.set_index([\"year\", \"week\"]).sort_index().reset_index()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/_dcfmqnx4mxbhf22rbww8kwc0000gp/T/ipykernel_7998/59441959.py:1: DtypeWarning: Columns (249,256) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  papers_features = pd.read_csv(\"papers/cl_cv_papers_features.csv\")\n",
      "/var/folders/s6/_dcfmqnx4mxbhf22rbww8kwc0000gp/T/ipykernel_7998/59441959.py:2: DtypeWarning: Columns (0,249) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  news_features = pd.read_csv(\"news/news_features.csv\")\n"
     ]
    }
   ],
   "source": [
    "papers_features = pd.read_csv(\"papers/cl_cv_papers_features.csv\")\n",
    "news_features = pd.read_csv(\"news/news_features.csv\")\n",
    "reddit_features = pd.read_csv(\"reddit/reddit_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_features[\"final_date\"] = pd.to_datetime(papers_features[\"final_date\"])\n",
    "papers_features[\"day\"] = papers_features[\"final_date\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_features[\"created_utc\"] = pd.to_datetime(reddit_features[\"created_utc\"])\n",
    "reddit_features[\"day\"] = reddit_features[\"created_utc\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_features = papers_features.dropna(subset=[\"year\", \"month\", \"day\"])\n",
    "news_features = news_features.dropna(subset=[\"year\", \"month\", \"day\"])\n",
    "reddit_features = reddit_features.dropna(subset=[\"year\", \"month\", \"day\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_similarities = compute_similarities_based_on_features(\n",
    "    papers_features, \"year\", \"month\", \"day\"\n",
    ")\n",
    "news_similarities = compute_similarities_based_on_features(\n",
    "    news_features, \"year\", \"month\", \"day\"\n",
    ")\n",
    "reddit_similarities = compute_similarities_based_on_features(\n",
    "    reddit_features, \"year\", \"month\", \"day\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_clean_similarities = clean_similarities(papers_similarities)\n",
    "news_clean_similarities = clean_similarities(news_similarities)\n",
    "reddit_clean_similarities = clean_similarities(reddit_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/_dcfmqnx4mxbhf22rbww8kwc0000gp/T/ipykernel_7998/1713965141.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  papers_ai_detection_df = pd.read_csv(\"papers/cl_cv_papers_ai_written.csv\")\n"
     ]
    }
   ],
   "source": [
    "papers_threshold = 0.82\n",
    "papers_ai_detection_df = pd.read_csv(\"papers/cl_cv_papers_ai_written.csv\")\n",
    "papers_ai_detection_df[\"final_date\"] = pd.to_datetime(\n",
    "    papers_ai_detection_df[\"final_date\"]\n",
    ")\n",
    "papers_ai_detection_df[\"week\"] = (\n",
    "    papers_ai_detection_df[\"final_date\"].dt.isocalendar().week\n",
    ")\n",
    "\n",
    "papers_ai_detection_df[\"ai_written\"] = (\n",
    "    papers_ai_detection_df[\"ai_written\"] < papers_threshold\n",
    ")\n",
    "\n",
    "papers_ai_detection_df = papers_ai_detection_df[papers_ai_detection_df[\"year\"] >= 2018]\n",
    "papers_ai_detection_df = (\n",
    "    papers_ai_detection_df.groupby([\"year\", \"week\"])[\"ai_written\"].mean().reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_threshold = 0.83\n",
    "news_ai_detection_df = pd.read_csv(\"news/news_ai_written.csv\")\n",
    "news_ai_detection_df[\"date\"] = pd.to_datetime(\n",
    "    news_ai_detection_df[\"year\"].astype(int).astype(str)\n",
    "    + \"-\"\n",
    "    + news_ai_detection_df[\"month\"].astype(int).astype(str)\n",
    "    + \"-\"\n",
    "    + news_ai_detection_df[\"day\"].astype(int).astype(str)\n",
    ")\n",
    "news_ai_detection_df[\"week\"] = news_ai_detection_df[\"date\"].dt.isocalendar().week\n",
    "\n",
    "\n",
    "news_ai_detection_df[\"ai_written\"] = news_ai_detection_df[\"ai_written\"] < news_threshold\n",
    "\n",
    "news_ai_detection_df = news_ai_detection_df[news_ai_detection_df[\"year\"] >= 2018]\n",
    "news_ai_detection_df = (\n",
    "    news_ai_detection_df.groupby([\"year\", \"week\"])[\"ai_written\"].mean().reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_threshold = 0.87\n",
    "reddit_ai_detection_df = pd.read_csv(\"reddit/reddit_ai_written.csv\")\n",
    "reddit_ai_detection_df[\"created_utc\"] = pd.to_datetime(\n",
    "    reddit_ai_detection_df[\"created_utc\"]\n",
    ")\n",
    "reddit_ai_detection_df[\"week\"] = (\n",
    "    reddit_ai_detection_df[\"created_utc\"].dt.isocalendar().week\n",
    ")\n",
    "\n",
    "reddit_ai_detection_df[\"ai_written\"] = (\n",
    "    reddit_ai_detection_df[\"ai_written\"] < reddit_threshold\n",
    ")\n",
    "\n",
    "reddit_ai_detection_df = reddit_ai_detection_df[reddit_ai_detection_df[\"year\"] >= 2018]\n",
    "reddit_ai_detection_df = (\n",
    "    reddit_ai_detection_df.groupby([\"year\", \"week\"])[\"ai_written\"].mean().reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>similarity_lex_avg_word_length</th>\n",
       "      <th>mean_lex_avg_word_length</th>\n",
       "      <th>similarity_lex_avg_sent_length_by_char</th>\n",
       "      <th>mean_lex_avg_sent_length_by_char</th>\n",
       "      <th>similarity_lex_avg_sent_length_by_word</th>\n",
       "      <th>mean_lex_avg_sent_length_by_word</th>\n",
       "      <th>similarity_lex_special_char_count</th>\n",
       "      <th>mean_lex_special_char_count</th>\n",
       "      <th>...</th>\n",
       "      <th>similarity_liwc_relig</th>\n",
       "      <th>mean_liwc_relig</th>\n",
       "      <th>similarity_liwc_death</th>\n",
       "      <th>mean_liwc_death</th>\n",
       "      <th>similarity_liwc_assent</th>\n",
       "      <th>mean_liwc_assent</th>\n",
       "      <th>similarity_liwc_nonfl</th>\n",
       "      <th>mean_liwc_nonfl</th>\n",
       "      <th>similarity_liwc_filler</th>\n",
       "      <th>mean_liwc_filler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.172561</td>\n",
       "      <td>7.307538</td>\n",
       "      <td>832.039789</td>\n",
       "      <td>150.960982</td>\n",
       "      <td>15.976867</td>\n",
       "      <td>21.958084</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.154852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>1.169593e-06</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>7.054309e-07</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>0.189494</td>\n",
       "      <td>7.237418</td>\n",
       "      <td>1113.009707</td>\n",
       "      <td>155.331594</td>\n",
       "      <td>23.177587</td>\n",
       "      <td>22.753178</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.157301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>2.244631e-06</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>3.844527e-07</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>0.197138</td>\n",
       "      <td>7.314943</td>\n",
       "      <td>1160.273049</td>\n",
       "      <td>155.830053</td>\n",
       "      <td>20.561630</td>\n",
       "      <td>22.704009</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.154850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>9.259738e-07</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>3.980952e-06</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>0.251596</td>\n",
       "      <td>7.170824</td>\n",
       "      <td>784.735574</td>\n",
       "      <td>151.325989</td>\n",
       "      <td>14.065721</td>\n",
       "      <td>22.161425</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.150903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>3.276712e-06</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>1.341764e-06</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>0.206723</td>\n",
       "      <td>7.183508</td>\n",
       "      <td>1376.853383</td>\n",
       "      <td>154.641491</td>\n",
       "      <td>29.922524</td>\n",
       "      <td>22.794156</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.147586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>4.502314e-07</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>3.888846e-07</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>2024</td>\n",
       "      <td>42</td>\n",
       "      <td>0.196291</td>\n",
       "      <td>7.599473</td>\n",
       "      <td>772.249002</td>\n",
       "      <td>163.650653</td>\n",
       "      <td>14.862925</td>\n",
       "      <td>22.554869</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.177594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>1.541024e-06</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>8.411598e-07</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>2024</td>\n",
       "      <td>43</td>\n",
       "      <td>0.202893</td>\n",
       "      <td>7.554698</td>\n",
       "      <td>1704.550412</td>\n",
       "      <td>165.482817</td>\n",
       "      <td>26.658731</td>\n",
       "      <td>22.913977</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>0.178908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>5.760000e-07</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>1.348313e-06</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>2024</td>\n",
       "      <td>44</td>\n",
       "      <td>0.194812</td>\n",
       "      <td>7.546029</td>\n",
       "      <td>1098.058386</td>\n",
       "      <td>163.304475</td>\n",
       "      <td>19.292829</td>\n",
       "      <td>22.644112</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.176944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>5.525907e-07</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>2.888332e-06</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>2024</td>\n",
       "      <td>45</td>\n",
       "      <td>0.223838</td>\n",
       "      <td>7.609850</td>\n",
       "      <td>10753.584277</td>\n",
       "      <td>172.207578</td>\n",
       "      <td>180.283435</td>\n",
       "      <td>23.625760</td>\n",
       "      <td>0.002560</td>\n",
       "      <td>0.179222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>5.634324e-07</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>7.167614e-06</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>2024</td>\n",
       "      <td>46</td>\n",
       "      <td>0.215541</td>\n",
       "      <td>7.512641</td>\n",
       "      <td>1170.975000</td>\n",
       "      <td>162.488466</td>\n",
       "      <td>21.034873</td>\n",
       "      <td>22.581401</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.176861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.003184</td>\n",
       "      <td>1.563453e-06</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>2.226227e-06</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows Ã— 628 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  week  similarity_lex_avg_word_length  mean_lex_avg_word_length  \\\n",
       "0    2018     1                        0.172561                  7.307538   \n",
       "1    2018     2                        0.189494                  7.237418   \n",
       "2    2018     3                        0.197138                  7.314943   \n",
       "3    2018     4                        0.251596                  7.170824   \n",
       "4    2018     5                        0.206723                  7.183508   \n",
       "..    ...   ...                             ...                       ...   \n",
       "355  2024    42                        0.196291                  7.599473   \n",
       "356  2024    43                        0.202893                  7.554698   \n",
       "357  2024    44                        0.194812                  7.546029   \n",
       "358  2024    45                        0.223838                  7.609850   \n",
       "359  2024    46                        0.215541                  7.512641   \n",
       "\n",
       "     similarity_lex_avg_sent_length_by_char  mean_lex_avg_sent_length_by_char  \\\n",
       "0                                832.039789                        150.960982   \n",
       "1                               1113.009707                        155.331594   \n",
       "2                               1160.273049                        155.830053   \n",
       "3                                784.735574                        151.325989   \n",
       "4                               1376.853383                        154.641491   \n",
       "..                                      ...                               ...   \n",
       "355                              772.249002                        163.650653   \n",
       "356                             1704.550412                        165.482817   \n",
       "357                             1098.058386                        163.304475   \n",
       "358                            10753.584277                        172.207578   \n",
       "359                             1170.975000                        162.488466   \n",
       "\n",
       "     similarity_lex_avg_sent_length_by_word  mean_lex_avg_sent_length_by_word  \\\n",
       "0                                 15.976867                         21.958084   \n",
       "1                                 23.177587                         22.753178   \n",
       "2                                 20.561630                         22.704009   \n",
       "3                                 14.065721                         22.161425   \n",
       "4                                 29.922524                         22.794156   \n",
       "..                                      ...                               ...   \n",
       "355                               14.862925                         22.554869   \n",
       "356                               26.658731                         22.913977   \n",
       "357                               19.292829                         22.644112   \n",
       "358                              180.283435                         23.625760   \n",
       "359                               21.034873                         22.581401   \n",
       "\n",
       "     similarity_lex_special_char_count  mean_lex_special_char_count  ...  \\\n",
       "0                             0.001641                     0.154852  ...   \n",
       "1                             0.002896                     0.157301  ...   \n",
       "2                             0.001454                     0.154850  ...   \n",
       "3                             0.001470                     0.150903  ...   \n",
       "4                             0.001282                     0.147586  ...   \n",
       "..                                 ...                          ...  ...   \n",
       "355                           0.002086                     0.177594  ...   \n",
       "356                           0.002349                     0.178908  ...   \n",
       "357                           0.002267                     0.176944  ...   \n",
       "358                           0.002560                     0.179222  ...   \n",
       "359                           0.002155                     0.176861  ...   \n",
       "\n",
       "     similarity_liwc_relig  mean_liwc_relig  similarity_liwc_death  \\\n",
       "0                 0.000012         0.002445           1.169593e-06   \n",
       "1                 0.000009         0.001578           2.244631e-06   \n",
       "2                 0.000010         0.001945           9.259738e-07   \n",
       "3                 0.000011         0.001890           3.276712e-06   \n",
       "4                 0.000009         0.001637           4.502314e-07   \n",
       "..                     ...              ...                    ...   \n",
       "355               0.000025         0.003952           1.541024e-06   \n",
       "356               0.000018         0.003318           5.760000e-07   \n",
       "357               0.000017         0.003029           5.525907e-07   \n",
       "358               0.000020         0.003352           5.634324e-07   \n",
       "359               0.000018         0.003184           1.563453e-06   \n",
       "\n",
       "     mean_liwc_death  similarity_liwc_assent  mean_liwc_assent  \\\n",
       "0           0.000170            7.054309e-07          0.000133   \n",
       "1           0.000245            3.844527e-07          0.000071   \n",
       "2           0.000175            3.980952e-06          0.000260   \n",
       "3           0.000223            1.341764e-06          0.000193   \n",
       "4           0.000093            3.888846e-07          0.000087   \n",
       "..               ...                     ...               ...   \n",
       "355         0.000087            8.411598e-07          0.000093   \n",
       "356         0.000073            1.348313e-06          0.000113   \n",
       "357         0.000096            2.888332e-06          0.000215   \n",
       "358         0.000078            7.167614e-06          0.000233   \n",
       "359         0.000113            2.226227e-06          0.000197   \n",
       "\n",
       "     similarity_liwc_nonfl  mean_liwc_nonfl  similarity_liwc_filler  \\\n",
       "0                 0.000004         0.000719                0.000003   \n",
       "1                 0.000002         0.000475                0.000001   \n",
       "2                 0.000007         0.000692                0.000002   \n",
       "3                 0.000007         0.000776                0.000004   \n",
       "4                 0.000005         0.000817                0.000003   \n",
       "..                     ...              ...                     ...   \n",
       "355               0.000003         0.000531                0.000005   \n",
       "356               0.000004         0.000678                0.000004   \n",
       "357               0.000005         0.000619                0.000005   \n",
       "358               0.000003         0.000500                0.000004   \n",
       "359               0.000005         0.000655                0.000007   \n",
       "\n",
       "     mean_liwc_filler  \n",
       "0            0.000391  \n",
       "1            0.000237  \n",
       "2            0.000336  \n",
       "3            0.000375  \n",
       "4            0.000384  \n",
       "..                ...  \n",
       "355          0.000602  \n",
       "356          0.000602  \n",
       "357          0.000695  \n",
       "358          0.000506  \n",
       "359          0.000896  \n",
       "\n",
       "[360 rows x 628 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_clean_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_clean_similarities = papers_clean_similarities.drop(\n",
    "    [col for col in papers_clean_similarities.columns if col.startswith(\"mean_\")],\n",
    "    axis=1,\n",
    ")\n",
    "news_clean_similarities = news_clean_similarities.drop(\n",
    "    [col for col in news_clean_similarities.columns if col.startswith(\"mean_\")], axis=1\n",
    ")\n",
    "reddit_clean_similarities = reddit_clean_similarities.drop(\n",
    "    [col for col in reddit_clean_similarities.columns if col.startswith(\"mean_\")],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_about_complexity = [\n",
    "    \"similarity_voc_simpson_index\",\n",
    "    \"similarity_voc_shannon_entropy\",\n",
    "    \"similarity_lex_avg_dependency_link_length\",\n",
    "    \"similarity_voc_type_token_ratio\",\n",
    "    \"similarity_voc_hapax_legomena\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each of the dataframes, keep only the complexity-related columns and also year and week\n",
    "\n",
    "papers_clean_similarities = papers_clean_similarities[\n",
    "    [\"year\", \"week\"] + columns_about_complexity\n",
    "]\n",
    "news_clean_similarities = news_clean_similarities[\n",
    "    [\"year\", \"week\"] + columns_about_complexity\n",
    "]\n",
    "reddit_clean_similarities = reddit_clean_similarities[\n",
    "    [\"year\", \"week\"] + columns_about_complexity\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_merged_df = pd.merge(\n",
    "    papers_clean_similarities,\n",
    "    papers_ai_detection_df,\n",
    "    on=[\"year\", \"week\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_ai\"),\n",
    ")\n",
    "\n",
    "\n",
    "news_merged_df = pd.merge(\n",
    "    news_clean_similarities,\n",
    "    news_ai_detection_df,\n",
    "    on=[\"year\", \"week\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_ai\"),\n",
    ")\n",
    "\n",
    "\n",
    "reddit_merged_df = pd.merge(\n",
    "    reddit_clean_similarities,\n",
    "    reddit_ai_detection_df,\n",
    "    on=[\"year\", \"week\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_ai\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of the columns in complexity_columns, and also ai_written, normalize them\n",
    "for df in [papers_merged_df, news_merged_df, reddit_merged_df]:\n",
    "    for col in columns_about_complexity + [\"ai_written\"]:\n",
    "        scaler = StandardScaler()\n",
    "        df[col] = scaler.fit_transform(df[[col]])\n",
    "\n",
    "    df[\"complexity\"] = np.mean(\n",
    "        [\n",
    "            df[\"similarity_voc_simpson_index\"],\n",
    "            df[\"similarity_voc_shannon_entropy\"],\n",
    "            df[\"similarity_lex_avg_dependency_link_length\"],\n",
    "            df[\"similarity_voc_type_token_ratio\"],\n",
    "            df[\"similarity_voc_hapax_legomena\"],\n",
    "        ],\n",
    "        axis=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_merged_df.to_csv(\"papers/papers_merged_weekly_df.csv\", index=False)\n",
    "news_merged_df.to_csv(\"news/news_merged_weekly_df.csv\", index=False)\n",
    "reddit_merged_df.to_csv(\"reddit/reddit_merged_weekly_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
