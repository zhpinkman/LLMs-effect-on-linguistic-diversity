{"1812.10788": {"original_text": "  Hyperspectral remote sensing is a prominent research topic in data\nprocessing. Most of the spectral unmixing algorithms are developed by adopting\nthe linear mixing models. Nonnegative matrix factorization (NMF) and its\ndevelopments are used widely for estimation of signatures and fractional\nabundances in the SU problem. Sparsity constraints was added to NMF, and was\nregularized by $ L_ {q} $ norm. In this paper, at first hyperspectral images\nare clustered by fuzzy c- means method, and then a new algorithm based on\nsparsity constrained distributed optimization is used for spectral unmixing. In\nthe proposed algorithm, a network including clusters is employed. Each pixel in\nthe hyperspectral images considered as a node in this network. The proposed\nalgorithm is optimized with diffusion LMS strategy, and then the update\nequations for fractional abundance and signature matrices are obtained.\nSimulation results based on defined performance metrics illustrate advantage of\nthe proposed algorithm in spectral unmixing of hyperspectral data compared with\nother methods.\n", "rewritten_text": "Hyperspectral remote sensing is a prominent research topic in data processing.  Most spectral unmixing algorithms are based on linear mixing models. Nonnegative matrix factorization (NMF) and its variants are widely used to estimate spectral signatures and fractional abundances.  To improve NMF, sparsity constraints have been added and regularized using the $L_q$ norm. This paper presents a novel spectral unmixing algorithm that first clusters hyperspectral images using fuzzy c-means clustering.  A sparsity-constrained distributed optimization algorithm is then applied, employing a network where each pixel in the hyperspectral image represents a node.  The algorithm is optimized using a diffusion least mean squares (LMS) strategy, yielding update equations for the fractional abundance and signature matrices. Simulation results, based on established performance metrics, demonstrate the superiority of the proposed algorithm compared to existing methods for hyperspectral spectral unmixing.\n"}, "2305.04628": {"original_text": "  In this paper, we introduce a novel framework for the challenging problem of\nOne-Shot Unsupervised Domain Adaptation (OSUDA), which aims to adapt to a\ntarget domain with only a single unlabeled target sample. Unlike existing\napproaches that rely on large labeled source and unlabeled target data, our\nTarget-driven One-Shot UDA (TOS-UDA) approach employs a learnable augmentation\nstrategy guided by the target sample's style to align the source distribution\nwith the target distribution. Our method consists of three modules: an\naugmentation module, a style alignment module, and a classifier. Unlike\nexisting methods, our augmentation module allows for strong transformations of\nthe source samples, and the style of the single target sample available is\nexploited to guide the augmentation by ensuring perceptual similarity.\nFurthermore, our approach integrates augmentation with style alignment,\neliminating the need for separate pre-training on additional datasets. Our\nmethod outperforms or performs comparably to existing OS-UDA methods on the\nDigits and DomainNet benchmarks.\n", "rewritten_text": "This paper introduces a novel framework for the challenging problem of One-Shot Unsupervised Domain Adaptation (OSUDA), which adapts to a target domain using only a single unlabeled target sample.  Unlike existing approaches requiring large labeled source and unlabeled target datasets, our Target-driven One-Shot UDA (TOS-UDA) method employs a learnable augmentation strategy guided by the target sample's style to align the source and target distributions.  TOS-UDA comprises three modules: an augmentation module, a style alignment module, and a classifier.  Our augmentation module enables strong transformations of source samples, leveraging the single target sample's style to ensure perceptual similarity and guide the augmentation process.  Unlike previous methods, this integrated approach combines augmentation and style alignment, eliminating the need for separate pre-training.  On the Digits and DomainNet benchmarks, our method achieves performance comparable to or exceeding existing OS-UDA methods.\n"}, "2011.14589": {"original_text": "  Monocular 3D object detection, with the aim of predicting the geometric\nproperties of on-road objects, is a promising research topic for the\nintelligent perception systems of autonomous driving. Most state-of-the-art\nmethods follow a keypoint-based paradigm, where the keypoints of objects are\npredicted and employed as the basis for regressing the other geometric\nproperties. In this work, a unified network named as FADNet is presented to\naddress the task of monocular 3D object detection. In contrast to previous\nkeypoint-based methods, we propose to divide the output modalities into\ndifferent groups according to the estimation difficulty of object properties.\nDifferent groups are treated differently and sequentially associated by a\nconvolutional Gated Recurrent Unit. Another contribution of this work is the\nstrategy of depth hint augmentation. To provide characterized depth patterns as\nhints for depth estimation, a dedicated depth hint module is designed to\ngenerate row-wise features named as depth hints, which are explicitly\nsupervised in a bin-wise manner. The contributions of this work are validated\nby conducting experiments and ablation study on the KITTI benchmark. Without\nutilizing depth priors, post optimization, or other refinement modules, our\nnetwork performs competitively against state-of-the-art methods while\nmaintaining a decent running speed.\n", "rewritten_text": "Monocular 3D object detection for autonomous driving, focusing on predicting the geometric properties of on-road objects, is a promising research area.  Most state-of-the-art methods employ a keypoint-based paradigm, regressing geometric properties from predicted object keypoints.  This work presents FADNet, a unified network for monocular 3D object detection.  Unlike previous keypoint-based approaches, we propose a novel architecture that divides output modalities into groups based on estimation difficulty.  These groups are processed sequentially and associated using a convolutional Gated Recurrent Unit (GRU).  Furthermore, we introduce a depth hint augmentation strategy.  A dedicated depth hint module generates row-wise features (depth hints) to aid depth estimation; these hints are trained with bin-wise supervision.  Experiments and ablation studies on the KITTI benchmark validate our contributions.  Without depth priors, post-optimization, or refinement modules, FADNet achieves state-of-the-art performance while maintaining reasonable speed.\n"}, "2307.13958": {"original_text": "  Recently, vision transformer based multimodal learning methods have been\nproposed to improve the robustness of face anti-spoofing (FAS) systems.\nHowever, multimodal face data collected from the real world is often imperfect\ndue to missing modalities from various imaging sensors. Recently,\nflexible-modal FAS~\\cite{yu2023flexible} has attracted more attention, which\naims to develop a unified multimodal FAS model using complete multimodal face\ndata but is insensitive to test-time missing modalities. In this paper, we\ntackle one main challenge in flexible-modal FAS, i.e., when missing modality\noccurs either during training or testing in real-world situations. Inspired by\nthe recent success of the prompt learning in language models, we propose\n\\textbf{V}isual \\textbf{P}rompt flexible-modal \\textbf{FAS} (VP-FAS), which\nlearns the modal-relevant prompts to adapt the frozen pre-trained foundation\nmodel to downstream flexible-modal FAS task. Specifically, both vanilla visual\nprompts and residual contextual prompts are plugged into multimodal\ntransformers to handle general missing-modality cases, while only requiring\nless than 4\\% learnable parameters compared to training the entire model.\nFurthermore, missing-modality regularization is proposed to force models to\nlearn consistent multimodal feature embeddings when missing partial modalities.\nExtensive experiments conducted on two multimodal FAS benchmark datasets\ndemonstrate the effectiveness of our VP-FAS framework that improves the\nperformance under various missing-modality cases while alleviating the\nrequirement of heavy model re-training.\n", "rewritten_text": "Recent advancements in face anti-spoofing (FAS) leverage vision transformer-based multimodal learning to enhance robustness.  However, real-world multimodal face data often suffers from incomplete modalities due to sensor limitations.  Flexible-modal FAS methods [Yu et al., 2023] address this by aiming for unified models robust to missing modalities at test time. This paper tackles a key challenge in flexible-modal FAS: handling missing modalities during both training and testing.  Inspired by prompt learning in language models, we propose Visual Prompt flexible-modal FAS (VP-FAS). VP-FAS adapts a frozen pre-trained foundation model to the flexible-modal FAS task by learning modality-relevant prompts.  Specifically, we incorporate both visual and residual contextual prompts into multimodal transformers to address various missing-modality scenarios, requiring fewer than 4% of the parameters compared to training the entire model from scratch.  Furthermore, we introduce missing-modality regularization to ensure consistent multimodal feature embeddings even with partial modality absence.  Extensive experiments on two benchmark multimodal FAS datasets demonstrate VP-FAS's effectiveness in improving performance under various missing-modality conditions, while significantly reducing the need for extensive model retraining.\n"}, "1803.0634": {"original_text": "  We present a method for estimating detailed scene illumination using human\nfaces in a single image. In contrast to previous works that estimate lighting\nin terms of low-order basis functions or distant point lights, our technique\nestimates illumination at a higher precision in the form of a non-parametric\nenvironment map. Based on the observation that faces can exhibit strong\nhighlight reflections from a broad range of lighting directions, we propose a\ndeep neural network for extracting highlights from faces, and then trace these\nreflections back to the scene to acquire the environment map. Since real\ntraining data for highlight extraction is very limited, we introduce an\nunsupervised scheme for finetuning the network on real images, based on the\nconsistent diffuse chromaticity of a given face seen in multiple real images.\nIn tracing the estimated highlights to the environment, we reduce the blurring\neffect of skin reflectance on reflected light through a deconvolution\ndetermined by prior knowledge on face material properties. Comparisons to\nprevious techniques for highlight extraction and illumination estimation show\nthe state-of-the-art performance of this approach on a variety of indoor and\noutdoor scenes.\n", "rewritten_text": "This paper presents a novel method for estimating detailed scene illumination from a single image using human faces. Unlike previous methods that approximate lighting with low-order basis functions or distant point light sources, our technique reconstructs a high-resolution, non-parametric environment map.  Leveraging the observation that facial highlights reflect a wide range of lighting directions, we propose a deep neural network to extract these highlights.  These extracted highlights are then back-projected onto the scene to recover the environment map.  Addressing the scarcity of real-world training data for highlight extraction, we introduce an unsupervised fine-tuning scheme that exploits the consistent diffuse chromaticity of faces across multiple images.  To mitigate the blurring effect of skin reflectance on the reflected light during back-projection, we employ a deconvolution process informed by prior knowledge of facial material properties.  Comparative evaluations against existing highlight extraction and illumination estimation techniques demonstrate state-of-the-art performance across diverse indoor and outdoor scenes.\n"}, "1902.00595": {"original_text": "  Intuitively, human readers cope easily with errors in text; typos,\nmisspelling, word substitutions, etc. do not unduly disrupt natural reading.\nPrevious work indicates that letter transpositions result in increased reading\ntimes, but it is unclear if this effect generalizes to more natural errors. In\nthis paper, we report an eye-tracking study that compares two error types\n(letter transpositions and naturally occurring misspelling) and two error rates\n(10% or 50% of all words contain errors). We find that human readers show\nunimpaired comprehension in spite of these errors, but error words cause more\nreading difficulty than correct words. Also, transpositions are more difficult\nthan misspellings, and a high error rate increases difficulty for all words,\nincluding correct ones. We then present a computational model that uses\ncharacter-based (rather than traditional word-based) surprisal to account for\nthese results. The model explains that transpositions are harder than\nmisspellings because they contain unexpected letter combinations. It also\nexplains the error rate effect: upcoming words are more difficultto predict\nwhen the context is degraded, leading to increased surprisal.\n", "rewritten_text": "Human readers readily tolerate textual errors\u2014typos, misspellings, and word substitutions\u2014with minimal disruption to comprehension. While previous research suggests that letter transpositions increase reading times, the extent to which this effect generalizes to more naturally occurring errors remains unclear.  This paper reports an eye-tracking study comparing two error types (letter transpositions and naturally occurring misspellings) at two error rates (10% and 50% of words).  Results indicate unimpaired comprehension despite the errors; however, error words consistently elicited greater reading difficulty than correct words.  Furthermore, transpositions proved more difficult than misspellings, and a higher error rate increased difficulty for all words, including correct ones.  We subsequently present a computational model employing character-based, rather than word-based, surprisal to account for these findings.  The model posits that transpositions are more challenging due to their unexpected letter combinations and attributes the error rate effect to the degraded context, which hinders word prediction and increases surprisal.\n"}, "cs/0406031": {"original_text": "  This paper describes a standalone, publicly-available implementation of the\nResolution of Anaphora Procedure (RAP) given by Lappin and Leass (1994). The\nRAP algorithm resolves third person pronouns, lexical anaphors, and identifies\npleonastic pronouns. Our implementation, JavaRAP, fills a current need in\nanaphora resolution research by providing a reference implementation that can\nbe benchmarked against current algorithms. The implementation uses the\nstandard, publicly available Charniak (2000) parser as input, and generates a\nlist of anaphora-antecedent pairs as output. Alternately, an in-place\nannotation or substitution of the anaphors with their antecedents can be\nproduced. Evaluation on the MUC-6 co-reference task shows that JavaRAP has an\naccuracy of 57.9%, similar to the performance given previously in the\nliterature (e.g., Preiss 2002).\n", "rewritten_text": "This paper presents a standalone, publicly available implementation of Lappin and Leass's (1994) Resolution of Anaphora Procedure (RAP).  JavaRAP resolves third-person pronouns and lexical anaphors, and identifies pleonastic pronouns.  It addresses a critical need in anaphora resolution research by providing a benchmarkable reference implementation.  Using the publicly available Charniak (2000) parser as input, JavaRAP outputs a list of anaphora-antecedent pairs, or optionally, performs in-place annotation or substitution of anaphors with their antecedents.  Evaluation on the MUC-6 coreference task yielded an accuracy of 57.9%, comparable to results reported in previous literature (e.g., Preiss, 2002).\n"}, "2010.13508": {"original_text": "  The SHApe Recovery from Partial textured 3D scans challenge, SHARP 2020, is\nthe first edition of a challenge fostering and benchmarking methods for\nrecovering complete textured 3D scans from raw incomplete data. SHARP 2020 is\norganised as a workshop in conjunction with ECCV 2020. There are two\ncomplementary challenges, the first one on 3D human scans, and the second one\non generic objects. Challenge 1 is further split into two tracks, focusing,\nfirst, on large body and clothing regions, and, second, on fine body details. A\nnovel evaluation metric is proposed to quantify jointly the shape\nreconstruction, the texture reconstruction and the amount of completed data.\nAdditionally, two unique datasets of 3D scans are proposed, to provide raw\nground-truth data for the benchmarks. The datasets are released to the\nscientific community. Moreover, an accompanying custom library of software\nroutines is also released to the scientific community. It allows for processing\n3D scans, generating partial data and performing the evaluation. Results of the\ncompetition, analysed in comparison to baselines, show the validity of the\nproposed evaluation metrics, and highlight the challenging aspects of the task\nand of the datasets. Details on the SHARP 2020 challenge can be found at\nhttps://cvi2.uni.lu/sharp2020/.\n", "rewritten_text": "The SHARP 2020 challenge (Shape Recovery from Partial Textured 3D Scans) is the inaugural benchmark for methods recovering complete textured 3D scans from incomplete raw data.  Held as a workshop in conjunction with ECCV 2020, it comprises two complementary challenges: one focusing on 3D human scans, and the other on generic objects.  The human scan challenge is further divided into two tracks: large body and clothing regions, and fine body details.  A novel evaluation metric jointly quantifies shape and texture reconstruction accuracy, as well as the amount of completed data.  Two unique 3D scan datasets, providing raw ground truth data, have been released to the scientific community, along with a custom software library for processing 3D scans, generating partial data, and performing evaluation.  Comparative analysis of competition results against baselines validates the proposed metric and highlights the challenges posed by the task and datasets.  Further details on the SHARP 2020 challenge are available at https://cvi2.uni.lu/sharp2020/.\n"}, "2305.08844": {"original_text": "  Despite their unprecedented success, even the largest language models make\nmistakes. Similar to how humans learn and improve using feedback, previous work\nproposed providing language models with natural language feedback to guide them\nin repairing their outputs. Because human-generated critiques are expensive to\nobtain, researchers have devised learned critique generators in lieu of human\ncritics while assuming one can train downstream models to utilize generated\nfeedback. However, this approach does not apply to black-box or limited access\nmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\nlarge general-purpose language agents, fine-tuning is neither computationally\nnor spatially efficient as it results in multiple copies of the network. In\nthis work, we introduce RL4F (Reinforcement Learning for Feedback), a\nmulti-agent collaborative framework where the critique generator is trained to\nmaximize end-task performance of GPT-3, a fixed model more than 200 times its\nsize. RL4F produces critiques that help GPT-3 revise its outputs. We study\nthree datasets for action planning, summarization and alphabetization and show\nrelative improvements up to 10% in multiple text similarity metrics over other\nlearned, retrieval-augmented or prompting-based critique generators.\n", "rewritten_text": "Even the largest language models, despite their unprecedented success, are prone to errors.  Inspired by human learning through feedback, prior research proposed using natural language feedback to improve model outputs.  However, the high cost of human-generated critiques has led to the development of learned critique generators.  This approach, however, is inapplicable to black-box or restricted-access models like ChatGPT, which cannot be fine-tuned.  Furthermore, fine-tuning large, general-purpose language models is computationally and spatially inefficient due to the creation of multiple model copies.\n\nThis work introduces RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework.  RL4F trains a critique generator to maximize the end-task performance of GPT-3, a fixed model over 200 times larger than the generator.  The generated critiques guide GPT-3 to revise its outputs.  Evaluated on three datasets encompassing action planning, summarization, and alphabetization, RL4F demonstrates improvements of up to 10% in multiple text similarity metrics compared to other learned, retrieval-augmented, or prompting-based critique generators.\n"}, "2312.14697": {"original_text": "  Polarization information of the light can provide rich cues for computer\nvision and scene understanding tasks, such as the type of material, pose, and\nshape of the objects. With the advent of new and cheap polarimetric sensors,\nthis imaging modality is becoming accessible to a wider public for solving\nproblems such as pose estimation, 3D reconstruction, underwater navigation, and\ndepth estimation. However, we observe several limitations regarding the usage\nof this sensorial modality, as well as a lack of standards and publicly\navailable tools to analyze polarization images. Furthermore, although\npolarization camera manufacturers usually provide acquisition tools to\ninterface with their cameras, they rarely include processing algorithms that\nmake use of the polarization information. In this paper, we review recent\nadvances in applications that involve polarization imaging, including a\ncomprehensive survey of recent advances on polarization for vision and robotics\nperception tasks. We also introduce a complete software toolkit that provides\ncommon standards to communicate with and process information from most of the\nexisting micro-grid polarization cameras on the market. The toolkit also\nimplements several image processing algorithms for this modality, and it is\npublicly available on GitHub: https://github.com/vibot-lab/Pola4all_JEI_2023.\n", "rewritten_text": "Polarization imaging provides rich cues for computer vision and scene understanding, revealing information about material type, object pose, and shape.  The recent availability of affordable polarimetric sensors is democratizing this imaging modality, enabling applications such as pose estimation, 3D reconstruction, underwater navigation, and depth estimation.  However, current usage is hampered by several limitations, including a lack of standardization and publicly available analysis tools.  While polarization camera manufacturers typically provide acquisition software, they rarely offer processing algorithms that leverage the polarization information.\n\nThis paper reviews recent advances in polarization imaging applications, comprehensively surveying its use in vision and robotics perception.  We also introduce Pola4all, a publicly available software toolkit (https://github.com/vibot-lab/Pola4all_JEI_2023) that establishes common standards for interfacing with and processing data from most commercially available micro-grid polarization cameras.  The toolkit includes several implemented image processing algorithms for this modality.\n"}, "1809.04191": {"original_text": "  To realize the promise of ubiquitous embedded deep network inference, it is\nessential to seek limits of energy and area efficiency. To this end,\nlow-precision networks offer tremendous promise because both energy and area\nscale down quadratically with the reduction in precision. Here we demonstrate\nResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on\nthe ImageNet classification benchmark that, at 8-bit precision exceed the\naccuracy of the full-precision baseline networks after one epoch of finetuning,\nthereby leveraging the availability of pretrained models. We also demonstrate\nResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn 4-bit models that match\nthe accuracy of the full-precision baseline networks -- the highest scores to\ndate. Surprisingly, the weights of the low-precision networks are very close\n(in cosine similarity) to the weights of the corresponding baseline networks,\nmaking training from scratch unnecessary.\n  We find that gradient noise due to quantization during training increases\nwith reduced precision, and seek ways to overcome this noise. The number of\niterations required by SGD to achieve a given training error is related to the\nsquare of (a) the distance of the initial solution from the final plus (b) the\nmaximum variance of the gradient estimates. Therefore, we (a) reduce solution\ndistance by starting with pretrained fp32 precision baseline networks and\nfine-tuning, and (b) combat gradient noise introduced by quantization by\ntraining longer and reducing learning rates. Sensitivity analysis indicates\nthat these simple techniques, coupled with proper activation function range\ncalibration to take full advantage of the limited precision, are sufficient to\ndiscover low-precision networks, if they exist, close to fp32 precision\nbaseline networks. The results herein provide evidence that 4-bits suffice for\nclassification.\n", "rewritten_text": "Ubiquitous embedded deep network inference requires maximizing energy and area efficiency.  Low-precision networks offer significant potential, as energy and area consumption scale quadratically with precision reduction.  We demonstrate that 8-bit versions of ResNet-18, -34, -50, -152, Inception-v3, DenseNet-161, and VGG-16bn, fine-tuned for one epoch on the ImageNet classification benchmark, surpass the accuracy of their full-precision counterparts, leveraging pre-trained models.  Furthermore, we achieve state-of-the-art results with 4-bit versions of ResNet-18, -34, -50, -152, DenseNet-161, and VGG-16bn, matching the accuracy of their full-precision baselines.  Surprisingly, the weights of these low-precision networks exhibit high cosine similarity to their full-precision counterparts, eliminating the need for training from scratch.\n\nReduced precision increases gradient noise during training.  The number of stochastic gradient descent (SGD) iterations required to reach a given training error is proportional to the square of the sum of: (a) the distance between the initial and final solutions, and (b) the maximum variance of the gradient estimates.  To mitigate this, we employ two strategies: (a) fine-tuning pre-trained full-precision (FP32) networks to reduce the initial solution distance, and (b) extending training duration and reducing learning rates to counteract quantization noise.  Sensitivity analysis suggests that these simple techniques, combined with careful activation function range calibration to fully utilize the limited precision, are sufficient to identify high-performing low-precision networks, if they exist, closely approximating their FP32 counterparts.  Our findings strongly suggest that 4-bit precision is sufficient for image classification.\n"}, "2103.14799": {"original_text": "  Image representation is an important topic in computer vision and pattern\nrecognition. It plays a fundamental role in a range of applications towards\nunderstanding visual contents. Moment-based image representation has been\nreported to be effective in satisfying the core conditions of semantic\ndescription due to its beneficial mathematical properties, especially geometric\ninvariance and independence. This paper presents a comprehensive survey of the\northogonal moments for image representation, covering recent advances in\nfast/accurate calculation, robustness/invariance optimization, definition\nextension, and application. We also create a software package for a variety of\nwidely-used orthogonal moments and evaluate such methods in a same base. The\npresented theory analysis, software implementation, and evaluation results can\nsupport the community, particularly in developing novel techniques and\npromoting real-world applications.\n", "rewritten_text": "Image representation is crucial in computer vision and pattern recognition, underpinning numerous applications in visual content understanding.  Moment-based image representation, particularly orthogonal moments, has proven effective in semantic description due to its advantageous mathematical properties, including geometric invariance and independence. This paper provides a comprehensive survey of orthogonal moments for image representation, covering recent advancements in fast and accurate calculation, robustness and invariance optimization, extended definitions, and applications.  We also present a software package implementing a variety of widely used orthogonal moments, allowing for comparative evaluation on a common basis.  The theoretical analysis, software implementation, and evaluation results presented herein aim to support the research community in developing novel techniques and fostering real-world applications.\n"}, "2406.19297": {"original_text": "  Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we\ndemonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)\napproach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.\n", "rewritten_text": "Continual learning incrementally trains models on sequential tasks, aiming to learn new tasks while minimizing performance degradation on previously learned ones.  Existing approaches combining continual learning and visual question answering (VQA) neglect the impact of multimodal input on learning dynamics. This paper demonstrates that different modalities evolve at varying rates across a sequence of tasks, a phenomenon observed in both established encoder-only models and modern vision-and-language (V&L) architectures.  Motivated by this, we propose Modality-Aware Feature Distillation (MAFED), a method that outperforms existing baselines across various model scales in three multimodal continual learning scenarios.  Ablation studies further show that modality-aware distillation effectively complements experience replay.  Our findings highlight the crucial role of addressing modality-specific dynamics in mitigating catastrophic forgetting within multimodal continual learning.\n"}, "2410.15865": {"original_text": "  Grammatical features such as number and gender serve two central functions in\nhuman languages. While they encode salient semantic attributes like numerosity\nand animacy, they also offload sentence processing cost by predictably linking\nwords together via grammatical agreement. Grammars exhibit consistent\norganizational patterns across diverse languages, invariably rooted in a\nsemantic foundation, a widely confirmed but still theoretically unexplained\nphenomenon. To explain the basis of universal grammatical patterns, we unify\ntwo fundamental properties of grammar, semantic encoding and agreement-based\npredictability, into a single information-theoretic objective under cognitive\nconstraints. Our analyses reveal that grammatical organization provably\ninherits from perceptual attributes, but that grammars empirically prioritize\nfunctional goals, promoting efficient language processing over semantic\nencoding.\n", "rewritten_text": "Grammatical features like number and gender serve two crucial functions in human language.  They encode salient semantic attributes such as numerosity and animacy, and they also reduce sentence processing demands by predictably linking words through grammatical agreement.  While grammars across diverse languages exhibit consistent organizational patterns rooted in semantics\u2014a widely observed but theoretically unexplained phenomenon\u2014this paper offers a novel explanation. We propose that universal grammatical patterns arise from a unified information-theoretic objective, balancing semantic encoding and agreement-based predictability under cognitive constraints. Our analysis reveals that, while grammatical organization demonstrably inherits from perceptual attributes, grammars prioritize functional goals, optimizing for efficient language processing over purely semantic encoding.\n"}, "2306.05119": {"original_text": "  Factuality is important to dialogue summarization. Factual error correction\n(FEC) of model-generated summaries is one way to improve factuality. Current\nFEC evaluation that relies on factuality metrics is not reliable and detailed\nenough. To address this problem, we are the first to manually annotate a FEC\ndataset for dialogue summarization containing 4000 items and propose FERRANTI,\na fine-grained evaluation framework based on reference correction that\nautomatically evaluates the performance of FEC models on different error\ncategories. Using this evaluation framework, we conduct sufficient experiments\nwith FEC approaches under a variety of settings and find the best training\nmodes and significant differences in the performance of the existing approaches\non different factual error categories.\n", "rewritten_text": "Factuality is crucial for dialogue summarization.  Improving factuality often involves factual error correction (FEC) of model-generated summaries. However, current FEC evaluation, relying solely on factuality metrics, lacks reliability and detail.  To address this, we introduce the first manually annotated FEC dataset for dialogue summarization, comprising 4,000 items.  We also propose FERRANTI, a fine-grained evaluation framework based on reference corrections that automatically assesses FEC model performance across various error categories.  Extensive experiments using FERRANTI with diverse FEC approaches and settings reveal optimal training strategies and significant performance variations across different factual error types.\n"}, "2410.16190": {"original_text": "  This work explores how human judgement about salient regions of an image can\nbe introduced into deep convolutional neural network (DCNN) training.\nTraditionally, training of DCNNs is purely data-driven. This often results in\nlearning features of the data that are only coincidentally correlated with\nclass labels. Human saliency can guide network training using our proposed new\ncomponent of the loss function that ConveYs Brain Oversight to Raise\nGeneralization (CYBORG) and penalizes the model for using non-salient regions.\nThis mechanism produces DCNNs achieving higher accuracy and generalization\ncompared to using the same training data without human salience. Experimental\nresults demonstrate that CYBORG applies across multiple network architectures\nand problem domains (detection of synthetic faces, iris presentation attacks\nand anomalies in chest X-rays), while requiring significantly less data than\ntraining without human saliency guidance. Visualizations show that\nCYBORG-trained models' saliency is more consistent across independent training\nruns than traditionally-trained models, and also in better agreement with\nhumans. To lower the cost of collecting human annotations, we also explore\nusing deep learning to provide automated annotations. CYBORG training of CNNs\naddresses important issues such as reducing the appetite for large training\nsets, increasing interpretability, and reducing fragility by generalizing\nbetter to new types of data.\n", "rewritten_text": "This work investigates incorporating human judgment of image saliency into deep convolutional neural network (DCNN) training.  Traditional DCNN training is purely data-driven, often leading to the learning of features coincidentally correlated with class labels.  To address this, we propose a novel loss function component, ConveYs Brain Oversight to Raise Generalization (CYBORG), which penalizes the model for utilizing non-salient regions.  This mechanism yields DCNNs with improved accuracy and generalization compared to models trained on the same data without human saliency guidance.  Experimental results across diverse network architectures and problem domains\u2014including synthetic face detection, iris presentation attack detection, and chest X-ray anomaly detection\u2014demonstrate CYBORG's effectiveness while requiring significantly less training data.  Visualizations reveal that CYBORG-trained models exhibit more consistent saliency across independent training runs and better agreement with human judgments.  To mitigate the cost of human annotation, we further explore the use of deep learning for automated annotation.  In summary, CYBORG training addresses critical issues in DCNN training, including reducing the need for large datasets, enhancing interpretability, and improving robustness through better generalization to unseen data.\n"}, "2311.07611": {"original_text": "  In this study we intentionally introduce biases into large language model\nresponses in an attempt to create specific personas for interactive media\npurposes. We explore the differences between open source models such as\nFalcon-7b and the GPT-4 model from Open AI, and we quantify some differences in\nresponses afforded by the two systems. We find that the guardrails in the GPT-4\nmixture of experts models with a supervisor, while useful in assuring AI\nalignment in general, are detrimental in trying to construct personas with a\nvariety of uncommon viewpoints. This study aims to set the groundwork for\nfuture exploration in intentional biases of large language models such that\nthese practices can be applied in the creative field, and new forms of media.\n", "rewritten_text": "This study investigates the introduction of intentional biases into large language models (LLMs) to create specific personas for interactive media.  We compare the responses of open-source models, such as Falcon-7b, with those of OpenAI's GPT-4, quantifying key differences.  We find that the safety mechanisms\u2014the mixture-of-experts architecture with supervision\u2014employed in GPT-4, while beneficial for general AI alignment, hinder the creation of personas with diverse and unconventional viewpoints.  This work lays the groundwork for future research on the intentional biasing of LLMs for creative applications and novel media forms.\n"}, "2409.01522": {"original_text": "  Long-term motion generation is a challenging task that requires producing\ncoherent and realistic sequences over extended durations. Current methods\nprimarily rely on framewise motion representations, which capture only static\nspatial details and overlook temporal dynamics. This approach leads to\nsignificant redundancy across the temporal dimension, complicating the\ngeneration of effective long-term motion. To overcome these limitations, we\nintroduce the novel concept of Lagrangian Motion Fields, specifically designed\nfor long-term motion generation. By treating each joint as a Lagrangian\nparticle with uniform velocity over short intervals, our approach condenses\nmotion representations into a series of \"supermotions\" (analogous to\nsuperpixels). This method seamlessly integrates static spatial information with\ninterpretable temporal dynamics, transcending the limitations of existing\nnetwork architectures and motion sequence content types. Our solution is\nversatile and lightweight, eliminating the need for neural network\npreprocessing. Our approach excels in tasks such as long-term music-to-dance\ngeneration and text-to-motion generation, offering enhanced efficiency,\nsuperior generation quality, and greater diversity compared to existing\nmethods. Additionally, the adaptability of Lagrangian Motion Fields extends to\napplications like infinite motion looping and fine-grained controlled motion\ngeneration, highlighting its broad utility. Video demonstrations are available\nat \\url{https://plyfager.github.io/LaMoG}.\n", "rewritten_text": "Generating long-term motion is a challenging task requiring coherent and realistic sequences over extended durations.  Current methods primarily rely on frame-wise motion representations, which capture only static spatial details and neglect temporal dynamics. This leads to significant temporal redundancy, hindering effective long-term motion generation.  To address this, we introduce Lagrangian Motion Fields (LMFs), a novel approach specifically designed for long-term motion generation.  By modeling each joint as a Lagrangian particle with uniform velocity over short intervals, LMFs condense motion representations into a series of \"supermotions\" (analogous to superpixels). This seamlessly integrates static spatial information with interpretable temporal dynamics, overcoming limitations of existing network architectures and motion sequence types.  Our method is versatile and computationally efficient, eliminating the need for neural network preprocessing.  LMFs excel in tasks such as music-to-dance and text-to-motion generation, offering improved efficiency, higher-quality generation, and greater diversity than existing methods.  Furthermore, their adaptability extends to applications including infinite motion looping and fine-grained motion control.  Video demonstrations are available at [https://plyfager.github.io/LaMoG](https://plyfager.github.io/LaMoG).\n"}, "1704.06756": {"original_text": "  We have developed convolutional neural networks (CNN) for a facial expression\nrecognition task. The goal is to classify each facial image into one of the\nseven facial emotion categories considered in this study. We trained CNN models\nwith different depth using gray-scale images. We developed our models in Torch\nand exploited Graphics Processing Unit (GPU) computation in order to expedite\nthe training process. In addition to the networks performing based on raw pixel\ndata, we employed a hybrid feature strategy by which we trained a novel CNN\nmodel with the combination of raw pixel data and Histogram of Oriented\nGradients (HOG) features. To reduce the overfitting of the models, we utilized\ndifferent techniques including dropout and batch normalization in addition to\nL2 regularization. We applied cross validation to determine the optimal\nhyper-parameters and evaluated the performance of the developed models by\nlooking at their training histories. We also present the visualization of\ndifferent layers of a network to show what features of a face can be learned by\nCNN models.\n", "rewritten_text": "We developed convolutional neural networks (CNNs) for facial expression recognition, classifying facial images into seven emotion categories.  We trained CNN models of varying depths using grayscale images, leveraging Torch and GPU computation to accelerate training.  In addition to models using raw pixel data, we developed a hybrid approach incorporating Histogram of Oriented Gradients (HOG) features.  To mitigate overfitting, we employed dropout, batch normalization, and L2 regularization.  Hyperparameter optimization was achieved through cross-validation, and model performance was evaluated using training histories.  Finally, we visualized network layers to illustrate the facial features learned by the CNNs.\n"}, "2312.17292": {"original_text": "  Word embedding methods (WEMs) are extensively used for representing text\ndata. The dimensionality of these embeddings varies across various tasks and\nimplementations. The effect of dimensionality change on the accuracy of the\ndownstream task is a well-explored question. However, how the dimensionality\nchange affects the bias of word embeddings needs to be investigated. Using the\nEnglish Wikipedia corpus, we study this effect for two static (Word2Vec and\nfastText) and two context-sensitive (ElMo and BERT) WEMs. We have two\nobservations. First, there is a significant variation in the bias of word\nembeddings with the dimensionality change. Second, there is no uniformity in\nhow the dimensionality change affects the bias of word embeddings. These\nfactors should be considered while selecting the dimensionality of word\nembeddings.\n", "rewritten_text": "Word embedding methods (WEMs) are widely used to represent text data.  The dimensionality of these embeddings varies across different tasks and implementations. While the impact of dimensionality on downstream task accuracy is well-studied, the effect on embedding bias remains under-investigated.  This study examines this effect using the English Wikipedia corpus and four WEMs: two static methods (Word2Vec and fastText) and two context-sensitive methods (ELMo and BERT).  Our findings reveal two key observations: first, embedding bias varies significantly with dimensionality changes; second, this effect is not uniform across different WEMs.  Therefore, careful consideration of dimensionality is crucial when selecting word embeddings.\n"}, "2401.07745": {"original_text": "  Open-vocabulary 3D instance segmentation is cutting-edge for its ability to\nsegment 3D instances without predefined categories. However, progress in 3D\nlags behind its 2D counterpart due to limited annotated 3D data. To address\nthis, recent works first generate 2D open-vocabulary masks through 2D models\nand then merge them into 3D instances based on metrics calculated between two\nneighboring frames. In contrast to these local metrics, we propose a novel\nmetric, view consensus rate, to enhance the utilization of multi-view\nobservations. The key insight is that two 2D masks should be deemed part of the\nsame 3D instance if a significant number of other 2D masks from different views\ncontain both these two masks. Using this metric as edge weight, we construct a\nglobal mask graph where each mask is a node. Through iterative clustering of\nmasks showing high view consensus, we generate a series of clusters, each\nrepresenting a distinct 3D instance. Notably, our model is training-free.\nThrough extensive experiments on publicly available datasets, including\nScanNet++, ScanNet200 and MatterPort3D, we demonstrate that our method achieves\nstate-of-the-art performance in open-vocabulary 3D instance segmentation. Our\nproject page is at https://pku-epic.github.io/MaskClustering.\n", "rewritten_text": "Open-vocabulary 3D instance segmentation, capable of segmenting 3D instances without predefined categories, is a cutting-edge field.  However, progress in 3D lags behind its 2D counterpart due to the scarcity of annotated 3D data.  Existing methods often generate 2D open-vocabulary masks using 2D models and then merge them into 3D instances based on metrics computed between adjacent frames.  In contrast, we propose a novel metric, *view consensus rate*, to leverage multi-view observations more effectively.  Our key insight is that two 2D masks should be considered part of the same 3D instance if a substantial number of 2D masks from different viewpoints consistently include both.  Using this metric as edge weights, we construct a global mask graph, where each mask represents a node.  Iterative clustering of masks exhibiting high view consensus then yields a set of clusters, each representing a distinct 3D instance.  Importantly, our method is training-free.  Extensive experiments on publicly available datasets, including ScanNet++, ScanNet200, and MatterPort3D, demonstrate state-of-the-art performance in open-vocabulary 3D instance segmentation.  Our project page is available at https://pku-epic.github.io/MaskClustering.\n"}, "2404.05916": {"original_text": "  Echocardiography segmentation for cardiac analysis is time-consuming and\nresource-intensive due to the variability in image quality and the necessity to\nprocess scans from various standard views. While current automated segmentation\nmethods in echocardiography show promising performance, they are trained on\nspecific scan views to analyze corresponding data. However, this solution has a\nlimitation as the number of required models increases with the number of\nstandard views. To address this, in this paper, we present a prompt-driven\nuniversal method for view-agnostic echocardiography analysis. Considering the\ndomain shift between standard views, we first introduce a method called prompt\nmatching, aimed at learning prompts specific to different views by matching\nprompts and querying input embeddings using a pre-trained vision model. Then,\nwe utilized a pre-trained medical language model to align textual information\nwith pixel data for accurate segmentation. Extensive experiments on three\nstandard views showed that our approach significantly outperforms the\nstate-of-the-art universal methods and achieves comparable or even better\nperformances over the segmentation model trained and tested on same views.\n", "rewritten_text": "Echocardiography segmentation for cardiac analysis is time-consuming and resource-intensive due to variable image quality and the need to process scans from multiple standard views.  Although current automated segmentation methods show promise, they typically train separate models for each view, leading to a proliferation of models as the number of views increases.  To address this limitation, we present a novel prompt-driven, view-agnostic method for echocardiography analysis.  To mitigate the domain shift between views, we introduce a prompt matching technique that learns view-specific prompts by matching prompts and input embeddings from a pre-trained vision model.  Subsequently, a pre-trained medical language model aligns textual information with pixel data to enable accurate segmentation.  Extensive experiments across three standard views demonstrate that our approach significantly outperforms state-of-the-art universal methods and achieves comparable or superior performance to models trained and tested on individual views.\n"}, "1908.09884": {"original_text": "  We consider the problem of discovering novel object categories in an image\ncollection. While these images are unlabelled, we also assume prior knowledge\nof related but different image classes. We use such prior knowledge to reduce\nthe ambiguity of clustering, and improve the quality of the newly discovered\nclasses. Our contributions are twofold. The first contribution is to extend\nDeep Embedded Clustering to a transfer learning setting; we also improve the\nalgorithm by introducing a representation bottleneck, temporal ensembling, and\nconsistency. The second contribution is a method to estimate the number of\nclasses in the unlabelled data. This also transfers knowledge from the known\nclasses, using them as probes to diagnose different choices for the number of\nclasses in the unlabelled subset. We thoroughly evaluate our method,\nsubstantially outperforming state-of-the-art techniques in a large number of\nbenchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN.\n", "rewritten_text": "This paper addresses the problem of discovering novel object categories within an unlabeled image collection, leveraging prior knowledge of related, but distinct, image classes.  This prior knowledge mitigates clustering ambiguity and enhances the quality of the discovered categories.  Our contributions are twofold:  First, we extend Deep Embedded Clustering to a transfer learning framework, incorporating a representation bottleneck, temporal ensembling, and consistency regularization to improve algorithm performance. Second, we introduce a novel method for estimating the number of classes in the unlabeled data, transferring knowledge from known classes to guide the selection of the optimal number of clusters.  We demonstrate the effectiveness of our approach through extensive evaluation on several benchmark datasets, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN, achieving substantial improvements over state-of-the-art methods.\n"}, "2212.08334": {"original_text": "  Scene understanding has made tremendous progress over the past few years, as\ndata acquisition systems are now providing an increasing amount of data of\nvarious modalities (point cloud, depth, RGB...). However, this improvement\ncomes at a large cost on computation resources and data annotation\nrequirements. To analyze geometric information and images jointly, many\napproaches rely on both a 2D loss and 3D loss, requiring not only 2D per\npixel-labels but also 3D per-point labels. However, obtaining a 3D groundtruth\nis challenging, time-consuming and error-prone. In this paper, we show that\nimage segmentation can benefit from 3D geometric information without requiring\na 3D groundtruth, by training the geometric feature extraction and the 2D\nsegmentation network jointly, in an end-to-end fashion, using only the 2D\nsegmentation loss. Our method starts by extracting a map of 3D features\ndirectly from a provided point cloud by using a lightweight 3D neural network.\nThe 3D feature map, merged with the RGB image, is then used as an input to a\nclassical image segmentation network. Our method can be applied to many 2D\nsegmentation networks, improving significantly their performance with only a\nmarginal network weight increase and light input dataset requirements, since no\n3D groundtruth is required.\n", "rewritten_text": "Scene understanding has advanced significantly in recent years, driven by increasingly sophisticated data acquisition systems providing diverse modalities (point clouds, depth maps, RGB images, etc.).  However, this progress comes at the cost of substantial computational resources and demanding data annotation requirements.  Many approaches jointly analyze geometric and image information using both 2D and 3D loss functions, necessitating per-pixel 2D labels and per-point 3D labels.  Acquiring accurate 3D ground truth, however, is challenging, time-consuming, and prone to error.\n\nThis paper demonstrates that image segmentation can leverage 3D geometric information without requiring 3D ground truth.  We achieve this by jointly training a geometric feature extraction network and a 2D segmentation network in an end-to-end fashion, using only a 2D segmentation loss.  Our method begins by extracting a 3D feature map directly from a point cloud using a lightweight 3D neural network. This 3D feature map is then fused with the RGB image and fed into a standard image segmentation network.  Our approach is compatible with various 2D segmentation networks, significantly improving their performance with minimal increases in network parameters and data requirements, as 3D ground truth is not needed.\n"}, "2309.09379": {"original_text": "  Nowadays, photogrammetrically derived point clouds are widely used in many\ncivilian applications due to their low cost and flexibility in acquisition.\nTypically, photogrammetric point clouds are assessed through reference data\nsuch as LiDAR point clouds. However, when reference data are not available, the\nassessment of photogrammetric point clouds may be challenging. Since these\npoint clouds are algorithmically derived, their accuracies and precisions are\nhighly varying with the camera networks, scene complexity, and dense image\nmatching (DIM) algorithms, and there is no standard error metric to determine\nper-point errors. The theory of internal reliability of camera networks has\nbeen well studied through first-order error estimation of Bundle Adjustment\n(BA), which is used to understand the errors of 3D points assuming known\nmeasurement errors. However, the measurement errors of the DIM algorithms are\nintricate to an extent that every single point may have its error function\ndetermined by factors such as pixel intensity, texture entropy, and surface\nsmoothness. Despite the complexity, there exist a few common metrics that may\naid the process of estimating the posterior reliability of the derived points,\nespecially in a multi-view stereo (MVS) setup when redundancies are present. In\nthis paper, by using an aerial oblique photogrammetric block with LiDAR\nreference data, we analyze several internal matching metrics within a common\nMVS framework, including statistics in ray convergence, intersection angles,\nDIM energy, etc.\n", "rewritten_text": "Photogrammetric point clouds are widely used in civilian applications due to their low cost and acquisition flexibility.  Typically, their accuracy is assessed using reference data, such as LiDAR point clouds.  However, in the absence of reference data, assessment becomes challenging.  The accuracy and precision of algorithmically derived point clouds vary significantly depending on camera network geometry, scene complexity, and dense image matching (DIM) algorithms.  Furthermore, a standardized per-point error metric is lacking.  While the internal reliability of camera networks has been extensively studied using first-order error estimation of Bundle Adjustment (BA), assuming known measurement errors, the measurement errors inherent in DIM algorithms are complex.  Each point's error can be influenced by factors such as pixel intensity, texture entropy, and surface smoothness.  Despite this complexity, several common metrics can help estimate the posterior reliability of derived points, particularly in multi-view stereo (MVS) setups with redundant observations.  This paper analyzes several internal matching metrics within a common MVS framework using an aerial oblique photogrammetric block with LiDAR reference data.  These metrics include statistics on ray convergence, intersection angles, and DIM energy.\n"}, "2306.05061": {"original_text": "  Multi-task visual perception has a wide range of applications in scene\nunderstanding such as autonomous driving. In this work, we devise an efficient\nunified framework to solve multiple common perception tasks, including instance\nsegmentation, semantic segmentation, monocular 3D detection, and depth\nestimation. Simply sharing the same visual feature representations for these\ntasks impairs the performance of tasks, while independent task-specific feature\nextractors lead to parameter redundancy and latency. Thus, we design two\nfeature-merge branches to learn feature basis, which can be useful to, and thus\nshared by, multiple perception tasks. Then, each task takes the corresponding\nfeature basis as the input of the prediction task head to fulfill a specific\ntask. In particular, one feature merge branch is designed for instance-level\nrecognition the other for dense predictions. To enhance inter-branch\ncommunication, the instance branch passes pixel-wise spatial information of\neach instance to the dense branch using efficient dynamic convolution\nweighting. Moreover, a simple but effective dynamic routing mechanism is\nproposed to isolate task-specific features and leverage common properties among\ntasks. Our proposed framework, termed D2BNet, demonstrates a unique approach to\nparameter-efficient predictions for multi-task perception. In addition, as\ntasks benefit from co-training with each other, our solution achieves on par\nresults on partially labeled settings on nuScenes and outperforms previous\nworks for 3D detection and depth estimation on the Cityscapes dataset with full\nsupervision.\n", "rewritten_text": "This work presents D2BNet, an efficient unified framework for multi-task visual perception, addressing applications such as autonomous driving.  We tackle multiple common perception tasks\u2014instance segmentation, semantic segmentation, monocular 3D detection, and depth estimation\u2014within a single framework.  Simply sharing visual feature representations degrades performance, while independent task-specific feature extractors are inefficient due to parameter redundancy and increased latency.  To overcome this, D2BNet employs two feature-merge branches to learn shared feature bases: one for instance-level recognition and one for dense predictions.  Efficient dynamic convolution weighting allows the instance branch to communicate pixel-wise spatial information to the dense branch, enhancing inter-branch communication.  A dynamic routing mechanism further isolates task-specific features while leveraging shared properties across tasks.  D2BNet achieves comparable performance to state-of-the-art methods on partially labeled nuScenes data and surpasses existing approaches on fully supervised Cityscapes data, particularly for 3D detection and depth estimation.  This demonstrates the effectiveness of D2BNet's parameter-efficient approach to multi-task visual perception, leveraging the benefits of co-training among tasks.\n"}, "2107.00977": {"original_text": "  Cardiac ultrasound imaging is used to diagnose various heart diseases. Common\nanalysis pipelines involve manual processing of the video frames by expert\nclinicians. This suffers from intra- and inter-observer variability. We propose\na novel approach to ultrasound video analysis using a transformer architecture\nbased on a Residual Auto-Encoder Network and a BERT model adapted for token\nclassification. This enables videos of any length to be processed. We apply our\nmodel to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection\nand the automated computation of the left ventricular ejection fraction. We\nachieve an average frame distance of 3.36 frames for the ES and 7.17 frames for\nthe ED on videos of arbitrary length. Our end-to-end learnable approach can\nestimate the ejection fraction with a MAE of 5.95 and $R^2$ of 0.52 in 0.15s\nper video, showing that segmentation is not the only way to predict ejection\nfraction. Code and models are available at https://github.com/HReynaud/UVT.\n", "rewritten_text": "Cardiac ultrasound imaging is commonly used to diagnose heart disease, but current analysis pipelines rely on manual video frame processing by expert clinicians, leading to intra- and inter-observer variability.  We propose a novel approach to ultrasound video analysis using a transformer architecture incorporating a residual autoencoder network and a BERT model adapted for token classification. This allows for the processing of videos of any length.  We applied our model to detect end-systolic (ES) and end-diastolic (ED) frames and automatically compute left ventricular ejection fraction (LVEF).  On videos of arbitrary length, our model achieved a mean frame distance of 3.36 for ES and 7.17 for ED frames.  Our end-to-end trainable approach estimated LVEF with a mean absolute error (MAE) of 5.95 and an R\u00b2 of 0.52, processing each video in 0.15 seconds. This demonstrates that accurate LVEF prediction is achievable without explicit segmentation. Code and pre-trained models are available at https://github.com/HReynaud/UVT.\n"}, "1710.05994": {"original_text": "  Recent advancements in neutron and X-ray sources, instrumentation and data\ncollection modes have significantly increased the experimental data size (which\ncould easily contain 10$^{8}$ -- 10$^{10}$ data points), so that conventional\nvolumetric visualization approaches become inefficient for both still imaging\nand interactive OpenGL rendition in a 3D setting. We introduce a new approach\nbased on the unsupervised machine learning algorithm, Density-Based Spatial\nClustering of Applications with Noise (DBSCAN), to efficiently analyze and\nvisualize large volumetric datasets. Here we present two examples of analyzing\nand visualizing datasets from the diffuse scattering experiment of a single\ncrystal sample and the tomographic reconstruction of a neutron scanning of a\nturbine blade. We found that by using the intensity as the weighting factor in\nthe clustering process, DBSCAN becomes very effective in de-noising and\nfeature/boundary detection, and thus enables better visualization of the\nhierarchical internal structures of the neutron scattering data.\n", "rewritten_text": "Recent advancements in neutron and X-ray sources, instrumentation, and data collection techniques have dramatically increased experimental dataset sizes (reaching 10<sup>8</sup>\u201310<sup>10</sup> data points).  This renders conventional volumetric visualization methods inefficient for both static imaging and interactive 3D rendering using OpenGL.  We present a novel approach leveraging the unsupervised machine learning algorithm, Density-Based Spatial Clustering of Applications with Noise (DBSCAN), for efficient analysis and visualization of large volumetric datasets.  We demonstrate this approach with two examples: analysis and visualization of data from a single-crystal diffuse scattering experiment and from neutron tomography of a turbine blade.  Using intensity as a weighting factor in the clustering process, DBSCAN proves highly effective in noise reduction and feature/boundary detection, facilitating improved visualization of the hierarchical internal structures within the neutron scattering data.\n"}, "1812.0081": {"original_text": "  It is well known that the generative adversarial nets (GANs) are remarkably\ndifficult to train. The recently proposed Wasserstein GAN (WGAN) creates\nprincipled research directions towards addressing these issues. But we found in\npractice that gradient penalty WGANs (GP-WGANs) still suffer from training\ninstability. In this paper, we combine a Total Variational (TV) regularizing\nterm into the WGAN formulation instead of weight clipping or gradient penalty,\nwhich implies that the Lipschitz constraint is enforced on the critic network.\nOur proposed method is more stable at training than GP-WGANs and works well\nacross varied GAN architectures. We also present a method to control the\ntrade-off between image diversity and visual quality. It does not bring any\ncomputation burden.\n", "rewritten_text": "Generative adversarial networks (GANs) are notoriously difficult to train.  While the Wasserstein GAN (WGAN) offered a principled approach to addressing these challenges, gradient penalty WGANs (GP-WGANs) remain susceptible to training instability.  This paper proposes a novel approach that incorporates a total variation (TV) regularization term into the WGAN formulation, replacing weight clipping and gradient penalties to enforce the Lipschitz constraint on the critic network.  Our method exhibits improved training stability compared to GP-WGANs and demonstrates effectiveness across diverse GAN architectures.  Furthermore, we introduce a mechanism to control the balance between image diversity and visual quality without incurring additional computational overhead.\n"}, "1709.06247": {"original_text": "  Most of convolutional neural networks share the same characteristic: each\nconvolutional layer is followed by a nonlinear activation layer where Rectified\nLinear Unit (ReLU) is the most widely used. In this paper, we argue that the\ndesigned structure with the equal ratio between these two layers may not be the\nbest choice since it could result in the poor generalization ability. Thus, we\ntry to investigate a more suitable method on using ReLU to explore the better\nnetwork architectures. Specifically, we propose a proportional module to keep\nthe ratio between convolution and ReLU amount to be N:M (N>M). The proportional\nmodule can be applied in almost all networks with no extra computational cost\nto improve the performance. Comprehensive experimental results indicate that\nthe proposed method achieves better performance on different benchmarks with\ndifferent network architectures, thus verify the superiority of our work.\n", "rewritten_text": "Many convolutional neural networks (CNNs) share a common characteristic: each convolutional layer is followed by a nonlinear activation layer, most commonly employing the Rectified Linear Unit (ReLU).  This paper argues that the conventional 1:1 ratio between convolutional and ReLU layers may not be optimal, potentially hindering generalization.  We therefore investigate alternative ReLU application strategies to improve network architectures.  Specifically, we propose a proportional module that maintains a configurable N:M ratio (N > M) of convolutional layers to ReLU layers. This module integrates seamlessly into most CNNs without incurring additional computational overhead, enhancing performance.  Extensive experimental results across diverse benchmarks and network architectures demonstrate the superior performance of our proposed method, validating its effectiveness.\n"}, "2201.06674": {"original_text": "  Providing feedback on the argumentation of the learner is essential for\ndeveloping critical thinking skills, however, it requires a lot of time and\neffort. To mitigate the overload on teachers, we aim to automate a process of\nproviding feedback, especially giving diagnostic comments which point out the\nweaknesses inherent in the argumentation. It is recommended to give specific\ndiagnostic comments so that learners can recognize the diagnosis without\nmisinterpretation. However, it is not obvious how the task of providing\nspecific diagnostic comments should be formulated. We present a formulation of\nthe task as template selection and slot filling to make an automatic evaluation\neasier and the behavior of the model more tractable. The key to the formulation\nis the possibility of creating a template set that is sufficient for practical\nuse. In this paper, we define three criteria that a template set should\nsatisfy: expressiveness, informativeness, and uniqueness, and verify the\nfeasibility of creating a template set that satisfies these criteria as a first\ntrial. We will show that it is feasible through an annotation study that\nconverts diagnostic comments given in a text to a template format. The corpus\nused in the annotation study is publicly available.\n", "rewritten_text": "Providing feedback on learners' argumentation is crucial for developing critical thinking skills, but it is also time-consuming. To alleviate the burden on educators, we propose automating the feedback process, particularly the generation of diagnostic comments identifying weaknesses in arguments.  Effective diagnostic comments should be specific and unambiguous. However, automating the generation of such comments presents a significant challenge.  We address this by formulating the task as template selection and slot filling, facilitating automated evaluation and model tractability.  The feasibility of this approach hinges on creating a practically sufficient template set.  This paper defines three criteria for such a set\u2014expressiveness, informativeness, and uniqueness\u2014and then empirically assesses the feasibility of creating a set that meets these criteria.  A pilot annotation study, converting existing diagnostic comments into our template format, demonstrates feasibility.  The corpus used in this study is publicly available.\n"}, "2008.01167": {"original_text": "  Current anchor-free object detectors label all the features that spatially\nfall inside a predefined central region of a ground-truth box as positive. This\napproach causes label noise during training, since some of these positively\nlabeled features may be on the background or an occluder object, or they are\nsimply not discriminative features. In this paper, we propose a new labeling\nstrategy aimed to reduce the label noise in anchor-free detectors. We sum-pool\npredictions stemming from individual features into a single prediction. This\nallows the model to reduce the contributions of non-discriminatory features\nduring training. We develop a new one-stage, anchor-free object detector,\nPPDet, to employ this labeling strategy during training and a similar\nprediction pooling method during inference. On the COCO dataset, PPDet achieves\nthe best performance among anchor-free top-down detectors and performs on-par\nwith the other state-of-the-art methods. It also outperforms all major\none-stage and two-stage methods in small object detection (${AP}_{S}$ $31.4$).\nCode is available at https://github.com/nerminsamet/ppdet\n", "rewritten_text": "Existing anchor-free object detectors designate all features within a predefined central region of a ground-truth bounding box as positive.  This approach introduces label noise during training, as some positively labeled features may belong to the background, an occluding object, or be non-discriminative.  This paper presents a novel labeling strategy to mitigate this label noise.  We employ sum-pooling to aggregate predictions from individual features into a single prediction, thereby reducing the influence of non-discriminative features during training.  We introduce PPDet, a new one-stage, anchor-free object detector that utilizes this labeling strategy during training and a similar prediction pooling method during inference.  On the COCO dataset, PPDet achieves state-of-the-art performance among anchor-free top-down detectors and comparable results to other leading methods.  Furthermore, it surpasses all major one-stage and two-stage detectors in small object detection (AP<sub>S</sub> = 31.4). Code is available at https://github.com/nerminsamet/ppdet.\n"}, "1807.10657": {"original_text": "  Saliency map estimation in computer vision aims to estimate the locations\nwhere people gaze in images. Since people tend to look at objects in images,\nthe parameters of the model pretrained on ImageNet for image classification are\nuseful for the saliency map estimation. However, there is no research on the\nrelationship between the image classification accuracy and the performance of\nthe saliency map estimation. In this paper, it is shown that there is a strong\ncorrelation between image classification accuracy and saliency map estimation\naccuracy. We also investigated the effective architecture based on multi scale\nimages and the upsampling layers to refine the saliency-map resolution. Our\nmodel achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003\ndatasets. In the MIT Saliency Benchmark, our model achieved the best\nperformance in some metrics and competitive results in the other metrics.\n", "rewritten_text": "Saliency map estimation in computer vision aims to predict human gaze locations within images.  Because humans tend to fixate on objects, parameters from ImageNet-pretrained image classification models are often beneficial. However, the relationship between image classification accuracy and saliency map estimation performance remains unexplored. This paper demonstrates a strong correlation between these two accuracies.  Furthermore, we investigated effective architectures incorporating multi-scale images and upsampling layers to refine saliency map resolution. Our model achieved state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003 datasets, and exhibited best-in-class performance on several metrics and competitive results on others within the MIT Saliency Benchmark.\n"}, "1812.05642": {"original_text": "  Unsupervised learning for geometric perception (depth, optical flow, etc.) is\nof great interest to autonomous systems. Recent works on unsupervised learning\nhave made considerable progress on perceiving geometry; however, they usually\nignore the coherence of objects and perform poorly under scenarios with dark\nand noisy environments. In contrast, supervised learning algorithms, which are\nrobust, require large labeled geometric dataset. This paper introduces SIGNet,\na novel framework that provides robust geometry perception without requiring\ngeometrically informative labels. Specifically, SIGNet integrates semantic\ninformation to make depth and flow predictions consistent with objects and\nrobust to low lighting conditions. SIGNet is shown to improve upon the\nstate-of-the-art unsupervised learning for depth prediction by 30% (in squared\nrelative error). In particular, SIGNet improves the dynamic object class\nperformance by 39% in depth prediction and 29% in flow prediction. Our code\nwill be made available at https://github.com/mengyuest/SIGNet\n", "rewritten_text": "Unsupervised learning for geometric perception (e.g., depth and optical flow) is crucial for autonomous systems.  While recent unsupervised learning methods have advanced geometric perception, they often neglect object coherence and perform poorly in dark or noisy environments.  Conversely, robust supervised learning algorithms require extensive labeled geometric datasets.  This paper introduces SIGNet, a novel framework for robust geometric perception without requiring geometrically informative labels.  Specifically, SIGNet integrates semantic information to ensure depth and flow predictions are both object-consistent and robust to low-light conditions.  Results demonstrate that SIGNet improves state-of-the-art unsupervised depth prediction by 30% (in terms of squared relative error).  Furthermore, SIGNet achieves a 39% improvement in depth prediction and a 29% improvement in flow prediction for dynamic objects.  Our code is available at https://github.com/mengyuest/SIGNet.\n"}, "2212.09098": {"original_text": "  Fine-grained semantic segmentation of a person's face and head, including\nfacial parts and head components, has progressed a great deal in recent years.\nHowever, it remains a challenging task, whereby considering ambiguous\nocclusions and large pose variations are particularly difficult. To overcome\nthese difficulties, we propose a novel framework termed Mask-FPAN. It uses a\nde-occlusion module that learns to parse occluded faces in a semi-supervised\nway. In particular, face landmark localization, face occlusionstimations, and\ndetected head poses are taken into account. A 3D morphable face model combined\nwith the UV GAN improves the robustness of 2D face parsing. In addition, we\nintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for face\nparing work. The proposed Mask-FPAN framework addresses the face parsing\nproblem in the wild and shows significant performance improvements with MIOU\nfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging face\ndatasets.\n", "rewritten_text": "Recent years have witnessed significant advancements in fine-grained semantic segmentation of human faces and heads, including facial features and head components.  However, this remains a challenging task, particularly when addressing ambiguous occlusions and large pose variations.  To mitigate these challenges, we propose Mask-FPAN, a novel framework incorporating a de-occlusion module that learns to parse occluded faces semi-supervisedly.  This module leverages face landmark localization, occlusion estimations, and detected head poses.  Furthermore, a 3D morphable face model combined with a UV-GAN enhances the robustness of 2D face parsing.  We also introduce two new datasets, FaceOccMask-HQ and CelebAMaskOcc-HQ, to facilitate face parsing research.  Mask-FPAN effectively addresses the problem of face parsing in unconstrained environments, achieving a significant performance improvement, increasing the mean Intersection over Union (mIoU) from 0.7353 to 0.9013 compared to state-of-the-art methods on challenging face datasets.\n"}, "2206.08347": {"original_text": "  By leveraging contrastive learning, clustering, and other pretext tasks,\nunsupervised methods for learning image representations have reached impressive\nresults on standard benchmarks. The result has been a crowded field - many\nmethods with substantially different implementations yield results that seem\nnearly identical on popular benchmarks, such as linear evaluation on ImageNet.\nHowever, a single result does not tell the whole story. In this paper, we\ncompare methods using performance-based benchmarks such as linear evaluation,\nnearest neighbor classification, and clustering for several different datasets,\ndemonstrating the lack of a clear front-runner within the current\nstate-of-the-art. In contrast to prior work that performs only supervised vs.\nunsupervised comparison, we compare several different unsupervised methods\nagainst each other. To enrich this comparison, we analyze embeddings with\nmeasurements such as uniformity, tolerance, and centered kernel alignment\n(CKA), and propose two new metrics of our own: nearest neighbor graph\nsimilarity and linear prediction overlap. We reveal through our analysis that\nin isolation, single popular methods should not be treated as though they\nrepresent the field as a whole, and that future work ought to consider how to\nleverage the complimentary nature of these methods. We also leverage CKA to\nprovide a framework to robustly quantify augmentation invariance, and provide a\nreminder that certain types of invariance will be undesirable for downstream\ntasks.\n", "rewritten_text": "Unsupervised image representation learning, leveraging contrastive learning, clustering, and other pretext tasks, has achieved impressive results on standard benchmarks.  However, this success has led to a crowded field, with numerous methods exhibiting nearly identical performance\u2014for example, on ImageNet linear evaluation\u2014despite substantially different implementations.  A single benchmark score, therefore, provides an incomplete picture.\n\nThis paper presents a comprehensive comparison of state-of-the-art unsupervised methods across multiple datasets and benchmarks, including linear evaluation, nearest neighbor classification, and clustering.  Unlike previous work focusing solely on supervised versus unsupervised comparisons, we directly compare various unsupervised techniques.  To enrich this analysis, we employ established metrics such as uniformity, tolerance, and centered kernel alignment (CKA), and introduce two novel metrics: nearest neighbor graph similarity and linear prediction overlap.\n\nOur findings reveal that no single method consistently outperforms others, challenging the notion that any one approach represents the field as a whole.  Future research should explore the complementary strengths of these methods.  Furthermore, we utilize CKA to robustly quantify augmentation invariance, highlighting the potential drawbacks of certain invariances for downstream tasks.\n"}, "2108.08432": {"original_text": "  Deep learning has achieved remarkable success in medicalimage segmentation,\nbut it usually requires a large numberof images labeled with fine-grained\nsegmentation masks, andthe annotation of these masks can be very expensive\nandtime-consuming. Therefore, recent methods try to use un-supervised domain\nadaptation (UDA) methods to borrow in-formation from labeled data from other\ndatasets (source do-mains) to a new dataset (target domain). However, due tothe\nabsence of labels in the target domain, the performance ofUDA methods is much\nworse than that of the fully supervisedmethod. In this paper, we propose a\nweakly supervised do-main adaptation setting, in which we can partially label\nnewdatasets with bounding boxes, which are easier and cheaperto obtain than\nsegmentation masks. Accordingly, we proposea new weakly-supervised domain\nadaptation method calledBox-Adapt, which fully explores the fine-grained\nsegmenta-tion mask in the source domain and the weak bounding boxin the target\ndomain. Our Box-Adapt is a two-stage methodthat first performs joint training\non the source and target do-mains, and then conducts self-training with the\npseudo-labelsof the target domain. We demonstrate the effectiveness of\nourmethod in the liver segmentation task. Weakly supervised do-main adaptation\n", "rewritten_text": "Deep learning has achieved remarkable success in medical image segmentation, but typically requires a large number of images with fine-grained segmentation masks.  Annotating these masks is expensive and time-consuming.  Consequently, recent methods employ unsupervised domain adaptation (UDA) to leverage labeled data from source domains to improve performance on a new, unlabeled target domain. However, the absence of target domain labels significantly hinders UDA performance compared to fully supervised methods.\n\nThis paper proposes a weakly supervised domain adaptation setting, utilizing partially labeled datasets with bounding boxes\u2014which are easier and cheaper to obtain than segmentation masks.  We introduce Box-Adapt, a novel weakly supervised domain adaptation method that leverages fine-grained segmentation masks from the source domain and weak bounding box annotations from the target domain.  Box-Adapt is a two-stage method:  it first performs joint training on the source and target domains, then conducts self-training using pseudo-labels generated from the target domain.  We demonstrate the effectiveness of our method on a liver segmentation task.\n"}, "2203.02231": {"original_text": "  Light field disparity estimation is an essential task in computer vision with\nvarious applications. Although supervised learning-based methods have achieved\nboth higher accuracy and efficiency than traditional optimization-based\nmethods, the dependency on ground-truth disparity for training limits the\noverall generalization performance not to say for real-world scenarios where\nthe ground-truth disparity is hard to capture. In this paper, we argue that\nunsupervised methods can achieve comparable accuracy, but, more importantly,\nmuch higher generalization capacity and efficiency than supervised methods.\nSpecifically, we present the Occlusion Pattern Aware Loss, named OPAL, which\nsuccessfully extracts and encodes the general occlusion patterns inherent in\nthe light field for loss calculation. OPAL enables: i) accurate and robust\nestimation by effectively handling occlusions without using any ground-truth\ninformation for training and ii) much efficient performance by significantly\nreducing the network parameters required for accurate inference. Besides, a\ntransformer-based network and a refinement module are proposed for achieving\neven more accurate results. Extensive experiments demonstrate our method not\nonly significantly improves the accuracy compared with the SOTA unsupervised\nmethods, but also possesses strong generalization capacity, even for real-world\ndata, compared with supervised methods. Our code will be made publicly\navailable.\n", "rewritten_text": "Light field disparity estimation is a crucial task in computer vision with numerous applications. While supervised learning methods have surpassed traditional optimization-based approaches in both accuracy and efficiency, their reliance on ground-truth disparity for training hinders generalization, particularly in real-world scenarios where obtaining ground truth is challenging.  This paper demonstrates that unsupervised methods can achieve comparable accuracy and, significantly, superior generalization and efficiency.  We introduce Occlusion Pattern Aware Loss (OPAL), which effectively extracts and encodes inherent light field occlusion patterns for loss calculation. OPAL enables: (i) accurate and robust disparity estimation by effectively handling occlusions without ground-truth training data, and (ii) improved efficiency through a significant reduction in network parameters.  Furthermore, we propose a transformer-based network and a refinement module to enhance accuracy.  Extensive experiments show that our method significantly improves accuracy over state-of-the-art (SOTA) unsupervised methods and exhibits strong generalization capabilities, even on real-world data, surpassing supervised methods.  Our code will be publicly available.\n"}, "2006.1038": {"original_text": "  Video semantic segmentation is active in recent years benefited from the\ngreat progress of image semantic segmentation. For such a task, the per-frame\nimage segmentation is generally unacceptable in practice due to high\ncomputation cost. To tackle this issue, many works use the flow-based feature\npropagation to reuse the features of previous frames. However, the optical flow\nestimation inevitably suffers inaccuracy and then causes the propagated\nfeatures distorted. In this paper, we propose distortion-aware feature\ncorrection to alleviate the issue, which improves video segmentation\nperformance by correcting distorted propagated features. To be specific, we\nfirstly propose to transfer distortion patterns from feature into image space\nand conduct effective distortion map prediction. Benefited from the guidance of\ndistortion maps, we proposed Feature Correction Module (FCM) to rectify\npropagated features in the distorted areas. Our proposed method can\nsignificantly boost the accuracy of video semantic segmentation at a low price.\nThe extensive experimental results on Cityscapes and CamVid show that our\nmethod outperforms the recent state-of-the-art methods.\n", "rewritten_text": "Video semantic segmentation has seen significant advancements in recent years, largely driven by progress in image semantic segmentation.  However, per-frame image segmentation is computationally expensive and impractical for video.  To address this, many methods employ flow-based feature propagation to reuse features from previous frames.  Unfortunately, inaccuracies in optical flow estimation lead to distorted propagated features.  This paper proposes distortion-aware feature correction to mitigate this problem, improving video segmentation performance by rectifying these distortions.  Specifically, we introduce a novel approach to transfer distortion patterns from the feature space to the image space, enabling effective distortion map prediction.  Guided by these distortion maps, our proposed Feature Correction Module (FCM) refines propagated features in distorted regions.  Our method significantly improves the accuracy of video semantic segmentation with minimal computational overhead.  Extensive experiments on the Cityscapes and CamVid datasets demonstrate that our approach outperforms state-of-the-art methods.\n"}, "2101.11251": {"original_text": "  Junctions reflect the important geometrical structure information of the\nimage, and are of primary significance to applications such as image matching\nand motion analysis. Previous event-based feature extraction methods are mainly\nfocused on corners, which mainly find their locations, however, ignoring the\ngeometrical structure information like orientations and scales of edges. This\npaper adapts the frame-based a-contrario junction detector(ACJ) to event data,\nproposing the event-based a-contrario junction detector(e-ACJ), which yields\njunctions' locations while giving the scales and orientations of their\nbranches. The proposed method relies on an a-contrario model and can operate on\nasynchronous events directly without generating synthesized event frames. We\nevaluate the performance on public event datasets. The result shows our method\nsuccessfully finds the orientations and scales of branches, while maintaining\nhigh accuracy in junction's location.\n", "rewritten_text": "Junctions are crucial geometric features in images, essential for applications such as image matching and motion analysis.  Existing event-based feature extraction methods primarily focus on corner detection, neglecting crucial geometric information like edge orientations and scales. This paper adapts the frame-based *a contrario* junction detector (ACJ) to event data, proposing an event-based *a contrario* junction detector (e-ACJ).  e-ACJ identifies junction locations while simultaneously providing the scale and orientation of their constituent branches.  This method leverages an *a contrario* model and processes asynchronous events directly, eliminating the need for synthetic event frame generation.  Evaluation on public event datasets demonstrates that our method accurately identifies junction locations, while also successfully extracting branch orientations and scales.\n"}, "2210.16239": {"original_text": "  The Hyperspectral image (HSI) contains several hundred bands of the same\nregion called the Ground Truth (GT). The bands are taken in juxtaposed\nfrequencies, but some of them are noisily measured or contain no information.\nFor the classification, the selection of bands, affects significantly the\nresults of classification, in fact, using a subset of relevant bands, these\nresults can be better than those obtained using all bands, from which the need\nto reduce the dimensionality of the HSI. In this paper, a categorization of\ndimensionality reduction methods, according to the generation process, is\npresented. Furthermore, we reproduce an algorithm based on mutual information\n(MI) to reduce dimensionality by features selection and we introduce an\nalgorithm using mutual information and homogeneity. The two schemas are a\nfilter strategy. Finally, to validate this, we consider the case study AVIRIS\nHSI 92AV3C.\n  Keywords: Hyperspectrale images; classification; features selection; mutual\ninformation; homogeneity\n", "rewritten_text": "Hyperspectral images (HSI) comprise hundreds of bands capturing the same ground truth (GT) region.  While these bands are acquired at juxtaposed frequencies, some may be noisy or contain no useful information.  Band selection significantly impacts classification accuracy; using a relevant subset often yields superior results compared to using all bands, thus necessitating dimensionality reduction. This paper presents a categorization of dimensionality reduction methods based on their generation process.  We reproduce a mutual information (MI)-based feature selection algorithm and introduce a novel algorithm incorporating both mutual information and homogeneity. Both approaches employ a filter strategy.  The AVIRIS HSI 92AV3C dataset is used for validation.\n\nKeywords: Hyperspectral images; classification; feature selection; mutual information; homogeneity\n"}, "2108.07127": {"original_text": "  We translate a closed text that is known in advance and available in many\nlanguages into a new and severely low resource language. Most human translation\nefforts adopt a portion-based approach to translate consecutive pages/chapters\nin order, which may not suit machine translation. We compare the portion-based\napproach that optimizes coherence of the text locally with the random sampling\napproach that increases coverage of the text globally. Our results show that\nthe random sampling approach performs better. When training on a seed corpus of\n~1,000 lines from the Bible and testing on the rest of the Bible (~30,000\nlines), random sampling gives a performance gain of +11.0 BLEU using English as\na simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a\nMayan language. Furthermore, we compare three ways of updating machine\ntranslation models with increasing amount of human post-edited data through\niterations. We find that adding newly post-edited data to training after\nvocabulary update without self-supervision performs the best. We propose an\nalgorithm for human and machine to work together seamlessly to translate a\nclosed text into a severely low resource language.\n", "rewritten_text": "We translated a known, multilingual closed-text corpus into a severely low-resource language.  While most human translation employs a sequential, portion-based approach (translating consecutive pages or chapters), this may be suboptimal for machine translation (MT).  We compared a portion-based approach, which prioritizes local text coherence, with a random sampling approach, which prioritizes global text coverage.  Our results demonstrate that random sampling yields superior performance.  Training on a seed corpus of approximately 1,000 lines from the Bible and testing on the remaining ~30,000 lines, random sampling achieved a +11.0 BLEU score improvement using English as a simulated low-resource language and a +4.9 BLEU improvement using Eastern Pokomchi, a Mayan language.\n\nFurthermore, we evaluated three methods for iteratively updating MT models with increasing amounts of human post-edited data.  We found that adding newly post-edited data to the training data *after* vocabulary updates, without self-supervision, produced the best results.  We propose a novel algorithm for seamless human-machine collaboration in translating closed texts into severely low-resource languages.\n"}, "2103.10978": {"original_text": "  This paper addresses the problem of 3D human body shape and pose estimation\nfrom RGB images. Recent progress in this field has focused on single images,\nvideo or multi-view images as inputs. In contrast, we propose a new task: shape\nand pose estimation from a group of multiple images of a human subject, without\nconstraints on subject pose, camera viewpoint or background conditions between\nimages in the group. Our solution to this task predicts distributions over SMPL\nbody shape and pose parameters conditioned on the input images in the group. We\nprobabilistically combine predicted body shape distributions from each image to\nobtain a final multi-image shape prediction. We show that the additional body\nshape information present in multi-image input groups improves 3D human shape\nestimation metrics compared to single-image inputs on the SSP-3D dataset and a\nprivate dataset of tape-measured humans. In addition, predicting distributions\nover 3D bodies allows us to quantify pose prediction uncertainty, which is\nuseful when faced with challenging input images with significant occlusion. Our\nmethod demonstrates meaningful pose uncertainty on the 3DPW dataset and is\ncompetitive with the state-of-the-art in terms of pose estimation metrics.\n", "rewritten_text": "This paper addresses 3D human body shape and pose estimation from RGB images.  While recent progress has focused on single images, video, or multi-view images, we introduce a novel task: estimating shape and pose from an unconstrained group of multiple images of a single subject.  This group may contain images with varying poses, viewpoints, and background conditions.  Our approach predicts distributions over SMPL body shape and pose parameters conditioned on the input image group.  We probabilistically fuse the predicted shape distributions from each image to generate a final multi-image shape prediction.  Experiments on the SSP-3D dataset and a private dataset of anthropometrically measured subjects demonstrate that this multi-image approach improves 3D human shape estimation metrics compared to single-image methods.  Furthermore, by predicting distributions over 3D body shapes, we quantify pose prediction uncertainty, a valuable feature when dealing with challenging, occluded images.  Our method exhibits meaningful pose uncertainty quantification on the 3DPW dataset and achieves state-of-the-art performance in pose estimation metrics.\n"}, "2307.01447": {"original_text": "  Accurately matching local features between a pair of images is a challenging\ncomputer vision task. Previous studies typically use attention based graph\nneural networks (GNNs) with fully-connected graphs over keypoints within/across\nimages for visual and geometric information reasoning. However, in the context\nof feature matching, considerable keypoints are non-repeatable due to occlusion\nand failure of the detector, and thus irrelevant for message passing. The\nconnectivity with non-repeatable keypoints not only introduces redundancy,\nresulting in limited efficiency, but also interferes with the representation\naggregation process, leading to limited accuracy. Targeting towards high\naccuracy and efficiency, we propose MaKeGNN, a sparse attention-based GNN\narchitecture which bypasses non-repeatable keypoints and leverages matchable\nones to guide compact and meaningful message passing. More specifically, our\nBilateral Context-Aware Sampling Module first dynamically samples two small\nsets of well-distributed keypoints with high matchability scores from the image\npair. Then, our Matchable Keypoint-Assisted Context Aggregation Module regards\nsampled informative keypoints as message bottlenecks and thus constrains each\nkeypoint only to retrieve favorable contextual information from intra- and\ninter- matchable keypoints, evading the interference of irrelevant and\nredundant connectivity with non-repeatable ones. Furthermore, considering the\npotential noise in initial keypoints and sampled matchable ones, the MKACA\nmodule adopts a matchability-guided attentional aggregation operation for purer\ndata-dependent context propagation. By these means, we achieve the\nstate-of-the-art performance on relative camera estimation, fundamental matrix\nestimation, and visual localization, while significantly reducing computational\nand memory complexity compared to typical attentional GNNs.\n", "rewritten_text": "Accurately matching local features between image pairs is a challenging computer vision task.  Existing methods often employ attention-based graph neural networks (GNNs) with fully connected graphs over keypoints, reasoning about visual and geometric information. However, many keypoints are non-repeatable due to occlusion or detector failure, rendering them irrelevant for message passing.  The inclusion of these non-repeatable keypoints introduces redundancy, limiting efficiency and hindering accurate representation aggregation.\n\nTo improve both accuracy and efficiency, we propose MaKeGNN, a sparse attention-based GNN architecture that bypasses non-repeatable keypoints, leveraging only matchable ones for compact and informative message passing.  Specifically, our Bilateral Context-Aware Sampling (BCAS) module dynamically samples two small sets of well-distributed, highly matchable keypoints from each image.  Our Matchable Keypoint-Assisted Context Aggregation (MKACA) module then uses these sampled keypoints as message bottlenecks, restricting each keypoint's context retrieval to intra- and inter-image matchable keypoints, thus avoiding interference from non-repeatable keypoints.  Furthermore, to mitigate noise in both initial and sampled keypoints, MKACA employs a matchability-guided attentional aggregation for more robust context propagation.\n\nThis approach achieves state-of-the-art performance on relative camera estimation, fundamental matrix estimation, and visual localization, while significantly reducing computational and memory complexity compared to traditional attention-based GNNs.\n"}, "1703.00087": {"original_text": "  Lesion segmentation is the first step in most automatic melanoma recognition\nsystems. Deficiencies and difficulties in dermoscopic images such as color\ninconstancy, hair occlusion, dark corners and color charts make lesion\nsegmentation an intricate task. In order to detect the lesion in the presence\nof these problems, we propose a supervised saliency detection method tailored\nfor dermoscopic images based on the discriminative regional feature integration\n(DRFI). DRFI method incorporates multi-level segmentation, regional contrast,\nproperty, background descriptors, and a random forest regressor to create\nsaliency scores for each region in the image. In our improved saliency\ndetection method, mDRFI, we have added some new features to regional property\ndescriptors. Also, in order to achieve more robust regional background\ndescriptors, a thresholding algorithm is proposed to obtain a new\npseudo-background region. Findings reveal that mDRFI is superior to DRFI in\ndetecting the lesion as the salient object in dermoscopic images. The proposed\noverall lesion segmentation framework uses detected saliency map to construct\nan initial mask of the lesion through thresholding and post-processing\noperations. The initial mask is then evolving in a level set framework to fit\nbetter on the lesion's boundaries. The results of evaluation tests on three\npublic datasets show that our proposed segmentation method outperforms the\nother conventional state-of-the-art segmentation algorithms and its performance\nis comparable with most recent approaches that are based on deep convolutional\nneural networks.\n", "rewritten_text": "Lesion segmentation is the crucial first step in most automated melanoma recognition systems.  However, dermoscopic images often present challenges such as color inconsistencies, hair occlusion, dark corners, and color charts, rendering accurate segmentation a complex task. To address these challenges, we propose mDRFI, an improved supervised saliency detection method tailored for dermoscopic images.  Building upon the discriminative regional feature integration (DRFI) method, mDRFI incorporates multi-level segmentation, regional contrast, enhanced regional property descriptors, and robust background descriptors generated using a novel thresholding algorithm to create a pseudo-background region.  A random forest regressor assigns saliency scores to each image region.  Our enhancements result in superior lesion detection compared to the original DRFI method.  The complete lesion segmentation framework utilizes the resulting saliency map to generate an initial lesion mask via thresholding and post-processing.  This mask is then refined using a level set framework to accurately delineate lesion boundaries.  Evaluation on three publicly available datasets demonstrates that our proposed method outperforms conventional state-of-the-art segmentation algorithms and achieves performance comparable to the most recent deep convolutional neural network-based approaches.\n"}, "2405.12209": {"original_text": "  Recent advancements in large language models (LLMs) have showcased\nsignificant improvements in mathematics. However, traditional math benchmarks\nlike GSM8k offer a unidimensional perspective, falling short in providing a\nholistic assessment of the LLMs' math capabilities. To address this gap, we\nintroduce MathBench, a new benchmark that rigorously assesses the mathematical\ncapabilities of large language models. MathBench spans a wide range of\nmathematical disciplines, offering a detailed evaluation of both theoretical\nunderstanding and practical problem-solving skills. The benchmark progresses\nthrough five distinct stages, from basic arithmetic to college mathematics, and\nis structured to evaluate models at various depths of knowledge. Each stage\nincludes theoretical questions and application problems, allowing us to measure\na model's mathematical proficiency and its ability to apply concepts in\npractical scenarios. MathBench aims to enhance the evaluation of LLMs'\nmathematical abilities, providing a nuanced view of their knowledge\nunderstanding levels and problem solving skills in a bilingual context. The\nproject is released at https://github.com/open-compass/MathBench .\n", "rewritten_text": "Recent advancements in large language models (LLMs) have significantly improved their mathematical capabilities.  However, traditional benchmarks like GSM8k offer a limited, unidimensional perspective, failing to provide a holistic assessment of LLMs' mathematical proficiency.  To address this limitation, we introduce MathBench, a comprehensive benchmark rigorously evaluating the mathematical abilities of LLMs.  MathBench spans a wide range of mathematical disciplines, providing a detailed evaluation of both theoretical understanding and practical problem-solving skills.  Progressing through five distinct stages, from basic arithmetic to college-level mathematics, the benchmark assesses models at various levels of knowledge depth. Each stage includes theoretical questions and application problems, enabling the measurement of a model's mathematical proficiency and its ability to apply concepts to practical scenarios.  MathBench aims to enhance the evaluation of LLMs' mathematical abilities, offering a nuanced understanding of their knowledge and problem-solving skills in a bilingual context. The project is available at https://github.com/open-compass/MathBench.\n"}, "2407.03993": {"original_text": "  Natural language counterfactual generation aims to minimally modify a given\ntext such that the modified text will be classified into a different class. The\ngenerated counterfactuals provide insight into the reasoning behind a model's\npredictions by highlighting which words significantly influence the outcomes.\nAdditionally, they can be used to detect model fairness issues and augment the\ntraining data to enhance the model's robustness. A substantial amount of\nresearch has been conducted to generate counterfactuals for various NLP tasks,\nemploying different models and methodologies. With the rapid growth of studies\nin this field, a systematic review is crucial to guide future researchers and\ndevelopers. To bridge this gap, this survey provides a comprehensive overview\nof textual counterfactual generation methods, particularly those based on Large\nLanguage Models. We propose a new taxonomy that systematically categorizes the\ngeneration methods into four groups and summarizes the metrics for evaluating\nthe generation quality. Finally, we discuss ongoing research challenges and\noutline promising directions for future work.\n", "rewritten_text": "Natural language counterfactual generation aims to minimally alter a given text to change its predicted class.  These counterfactuals illuminate a model's reasoning by revealing words significantly impacting predictions.  Furthermore, they can detect biases and augment training data to improve model robustness.  Extensive research has generated counterfactuals for diverse NLP tasks using various models and methodologies.  Given the field's rapid growth, a systematic review is essential to guide future research and development.  This survey addresses this need by comprehensively reviewing textual counterfactual generation methods, focusing on those employing large language models (LLMs).  We introduce a novel taxonomy categorizing these methods into four groups and summarize relevant evaluation metrics.  Finally, we discuss current challenges and promising future research directions.\n"}, "1909.13411": {"original_text": "  Mesoscale eddies play a significant role in marine energy transport, marine\nbiological environment and marine climate. Due to their huge impact on the\nocean, mesoscale eddy detection has become a hot research area in recent years.\nTherefore, more and more people are entering the field of mesoscale eddy\ndetection. However, the existing detection methods mainly based on traditional\ndetection methods typically only use Sea Surface Height (SSH) as a variable to\ndetect, resulting in inaccurate performance. In this paper, we propose a\nmesoscale eddy detection method based on multivariate fusion data to solve this\nproblem. We not only use the SSH variable, but also add the two variables: Sea\nSurface Temperature (SST) and velocity of flow, achieving a multivariate\ninformation fusion input. We design a novel symmetric network, which merges\nlow-level feature maps from the downsampling pathway and high-level feature\nmaps from the upsampling pathway by lateral connection. In addition, we apply\ndilated convolutions to network structure to increase the receptive field and\nobtain more contextual information in the case of constant parameter. In the\nend, we demonstrate the effectiveness of our method on dataset provided by us,\nachieving the test set performance of 97.06% , greatly improved the performance\nof previous methods of mesoscale eddy detection.\n", "rewritten_text": "Mesoscale eddies significantly influence marine energy transport, the biological environment, and the climate.  Their substantial impact has made mesoscale eddy detection a rapidly growing research area.  However, existing methods, primarily relying on Sea Surface Height (SSH), often yield inaccurate results.  This paper proposes a novel mesoscale eddy detection method using multivariate data fusion.  Instead of relying solely on SSH, our approach incorporates Sea Surface Temperature (SST) and flow velocity, creating a multi-variate input.  We designed a symmetric network that laterally connects low-level feature maps from the downsampling pathway with high-level feature maps from the upsampling pathway.  Dilated convolutions enhance the receptive field, capturing more contextual information without increasing parameters.  Our method achieves a 97.06% accuracy on our dataset, significantly outperforming previous mesoscale eddy detection methods.\n"}, "1808.07733": {"original_text": "  We analyze the performance of different sentiment classification models on\nsyntactically complex inputs like A-but-B sentences. The first contribution of\nthis analysis addresses reproducible research: to meaningfully compare\ndifferent models, their accuracies must be averaged over far more random seeds\nthan what has traditionally been reported. With proper averaging in place, we\nnotice that the distillation model described in arXiv:1603.06318v4 [cs.LG],\nwhich incorporates explicit logic rules for sentiment classification, is\nineffective. In contrast, using contextualized ELMo embeddings\n(arXiv:1802.05365v2 [cs.CL]) instead of logic rules yields significantly better\nperformance. Additionally, we provide analysis and visualizations that\ndemonstrate ELMo's ability to implicitly learn logic rules. Finally, a\ncrowdsourced analysis reveals how ELMo outperforms baseline models even on\nsentences with ambiguous sentiment labels.\n", "rewritten_text": "This study analyzes the performance of various sentiment classification models on syntactically complex inputs, specifically A-but-B sentences.  Our first contribution addresses reproducibility:  meaningful model comparisons require averaging accuracy across significantly more random seeds than previously reported.  With robust averaging, we find that the distillation model (arXiv:1603.06318v4 [cs.LG]), which incorporates explicit logic rules, performs poorly.  In contrast, using contextualized ELMo embeddings (arXiv:1802.05365v2 [cs.CL]) instead of logic rules yields substantially improved performance.  Furthermore, our analysis and visualizations demonstrate ELMo's implicit learning of logic rules. Finally, a crowdsourced analysis confirms ELMo's superior performance over baseline models, even on sentences with ambiguous sentiment.\n"}, "2011.09634": {"original_text": "  In this paper, we teach machines to understand visuals and natural language\nby learning the mapping between sentences and noisy video snippets without\nexplicit annotations. Firstly, we define a self-supervised learning framework\nthat captures the cross-modal information. A novel adversarial learning module\nis then introduced to explicitly handle the noises in the natural videos, where\nthe subtitle sentences are not guaranteed to be strongly corresponded to the\nvideo snippets. For training and evaluation, we contribute a new dataset\n`ApartmenTour' that contains a large number of online videos and subtitles. We\ncarry out experiments on the bidirectional retrieval tasks between sentences\nand videos, and the results demonstrate that our proposed model achieves the\nstate-of-the-art performance on both retrieval tasks and exceeds several strong\nbaselines. The dataset can be downloaded at https://github.com/zyj-13/WAL.\n", "rewritten_text": "This paper presents a novel self-supervised learning framework that enables machines to understand visual and natural language information by learning the mapping between sentences and noisy video snippets without explicit annotations.  A key component is a novel adversarial learning module designed to mitigate the effects of noise inherent in natural videos, where subtitles may not perfectly align with corresponding video segments.  To facilitate training and evaluation, we introduce the ApartmentTour dataset, a large collection of online videos and their associated subtitles.  Experiments on bidirectional sentence-video retrieval tasks demonstrate that our model achieves state-of-the-art performance, surpassing several strong baselines. The ApartmentTour dataset is available at https://github.com/zyj-13/WAL.\n"}, "1708.01654": {"original_text": "  We demonstrate the use of shape-from-shading (SfS) to improve both the\nquality and the robustness of 3D reconstruction of dynamic objects captured by\na single camera. Unlike previous approaches that made use of SfS as a\npost-processing step, we offer a principled integrated approach that solves\ndynamic object tracking and reconstruction and SfS as a single unified cost\nfunction. Moving beyond Lambertian S f S , we propose a general approach that\nmodels both specularities and shading while simultaneously tracking and\nreconstructing general dynamic objects. Solving these problems jointly prevents\nthe kinds of tracking failures which can not be recovered from by pipeline\napproaches. We show state-of-the-art results both qualitatively and\nquantitatively.\n", "rewritten_text": "This paper presents a novel approach to 3D reconstruction of dynamic objects from a single camera, leveraging shape-from-shading (SfS) to enhance both reconstruction quality and robustness.  Unlike previous methods that applied SfS as a post-processing step, our integrated approach jointly solves dynamic object tracking, reconstruction, and SfS within a unified cost function.  Extending beyond Lambertian SfS, we propose a general model that accounts for both specularities and shading during simultaneous tracking and reconstruction. This joint optimization prevents the cascading failures inherent in pipeline approaches.  We demonstrate state-of-the-art results through both qualitative and quantitative evaluations.\n"}, "1711.07721": {"original_text": "  Post capture refocusing effect in smartphone cameras is achievable by using\nfocal stacks. However, the accuracy of this effect is totally dependent on the\ncombination of the depth layers in the stack. The accuracy of the extended\ndepth of field effect in this application can be improved significantly by\ncomputing an accurate depth map which has been an open issue for decades. To\ntackle this issue, in this paper, a framework is proposed based on\nPreconditioned Alternating Direction Method of Multipliers (PADMM) for depth\nfrom the focal stack and synthetic defocus application. In addition to its\nability to provide high structural accuracy and occlusion handling, the\noptimization function of the proposed method can, in fact, converge faster and\nbetter than state of the art methods. The evaluation has been done on 21 sets\nof focal stacks and the optimization function has been compared against 5 other\nmethods. Preliminary results indicate that the proposed method has a better\nperformance in terms of structural accuracy and optimization in comparison to\nthe current state of the art methods.\n", "rewritten_text": "Smartphone cameras can achieve post-capture refocusing using focal stacks.  However, the accuracy of this effect is heavily dependent on the depth layer combination within the stack.  Improving the accuracy of extended depth of field (EDOF) effects has been a longstanding challenge.  This paper proposes a novel framework based on the Preconditioned Alternating Direction Method of Multipliers (PADMM) for depth estimation from focal stacks and subsequent synthetic defocus application.  Our method offers superior structural accuracy and occlusion handling, while also exhibiting faster and improved convergence compared to state-of-the-art methods.  Evaluated on 21 focal stack datasets and benchmarked against five other methods, preliminary results demonstrate that our approach achieves superior performance in both structural accuracy and optimization speed.\n"}, "2004.06376": {"original_text": "  Understanding the shape of a scene from a single color image is a formidable\ncomputer vision task. However, most methods aim to predict the geometry of\nsurfaces that are visible to the camera, which is of limited use when planning\npaths for robots or augmented reality agents. Such agents can only move when\ngrounded on a traversable surface, which we define as the set of classes which\nhumans can also walk over, such as grass, footpaths and pavement. Models which\npredict beyond the line of sight often parameterize the scene with voxels or\nmeshes, which can be expensive to use in machine learning frameworks.\n  We introduce a model to predict the geometry of both visible and occluded\ntraversable surfaces, given a single RGB image as input. We learn from stereo\nvideo sequences, using camera poses, per-frame depth and semantic segmentation\nto form training data, which is used to supervise an image-to-image network. We\ntrain models from the KITTI driving dataset, the indoor Matterport dataset, and\nfrom our own casually captured stereo footage. We find that a surprisingly low\nbar for spatial coverage of training scenes is required. We validate our\nalgorithm against a range of strong baselines, and include an assessment of our\npredictions for a path-planning task.\n", "rewritten_text": "Predicting scene geometry from a single color image is a challenging computer vision problem.  Existing methods primarily focus on predicting the geometry of visible surfaces, limiting their applicability to robot or augmented reality (AR) agent path planning.  Such agents require traversable surfaces\u2014defined here as areas humans can walk on, including grass, footpaths, and pavement\u2014for navigation.  Models predicting beyond the line of sight often employ computationally expensive voxel or mesh representations, hindering their use in machine learning frameworks.\n\nThis paper introduces a model that predicts the geometry of both visible and occluded traversable surfaces from a single RGB image.  We train this image-to-image network using stereo video sequences, leveraging camera poses, per-frame depth maps, and semantic segmentation to generate training data.  Our training data comprises the KITTI driving dataset, the indoor Matterport dataset, and our own casually captured stereo footage.  Surprisingly, we find that a relatively small spatial coverage of training scenes is sufficient.  We evaluate our algorithm against several strong baselines and assess its performance in a path-planning task.\n"}, "2107.08173": {"original_text": "  This ability to learn consecutive tasks without forgetting how to perform\npreviously trained problems is essential for developing an online dialogue\nsystem. This paper proposes an effective continual learning for the\ntask-oriented dialogue system with iterative network pruning, expanding and\nmasking (TPEM), which preserves performance on previously encountered tasks\nwhile accelerating learning progress on subsequent tasks. Specifically, TPEM\n(i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts\nnetwork expanding to create free weights for new tasks, and (iii) introduces\ntask-specific network masking to alleviate the negative impact of fixed weights\nof old tasks on new tasks. We conduct extensive experiments on seven different\ntasks from three benchmark datasets and show empirically that TPEM leads to\nsignificantly improved results over the strong competitors. For\nreproducibility, we submit the code and data at:\nhttps://github.com/siat-nlp/TPEM\n", "rewritten_text": "Continual learning, the ability to learn consecutive tasks without forgetting previously acquired skills, is crucial for developing effective online dialogue systems.  This paper introduces TPEM (Task-oriented dialogue system with Iterative network Pruning, Expanding, and Masking), a novel continual learning approach that maintains performance on past tasks while accelerating learning on new ones.  TPEM achieves this by (i) leveraging network pruning to retain knowledge from previous tasks, (ii) expanding the network to allocate free weights for new tasks, and (iii) employing task-specific masking to mitigate the negative interference of fixed weights from old tasks on new tasks.  Extensive experiments across seven tasks from three benchmark datasets demonstrate that TPEM significantly outperforms existing state-of-the-art methods.  For reproducibility, the code and data are available at: https://github.com/siat-nlp/TPEM\n"}, "1811.09789": {"original_text": "  There has been much recent work on image captioning models that describe the\nfactual aspects of an image. Recently, some models have incorporated\nnon-factual aspects into the captions, such as sentiment or style. However,\nsuch models typically have difficulty in balancing the semantic aspects of the\nimage and the non-factual dimensions of the caption; in addition, it can be\nobserved that humans may focus on different aspects of an image depending on\nthe chosen sentiment or style of the caption. To address this, we design an\nattention-based model to better add sentiment to image captions. The model\nembeds and learns sentiment with respect to image-caption data, and uses both\nhigh-level and word-level sentiment information during the learning process.\nThe model outperforms the state-of-the-art work in image captioning with\nsentiment using standard evaluation metrics. An analysis of generated captions\nalso shows that our model does this by a better selection of the\nsentiment-bearing adjectives and adjective-noun pairs.\n", "rewritten_text": "Recent research in image captioning has focused on models that describe the factual content of images.  While some models now incorporate non-factual aspects like sentiment or style, they often struggle to balance semantic accuracy with these stylistic dimensions.  Furthermore, human captioning demonstrates a dependence on chosen sentiment or style, influencing the aspects of the image emphasized.  To address this, we propose an attention-based model for generating sentiment-enhanced image captions.  This model embeds and learns sentiment from image-caption data, leveraging both high-level and word-level sentiment information during training.  Our model surpasses state-of-the-art sentiment-enhanced image captioning models, as measured by standard evaluation metrics.  Analysis of generated captions reveals that this improvement stems from a more effective selection of sentiment-bearing adjectives and adjective-noun pairs.\n"}, "2310.15724": {"original_text": "  Pre-trained language models (PLMs) have achieved remarkable results on NLP\ntasks but at the expense of huge parameter sizes and the consequent\ncomputational costs. In this paper, we propose Variator, a parameter-efficient\nacceleration method that enhances computational efficiency through\nplug-and-play compression plugins. Compression plugins are designed to reduce\nthe sequence length via compressing multiple hidden vectors into one and\ntrained with original PLMs frozen. Different from traditional model\nacceleration methods, which compress PLMs to smaller sizes, Variator offers two\ndistinct advantages: (1) In real-world applications, the plug-and-play nature\nof our compression plugins enables dynamic selection of different compression\nplugins with varying acceleration ratios based on the current workload. (2) The\ncompression plugin comprises a few compact neural network layers with minimal\nparameters, significantly saving storage and memory overhead, particularly in\nscenarios with a growing number of tasks. We validate the effectiveness of\nVariator on seven datasets. Experimental results show that Variator can save\n53% computational costs using only 0.9% additional parameters with a\nperformance drop of less than 2%. Moreover, when the model scales to billions\nof parameters, Variator matches the strong performance of uncompressed PLMs.\n", "rewritten_text": "Pre-trained language models (PLMs) achieve remarkable results on natural language processing (NLP) tasks, but at the cost of substantial parameter sizes and computational expense.  This paper introduces Variator, a parameter-efficient acceleration method that enhances computational efficiency using plug-and-play compression plugins. These plugins reduce sequence length by compressing multiple hidden vectors into a single vector, trained while the original PLM remains frozen.  Unlike traditional model acceleration methods that reduce PLM size, Variator offers two key advantages: (1) its plug-and-play architecture allows dynamic selection of plugins with varying acceleration ratios to optimize for diverse workloads; and (2) its compact, few-layered neural network design minimizes parameter count, significantly reducing storage and memory overhead, especially beneficial for applications with numerous tasks.  We validate Variator's effectiveness across seven datasets.  Experimental results demonstrate a 53% reduction in computational cost with only a 0.9% increase in parameters and less than a 2% performance degradation. Furthermore, when scaling to billion-parameter models, Variator maintains the strong performance of uncompressed PLMs.\n"}, "2110.02929": {"original_text": "  Event-based dynamic vision sensors provide very sparse output in the form of\nspikes, which makes them suitable for low-power applications. Convolutional\nspiking neural networks model such event-based data and develop their full\nenergy-saving potential when deployed on asynchronous neuromorphic hardware.\nEvent-based vision being a nascent field, the sensitivity of spiking neural\nnetworks to potentially malicious adversarial attacks has received little\nattention so far. We show how white-box adversarial attack algorithms can be\nadapted to the discrete and sparse nature of event-based visual data, and\ndemonstrate smaller perturbation magnitudes at higher success rates than the\ncurrent state-of-the-art algorithms. For the first time, we also verify the\neffectiveness of these perturbations directly on neuromorphic hardware.\nFinally, we discuss the properties of the resulting perturbations, the effect\nof adversarial training as a defense strategy, and future directions.\n", "rewritten_text": "Event-based dynamic vision sensors (EDVS) generate sparse, spike-based output, making them ideal for low-power applications.  Convolutional spiking neural networks (CSNNs) are well-suited to process this data, realizing their full energy-saving potential when deployed on asynchronous neuromorphic hardware.  However, as event-based vision is a nascent field, the vulnerability of CSNNs to adversarial attacks remains largely unexplored.  This work adapts existing white-box adversarial attack algorithms to the discrete and sparse nature of event-based visual data, demonstrating smaller perturbation magnitudes and higher success rates than current state-of-the-art methods.  Furthermore, we provide the first verification of these attacks' effectiveness on neuromorphic hardware.  Finally, we analyze the properties of the resulting adversarial perturbations, explore the efficacy of adversarial training as a defense mechanism, and outline future research directions.\n"}, "2405.20259": {"original_text": "  The proliferation of deep learning solutions and the scarcity of large\nannotated datasets pose significant challenges in real-world applications.\nVarious strategies have been explored to overcome this challenge, with data\naugmentation (DA) approaches emerging as prominent solutions. DA approaches\ninvolve generating additional examples by transforming existing labeled data,\nthereby enriching the dataset and helping deep learning models achieve improved\ngeneralization without succumbing to overfitting. In real applications, where\nsolutions based on deep learning are widely used, there is facial expression\nrecognition (FER), which plays an essential role in human communication,\nimproving a range of knowledge areas (e.g., medicine, security, and marketing).\nIn this paper, we propose a simple and comprehensive face data augmentation\napproach based on mixed face component regularization that outperforms the\nclassical DA approaches from the literature, including the MixAugment which is\na specific approach for the target task in two well-known FER datasets existing\nin the literature.\n", "rewritten_text": "The proliferation of deep learning solutions is hampered by the scarcity of large, annotated datasets, posing significant challenges for real-world applications.  Data augmentation (DA) has emerged as a prominent strategy to address this limitation.  DA techniques generate additional training examples by transforming existing labeled data, enriching the dataset and improving the generalization performance of deep learning models while mitigating overfitting.  Facial expression recognition (FER), crucial for human-computer interaction and impacting diverse fields such as medicine, security, and marketing, exemplifies a real-world application heavily reliant on deep learning.  This paper proposes a novel, simple, and comprehensive face data augmentation approach based on mixed face component regularization.  Our method outperforms existing state-of-the-art DA techniques, including MixAugment\u2014a specific approach for FER\u2014on two widely used benchmark datasets.\n"}, "2308.09983": {"original_text": "  Early detection of dysplasia of the cervix is critical for cervical cancer\ntreatment. However, automatic cervical dysplasia diagnosis via visual\ninspection, which is more appropriate in low-resource settings, remains a\nchallenging problem. Though promising results have been obtained by recent deep\nlearning models, their performance is significantly hindered by the limited\nscale of the available cervix datasets. Distinct from previous methods that\nlearn from a single dataset, we propose to leverage cross-domain cervical\nimages that were collected in different but related clinical studies to improve\nthe model's performance on the targeted cervix dataset. To robustly learn the\ntransferable information across datasets, we propose a novel prototype-based\nknowledge filtering method to estimate the transferability of cross-domain\nsamples. We further optimize the shared feature space by aligning the\ncross-domain image representations simultaneously on domain level with early\nalignment and class level with supervised contrastive learning, which endows\nmodel training and knowledge transfer with stronger robustness. The empirical\nresults on three real-world benchmark cervical image datasets show that our\nproposed method outperforms the state-of-the-art cervical dysplasia visual\ninspection by an absolute improvement of 4.7% in top-1 accuracy, 7.0% in\nprecision, 1.4% in recall, 4.6% in F1 score, and 0.05 in ROC-AUC.\n", "rewritten_text": "Early detection of cervical dysplasia is crucial for effective cervical cancer treatment.  However, automated visual diagnosis, particularly valuable in low-resource settings, remains challenging due to limitations in available datasets. While recent deep learning models show promise, their performance is significantly hampered by the small size of existing cervical image datasets.  Unlike previous single-dataset approaches, we propose a novel method leveraging cross-domain cervical images from different but related clinical studies to enhance model performance on a target dataset.  To robustly transfer knowledge across domains, we introduce a prototype-based knowledge filtering method to assess the transferability of individual samples.  Furthermore, we optimize the shared feature space by aligning cross-domain image representations at both the domain level (using early alignment) and the class level (using supervised contrastive learning), thereby improving model robustness and knowledge transfer.  Empirical results on three real-world benchmark datasets demonstrate that our method surpasses state-of-the-art cervical dysplasia visual inspection, achieving absolute improvements of 4.7% in top-1 accuracy, 7.0% in precision, 1.4% in recall, 4.6% in F1-score, and 0.05 in ROC-AUC.\n"}, "1704.02268": {"original_text": "  Unsupervised learning of visual similarities is of paramount importance to\ncomputer vision, particularly due to lacking training data for fine-grained\nsimilarities. Deep learning of similarities is often based on relationships\nbetween pairs or triplets of samples. Many of these relations are unreliable\nand mutually contradicting, implying inconsistencies when trained without\nsupervision information that relates different tuples or triplets to each\nother. To overcome this problem, we use local estimates of reliable\n(dis-)similarities to initially group samples into compact surrogate classes\nand use local partial orders of samples to classes to link classes to each\nother. Similarity learning is then formulated as a partial ordering task with\nsoft correspondences of all samples to classes. Adopting a strategy of\nself-supervision, a CNN is trained to optimally represent samples in a mutually\nconsistent manner while updating the classes. The similarity learning and\ngrouping procedure are integrated in a single model and optimized jointly. The\nproposed unsupervised approach shows competitive performance on detailed pose\nestimation and object classification.\n", "rewritten_text": "Unsupervised learning of visual similarity is crucial for computer vision, especially given the scarcity of training data for fine-grained distinctions.  Deep learning approaches often rely on pairwise or triplet comparisons, many of which are unreliable and contradictory, leading to inconsistencies in unsupervised training.  To address this, we propose a novel method that leverages locally estimated reliable (dis)similarities to initially group samples into compact surrogate classes.  These classes are then interconnected using local partial orders between samples and classes.  Similarity learning is subsequently formulated as a partial ordering task with soft correspondences between all samples and classes.  Employing a self-supervised strategy, a convolutional neural network (CNN) is trained to generate mutually consistent sample representations while simultaneously updating the classes.  The similarity learning and grouping processes are integrated into a single, jointly optimized model.  Our unsupervised approach demonstrates competitive performance on detailed pose estimation and object classification tasks.\n"}, "1909.07598": {"original_text": "  Multi-hop question answering (QA) requires an information retrieval (IR)\nsystem that can find \\emph{multiple} supporting evidence needed to answer the\nquestion, making the retrieval process very challenging. This paper introduces\nan IR technique that uses information of entities present in the initially\nretrieved evidence to learn to `\\emph{hop}' to other relevant evidence. In a\nsetting, with more than \\textbf{5 million} Wikipedia paragraphs, our approach\nleads to significant boost in retrieval performance. The retrieved evidence\nalso increased the performance of an existing QA model (without any training)\non the \\hotpot benchmark by \\textbf{10.59} F1.\n", "rewritten_text": "Multi-hop question answering (QA) demands an information retrieval (IR) system capable of identifying multiple supporting documents necessary to answer a question, a process fraught with challenges. This paper introduces an IR technique that leverages entity information from initially retrieved evidence to iteratively locate additional relevant documents.  In a setting with over 5 million Wikipedia paragraphs, our approach yields a significant improvement in retrieval performance.  Furthermore, the retrieved evidence, without any additional model training, boosted the performance of an existing QA model on the HotpotQA benchmark by 10.59 F1 points.\n"}, "2211.06726": {"original_text": "  Vision Transformer (ViT) is a pioneering deep learning framework that can\naddress real-world computer vision issues, such as image classification and\nobject recognition. Importantly, ViTs are proven to outperform traditional deep\nlearning models, such as convolutional neural networks (CNNs). Relatively\nrecently, a number of ViT mutations have been transplanted into the field of\nmedical imaging, thereby resolving a variety of critical classification and\nsegmentation challenges, especially in terms of brain imaging data. In this\nwork, we provide a novel multimodal deep learning pipeline, MultiCrossViT,\nwhich is capable of analyzing both structural MRI (sMRI) and static functional\nnetwork connectivity (sFNC) data for the prediction of schizophrenia disease.\nOn a dataset with minimal training subjects, our novel model can achieve an AUC\nof 0.832. Finally, we visualize multiple brain regions and covariance patterns\nmost relevant to schizophrenia based on the resulting ViT attention maps by\nextracting features from transformer encoders.\n", "rewritten_text": "Vision Transformers (ViTs) are a groundbreaking deep learning architecture addressing real-world computer vision tasks such as image classification and object recognition.  Demonstrating superior performance to traditional convolutional neural networks (CNNs), ViTs have recently been adapted for medical imaging, effectively tackling challenging classification and segmentation problems, particularly in brain imaging.  This work introduces MultiCrossViT, a novel multimodal deep learning pipeline that analyzes structural MRI (sMRI) and static functional network connectivity (sFNC) data to predict schizophrenia.  Even with a limited training dataset, our model achieves an area under the curve (AUC) of 0.832.  Furthermore, by extracting features from the transformer encoders, we visualize brain regions and covariance patterns most relevant to schizophrenia using the resulting ViT attention maps.\n"}, "1607.02504": {"original_text": "  We present a method to predict image deformations based on patch-wise image\nappearance. Specifically, we design a patch-based deep encoder-decoder network\nwhich learns the pixel/voxel-wise mapping between image appearance and\nregistration parameters. Our approach can predict general deformation\nparameterizations, however, we focus on the large deformation diffeomorphic\nmetric mapping (LDDMM) registration model. By predicting the LDDMM\nmomentum-parameterization we retain the desirable theoretical properties of\nLDDMM, while reducing computation time by orders of magnitude: combined with\npatch pruning, we achieve a 1500x/66x speed up compared to GPU-based\noptimization for 2D/3D image registration. Our approach has better prediction\naccuracy than predicting deformation or velocity fields and results in\ndiffeomorphic transformations. Additionally, we create a Bayesian probabilistic\nversion of our network, which allows evaluation of deformation field\nuncertainty through Monte Carlo sampling using dropout at test time. We show\nthat deformation uncertainty highlights areas of ambiguous deformations. We\ntest our method on the OASIS brain image dataset in 2D and 3D.\n", "rewritten_text": "We propose a novel method for predicting image deformations based on patch-wise appearance.  This method employs a deep encoder-decoder network that learns a pixel/voxel-wise mapping between image appearance and large deformation diffeomorphic metric mapping (LDDMM) registration parameters.  Specifically, we predict the LDDMM momentum parameterization, preserving LDDMM's desirable theoretical properties while achieving significant computational speedups.  Combined with patch pruning, our approach accelerates 2D and 3D image registration by factors of 1500x and 66x, respectively, compared to GPU-based optimization.  Our method surpasses the accuracy of approaches predicting deformation or velocity fields directly, while guaranteeing diffeomorphic transformations.  Furthermore, we present a Bayesian probabilistic extension of our network, enabling uncertainty quantification of the deformation field via Monte Carlo dropout sampling during inference.  This uncertainty quantification effectively highlights regions of ambiguous deformation. We validate our method on the OASIS brain image dataset in both 2D and 3D.\n"}, "1911.08007": {"original_text": "  The classification of streets on road networks has been focused on the\nvehicular transportational features of streets such as arterials, major roads,\nminor roads and so forth based on their transportational use. City authorities\non the other hand have been shifting to more urban inclusive planning of\nstreets, encompassing the side use of a street combined with the\ntransportational features of a street. In such classification schemes, streets\nare labeled for example as commercial throughway, residential neighborhood,\npark etc. This modern approach to urban planning has been adopted by major\ncities such as the city of San Francisco, the states of Florida and\nPennsylvania among many others. Currently, the process of labeling streets\naccording to their contexts is manual and hence is tedious and time consuming.\nIn this paper, we propose an approach to collect and label imagery data then\ndeploy advancements in computer vision towards modern urban planning. We\ncollect and label street imagery then train deep convolutional neural networks\n(CNN) to perform the classification of street context. We show that CNN models\ncan perform well achieving accuracies in the 81% to 87%, we then visualize\nsamples from the embedding space of streets using the t-SNE method and apply\nclass activation mapping methods to interpret the features in street imagery\ncontributing to output classification from a model.\n", "rewritten_text": "Traditional road network classification focuses on vehicular transport, categorizing streets as arterials, major roads, minor roads, etc., based on their functional use.  However, urban planning is increasingly adopting a more inclusive approach, considering both transportation and contextual land uses.  This results in classifications such as \"commercial throughway,\" \"residential neighborhood,\" or \"park.\"  Major cities, including San Francisco, and states like Florida and Pennsylvania, have embraced this modern approach.  Currently, this contextual street labeling is a manual, tedious, and time-consuming process.\n\nThis paper proposes an automated approach leveraging computer vision.  We collect and label street imagery to train deep convolutional neural networks (CNNs) for street context classification.  Our CNN models achieve 81%\u201387% accuracy.  Furthermore, we visualize street embeddings using t-SNE and employ class activation mapping to interpret the model's feature extraction from street imagery.\n"}, "1811.11387": {"original_text": "  The success of deep neural networks generally requires a vast amount of\ntraining data to be labeled, which is expensive and unfeasible in scale,\nespecially for video collections. To alleviate this problem, in this paper, we\npropose 3DRotNet: a fully self-supervised approach to learn spatiotemporal\nfeatures from unlabeled videos. A set of rotations are applied to all videos,\nand a pretext task is defined as prediction of these rotations. When\naccomplishing this task, 3DRotNet is actually trained to understand the\nsemantic concepts and motions in videos. In other words, it learns a\nspatiotemporal video representation, which can be transferred to improve video\nunderstanding tasks in small datasets. Our extensive experiments successfully\ndemonstrate the effectiveness of the proposed framework on action recognition,\nleading to significant improvements over the state-of-the-art self-supervised\nmethods. With the self-supervised pre-trained 3DRotNet from large datasets, the\nrecognition accuracy is boosted up by 20.4% on UCF101 and 16.7% on HMDB51\nrespectively, compared to the models trained from scratch.\n", "rewritten_text": "Deep neural networks typically require extensive labeled training data, a costly and often impractical constraint, particularly for video datasets.  To address this, we propose 3DRotNet, a fully self-supervised approach for learning spatiotemporal features from unlabeled videos.  We apply a set of rotations to each video and define a pretext task: predicting these rotations.  This task implicitly trains 3DRotNet to understand semantic concepts and motion within the videos, effectively learning a spatiotemporal representation transferable to downstream video understanding tasks, even with limited data.  Extensive experiments on action recognition demonstrate the effectiveness of our framework, significantly outperforming state-of-the-art self-supervised methods.  Pre-training 3DRotNet on large datasets yields a 20.4% accuracy improvement on UCF101 and a 16.7% improvement on HMDB51 compared to models trained from scratch.\n"}, "2402.01619": {"original_text": "  Program induction (PI) has become a promising paradigm for using knowledge\nbases (KBs) to help large language models (LLMs) answer complex\nknowledge-intensive questions. Nonetheless, PI typically relies on a large\nnumber of parallel question-program pairs to make the LLM aware of the schema\nof the given KB, and is thus challenging for many low-resourced KBs that lack\nannotated data. To this end, we propose KB-Plugin, a plug-and-play framework\nthat enables LLMs to induce programs over any low-resourced KB. Firstly,\nKB-Plugin adopts self-supervised learning to encode the detailed schema\ninformation of a given KB into a pluggable module, namely schema plugin.\nSecondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB\nto train another pluggable module, namely PI plugin, which can help the LLM\nextract question-relevant schema information from the schema plugin of any KB\nand utilize this information to induce programs over this KB. Experiments on\nfive heterogeneous KBQA datasets show that KB-Plugin achieves better or\ncomparable performance with 25$\\times$ smaller backbone LLM compared to SoTA PI\nmethods for low-resourced KBs, and even approaches the performance of\nsupervised methods. Our code and data are available at\nhttps://github.com/THU-KEG/KB-Plugin.\n", "rewritten_text": "Program induction (PI) offers a promising paradigm for leveraging knowledge bases (KBs) to enhance large language models' (LLMs) ability to answer complex, knowledge-intensive questions.  However, PI typically requires a large number of parallel question-program pairs to familiarize the LLM with the KB's schema, posing a significant challenge for low-resource KBs lacking annotated data.  To address this, we introduce KB-Plugin, a plug-and-play framework enabling LLMs to induce programs over any low-resource KB.  KB-Plugin employs a two-pronged approach: first, it uses self-supervised learning to encode a given KB's schema information into a pluggable \"schema plugin\" module. Second, it leverages abundant annotated data from a high-resource KB to train a \"PI plugin\" module. This module enables the LLM to extract question-relevant schema information from the schema plugin of *any* KB and use this information for program induction.  Experiments across five heterogeneous KBQA datasets demonstrate that KB-Plugin achieves comparable or superior performance to state-of-the-art (SoTA) PI methods for low-resource KBs, even with a backbone LLM 25 times smaller.  Furthermore, its performance approaches that of supervised methods. Our code and data are available at https://github.com/THU-KEG/KB-Plugin.\n"}, "1812.01458": {"original_text": "  Recent advances in deep learning have shown exciting promise in filling large\nholes and lead to another orientation for image inpainting. However, existing\nlearning-based methods often create artifacts and fallacious textures because\nof insufficient cognition understanding. Previous generative networks are\nlimited with single receptive type and give up pooling in consideration of\ndetail sharpness. Human cognition is constant regardless of the target\nattribute. As multiple receptive fields improve the ability of abstract image\ncharacterization and pooling can keep feature invariant, specifically, deep\ninception learning is adopted to promote high-level feature representation and\nenhance model learning capacity for local patches. Moreover, approaches for\ngenerating diverse mask images are introduced and a random mask dataset is\ncreated. We benchmark our methods on ImageNet, Places2 dataset, and CelebA-HQ.\nExperiments for regular, irregular, and custom regions completion are all\nperformed and free-style image inpainting is also presented. Quantitative\ncomparisons with previous state-of-the-art methods show that ours obtain much\nmore natural image completions.\n", "rewritten_text": "Recent advances in deep learning have shown exciting promise in image inpainting, particularly in filling large holes. However, existing learning-based methods often generate artifacts and unrealistic textures due to limitations in their understanding of image content.  Previous generative networks, constrained by single receptive fields and the omission of pooling to preserve detail sharpness, have been insufficient.  In contrast, human visual cognition remains consistent regardless of the target image attributes.  To address these limitations, we leverage multiple receptive fields to improve abstract image characterization and incorporate pooling to maintain feature invariance.  Specifically, we adopt deep inception learning to enhance high-level feature representation and improve the model's capacity for learning local patch details.  Furthermore, we introduce methods for generating diverse mask images and create a random mask dataset.  We benchmark our method on ImageNet, Places2, and CelebA-HQ datasets, performing experiments on regular, irregular, and custom region completion, as well as free-style image inpainting.  Quantitative comparisons with state-of-the-art methods demonstrate that our approach produces significantly more natural image completions.\n"}, "cs/0207005": {"original_text": "  We present a broad coverage Japanese grammar written in the HPSG formalism\nwith MRS semantics. The grammar is created for use in real world applications,\nsuch that robustness and performance issues play an important role. It is\nconnected to a POS tagging and word segmentation tool. This grammar is being\ndeveloped in a multilingual context, requiring MRS structures that are easily\ncomparable across languages.\n", "rewritten_text": "This paper presents a comprehensive Japanese grammar implemented using the Head-driven Phrase Structure Grammar (HPSG) formalism with Minimal Recursion Semantics (MRS).  Designed for real-world applications, the grammar prioritizes robustness and performance. It integrates with a part-of-speech (POS) tagging and word segmentation tool.  Furthermore, its development within a multilingual context necessitates readily comparable MRS structures across languages.\n"}, "2307.10696": {"original_text": "  Improving the feature representation ability is the foundation of many whole\nslide pathological image (WSIs) tasks. Recent works have achieved great success\nin pathological-specific self-supervised learning (SSL). However, most of them\nonly focus on learning patch-level representations, thus there is still a gap\nbetween pretext and slide-level downstream tasks, e.g., subtyping, grading and\nstaging. Aiming towards slide-level representations, we propose Slide-Level\nPrototypical Distillation (SLPD) to explore intra- and inter-slide semantic\nstructures for context modeling on WSIs. Specifically, we iteratively perform\nintra-slide clustering for the regions (4096x4096 patches) within each WSI to\nyield the prototypes and encourage the region representations to be closer to\nthe assigned prototypes. By representing each slide with its prototypes, we\nfurther select similar slides by the set distance of prototypes and assign the\nregions by cross-slide prototypes for distillation. SLPD achieves\nstate-of-the-art results on multiple slide-level benchmarks and demonstrates\nthat representation learning of semantic structures of slides can make a\nsuitable proxy task for WSI analysis. Code will be available at\nhttps://github.com/Carboxy/SLPD.\n", "rewritten_text": "Effective feature representation is fundamental to many whole-slide pathological image (WSI) analysis tasks.  While recent work using pathology-specific self-supervised learning (SSL) has shown significant progress, most methods focus on patch-level representations, creating a disconnect between pretext tasks and slide-level downstream applications such as subtyping, grading, and staging.  To address this, we propose Slide-Level Prototypical Distillation (SLPD), a novel method that leverages intra- and inter-slide semantic structures for improved WSI context modeling.  SLPD iteratively performs intra-slide clustering of 4096x4096 patches within each WSI to generate prototypes, encouraging region representations to align with their assigned prototypes.  Each slide is then represented by its prototypes, enabling the selection of similar slides based on prototype set distance.  Finally, cross-slide prototype distillation refines region representations.  SLPD achieves state-of-the-art results on multiple slide-level benchmarks, demonstrating that learning the semantic structure of slides provides an effective proxy task for WSI analysis.  Code is available at https://github.com/Carboxy/SLPD.\n"}, "2205.05869": {"original_text": "  We address the task of view synthesis, generating novel views of a scene\ngiven a set of images as input. In many recent works such as NeRF (Mildenhall\net al., 2020), the scene geometry is parameterized using neural implicit\nrepresentations (i.e., MLPs). Implicit neural representations have achieved\nimpressive visual quality but have drawbacks in computational efficiency. In\nthis work, we propose a new approach that performs view synthesis using point\nclouds. It is the first point-based method that achieves better visual quality\nthan NeRF while being 100x faster in rendering speed. Our approach builds on\nexisting works on differentiable point-based rendering but introduces a novel\ntechnique we call \"Sculpted Neural Points (SNP)\", which significantly improves\nthe robustness to errors and holes in the reconstructed point cloud. We further\npropose to use view-dependent point features based on spherical harmonics to\ncapture non-Lambertian surfaces, and new designs in the point-based rendering\npipeline that further boost the performance. Finally, we show that our system\nsupports fine-grained scene editing. Code is available at\nhttps://github.com/princeton-vl/SNP.\n", "rewritten_text": "This work addresses the task of novel view synthesis, generating new views from a set of input images.  Many recent approaches, such as NeRF (Mildenhall et al., 2020), employ neural implicit representations (e.g., MLPs) to parameterize scene geometry. While these methods achieve impressive visual quality, they suffer from computational inefficiencies.  We propose a novel point-cloud-based approach to view synthesis, the first to surpass NeRF in visual quality while achieving a 100x speedup in rendering.  Building upon differentiable point-based rendering techniques, we introduce Sculpted Neural Points (SNP), a novel method that significantly improves robustness to errors and holes in the reconstructed point cloud.  Furthermore, we leverage view-dependent point features based on spherical harmonics to model non-Lambertian surfaces and incorporate novel pipeline designs to further enhance performance.  Finally, we demonstrate the system's capacity for fine-grained scene editing. Code is available at https://github.com/princeton-vl/SNP.\n"}, "1910.03484": {"original_text": "  In Natural Language Generation (NLG), End-to-End (E2E) systems trained\nthrough deep learning have recently gained a strong interest. Such deep models\nneed a large amount of carefully annotated data to reach satisfactory\nperformance. However, acquiring such datasets for every new NLG application is\na tedious and time-consuming task. In this paper, we propose a semi-supervised\ndeep learning scheme that can learn from non-annotated data and annotated data\nwhen available. It uses an NLG and a Natural Language Understanding (NLU)\nsequence-to-sequence models which are learned jointly to compensate for the\nlack of annotation. Experiments on two benchmark datasets show that, with\nlimited amount of annotated data, the method can achieve very competitive\nresults while not using any pre-processing or re-scoring tricks. These findings\nopen the way to the exploitation of non-annotated datasets which is the current\nbottleneck for the E2E NLG system development to new applications.\n", "rewritten_text": "Recently, end-to-end (E2E) Natural Language Generation (NLG) systems trained via deep learning have attracted significant interest.  However, these deep models require large quantities of carefully annotated data to achieve satisfactory performance, making data acquisition a significant bottleneck for new NLG applications.  This paper proposes a semi-supervised deep learning approach that leverages both annotated and unannotated data.  Our method jointly trains an NLG and a Natural Language Understanding (NLU) sequence-to-sequence model to compensate for limited annotation.  Experiments on two benchmark datasets demonstrate that, even with limited annotated data, our method achieves highly competitive results without preprocessing or rescoring techniques.  These findings pave the way for exploiting unannotated datasets, thereby addressing a critical limitation in E2E NLG system development for novel applications.\n"}, "2109.09883": {"original_text": "  Few-shot classification aims at classifying categories of a novel task by\nlearning from just a few (typically, 1 to 5) labelled examples. An effective\napproach to few-shot classification involves a prior model trained on a\nlarge-sample base domain, which is then finetuned over the novel few-shot task\nto yield generalizable representations. However, task-specific finetuning is\nprone to overfitting due to the lack of enough training examples. To alleviate\nthis issue, we propose a new finetuning approach based on contrastive learning\nthat reuses unlabelled examples from the base domain in the form of\ndistractors. Unlike the nature of unlabelled data used in prior works,\ndistractors belong to classes that do not overlap with the novel categories. We\ndemonstrate for the first time that inclusion of such distractors can\nsignificantly boost few-shot generalization. Our technical novelty includes a\nstochastic pairing of examples sharing the same category in the few-shot task\nand a weighting term that controls the relative influence of task-specific\nnegatives and distractors. An important aspect of our finetuning objective is\nthat it is agnostic to distractor labels and hence applicable to various base\ndomain settings. Compared to state-of-the-art approaches, our method shows\naccuracy gains of up to $12\\%$ in cross-domain and up to $5\\%$ in unsupervised\nprior-learning settings.\n", "rewritten_text": "Few-shot classification aims to classify novel categories using only a few (typically 1\u20135) labeled examples.  Effective approaches leverage a pre-trained model on a large base domain, subsequently fine-tuned on the novel few-shot task to generate generalizable representations.  However, this fine-tuning is susceptible to overfitting due to limited training data.  To mitigate this, we propose a novel contrastive learning-based fine-tuning approach that incorporates unlabeled examples from the base domain as distractors.  Unlike previous methods, these distractors belong to classes entirely disjoint from the novel categories.  We demonstrate, for the first time, that including such distractors significantly improves few-shot generalization.  Our approach introduces a stochastic pairing of same-category examples within the few-shot task and a weighting mechanism to balance the influence of task-specific negatives and distractors.  Crucially, our fine-tuning objective is agnostic to distractor labels, making it applicable across diverse base domains.  Compared to state-of-the-art methods, our approach achieves accuracy gains of up to 12% in cross-domain settings and up to 5% in unsupervised pre-training scenarios.\n"}, "2003.12137": {"original_text": "  We explore novel approaches to the task of image generation from their\nrespective captions, building on state-of-the-art GAN architectures.\nParticularly, we baseline our models with the Attention-based GANs that learn\nattention mappings from words to image features. To better capture the features\nof the descriptions, we then built a novel cyclic design that learns an inverse\nfunction to maps the image back to original caption. Additionally, we\nincorporated recently developed BERT pretrained word embeddings as our initial\ntext featurizer and observe a noticeable improvement in qualitative and\nquantitative performance compared to the Attention GAN baseline.\n", "rewritten_text": "This work explores novel approaches to image generation from captions, building upon state-of-the-art Generative Adversarial Network (GAN) architectures.  We establish a baseline using attention-based GANs that learn mappings from words to image features. To enhance feature capture from captions, we introduce a novel cyclic design that learns an inverse mapping from generated images back to the original captions.  Furthermore, we leverage pre-trained BERT word embeddings for initial text featurization, resulting in noticeable improvements in both qualitative and quantitative performance compared to the attention-based GAN baseline.\n"}, "2003.02683": {"original_text": "  We introduce the first method for automatic image generation from scene-level\nfreehand sketches. Our model allows for controllable image generation by\nspecifying the synthesis goal via freehand sketches. The key contribution is an\nattribute vector bridged Generative Adversarial Network called EdgeGAN, which\nsupports high visual-quality object-level image content generation without\nusing freehand sketches as training data. We have built a large-scale composite\ndataset called SketchyCOCO to support and evaluate the solution. We validate\nour approach on the tasks of both object-level and scene-level image generation\non SketchyCOCO. Through quantitative, qualitative results, human evaluation and\nablation studies, we demonstrate the method's capacity to generate realistic\ncomplex scene-level images from various freehand sketches.\n", "rewritten_text": "This paper introduces the first method for automatically generating images from freehand scene-level sketches.  Our model enables controllable image synthesis by specifying the desired output via these sketches.  The core contribution is EdgeGAN, an attribute-vector-bridged Generative Adversarial Network (GAN) that generates high-quality object-level image content without requiring freehand sketches during training.  We created SketchyCOCO, a large-scale composite dataset, to support and evaluate our approach.  Experiments on SketchyCOCO demonstrate the method's effectiveness in both object-level and scene-level image generation.  Quantitative and qualitative results, along with human evaluation and ablation studies, confirm its ability to generate realistic, complex scene-level images from diverse freehand sketches.\n"}, "2409.00942": {"original_text": "  Normalizing flows, a category of probabilistic models famed for their\ncapabilities in modeling complex data distributions, have exhibited remarkable\nefficacy in unsupervised anomaly detection. This paper explores the potential\nof normalizing flows in multi-class anomaly detection, wherein the normal data\nis compounded with multiple classes without providing class labels. Through the\nintegration of vector quantization (VQ), we empower the flow models to\ndistinguish different concepts of multi-class normal data in an unsupervised\nmanner, resulting in a novel flow-based unified method, named VQ-Flow.\nSpecifically, our VQ-Flow leverages hierarchical vector quantization to\nestimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for\nconcept distinction and its concomitant Concept-Specific Pattern Codebook\n(CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow\nare conditioned on the concept-specific patterns captured in CSPC, capable of\nmodeling specific normal patterns associated with different concepts. Moreover,\nCPC further enables our VQ-Flow for concept-aware distribution modeling,\nfaithfully mimicking the intricate multi-class normal distribution through a\nmixed Gaussian distribution reparametrized on the conceptual prototypes.\nThrough the introduction of vector quantization, the proposed VQ-Flow advances\nthe state-of-the-art in multi-class anomaly detection within a unified training\nscheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase\nis publicly available at https://github.com/cool-xuan/vqflow.\n", "rewritten_text": "Normalizing flows, probabilistic models known for their ability to model complex data distributions, have demonstrated remarkable effectiveness in unsupervised anomaly detection.  This paper explores their application to multi-class anomaly detection, where unlabeled data comprises multiple normal classes.  We introduce a novel flow-based method, VQ-Flow, which leverages vector quantization (VQ) to distinguish these classes unsupervisedly.\n\nVQ-Flow employs hierarchical vector quantization to learn two codebooks: a Conceptual Prototype Codebook (CPC) for distinguishing concepts, and a Concept-Specific Pattern Codebook (CSPC) for capturing the patterns within each concept.  The flow models are conditioned on the CSPC, enabling them to model the specific normal patterns associated with each concept.  Furthermore, the CPC allows for concept-aware distribution modeling, approximating the complex multi-class normal distribution as a mixture of Gaussians parameterized by the conceptual prototypes.\n\nBy incorporating vector quantization, VQ-Flow advances the state-of-the-art in multi-class anomaly detection using a unified training scheme.  On the MVTec AD dataset, it achieves a Detection/Localization AUROC of 99.5%/98.3%. The code is publicly available at https://github.com/cool-xuan/vqflow.\n"}, "1708.00801": {"original_text": "  We study the impact of big models (in terms of the degree of lexicalization)\nand big data (in terms of the training corpus size) on dependency grammar\ninduction. We experimented with L-DMV, a lexicalized version of Dependency\nModel with Valence and L-NDMV, our lexicalized extension of the Neural\nDependency Model with Valence. We find that L-DMV only benefits from very small\ndegrees of lexicalization and moderate sizes of training corpora. L-NDMV can\nbenefit from big training data and lexicalization of greater degrees,\nespecially when enhanced with good model initialization, and it achieves a\nresult that is competitive with the current state-of-the-art.\n", "rewritten_text": "This study investigates the effects of model size (lexicalization degree) and training data size on dependency grammar induction.  We experimented with L-DMV, a lexicalized Dependency Model with Valence, and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence.  Results indicate that L-DMV benefits only from minimal lexicalization and moderately sized training corpora.  Conversely, L-NDMV demonstrates improved performance with larger training datasets and higher degrees of lexicalization, particularly when initialized effectively.  This approach achieves state-of-the-art results.\n"}, "2303.1309": {"original_text": "  Recent trends in semi-supervised learning have significantly boosted the\nperformance of 3D semi-supervised medical image segmentation. Compared with 2D\nimages, 3D medical volumes involve information from different directions, e.g.,\ntransverse, sagittal, and coronal planes, so as to naturally provide\ncomplementary views. These complementary views and the intrinsic similarity\namong adjacent 3D slices inspire us to develop a novel annotation way and its\ncorresponding semi-supervised model for effective segmentation. Specifically,\nwe firstly propose the orthogonal annotation by only labeling two orthogonal\nslices in a labeled volume, which significantly relieves the burden of\nannotation. Then, we perform registration to obtain the initial pseudo labels\nfor sparsely labeled volumes. Subsequently, by introducing unlabeled volumes,\nwe propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that\nexploits dense pseudo labels in early stage and sparse labels in later stage\nand meanwhile forces consistent output of two networks. Experimental results on\nthree benchmark datasets validated our effectiveness in performance and\nefficiency in annotation. For example, with only 10 annotated slices, our\nmethod reaches a Dice up to 86.93% on KiTS19 dataset.\n", "rewritten_text": "Recent advancements in semi-supervised learning have significantly improved the performance of 3D medical image segmentation.  Unlike 2D images, 3D medical volumes inherently incorporate information from multiple perspectives (e.g., transverse, sagittal, and coronal planes), providing complementary views.  This inherent complementarity and the intrinsic similarity between adjacent slices motivated the development of a novel annotation method and corresponding semi-supervised model for efficient segmentation.  Specifically, we propose orthogonal annotation, requiring labels for only two orthogonal slices within a labeled volume, substantially reducing annotation effort.  Initial pseudo-labels for sparsely labeled volumes are then generated via registration.  Finally, incorporating unlabeled volumes, we introduce a dual-network paradigm, Dense-Sparse Co-training (DeSCO), which leverages dense pseudo-labels in the early stages and sparse labels in later stages, while enforcing consistent outputs from both networks.  Experiments on three benchmark datasets demonstrate the method's effectiveness and annotation efficiency.  For instance, using only 10 annotated slices, our method achieved a Dice score of up to 86.93% on the KiTS19 dataset.\n"}, "2401.08123": {"original_text": "  Guided depth super-resolution (GDSR) involves restoring missing depth details\nusing the high-resolution RGB image of the same scene. Previous approaches have\nstruggled with the heterogeneity and complementarity of the multi-modal inputs,\nand neglected the issues of modal misalignment, geometrical misalignment, and\nfeature selection. In this study, we rethink some essential components in GDSR\nnetworks and propose a simple yet effective Dynamic Dual Alignment and\nAggregation network (D2A2). D2A2 mainly consists of 1) a dynamic dual alignment\nmodule that adapts to alleviate the modal misalignment via a learnable domain\nalignment block and geometrically align cross-modal features by learning the\noffset; and 2) a mask-to-pixel feature aggregate module that uses the gated\nmechanism and pixel attention to filter out irrelevant texture noise from RGB\nfeatures and combine the useful features with depth features. By combining the\nstrengths of RGB and depth features while minimizing disturbance introduced by\nthe RGB image, our method with simple reuse and redesign of basic components\nachieves state-of-the-art performance on multiple benchmark datasets. The code\nis available at https://github.com/JiangXinni/D2A2.\n", "rewritten_text": "Guided depth super-resolution (GDSR) aims to recover missing depth details using a corresponding high-resolution RGB image.  Previous methods have struggled with the heterogeneous and complementary nature of these multimodal inputs, often neglecting modal and geometric misalignment, as well as effective feature selection.  This study presents a novel, simple yet effective Dynamic Dual Alignment and Aggregation network (D2A2) that addresses these limitations.  D2A2 comprises: 1) a dynamic dual alignment module that mitigates modal misalignment using a learnable domain alignment block and corrects geometric misalignment by learning an offset between cross-modal features; and 2) a mask-to-pixel feature aggregation module that employs a gated mechanism and pixel attention to suppress irrelevant texture noise from the RGB features and selectively integrate useful information with the depth features.  By effectively combining RGB and depth features while minimizing RGB-induced noise, our method, through the reuse and redesign of basic network components, achieves state-of-the-art performance on multiple benchmark datasets. The code is available at https://github.com/JiangXinni/D2A2.\n"}, "2112.09414": {"original_text": "  By highlighting the regions of the input image that contribute the most to\nthe decision, saliency maps have become a popular method to make neural\nnetworks interpretable. In medical imaging, they are particularly well-suited\nto explain neural networks in the context of abnormality localization. However,\nfrom our experiments, they are less suited to classification problems where the\nfeatures that allow to distinguish between the different classes are spatially\ncorrelated, scattered and definitely non-trivial. In this paper we thus propose\na new paradigm for better interpretability. To this end we provide the user\nwith relevant and easily interpretable information so that he can form his own\nopinion. We use Disentangled Variational Auto-Encoders which latent\nrepresentation is divided into two components: the non-interpretable part and\nthe disentangled part. The latter accounts for the categorical variables\nexplicitly representing the different classes of interest. In addition to\nproviding the class of a given input sample, such a model offers the\npossibility to transform the sample from a given class to a sample of another\nclass, by modifying the value of the categorical variables in the latent\nrepresentation. This paves the way to easier interpretation of class\ndifferences. We illustrate the relevance of this approach in the context of\nautomatic sex determination from hip bones in forensic medicine. The features\nencoded by the model, that distinguish the different classes were found to be\nconsistent with expert knowledge.\n", "rewritten_text": "Saliency maps, which highlight the input image regions most influential on a neural network's decision, have become a popular method for improving model interpretability.  In medical imaging, they are particularly useful for explaining abnormality localization. However, our experiments demonstrate their limitations in classification problems where distinguishing features are spatially scattered, correlated, and complex.  This paper proposes a novel approach to enhance interpretability by providing users with relevant, easily understandable information to facilitate informed judgment.  We utilize Disentangled Variational Autoencoders (VAEs) whose latent representation is partitioned into interpretable and non-interpretable components. The interpretable component explicitly represents categorical variables corresponding to the classes of interest.  Beyond classifying input samples, this model allows transformation between classes by modifying the categorical variables in the latent space, thereby facilitating the interpretation of class differences.  We demonstrate this approach's effectiveness in automatically determining sex from hip bones in forensic medicine, where the model's encoded distinguishing features align with expert knowledge.\n"}, "1910.06431": {"original_text": "  There has been great success recently in tackling challenging NLP tasks by\nneural networks which have been pre-trained and fine-tuned on large amounts of\ntask data. In this paper, we investigate one such model, BERT for\nquestion-answering, with the aim to analyze why it is able to achieve\nsignificantly better results than other models. We run DeepLIFT on the model\npredictions and test the outcomes to monitor shift in the attention values for\ninput. We also cluster the results to analyze any possible patterns similar to\nhuman reasoning depending on the kind of input paragraph and question the model\nis trying to answer.\n", "rewritten_text": "Recent advancements in natural language processing (NLP) have leveraged pre-trained and fine-tuned neural networks to achieve significant success on challenging tasks.  This paper investigates BERT, a prominent question-answering model, to analyze its superior performance compared to other models.  We employ DeepLIFT to analyze model predictions, monitoring shifts in attention weights for input tokens.  Furthermore, we cluster the results to identify patterns potentially analogous to human reasoning, considering variations in input paragraphs and questions.\n"}, "2410.15277": {"original_text": "  Retrieval-augmented generation (RAG) can supplement large language models\n(LLMs) by integrating external knowledge. However, as the number of retrieved\ndocuments increases, the input length to LLMs grows linearly, causing a\ndramatic increase in latency and a degradation in long-context understanding.\nThis is particularly serious for multi-hop questions that require a chain of\nreasoning across documents. To accelerate inference, reduce costs, and minimize\ndistractions, this paper presents BRIEF (Bridging Retrieval and Inference\nthrough Evidence Fusion), a lightweight approach that performs query-aware\nmulti-hop reasoning by compressing retrieved documents into highly dense\ntextual summaries to integrate into in-context learning. To enable learning\ncompression for multi-hop reasoning, we curate synthetic data by extracting\natomic proposition expressions that encapsulate distinct factoids from the\nsource documents to compose synthetic summaries. Based on our synthetic data\nbuilt entirely by open-source models, BRIEF generates more concise summaries\nand enables a range of LLMs to achieve exceptional open-domain question\nanswering (QA) performance. For example, on HotpotQA, BRIEF improves the\ncompression rate by 2 times compared to the state-of-the-art baseline, while\noutperforming it by 3.00% EM and 4.16% F1 with Flan-UL2 as the reader LM. It\nalso generates more concise summaries than proprietary GPT-3.5, while\ndemonstrating nearly identical QA performance.\n", "rewritten_text": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge.  However, increasing the number of retrieved documents linearly increases LLM input length, leading to significant latency and degraded long-context understanding. This problem is particularly acute for multi-hop questions requiring chained reasoning across multiple documents.  To address this, we introduce BRIEF (Bridging Retrieval and Inference through Evidence Fusion), a lightweight approach that enables query-aware multi-hop reasoning. BRIEF compresses retrieved documents into highly dense textual summaries for efficient in-context learning.  To facilitate learning this compression for multi-hop reasoning, we generate synthetic training data by extracting atomic proposition expressions\u2014representing distinct factoids\u2014from source documents and composing them into synthetic summaries.  Using this synthetic data, generated entirely by open-source models, BRIEF produces more concise summaries, enabling various LLMs to achieve superior open-domain question answering (QA) performance.  For instance, on the HotpotQA benchmark, BRIEF doubles the compression rate compared to the state-of-the-art baseline, while simultaneously improving performance by 3.00% exact match (EM) and 4.16% F1-score using Flan-UL2 as the reader LLM.  Furthermore, BRIEF generates more concise summaries than proprietary GPT-3.5, achieving nearly identical QA performance.\n"}, "2403.17512": {"original_text": "  Improving the efficiency of current neural networks and modeling them in\nbiological neural systems have become popular research directions in recent\nyears. Pulse-coupled neural network (PCNN) is a well applicated model for\nimitating the computation characteristics of the human brain in computer vision\nand neural network fields. However, differences between the PCNN and biological\nneural systems remain: limited neural connection, high computational cost, and\nlack of stochastic property. In this study, random-coupled neural network\n(RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic\ncomputing via a random inactivation process. This process randomly closes some\nneural connections in the RCNN model, realized by the random inactivation\nweight matrix of link input. This releases the computational burden of PCNN,\nmaking it affordable to achieve vast neural connections. Furthermore, the image\nand video processing mechanisms of RCNN are researched. It encodes constant\nstimuli as periodic spike trains and periodic stimuli as chaotic spike trains,\nthe same as biological neural information encoding characteristics. Finally,\nthe RCNN is applicated to image segmentation, fusion, and pulse shape\ndiscrimination subtasks. It is demonstrated to be robust, efficient, and highly\nanti-noised, with outstanding performance in all applications mentioned above.\n", "rewritten_text": "Recent years have witnessed a surge in research focused on improving the efficiency of neural networks and modeling them after biological neural systems.  The pulse-coupled neural network (PCNN) has been widely applied as a model mimicking the computational characteristics of the human brain, particularly in computer vision and neural network research. However, limitations remain, including restricted neural connectivity, high computational cost, and a lack of stochasticity.  This study proposes a random-coupled neural network (RCNN) to address these shortcomings.  The RCNN employs a random inactivation process, implemented via a random inactivation weight matrix, to selectively disable neural connections. This reduces the computational burden of the PCNN, enabling the use of significantly larger networks.  Furthermore, we investigated the RCNN's image and video processing mechanisms, finding that it encodes constant stimuli as periodic spike trains and periodic stimuli as chaotic spike trains\u2014mirroring biological neural information encoding.  Finally, the RCNN was applied to image segmentation, fusion, and pulse shape discrimination, demonstrating robustness, efficiency, high noise immunity, and superior performance across all tasks.\n"}, "1907.0916": {"original_text": "  Facial Micro-Expressions (MEs) are spontaneous, involuntary facial movements\nwhen a person experiences an emotion but deliberately or unconsciously attempts\nto conceal his or her genuine emotions. Recently, ME recognition has attracted\nincreasing attention due to its potential applications such as clinical\ndiagnosis, business negotiation, interrogations, and security. However, it is\nexpensive to build large scale ME datasets, mainly due to the difficulty of\ninducing spontaneous MEs. This limits the application of deep learning\ntechniques which require lots of training data. In this paper, we propose a\nsimple, efficient yet robust descriptor called Extended Local Binary Patterns\non Three Orthogonal Planes (ELBPTOP) for ME recognition. ELBPTOP consists of\nthree complementary binary descriptors: LBPTOP and two novel ones Radial\nDifference LBPTOP (RDLBPTOP) and Angular Difference LBPTOP (ADLBPTOP), which\nexplore the local second order information along the radial and angular\ndirections contained in ME video sequences. ELBPTOP is a novel ME descriptor\ninspired by unique and subtle facial movements. It is computationally efficient\nand only marginally increases the cost of computing LBPTOP, yet is extremely\neffective for ME recognition. In addition, by firstly introducing Whitened\nPrincipal Component Analysis (WPCA) to ME recognition, we can further obtain\nmore compact and discriminative feature representations, then achieve\nsignificantly computational savings. Extensive experimental evaluation on three\npopular spontaneous ME datasets SMIC, CASME II and SAMM show that our proposed\nELBPTOP approach significantly outperforms the previous state-of-the-art on all\nthree single evaluated datasets and achieves promising results on\ncross-database recognition.Our code will be made available.\n", "rewritten_text": "Facial micro-expressions (MEs) are spontaneous, involuntary facial movements that occur when a person experiences an emotion but attempts to conceal it, either deliberately or unconsciously.  The potential applications of ME recognition\u2014including clinical diagnosis, business negotiations, interrogations, and security\u2014have spurred increasing interest. However, the high cost of creating large-scale ME datasets, primarily due to the difficulty of eliciting spontaneous MEs, limits the application of data-intensive deep learning techniques.  This paper proposes a simple, efficient, and robust descriptor for ME recognition: Extended Local Binary Patterns on Three Orthogonal Planes (ELBPTOP).  ELBPTOP comprises three complementary binary descriptors: LBPTOP and two novel descriptors, Radial Difference LBPTOP (RDLBPTOP) and Angular Difference LBPTOP (ADLBPTOP), which leverage second-order local information along the radial and angular directions within ME video sequences.  Inspired by the unique and subtle movements characteristic of MEs, ELBPTOP is computationally efficient, adding only marginally to the cost of computing LBPTOP while significantly improving ME recognition accuracy.  Furthermore, by incorporating Whitened Principal Component Analysis (WPCA) \u2013 a novel application in ME recognition \u2013 we achieve more compact and discriminative feature representations, resulting in substantial computational savings.  Extensive experimental evaluation on three widely used spontaneous ME datasets (SMIC, CASME II, and SAMM) demonstrates that our proposed ELBPTOP approach significantly outperforms the state-of-the-art on each dataset individually and achieves promising results in cross-database recognition.  Our code will be made publicly available.\n"}, "2402.11431": {"original_text": "  To address the issue of increased triangulation uncertainty caused by\nselecting views with small camera baselines in Structure from Motion (SFM) view\nselection, this paper proposes a robust error-resistant view selection method.\nThe method utilizes a triangulation-based computation to obtain an\nerror-resistant model, which is then used to construct an error-resistant\nmatrix. The sorting results of each row in the error-resistant matrix determine\nthe candidate view set for each view. By traversing the candidate view sets of\nall views and completing the missing views based on the error-resistant matrix,\nthe integrity of 3D reconstruction is ensured. Experimental comparisons between\nthis method and the exhaustive method with the highest accuracy in the COLMAP\nprogram are conducted in terms of average reprojection error and absolute\ntrajectory error in the reconstruction results. The proposed method\ndemonstrates an average reduction of 29.40% in reprojection error accuracy and\n5.07% in absolute trajectory error on the TUM dataset and DTU dataset.\n", "rewritten_text": "This paper presents a robust, error-resistant view selection method for Structure from Motion (SFM) to mitigate the increased triangulation uncertainty associated with short camera baselines.  The method employs a triangulation-based computation to generate an error-resistant matrix.  Each row's sorted values within this matrix define a candidate view set for the corresponding view.  By iteratively traversing these candidate sets and filling in missing views based on the error-resistant matrix, the method ensures the integrity of the 3D reconstruction.  Comparative experiments against COLMAP's exhaustive, highest-accuracy method were conducted on the TUM and DTU datasets, measuring average reprojection error and absolute trajectory error.  Results show an average reduction of 29.40% in reprojection error and 5.07% in absolute trajectory error using the proposed method.\n"}, "2306.12693": {"original_text": "  This paper gives an Indic-to-Indic (IL-IL) MNMT baseline model for 11 ILs\nimplemented on the Samanantar corpus and analyzed on the Flores-200 corpus. All\nthe models are evaluated using the BLEU score. In addition, the languages are\nclassified under three groups namely East Indo- Aryan (EI), Dravidian (DR), and\nWest Indo-Aryan (WI). The effect of language relatedness on MNMT model\nefficiency is studied. Owing to the presence of large corpora from English (EN)\nto ILs, MNMT IL-IL models using EN as a pivot are also built and examined. To\nachieve this, English- Indic (EN-IL) models are also developed, with and\nwithout the usage of related languages. Results reveal that using related\nlanguages is beneficial for the WI group only, while it is detrimental for the\nEI group and shows an inconclusive effect on the DR group, but it is useful for\nEN-IL models. Thus, related language groups are used to develop pivot MNMT\nmodels. Furthermore, the IL corpora are transliterated from the corresponding\nscripts to a modified ITRANS script, and the best MNMT models from the previous\napproaches are built on the transliterated corpus. It is observed that the\nusage of pivot models greatly improves MNMT baselines with AS-TA achieving the\nminimum BLEU score and PA-HI achieving the maximum score. Among languages, AS,\nML, and TA achieve the lowest BLEU score, whereas HI, PA, and GU perform the\nbest. Transliteration also helps the models with few exceptions. The best\nincrement of scores is observed in ML, TA, and BN and the worst average\nincrement is observed in KN, HI, and PA, across all languages. The best model\nobtained is the PA-HI language pair trained on PAWI transliterated corpus which\ngives 24.29 BLEU.\n", "rewritten_text": "This paper presents a baseline Indic-to-Indic (IL-IL) machine neural machine translation (MNMT) model for eleven Indic languages (ILs).  The model was trained on the Samanantar corpus and evaluated on the Flores-200 corpus using BLEU score.  The eleven languages were grouped into three families: East Indo-Aryan (EI), Dravidian (DR), and West Indo-Aryan (WI).  The impact of language relatedness on MNMT model performance was investigated.  Given the availability of large English (EN)-IL corpora, pivot MNMT models using English as an intermediary language were also developed and analyzed.  These EN-IL models were trained both with and without related languages.\n\nResults indicate that incorporating related languages benefited only the WI group, while hindering performance for the EI group and showing no clear effect on the DR group.  However, using related languages proved beneficial for the EN-IL models.  Consequently, related language groups were utilized to develop pivot MNMT models.  Furthermore, the IL corpora were transliterated from their native scripts to a modified ITRANS script, and the best MNMT models from the previous approaches were retrained on this transliterated data.\n\nUsing pivot models significantly improved the baseline MNMT performance.  The lowest BLEU score was achieved by the Assamese-Tamil (AS-TA) pair, while the highest was achieved by the Punjabi-Hindi (PA-HI) pair.  Assamese (AS), Malayalam (ML), and Tamil (TA) consistently achieved the lowest BLEU scores, whereas Hindi (HI), Punjabi (PA), and Gujarati (GU) performed best.  Transliteration generally improved model performance, with notable exceptions.  The largest BLEU score improvements were observed for Malayalam (ML), Tamil (TA), and Bengali (BN), while the smallest average improvements were seen for Kannada (KN), Hindi (HI), and Punjabi (PA).  The best-performing model was the PA-HI language pair trained on the transliterated PA-WI corpus, achieving a BLEU score of 24.29.\n"}, "2403.08487": {"original_text": "  Diffusion models pose risks of privacy breaches and copyright disputes,\nprimarily stemming from the potential utilization of unauthorized data during\nthe training phase. The Training Membership Inference (TMI) task aims to\ndetermine whether a specific sample has been used in the training process of a\ntarget model, representing a critical tool for privacy violation verification.\nHowever, the increased stochasticity inherent in diffusion renders traditional\nshadow-model-based or metric-based methods ineffective when applied to\ndiffusion models. Moreover, existing methods only yield binary classification\nlabels which lack necessary comprehensibility in practical applications. In\nthis paper, we explore a novel perspective for the TMI task by leveraging the\nintrinsic generative priors within the diffusion model. Compared with unseen\nsamples, training samples exhibit stronger generative priors within the\ndiffusion model, enabling the successful reconstruction of substantially\ndegraded training images. Consequently, we propose the Degrade Restore Compare\n(DRC) framework. In this framework, an image undergoes sequential degradation\nand restoration, and its membership is determined by comparing it with the\nrestored counterpart. Experimental results verify that our approach not only\nsignificantly outperforms existing methods in terms of accuracy but also\nprovides comprehensible decision criteria, offering evidence for potential\nprivacy violations.\n", "rewritten_text": "Diffusion models present significant risks of privacy breaches and copyright infringement, primarily due to the potential use of unauthorized data during training.  Training Membership Inference (TMI) aims to determine whether a specific data sample was used to train a target model, serving as a crucial tool for verifying privacy violations.  However, the inherent stochasticity of diffusion models renders traditional shadow-model and metric-based TMI methods ineffective.  Furthermore, existing methods produce only binary classifications, lacking the necessary interpretability for practical applications.\n\nThis paper introduces a novel approach to the TMI task by leveraging the intrinsic generative priors within diffusion models.  Training samples exhibit stronger generative priors than unseen samples, allowing for the substantially more successful reconstruction of degraded training images.  We propose the Degrade-Restore-Compare (DRC) framework, which sequentially degrades and restores an image, determining its membership based on a comparison with the restored counterpart.  Experimental results demonstrate that our approach significantly surpasses existing methods in accuracy while providing interpretable decision criteria, offering compelling evidence of potential privacy violations.\n"}, "2309.03390": {"original_text": "  In this paper, we seek a new method in designing an iris recognition system.\nIn this method, first the Haar wavelet features are extracted from iris images.\nThe advantage of using these features is the high-speed extraction, as well as\nbeing unique to each iris. Then the back propagation neural network (BPNN) is\nused as a classifier. In this system, the BPNN parallel algorithms and their\nimplementation on GPUs have been used by the aid of CUDA in order to speed up\nthe learning process. Finally, the system performance and the speeding outcomes\nin a way that this algorithm is done in series are presented.\n", "rewritten_text": "This paper proposes a novel iris recognition system.  The system first extracts Haar wavelet features from iris images, leveraging their speed and uniqueness.  A backpropagation neural network (BPNN), implemented using CUDA-accelerated parallel algorithms on GPUs, then classifies these features.  Finally, the system's performance is evaluated, comparing its speed to a serial implementation.\n"}, "2311.10436": {"original_text": "  Since their inception, embeddings have become a primary ingredient in many\nflavours of Natural Language Processing (NLP) tasks supplanting earlier types\nof representation. Even though multilingual embeddings have been used for the\nincreasing number of multilingual tasks, due to the scarcity of parallel\ntraining data, low-resource languages such as Sinhala, tend to focus more on\nmonolingual embeddings. Then when it comes to the aforementioned multi-lingual\ntasks, it is challenging to utilize these monolingual embeddings given that\neven if the embedding spaces have a similar geometric arrangement due to an\nidentical training process, the embeddings of the languages considered are not\naligned. This is solved by the embedding alignment task. Even in this,\nhigh-resource language pairs are in the limelight while low-resource languages\nsuch as Sinhala which is in dire need of help seem to have fallen by the\nwayside. In this paper, we try to align Sinhala and English word embedding\nspaces based on available alignment techniques and introduce a benchmark for\nSinhala language embedding alignment. In addition to that, to facilitate the\nsupervised alignment, as an intermediate task, we also introduce\nSinhala-English alignment datasets. These datasets serve as our anchor datasets\nfor supervised word embedding alignment. Even though we do not obtain results\ncomparable to the high-resource languages such as French, German, or Chinese,\nwe believe our work lays the groundwork for more specialized alignment between\nEnglish and Sinhala embeddings.\n", "rewritten_text": "Since their inception, word embeddings have become a cornerstone of many natural language processing (NLP) tasks, largely replacing earlier representation methods.  While multilingual embeddings are increasingly used for multilingual tasks, the scarcity of parallel training data hinders their application to low-resource languages like Sinhala, which often rely on monolingual embeddings.  This poses a challenge for multilingual tasks because, even with similar training processes and potentially similar geometric arrangements in embedding spaces, the embeddings of different languages are not aligned.  Embedding alignment addresses this issue. However, research predominantly focuses on high-resource language pairs, neglecting low-resource languages such as Sinhala.\n\nThis paper addresses this gap by aligning Sinhala and English word embedding spaces using existing alignment techniques and introducing a benchmark for Sinhala embedding alignment.  Furthermore, to facilitate supervised alignment, we introduce Sinhala-English alignment datasets, serving as anchor datasets for this task.  Although our results do not match those achieved with high-resource languages like French, German, or Chinese, we believe this work provides a foundation for more specialized English-Sinhala embedding alignment.\n"}, "2301.08245": {"original_text": "  Estimating depth from images nowadays yields outstanding results, both in\nterms of in-domain accuracy and generalization. However, we identify two main\nchallenges that remain open in this field: dealing with non-Lambertian\nmaterials and effectively processing high-resolution images. Purposely, we\npropose a novel dataset that includes accurate and dense ground-truth labels at\nhigh resolution, featuring scenes containing several specular and transparent\nsurfaces. Our acquisition pipeline leverages a novel deep space-time stereo\nframework, enabling easy and accurate labeling with sub-pixel precision. The\ndataset is composed of 606 samples collected in 85 different scenes, each\nsample includes both a high-resolution pair (12 Mpx) as well as an unbalanced\nstereo pair (Left: 12 Mpx, Right: 1.1 Mpx), typical of modern mobile devices\nthat mount sensors with different resolutions. Additionally, we provide\nmanually annotated material segmentation masks and 15K unlabeled samples. The\ndataset is composed of a train set and two test sets, the latter devoted to the\nevaluation of stereo and monocular depth estimation networks. Our experiments\nhighlight the open challenges and future research directions in this field.\n", "rewritten_text": "Modern image-based depth estimation achieves outstanding results in both in-domain accuracy and generalization.  However, two key challenges remain: handling non-Lambertian materials and efficiently processing high-resolution images.  To address these, we introduce a novel dataset with accurate, dense, high-resolution ground truth labels, featuring scenes with numerous specular and transparent surfaces.  Our acquisition pipeline utilizes a novel deep space-time stereo framework, enabling easy and accurate sub-pixel labeling.\n\nThe dataset comprises 606 samples from 85 diverse scenes. Each sample includes a high-resolution stereo pair (12 MP) and an unbalanced stereo pair (left: 12 MP, right: 1.1 MP), mimicking the disparate resolutions of modern mobile device sensors.  We also provide manually annotated material segmentation masks and 15,000 unlabeled samples.  The dataset is divided into a training set and two test sets, the latter designed for evaluating both stereo and monocular depth estimation networks.  Our experiments illuminate persistent challenges and promising future research directions in this field.\n"}, "1709.00813": {"original_text": "  Various text analysis techniques exist, which attempt to uncover unstructured\ninformation from text. In this work, we explore using statistical dependence\nmeasures for textual classification, representing text as word vectors. Student\nsatisfaction scores on a 3-point scale and their free text comments written\nabout university subjects are used as the dataset. We have compared two textual\nrepresentations: a frequency word representation and term frequency\nrelationship to word vectors, and found that word vectors provide a greater\naccuracy. However, these word vectors have a large number of features which\naggravates the burden of computational complexity. Thus, we explored using a\nnon-linear dependency measure for feature selection by maximizing the\ndependence between the text reviews and corresponding scores. Our quantitative\nand qualitative analysis on a student satisfaction dataset shows that our\napproach achieves comparable accuracy to the full feature vector, while being\nan order of magnitude faster in testing. These text analysis and feature\nreduction techniques can be used for other textual data applications such as\nsentiment analysis.\n", "rewritten_text": "This work explores the use of statistical dependence measures for textual classification, leveraging word vector representations of text to analyze student satisfaction.  We utilized a dataset comprising student satisfaction scores (on a 3-point scale) and corresponding free-text comments about university subjects.  Two textual representations were compared: a simple frequency word representation and a term frequency-inverse document frequency (TF-IDF) word vector representation.  While word vectors yielded higher accuracy, their high dimensionality posed a computational challenge.  To address this, we employed a non-linear dependence measure for feature selection, maximizing the dependence between text reviews and satisfaction scores.  Our quantitative and qualitative analysis demonstrates that this approach achieves accuracy comparable to the full feature vector model while exhibiting a significant (order-of-magnitude) speed improvement during testing.  These text analysis and feature reduction techniques are readily applicable to other text-based applications, such as sentiment analysis.\n"}, "1704.0803": {"original_text": "  Some lung diseases are related to bronchial airway structures and morphology.\nAlthough airway segmentation from chest CT volumes is an important task in the\ncomputer-aided diagnosis and surgery assistance systems for the chest, complete\n3-D airway structure segmentation is a quite challenging task due to its\ncomplex tree-like structure. In this paper, we propose a new airway\nsegmentation method from 3D chest CT volumes based on volume of interests (VOI)\nusing gradient vector flow (GVF). This method segments the bronchial regions by\napplying the cavity enhancement filter (CEF) to trace the bronchial tree\nstructure from the trachea. It uses the CEF in the VOI to segment each branch.\nAnd a tube-likeness function based on GVF and the GVF magnitude map in each VOI\nare utilized to assist predicting the positions and directions of child\nbranches. By calculating the tube-likeness function based on GVF and the GVF\nmagnitude map, the airway-like candidate structures are identified and their\ncentrelines are extracted. Based on the extracted centrelines, we can detect\nthe branch points of the bifurcations and directions of the airway branches in\nthe next level. At the same time, a leakage detection is performed to avoid the\nleakage by analysing the pixel information and the shape information of airway\ncandidate regions extracted in the VOI. Finally, we unify all of the extracted\nbronchial regions to form an integrated airway tree. Preliminary experiments\nusing four cases of chest CT volumes demonstrated that the proposed method can\nextract more bronchial branches in comparison with other methods.\n", "rewritten_text": "Several lung diseases are associated with bronchial airway structure and morphology.  Accurate three-dimensional (3D) airway segmentation from chest computed tomography (CT) volumes is crucial for computer-aided diagnosis and surgical planning; however, this remains a challenging task due to the complex, tree-like nature of the airway system.  This paper presents a novel airway segmentation method for 3D chest CT volumes using a volume of interest (VOI)-based approach with gradient vector flow (GVF).  The method employs a cavity enhancement filter (CEF) to trace the bronchial tree structure from the trachea, segmenting each branch within its respective VOI.  A tube-likeness function, based on GVF and its magnitude map within each VOI, assists in predicting the position and orientation of daughter branches.  This function identifies airway-like candidate structures, extracts their centerlines, and facilitates the detection of bifurcation branch points and subsequent branch orientations.  A leakage detection mechanism, analyzing pixel and shape information of candidate airway regions within each VOI, prevents segmentation errors.  Finally, all extracted bronchial regions are integrated to form a complete airway tree.  Preliminary experiments on four chest CT volumes demonstrate that the proposed method extracts a greater number of bronchial branches compared to existing methods.\n"}, "1906.03249": {"original_text": "  Word embeddings are traditionally trained on a large corpus in an\nunsupervised setting, with no specific design for incorporating domain\nknowledge. This can lead to unsatisfactory performances when training data\noriginate from heterogeneous domains. In this paper, we propose two novel\nmechanisms for domain-aware word embedding training, namely domain indicator\nand domain attention, which integrate domain-specific knowledge into the widely\nused SG and CBOW models, respectively. The two methods are based on a joint\nlearning paradigm and ensure that words in a target domain are intensively\nfocused when trained on a source domain corpus. Qualitative and quantitative\nevaluation confirm the validity and effectiveness of our models. Compared to\nbaseline methods, our method is particularly effective in near-cold-start\nscenarios.\n", "rewritten_text": "Traditional word embedding methods, trained unsupervisedly on large corpora, lack explicit mechanisms for incorporating domain knowledge. This limitation often results in suboptimal performance when training data span heterogeneous domains.  This paper introduces two novel domain-aware word embedding training mechanisms: domain indicator and domain attention.  These methods integrate domain-specific knowledge into Skip-gram (SG) and Continuous Bag-of-Words (CBOW) models, respectively, using a joint learning paradigm.  This paradigm ensures focused attention on words within the target domain, even when training on a source domain corpus.  Qualitative and quantitative evaluations demonstrate the effectiveness and validity of our proposed models, particularly in near-cold-start scenarios, outperforming baseline methods.\n"}, "2305.17975": {"original_text": "  Automated assembly of 3D fractures is essential in orthopedics, archaeology,\nand our daily life. This paper presents Jigsaw, a novel framework for\nassembling physically broken 3D objects from multiple pieces. Our approach\nleverages hierarchical features of global and local geometry to match and align\nthe fracture surfaces. Our framework consists of four components: (1) front-end\npoint feature extractor with attention layers, (2) surface segmentation to\nseparate fracture and original parts, (3) multi-parts matching to find\ncorrespondences among fracture surface points, and (4) robust global alignment\nto recover the global poses of the pieces. We show how to jointly learn\nsegmentation and matching and seamlessly integrate feature matching and\nrigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and\nachieve superior performance compared to state-of-the-art methods. Our method\nalso generalizes well to diverse fracture modes, objects, and unseen instances.\nTo the best of our knowledge, this is the first learning-based method designed\nspecifically for 3D fracture assembly over multiple pieces. Our code is\navailable at https://jiaxin-lu.github.io/Jigsaw/.\n", "rewritten_text": "Automated assembly of 3D fractured objects is crucial in diverse fields, including orthopedics, archaeology, and everyday life.  This paper introduces Jigsaw, a novel framework for assembling physically broken 3D objects from multiple fragments.  Our approach leverages hierarchical features of both global and local geometry to match and align fracture surfaces.  The framework comprises four key components: (1) a front-end point feature extractor incorporating attention layers; (2) surface segmentation to delineate fracture surfaces from intact object surfaces; (3) multi-part matching to identify correspondences between fracture surface points; and (4) robust global alignment to recover the fragments' global poses.  We demonstrate a method for jointly learning segmentation and matching, seamlessly integrating feature matching with rigidity constraints.  Evaluation on the Breaking Bad dataset reveals superior performance compared to state-of-the-art methods.  Furthermore, our method exhibits strong generalization across diverse fracture modes, object types, and unseen instances.  To our knowledge, this is the first learning-based method specifically designed for the assembly of 3D fractured objects comprising multiple fragments.  Our code is available at https://jiaxin-lu.github.io/Jigsaw/.\n"}, "2312.08367": {"original_text": "  In this work, we propose an efficient Video-Language Alignment (ViLA)\nnetwork. Our ViLA model addresses both efficient frame sampling and effective\ncross-modal alignment in a unified way. In our ViLA network, we design a new\nlearnable text-guided Frame-Prompter together with a new cross-modal\ndistillation (QFormer-Distiller) module. Pre-trained large image-language\nmodels have shown promising results on problems such as visual question\nanswering (VQA). However, how to efficiently and effectively sample video\nframes when adapting pre-trained large image-language model to video-language\nalignment is still the major challenge. Compared with prior work, our ViLA\nmodel demonstrates the capability of selecting key frames with critical\ncontents, thus improving the video-language alignment accuracy while reducing\nthe inference latency +3.3% on NExT-QA Temporal with 3.0X speed up). Overall,\nour ViLA network outperforms the state-of-the-art methods on the video\nquestion-answering benchmarks: +4.6% on STAR Interaction, +2.2% on STAR average\nwith 3.0X speed up, ours 2-frames out-perform SeViLA 4-frames on the VLEP\ndataset with 4.2X speed-up. The code will be available at\nhttps://github.com/xijun-cs/ViLA.\n", "rewritten_text": "This work introduces ViLA (Video-Language Alignment), an efficient network for video-language alignment.  ViLA addresses efficient frame sampling and effective cross-modal alignment simultaneously.  Our approach incorporates a novel learnable text-guided Frame-Prompter and a cross-modal distillation module (QFormer-Distiller). While pre-trained large image-language models have shown promise in tasks like visual question answering (VQA), efficiently adapting them to video-language alignment, particularly regarding frame sampling, remains a significant challenge.  Compared to prior work, ViLA effectively selects key frames containing critical information, improving video-language alignment accuracy while significantly reducing inference latency.  Specifically, ViLA achieves a 3.3% accuracy improvement and 3.0x speedup on NExT-QA Temporal.  Furthermore, ViLA outperforms state-of-the-art methods on video question-answering benchmarks, achieving a 4.6% improvement on STAR Interaction, a 2.2% improvement on STAR average with a 3.0x speedup, and surpassing SeViLA's 4-frame performance on VLEP with a 2-frame model and a 4.2x speedup. The code is available at https://github.com/xijun-cs/ViLA.\n"}, "1809.10692": {"original_text": "  In this paper, we target the problem of fracture classification from clinical\nX-Ray images towards an automated Computer Aided Diagnosis (CAD) system.\nAlthough primarily dealing with an image classification problem, we argue that\nlocalizing the fracture in the image is crucial to make good class predictions.\nTherefore, we propose and thoroughly analyze several schemes for simultaneous\nfracture localization and classification. We show that using an auxiliary\nlocalization task, in general, improves the classification performance.\nMoreover, it is possible to avoid the need for additional localization\nannotations thanks to recent advancements in weakly-supervised deep learning\napproaches. Among such approaches, we investigate and adapt Spatial\nTransformers (ST), Self-Transfer Learning (STL), and localization from global\npooling layers. We provide a detailed quantitative and qualitative validation\non a dataset of 1347 femur fractures images and report high accuracy with\nregard to inter-expert correlation values reported in the literature. Our\ninvestigations show that i) lesion localization improves the classification\noutcome, ii) weakly-supervised methods improve baseline classification without\nany additional cost, iii) STL guides feature activations and boost performance.\nWe plan to make both the dataset and code available.\n", "rewritten_text": "This paper addresses automated fracture classification from clinical X-ray images for computer-aided diagnosis (CAD). While fundamentally an image classification problem, we demonstrate that accurate fracture localization is crucial for improved classification performance.  We propose and analyze several methods for simultaneous fracture localization and classification, showing that an auxiliary localization task generally enhances classification accuracy.  Furthermore, leveraging recent advances in weakly-supervised deep learning, we eliminate the need for additional localization annotations.  Specifically, we investigate and adapt spatial transformers (STs), self-transfer learning (STL), and localization from global pooling layers.  Our approach is validated quantitatively and qualitatively on a dataset of 1347 femur fracture images, achieving high accuracy comparable to inter-expert agreement reported in the literature.  Our findings indicate that: (i) lesion localization improves classification; (ii) weakly-supervised methods enhance baseline classification without added annotation costs; and (iii) STL effectively guides feature activations, boosting performance.  The dataset and code will be made publicly available.\n"}, "2309.16039": {"original_text": "  We present a series of long-context LLMs that support effective context\nwindows of up to 32,768 tokens. Our model series are built through continual\npretraining from Llama 2 with longer training sequences and on a dataset where\nlong texts are upsampled. We perform extensive evaluation on language modeling,\nsynthetic context probing tasks, and a wide range of research benchmarks. On\nresearch benchmarks, our models achieve consistent improvements on most regular\ntasks and significant improvements on long-context tasks over Llama 2. Notably,\nwith a cost-effective instruction tuning procedure that does not require\nhuman-annotated long instruction data, the 70B variant can already surpass\ngpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.\nAlongside these results, we provide an in-depth analysis on the individual\ncomponents of our method. We delve into Llama's position encodings and discuss\nits limitation in modeling long dependencies. We also examine the impact of\nvarious design choices in the pretraining process, including the data mix and\nthe training curriculum of sequence lengths -- our ablation experiments suggest\nthat having abundant long texts in the pretrain dataset is not the key to\nachieving strong performance, and we empirically verify that long context\ncontinual pretraining is more efficient and similarly effective compared to\npretraining from scratch with long sequences.\n", "rewritten_text": "We introduce a series of long-context large language models (LLMs) with effective context windows of up to 32,768 tokens.  These models are built by continually pre-training Llama 2 with longer training sequences and a dataset oversampling long texts.  We conducted extensive evaluations on language modeling, synthetic context probing tasks, and a diverse set of research benchmarks.  Our models consistently outperform Llama 2 on most standard tasks and show significant improvements on long-context tasks.  Remarkably, a cost-effective instruction tuning procedure, requiring no human-annotated long instruction data, enables our 70B parameter model to surpass GPT-3.5-turbo-16k's overall performance on a suite of long-context benchmarks.  Furthermore, we provide an in-depth analysis of our methodology, examining Llama 2's positional encodings and their limitations in modeling long dependencies. We also investigate the impact of various pre-training design choices, including data composition and the training sequence length curriculum.  Our ablation studies indicate that abundant long texts in the pre-training dataset are not crucial for strong performance.  We empirically demonstrate that long-context continual pre-training is more efficient and equally effective compared to training from scratch with long sequences.\n"}, "2403.02211": {"original_text": "  Popular methods usually use a degradation model in a supervised way to learn\na watermark removal model. However, it is true that reference images are\ndifficult to obtain in the real world, as well as collected images by cameras\nsuffer from noise. To overcome these drawbacks, we propose a perceptive\nself-supervised learning network for noisy image watermark removal (PSLNet) in\nthis paper. PSLNet depends on a parallel network to remove noise and\nwatermarks. The upper network uses task decomposition ideas to remove noise and\nwatermarks in sequence. The lower network utilizes the degradation model idea\nto simultaneously remove noise and watermarks. Specifically, mentioned paired\nwatermark images are obtained in a self supervised way, and paired noisy images\n(i.e., noisy and reference images) are obtained in a supervised way. To enhance\nthe clarity of obtained images, interacting two sub-networks and fusing\nobtained clean images are used to improve the effects of image watermark\nremoval in terms of structural information and pixel enhancement. Taking into\ntexture information account, a mixed loss uses obtained images and features to\nachieve a robust model of noisy image watermark removal. Comprehensive\nexperiments show that our proposed method is very effective in comparison with\npopular convolutional neural networks (CNNs) for noisy image watermark removal.\nCodes can be obtained at https://github.com/hellloxiaotian/PSLNet.\n", "rewritten_text": "Existing watermark removal methods typically employ supervised learning with a degradation model.  However, obtaining clean reference images is challenging due to real-world noise inherent in captured images.  To address this, we propose PSLNet (Perceptive Self-Supervised Learning Network), a novel architecture for noisy image watermark removal.  PSLNet uses a parallel network structure: an upper branch employing sequential noise and watermark removal via task decomposition, and a lower branch leveraging a degradation model for simultaneous removal.  Crucially, paired watermark images are generated self-supervisedly, while paired noisy/clean images are obtained through supervised learning.  To enhance image clarity, the outputs of both sub-networks are fused, improving structural information and pixel fidelity.  A mixed loss function, incorporating texture information from both images and features, ensures robustness.  Extensive experiments demonstrate PSLNet's superior performance compared to popular convolutional neural networks (CNNs) for noisy image watermark removal.  The code is available at https://github.com/hellloxiaotian/PSLNet.\n"}, "2104.09386": {"original_text": "  Mapping a single exposure low dynamic range (LDR) image into a high dynamic\nrange (HDR) is considered among the most strenuous image to image translation\ntasks due to exposure-related missing information. This study tackles the\nchallenges of single-shot LDR to HDR mapping by proposing a novel two-stage\ndeep network. Notably, our proposed method aims to reconstruct an HDR image\nwithout knowing hardware information, including camera response function (CRF)\nand exposure settings. Therefore, we aim to perform image enhancement task like\ndenoising, exposure correction, etc., in the first stage. Additionally, the\nsecond stage of our deep network learns tone mapping and bit-expansion from a\nconvex set of data samples. The qualitative and quantitative comparisons\ndemonstrate that the proposed method can outperform the existing LDR to HDR\nworks with a marginal difference. Apart from that, we collected an LDR image\ndataset incorporating different camera systems. The evaluation with our\ncollected real-world LDR images illustrates that the proposed method can\nreconstruct plausible HDR images without presenting any visual artefacts. Code\navailable: https://github. com/sharif-apu/twostageHDR_NTIRE21.\n", "rewritten_text": "Mapping a single low dynamic range (LDR) image to a high dynamic range (HDR) image is a challenging image-to-image translation task due to the loss of information inherent in the single exposure. This study addresses this challenge by proposing a novel two-stage deep network for single-shot LDR-to-HDR mapping.  Our method uniquely reconstructs HDR images without requiring knowledge of hardware parameters, such as the camera response function (CRF) and exposure settings.  The first stage performs image enhancement tasks, including denoising and exposure correction. The second stage learns tone mapping and bit-expansion from a convex set of training data.  Qualitative and quantitative comparisons demonstrate that our method achieves performance comparable to existing LDR-to-HDR methods.  Furthermore, we have created a new dataset of LDR images captured with various camera systems.  Evaluation on this real-world dataset shows that our method reconstructs plausible HDR images without noticeable artifacts. Code is available at: https://github.com/sharif-apu/twostageHDR_NTIRE21.\n"}, "2409.09170": {"original_text": "  The diagnosis and treatment of individuals with communication disorders\noffers many opportunities for the application of speech technology, but\nresearch so far has not adequately considered: the diversity of conditions, the\nrole of pragmatic deficits, and the challenges of limited data. This paper\nexplores how a general-purpose model of perceived pragmatic similarity may\novercome these limitations. It explains how it might support several use cases\nfor clinicians and clients, and presents evidence that a simple model can\nprovide value, and in particular can capture utterance aspects that are\nrelevant to diagnoses of autism and specific language impairment.\n", "rewritten_text": "Speech technology offers significant opportunities in diagnosing and treating communication disorders. However, existing research inadequately addresses the diversity of these conditions, the impact of pragmatic deficits, and the challenges posed by limited data. This paper explores how a general-purpose model of perceived pragmatic similarity can mitigate these limitations.  We demonstrate its potential to support various clinical applications for both clinicians and clients, providing evidence that even a simple model can offer valuable insights, particularly in capturing utterance features relevant to diagnosing autism spectrum disorder and specific language impairment.\n"}, "2309.04462": {"original_text": "  Real-world application of chest X-ray abnormality classification requires\ndealing with several challenges: (i) limited training data; (ii) training and\nevaluation sets that are derived from different domains; and (iii) classes that\nappear during training may have partial overlap with classes of interest during\nevaluation. To address these challenges, we present an integrated framework\ncalled Generalized Cross-Domain Multi-Label Few-Shot Learning (GenCDML-FSL).\nThe framework supports overlap in classes during training and evaluation,\ncross-domain transfer, adopts meta-learning to learn using few training\nsamples, and assumes each chest X-ray image is either normal or associated with\none or more abnormalities. Furthermore, we propose Generalized Episodic\nTraining (GenET), a training strategy that equips models to operate with\nmultiple challenges observed in the GenCDML-FSL scenario. Comparisons with\nwell-established methods such as transfer learning, hybrid transfer learning,\nand multi-label meta-learning on multiple datasets show the superiority of our\napproach.\n", "rewritten_text": "Real-world application of chest X-ray abnormality classification faces several challenges: limited training data, domain discrepancies between training and evaluation sets, and partial overlap between training and evaluation classes.  To address these, we introduce GenCDML-FSL (Generalized Cross-Domain Multi-Label Few-Shot Learning), an integrated framework that explicitly handles class overlap, enables cross-domain transfer, and employs meta-learning for effective few-shot learning.  Assuming each chest X-ray image is either normal or exhibits one or more abnormalities, we further propose Generalized Episodic Training (GenET), a novel training strategy designed to mitigate the challenges inherent in the GenCDML-FSL setting.  Comparative evaluations against established methods\u2014including transfer learning, hybrid transfer learning, and multi-label meta-learning\u2014across multiple datasets demonstrate the superiority of our approach.\n"}, "1811.071": {"original_text": "  Few-shot deep learning is a topical challenge area for scaling visual\nrecognition to open ended growth of unseen new classes with limited labeled\nexamples. A promising approach is based on metric learning, which trains a deep\nembedding to support image similarity matching. Our insight is that effective\ngeneral purpose matching requires non-linear comparison of features at multiple\nabstraction levels. We thus propose a new deep comparison network comprised of\nembedding and relation modules that learn multiple non-linear distance metrics\nbased on different levels of features simultaneously. Furthermore, to reduce\nover-fitting and enable the use of deeper embeddings, we represent images as\ndistributions rather than vectors via learning parameterized Gaussian noise\nregularization. The resulting network achieves excellent performance on both\nminiImageNet and tieredImageNet.\n", "rewritten_text": "Scaling visual recognition to accommodate the open-ended growth of unseen classes with limited labeled examples remains a significant challenge in few-shot deep learning.  A promising approach leverages metric learning, training a deep embedding to facilitate image similarity matching.  We hypothesize that effective, general-purpose matching necessitates non-linear feature comparisons across multiple abstraction levels.  Therefore, we propose a novel deep comparison network comprising embedding and relation modules that simultaneously learn multiple non-linear distance metrics based on diverse feature levels.  To mitigate overfitting and enable the use of deeper embeddings, we represent images as distributions, rather than vectors, employing parameterized Gaussian noise regularization.  This network achieves state-of-the-art performance on both miniImageNet and tieredImageNet.\n"}, "2306.07279": {"original_text": "  We introduce Cap3D, an automatic approach for generating descriptive text for\n3D objects. This approach utilizes pretrained models from image captioning,\nimage-text alignment, and LLM to consolidate captions from multiple views of a\n3D asset, completely side-stepping the time-consuming and costly process of\nmanual annotation. We apply Cap3D to the recently introduced large-scale 3D\ndataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted\nusing 41k human annotations from the same dataset, demonstrates that Cap3D\nsurpasses human-authored descriptions in terms of quality, cost, and speed.\nThrough effective prompt engineering, Cap3D rivals human performance in\ngenerating geometric descriptions on 17k collected annotations from the ABO\ndataset. Finally, we finetune Text-to-3D models on Cap3D and human captions,\nand show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E,\nand DreamFusion.\n", "rewritten_text": "We introduce Cap3D, an automated approach for generating descriptive text for 3D objects.  Leveraging pretrained models for image captioning, image-text alignment, and large language models (LLMs), Cap3D consolidates captions from multiple views of a 3D asset, eliminating the need for time-consuming and costly manual annotation.  Applied to the large-scale Objaverse 3D dataset, Cap3D generated 660,000 3D-text pairs.  Evaluation against 41,000 human annotations from the same dataset demonstrates that Cap3D surpasses human-authored descriptions in quality, cost, and speed.  Furthermore, through effective prompt engineering, Cap3D achieves performance comparable to humans in generating geometric descriptions, as evidenced by evaluation on 17,000 annotations from the ABO dataset. Finally, we fine-tuned text-to-3D models using both Cap3D-generated and human-authored captions, demonstrating that Cap3D outperforms state-of-the-art methods, including Point-E, Shape-E, and DreamFusion.\n"}, "2311.08469": {"original_text": "  Language technologies that accurately model the dynamics of events must\nperform commonsense reasoning. Existing work evaluating commonsense reasoning\nfocuses on making inferences about common, everyday situations. To instead\ninvestigate the ability to model unusual, unexpected, and unlikely situations,\nwe explore the task of uncommonsense abductive reasoning. Given a piece of\ncontext with an unexpected outcome, this task requires reasoning abductively to\ngenerate an explanation that makes the unexpected outcome more likely in the\ncontext. To this end, we curate and release a new English language corpus\ncalled UNcommonsense. We characterize the performance differences between human\nexplainers and the best-performing large language models, finding that\nmodel-enhanced human-written explanations achieve the highest quality by\ntrading off between specificity and diversity. Finally, we experiment with\nseveral imitation learning algorithms to train open and accessible language\nmodels on this task. When compared with the vanilla supervised fine-tuning\napproach, these methods consistently reduce lose rates on both common and\nuncommonsense abductive reasoning judged by human evaluators.\n", "rewritten_text": "Accurate modeling of event dynamics in language technologies requires commonsense reasoning.  Existing evaluations of commonsense reasoning primarily focus on common, everyday scenarios.  To investigate the ability to model unusual and unexpected situations, we introduce the task of uncommonsense abductive reasoning.  Given a context with an unexpected outcome, this task requires abductive reasoning to generate an explanation that increases the likelihood of that outcome within the context.  To facilitate this research, we curated and released UNcommonsense, a new English language corpus.  We analyzed the performance differences between human explainers and state-of-the-art large language models (LLMs), finding that model-enhanced human explanations achieved the highest quality by balancing specificity and diversity.  Finally, we experimented with several imitation learning algorithms to train open-access LLMs on this task. Compared to standard supervised fine-tuning, these methods consistently reduced error rates on both common and uncommonsense abductive reasoning, as judged by human evaluators.\n"}, "2305.1374": {"original_text": "  Tense inconsistency frequently occurs in machine translation. However, there\nare few criteria to assess the model's mastery of tense prediction from a\nlinguistic perspective. In this paper, we present a parallel tense test set,\ncontaining French-English 552 utterances. We also introduce a corresponding\nbenchmark, tense prediction accuracy. With the tense test set and the\nbenchmark, researchers are able to measure the tense consistency performance of\nmachine translation systems for the first time.\n", "rewritten_text": "Machine translation frequently suffers from tense inconsistency.  However, few linguistic criteria exist for assessing a model's proficiency in tense prediction. This paper presents a parallel French-English tense test set comprising 552 utterances and introduces a corresponding benchmark: tense prediction accuracy. This test set and benchmark enable, for the first time, the measurement of tense consistency in machine translation systems.\n"}, "2410.19294": {"original_text": "  Vision-language models, such as CLIP, have shown impressive generalization\ncapacities when using appropriate text descriptions. While optimizing prompts\non downstream labeled data has proven effective in improving performance, these\nmethods entail labor costs for annotations and are limited by their quality.\nAdditionally, since CLIP is pre-trained on highly imbalanced Web-scale data, it\nsuffers from inherent label bias that leads to suboptimal performance. To\ntackle the above challenges, we propose a label-Free prompt distribution\nlearning and bias correction framework, dubbed as **Frolic**, which boosts\nzero-shot performance without the need for labeled data. Specifically, our\nFrolic learns distributions over prompt prototypes to capture diverse visual\nrepresentations and adaptively fuses these with the original CLIP through\nconfidence matching. This fused model is further enhanced by correcting label\nbias via a label-free logit adjustment. Notably, our method is not only\ntraining-free but also circumvents the necessity for hyper-parameter tuning.\nExtensive experimental results across 16 datasets demonstrate the efficacy of\nour approach, particularly outperforming the state-of-the-art by an average of\n$2.6\\%$ on 10 datasets with CLIP ViT-B/16 and achieving an average margin of\n$1.5\\%$ on ImageNet and its five distribution shifts with CLIP ViT-B/16. Codes\nare available in https://github.com/zhuhsingyuu/Frolic.\n", "rewritten_text": "Vision-language models like CLIP demonstrate impressive generalization capabilities when provided with appropriate text descriptions.  While prompt optimization using downstream labeled data improves performance, this approach is costly and limited by annotation quality.  Furthermore, CLIP's pre-training on highly imbalanced web-scale data introduces inherent label bias, hindering optimal performance.  To address these limitations, we propose Frolic, a label-free prompt distribution learning and bias correction framework that enhances zero-shot performance without labeled data.  Specifically, Frolic learns distributions over prompt prototypes to capture diverse visual representations, adaptively fusing them with the original CLIP model via confidence matching.  This fused model is further improved by correcting label bias through label-free logit adjustment.  Significantly, our method is training-free and requires no hyperparameter tuning.  Extensive experiments across 16 datasets demonstrate Frolic's efficacy, surpassing the state-of-the-art by an average of 2.6% on 10 datasets using CLIP ViT-B/16 and achieving an average improvement of 1.5% on ImageNet and its five distribution shifts using CLIP ViT-B/16.  Code is available at https://github.com/zhuhsingyuu/Frolic.\n"}, "1809.01219": {"original_text": "  A novel graph-to-tree conversion mechanism called the deep-tree generation\n(DTG) algorithm is first proposed to predict text data represented by graphs.\nThe DTG method can generate a richer and more accurate representation for nodes\n(or vertices) in graphs. It adds flexibility in exploring the vertex\nneighborhood information to better reflect the second order proximity and\nhomophily equivalence in a graph. Then, a Deep-Tree Recursive Neural Network\n(DTRNN) method is presented and used to classify vertices that contains text\ndata in graphs. To demonstrate the effectiveness of the DTRNN method, we apply\nit to three real-world graph datasets and show that the DTRNN method\noutperforms several state-of-the-art benchmarking methods.\n", "rewritten_text": "This paper introduces a novel graph-to-tree conversion mechanism, the Deep-Tree Generation (DTG) algorithm, for representing text data in graphs.  DTG generates richer, more accurate node representations by flexibly exploring vertex neighborhood information, better capturing second-order proximity and homophily.  We then present a Deep-Tree Recursive Neural Network (DTRNN) method for classifying text-containing vertices in graphs.  Experiments on three real-world datasets demonstrate that DTRNN outperforms several state-of-the-art baselines.\n"}, "2403.09636": {"original_text": "  Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.\n", "rewritten_text": "Large language models (LLMs) rely heavily on transformer architectures.  However, their generative efficiency is hampered by the need to store a key-value cache of past tokens, whose size scales linearly with both input sequence length and batch size.  To address this, we introduce Dynamic Memory Compression (DMC), a method for online key-value cache compression during inference.  Crucially, DMC learns to apply varying compression ratios across different attention heads and layers.  By retrofitting DMC into pre-trained LLMs such as Llama 2 (7B, 13B, and 70B parameters), we achieve up to a 7x throughput increase during autoregressive inference on an NVIDIA H100 GPU.  This is accomplished through continued pre-training on a negligible fraction of the original data, without adding any additional parameters.  DMC maintains original downstream performance even with up to 4x cache compression, surpassing both up-trained grouped-query attention (GQA) and key-value eviction policies (H\u2082O, TOVA).  Furthermore, DMC and GQA can be combined for additive performance gains.  Therefore, DMC offers a drop-in replacement for existing LLM key-value caching mechanisms, enabling the processing of longer contexts and larger batches within any given memory constraint.\n"}, "2108.06536": {"original_text": "  We address the problem of generalized zero-shot semantic segmentation (GZS3)\npredicting pixel-wise semantic labels for seen and unseen classes. Most GZS3\nmethods adopt a generative approach that synthesizes visual features of unseen\nclasses from corresponding semantic ones (e.g., word2vec) to train novel\nclassifiers for both seen and unseen classes. Although generative methods show\ndecent performance, they have two limitations: (1) the visual features are\nbiased towards seen classes; (2) the classifier should be retrained whenever\nnovel unseen classes appear. We propose a discriminative approach to address\nthese limitations in a unified framework. To this end, we leverage visual and\nsemantic encoders to learn a joint embedding space, where the semantic encoder\ntransforms semantic features to semantic prototypes that act as centers for\nvisual features of corresponding classes. Specifically, we introduce\nboundary-aware regression (BAR) and semantic consistency (SC) losses to learn\ndiscriminative features. Our approach to exploiting the joint embedding space,\ntogether with BAR and SC terms, alleviates the seen bias problem. At test time,\nwe avoid the retraining process by exploiting semantic prototypes as a\nnearest-neighbor (NN) classifier. To further alleviate the bias problem, we\nalso propose an inference technique, dubbed Apollonius calibration (AC), that\nmodulates the decision boundary of the NN classifier to the Apollonius circle\nadaptively. Experimental results demonstrate the effectiveness of our\nframework, achieving a new state of the art on standard benchmarks.\n", "rewritten_text": "This paper addresses the problem of generalized zero-shot semantic segmentation (GZS3), predicting pixel-wise semantic labels for both seen and unseen classes.  Existing GZS3 methods predominantly employ generative approaches, synthesizing visual features for unseen classes from semantic representations (e.g., word embeddings) to train classifiers for all classes. While these methods achieve reasonable performance, they suffer from two key limitations: (1) a bias towards seen classes in the synthesized visual features, and (2) the requirement to retrain the classifier whenever new unseen classes are introduced.\n\nWe propose a novel discriminative approach to overcome these limitations within a unified framework.  This approach leverages visual and semantic encoders to learn a shared embedding space.  The semantic encoder maps semantic features to semantic prototypes, which serve as class centers for visual features.  Crucially, we introduce boundary-aware regression (BAR) and semantic consistency (SC) losses to learn discriminative features.  Our joint embedding space, coupled with BAR and SC, mitigates the bias towards seen classes.  At inference time, we avoid retraining by employing a nearest-neighbor (NN) classifier using the learned semantic prototypes.  Furthermore, to further reduce bias, we introduce Apollonius calibration (AC), an inference technique that adaptively adjusts the NN classifier's decision boundary to an Apollonius circle.  Extensive experiments demonstrate the effectiveness of our framework, establishing a new state-of-the-art on standard benchmarks.\n"}, "1907.10815": {"original_text": "  Improvements in data-capture and face modeling techniques have enabled us to\ncreate high-fidelity realistic face models. However, driving these realistic\nface models requires special input data, e.g. 3D meshes and unwrapped textures.\nAlso, these face models expect clean input data taken under controlled lab\nenvironments, which is very different from data collected in the wild. All\nthese constraints make it challenging to use the high-fidelity models in\ntracking for commodity cameras. In this paper, we propose a self-supervised\ndomain adaptation approach to enable the animation of high-fidelity face models\nfrom a commodity camera. Our approach first circumvents the requirement for\nspecial input data by training a new network that can directly drive a face\nmodel just from a single 2D image. Then, we overcome the domain mismatch\nbetween lab and uncontrolled environments by performing self-supervised domain\nadaptation based on \"consecutive frame texture consistency\" based on the\nassumption that the appearance of the face is consistent over consecutive\nframes, avoiding the necessity of modeling the new environment such as lighting\nor background. Experiments show that we are able to drive a high-fidelity face\nmodel to perform complex facial motion from a cellphone camera without\nrequiring any labeled data from the new domain.\n", "rewritten_text": "Advances in data capture and face modeling have yielded highly realistic, high-fidelity face models.  However, animating these models typically requires specialized input data, such as 3D meshes and unwrapped textures, and assumes clean data acquired under controlled laboratory conditions\u2014a stark contrast to real-world data.  These limitations hinder the application of high-fidelity models to commodity camera tracking.  This paper proposes a self-supervised domain adaptation approach to animate high-fidelity face models using commodity cameras.  Our approach addresses the need for specialized input by training a network to directly drive a face model from a single 2D image.  Furthermore, we mitigate the domain gap between laboratory and uncontrolled environments through self-supervised domain adaptation based on consecutive frame texture consistency. This leverages the assumption of consistent facial appearance across consecutive frames, eliminating the need to explicitly model environmental factors like lighting and background.  Experiments demonstrate successful animation of a high-fidelity face model with complex facial motion using a cellphone camera, without requiring any labeled data from the target domain.\n"}, "2311.10651": {"original_text": "  Analysis of the 3D Texture is indispensable for various tasks, such as\nretrieval, segmentation, classification, and inspection of sculptures, knitted\nfabrics, and biological tissues. A 3D texture is a locally repeated surface\nvariation independent of the surface's overall shape and can be determined\nusing the local neighborhood and its characteristics. Existing techniques\ntypically employ computer vision techniques that analyze a 3D mesh globally,\nderive features, and then utilize the obtained features for retrieval or\nclassification. Several traditional and learning-based methods exist in the\nliterature, however, only a few are on 3D texture, and nothing yet, to the best\nof our knowledge, on the unsupervised schemes. This paper presents an original\nframework for the unsupervised segmentation of the 3D texture on the mesh\nmanifold. We approach this problem as binary surface segmentation, partitioning\nthe mesh surface into textured and non-textured regions without prior\nannotation. We devise a mutual transformer-based system comprising a label\ngenerator and a cleaner. The two models take geometric image representations of\nthe surface mesh facets and label them as texture or non-texture across an\niterative mutual learning scheme. Extensive experiments on three publicly\navailable datasets with diverse texture patterns demonstrate that the proposed\nframework outperforms standard and SOTA unsupervised techniques and competes\nreasonably with supervised methods.\n", "rewritten_text": "Analyzing 3D texture is crucial for diverse applications, including the retrieval, segmentation, classification, and inspection of sculptures, knitted fabrics, and biological tissues.  A 3D texture is a locally repeated surface variation independent of the overall shape and is characterized by local neighborhood properties.  Existing methods typically employ global computer vision techniques on 3D meshes to extract features for subsequent retrieval or classification. While numerous traditional and learning-based methods exist, few focus specifically on 3D texture analysis, and to our knowledge, none address unsupervised segmentation.  This paper introduces a novel unsupervised framework for segmenting 3D texture on mesh manifolds.  We formulate this as a binary surface segmentation problem, partitioning the mesh into textured and non-textured regions without requiring prior annotation.  Our approach utilizes a mutual transformer-based system consisting of a label generator and a cleaner.  These models iteratively refine labels (texture or non-texture) for geometric image representations of surface mesh facets through a mutual learning process.  Extensive experiments on three publicly available datasets with varied texture patterns demonstrate that our framework surpasses state-of-the-art (SOTA) unsupervised techniques and achieves competitive performance against supervised methods.\n"}, "1906.05416": {"original_text": "  We introduce a novel method of generating synthetic question answering\ncorpora by combining models of question generation and answer extraction, and\nby filtering the results to ensure roundtrip consistency. By pretraining on the\nresulting corpora we obtain significant improvements on SQuAD2 and NQ,\nestablishing a new state-of-the-art on the latter. Our synthetic data\ngeneration models, for both question generation and answer extraction, can be\nfully reproduced by finetuning a publicly available BERT model on the\nextractive subsets of SQuAD2 and NQ. We also describe a more powerful variant\nthat does full sequence-to-sequence pretraining for question generation,\nobtaining exact match and F1 at less than 0.1% and 0.4% from human performance\non SQuAD2.\n", "rewritten_text": "We present a novel method for generating synthetic question-answering corpora.  This method combines question generation and answer extraction models, filtering the results to ensure round-trip consistency.  Pretraining on these corpora yields significant improvements on the SQuAD2 and Natural Questions (NQ) benchmarks, achieving state-of-the-art performance on the latter.  Our synthetic data generation models\u2014for both question generation and answer extraction\u2014are fully reproducible by fine-tuning a publicly available BERT model on the extractive subsets of SQuAD2 and NQ.  Furthermore, we describe a more powerful variant employing full sequence-to-sequence pretraining for question generation, achieving Exact Match and F1 scores within 0.1% and 0.4% of human performance on SQuAD2, respectively.\n"}, "2404.02393": {"original_text": "  While multilingual machine translation (MNMT) systems hold substantial\npromise, they also have security vulnerabilities. Our research highlights that\nMNMT systems can be susceptible to a particularly devious style of backdoor\nattack, whereby an attacker injects poisoned data into a low-resource language\npair to cause malicious translations in other languages, including\nhigh-resource languages. Our experimental results reveal that injecting less\nthan 0.01% poisoned data into a low-resource language pair can achieve an\naverage 20% attack success rate in attacking high-resource language pairs. This\ntype of attack is of particular concern, given the larger attack surface of\nlanguages inherent to low-resource settings. Our aim is to bring attention to\nthese vulnerabilities within MNMT systems with the hope of encouraging the\ncommunity to address security concerns in machine translation, especially in\nthe context of low-resource languages.\n", "rewritten_text": "Multilingual machine translation (MNMT) systems, while promising, harbor significant security vulnerabilities.  Our research demonstrates that MNMT systems are susceptible to a novel backdoor attack:  injecting poisoned data into a low-resource language pair to trigger malicious translations in other, high-resource languages.  Experiments reveal that injecting less than 0.01% poisoned data into a low-resource language pair achieves an average 20% attack success rate against high-resource pairs. This vulnerability is particularly concerning due to the inherently larger attack surface presented by low-resource languages.  We aim to raise awareness of these vulnerabilities to encourage the machine translation community to address security concerns, especially within the context of low-resource language translation.\n"}, "2104.00556": {"original_text": "  Two-view structure-from-motion (SfM) is the cornerstone of 3D reconstruction\nand visual SLAM. Existing deep learning-based approaches formulate the problem\nby either recovering absolute pose scales from two consecutive frames or\npredicting a depth map from a single image, both of which are ill-posed\nproblems. In contrast, we propose to revisit the problem of deep two-view SfM\nby leveraging the well-posedness of the classic pipeline. Our method consists\nof 1) an optical flow estimation network that predicts dense correspondences\nbetween two frames; 2) a normalized pose estimation module that computes\nrelative camera poses from the 2D optical flow correspondences, and 3) a\nscale-invariant depth estimation network that leverages epipolar geometry to\nreduce the search space, refine the dense correspondences, and estimate\nrelative depth maps. Extensive experiments show that our method outperforms all\nstate-of-the-art two-view SfM methods by a clear margin on KITTI depth, KITTI\nVO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth\nestimation.\n", "rewritten_text": "Two-view structure-from-motion (SfM) is fundamental to 3D reconstruction and visual simultaneous localization and mapping (SLAM).  Existing deep learning approaches typically address this problem by either recovering absolute pose scales from consecutive frames or predicting depth maps from single images\u2014both ill-posed formulations.  In contrast, we propose a novel deep two-view SfM approach that leverages the well-posedness of classical pipelines. Our method comprises three components: 1) an optical flow estimation network predicting dense correspondences between two frames; 2) a normalized pose estimation module computing relative camera poses from these correspondences; and 3) a scale-invariant depth estimation network that exploits epipolar geometry to constrain the search space, refine correspondences, and estimate relative depth maps.  Extensive experiments on the KITTI depth, KITTI visual odometry (VO), MVS, Scenes11, and SUN3D datasets demonstrate that our method significantly outperforms all state-of-the-art two-view SfM methods in both relative pose and depth estimation.\n"}, "2309.03903": {"original_text": "  Training data for video segmentation are expensive to annotate. This impedes\nextensions of end-to-end algorithms to new video segmentation tasks, especially\nin large-vocabulary settings. To 'track anything' without training on video\ndata for every individual task, we develop a decoupled video segmentation\napproach (DEVA), composed of task-specific image-level segmentation and\nclass/task-agnostic bi-directional temporal propagation. Due to this design, we\nonly need an image-level model for the target task (which is cheaper to train)\nand a universal temporal propagation model which is trained once and\ngeneralizes across tasks. To effectively combine these two modules, we use\nbi-directional propagation for (semi-)online fusion of segmentation hypotheses\nfrom different frames to generate a coherent segmentation. We show that this\ndecoupled formulation compares favorably to end-to-end approaches in several\ndata-scarce tasks including large-vocabulary video panoptic segmentation,\nopen-world video segmentation, referring video segmentation, and unsupervised\nvideo object segmentation. Code is available at:\nhttps://hkchengrex.github.io/Tracking-Anything-with-DEVA\n", "rewritten_text": "Annotating training data for video segmentation is expensive, hindering the extension of end-to-end algorithms to new tasks, particularly those with large vocabularies.  To address this, we introduce DEVA (Decoupled Video Segmentation), a novel approach that enables \"tracking anything\" without requiring per-task video training.  DEVA decouples video segmentation into task-specific image-level segmentation and a class/task-agnostic bi-directional temporal propagation module. This design requires only an image-level model (inexpensive to train) for each target task and a single, universally trained temporal propagation model that generalizes across tasks.  These modules are effectively combined using bi-directional propagation for (semi-)online fusion of segmentation hypotheses across frames, resulting in coherent segmentations.  We demonstrate that this decoupled approach outperforms end-to-end methods on several data-scarce tasks, including large-vocabulary video panoptic segmentation, open-world video segmentation, referring video segmentation, and unsupervised video object segmentation. Code is available at: https://hkchengrex.github.io/Tracking-Anything-with-DEVA\n"}, "2312.08869": {"original_text": "  We are living in a world surrounded by diverse and \"smart\" devices with rich\nmodalities of sensing ability. Conveniently capturing the interactions between\nus humans and these objects remains far-reaching. In this paper, we present\nI'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the\nhuman and object in a novel setting: using a minimal amount of RGB camera and\nobject-mounted Inertial Measurement Unit (IMU). It combines general motion\ninference and category-aware refinement. For the former, we introduce a\nholistic human-object tracking method to fuse the IMU signals and the RGB\nstream and progressively recover the human motions and subsequently the\ncompanion object motions. For the latter, we tailor a category-aware motion\ndiffusion model, which is conditioned on both the raw IMU observations and the\nresults from the previous stage under over-parameterization representation. It\nsignificantly refines the initial results and generates vivid body, hand, and\nobject motions. Moreover, we contribute a large dataset with ground truth human\nand object motions, dense RGB inputs, and rich object-mounted IMU measurements.\nExtensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid\ncapture setting. Our dataset and code will be released to the community.\n", "rewritten_text": "We live in a world saturated with diverse, \"smart\" devices possessing rich sensing modalities.  However, conveniently capturing human-object interactions remains a significant challenge. This paper presents I'm-HOI, a novel monocular scheme for accurately capturing the 3D motion of both humans and objects using minimal hardware: an RGB camera and an object-mounted inertial measurement unit (IMU).  I'm-HOI combines general motion inference with category-aware refinement.  First, a holistic human-object tracking method fuses IMU signals and the RGB stream to progressively recover human and then object motion.  Second, a category-aware motion diffusion model, conditioned on raw IMU observations and the previous stage's results (using an over-parameterized representation), significantly refines these initial results, generating realistic body, hand, and object motions.  Furthermore, we contribute a large dataset with ground truth human and object motion, dense RGB input, and rich object-mounted IMU measurements.  Extensive experiments demonstrate I'm-HOI's effectiveness in this hybrid capture setting. Our dataset and code will be publicly released.\n"}, "1810.04864": {"original_text": "  We present a comparison of word-based and character-based\nsequence-to-sequence models for data-to-text natural language generation, which\ngenerate natural language descriptions for structured inputs. On the datasets\nof two recent generation challenges, our models achieve comparable or better\nautomatic evaluation results than the best challenge submissions. Subsequent\ndetailed statistical and human analyses shed light on the differences between\nthe two input representations and the diversity of the generated texts. In a\ncontrolled experiment with synthetic training data generated from templates, we\ndemonstrate the ability of neural models to learn novel combinations of the\ntemplates and thereby generalize beyond the linguistic structures they were\ntrained on.\n", "rewritten_text": "This paper compares word-based and character-based sequence-to-sequence models for data-to-text natural language generation (NLG), which produces natural language descriptions from structured inputs.  Our models achieve comparable or superior automatic evaluation results to the best submissions in two recent NLG challenges.  Further statistical and human analyses reveal the impact of input representation on the diversity of generated text.  Finally, a controlled experiment using synthetic training data generated from templates demonstrates the neural models' ability to learn novel template combinations, generalizing beyond their training data's linguistic structures.\n"}, "2405.16116": {"original_text": "  Scene Graph Generation (SGG) can extract abstract semantic relations between\nentities in images as graph representations. This task holds strong promises\nfor other downstream tasks such as the embodied cognition of an autonomous\nagent. However, to power such applications, SGG needs to solve the gap of\nreal-time latency. In this work, we propose to investigate the bottlenecks of\ncurrent approaches for real-time constraint applications. Then, we propose a\nsimple yet effective implementation of a real-time SGG approach using YOLOV8 as\nan object detection backbone. Our implementation is the first to obtain more\nthan 48 FPS for the task with no loss of accuracy, successfully outperforming\nany other lightweight approaches. Our code is freely available at\nhttps://github.com/Maelic/SGG-Benchmark.\n", "rewritten_text": "Scene graph generation (SGG) extracts abstract semantic relationships between image entities as graph representations.  This capability holds significant promise for downstream tasks, such as enabling embodied cognition in autonomous agents.  However, real-time latency remains a critical challenge for practical applications.  This work investigates bottlenecks in existing SGG approaches designed for real-time constraints. We propose a simple yet effective real-time SGG method using YOLOv8 for object detection.  Our implementation achieves over 48 frames per second (FPS) without sacrificing accuracy, surpassing all existing lightweight approaches. The code is publicly available at https://github.com/Maelic/SGG-Benchmark.\n"}, "2012.08549": {"original_text": "  Voice Assistants such as Alexa, Siri, and Google Assistant typically use a\ntwo-stage Spoken Language Understanding pipeline; first, an Automatic Speech\nRecognition (ASR) component to process customer speech and generate text\ntranscriptions, followed by a Natural Language Understanding (NLU) component to\nmap transcriptions to an actionable hypothesis. An end-to-end (E2E) system that\ngoes directly from speech to a hypothesis is a more attractive option. These\nsystems were shown to be smaller, faster, and better optimized. However, they\nrequire massive amounts of end-to-end training data and in addition, don't take\nadvantage of the already available ASR and NLU training data.\n  In this work, we propose an E2E system that is designed to jointly train on\nmultiple speech-to-text tasks, such as ASR (speech-transcription) and SLU\n(speech-hypothesis), and text-to-text tasks, such as NLU (text-hypothesis). We\ncall this the Audio-Text All-Task (AT-AT) Model and we show that it beats the\nperformance of E2E models trained on individual tasks, especially ones trained\non limited data. We show this result on an internal music dataset and two\npublic datasets, FluentSpeech and SNIPS Audio, where we achieve\nstate-of-the-art results. Since our model can process both speech and text\ninput sequences and learn to predict a target sequence, it also allows us to do\nzero-shot E2E SLU by training on only text-hypothesis data (without any speech)\nfrom a new domain. We evaluate this ability of our model on the Facebook TOP\ndataset and set a new benchmark for zeroshot E2E performance. We will soon\nrelease the audio data collected for the TOP dataset for future research.\n", "rewritten_text": "Voice assistants like Alexa, Siri, and Google Assistant typically employ a two-stage spoken language understanding (SLU) pipeline: automatic speech recognition (ASR) transcribes speech to text, followed by natural language understanding (NLU) mapping the transcription to an actionable interpretation.  While end-to-end (E2E) systems, which directly map speech to interpretation, offer advantages in size, speed, and optimization, they require massive E2E training datasets and cannot leverage existing ASR and NLU training data.\n\nThis work proposes an E2E system, the Audio-Text All-Task (AT-AT) model, jointly trained on multiple speech-to-text tasks (ASR and SLU) and text-to-text tasks (NLU).  We demonstrate that AT-AT surpasses the performance of E2E models trained on individual tasks, particularly with limited data.  This improvement is shown on an internal music dataset and two public datasets, FluentSpeech and SNIPS Audio, achieving state-of-the-art results.  Furthermore, AT-AT's ability to process both speech and text inputs enables zero-shot E2E SLU by training solely on text-hypothesis data from a new domain.  We evaluate this capability on the Facebook TOP dataset, establishing a new benchmark for zero-shot E2E performance.  The audio data collected for the TOP dataset will be publicly released to facilitate future research.\n"}, "2207.09059": {"original_text": "  Few-shot open-set recognition aims to classify both seen and novel images\ngiven only limited training data of seen classes. The challenge of this task is\nthat the model is required not only to learn a discriminative classifier to\nclassify the pre-defined classes with few training data but also to reject\ninputs from unseen classes that never appear at training time. In this paper,\nwe propose to solve the problem from two novel aspects. First, instead of\nlearning the decision boundaries between seen classes, as is done in standard\nclose-set classification, we reserve space for unseen classes, such that images\nlocated in these areas are recognized as the unseen classes. Second, to\neffectively learn such decision boundaries, we propose to utilize the\nbackground features from seen classes. As these background regions do not\nsignificantly contribute to the decision of close-set classification, it is\nnatural to use them as the pseudo unseen classes for classifier learning. Our\nextensive experiments show that our proposed method not only outperforms\nmultiple baselines but also sets new state-of-the-art results on three popular\nbenchmarks, namely tieredImageNet, miniImageNet, and Caltech-USCD\nBirds-200-2011 (CUB).\n", "rewritten_text": "Few-shot open-set recognition addresses the challenge of classifying both seen and unseen image classes with limited training data for the seen classes.  This task requires the model to learn a discriminative classifier for the known classes while simultaneously rejecting inputs from unseen classes absent from the training set.  This paper proposes a novel two-pronged approach. First, unlike standard closed-set classification which focuses on defining decision boundaries between seen classes, we explicitly reserve feature space for unseen classes, classifying images in these regions as belonging to the unseen category. Second, to effectively learn these decision boundaries, we leverage background features from the seen classes.  Since these background regions are typically inconsequential in closed-set classification, they serve as effective pseudo-unseen class examples for training.  Extensive experiments demonstrate that our method surpasses multiple baselines and achieves state-of-the-art results on three widely used benchmarks: tieredImageNet, miniImageNet, and Caltech-UCSD Birds-200-2011 (CUB).\n"}, "1710.04943": {"original_text": "  In this paper, we report on our efforts for using Deep Learning for\nclassifying artifacts and their features in digital visuals as a part of the\nNeoclassica framework. It was conceived to provide scholars with new methods\nfor analyzing and classifying artifacts and aesthetic forms from the era of\nClassicism. The framework accommodates both traditional knowledge\nrepresentation as a formal ontology and data-driven knowledge discovery, where\ncultural patterns will be identified by means of algorithms in statistical\nanalysis and machine learning. We created a Deep Learning approach trained on\nphotographs to classify the objects inside these photographs. In a next step,\nwe will apply a different Deep Learning approach. It is capable of locating\nmultiple objects inside an image and classifying them with a high accuracy.\n", "rewritten_text": "This paper details our application of deep learning to classify artifacts and their features within digital images as part of the Neoclassica framework.  Developed to provide scholars with novel methods for analyzing and classifying artifacts and aesthetic forms from the Classical era, the framework integrates traditional knowledge representation (a formal ontology) with data-driven knowledge discovery.  Cultural patterns are identified using statistical analysis and machine learning algorithms.  We have developed a deep learning model trained on photographs to classify depicted objects.  Future work will involve a different deep learning approach capable of accurately locating and classifying multiple objects within a single image.\n"}, "2111.07239": {"original_text": "  Object detection has achieved promising performance on clean datasets, but\nhow to achieve better tradeoff between the adversarial robustness and clean\nprecision is still under-explored. Adversarial training is the mainstream\nmethod to improve robustness, but most of the works will sacrifice clean\nprecision to gain robustness than standard training. In this paper, we propose\nUnified Decoupled Feature Alignment (UDFA), a novel fine-tuning paradigm which\nachieves better performance than existing methods, by fully exploring the\ncombination between self-knowledge distillation and adversarial training for\nobject detection. We first use decoupled fore/back-ground features to construct\nself-knowledge distillation branch between clean feature representation from\npretrained detector (served as teacher) and adversarial feature representation\nfrom student detector. Then we explore the self-knowledge distillation from a\nnew angle by decoupling original branch into a self-supervised learning branch\nand a new self-knowledge distillation branch. With extensive experiments on the\nPASCAL-VOC and MS-COCO benchmarks, the evaluation results show that UDFA can\nsurpass the standard training and state-of-the-art adversarial training methods\nfor object detection. For example, compared with teacher detector, our approach\non GFLV2 with ResNet-50 improves clean precision by 2.2 AP on PASCAL-VOC;\ncompared with SOTA adversarial training methods, our approach improves clean\nprecision by 1.6 AP, while improving adversarial robustness by 0.5 AP. Our code\nwill be available at https://github.com/grispeut/udfa.\n", "rewritten_text": "Object detection achieves promising performance on clean datasets, but optimizing the trade-off between adversarial robustness and clean precision remains under-explored.  While adversarial training is the dominant approach to enhance robustness, it typically compromises clean precision compared to standard training.  This paper introduces Unified Decoupled Feature Alignment (UDFA), a novel fine-tuning paradigm that surpasses existing methods by leveraging a synergistic combination of self-knowledge distillation and adversarial training for object detection.  UDFA employs decoupled foreground/background features to construct a self-knowledge distillation branch between a pretrained detector (teacher) and a student detector's adversarial feature representation.  Furthermore, we refine this approach by decoupling the original branch into self-supervised learning and a novel self-knowledge distillation branch.  Extensive experiments on PASCAL VOC and MS COCO benchmarks demonstrate that UDFA outperforms standard training and state-of-the-art adversarial training methods.  Specifically, compared to the teacher detector (GFLV2 with ResNet-50), UDFA improves clean precision by 2.2 AP on PASCAL VOC.  Against state-of-the-art adversarial training methods, UDFA achieves a 1.6 AP improvement in clean precision and a 0.5 AP improvement in adversarial robustness.  Our code is available at https://github.com/grispeut/udfa.\n"}, "2304.05694": {"original_text": "  Self-attention modules have demonstrated remarkable capabilities in capturing\nlong-range relationships and improving the performance of point cloud tasks.\nHowever, point cloud objects are typically characterized by complex,\ndisordered, and non-Euclidean spatial structures with multiple scales, and\ntheir behavior is often dynamic and unpredictable. The current self-attention\nmodules mostly rely on dot product multiplication and dimension alignment among\nquery-key-value features, which cannot adequately capture the multi-scale\nnon-Euclidean structures of point cloud objects. To address these problems,\nthis paper proposes a self-attention plug-in module with its variants,\nMulti-scale Geometry-aware Transformer (MGT). MGT processes point cloud data\nwith multi-scale local and global geometric information in the following three\naspects. At first, the MGT divides point cloud data into patches with multiple\nscales. Secondly, a local feature extractor based on sphere mapping is proposed\nto explore the geometry inner each patch and generate a fixed-length\nrepresentation for each patch. Thirdly, the fixed-length representations are\nfed into a novel geodesic-based self-attention to capture the global\nnon-Euclidean geometry between patches. Finally, all the modules are integrated\ninto the framework of MGT with an end-to-end training scheme. Experimental\nresults demonstrate that the MGT vastly increases the capability of capturing\nmulti-scale geometry using the self-attention mechanism and achieves strong\ncompetitive performance on mainstream point cloud benchmarks.\n", "rewritten_text": "Self-attention modules have shown remarkable success in capturing long-range relationships and improving point cloud task performance.  However, point clouds often exhibit complex, disordered, multi-scale, and non-Euclidean spatial structures, with dynamic and unpredictable behavior.  Existing self-attention modules, primarily relying on dot-product attention and feature alignment between query, key, and value vectors, inadequately capture these multi-scale, non-Euclidean characteristics.  To address this, we propose the Multi-scale Geometry-aware Transformer (MGT), a novel self-attention plug-in module and its variants.  MGT leverages multi-scale local and global geometric information in three key ways:  (1) it partitions the point cloud into multi-scale patches; (2) it employs a sphere-mapping-based local feature extractor to capture the geometry within each patch, generating a fixed-length representation; and (3) it utilizes a novel geodesic-based self-attention mechanism to capture global non-Euclidean relationships between patches.  These modules are integrated into an end-to-end trainable MGT framework.  Experimental results demonstrate that MGT significantly enhances the ability to capture multi-scale geometry using self-attention, achieving state-of-the-art performance on standard point cloud benchmarks.\n"}, "2108.07708": {"original_text": "  Can language models learn grounded representations from text distribution\nalone? This question is both central and recurrent in natural language\nprocessing; authors generally agree that grounding requires more than textual\ndistribution. We propose to experimentally test this claim: if any two words\nhave different meanings and yet cannot be distinguished from distribution\nalone, then grounding is out of the reach of text-based models. To that end, we\npresent early work on an online game for the collection of human judgments on\nthe distributional similarity of word pairs in five languages. We further\nreport early results of our data collection campaign.\n", "rewritten_text": "The ability of language models to learn grounded representations solely from text distributions is a central and recurring question in natural language processing.  While a consensus exists that grounding requires more than textual distribution alone, we propose an experimental test of this hypothesis: if two words with distinct meanings are indistinguishable based on distributional information, then text-based models cannot achieve grounding.  To this end, we present preliminary work on an online game designed to collect human judgments on the distributional similarity of word pairs across five languages.  Early results from our data collection are also reported.\n"}, "2302.03194": {"original_text": "  We propose two methods to make unsupervised domain adaptation (UDA) more\nparameter efficient using adapters, small bottleneck layers interspersed with\nevery layer of the large-scale pre-trained language model (PLM). The first\nmethod deconstructs UDA into a two-step process: first by adding a domain\nadapter to learn domain-invariant information and then by adding a task adapter\nthat uses domain-invariant information to learn task representations in the\nsource domain. The second method jointly learns a supervised classifier while\nreducing the divergence measure. Compared to strong baselines, our simple\nmethods perform well in natural language inference (MNLI) and the cross-domain\nsentiment classification task. We even outperform unsupervised domain\nadaptation methods such as DANN and DSN in sentiment classification, and we are\nwithin 0.85% F1 for natural language inference task, by fine-tuning only a\nfraction of the full model parameters. We release our code at\nhttps://github.com/declare-lab/domadapter\n", "rewritten_text": "We propose two parameter-efficient methods for unsupervised domain adaptation (UDA) using adapters\u2014small bottleneck layers interspersed throughout a large-scale pre-trained language model (PLM).  The first method decomposes UDA into a two-step process:  first, a domain adapter learns domain-invariant features; second, a task adapter leverages these features to learn task representations within the source domain. The second method jointly learns a supervised classifier while minimizing a divergence measure.  Our simple methods achieve strong performance on natural language inference (MNLI) and cross-domain sentiment classification, outperforming unsupervised domain adaptation baselines such as DANN and DSN on sentiment classification.  On MNLI, our performance is within 0.85% F1 of the state-of-the-art, despite fine-tuning only a fraction of the model's parameters.  Our code is available at https://github.com/declare-lab/domadapter.\n"}, "2103.04059": {"original_text": "  Few-shot class incremental learning (FSCIL) portrays the problem of learning\nnew concepts gradually, where only a few examples per concept are available to\nthe learner. Due to the limited number of examples for training, the techniques\ndeveloped for standard incremental learning cannot be applied verbatim to\nFSCIL. In this work, we introduce a distillation algorithm to address the\nproblem of FSCIL and propose to make use of semantic information during\ntraining. To this end, we make use of word embeddings as semantic information\nwhich is cheap to obtain and which facilitate the distillation process.\nFurthermore, we propose a method based on an attention mechanism on multiple\nparallel embeddings of visual data to align visual and semantic vectors, which\nreduces issues related to catastrophic forgetting. Via experiments on\nMiniImageNet, CUB200, and CIFAR100 dataset, we establish new state-of-the-art\nresults by outperforming existing approaches.\n", "rewritten_text": "Few-shot class incremental learning (FSCIL) addresses the challenge of gradually learning new concepts with limited training data\u2014only a few examples per concept are available.  Standard incremental learning techniques are unsuitable for FSCIL due to this data scarcity.  This work introduces a novel distillation algorithm for FSCIL, leveraging semantic information during training.  We utilize readily available word embeddings as a cost-effective source of semantic information to enhance the distillation process.  Further, we propose an attention mechanism operating on multiple parallel visual embeddings to align visual and semantic vectors, mitigating catastrophic forgetting.  Experiments on MiniImageNet, CUB-200, and CIFAR-100 datasets demonstrate state-of-the-art performance, surpassing existing approaches.\n"}, "1710.07395": {"original_text": "  In the wake of a polarizing election, the cyber world is laden with hate\nspeech. Context accompanying a hate speech text is useful for identifying hate\nspeech, which however has been largely overlooked in existing datasets and hate\nspeech detection models. In this paper, we provide an annotated corpus of hate\nspeech with context information well kept. Then we propose two types of hate\nspeech detection models that incorporate context information, a logistic\nregression model with context features and a neural network model with learning\ncomponents for context. Our evaluation shows that both models outperform a\nstrong baseline by around 3% to 4% in F1 score and combining these two models\nfurther improve the performance by another 7% in F1 score.\n", "rewritten_text": "Following a divisive election, online platforms are rife with hate speech.  While contextual information surrounding hate speech is crucial for its identification, existing datasets and detection models have largely neglected this aspect. This paper presents a newly annotated corpus of hate speech, meticulously preserving contextual information. We then propose two novel hate speech detection models that leverage this context: a logistic regression model incorporating contextual features and a neural network model with context-aware learning components.  Our evaluation demonstrates that both models surpass a strong baseline by approximately 3\u20134% in F1 score, and their ensemble further improves performance by an additional 7% in F1 score.\n"}, "2401.17597": {"original_text": "  Multi-turn dialogues are characterized by their extended length and the\npresence of turn-taking conversations. Traditional language models often\noverlook the distinct features of these dialogues by treating them as regular\ntext. In this paper, we propose a speaker-enhanced pre-training method for long\ndialogue summarization, which leverages the inherent structure of multiple-turn\ndialogues. To support our study, we curate a diverse dataset that includes\ntranscripts from real-world scenarios, movie or TV show transcripts, and\ndialogues generated by a Large Language Model. We then perform a pre-training,\nwhich encompasses the detection of speaker changes, and masked utterance\ngeneration. Experimental results of fine-tuned models demonstrate that our\nmodel achieves state-of-the-art performance on downstream benchmarks with long\ncontext, surpassing baseline models and highlighting the effectiveness of our\napproach. Our findings highlight the importance of curating pre-training\ndatasets that exhibit diversity and variations in length distribution to ensure\neffective alignment with downstream datasets.\n", "rewritten_text": "Multi-turn dialogues are characterized by their length and turn-taking structure.  Traditional language models often fail to account for these characteristics, treating them as standard text. This paper proposes a speaker-enhanced pre-training method for long dialogue summarization that leverages the inherent structure of multi-turn conversations.  We curated a diverse dataset comprising transcripts from real-world scenarios, movies/TV shows, and large language model (LLM) generated dialogues.  Pre-training incorporates speaker change detection and masked utterance generation.  Fine-tuned models achieved state-of-the-art performance on long-context downstream benchmarks, surpassing baselines and demonstrating the effectiveness of our approach.  Our results underscore the importance of diverse, length-varied pre-training datasets for optimal alignment with downstream tasks.\n"}, "2309.01036": {"original_text": "  Spatial transcriptomics is an emerging technology that aligns histopathology\nimages with spatially resolved gene expression profiling. It holds the\npotential for understanding many diseases but faces significant bottlenecks\nsuch as specialized equipment and domain expertise. In this work, we present\nSEPAL, a new model for predicting genetic profiles from visual tissue\nappearance. Our method exploits the biological biases of the problem by\ndirectly supervising relative differences with respect to mean expression, and\nleverages local visual context at every coordinate to make predictions using a\ngraph neural network. This approach closes the gap between complete locality\nand complete globality in current methods. In addition, we propose a novel\nbenchmark that aims to better define the task by following current best\npractices in transcriptomics and restricting the prediction variables to only\nthose with clear spatial patterns. Our extensive evaluation in two different\nhuman breast cancer datasets indicates that SEPAL outperforms previous\nstate-of-the-art methods and other mechanisms of including spatial context.\n", "rewritten_text": "Spatial transcriptomics, a rapidly developing technology, integrates histopathology images with spatially resolved gene expression profiling, offering significant potential for disease understanding.  However, its widespread adoption is hindered by the need for specialized equipment and expertise.  This work introduces SEPAL, a novel model for predicting gene expression profiles from tissue morphology.  Our method leverages biological biases by directly supervising relative differences from mean expression and utilizes a graph neural network to incorporate local visual context at each coordinate. This approach bridges the gap between purely local and purely global methods.  Furthermore, we propose a novel benchmark dataset designed to better define the task, adhering to best practices in transcriptomics and focusing prediction on genes exhibiting clear spatial patterns.  Extensive evaluation on two human breast cancer datasets demonstrates that SEPAL surpasses existing state-of-the-art methods and alternative approaches to spatial context integration.\n"}, "2011.0212": {"original_text": "  Diabetic retinopathy (DR) is one of the leading causes of blindness. However,\nno specific symptoms of early DR lead to a delayed diagnosis, which results in\ndisease progression in patients. To determine the disease severity levels,\nophthalmologists need to focus on the discriminative parts of the fundus\nimages. In recent years, deep learning has achieved great success in medical\nimage analysis. However, most works directly employ algorithms based on\nconvolutional neural networks (CNNs), which ignore the fact that the difference\namong classes is subtle and gradual. Hence, we consider automatic image grading\nof DR as a fine-grained classification task, and construct a bilinear model to\nidentify the pathologically discriminative areas. In order to leverage the\nordinal information among classes, we use an ordinal regression method to\nobtain the soft labels. In addition, other than only using a categorical loss\nto train our network, we also introduce the metric loss to learn a more\ndiscriminative feature space. Experimental results demonstrate the superior\nperformance of the proposed method on two public IDRiD and DeepDR datasets.\n", "rewritten_text": "Diabetic retinopathy (DR) is a leading cause of blindness.  Early DR often lacks specific symptoms, delaying diagnosis and allowing disease progression.  Accurate assessment of disease severity requires ophthalmologists to carefully analyze discriminative features within fundus images.  While deep learning has shown great promise in medical image analysis, most convolutional neural network (CNN)-based approaches overlook the subtle and gradual distinctions between DR severity levels.  Therefore, we frame automatic DR grading as a fine-grained classification problem. We propose a bilinear model to identify pathologically discriminative areas and incorporate ordinal regression to leverage the inherent ordinality among DR severity classes, generating soft labels.  Furthermore, we augment the categorical loss function with a metric loss to learn a more discriminative feature space.  Experiments on the public IDRiD and DeepDR datasets demonstrate the superior performance of our proposed method.\n"}, "1905.02462": {"original_text": "  Recently, image super-resolution has been widely studied and achieved\nsignificant progress by leveraging the power of deep convolutional neural\nnetworks. However, there has been limited advancement in video super-resolution\n(VSR) due to the complex temporal patterns in videos. In this paper, we\ninvestigate how to adapt state-of-the-art methods of image super-resolution for\nvideo super-resolution. The proposed adapting method is straightforward. The\ninformation among successive frames is well exploited, while the overhead on\nthe original image super-resolution method is negligible. Furthermore, we\npropose a learning-based method to ensemble the outputs from multiple\nsuper-resolution models. Our methods show superior performance and rank second\nin the NTIRE2019 Video Super-Resolution Challenge Track 1.\n", "rewritten_text": "Recent advancements in image super-resolution (ISR) have leveraged deep convolutional neural networks (CNNs) to achieve significant progress.  However, video super-resolution (VSR) has lagged due to the inherent complexity of temporal dependencies in video data. This paper explores adapting state-of-the-art ISR methods to VSR.  Our proposed adaptation method is straightforward, effectively exploiting inter-frame information with negligible overhead compared to the original ISR method.  Furthermore, we introduce a learning-based ensemble method to combine outputs from multiple super-resolution models.  These methods achieved superior performance, earning second place in Track 1 of the NTIRE 2019 Video Super-Resolution Challenge.\n"}, "2312.14157": {"original_text": "  3D hand tracking from a monocular video is a very challenging problem due to\nhand interactions, occlusions, left-right hand ambiguity, and fast motion. Most\nexisting methods rely on RGB inputs, which have severe limitations under\nlow-light conditions and suffer from motion blur. In contrast, event cameras\ncapture local brightness changes instead of full image frames and do not suffer\nfrom the described effects. Unfortunately, existing image-based techniques\ncannot be directly applied to events due to significant differences in the data\nmodalities. In response to these challenges, this paper introduces the first\nframework for 3D tracking of two fast-moving and interacting hands from a\nsingle monocular event camera. Our approach tackles the left-right hand\nambiguity with a novel semi-supervised feature-wise attention mechanism and\nintegrates an intersection loss to fix hand collisions. To facilitate advances\nin this research domain, we release a new synthetic large-scale dataset of two\ninteracting hands, Ev2Hands-S, and a new real benchmark with real event streams\nand ground-truth 3D annotations, Ev2Hands-R. Our approach outperforms existing\nmethods in terms of the 3D reconstruction accuracy and generalises to real data\nunder severe light conditions.\n", "rewritten_text": "Monocular 3D hand tracking presents significant challenges due to hand interactions, occlusions, left-right ambiguity, and rapid motion.  Existing methods, primarily relying on RGB input, suffer from limitations in low-light conditions and motion blur.  Event cameras, which capture local brightness changes rather than full frames, mitigate these issues. However, existing image-based techniques are not directly applicable to event data due to fundamental modality differences.\n\nThis paper introduces the first framework for 3D tracking of two interacting hands, moving rapidly, from a single monocular event camera.  Our approach addresses left-right ambiguity using a novel semi-supervised feature-wise attention mechanism and incorporates an intersection loss to resolve hand collisions.  To advance research in this area, we release two new datasets: Ev2Hands-S, a large-scale synthetic dataset of interacting hands, and Ev2Hands-R, a real-world benchmark with event streams and 3D ground truth annotations.  Our method surpasses existing approaches in 3D reconstruction accuracy and demonstrates robust generalization to real-world data under challenging low-light conditions.\n"}, "2010.08243": {"original_text": "  3D object detectors based only on LiDAR point clouds hold the\nstate-of-the-art on modern street-view benchmarks. However, LiDAR-based\ndetectors poorly generalize across domains due to domain shift. In the case of\nLiDAR, in fact, domain shift is not only due to changes in the environment and\nin the object appearances, as for visual data from RGB cameras, but is also\nrelated to the geometry of the point clouds (e.g., point density variations).\nThis paper proposes SF-UDA$^{3D}$, the first Source-Free Unsupervised Domain\nAdaptation (SF-UDA) framework to domain-adapt the state-of-the-art PointRCNN 3D\ndetector to target domains for which we have no annotations (unsupervised),\nneither we hold images nor annotations of the source domain (source-free).\nSF-UDA$^{3D}$ is novel on both aspects. Our approach is based on\npseudo-annotations, reversible scale-transformations and motion coherency.\nSF-UDA$^{3D}$ outperforms both previous domain adaptation techniques based on\nfeatures alignment and state-of-the-art 3D object detection methods which\nadditionally use few-shot target annotations or target annotation statistics.\nThis is demonstrated by extensive experiments on two large-scale datasets,\ni.e., KITTI and nuScenes.\n", "rewritten_text": "LiDAR-based 3D object detectors currently achieve state-of-the-art performance on modern street-view benchmarks.  However, these detectors suffer from poor cross-domain generalization due to domain shift.  Unlike RGB camera data, where domain shift arises from environmental and object appearance changes, LiDAR domain shift is also significantly influenced by point cloud geometry, such as variations in point density.  This paper introduces SF-UDA\u00b3D, the first source-free unsupervised domain adaptation (SF-UDA) framework for adapting state-of-the-art PointRCNN 3D detectors to unlabeled target domains.  Crucially, SF-UDA\u00b3D requires neither source domain annotations nor images.  This novel approach leverages pseudo-annotations, reversible scale transformations, and motion coherency.  Extensive experiments on the KITTI and nuScenes datasets demonstrate that SF-UDA\u00b3D outperforms existing domain adaptation techniques based on feature alignment, as well as state-of-the-art 3D object detection methods that utilize limited target annotations or annotation statistics.\n"}, "2305.09407": {"original_text": "  Visual quality inspection in high performance manufacturing can benefit from\nautomation, due to cost savings and improved rigor. Deep learning techniques\nare the current state of the art for generic computer vision tasks like\nclassification and object detection. Manufacturing data can pose a challenge\nfor deep learning because data is highly repetitive and there are few images of\ndefects or deviations to learn from. Deep learning models trained with such\ndata can be fragile and sensitive to context, and can under-detect new defects\nnot found in the training data. In this work, we explore training defect\ndetection models to learn specific defects out of context, so that they are\nmore likely to be detected in new situations. We demonstrate how models trained\non diverse images containing a common defect type can pick defects out in new\ncircumstances. Such generic models could be more robust to new defects not\nfound data collected for training, and can reduce data collection impediments\nto implementing visual inspection on production lines. Additionally, we\ndemonstrate that object detection models trained to predict a label and\nbounding box outperform classifiers that predict a label only on held out test\ndata typical of manufacturing inspection tasks. Finally, we studied the factors\nthat affect generalization in order to train models that work under a wider\nrange of conditions.\n", "rewritten_text": "High-performance manufacturing benefits from automated visual quality inspection due to reduced costs and enhanced rigor.  While deep learning excels at generic computer vision tasks like classification and object detection, manufacturing data presents challenges.  The highly repetitive nature of this data, coupled with the scarcity of defect images, can lead to fragile models sensitive to context and prone to under-detecting novel defects.  This work explores training defect detection models to learn defect features *out of context*, thereby improving their ability to detect defects in unseen situations.  We demonstrate that models trained on diverse images containing a common defect type successfully identify that defect in novel contexts.  These generalized models exhibit greater robustness to unseen defects, mitigating the data collection hurdles often encountered in implementing visual inspection on production lines.  Furthermore, we show that object detection models, predicting both a label and bounding box, outperform label-only classifiers on held-out test data representative of manufacturing inspection scenarios. Finally, we investigated factors influencing model generalization to improve performance across a wider range of conditions.\n"}, "2406.09858": {"original_text": "  The visual quality of an image is confounded by a number of intertwined\nfactors including its semantic content, distortion characteristics and\nappearance properties such as brightness, contrast, sharpness, and\ncolourfulness. Distilling high level knowledge about all these quality bearing\nattributes is crucial for developing objective Image Quality Assessment\n(IQA).While existing solutions have modeled some of these aspects, a\ncomprehensive solution that involves all these important quality related\nattributes has not yet been developed. In this paper, we present a new blind\nIQA (BIQA) model termed Self-supervision and Vision-Language supervision Image\nQUality Evaluator (SLIQUE) that features a joint vision-language and visual\ncontrastive representation learning framework for acquiring high level\nknowledge about the images semantic contents, distortion characteristics and\nappearance properties for IQA. For training SLIQUE, we have developed a\nsystematic approach to constructing a first of its kind large image database\nannotated with all three categories of quality relevant texts. The Text\nAnnotated Distortion, Appearance and Content (TADAC) database has over 1.6\nmillion images annotated with textual descriptions of their semantic contents,\ndistortion characteristics and appearance properties. The method for\nconstructing TADAC and the database itself will be particularly useful for\nexploiting vision-language modeling for advanced IQA applications. Extensive\nexperimental results show that SLIQUE has superior performances over state of\nthe art, demonstrating the soundness of its design principle and the\neffectiveness of its implementation.\n", "rewritten_text": "Image quality is a complex interplay of semantic content, distortion characteristics, and appearance properties (brightness, contrast, sharpness, and colorfulness).  Developing objective Image Quality Assessment (IQA) requires distilling high-level knowledge about all these attributes. While existing solutions model some of these aspects, a comprehensive approach encompassing all relevant attributes remains elusive.  This paper introduces SLIQUE (Self-supervised and Vision-Language supervised Image QUality Evaluator), a novel blind IQA (BIQA) model.  SLIQUE employs a joint vision-language and visual contrastive representation learning framework to acquire high-level knowledge about an image's semantic content, distortion characteristics, and appearance properties.  To train SLIQUE, we created the Text Annotated Distortion, Appearance, and Content (TADAC) database\u2014a first-of-its-kind, large-scale image dataset (over 1.6 million images) annotated with textual descriptions of these three quality-relevant categories.  The TADAC database and its construction method are valuable resources for leveraging vision-language modeling in advanced IQA applications.  Extensive experimental results demonstrate SLIQUE's superior performance compared to state-of-the-art methods, validating its design and implementation.\n"}, "2406.16860": {"original_text": "  We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, addressing the difficulties involved in consolidating and\ninterpreting results from various tasks, and introduce a new vision-centric\nbenchmark, CV-Bench. To further improve visual grounding, we propose the\nSpatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that\nintegrates high-resolution vision features with LLMs while reducing the number\nof tokens. Additionally, we discuss the curation of high-quality visual\ninstruction-tuning data from publicly available sources, emphasizing the\nimportance of data source balancing and distribution ratio. Collectively,\nCambrian-1 not only achieves state-of-the-art performance but also serves as a\ncomprehensive, open cookbook for instruction-tuned MLLMs. We provide model\nweights, code, supporting tools, datasets, and detailed instruction-tuning and\nevaluation recipes. We hope our release will inspire and accelerate\nadvancements in multimodal systems and visual representation learning.\n", "rewritten_text": "We introduce Cambrian-1, a family of vision-centric multimodal large language models (MLLMs).  While powerful language models enhance multimodal capabilities, the design of vision components often remains insufficiently explored and disconnected from advancements in visual representation learning. This gap hinders accurate grounding in real-world scenarios.  To address this, we leverage LLMs and visual instruction tuning to evaluate diverse visual representations\u2014including self-supervised, strongly supervised, and hybrid approaches\u2014across over 20 vision encoders.  We critically analyze existing MLLM benchmarks, highlighting the challenges in consolidating and interpreting results across various tasks, and introduce a new vision-centric benchmark, CV-Bench.  Furthermore, we propose the Spatial Vision Aggregator (SVA), a dynamic, spatially aware module that efficiently integrates high-resolution visual features with LLMs while minimizing token count.  We also detail the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of balanced data sources and distribution ratios.  Cambrian-1 achieves state-of-the-art performance and serves as a comprehensive, open-source resource for instruction-tuned MLLMs.  We release model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation protocols.  Our work aims to inspire and accelerate progress in multimodal systems and visual representation learning.\n"}, "2403.09593": {"original_text": "  Names are essential to both human cognition and vision-language models.\nOpen-vocabulary models utilize class names as text prompts to generalize to\ncategories unseen during training. However, the precision of these names is\noften overlooked in existing datasets. In this paper, we address this\nunderexplored problem by presenting a framework for \"renovating\" names in\nopen-vocabulary segmentation benchmarks (RENOVATE). Our framework features a\nrenaming model that enhances the quality of names for each visual segment.\nThrough experiments, we demonstrate that our renovated names help train\nstronger open-vocabulary models with up to 15% relative improvement and\nsignificantly enhance training efficiency with improved data quality. We also\nshow that our renovated names improve evaluation by better measuring\nmisclassification and enabling fine-grained model analysis. We will provide our\ncode and relabelings for several popular segmentation datasets (MS COCO,\nADE20K, Cityscapes) to the research community.\n", "rewritten_text": "Names are fundamental to both human cognition and vision-language models. Open-vocabulary models rely on class names as text prompts to generalize to unseen categories, yet the precision of these names is often overlooked in existing datasets.  This paper addresses this under-explored problem by introducing RENOVATE, a framework for renovating names in open-vocabulary segmentation benchmarks.  RENOVATE features a renaming model that improves the quality of names for each visual segment.  Experiments demonstrate that these renovated names enable the training of stronger open-vocabulary models, achieving up to a 15% relative improvement and significantly enhancing training efficiency through improved data quality.  Furthermore, renovated names improve evaluation by providing a more accurate measure of misclassification and facilitating fine-grained model analysis.  To foster reproducibility, we will release our code and relabeled datasets (MS COCO, ADE20K, and Cityscapes) to the research community.\n"}, "2310.13201": {"original_text": "  Early identification of abnormalities in plants is an important task for\nensuring proper growth and achieving high yields from crops. Precision\nagriculture can significantly benefit from modern computer vision tools to make\nfarming strategies addressing these issues efficient and effective. As farming\nlands are typically quite large, farmers have to manually check vast areas to\ndetermine the status of the plants and apply proper treatments. In this work,\nwe consider the problem of automatically identifying abnormal regions in maize\nplants from images captured by a UAV. Using deep learning techniques, we have\ndeveloped a methodology which can detect different levels of abnormality (i.e.,\nlow, medium, high or no abnormality) in maize plants independently of their\ngrowth stage. The primary goal is to identify anomalies at the earliest\npossible stage in order to maximize the effectiveness of potential treatments.\nAt the same time, the proposed system can provide valuable information to human\nannotators for ground truth data collection by helping them to focus their\nattention on a much smaller set of images only. We have experimented with two\ndifferent but complimentary approaches, the first considering abnormality\ndetection as a classification problem and the second considering it as a\nregression problem. Both approaches can be generalized to different types of\nabnormalities and do not make any assumption about the abnormality occurring at\nan early plant growth stage which might be easier to detect due to the plants\nbeing smaller and easier to separate. As a case study, we have considered a\npublicly available data set which exhibits mostly Nitrogen deficiency in maize\nplants of various growth stages. We are reporting promising preliminary results\nwith an 88.89\\% detection accuracy of low abnormality and 100\\% detection\naccuracy of no abnormality.\n", "rewritten_text": "Early identification of plant abnormalities is crucial for optimal crop growth and yield.  Precision agriculture can significantly benefit from computer vision to improve the efficiency and effectiveness of farming strategies.  Traditional methods require farmers to manually inspect large areas, a time-consuming and labor-intensive process.  This work addresses this challenge by developing a deep-learning methodology for automatically identifying abnormal regions in maize plants from unmanned aerial vehicle (UAV) imagery.  Our approach detects varying levels of abnormality (low, medium, high, or none) regardless of the plant's growth stage, aiming for early anomaly detection to maximize treatment efficacy.  The system also assists human annotators by focusing their attention on a smaller subset of images for ground truth data collection.  We explored two complementary approaches:  classification and regression.  Both are generalizable to various abnormalities and do not assume early-stage detection, despite the potential ease of identification in smaller plants.  Using a publicly available dataset primarily exhibiting nitrogen deficiency in maize plants of diverse growth stages, we achieved promising preliminary results: 88.89% accuracy in detecting low abnormality and 100% accuracy in detecting no abnormality.\n"}, "1711.0664": {"original_text": "  We investigate the problem of producing structured graph representations of\nvisual scenes. Our work analyzes the role of motifs: regularly appearing\nsubstructures in scene graphs. We present new quantitative insights on such\nrepeated structures in the Visual Genome dataset. Our analysis shows that\nobject labels are highly predictive of relation labels but not vice-versa. We\nalso find that there are recurring patterns even in larger subgraphs: more than\n50% of graphs contain motifs involving at least two relations. Our analysis\nmotivates a new baseline: given object detections, predict the most frequent\nrelation between object pairs with the given labels, as seen in the training\nset. This baseline improves on the previous state-of-the-art by an average of\n3.6% relative improvement across evaluation settings. We then introduce Stacked\nMotif Networks, a new architecture designed to capture higher order motifs in\nscene graphs that further improves over our strong baseline by an average 7.1%\nrelative gain. Our code is available at github.com/rowanz/neural-motifs.\n", "rewritten_text": "This paper investigates the generation of structured graph representations for visual scenes.  We analyze the role of motifs\u2014repeated substructures within scene graphs\u2014and present novel quantitative insights based on the Visual Genome dataset.  Our analysis reveals that object labels strongly predict relation labels, but the converse is not true.  Furthermore, we find that recurring patterns exist even in larger subgraphs, with over 50% of graphs containing motifs involving at least two relations.  These findings motivate a new baseline: given object detections, predict the most frequent relation between object pairs based on their labels, as observed in the training data. This baseline achieves a 3.6% average relative improvement over the prior state-of-the-art across all evaluation settings.  We then introduce Stacked Motif Networks, a novel architecture designed to capture higher-order motifs in scene graphs, yielding a further 7.1% average relative improvement over our strong baseline.  Our code is available at github.com/rowanz/neural-motifs.\n"}, "2207.01932": {"original_text": "  Image Coding for Machines (ICM) aims to compress images for AI tasks analysis\nrather than meeting human perception. Learning a kind of feature that is both\ngeneral (for AI tasks) and compact (for compression) is pivotal for its\nsuccess. In this paper, we attempt to develop an ICM framework by learning\nuniversal features while also considering compression. We name such features as\nomnipotent features and the corresponding framework as Omni-ICM. Considering\nself-supervised learning (SSL) improves feature generalization, we integrate it\nwith the compression task into the Omni-ICM framework to learn omnipotent\nfeatures. However, it is non-trivial to coordinate semantics modeling in SSL\nand redundancy removing in compression, so we design a novel information\nfiltering (IF) module between them by co-optimization of instance\ndistinguishment and entropy minimization to adaptively drop information that is\nweakly related to AI tasks (e.g., some texture redundancy). Different from\nprevious task-specific solutions, Omni-ICM could directly support AI tasks\nanalysis based on the learned omnipotent features without joint training or\nextra transformation. Albeit simple and intuitive, Omni-ICM significantly\noutperforms existing traditional and learning-based codecs on multiple\nfundamental vision tasks.\n", "rewritten_text": "Image coding for machines (ICM) aims to compress images for AI task analysis, prioritizing efficiency over human perceptual quality.  Success hinges on learning features that are both generalizable across AI tasks and compact for efficient compression. This paper presents Omni-ICM, a novel ICM framework that learns such \"omnipotent\" features by integrating self-supervised learning (SSL) with compression.  Because SSL enhances feature generalization, we incorporate it directly into the Omni-ICM framework.  However, coordinating the semantic modeling of SSL with the redundancy removal of compression is challenging.  To address this, we introduce a novel information filtering (IF) module that co-optimizes instance distinguishability and entropy minimization. This module adaptively discards information weakly relevant to AI tasks, such as texture redundancy. Unlike previous task-specific approaches, Omni-ICM directly supports AI task analysis using its learned omnipotent features, eliminating the need for joint training or additional transformations.  Despite its simplicity, Omni-ICM significantly outperforms existing traditional and learning-based codecs on multiple fundamental vision tasks.\n"}, "1711.07183": {"original_text": "  Generating adversarial examples is an intriguing problem and an important way\nof understanding the working mechanism of deep neural networks. Most existing\napproaches generated perturbations in the image space, i.e., each pixel can be\nmodified independently. However, in this paper we pay special attention to the\nsubset of adversarial examples that correspond to meaningful changes in 3D\nphysical properties (like rotation and translation, illumination condition,\netc.). These adversaries arguably pose a more serious concern, as they\ndemonstrate the possibility of causing neural network failure by easy\nperturbations of real-world 3D objects and scenes.\n  In the contexts of object classification and visual question answering, we\naugment state-of-the-art deep neural networks that receive 2D input images with\na rendering module (either differentiable or not) in front, so that a 3D scene\n(in the physical space) is rendered into a 2D image (in the image space), and\nthen mapped to a prediction (in the output space). The adversarial\nperturbations can now go beyond the image space, and have clear meanings in the\n3D physical world. Though image-space adversaries can be interpreted as\nper-pixel albedo change, we verify that they cannot be well explained along\nthese physically meaningful dimensions, which often have a non-local effect.\nBut it is still possible to successfully attack beyond the image space on the\nphysical space, though this is more difficult than image-space attacks,\nreflected in lower success rates and heavier perturbations required.\n", "rewritten_text": "Generating adversarial examples is a crucial problem for understanding the inner workings of deep neural networks.  Most existing approaches generate perturbations in image space, modifying each pixel independently.  This paper, however, focuses on adversarial examples corresponding to meaningful changes in 3D physical properties, such as rotation, translation, and illumination.  These adversaries represent a more significant threat, demonstrating the vulnerability of neural networks to easily manipulated real-world 3D objects and scenes.\n\nIn the contexts of object classification and visual question answering, we augment state-of-the-art deep neural networks (receiving 2D input images) with a rendering module (differentiable or otherwise). This module renders a 3D scene (in physical space) into a 2D image (in image space), which is then mapped to a prediction (in output space).  This allows adversarial perturbations to extend beyond image space, possessing clear meanings in the 3D physical world. While image-space adversaries can be interpreted as per-pixel albedo changes, we demonstrate that they lack a consistent explanation along these physically meaningful dimensions, which often exhibit non-local effects.  Although attacking in physical space is more challenging than in image space\u2014resulting in lower success rates and requiring larger perturbations\u2014successful attacks are still possible.\n"}, "2105.02039": {"original_text": "  In this paper, we fill the research gap by adopting state-of-the-art computer\nvision techniques for the data extraction stage in a data mining system. As\nshown in Fig.1, this stage contains two subtasks, namely, plot element\ndetection and data conversion. For building a robust box detector, we\ncomprehensively compare different deep learning-based methods and find a\nsuitable method to detect box with high precision. For building a robust point\ndetector, a fully convolutional network with feature fusion module is adopted,\nwhich can distinguish close points compared to traditional methods. The\nproposed system can effectively handle various chart data without making\nheuristic assumptions. For data conversion, we translate the detected element\ninto data with semantic value. A network is proposed to measure feature\nsimilarities between legends and detected elements in the legend matching\nphase. Furthermore, we provide a baseline on the competition of Harvesting raw\ntables from Infographics. Some key factors have been found to improve the\nperformance of each stage. Experimental results demonstrate the effectiveness\nof the proposed system.\n", "rewritten_text": "This paper addresses a research gap by applying state-of-the-art computer vision techniques to the data extraction stage of a data mining system.  As illustrated in Figure 1, this stage comprises two subtasks: plot element detection and data conversion.  For robust box detection, we comprehensively compared various deep learning methods, selecting one that achieves high precision.  Robust point detection is achieved using a fully convolutional network with a feature fusion module, which outperforms traditional methods in distinguishing closely spaced points.  The proposed system effectively handles diverse chart data without relying on heuristic assumptions.  Data conversion involves translating detected elements into semantically meaningful data.  A novel network measures feature similarity between legends and detected elements during legend matching.  Finally, we establish a baseline for the \"Harvesting Raw Tables from Infographics\" competition.  Key performance-enhancing factors for each stage are identified.  Experimental results demonstrate the system's effectiveness.\n"}, "2401.08508": {"original_text": "  Sentiment analysis and emotion detection are important research topics in\nnatural language processing (NLP) and benefit many downstream tasks. With the\nwidespread application of LLMs, researchers have started exploring the\napplication of LLMs based on instruction-tuning in the field of sentiment\nanalysis. However, these models only focus on single aspects of affective\nclassification tasks (e.g. sentimental polarity or categorical emotions), and\noverlook the regression tasks (e.g. sentiment strength or emotion intensity),\nwhich leads to poor performance in downstream tasks. The main reason is the\nlack of comprehensive affective instruction tuning datasets and evaluation\nbenchmarks, which cover various affective classification and regression tasks.\nMoreover, although emotional information is useful for downstream tasks,\nexisting downstream datasets lack high-quality and comprehensive affective\nannotations. In this paper, we propose EmoLLMs, the first series of\nopen-sourced instruction-following LLMs for comprehensive affective analysis\nbased on fine-tuning various LLMs with instruction data, the first multi-task\naffective analysis instruction dataset (AAID) with 234K data samples based on\nvarious classification and regression tasks to support LLM instruction tuning,\nand a comprehensive affective evaluation benchmark (AEB) with 14 tasks from\nvarious sources and domains to test the generalization ability of LLMs. We\npropose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various\naffective instruction tasks. We compare our model with a variety of LLMs on\nAEB, where our models outperform all other open-sourced LLMs, and surpass\nChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve\nthe ChatGPT-level and GPT-4-level generalization capabilities on affective\nanalysis tasks, and demonstrates our models can be used as affective annotation\ntools.\n", "rewritten_text": "Sentiment analysis and emotion detection are crucial research areas in natural language processing (NLP), benefiting numerous downstream applications.  While the widespread adoption of large language models (LLMs) has spurred research into instruction-tuned LLMs for sentiment analysis, these models typically focus solely on single aspects of affective classification (e.g., sentiment polarity or categorical emotions), neglecting regression tasks (e.g., sentiment strength or emotion intensity). This limitation hinders performance in downstream applications.  The primary cause is the scarcity of comprehensive instruction-tuning datasets and evaluation benchmarks encompassing diverse affective classification and regression tasks.  Furthermore, despite the value of emotional information for downstream tasks, existing datasets often lack high-quality, comprehensive affective annotations.\n\nThis paper introduces EmoLLMs, the first open-sourced series of instruction-following LLMs for comprehensive affective analysis.  We achieve this by fine-tuning various LLMs using a novel multi-task affective analysis instruction dataset (AAID), comprising 234,000 samples across diverse classification and regression tasks.  We also introduce a comprehensive affective evaluation benchmark (AEB) containing 14 tasks from various sources and domains, designed to assess LLM generalization capabilities.  Our EmoLLMs, fine-tuned with AAID, excel at various affective instruction tasks.  Evaluated on AEB, our models outperform all other open-sourced LLMs and surpass ChatGPT and GPT-4 on most tasks, demonstrating ChatGPT- and GPT-4-level generalization in affective analysis and establishing their potential as affective annotation tools.\n"}, "2210.05564": {"original_text": "  Semantic segmentation is a fundamental topic in computer vision. Several deep\nlearning methods have been proposed for semantic segmentation with outstanding\nresults. However, these models require a lot of densely annotated images. To\naddress this problem, we propose a new algorithm that uses HyperGraph\nConvolutional Networks for Weakly-supervised Semantic Segmentation\n(HyperGCN-WSS). Our algorithm constructs spatial and k-Nearest Neighbor (k-NN)\ngraphs from the images in the dataset to generate the hypergraphs. Then, we\ntrain a specialized HyperGraph Convolutional Network (HyperGCN) architecture\nusing some weak signals. The outputs of the HyperGCN are denominated\npseudo-labels, which are later used to train a DeepLab model for semantic\nsegmentation. HyperGCN-WSS is evaluated on the PASCAL VOC 2012 dataset for\nsemantic segmentation, using scribbles or clicks as weak signals. Our algorithm\nshows competitive performance against previous methods.\n", "rewritten_text": "Semantic segmentation is a fundamental task in computer vision.  While deep learning has yielded impressive results, these models typically require extensive densely annotated training data.  To address this data scarcity, we propose HyperGCN-WSS (HyperGraph Convolutional Network for Weakly-Supervised Semantic Segmentation), a novel algorithm that leverages weakly supervised learning.  HyperGCN-WSS constructs spatial and k-Nearest Neighbor (k-NN) graphs from the input images to generate hypergraphs.  A specialized HyperGraph Convolutional Network (HyperGCN) architecture is then trained using weak supervisory signals (e.g., scribbles or clicks).  The HyperGCN's output, termed pseudo-labels, are subsequently used to train a DeepLab model for semantic segmentation.  Evaluated on the PASCAL VOC 2012 dataset, HyperGCN-WSS demonstrates competitive performance against state-of-the-art methods.\n"}, "2112.0618": {"original_text": "  We present 360-DFPE, a sequential floor plan estimation method that directly\ntakes 360-images as input without relying on active sensors or 3D information.\nOur approach leverages a loosely coupled integration between a monocular visual\nSLAM solution and a monocular 360-room layout approach, which estimate camera\nposes and layout geometries, respectively. Since our task is to sequentially\ncapture the floor plan using monocular images, the entire scene structure, room\ninstances, and room shapes are unknown. To tackle these challenges, we first\nhandle the scale difference between visual odometry and layout geometry via\nformulating an entropy minimization process, which enables us to directly align\n360-layouts without knowing the entire scene in advance. Second, to\nsequentially identify individual rooms, we propose a novel room identification\nalgorithm that tracks every room along the camera exploration using geometry\ninformation. Lastly, to estimate the final shape of the room, we propose a\nshortest path algorithm with an iterative coarse-to-fine strategy, which\nimproves prior formulations with higher accuracy and faster run-time. Moreover,\nwe collect a new floor plan dataset with challenging large-scale scenes,\nproviding both point clouds and sequential 360-image information. Experimental\nresults show that our monocular solution achieves favorable performance against\nthe current state-of-the-art algorithms that rely on active sensors and require\nthe entire scene reconstruction data in advance.\n", "rewritten_text": "We introduce 360-DFPE, a sequential floor plan estimation method that directly uses 360\u00b0 images as input, eliminating the need for active sensors or pre-existing 3D information.  Our approach employs a loosely coupled integration of monocular visual simultaneous localization and mapping (SLAM) and a monocular 360\u00b0 room layout estimation method to respectively estimate camera poses and layout geometries.  Because our method sequentially captures the floor plan using only monocular images, the complete scene structure, individual rooms, and their shapes are initially unknown.  To address this, we first resolve the scale discrepancy between visual odometry and layout geometry through an entropy minimization process, enabling direct alignment of 360\u00b0 layouts without prior knowledge of the entire scene. Second, we introduce a novel room identification algorithm that tracks individual rooms throughout camera exploration using geometric information. Finally, we propose a shortest-path algorithm with an iterative coarse-to-fine strategy for accurate and efficient room shape estimation, improving upon existing methods.  Furthermore, we contribute a new floor plan dataset featuring challenging large-scale scenes, providing both point cloud and sequential 360\u00b0 image data.  Experimental results demonstrate that our monocular solution achieves performance comparable to, or exceeding, state-of-the-art algorithms that rely on active sensors and complete scene reconstruction.\n"}, "2205.08811": {"original_text": "  Object pose estimation is crucial for robotic applications and augmented\nreality. Beyond instance level 6D object pose estimation methods, estimating\ncategory-level pose and shape has become a promising trend. As such, a new\nresearch field needs to be supported by well-designed datasets. To provide a\nbenchmark with high-quality ground truth annotations to the community, we\nintroduce a multimodal dataset for category-level object pose estimation with\nphotometrically challenging objects termed PhoCaL. PhoCaL comprises 60 high\nquality 3D models of household objects over 8 categories including highly\nreflective, transparent and symmetric objects. We developed a novel\nrobot-supported multi-modal (RGB, depth, polarisation) data acquisition and\nannotation process. It ensures sub-millimeter accuracy of the pose for opaque\ntextured, shiny and transparent objects, no motion blur and perfect camera\nsynchronisation. To set a benchmark for our dataset, state-of-the-art RGB-D and\nmonocular RGB methods are evaluated on the challenging scenes of PhoCaL.\n", "rewritten_text": "Object pose estimation is crucial for robotic applications and augmented reality.  While instance-level 6D object pose estimation methods are well-established, estimating category-level pose and shape is a promising emerging area.  This necessitates well-designed datasets to support research.  To this end, we introduce PhoCaL, a multimodal dataset for category-level object pose estimation featuring photometrically challenging objects.  PhoCaL comprises 60 high-quality 3D models of household objects across eight categories, including highly reflective, transparent, and symmetric objects.  We developed a novel robot-assisted multimodal (RGB, depth, polarization) data acquisition and annotation process, ensuring sub-millimeter pose accuracy for opaque, textured, shiny, and transparent objects, eliminating motion blur, and guaranteeing perfect camera synchronization.  To benchmark the dataset, we evaluate state-of-the-art RGB-D and monocular RGB methods on PhoCaL's challenging scenes.\n"}, "2005.14439": {"original_text": "  Dynamic routing networks, aimed at finding the best routing paths in the\nnetworks, have achieved significant improvements to neural networks in terms of\naccuracy and efficiency. In this paper, we see dynamic routing networks in a\nfresh light, formulating a routing method as a mapping from a sample space to a\nrouting space. From the perspective of space mapping, prevalent methods of\ndynamic routing didn't consider how inference paths would be distributed in the\nrouting space. Thus, we propose a novel method, termed CoDiNet, to model the\nrelationship between a sample space and a routing space by regularizing the\ndistribution of routing paths with the properties of consistency and diversity.\nSpecifically, samples with similar semantics should be mapped into the same\narea in routing space, while those with dissimilar semantics should be mapped\ninto different areas. Moreover, we design a customizable dynamic routing\nmodule, which can strike a balance between accuracy and efficiency. When\ndeployed upon ResNet models, our method achieves higher performance and\neffectively reduces average computational cost on four widely used datasets.\n", "rewritten_text": "Dynamic routing networks have significantly improved neural network accuracy and efficiency by optimizing path selection.  This paper offers a novel perspective, framing dynamic routing as a mapping from a sample space to a routing space.  Existing methods, however, neglect the distribution of inference paths within this routing space.  We therefore propose CoDiNet, a method that regularizes the distribution of routing paths by enforcing consistency and diversity.  Specifically, semantically similar samples are mapped to proximate regions in the routing space, while dissimilar samples are mapped to distinct regions.  Furthermore, we introduce a customizable dynamic routing module that balances accuracy and efficiency.  Experiments on four widely used datasets demonstrate that, when integrated with ResNet models, CoDiNet achieves superior performance while significantly reducing average computational cost.\n"}, "2110.05594": {"original_text": "  We present a modern solution to the multi-view photometric stereo problem\n(MVPS). Our work suitably exploits the image formation model in a MVPS\nexperimental setup to recover the dense 3D reconstruction of an object from\nimages. We procure the surface orientation using a photometric stereo (PS)\nimage formation model and blend it with a multi-view neural radiance field\nrepresentation to recover the object's surface geometry. Contrary to the\nprevious multi-staged framework to MVPS, where the position, iso-depth\ncontours, or orientation measurements are estimated independently and then\nfused later, our method is simple to implement and realize. Our method performs\nneural rendering of multi-view images while utilizing surface normals estimated\nby a deep photometric stereo network. We render the MVPS images by considering\nthe object's surface normals for each 3D sample point along the viewing\ndirection rather than explicitly using the density gradient in the volume space\nvia 3D occupancy information. We optimize the proposed neural radiance field\nrepresentation for the MVPS setup efficiently using a fully connected deep\nnetwork to recover the 3D geometry of an object. Extensive evaluation on the\nDiLiGenT-MV benchmark dataset shows that our method performs better than the\napproaches that perform only PS or only multi-view stereo (MVS) and provides\ncomparable results against the state-of-the-art multi-stage fusion methods.\n", "rewritten_text": "This paper presents a novel solution to the multi-view photometric stereo (MVPS) problem.  Leveraging the image formation model inherent in MVPS setups, our method recovers dense 3D reconstructions from multiple images.  We achieve this by combining surface orientation estimation using a photometric stereo (PS) model with a multi-view neural radiance field (NeRF) representation. Unlike previous multi-stage MVPS approaches that independently estimate and fuse position, iso-depth contours, or orientation, our method offers a simpler, unified implementation.  It performs neural rendering of multi-view images using surface normals estimated by a deep PS network.  Instead of relying on 3D occupancy information and density gradients, our rendering process considers surface normals at each 3D sample point along the viewing direction.  A fully connected deep network efficiently optimizes the NeRF representation for the MVPS setup, recovering the object's 3D geometry.  Extensive evaluation on the DiLiGenT-MV benchmark dataset demonstrates superior performance compared to methods relying solely on PS or multi-view stereo (MVS), and yields results comparable to state-of-the-art multi-stage fusion techniques.\n"}, "2206.13263": {"original_text": "  Robust maritime obstacle detection is critical for safe navigation of\nautonomous boats and timely collision avoidance. The current state-of-the-art\nis based on deep segmentation networks trained on large datasets. However,\nper-pixel ground truth labeling of such datasets is labor-intensive and\nexpensive. We propose a new scaffolding learning regime (SLR) that leverages\nweak annotations consisting of water edges, the horizon location, and obstacle\nbounding boxes to train segmentation-based obstacle detection networks, thereby\nreducing the required ground truth labeling effort by a factor of twenty. SLR\ntrains an initial model from weak annotations and then alternates between\nre-estimating the segmentation pseudo-labels and improving the network\nparameters. Experiments show that maritime obstacle segmentation networks\ntrained using SLR on weak annotations not only match but outperform the same\nnetworks trained with dense ground truth labels, which is a remarkable result.\nIn addition to the increased accuracy, SLR also increases domain generalization\nand can be used for domain adaptation with a low manual annotation load. The\nSLR code and pre-trained models are available at\nhttps://github.com/lojzezust/SLR .\n", "rewritten_text": "Robust maritime obstacle detection is critical for the safe navigation of autonomous boats and timely collision avoidance.  Current state-of-the-art methods rely on deep segmentation networks trained on large, labor-intensive and expensive datasets requiring per-pixel ground truth labeling.  We propose a novel scaffolding learning regime (SLR) that leverages weak annotations\u2014water edges, horizon location, and obstacle bounding boxes\u2014to train segmentation-based obstacle detection networks.  This reduces the ground truth labeling effort by a factor of twenty.  SLR trains an initial model using these weak annotations and then iteratively refines both segmentation pseudo-labels and network parameters.  Experiments demonstrate that maritime obstacle segmentation networks trained with SLR on weakly annotated data not only match but surpass the performance of networks trained with dense ground truth labels\u2014a remarkable result.  Beyond improved accuracy, SLR enhances domain generalization and facilitates domain adaptation with minimal manual annotation. The SLR code and pre-trained models are available at https://github.com/lojzezust/SLR.\n"}, "2204.07126": {"original_text": "  Recent development of neural implicit function has shown tremendous success\non high-quality 3D shape reconstruction. However, most works divide the space\ninto inside and outside of the shape, which limits their representing power to\nsingle-layer and watertight shapes. This limitation leads to tedious data\nprocessing (converting non-watertight raw data to watertight) as well as the\nincapability of representing general object shapes in the real world. In this\nwork, we propose a novel method to represent general shapes including\nnon-watertight shapes and shapes with multi-layer surfaces. We introduce\nGeneral Implicit Function for 3D Shape (GIFS), which models the relationships\nbetween every two points instead of the relationships between points and\nsurfaces. Instead of dividing 3D space into predefined inside-outside regions,\nGIFS encodes whether two points are separated by any surface. Experiments on\nShapeNet show that GIFS outperforms previous state-of-the-art methods in terms\nof reconstruction quality, rendering efficiency, and visual fidelity. Project\npage is available at https://jianglongye.com/gifs .\n", "rewritten_text": "Recent advancements in neural implicit functions have demonstrated remarkable success in high-quality 3D shape reconstruction.  However, most existing methods partition space into regions inside and outside the shape, limiting their representational power to single-layer, watertight shapes. This limitation necessitates tedious data preprocessing (converting non-watertight raw data to watertight meshes) and prevents the representation of general, real-world object shapes.  This work proposes a novel method, General Implicit Function for 3D Shapes (GIFS), to represent general shapes, including non-watertight and multi-layer surfaces.  Unlike previous methods, GIFS models the relationships between all pairs of points, rather than focusing solely on point-surface relationships.  Instead of predefining inside-outside regions, GIFS encodes whether two points are separated by any surface.  Experiments on ShapeNet demonstrate that GIFS surpasses state-of-the-art methods in reconstruction quality, rendering efficiency, and visual fidelity.  The project page is available at https://jianglongye.com/gifs.\n"}, "2103.03166": {"original_text": "  Annotated medical images are typically rarer than labeled natural images\nsince they are limited by domain knowledge and privacy constraints. Recent\nadvances in transfer and contrastive learning have provided effective solutions\nto tackle such issues from different perspectives. The state-of-the-art\ntransfer learning (e.g., Big Transfer (BiT)) and contrastive learning (e.g.,\nSimple Siamese Contrastive Learning (SimSiam)) approaches have been\ninvestigated independently, without considering the complementary nature of\nsuch techniques. It would be appealing to accelerate contrastive learning with\ntransfer learning, given that slow convergence speed is a critical limitation\nof modern contrastive learning approaches. In this paper, we investigate the\nfeasibility of aligning BiT with SimSiam. From empirical analyses, different\nnormalization techniques (Group Norm in BiT vs. Batch Norm in SimSiam) are the\nkey hurdle of adapting BiT to SimSiam. When combining BiT with SimSiam, we\nevaluated the performance of using BiT, SimSiam, and BiT+SimSiam on CIFAR-10\nand HAM10000 datasets. The results suggest that the BiT models accelerate the\nconvergence speed of SimSiam. When used together, the model gives superior\nperformance over both of its counterparts. We hope this study will motivate\nresearchers to revisit the task of aggregating big pre-trained models with\ncontrastive learning models for image analysis.\n", "rewritten_text": "Annotated medical images are scarcer than labeled natural images due to limitations imposed by domain expertise and privacy concerns.  Recent advancements in transfer and contrastive learning offer promising solutions to this challenge. While state-of-the-art transfer learning methods (e.g., Big Transfer (BiT)) and contrastive learning methods (e.g., Simple Siamese Contrastive Learning (SimSiam)) have been explored independently, their complementary nature remains largely untapped.  Given the slow convergence speed of modern contrastive learning, accelerating it with transfer learning is highly desirable.  This paper investigates the feasibility of integrating BiT with SimSiam.  Our empirical analysis reveals that differing normalization techniques (Group Normalization in BiT versus Batch Normalization in SimSiam) pose a significant challenge to their integration.  We evaluated the performance of BiT, SimSiam, and their combination (BiT+SimSiam) on the CIFAR-10 and HAM10000 datasets.  Results demonstrate that BiT accelerates SimSiam's convergence and that the combined model outperforms both individual approaches.  This study encourages further research into integrating large pre-trained models with contrastive learning models for improved image analysis.\n"}, "2401.00642": {"original_text": "  During times of increasing antibiotic resistance and the spread of infectious\ndiseases like COVID-19, it is important to classify genes related to antibiotic\nresistance. As natural language processing has advanced with transformer-based\nlanguage models, many language models that learn characteristics of nucleotide\nsequences have also emerged. These models show good performance in classifying\nvarious features of nucleotide sequences. When classifying nucleotide\nsequences, not only the sequence itself, but also various background knowledge\nis utilized. In this study, we use not only a nucleotide sequence-based\nlanguage model but also a text language model based on PubMed articles to\nreflect more biological background knowledge in the model. We propose a method\nto fine-tune the nucleotide sequence language model and the text language model\nbased on various databases of antibiotic resistance genes. We also propose an\nLLM-based augmentation technique to supplement the data and an ensemble method\nto effectively combine the two models. We also propose a benchmark for\nevaluating the model. Our method achieved better performance than the\nnucleotide sequence language model in the drug resistance class prediction.\n", "rewritten_text": "The rise of antibiotic resistance and infectious diseases like COVID-19 underscores the critical need for accurate classification of antibiotic resistance genes.  While transformer-based language models have advanced natural language processing,  similar models have emerged for analyzing nucleotide sequences, demonstrating strong performance in various sequence feature classifications.  However, effective classification relies not only on sequence data but also on incorporating relevant biological context.  This study leverages both a nucleotide sequence language model and a text language model trained on PubMed articles to integrate broader biological knowledge.  We propose a novel method for fine-tuning these models using diverse antibiotic resistance gene databases, complemented by an LLM-based data augmentation technique and an ensemble method for optimal model fusion.  Furthermore, we introduce a benchmark for evaluating model performance.  Our approach significantly outperforms the nucleotide sequence-only model in predicting drug resistance classes.\n"}, "2406.15718": {"original_text": "  As large language models (LLMs) increasingly permeate daily lives, there is a\ngrowing demand for real-time interactions that mirror human conversations.\nTraditional turn-based chat systems driven by LLMs prevent users from verbally\ninteracting with the system while it is generating responses. To overcome these\nlimitations, we adapt existing LLMs to \\textit{duplex models} so that these\nLLMs can listen for users while generating output and dynamically adjust\nthemselves to provide users with instant feedback. % such as in response to\ninterruptions. Specifically, we divide the queries and responses of\nconversations into several time slices and then adopt a\ntime-division-multiplexing (TDM) encoding-decoding strategy to\npseudo-simultaneously process these slices. Furthermore, to make LLMs\nproficient enough to handle real-time conversations, we build a fine-tuning\ndataset consisting of alternating time slices of queries and responses as well\nas covering typical feedback types in instantaneous interactions. Our\nexperiments show that although the queries and responses of conversations are\nsegmented into incomplete slices for processing, LLMs can preserve their\noriginal performance on standard benchmarks with a few fine-tuning steps on our\ndataset. Automatic and human evaluation indicate that duplex models make\nuser-AI interactions more natural and human-like, and greatly improve user\nsatisfaction compared to vanilla LLMs. Our duplex model and dataset will be\nreleased.\n", "rewritten_text": "The increasing prevalence of large language models (LLMs) in daily life necessitates real-time, conversational interactions mirroring human dialogue.  Traditional turn-based LLM-driven chat systems hinder natural verbal interaction due to response generation delays.  To address this, we adapt LLMs into duplex models capable of simultaneous listening and response generation, enabling immediate feedback.  This is achieved by segmenting conversational queries and responses into time slices and employing a time-division-multiplexing (TDM) encoding-decoding strategy for pseudo-simultaneous processing.  Furthermore, to enhance LLM performance in real-time conversations, we created a fine-tuning dataset comprising alternating time-sliced queries and responses, encompassing typical instantaneous feedback scenarios.  Our experiments demonstrate that despite processing incomplete conversational slices, LLMs maintain performance on standard benchmarks after minimal fine-tuning on our dataset.  Automatic and human evaluations reveal that duplex models foster more natural and human-like user-AI interactions, significantly improving user satisfaction compared to standard LLMs.  Our duplex model and dataset will be publicly released.\n"}, "2011.01535": {"original_text": "  3D-LaneNet+ is a camera-based DNN method for anchor free 3D lane detection\nwhich is able to detect 3d lanes of any arbitrary topology such as splits,\nmerges, as well as short and perpendicular lanes. We follow recently proposed\n3D-LaneNet, and extend it to enable the detection of these previously\nunsupported lane topologies. Our output representation is an anchor free,\nsemi-local tile representation that breaks down lanes into simple lane segments\nwhose parameters can be learnt. In addition we learn, per lane instance,\nfeature embedding that reasons for the global connectivity of locally detected\nsegments to form full 3d lanes. This combination allows 3D-LaneNet+ to avoid\nusing lane anchors, non-maximum suppression, and lane model fitting as in the\noriginal 3D-LaneNet. We demonstrate the efficacy of 3D-LaneNet+ using both\nsynthetic and real world data. Results show significant improvement relative to\nthe original 3D-LaneNet that can be attributed to better generalization to\ncomplex lane topologies, curvatures and surface geometries.\n", "rewritten_text": "3D-LaneNet+ is a camera-based, anchor-free deep neural network (DNN) method for 3D lane detection.  Unlike its predecessor, 3D-LaneNet, it can detect lanes of arbitrary topology, including splits, merges, short lanes, and perpendicular lanes.  We extend 3D-LaneNet by employing a novel, anchor-free, semi-local tile representation that decomposes lanes into learnable segments.  Furthermore, we learn per-lane instance feature embeddings to infer the global connectivity of these local segments, forming complete 3D lane representations. This approach eliminates the need for lane anchors, non-maximum suppression, and explicit lane model fitting, unlike the original 3D-LaneNet.  We demonstrate 3D-LaneNet+'s effectiveness on both synthetic and real-world datasets, achieving significant improvements over 3D-LaneNet, particularly in generalizing to complex lane topologies, curvatures, and surface geometries.\n"}, "1612.06496": {"original_text": "  Image segmentation is a popular area of research in computer vision that has\nmany applications in automated image processing. A recent technique called\npiecewise flat embeddings (PFE) has been proposed for use in image\nsegmentation; PFE transforms image pixel data into a lower dimensional\nrepresentation where similar pixels are pulled close together and dissimilar\npixels are pushed apart. This technique has shown promising results, but its\noriginal formulation is not computationally feasible for large images. We\npropose two improvements to the algorithm for computing PFE: first, we\nreformulate portions of the algorithm to enable various linear algebra\noperations to be performed in parallel; second, we propose utilizing an\niterative linear solver (preconditioned conjugate gradient) to quickly solve a\nlinear least-squares problem that occurs in the inner loop of a nested\niteration. With these two computational improvements, we show on a publicly\navailable image database that PFE can be sped up by an order of magnitude\nwithout sacrificing segmentation performance. Our results make this technique\nmore practical for use on large data sets, not only for image segmentation, but\nfor general data clustering problems.\n", "rewritten_text": "Piecewise flat embeddings (PFE) are a promising technique for image segmentation, transforming image pixel data into a lower-dimensional representation where similar pixels cluster and dissimilar pixels separate.  While demonstrating promising results, the original PFE algorithm's computational cost prohibits its application to large images.  This paper presents two key improvements to address this limitation.  First, we reformulate portions of the algorithm to enable parallelization of linear algebra operations. Second, we employ a preconditioned conjugate gradient iterative linear solver to efficiently solve the linear least-squares problem within the nested iterative loop.  Using a publicly available image database, we demonstrate that these improvements accelerate PFE by an order of magnitude without compromising segmentation performance.  This enhanced efficiency makes PFE a practical solution for large datasets, extending its applicability beyond image segmentation to general data clustering tasks.\n"}, "1511.06783": {"original_text": "  We present a novel dataset and a novel algorithm for recognizing activities\nof daily living (ADL) from a first-person wearable camera. Handled objects are\ncrucially important for egocentric ADL recognition. For specific examination of\nobjects related to users' actions separately from other objects in an\nenvironment, many previous works have addressed the detection of handled\nobjects in images captured from head-mounted and chest-mounted cameras.\nNevertheless, detecting handled objects is not always easy because they tend to\nappear small in images. They can be occluded by a user's body. As described\nherein, we mount a camera on a user's wrist. A wrist-mounted camera can capture\nhandled objects at a large scale, and thus it enables us to skip object\ndetection process. To compare a wrist-mounted camera and a head-mounted camera,\nwe also develop a novel and publicly available dataset that includes videos and\nannotations of daily activities captured simultaneously by both cameras.\nAdditionally, we propose a discriminative video representation that retains\nspatial and temporal information after encoding frame descriptors extracted by\nConvolutional Neural Networks (CNN).\n", "rewritten_text": "This paper introduces a novel dataset and algorithm for recognizing activities of daily living (ADLs) from a first-person perspective using a wrist-mounted camera.  Accurate egocentric ADL recognition critically depends on identifying handled objects. While previous research has focused on detecting these objects in images from head-mounted and chest-mounted cameras\u2014a challenging task due to the objects' small size and potential occlusion by the user\u2014our approach utilizes a wrist-mounted camera. This allows for larger-scale capture of handled objects, eliminating the need for explicit object detection.  To facilitate comparison with head-mounted camera data, we have also created a novel, publicly available dataset containing synchronized video and annotations from both camera perspectives.  Furthermore, we propose a discriminative video representation that preserves both spatial and temporal information through the encoding of Convolutional Neural Network (CNN)-extracted frame descriptors.\n"}, "2305.11806": {"original_text": "  Neural metrics for machine translation evaluation, such as COMET, exhibit\nsignificant improvements in their correlation with human judgments, as compared\nto traditional metrics based on lexical overlap, such as BLEU. Yet, neural\nmetrics are, to a great extent, \"black boxes\" returning a single sentence-level\nscore without transparency about the decision-making process. In this work, we\ndevelop and compare several neural explainability methods and demonstrate their\neffectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our\nstudy reveals that these metrics leverage token-level information that can be\ndirectly attributed to translation errors, as assessed through comparison of\ntoken-level neural saliency maps with Multidimensional Quality Metrics (MQM)\nannotations and with synthetically-generated critical translation errors. To\nease future research, we release our code at:\nhttps://github.com/Unbabel/COMET/tree/explainable-metrics.\n", "rewritten_text": "Recent neural machine translation (NMT) evaluation metrics, such as COMET, demonstrate significantly improved correlation with human judgments compared to traditional lexical overlap metrics like BLEU.  However, these neural metrics often function as \"black boxes,\" providing only a single sentence-level score without transparent decision-making processes.  This work develops and compares several neural explainability methods to interpret state-of-the-art, fine-tuned neural metrics.  Our study reveals that these metrics utilize token-level information directly attributable to translation errors. This is demonstrated by comparing token-level neural saliency maps with Multidimensional Quality Metrics (MQM) annotations and synthetically generated critical translation errors.  To facilitate future research, our code is publicly available at: https://github.com/Unbabel/COMET/tree/explainable-metrics.\n"}, "1906.11143": {"original_text": "  Accurate segmentation of the optic disc (OD) and cup (OC)in fundus images\nfrom different datasets is critical for glaucoma disease screening. The\ncross-domain discrepancy (domain shift) hinders the generalization of deep\nneural networks to work on different domain datasets.In this work, we present\nan unsupervised domain adaptation framework,called Boundary and Entropy-driven\nAdversarial Learning (BEAL), to improve the OD and OC segmentation performance,\nespecially on the ambiguous boundary regions. In particular, our proposed BEAL\nframe-work utilizes the adversarial learning to encourage the boundary\nprediction and mask probability entropy map (uncertainty map) of the target\ndomain to be similar to the source ones, generating more accurate boundaries\nand suppressing the high uncertainty predictions of OD and OC segmentation. We\nevaluate the proposed BEAL framework on two public retinal fundus image\ndatasets (Drishti-GS and RIM-ONE-r3), and the experiment results demonstrate\nthat our method outperforms the state-of-the-art unsupervised domain adaptation\nmethods. Codes will be available at https://github.com/EmmaW8/BEAL.\n", "rewritten_text": "Accurate segmentation of the optic disc (OD) and cup (OC) in fundus images from diverse datasets is critical for glaucoma screening.  Cross-domain discrepancies (domain shift) hinder the generalization of deep neural networks across different datasets. This work presents Boundary and Entropy-driven Adversarial Learning (BEAL), an unsupervised domain adaptation framework designed to improve OD and OC segmentation performance, particularly in ambiguous boundary regions.  BEAL leverages adversarial learning to align the boundary predictions and mask probability entropy maps (uncertainty maps) of the target and source domains. This alignment generates more accurate boundaries and suppresses high-uncertainty predictions in OD and OC segmentation.  We evaluated BEAL on two public retinal fundus image datasets (Drishti-GS and RIM-ONE-r3).  Experimental results demonstrate that our method outperforms state-of-the-art unsupervised domain adaptation methods. Code is available at https://github.com/EmmaW8/BEAL.\n"}, "2108.08109": {"original_text": "  Illustrations are an essential transmission instrument. For an historian, the\nfirst step in studying their evolution in a corpus of similar manuscripts is to\nidentify which ones correspond to each other. This image collation task is\ndaunting for manuscripts separated by many lost copies, spreading over\ncenturies, which might have been completely re-organized and greatly modified\nto adapt to novel knowledge or belief and include hundreds of illustrations.\nOur contributions in this paper are threefold. First, we introduce the task of\nillustration collation and a large annotated public dataset to evaluate\nsolutions, including 6 manuscripts of 2 different texts with more than 2 000\nillustrations and 1 200 annotated correspondences. Second, we analyze state of\nthe art similarity measures for this task and show that they succeed in simple\ncases but struggle for large manuscripts when the illustrations have undergone\nvery significant changes and are discriminated only by fine details. Finally,\nwe show clear evidence that significant performance boosts can be expected by\nexploiting cycle-consistent correspondences. Our code and data are available on\nhttp://imagine.enpc.fr/~shenx/ImageCollation.\n", "rewritten_text": "Illustrations are essential tools for transmitting information.  For historians studying the evolution of illustrations within a corpus of similar manuscripts, the initial step is identifying corresponding images. This image collation task is daunting, particularly for manuscripts separated by numerous lost copies, spanning centuries, and potentially undergoing significant reorganization, modification to reflect evolving knowledge or beliefs, and incorporating hundreds of illustrations.\n\nThis paper makes three contributions. First, we introduce the task of illustration collation and a large, publicly available annotated dataset for evaluating solutions. This dataset comprises six manuscripts of two different texts, containing over 2,000 illustrations and 1,200 annotated correspondences. Second, we analyze state-of-the-art similarity measures for this task, demonstrating their effectiveness in simple cases but highlighting their limitations when applied to large manuscripts with significantly altered illustrations differentiated only by subtle details. Finally, we present strong evidence that exploiting cycle-consistent correspondences yields significant performance improvements.  Our code and data are available at http://imagine.enpc.fr/~shenx/ImageCollation.\n"}, "2204.01062": {"original_text": "  Several popular computer vision (CV) datasets, specifically employed for\nObject Detection (OD) in autonomous driving tasks exhibit biases due to a range\nof factors including weather and lighting conditions. These biases may impair a\nmodel's generalizability, rendering it ineffective for OD in novel and unseen\ndatasets. Especially, in autonomous driving, it may prove extremely high risk\nand unsafe for the vehicle and its surroundings. This work focuses on\nunderstanding these datasets better by identifying such \"good-weather\" bias.\nMethods to mitigate such bias which allows the OD models to perform better and\nimprove the robustness are also demonstrated. A simple yet effective OD\nframework for studying bias mitigation is proposed. Using this framework, the\nperformance on popular datasets is analyzed and a significant difference in\nmodel performance is observed. Additionally, a knowledge transfer technique and\na synthetic image corruption technique are proposed to mitigate the identified\nbias. Finally, using the DAWN dataset, the findings are validated on the OD\ntask, demonstrating the effectiveness of our techniques in mitigating\nreal-world \"good-weather\" bias. The experiments show that the proposed\ntechniques outperform baseline methods by averaged fourfold improvement.\n", "rewritten_text": "Many popular computer vision (CV) datasets used for object detection (OD) in autonomous driving exhibit biases stemming from factors such as weather and lighting conditions.  These biases can significantly impair a model's generalizability, rendering it unreliable for OD in novel, unseen datasets. This poses a substantial safety risk for autonomous vehicles and their surroundings.  This work addresses this issue by identifying and mitigating \"good-weather\" bias in OD datasets. We propose a simple yet effective OD framework for studying and mitigating this bias, analyzing its impact on popular datasets and demonstrating a significant performance difference.  We introduce a knowledge transfer technique and a synthetic image corruption technique to address the identified bias.  Finally, we validate our findings on the DAWN dataset, demonstrating the effectiveness of our techniques in mitigating real-world \"good-weather\" bias.  Experiments show that our proposed techniques achieve, on average, a fourfold improvement over baseline methods.\n"}, "2302.01097": {"original_text": "  Tree kernels have been proposed to be used in many areas as the automatic\nlearning of natural language applications. In this paper, we propose a new\nlinear time algorithm based on the concept of weighted tree automata for\nSubTree kernel computation. First, we introduce a new class of weighted tree\nautomata, called Root-Weighted Tree Automata, and their associated formal tree\nseries. Then we define, from this class, the SubTree automata that represent\ncompact computational models for finite tree languages. This allows us to\ndesign a theoretically guaranteed linear-time algorithm for computing the\nSubTree Kernel based on weighted tree automata intersection. The key idea\nbehind the proposed algorithm is to replace DAG reduction and nodes sorting\nsteps used in previous approaches by states equivalence classes computation\nallowed in the weighted tree automata approach. Our approach has three major\nadvantages: it is output-sensitive, it is free sensitive from the tree types\n(ordered trees versus unordered trees), and it is well adapted to any\nincremental tree kernel based learning methods. Finally, we conduct a variety\nof comparative experiments on a wide range of synthetic tree languages datasets\nadapted for a deep algorithm analysis. The obtained results show that the\nproposed algorithm outperforms state-of-the-art methods.\n", "rewritten_text": "Tree kernels have been widely applied in natural language processing.  This paper presents a novel linear-time algorithm for subtree kernel computation, based on weighted tree automata.  We introduce a new class of weighted tree automata, termed Root-Weighted Tree Automata, and their associated formal tree series.  From this class, we define Subtree automata, providing compact computational models for finite tree languages. This enables a theoretically guaranteed linear-time algorithm for subtree kernel computation via weighted tree automata intersection.  Our algorithm replaces the DAG reduction and node sorting steps of previous approaches with the computation of state equivalence classes, a feature afforded by the weighted tree automata framework.  This approach offers three key advantages: output sensitivity, independence from tree ordering (handling both ordered and unordered trees), and compatibility with incremental tree kernel learning methods.  Finally, we present a comprehensive experimental evaluation on diverse synthetic tree language datasets, designed for in-depth algorithmic analysis.  Results demonstrate that our algorithm outperforms state-of-the-art methods.\n"}, "2404.10877": {"original_text": "  In this paper, we aim to generate text classification data given arbitrary\nclass definitions (i.e., user instruction), so one can train a small text\nclassifier without any human annotation or raw corpus. Compared with pioneer\nattempts, our proposed Incubator is the first framework that can handle\ncomplicated and even mutually dependent classes (e.g., \"TED Talk given by\nEducator\" and \"Other\"). Specifically, Incubator is an LLM firstly tuned on the\ninstruction-to-data mappings that we obtained from classification datasets and\ndescriptions on HuggingFace together with in-context augmentation by GPT-4. We\nthen refine Incubator by learning on the cluster centers of semantic textual\nembeddings to emphasize the uniformity and semantic diversity in generations.\nWe compare Incubator on various classification tasks with strong baselines such\nas direct LLM-based inference and training data generation by prompt\nengineering. Experiments show Incubator is able to (1) perform well on\ntraditional benchmarks, (2) take label dependency and user preference into\nconsideration, and (3) enable logical text mining by incubating multiple\nclassifiers.\n", "rewritten_text": "This paper presents Incubator, the first framework capable of generating text classification data from arbitrary user-defined classes, eliminating the need for human annotation or a raw corpus to train small text classifiers.  Unlike previous approaches, Incubator handles complex and interdependent class definitions (e.g., \"TED Talk given by Educator\" and \"Other\").  It leverages a large language model (LLM) initially fine-tuned on instruction-to-data mappings derived from Hugging Face classification datasets and descriptions, augmented with GPT-4 in-context learning.  Subsequently, Incubator is refined by training on the cluster centers of semantic textual embeddings, promoting uniformity and semantic diversity in generated data.  Comparative experiments against strong baselines, including direct LLM inference and prompt-engineered data generation, demonstrate Incubator's superior performance: (1) achieving strong results on traditional benchmarks; (2) incorporating label dependencies and user preferences; and (3) facilitating logical text mining through the incubation of multiple classifiers.\n"}, "2210.09782": {"original_text": "  This paper focuses on developing a more effective method of hierarchical\npropagation for semi-supervised Video Object Segmentation (VOS). Based on\nvision transformers, the recently-developed Associating Objects with\nTransformers (AOT) approach introduces hierarchical propagation into VOS and\nhas shown promising results. The hierarchical propagation can gradually\npropagate information from past frames to the current frame and transfer the\ncurrent frame feature from object-agnostic to object-specific. However, the\nincrease of object-specific information will inevitably lead to the loss of\nobject-agnostic visual information in deep propagation layers. To solve such a\nproblem and further facilitate the learning of visual embeddings, this paper\nproposes a Decoupling Features in Hierarchical Propagation (DeAOT) approach.\nFirstly, DeAOT decouples the hierarchical propagation of object-agnostic and\nobject-specific embeddings by handling them in two independent branches.\nSecondly, to compensate for the additional computation from dual-branch\npropagation, we propose an efficient module for constructing hierarchical\npropagation, i.e., Gated Propagation Module, which is carefully designed with\nsingle-head attention. Extensive experiments show that DeAOT significantly\noutperforms AOT in both accuracy and efficiency. On YouTube-VOS, DeAOT can\nachieve 86.0% at 22.4fps and 82.0% at 53.4fps. Without test-time augmentations,\nwe achieve new state-of-the-art performance on four benchmarks, i.e.,\nYouTube-VOS (86.2%), DAVIS 2017 (86.2%), DAVIS 2016 (92.9%), and VOT 2020\n(0.622). Project page: https://github.com/z-x-yang/AOT.\n", "rewritten_text": "This paper presents DeAOT, a novel approach for more effective hierarchical propagation in semi-supervised video object segmentation (VOS).  Building upon the promising Associating Objects with Transformers (AOT) method, which leverages vision transformers for hierarchical propagation, DeAOT addresses AOT's limitation of losing object-agnostic visual information during deep propagation.  This loss occurs as object-specific information increases.  To mitigate this, DeAOT decouples the propagation of object-agnostic and object-specific embeddings into two independent branches.  Furthermore, to maintain efficiency despite the added computational cost of this dual-branch architecture, we introduce the Gated Propagation Module, an efficient single-head attention mechanism for hierarchical propagation.  Extensive experiments demonstrate DeAOT's significant superiority over AOT in both accuracy and efficiency.  On YouTube-VOS, DeAOT achieves 86.0% accuracy at 22.4 frames per second (fps) and 82.0% at 53.4 fps.  Without test-time augmentation, DeAOT establishes new state-of-the-art results on four benchmarks: YouTube-VOS (86.2%), DAVIS 2017 (86.2%), DAVIS 2016 (92.9%), and VOT 2020 (0.622).  Project page: https://github.com/z-x-yang/AOT.\n"}, "2406.18173": {"original_text": "  Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases.\n", "rewritten_text": "Large language models (LLMs) struggle with long texts due to limited context window sizes.  This study introduces UIO-LLMs, an unbiased incremental optimization approach for memory-enhanced transformers designed for long-context processing.  We propose a streamlined encoder-decoder framework with shared weights. The encoder encodes context segments into memory, which the decoder then uses to predict subsequent segment outputs.  By framing our memory-enhanced transformers as fully-connected recurrent neural networks (RNNs), we employ the Truncated Backpropagation Through Time (TBPTT) algorithm with novel incremental optimization techniques.  These techniques reduce computational complexity and mitigate gradient bias through unbiased optimization.  UIO-LLMs effectively handle long contexts; for example, they extend the context window of Llama2-7b-chat from 4K to 100K tokens with only a 2% increase in parameters, maintaining near-linear inference cost scaling with context length.\n"}, "2306.07713": {"original_text": "  Segment anything model (SAM), as the name suggests, is claimed to be capable\nof cutting out any object and demonstrates impressive zero-shot transfer\nperformance with the guidance of prompts. However, there is currently a lack of\ncomprehensive evaluation regarding its robustness under various corruptions.\nUnderstanding the robustness of SAM across different corruption scenarios is\ncrucial for its real-world deployment. Prior works show that SAM is biased\ntowards texture (style) rather than shape, motivated by which we start by\ninvestigating its robustness against style transfer, which is synthetic\ncorruption. Following by interpreting the effects of synthetic corruption as\nstyle changes, we proceed to conduct a comprehensive evaluation for its\nrobustness against 15 types of common corruption. These corruptions mainly fall\ninto categories such as digital, noise, weather, and blur, and within each\ncorruption category, we explore 5 severity levels to simulate real-world\ncorruption scenarios. Beyond the corruptions, we further assess the robustness\nof SAM against local occlusion and local adversarial patch attacks. To the best\nof our knowledge, our work is the first of its kind to evaluate the robustness\nof SAM under style change, local occlusion, and local adversarial patch\nattacks. Given that patch attacks visible to human eyes are easily detectable,\nwe further assess its robustness against global adversarial attacks that are\nimperceptible to human eyes. Overall, this work provides a comprehensive\nempirical study of the robustness of SAM, evaluating its performance under\nvarious corruptions and extending the assessment to critical aspects such as\nlocal occlusion, local adversarial patch attacks, and global adversarial\nattacks. These evaluations yield valuable insights into the practical\napplicability and effectiveness of SAM in addressing real-world challenges.\n", "rewritten_text": "The Segment Anything Model (SAM) claims the ability to segment any object and exhibits impressive zero-shot transfer performance when guided by prompts.  However, its robustness under various corruptions remains largely unevaluated.  Understanding this robustness is crucial for real-world deployment.  Prior work suggests SAM's bias towards texture over shape.  Therefore, we begin by investigating its robustness against style transfer, a form of synthetic corruption.  Interpreting the effects of synthetic corruption as style changes, we then conduct a comprehensive evaluation across fifteen common corruption types, categorized as digital, noise, weather, and blur, each with five severity levels to simulate real-world scenarios.  Furthermore, we assess SAM's robustness against local occlusion and local adversarial patch attacks.  To our knowledge, this is the first evaluation of SAM's robustness under style changes, local occlusion, and local adversarial patch attacks.  Given the detectability of visible patch attacks, we also evaluate its robustness against imperceptible global adversarial attacks.  This work provides a comprehensive empirical study of SAM's robustness, evaluating its performance under diverse corruptions and extending the assessment to local occlusion, local and global adversarial attacks.  These evaluations offer valuable insights into SAM's practical applicability and effectiveness in real-world scenarios.\n"}, "2103.1399": {"original_text": "  A fundamental challenge faced by existing Fine-Grained Sketch-Based Image\nRetrieval (FG-SBIR) models is the data scarcity -- model performances are\nlargely bottlenecked by the lack of sketch-photo pairs. Whilst the number of\nphotos can be easily scaled, each corresponding sketch still needs to be\nindividually produced. In this paper, we aim to mitigate such an upper-bound on\nsketch data, and study whether unlabelled photos alone (of which they are many)\ncan be cultivated for performances gain. In particular, we introduce a novel\nsemi-supervised framework for cross-modal retrieval that can additionally\nleverage large-scale unlabelled photos to account for data scarcity. At the\ncentre of our semi-supervision design is a sequential photo-to-sketch\ngeneration model that aims to generate paired sketches for unlabelled photos.\nImportantly, we further introduce a discriminator guided mechanism to guide\nagainst unfaithful generation, together with a distillation loss based\nregularizer to provide tolerance against noisy training samples. Last but not\nleast, we treat generation and retrieval as two conjugate problems, where a\njoint learning procedure is devised for each module to mutually benefit from\neach other. Extensive experiments show that our semi-supervised model yields\nsignificant performance boost over the state-of-the-art supervised\nalternatives, as well as existing methods that can exploit unlabelled photos\nfor FG-SBIR.\n", "rewritten_text": "Fine-grained sketch-based image retrieval (FG-SBIR) models suffer from a fundamental limitation: data scarcity.  Model performance is severely constrained by the lack of sketch-photo pairs, as while photo datasets are readily scalable, creating corresponding sketches requires significant manual effort.  This paper addresses this limitation by investigating whether unlabeled photos can be leveraged to improve performance.  We propose a novel semi-supervised framework for cross-modal retrieval that utilizes a large-scale unlabeled photo dataset.  Central to our approach is a sequential photo-to-sketch generation model that synthesizes paired sketches for unlabeled photos.  To ensure fidelity, we incorporate a discriminator-guided mechanism to prevent the generation of unrealistic sketches and a distillation loss to mitigate the impact of noisy training data.  Furthermore, we treat sketch generation and image retrieval as conjugate problems, employing a joint learning procedure to mutually benefit both modules.  Extensive experiments demonstrate that our semi-supervised model significantly outperforms state-of-the-art supervised methods and existing approaches that utilize unlabeled photos for FG-SBIR.\n"}, "1412.7854": {"original_text": "  Traditional object recognition approaches apply feature extraction, part\ndeformation handling, occlusion handling and classification sequentially while\nthey are independent from each other. Ouyang and Wang proposed a model for\njointly learning of all of the mentioned processes using one deep neural\nnetwork. We utilized, and manipulated their toolbox in order to apply it in car\ndetection scenarios where it had not been tested. Creating a single deep\narchitecture from these components, improves the interaction between them and\ncan enhance the performance of the whole system. We believe that the approach\ncan be used as a general purpose object detection toolbox. We tested the\nalgorithm on UIUC car dataset, and achieved an outstanding result. The accuracy\nof our method was 97 % while the previously reported results showed an accuracy\nof up to 91 %. We strongly believe that having an experiment on a larger\ndataset can show the advantage of using deep models over shallow ones.\n", "rewritten_text": "Traditional object recognition methods sequentially apply independent feature extraction, part deformation handling, occlusion handling, and classification.  Ouyang and Wang proposed a deep neural network model that jointly learns these processes. We adapted and modified their toolbox for car detection, a previously untested application.  Integrating these components into a single deep architecture improves their interaction and overall system performance.  This approach offers a potential general-purpose object detection toolbox.  Testing on the UIUC car dataset yielded a 97% accuracy rate, significantly exceeding previously reported results (up to 91%).  Further experimentation with larger datasets would likely further demonstrate the superiority of deep learning models over shallower approaches.\n"}, "1905.03672": {"original_text": "  In this paper, we are interested in boosting the representation capability of\nconvolution neural networks which utilizing the inverted residual structure.\nBased on the success of Inverted Residual structure[Sandler et al. 2018] and\nInterleaved Low-Rank Group Convolutions[Sun et al. 2018], we rethink this two\npattern of neural network structure, rather than NAS(Neural architecture\nsearch) method[Zoph and Le 2017; Pham et al. 2018; Liu et al. 2018b], we\nintroduce uneven point-wise group convolution, which provide a novel search\nspace for designing basic blocks to obtain better trade-off between\nrepresentation capability and computational cost. Meanwhile, we propose two\nnovel information flow patterns that will enable cross-group information flow\nfor multiple group convolution layers with and without any channel\npermute/shuffle operation. Dense experiments on image classification task show\nthat our proposed model, named Seesaw-Net, achieves state-of-the-art(SOTA)\nperformance with limited computation and memory cost. Our code will be\nopen-source and available together with pre-trained models.\n", "rewritten_text": "This paper explores enhancing the representational capabilities of convolutional neural networks (CNNs) employing inverted residual structures.  Building upon the success of inverted residual structures [Sandler et al., 2018] and interleaved low-rank group convolutions [Sun et al., 2018], we propose a novel approach that eschews neural architecture search (NAS) methods [Zoph and Le, 2017; Pham et al., 2018; Liu et al., 2018b]. Instead, we introduce uneven point-wise group convolutions, creating a new search space for designing basic blocks that optimize the trade-off between representational power and computational cost.  Furthermore, we propose two novel information flow patterns facilitating cross-group information flow across multiple group convolution layers, both with and without channel permutation/shuffling.  Extensive experiments on image classification demonstrate that our proposed model, Seesaw-Net, achieves state-of-the-art (SOTA) performance with reduced computational and memory overhead.  Our code and pre-trained models will be publicly available.\n"}, "1912.1163": {"original_text": "  Person re-identification has attracted many researchers' attention for its\nwide application, but it is still a very challenging task because only part of\nthe image information can be used for personnel matching. Most of current\nmethods uses CNN to learn to embeddings that can capture semantic similarity\ninformation among data points. Many of the state-of-the-arts methods use\ncomplex network structures with multiple branches that fuse multiple features\nwhile training or testing, using classification loss, Triplet loss or a\ncombination of the two as loss function. However, the method that using Triplet\nloss as loss function converges slowly, and the method in which pull features\nof the same class as close as possible in features space leads to poor feature\nstability. This paper will combine the ranking motivated structured loss,\nproposed a new metric learning loss function that make the features of the same\nclass are sparsely distributed into the range of small hyperspheres and the\nfeatures of different classes are uniformly distributed at a clearly angle. And\nadopted a new single-branch network structure that only using global feature\ncan also get great performance. The validity of our method is verified on the\nMarket1501 and DukeMTMC-ReID person re-identification datasets. Finally\nacquires 90.9% rank-1 accuracy and 80.8% mAP on DukeMTMC-reID, 95.3% rank-1\naccuracy and 88.7% mAP on Market1501. Codes and models are available in\nGithub.https://github.com/Qidian213/Ranked_Person_ReID.\n", "rewritten_text": "Person re-identification, while attracting significant research interest due to its wide applicability, remains a challenging task because only a portion of image information is usable for individual matching.  Most current methods leverage convolutional neural networks (CNNs) to learn embeddings that capture semantic similarity.  Many state-of-the-art methods employ complex multi-branch network architectures that fuse multiple features during training or testing, utilizing classification loss, triplet loss, or a combination thereof. However, methods relying solely on triplet loss suffer from slow convergence, and the tendency to tightly cluster features of the same class can lead to poor feature stability.\n\nThis paper proposes a novel metric learning loss function inspired by ranking-based structured losses.  This function encourages features of the same class to be sparsely distributed within small hyperspheres, while features of different classes are uniformly distributed at a significant angular separation.  Furthermore, we adopt a simpler single-branch network architecture that utilizes only global features, yet achieves excellent performance.  The effectiveness of our method is validated on the Market1501 and DukeMTMC-ReID datasets, achieving 95.3% rank-1 accuracy and 88.7% mean average precision (mAP) on Market1501, and 90.9% rank-1 accuracy and 80.8% mAP on DukeMTMC-ReID.  Code and pre-trained models are available on GitHub: [https://github.com/Qidian213/Ranked_Person_ReID](https://github.com/Qidian213/Ranked_Person_ReID).\n"}, "1907.1144": {"original_text": "  Pooling is one of the main elements in convolutional neural networks. The\npooling reduces the size of the feature map, enabling training and testing with\na limited amount of computation. This paper proposes a new pooling method named\nuniversal pooling. Unlike the existing pooling methods such as average pooling,\nmax pooling, and stride pooling with fixed pooling function, universal pooling\ngenerates any pooling function, depending on a given problem and dataset.\nUniversal pooling was inspired by attention methods and can be considered as a\nchannel-wise form of local spatial attention. Universal pooling is trained\njointly with the main network and it is shown that it includes the existing\npooling methods. Finally, when applied to two benchmark problems, the proposed\nmethod outperformed the existing pooling methods and performed with the\nexpected diversity, adapting to the given problem.\n", "rewritten_text": "Convolutional neural networks (CNNs) rely heavily on pooling to reduce feature map size, thereby enabling efficient training and testing. This paper introduces universal pooling, a novel pooling method that dynamically generates pooling functions tailored to specific problems and datasets. Unlike existing methods such as average, max, and stride pooling, which employ fixed functions, universal pooling is inspired by attention mechanisms and can be viewed as a channel-wise implementation of local spatial attention.  Trained jointly with the main network, universal pooling encompasses existing pooling methods as special cases.  Experiments on two benchmark problems demonstrate that it outperforms existing techniques, exhibiting the expected adaptability and diversity in response to varying problem characteristics.\n"}, "2406.17236": {"original_text": "  Although recent years have witnessed significant advancements in image\nediting thanks to the remarkable progress of text-to-image diffusion models,\nthe problem of non-rigid image editing still presents its complexities and\nchallenges. Existing methods often fail to achieve consistent results due to\nthe absence of unique identity characteristics. Thus, learning a personalized\nidentity prior might help with consistency in the edited results. In this\npaper, we explore a novel task: learning the personalized identity prior for\ntext-based non-rigid image editing. To address the problems in jointly learning\nprior and editing the image, we present LIPE, a two-stage framework designed to\ncustomize the generative model utilizing a limited set of images of the same\nsubject, and subsequently employ the model with learned prior for non-rigid\nimage editing. Experimental results demonstrate the advantages of our approach\nin various editing scenarios over past related leading methods in qualitative\nand quantitative ways.\n", "rewritten_text": "Recent advancements in image editing, driven by progress in text-to-image diffusion models, have not fully addressed the complexities of non-rigid image editing.  Existing methods often produce inconsistent results due to a lack of robust identity preservation.  To improve consistency, we propose learning a personalized identity prior.  This paper introduces LIPE, a novel two-stage framework for text-based non-rigid image editing that learns this prior.  LIPE customizes a generative model using a limited set of images from a single subject and then employs this model, incorporating the learned prior, for non-rigid editing.  Qualitative and quantitative experimental results demonstrate LIPE's superiority over state-of-the-art methods across various editing scenarios.\n"}, "2406.12679": {"original_text": "  Large Language Models (LLMs) are increasingly being used in educational and\nlearning applications. Research has demonstrated that controlling for style, to\nfit the needs of the learner, fosters increased understanding, promotes\ninclusion, and helps with knowledge distillation. To understand the\ncapabilities and limitations of contemporary LLMs in style control, we\nevaluated five state-of-the-art models: GPT-3.5, GPT-4, GPT-4o, Llama-3, and\nMistral-instruct- 7B across two style control tasks. We observed significant\ninconsistencies in the first task, with model performances averaging between\n5th and 8th grade reading levels for tasks intended for first-graders, and\nstandard deviations up to 27.6. For our second task, we observed a\nstatistically significant improvement in performance from 0.02 to 0.26.\nHowever, we find that even without stereotypes in reference texts, LLMs often\ngenerated culturally insensitive content during their tasks. We provide a\nthorough analysis and discussion of the results.\n", "rewritten_text": "Large language models (LLMs) are increasingly integrated into educational applications.  Research indicates that adapting style to learners' needs enhances comprehension, promotes inclusivity, and facilitates knowledge transfer. To assess the capabilities and limitations of current LLMs in style control, we evaluated five state-of-the-art models\u2014GPT-3.5, GPT-4, GPT-4o, Llama-3, and Mistral-instruct-7B\u2014across two style control tasks.  In the first task, we observed significant inconsistencies in model performance, with outputs ranging from 5th to 8th-grade reading levels on tasks designed for first graders (standard deviation: up to 27.6).  The second task showed a statistically significant performance improvement (from 0.02 to 0.26).  However, even without stereotypical reference texts, these LLMs frequently generated culturally insensitive content.  A detailed analysis and discussion of these findings are presented.\n"}, "1907.06882": {"original_text": "  Majority of state-of-the-art monocular depth estimation methods are\nsupervised learning approaches. The success of such approaches heavily depends\non the high-quality depth labels which are expensive to obtain. Some recent\nmethods try to learn depth networks by leveraging unsupervised cues from\nmonocular videos which are easier to acquire but less reliable. In this paper,\nwe propose to resolve this dilemma by transferring knowledge from synthetic\nvideos with easily obtainable ground-truth depth labels. Due to the stylish\ndifference between synthetic and real images, we propose a\ntemporally-consistent domain adaptation (TCDA) approach that simultaneously\nexplores labels in the synthetic domain and temporal constraints in the videos\nto improve style transfer and depth prediction. Furthermore, we make use of the\nground-truth optical flow and pose information in the synthetic data to learn\nmoving mask and pose prediction networks. The learned moving masks can filter\nout moving regions that produces erroneous temporal constraints and the\nestimated poses provide better initializations for estimating temporal\nconstraints. Experimental results demonstrate the effectiveness of our method\nand comparable performance against state-of-the-art.\n", "rewritten_text": "Most state-of-the-art monocular depth estimation methods rely on supervised learning, which necessitates expensive, high-quality depth labels.  While recent methods leverage unsupervised cues from readily available monocular videos, these cues are less reliable. This paper proposes a solution: transferring knowledge from synthetic videos with easily obtainable ground-truth depth labels.  To address the stylistic differences between synthetic and real images, we introduce a temporally-consistent domain adaptation (TCDA) approach.  TCDA simultaneously leverages synthetic domain labels and temporal video constraints to improve style transfer and depth prediction.  Furthermore, we utilize ground-truth optical flow and pose information from the synthetic data to train moving mask and pose prediction networks.  These learned moving masks filter out moving regions that introduce erroneous temporal constraints, while the estimated poses provide improved initializations for temporal constraint estimation.  Experimental results demonstrate the effectiveness of our method, achieving performance comparable to state-of-the-art techniques.\n"}, "2104.14839": {"original_text": "  Recently, various neural encoder-decoder models pioneered by Seq2Seq\nframework have been proposed to achieve the goal of generating more abstractive\nsummaries by learning to map input text to output text. At a high level, such\nneural models can freely generate summaries without any constraint on the words\nor phrases used. Moreover, their format is closer to human-edited summaries and\noutput is more readable and fluent. However, the neural model's abstraction\nability is a double-edged sword. A commonly observed problem with the generated\nsummaries is the distortion or fabrication of factual information in the\narticle. This inconsistency between the original text and the summary has\ncaused various concerns over its applicability, and the previous evaluation\nmethods of text summarization are not suitable for this issue. In response to\nthe above problems, the current research direction is predominantly divided\ninto two categories, one is to design fact-aware evaluation metrics to select\noutputs without factual inconsistency errors, and the other is to develop new\nsummarization systems towards factual consistency. In this survey, we focus on\npresenting a comprehensive review of these fact-specific evaluation methods and\ntext summarization models.\n", "rewritten_text": "Recent research has employed neural encoder-decoder models, pioneered by the Seq2Seq framework, to generate more abstractive text summaries by learning mappings between input and output text.  These models offer the advantage of freely generating summaries unconstrained by the source text's wording, resulting in outputs that are more readable, fluent, and resemble human-written summaries. However, this abstractive capability presents a challenge: neural models are prone to factual inconsistencies, distorting or fabricating information from the source text. This has raised concerns about their applicability and highlighted the inadequacy of traditional text summarization evaluation methods.  Consequently, current research focuses on two primary approaches: developing fact-aware evaluation metrics to identify summaries free from factual errors, and designing summarization systems that inherently prioritize factual consistency. This survey comprehensively reviews these fact-specific evaluation methods and summarization models.\n"}, "1701.01619": {"original_text": "  We present an approach to effectively use millions of images with noisy\nannotations in conjunction with a small subset of cleanly-annotated images to\nlearn powerful image representations. One common approach to combine clean and\nnoisy data is to first pre-train a network using the large noisy dataset and\nthen fine-tune with the clean dataset. We show this approach does not fully\nleverage the information contained in the clean set. Thus, we demonstrate how\nto use the clean annotations to reduce the noise in the large dataset before\nfine-tuning the network using both the clean set and the full set with reduced\nnoise. The approach comprises a multi-task network that jointly learns to clean\nnoisy annotations and to accurately classify images. We evaluate our approach\non the recently released Open Images dataset, containing ~9 million images,\nmultiple annotations per image and over 6000 unique classes. For the small\nclean set of annotations we use a quarter of the validation set with ~40k\nimages. Our results demonstrate that the proposed approach clearly outperforms\ndirect fine-tuning across all major categories of classes in the Open Image\ndataset. Further, our approach is particularly effective for a large number of\nclasses with wide range of noise in annotations (20-80% false positive\nannotations).\n", "rewritten_text": "We present a method for effectively leveraging millions of images with noisy annotations, in conjunction with a small set of cleanly annotated images, to learn robust image representations.  While a common approach involves pre-training a network on the noisy data and then fine-tuning it with the clean data, we demonstrate that this approach underutilizes the information in the clean set.  Instead, we propose using the clean annotations to denoise the large dataset *before* fine-tuning.  Our method employs a multi-task network that simultaneously learns to correct noisy annotations and accurately classify images.\n\nWe evaluated our approach on the Open Images dataset, comprising approximately 9 million images, multiple annotations per image, and over 6,000 unique classes.  A quarter of the validation set (approximately 40,000 images) served as our small, clean annotation set.  Our results show that this approach significantly outperforms direct fine-tuning across all major class categories within the Open Images dataset.  Furthermore, its effectiveness is particularly pronounced for classes exhibiting a wide range of annotation noise (20-80% false positives).\n"}, "1905.04215": {"original_text": "  We study the problem of unsupervised domain adaptation which aims to adapt\nmodels trained on a labeled source domain to a completely unlabeled target\ndomain. Recently, the cluster assumption has been applied to unsupervised\ndomain adaptation and achieved strong performance. One critical factor in\nsuccessful training of the cluster assumption is to impose the\nlocally-Lipschitz constraint to the model. Existing methods only impose the\nlocally-Lipschitz constraint around the training points while miss the other\nareas, such as the points in-between training data. In this paper, we address\nthis issue by encouraging the model to behave linearly in-between training\npoints. We propose a new regularization method called Virtual Mixup Training\n(VMT), which is able to incorporate the locally-Lipschitz constraint to the\nareas in-between training data. Unlike the traditional mixup model, our method\nconstructs the combination samples without using the label information,\nallowing it to apply to unsupervised domain adaptation. The proposed method is\ngeneric and can be combined with most existing models such as the recent\nstate-of-the-art model called VADA. Extensive experiments demonstrate that VMT\nsignificantly improves the performance of VADA on six domain adaptation\nbenchmark datasets. For the challenging task of adapting MNIST to SVHN, VMT can\nimprove the accuracy of VADA by over 30\\%. Code is available at\n\\url{https://github.com/xudonmao/VMT}.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) aims to adapt models trained on a labeled source domain to an unlabeled target domain.  Recent UDA methods leverage the cluster assumption, achieving strong performance.  A critical factor in their success is imposing a locally-Lipschitz constraint on the model. However, existing methods only enforce this constraint around training points, neglecting the regions between them.  This paper addresses this limitation by encouraging linear model behavior in these inter-point regions.  We propose Virtual Mixup Training (VMT), a novel regularization method that extends the locally-Lipschitz constraint to these areas. Unlike traditional mixup, VMT constructs synthetic samples without label information, making it suitable for UDA.  VMT is a generic method compatible with most existing models, including state-of-the-art architectures like VADA.  Extensive experiments on six benchmark datasets demonstrate VMT's significant performance improvements.  For the challenging MNIST-to-SVHN adaptation task, VMT boosts VADA's accuracy by over 30%. Code is available at [https://github.com/xudonmao/VMT](https://github.com/xudonmao/VMT).\n"}, "2311.17590": {"original_text": "  Achieving high synchronization in the synthesis of realistic, speech-driven\ntalking head videos presents a significant challenge. Traditional Generative\nAdversarial Networks (GAN) struggle to maintain consistent facial identity,\nwhile Neural Radiance Fields (NeRF) methods, although they can address this\nissue, often produce mismatched lip movements, inadequate facial expressions,\nand unstable head poses. A lifelike talking head requires synchronized\ncoordination of subject identity, lip movements, facial expressions, and head\nposes. The absence of these synchronizations is a fundamental flaw, leading to\nunrealistic and artificial outcomes. To address the critical issue of\nsynchronization, identified as the \"devil\" in creating realistic talking heads,\nwe introduce SyncTalk. This NeRF-based method effectively maintains subject\nidentity, enhancing synchronization and realism in talking head synthesis.\nSyncTalk employs a Face-Sync Controller to align lip movements with speech and\ninnovatively uses a 3D facial blendshape model to capture accurate facial\nexpressions. Our Head-Sync Stabilizer optimizes head poses, achieving more\nnatural head movements. The Portrait-Sync Generator restores hair details and\nblends the generated head with the torso for a seamless visual experience.\nExtensive experiments and user studies demonstrate that SyncTalk outperforms\nstate-of-the-art methods in synchronization and realism. We recommend watching\nthe supplementary video: https://ziqiaopeng.github.io/synctalk\n", "rewritten_text": "Synthesizing realistic, speech-driven talking head videos with high synchronization remains a significant challenge.  Traditional Generative Adversarial Networks (GANs) often struggle to maintain consistent facial identity, while Neural Radiance Fields (NeRFs), though capable of addressing identity preservation, frequently suffer from mismatched lip synchronization, inadequate facial expressions, and unstable head poses.  Lifelike talking heads require the synchronized coordination of identity, lip movements, facial expressions, and head pose; the lack of this synchronization is a fundamental limitation, resulting in unrealistic and artificial results.\n\nTo address this critical synchronization issue\u2014the primary obstacle in creating realistic talking heads\u2014we introduce SyncTalk, a novel NeRF-based method.  SyncTalk effectively preserves subject identity while significantly enhancing synchronization and realism in talking head synthesis.  This is achieved through a Face-Sync Controller for accurate lip synchronization with speech, a 3D facial blendshape model for capturing nuanced facial expressions, and a Head-Sync Stabilizer for optimizing natural head movements.  Finally, a Portrait-Sync Generator restores hair details and seamlessly blends the generated head with the torso.\n\nExtensive experiments and user studies demonstrate that SyncTalk outperforms state-of-the-art methods in both synchronization and realism.  For a visual demonstration, please see the supplementary video: https://ziqiaopeng.github.io/synctalk\n"}, "1901.07656": {"original_text": "  Word vector representations are well developed tools for various NLP and\nMachine Learning tasks and are known to retain significant semantic and\nsyntactic structure of languages. But they are prone to carrying and amplifying\nbias which can perpetrate discrimination in various applications. In this work,\nwe explore new simple ways to detect the most stereotypically gendered words in\nan embedding and remove the bias from them. We verify how names are masked\ncarriers of gender bias and then use that as a tool to attenuate bias in\nembeddings. Further, we extend this property of names to show how names can be\nused to detect other types of bias in the embeddings such as bias based on\nrace, ethnicity, and age.\n", "rewritten_text": "Word vector representations are well-established tools for numerous natural language processing (NLP) and machine learning tasks, effectively capturing significant semantic and syntactic linguistic structure.  However, they are susceptible to carrying and amplifying biases, potentially perpetuating discrimination in various applications. This work explores novel, straightforward methods for identifying the most stereotypically gendered words within word embeddings and mitigating this bias. We demonstrate how names serve as masked carriers of gender bias, leveraging this to attenuate bias in embeddings.  Furthermore, we extend this principle to show how names can be used to detect other biases, such as those related to race, ethnicity, and age.\n"}, "2210.02318": {"original_text": "  Recently, two-stage Deformable DETR introduced the query-based two-stage\nhead, a new type of two-stage head different from the region-based two-stage\nheads of classical detectors as Faster R-CNN. In query-based two-stage heads,\nthe second stage selects one feature per detection processed by a transformer,\ncalled the query, as opposed to pooling a rectangular grid of features\nprocessed by CNNs as in region-based detectors. In this work, we improve the\nquery-based head by improving the prior of the cross-attention operation with\nanchors, significantly speeding up the convergence while increasing its\nperformance. Additionally, we empirically show that by improving the\ncross-attention prior, auxiliary losses and iterative bounding box mechanisms\ntypically used by DETR-based detectors are no longer needed. By combining the\nbest of both the classical and the DETR-based detectors, our FQDet head peaks\nat 45.4 AP on the 2017 COCO validation set when using a ResNet-50+TPN backbone,\nonly after training for 12 epochs using the 1x schedule. We outperform other\nhigh-performing two-stage heads such as e.g. Cascade R-CNN, while using the\nsame backbone and while being computationally cheaper. Additionally, when using\nthe large ResNeXt-101-DCN+TPN backbone and multi-scale testing, our FQDet head\nachieves 52.9 AP on the 2017 COCO test-dev set after only 12 epochs of\ntraining. Code is released at https://github.com/CedricPicron/FQDet .\n", "rewritten_text": "Two-stage Deformable DETR introduced a novel query-based two-stage head, distinct from the region-based heads of classical detectors like Faster R-CNN.  Unlike region-based detectors, which pool features from a rectangular grid via CNNs, the query-based head selects a single transformer-processed feature (the query) per detection for the second stage.  This work enhances the query-based head by improving the cross-attention prior with anchors, significantly accelerating convergence and boosting performance.  Furthermore, our improvements obviate the need for auxiliary losses and iterative bounding box refinement mechanisms commonly employed in DETR-based detectors.  Combining the strengths of classical and DETR-based approaches, our FQDet head achieves a peak 45.4 AP on the COCO 2017 validation set with a ResNet-50+TPN backbone after only 12 epochs of 1x training.  This surpasses the performance of other high-performing two-stage heads, such as Cascade R-CNN, while maintaining computational efficiency and using the same backbone.  With a larger ResNeXt-101-DCN+TPN backbone and multi-scale testing, FQDet achieves 52.9 AP on the COCO 2017 test-dev set after just 12 epochs.  Code is available at https://github.com/CedricPicron/FQDet.\n"}, "1411.634": {"original_text": "  While widely acknowledged as highly effective in computer vision, multi-label\nMRFs with non-convex priors are difficult to optimize. To tackle this, we\nintroduce an algorithm that iteratively approximates the original energy with\nan appropriately weighted surrogate energy that is easier to minimize. Our\nalgorithm guarantees that the original energy decreases at each iteration. In\nparticular, we consider the scenario where the global minimizer of the weighted\nsurrogate energy can be obtained by a multi-label graph cut algorithm, and show\nthat our algorithm then lets us handle of large variety of non-convex priors.\nWe demonstrate the benefits of our method over state-of-the-art MRF energy\nminimization techniques on stereo and inpainting problems.\n", "rewritten_text": "Multi-label Markov Random Fields (MRFs) with non-convex priors, while highly effective in computer vision, are notoriously difficult to optimize.  To address this challenge, we introduce an iterative algorithm that approximates the original energy function with a weighted surrogate energy, which is significantly easier to minimize.  Our algorithm guarantees a monotonic decrease in the original energy at each iteration.  Specifically, we focus on scenarios where the global minimizer of the weighted surrogate energy is obtainable via a multi-label graph cut algorithm, demonstrating that our approach effectively handles a wide variety of non-convex priors.  We validate the efficacy of our method through comparisons with state-of-the-art MRF energy minimization techniques on stereo and inpainting tasks.\n"}, "2009.02649": {"original_text": "  Causality visualization can help people understand temporal chains of events,\nsuch as messages sent in a distributed system, cause and effect in a historical\nconflict, or the interplay between political actors over time. However, as the\nscale and complexity of these event sequences grows, even these visualizations\ncan become overwhelming to use. In this paper, we propose the use of textual\nnarratives as a data-driven storytelling method to augment causality\nvisualization. We first propose a design space for how textual narratives can\nbe used to describe causal data. We then present results from a crowdsourced\nuser study where participants were asked to recover causality information from\ntwo causality visualizations--causal graphs and Hasse diagrams--with and\nwithout an associated textual narrative. Finally, we describe CAUSEWORKS, a\ncausality visualization system for understanding how specific interventions\ninfluence a causal model. The system incorporates an automatic textual\nnarrative mechanism based on our design space. We validate CAUSEWORKS through\ninterviews with experts who used the system for understanding complex events.\n", "rewritten_text": "Causality visualization aids understanding of temporal event chains, such as message flows in distributed systems, cause-and-effect relationships in historical conflicts, or the dynamic interplay of political actors.  However, the scale and complexity of these sequences can render even sophisticated visualizations unwieldy. This paper proposes augmenting causality visualization with data-driven textual narratives.  We first define a design space for integrating textual narratives with causal data.  Next, we present results from a crowdsourced user study comparing the effectiveness of causal graphs and Hasse diagrams, with and without accompanying narratives, in conveying causality information. Finally, we introduce CAUSEWORKS, a causality visualization system designed to analyze the impact of interventions on causal models.  CAUSEWORKS incorporates an automated narrative generation mechanism based on our proposed design space and is validated through expert interviews assessing its utility in understanding complex events.\n"}, "2204.07183": {"original_text": "  We propose an interactive approach for 3D instance segmentation, where users\ncan iteratively collaborate with a deep learning model to segment objects in a\n3D point cloud directly. Current methods for 3D instance segmentation are\ngenerally trained in a fully-supervised fashion, which requires large amounts\nof costly training labels, and does not generalize well to classes unseen\nduring training. Few works have attempted to obtain 3D segmentation masks using\nhuman interactions. Existing methods rely on user feedback in the 2D image\ndomain. As a consequence, users are required to constantly switch between 2D\nimages and 3D representations, and custom architectures are employed to combine\nmultiple input modalities. Therefore, integration with existing standard 3D\nmodels is not straightforward. The core idea of this work is to enable users to\ninteract directly with 3D point clouds by clicking on desired 3D objects of\ninterest~(or their background) to interactively segment the scene in an\nopen-world setting. Specifically, our method does not require training data\nfrom any target domain, and can adapt to new environments where no appropriate\ntraining sets are available. Our system continuously adjusts the object\nsegmentation based on the user feedback and achieves accurate dense 3D\nsegmentation masks with minimal human effort (few clicks per object). Besides\nits potential for efficient labeling of large-scale and varied 3D datasets, our\napproach, where the user directly interacts with the 3D environment, enables\nnew applications in AR/VR and human-robot interaction.\n", "rewritten_text": "We propose an interactive approach to 3D instance segmentation that allows users to iteratively collaborate with a deep learning model to directly segment objects within a 3D point cloud.  Existing 3D instance segmentation methods typically rely on fully supervised training, demanding extensive and costly labeled data, and exhibiting poor generalization to unseen classes.  While few methods incorporate human interaction, these primarily utilize 2D image-based feedback. This necessitates constant switching between 2D and 3D representations, often employing custom architectures to fuse multiple input modalities, hindering straightforward integration with standard 3D models.\n\nOur approach enables direct user interaction with the 3D point cloud; users click on objects of interest (or background) to interactively segment the scene in an open-world setting.  Crucially, our method requires no training data from the target domain and adapts readily to novel environments lacking suitable training sets.  The system continuously refines object segmentation based on user feedback, achieving accurate, dense 3D segmentation masks with minimal user effort (a few clicks per object).  Beyond facilitating efficient labeling of large-scale, diverse 3D datasets, our direct 3D interaction paradigm opens new possibilities for augmented/virtual reality (AR/VR) and human-robot interaction applications.\n"}, "2012.0536": {"original_text": "  Semantic aware reconstruction is more advantageous than geometric-only\nreconstruction for future robotic and AR/VR applications because it represents\nnot only where things are, but also what things are. Object-centric mapping is\na task to build an object-level reconstruction where objects are separate and\nmeaningful entities that convey both geometry and semantic information. In this\npaper, we present MOLTR, a solution to object-centric mapping using only\nmonocular image sequences and camera poses. It is able to localise, track, and\nreconstruct multiple objects in an online fashion when an RGB camera captures a\nvideo of the surrounding. Given a new RGB frame, MOLTR firstly applies a\nmonocular 3D detector to localise objects of interest and extract their shape\ncodes that represent the object shapes in a learned embedding space. Detections\nare then merged to existing objects in the map after data association. Motion\nstate (i.e. kinematics and the motion status) of each object is tracked by a\nmultiple model Bayesian filter and object shape is progressively refined by\nfusing multiple shape code. We evaluate localisation, tracking, and\nreconstruction on benchmarking datasets for indoor and outdoor scenes, and show\nsuperior performance over previous approaches.\n", "rewritten_text": "Semantic-aware reconstruction offers significant advantages over geometry-only reconstruction for future robotic and AR/VR applications because it represents not only the spatial location but also the identity of objects.  Object-centric mapping aims to create an object-level reconstruction where objects are represented as distinct, meaningful entities possessing both geometric and semantic information. This paper presents MOLTR, a novel solution for object-centric mapping using only monocular image sequences and camera poses.  MOLTR performs online localization, tracking, and reconstruction of multiple objects from RGB video input.  Given a new RGB frame, MOLTR first employs a monocular 3D detector to localize objects of interest and extract their shape codes\u2014a learned embedding representing object shape.  These detections are then associated with existing objects in the map.  A multiple model Bayesian filter tracks the motion state (kinematics and motion status) of each object, while the object shape is progressively refined by fusing multiple shape codes.  We evaluate MOLTR's localization, tracking, and reconstruction performance on benchmark indoor and outdoor datasets, demonstrating superior results compared to existing approaches.\n"}, "1605.06052": {"original_text": "  Similarity scores in face recognition represent the proximity between pairs\nof images as computed by a matching algorithm. Given a large set of images and\nthe proximities between all pairs, a similarity score space is defined. Cluster\nanalysis was applied to the similarity score space to develop various\ntaxonomies. Given the number of subjects in the dataset, we used hierarchical\nmethods to aggregate images of the same subject. We also explored the hierarchy\nabove and below the subject level, including clusters that reflect gender and\nethnicity. Evidence supports the existence of clustering by race, gender,\nsubject, and illumination condition.\n", "rewritten_text": "Face recognition similarity scores quantify the proximity between image pairs, as determined by a matching algorithm.  A similarity score space is defined by the pairwise proximities within a large image set.  Hierarchical cluster analysis of this space generated various taxonomies.  Given the dataset's size, we employed hierarchical methods to group images of the same subject.  Our analysis also explored hierarchical levels above and below the subject level, revealing clusters based on gender, ethnicity, and other factors.  The results support the existence of distinct clusters based on race, gender, subject identity, and illumination conditions.\n"}, "2401.00653": {"original_text": "  Deceptive images can be shared in seconds with social networking services,\nposing substantial risks. Tampering traces, such as boundary artifacts and\nhigh-frequency information, have been significantly emphasized by massive\nnetworks in the Image Manipulation Localization (IML) field. However, they are\nprone to image post-processing operations, which limit the generalization and\nrobustness of existing methods. We present a novel Prompt-IML framework. We\nobserve that humans tend to discern the authenticity of an image based on both\nsemantic and high-frequency information, inspired by which, the proposed\nframework leverages rich semantic knowledge from pre-trained visual foundation\nmodels to assist IML. We are the first to design a framework that utilizes\nvisual foundation models specially for the IML task. Moreover, we design a\nFeature Alignment and Fusion module to align and fuse features of semantic\nfeatures with high-frequency features, which aims at locating tampered regions\nfrom multiple perspectives. Experimental results demonstrate that our model can\nachieve better performance on eight typical fake image datasets and outstanding\nrobustness.\n", "rewritten_text": "Deceptive images spread rapidly via social networking services, posing significant risks.  While the Image Manipulation Localization (IML) field has heavily emphasized tampering traces like boundary artifacts and high-frequency information, these are vulnerable to post-processing, limiting the robustness and generalizability of existing methods.  This paper introduces Prompt-IML, a novel framework inspired by human perception of image authenticity, which leverages both semantic and high-frequency information.  Unlike previous approaches, Prompt-IML uniquely integrates rich semantic knowledge from pre-trained visual foundation models to assist IML.  Furthermore, a Feature Alignment and Fusion module aligns and fuses semantic and high-frequency features, enabling multi-perspective detection of tampered regions.  Experimental results on eight benchmark datasets demonstrate superior performance and robustness.\n"}, "1809.08495": {"original_text": "  Earlier work demonstrates the promise of deep-learning-based approaches for\npoint cloud segmentation; however, these approaches need to be improved to be\npractically useful. To this end, we introduce a new model SqueezeSegV2 that is\nmore robust to dropout noise in LiDAR point clouds. With improved model\nstructure, training loss, batch normalization and additional input channel,\nSqueezeSegV2 achieves significant accuracy improvement when trained on real\ndata. Training models for point cloud segmentation requires large amounts of\nlabeled point-cloud data, which is expensive to obtain. To sidestep the cost of\ncollection and annotation, simulators such as GTA-V can be used to create\nunlimited amounts of labeled, synthetic data. However, due to domain shift,\nmodels trained on synthetic data often do not generalize well to the real\nworld. We address this problem with a domain-adaptation training pipeline\nconsisting of three major components: 1) learned intensity rendering, 2)\ngeodesic correlation alignment, and 3) progressive domain calibration. When\ntrained on real data, our new model exhibits segmentation accuracy improvements\nof 6.0-8.6% over the original SqueezeSeg. When training our new model on\nsynthetic data using the proposed domain adaptation pipeline, we nearly double\ntest accuracy on real-world data, from 29.0% to 57.4%. Our source code and\nsynthetic dataset will be open-sourced.\n", "rewritten_text": "Prior work has shown the potential of deep learning for point cloud segmentation; however, these methods require further improvement for practical application.  To address this, we introduce SqueezeSegV2, a novel model exhibiting enhanced robustness to dropout noise in LiDAR point clouds.  This improvement stems from a refined model architecture, a modified training loss function, the incorporation of batch normalization, and the addition of an input channel.  Consequently, SqueezeSegV2 achieves significant accuracy gains when trained on real-world data.\n\nTraining point cloud segmentation models necessitates large quantities of labeled data, which are costly to acquire. To mitigate this expense, we leverage simulators such as GTA-V to generate unlimited labeled synthetic data.  However, the domain gap between synthetic and real data hinders the generalization performance of models trained solely on synthetic data.  We overcome this limitation with a three-component domain adaptation pipeline: 1) learned intensity rendering, 2) geodesic correlation alignment, and 3) progressive domain calibration.\n\nWhen trained on real data, SqueezeSegV2 achieves a 6.0\u20138.6% increase in segmentation accuracy compared to the original SqueezeSeg.  Furthermore, when trained on synthetic data using our proposed domain adaptation pipeline, we observe a near doubling of real-world test accuracy, increasing from 29.0% to 57.4%.  Our source code and synthetic dataset will be publicly released.\n"}, "2209.11214": {"original_text": "  Automatic tomato disease recognition from leaf images is vital to avoid crop\nlosses by applying control measures on time. Even though recent deep\nlearning-based tomato disease recognition methods with classical training\nprocedures showed promising recognition results, they demand large labelled\ndata and involve expensive training. The traditional deep learning models\nproposed for tomato disease recognition also consume high memory and storage\nbecause of a high number of parameters. While lightweight networks overcome\nsome of these issues to a certain extent, they continue to show low performance\nand struggle to handle imbalanced data. In this paper, a novel Siamese\nnetwork-based lightweight framework is proposed for automatic tomato leaf\ndisease recognition. This framework achieves the highest accuracy of 96.97% on\nthe tomato subset obtained from the PlantVillage dataset and 95.48% on the\nTaiwan tomato leaf disease dataset. Experimental results further confirm that\nthe proposed framework is effective with imbalanced and small data. The\nbackbone deep network integrated with this framework is lightweight with\napproximately 2.9629 million trainable parameters, which is way lower than\nexisting lightweight deep networks.\n", "rewritten_text": "Accurate and timely tomato disease recognition from leaf images is crucial for preventing crop losses through prompt application of control measures.  While recent deep learning methods employing classical training procedures have yielded promising results, they require extensive labeled datasets and computationally expensive training.  Furthermore, traditional deep learning models for tomato disease recognition often consume significant memory and storage due to their large number of parameters. Although lightweight networks mitigate some of these limitations, they typically exhibit lower performance and struggle with imbalanced datasets.  This paper presents a novel lightweight Siamese network framework for automated tomato leaf disease recognition.  This framework achieves a high accuracy of 96.97% on a tomato subset of the PlantVillage dataset and 95.48% on the Taiwan tomato leaf disease dataset.  Experimental results demonstrate the framework's effectiveness with both imbalanced and small datasets.  The integrated backbone deep network is remarkably lightweight, possessing approximately 2.9629 million trainable parameters\u2014significantly fewer than existing lightweight deep networks.\n"}, "1912.01496": {"original_text": "  Stories are diverse and highly personalized, resulting in a large possible\noutput space for story generation. Existing end-to-end approaches produce\nmonotonous stories because they are limited to the vocabulary and knowledge in\na single training dataset. This paper introduces KG-Story, a three-stage\nframework that allows the story generation model to take advantage of external\nKnowledge Graphs to produce interesting stories. KG-Story distills a set of\nrepresentative words from the input prompts, enriches the word set by using\nexternal knowledge graphs, and finally generates stories based on the enriched\nword set. This distill-enrich-generate framework allows the use of external\nresources not only for the enrichment phase, but also for the distillation and\ngeneration phases. In this paper, we show the superiority of KG-Story for\nvisual storytelling, where the input prompt is a sequence of five photos and\nthe output is a short story. Per the human ranking evaluation, stories\ngenerated by KG-Story are on average ranked better than that of the\nstate-of-the-art systems. Our code and output stories are available at\nhttps://github.com/zychen423/KE-VIST.\n", "rewritten_text": "The inherent diversity and personalization of stories create a vast output space for story generation models.  Existing end-to-end approaches, however, often produce monotonous narratives due to their reliance on the limited vocabulary and knowledge within a single training dataset. This paper introduces KG-Story, a three-stage framework that leverages external Knowledge Graphs (KGs) to generate more engaging stories.  KG-Story first distills a set of representative words from the input prompt.  It then enriches this word set using external KGs. Finally, it generates a story based on the enriched word set. This distill-enrich-generate framework facilitates the integration of external resources throughout all three stages\u2014distillation, enrichment, and generation.  We demonstrate KG-Story's superiority in visual storytelling, where the input is a sequence of five photographs and the output is a short story.  Human evaluation rankings show that KG-Story consistently outperforms state-of-the-art systems. Our code and generated stories are available at https://github.com/zychen423/KE-VIST.\n"}, "2307.16825": {"original_text": "  With sufficient paired training samples, the supervised deep learning methods\nhave attracted much attention in image denoising because of their superior\nperformance. However, it is still very challenging to widely utilize the\nsupervised methods in real cases due to the lack of paired noisy-clean images.\nMeanwhile, most self-supervised denoising methods are ineffective as well when\napplied to the real-world denoising tasks because of their strict assumptions\nin applications. For example, as a typical method for self-supervised\ndenoising, the original blind spot network (BSN) assumes that the noise is\npixel-wise independent, which is much different from the real cases. To solve\nthis problem, we propose a novel self-supervised real image denoising framework\nnamed Sampling Difference As Perturbation (SDAP) based on Random Sub-samples\nGeneration (RSG) with a cyclic sample difference loss. Specifically, we dig\ndeeper into the properties of BSN to make it more suitable for real noise.\nSurprisingly, we find that adding an appropriate perturbation to the training\nimages can effectively improve the performance of BSN. Further, we propose that\nthe sampling difference can be considered as perturbation to achieve better\nresults. Finally we propose a new BSN framework in combination with our RSG\nstrategy. The results show that it significantly outperforms other\nstate-of-the-art self-supervised denoising methods on real-world datasets. The\ncode is available at https://github.com/p1y2z3/SDAP.\n", "rewritten_text": "Supervised deep learning methods have shown superior performance in image denoising when sufficient paired training data is available.  However, their widespread real-world application is hampered by the scarcity of paired noisy-clean image datasets.  Similarly, most self-supervised denoising methods prove ineffective in real-world scenarios due to restrictive assumptions. For instance, the Blind Spot Network (BSN), a typical self-supervised approach, assumes pixel-wise independent noise\u2014a condition rarely met in practice.\n\nTo address this limitation, we propose a novel self-supervised real image denoising framework, Sampling Difference As Perturbation (SDAP), based on Random Sub-samples Generation (RSG) and a cyclic sample difference loss.  Our work builds upon BSN, investigating ways to adapt it to real-world noise.  We discovered that introducing appropriate perturbations to the training images significantly improves BSN's performance.  We propose using the sampling difference as this perturbation.  Consequently, we present a modified BSN framework incorporating our RSG strategy.  Experimental results on real-world datasets demonstrate that our approach significantly outperforms other state-of-the-art self-supervised denoising methods.  The code is available at https://github.com/p1y2z3/SDAP.\n"}, "2409.16685": {"original_text": "  Integrating aerial imagery-based scene generation into applications like\nautonomous driving and gaming enhances realism in 3D environments, but\nchallenges remain in creating detailed content for occluded areas and ensuring\nreal-time, consistent rendering. In this paper, we introduce Skyeyes, a novel\nframework that can generate photorealistic sequences of ground view images\nusing only aerial view inputs, thereby creating a ground roaming experience.\nMore specifically, we combine a 3D representation with a view consistent\ngeneration model, which ensures coherence between generated images. This method\nallows for the creation of geometrically consistent ground view images, even\nwith large view gaps. The images maintain improved spatial-temporal coherence\nand realism, enhancing scene comprehension and visualization from aerial\nperspectives. To the best of our knowledge, there are no publicly available\ndatasets that contain pairwise geo-aligned aerial and ground view imagery.\nTherefore, we build a large, synthetic, and geo-aligned dataset using Unreal\nEngine. Both qualitative and quantitative analyses on this synthetic dataset\ndisplay superior results compared to other leading synthesis approaches. See\nthe project page for more results:\nhttps://chaoren2357.github.io/website-skyeyes/.\n", "rewritten_text": "Integrating aerial imagery into applications like autonomous driving and gaming enhances the realism of 3D environments.  However, generating detailed content for occluded areas and ensuring real-time, consistent rendering remain significant challenges.  This paper introduces SkyEyes, a novel framework for generating photorealistic sequences of ground-level images from aerial views alone, enabling immersive ground-roaming experiences.  SkyEyes combines a 3D representation with a view-consistent generation model to ensure coherence between generated images, producing geometrically consistent ground views even with substantial view gaps.  The resulting images exhibit improved spatiotemporal coherence and realism, enhancing scene understanding and visualization from aerial perspectives.  Due to the lack of publicly available datasets containing geo-aligned aerial and ground-level image pairs, we created a large, synthetic, geo-aligned dataset using Unreal Engine.  Qualitative and quantitative analyses on this dataset demonstrate superior performance compared to existing synthesis methods.  For further results, please visit the project page: https://chaoren2357.github.io/website-skyeyes/.\n"}, "2210.02844": {"original_text": "  In this paper, we explore the following question: Are synonym substitution\nattacks really synonym substitution attacks (SSAs)? We approach this question\nby examining how SSAs replace words in the original sentence and show that\nthere are still unresolved obstacles that make current SSAs generate invalid\nadversarial samples. We reveal that four widely used word substitution methods\ngenerate a large fraction of invalid substitution words that are ungrammatical\nor do not preserve the original sentence's semantics. Next, we show that the\nsemantic and grammatical constraints used in SSAs for detecting invalid word\nreplacements are highly insufficient in detecting invalid adversarial samples.\n", "rewritten_text": "This paper investigates the fundamental nature of synonym substitution attacks (SSAs).  We question whether attacks currently classified as SSAs truly merit this designation.  Our analysis examines how SSAs replace words, revealing significant obstacles that prevent the generation of valid adversarial examples.  We demonstrate that four widely used word substitution methods frequently produce ungrammatical or semantically invalid replacements.  Furthermore, we show that the semantic and grammatical constraints employed in SSAs to detect invalid word substitutions are inadequate for identifying invalid adversarial samples.\n"}, "2307.09769": {"original_text": "  Unsupervised domain adaptation (UDA) has increasingly gained interests for\nits capacity to transfer the knowledge learned from a labeled source domain to\nan unlabeled target domain. However, typical UDA methods require concurrent\naccess to both the source and target domain data, which largely limits its\napplication in medical scenarios where source data is often unavailable due to\nprivacy concern. To tackle the source data-absent problem, we present a novel\ntwo-stage source-free domain adaptation (SFDA) framework for medical image\nsegmentation, where only a well-trained source segmentation model and unlabeled\ntarget data are available during domain adaptation. Specifically, in the\nprototype-anchored feature alignment stage, we first utilize the weights of the\npre-trained pixel-wise classifier as source prototypes, which preserve the\ninformation of source features. Then, we introduce the bi-directional transport\nto align the target features with class prototypes by minimizing its expected\ncost. On top of that, a contrastive learning stage is further devised to\nutilize those pixels with unreliable predictions for a more compact target\nfeature distribution. Extensive experiments on a cross-modality medical\nsegmentation task demonstrate the superiority of our method in large domain\ndiscrepancy settings compared with the state-of-the-art SFDA approaches and\neven some UDA methods. Code is available at\nhttps://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) has gained increasing interest for its ability to transfer knowledge learned from a labeled source domain to an unlabeled target domain.  However, typical UDA methods require simultaneous access to both source and target domain data, significantly limiting their application in medical scenarios where source data is often unavailable due to privacy concerns. To address this source data absence, we propose a novel two-stage source-free domain adaptation (SFDA) framework for medical image segmentation.  This framework operates using only a pre-trained source segmentation model and unlabeled target data.  Specifically, the first stage, prototype-anchored feature alignment, utilizes the pre-trained pixel-wise classifier's weights as source prototypes, preserving source feature information.  Bi-directional transport then aligns target features with these class prototypes by minimizing the expected transport cost.  A subsequent contrastive learning stage refines the target feature distribution by focusing on pixels with unreliable predictions.  Extensive experiments on a cross-modality medical image segmentation task demonstrate our method's superiority over state-of-the-art SFDA and even some UDA approaches, particularly in scenarios with large domain discrepancies. Code is available at https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA.\n"}, "1811.08015": {"original_text": "  This paper introduces the problem of automatic font pairing. Font pairing is\nan important design task that is difficult for novices. Given a font selection\nfor one part of a document (e.g., header), our goal is to recommend a font to\nbe used in another part (e.g., body) such that the two fonts used together look\nvisually pleasing. There are three main challenges in font pairing. First, this\nis a fine-grained problem, in which the subtle distinctions between fonts may\nbe important. Second, rules and conventions of font pairing given by human\nexperts are difficult to formalize. Third, font pairing is an asymmetric\nproblem in that the roles played by header and body fonts are not\ninterchangeable. To address these challenges, we propose automatic font pairing\nthrough learning visual relationships from large-scale human-generated font\npairs. We introduce a new database for font pairing constructed from millions\nof PDF documents available on the Internet. We propose two font pairing\nalgorithms: dual-space k-NN and asymmetric similarity metric learning (ASML).\nThese two methods automatically learn fine-grained relationships from\nlarge-scale data. We also investigate several baseline methods based on the\nrules from professional designers. Experiments and user studies demonstrate the\neffectiveness of our proposed dataset and methods.\n", "rewritten_text": "This paper addresses the problem of automatic font pairing, a challenging design task, particularly for novice users.  Given a font selection for one document section (e.g., the header), our goal is to recommend a visually pleasing font for another section (e.g., the body text).  Three key challenges complicate this task: the fine-grained nature of font distinctions, the difficulty of formalizing expert-defined rules and conventions, and the asymmetry inherent in the relationship between header and body fonts.\n\nTo overcome these challenges, we propose an approach based on learning visual relationships from a large-scale dataset of human-generated font pairings.  This dataset, constructed from millions of publicly available online PDF documents, is a novel contribution of this work. We introduce two novel font pairing algorithms: dual-space k-Nearest Neighbors (k-NN) and Asymmetric Similarity Metric Learning (ASML).  Both methods leverage large-scale data to learn fine-grained relationships between fonts.  We also evaluate several baseline methods based on established design principles.  Our experiments and user studies demonstrate the effectiveness of both our proposed dataset and algorithms.\n"}, "2403.15119": {"original_text": "  Person re-identification (ReID) has made great strides thanks to the\ndata-driven deep learning techniques. However, the existing benchmark datasets\nlack diversity, and models trained on these data cannot generalize well to\ndynamic wild scenarios. To meet the goal of improving the explicit\ngeneralization of ReID models, we develop a new Open-World, Diverse,\nCross-Spatial-Temporal dataset named OWD with several distinct features. 1)\nDiverse collection scenes: multiple independent open-world and highly dynamic\ncollecting scenes, including streets, intersections, shopping malls, etc. 2)\nDiverse lighting variations: long time spans from daytime to nighttime with\nabundant illumination changes. 3) Diverse person status: multiple camera\nnetworks in all seasons with normal/adverse weather conditions and diverse\npedestrian appearances (e.g., clothes, personal belongings, poses, etc.). 4)\nProtected privacy: invisible faces for privacy critical applications. To\nimprove the implicit generalization of ReID, we further propose a Latent Domain\nExpansion (LDE) method to develop the potential of source data, which decouples\ndiscriminative identity-relevant and trustworthy domain-relevant features and\nimplicitly enforces domain-randomized identity feature space expansion with\nricher domain diversity to facilitate domain invariant representations. Our\ncomprehensive evaluations with most benchmark datasets in the community are\ncrucial for progress, although this work is far from the grand goal toward\nopen-world and dynamic wild applications.\n", "rewritten_text": "Person re-identification (ReID) has advanced significantly due to data-driven deep learning.  However, existing benchmark datasets lack diversity, hindering the generalization of trained models to dynamic, real-world scenarios. To address this, we introduce OWD, a novel open-world, diverse, cross-spatial-temporal dataset with several key features:  1) diverse collection scenes (streets, intersections, shopping malls, etc.), encompassing multiple independent, open-world, and highly dynamic environments; 2) diverse lighting variations, spanning day and night with significant illumination changes; 3) diverse person characteristics, including data collected across multiple camera networks, various seasons and weather conditions, and a wide range of pedestrian appearances (clothing, belongings, poses, etc.); and 4) protected privacy, with faces anonymized for privacy-sensitive applications.  To further improve ReID generalization, we propose Latent Domain Expansion (LDE), a method that leverages source data by decoupling discriminative identity-relevant features from domain-relevant features.  LDE implicitly expands the identity feature space with richer domain diversity, promoting domain-invariant representations.  While our comprehensive evaluation across major benchmark datasets demonstrates significant progress, further research is needed to fully realize the potential of open-world and dynamic ReID applications.\n"}, "1906.03731": {"original_text": "  Attention mechanisms have recently boosted performance on a range of NLP\ntasks. Because attention layers explicitly weight input components'\nrepresentations, it is also often assumed that attention can be used to\nidentify information that models found important (e.g., specific contextualized\nword tokens). We test whether that assumption holds by manipulating attention\nweights in already-trained text classification models and analyzing the\nresulting differences in their predictions. While we observe some ways in which\nhigher attention weights correlate with greater impact on model predictions, we\nalso find many ways in which this does not hold, i.e., where gradient-based\nrankings of attention weights better predict their effects than their\nmagnitudes. We conclude that while attention noisily predicts input components'\noverall importance to a model, it is by no means a fail-safe indicator.\n", "rewritten_text": "Recent advancements in natural language processing (NLP) have leveraged attention mechanisms to significantly improve performance across a range of tasks.  Because attention layers explicitly weight the representations of input components, it's often assumed that attention weights reveal which information the model deems most important (e.g., specific contextualized word tokens).  To test this assumption, we manipulated attention weights in pre-trained text classification models and analyzed the resulting changes in predictions. While we observed some correlation between higher attention weights and greater impact on predictions, we also found numerous instances where this correlation failed.  Specifically, gradient-based rankings of attention weights proved more reliable predictors of their effects than their magnitudes.  We conclude that, while attention weights offer a noisy approximation of input component importance, they are far from a reliable indicator.\n"}, "1611.08134": {"original_text": "  Color based re-identification methods usually rely on a distance function to\nmeasure the similarity between individuals. In this paper we study the behavior\nof several histogram distance measures in different color spaces. We wonder\nwhether there is a particular histogram distance measure better than others,\nlikewise also, if there is a color space that present better discrimination\nfeatures. Several experiments are designed and evaluated in several images to\nobtain measures against various color spaces. We test in several image\ndatabases. A measure ranking is generated to calculate the area under the CMC,\nthis area is the indicator used to evaluate which distance measure and color\nspace present the best performance for the considered databases. Also, other\nparameters such as the image division in horizontal stripes and number of\nhistogram bins, have been studied.\n", "rewritten_text": "Color-based person re-identification methods typically rely on distance functions to measure inter-individual similarity.  This paper investigates the performance of several histogram distance measures across different color spaces. We examine whether any particular histogram distance measure or color space consistently yields superior discrimination.  Experiments were conducted on multiple image datasets to evaluate performance across various color spaces.  A ranking of distance measures was generated based on the area under the Cumulative Matching Characteristic (CMC) curve, which served as the primary performance metric.  Furthermore, the impact of image partitioning (using horizontal stripes) and the number of histogram bins was also analyzed.\n"}, "2003.05065": {"original_text": "  For many of the physical phenomena around us, we have developed sophisticated\nmodels explaining their behavior. Nevertheless, measuring physical properties\nfrom visual observations is challenging due to the high number of causally\nunderlying physical parameters -- including material properties and external\nforces. In this paper, we propose to measure latent physical properties for\ncloth in the wind without ever having seen a real example before. Our solution\nis an iterative refinement procedure with simulation at its core. The algorithm\ngradually updates the physical model parameters by running a simulation of the\nobserved phenomenon and comparing the current simulation to a real-world\nobservation. The correspondence is measured using an embedding function that\nmaps physically similar examples to nearby points. We consider a case study of\ncloth in the wind, with curling flags as our leading example -- a seemingly\nsimple phenomena but physically highly involved. Based on the physics of cloth\nand its visual manifestation, we propose an instantiation of the embedding\nfunction. For this mapping, modeled as a deep network, we introduce a spectral\nlayer that decomposes a video volume into its temporal spectral power and\ncorresponding frequencies. Our experiments demonstrate that the proposed method\ncompares favorably to prior work on the task of measuring cloth material\nproperties and external wind force from a real-world video.\n", "rewritten_text": "Sophisticated models explain many physical phenomena. However, measuring physical properties from visual observations remains challenging due to the numerous underlying causal parameters, including material properties and external forces.  This paper proposes a novel method for measuring latent physical properties of cloth in wind, without prior real-world examples.  Our approach employs an iterative refinement procedure driven by simulation. The algorithm iteratively updates physical model parameters by comparing simulations to real-world observations.  This comparison uses an embedding function that maps physically similar examples to nearby points in an embedding space.  We focus on the seemingly simple, yet physically complex, example of a curling flag.  Based on the physics of cloth and its visual representation, we propose a deep-network-based embedding function incorporating a spectral layer that decomposes video volumes into temporal spectral power and frequencies.  Our experiments demonstrate that this method outperforms prior work in measuring cloth material properties and external wind forces from video.\n"}, "2408.14860": {"original_text": "  This paper presents DiffSurf, a transformer-based denoising diffusion model\nfor generating and reconstructing 3D surfaces. Specifically, we design a\ndiffusion transformer architecture that predicts noise from noisy 3D surface\nvertices and normals. With this architecture, DiffSurf is able to generate 3D\nsurfaces in various poses and shapes, such as human bodies, hands, animals and\nman-made objects. Further, DiffSurf is versatile in that it can address various\n3D downstream tasks including morphing, body shape variation and 3D human mesh\nfitting to 2D keypoints. Experimental results on 3D human model benchmarks\ndemonstrate that DiffSurf can generate shapes with greater diversity and higher\nquality than previous generative models. Furthermore, when applied to the task\nof single-image 3D human mesh recovery, DiffSurf achieves accuracy comparable\nto prior techniques at a near real-time rate.\n", "rewritten_text": "This paper introduces DiffSurf, a transformer-based denoising diffusion model for generating and reconstructing 3D surfaces.  Our approach employs a novel diffusion transformer architecture that predicts noise from noisy 3D surface vertices and normals.  This architecture enables DiffSurf to generate high-quality 3D surfaces exhibiting diverse poses and shapes, including human bodies, hands, animals, and man-made objects.  Moreover, DiffSurf's versatility extends to various downstream 3D tasks, such as morphing, body shape variation, and 2D keypoint-based 3D human mesh fitting.  Experiments on 3D human model benchmarks demonstrate that DiffSurf generates shapes with superior diversity and quality compared to existing generative models.  Furthermore, in single-image 3D human mesh recovery, DiffSurf achieves comparable accuracy to state-of-the-art methods at near real-time speeds.\n"}, "2111.1341": {"original_text": "  Manual annotation of medical images is highly subjective, leading to\ninevitable and huge annotation biases. Deep learning models may surpass human\nperformance on a variety of tasks, but they may also mimic or amplify these\nbiases. Although we can have multiple annotators and fuse their annotations to\nreduce stochastic errors, we cannot use this strategy to handle the bias caused\nby annotators' preferences. In this paper, we highlight the issue of\nannotator-related biases on medical image segmentation tasks, and propose a\nPreference-involved Annotation Distribution Learning (PADL) framework to\naddress it from the perspective of disentangling an annotator's preference from\nstochastic errors using distribution learning so as to produce not only a meta\nsegmentation but also the segmentation possibly made by each annotator. Under\nthis framework, a stochastic error modeling (SEM) module estimates the meta\nsegmentation map and average stochastic error map, and a series of human\npreference modeling (HPM) modules estimate each annotator's segmentation and\nthe corresponding stochastic error. We evaluated our PADL framework on two\nmedical image benchmarks with different imaging modalities, which have been\nannotated by multiple medical professionals, and achieved promising performance\non all five medical image segmentation tasks.\n", "rewritten_text": "Manual annotation of medical images is inherently subjective, resulting in significant annotation bias. While deep learning models can surpass human performance on various tasks, they risk mimicking or amplifying these biases.  Averaging annotations from multiple annotators mitigates stochastic errors, but not the systematic biases stemming from individual preferences.  This paper addresses annotator bias in medical image segmentation by proposing a Preference-involved Annotation Distribution Learning (PADL) framework.  PADL disentangles annotator preference from stochastic error using distribution learning, generating a meta-segmentation and individual annotator segmentations.  The framework comprises a stochastic error modeling (SEM) module, estimating the meta-segmentation and average stochastic error, and multiple human preference modeling (HPM) modules, each estimating an individual annotator's segmentation and associated stochastic error.  We evaluated PADL on two medical image benchmarks with diverse modalities and multiple expert annotations, achieving promising results across five segmentation tasks.\n"}, "2312.06598": {"original_text": "  Early action recognition is an important and challenging problem that enables\nthe recognition of an action from a partially observed video stream where the\nactivity is potentially unfinished or even not started. In this work, we\npropose a novel model that learns a prototypical representation of the full\naction for each class and uses it to regularize the architecture and the visual\nrepresentations of the partial observations. Our model is very simple in design\nand also efficient. We decompose the video into short clips, where a visual\nencoder extracts features from each clip independently. Later, a decoder\naggregates together in an online fashion features from all the clips for the\nfinal class prediction. During training, for each partial observation, the\nmodel is jointly trained to both predict the label as well as the action\nprototypical representation which acts as a regularizer. We evaluate our method\non multiple challenging real-world datasets and outperform the current\nstate-of-the-art by a significant margin. For example, on early recognition\nobserving only the first 10% of each video, our method improves the SOTA by\n+2.23 Top-1 accuracy on Something-Something-v2, +3.55 on UCF-101, +3.68 on\nSSsub21, and +5.03 on EPIC-Kitchens-55, where prior work used either\nmulti-modal inputs (e.g. optical-flow) or batched inference. Finally, we also\npresent exhaustive ablation studies to motivate the design choices we made, as\nwell as gather insights regarding what our model is learning semantically.\n", "rewritten_text": "Early action recognition, the task of identifying actions from partially observed video streams, presents a significant challenge.  Existing approaches often struggle when the observed activity is incomplete or hasn't begun.  This work introduces a novel, efficient model that learns a prototypical representation of each action class. This prototype regularizes both the model architecture and the visual representations of partial observations.\n\nOur model employs a simple yet effective design.  Short video clips are processed independently by a visual encoder to extract features.  A decoder then aggregates these features online to generate a final class prediction.  During training, the model jointly predicts the action label and the action prototype, using the latter as a regularizer.\n\nEvaluated on several challenging real-world datasets, our method significantly outperforms the state-of-the-art.  Specifically, when observing only the first 10% of each video, we achieve improvements of +2.23% Top-1 accuracy on Something-Something-v2, +3.55% on UCF-101, +3.68% on SSsub21, and +5.03% on EPIC-Kitchens-55.  These gains surpass previous methods that relied on multimodal inputs (e.g., optical flow) or batched inference.\n\nComprehensive ablation studies further validate our design choices and provide insights into the model's semantic learning.\n"}, "2007.1562": {"original_text": "  Named Entity Recognition (NER) is a fundamental NLP task, commonly formulated\nas classification over a sequence of tokens. Morphologically-Rich Languages\n(MRLs) pose a challenge to this basic formulation, as the boundaries of Named\nEntities do not necessarily coincide with token boundaries, rather, they\nrespect morphological boundaries. To address NER in MRLs we then need to answer\ntwo fundamental questions, namely, what are the basic units to be labeled, and\nhow can these units be detected and classified in realistic settings, i.e.,\nwhere no gold morphology is available. We empirically investigate these\nquestions on a novel NER benchmark, with parallel tokenlevel and morpheme-level\nNER annotations, which we develop for Modern Hebrew, a morphologically\nrich-and-ambiguous language. Our results show that explicitly modeling\nmorphological boundaries leads to improved NER performance, and that a novel\nhybrid architecture, in which NER precedes and prunes morphological\ndecomposition, greatly outperforms the standard pipeline, where morphological\ndecomposition strictly precedes NER, setting a new performance bar for both\nHebrew NER and Hebrew morphological decomposition tasks.\n", "rewritten_text": "Named Entity Recognition (NER) is a fundamental natural language processing (NLP) task, typically formulated as sequence classification over tokens.  Morphologically rich languages (MRLs) present a challenge because named entity boundaries often do not align with token boundaries but rather with morphological boundaries.  Addressing NER in MRLs requires answering two key questions:  what are the appropriate units for labeling, and how can these units be identified and classified realistically, i.e., without gold-standard morphological analysis?  We empirically investigate these questions using a novel NER benchmark with parallel token-level and morpheme-level annotations, developed for Modern Hebrew, a morphologically rich and ambiguous language.  Our results demonstrate that explicitly modeling morphological boundaries improves NER performance.  Furthermore, a novel hybrid architecture, where NER precedes and prunes morphological decomposition, significantly outperforms the standard pipeline (where morphological decomposition precedes NER), establishing a new performance benchmark for both Hebrew NER and Hebrew morphological decomposition.\n"}, "2102.12452": {"original_text": "  Probing classifiers have emerged as one of the prominent methodologies for\ninterpreting and analyzing deep neural network models of natural language\nprocessing. The basic idea is simple -- a classifier is trained to predict some\nlinguistic property from a model's representations -- and has been used to\nexamine a wide variety of models and properties. However, recent studies have\ndemonstrated various methodological limitations of this approach. This article\ncritically reviews the probing classifiers framework, highlighting their\npromises, shortcomings, and advances.\n", "rewritten_text": "Probing classifiers have become a prominent methodology for interpreting and analyzing deep neural networks in natural language processing (NLP).  The core principle is straightforward: a classifier is trained to predict a linguistic property from a model's internal representations. This approach has been widely applied to diverse models and properties.  However, recent research has revealed several methodological limitations. This article critically reviews the probing classifier framework, highlighting its potential, shortcomings, and recent advancements.\n"}, "2408.11567": {"original_text": "  Point cloud analysis has achieved significant development and is\nwell-performed in multiple downstream tasks like point cloud classification and\nsegmentation, etc. Being conscious of the simplicity of the position encoding\nstructure in Transformer-based architectures, we attach importance to the\nposition encoding as a high-dimensional part and the patch encoder to offer\nmulti-scale information. Together with the sequential Transformer, the whole\nmodule with position encoding comprehensively constructs a multi-scale feature\nabstraction module that considers both the local parts from the patch and the\nglobal parts from center points as position encoding. With only a few\nparameters, the position embedding module fits the setting of PEFT\n(Parameter-Efficient Fine-Tuning) tasks pretty well. Thus we unfreeze these\nparameters as a fine-tuning part. At the same time, we review the existing\nprompt and adapter tuning methods, proposing a fresh way of prompts and\nsynthesizing them with adapters as dynamic adjustments. Our Proposed method of\nPEFT tasks, namely PPT, with only 1.05% of parameters for training, gets\nstate-of-the-art results in several mainstream datasets, such as 95.01%\naccuracy in the ScanObjectNN OBJ_BG dataset. Codes will be released at\nhttps://github.com/zsc000722/PPT.\n", "rewritten_text": "Point cloud analysis has significantly advanced and performs well in various downstream tasks, including classification and segmentation.  Recognizing the limitations of simple positional encoding in Transformer-based architectures, we designed a high-dimensional positional encoding and a patch encoder to capture multi-scale information.  Combined with a sequential Transformer, this module comprehensively extracts multi-scale features, incorporating both local (patch-level) and global (center-point) context within the positional encoding.  This parameter-efficient positional embedding module (requiring only a small number of parameters) is well-suited for parameter-efficient fine-tuning (PEFT) tasks, and we therefore unfreeze these parameters during fine-tuning.  Furthermore, we propose a novel prompt-adapter synthesis method, improving upon existing prompt and adapter tuning techniques by dynamically adjusting prompts.  Our proposed PEFT method, PPT, achieves state-of-the-art results on several benchmark datasets using only 1.05% of the model parameters for training, including 95.01% accuracy on the ScanObjectNN OBJ_BG dataset.  Code is available at https://github.com/zsc000722/PPT.\n"}, "1607.0003": {"original_text": "  Human evaluation of machine translation normally uses sentence-level measures\nsuch as relative ranking or adequacy scales. However, these provide no insight\ninto possible errors, and do not scale well with sentence length. We argue for\na semantics-based evaluation, which captures what meaning components are\nretained in the MT output, thus providing a more fine-grained analysis of\ntranslation quality, and enabling the construction and tuning of\nsemantics-based MT. We present a novel human semantic evaluation measure, Human\nUCCA-based MT Evaluation (HUME), building on the UCCA semantic representation\nscheme. HUME covers a wider range of semantic phenomena than previous methods\nand does not rely on semantic annotation of the potentially garbled MT output.\nWe experiment with four language pairs, demonstrating HUME's broad\napplicability, and report good inter-annotator agreement rates and correlation\nwith human adequacy scores.\n", "rewritten_text": "Human evaluation of machine translation (MT) typically relies on sentence-level metrics like relative ranking or adequacy scales.  However, these offer limited insight into specific errors and do not scale well with sentence length.  We propose a semantics-based evaluation approach that identifies retained meaning components in the MT output, enabling a more fine-grained analysis of translation quality and facilitating the development and refinement of semantics-based MT systems.  We introduce a novel human semantic evaluation metric, Human UCCA-based MT Evaluation (HUME), leveraging the UCCA semantic representation scheme.  HUME encompasses a broader range of semantic phenomena than previous methods and avoids the need for semantic annotation of potentially flawed MT output.  Experiments across four language pairs demonstrate HUME's broad applicability, exhibiting strong inter-annotator agreement and a high correlation with human adequacy scores.\n"}, "2205.0809": {"original_text": "  Event cameras are bio-inspired sensors that capture per-pixel asynchronous\nintensity change rather than the synchronous absolute intensity frames captured\nby a classical camera sensor. Such cameras are ideal for robotics applications\nsince they have high temporal resolution, high dynamic range and low latency.\nHowever, due to their high temporal resolution, event cameras are particularly\nsensitive to flicker such as from fluorescent or LED lights. During every cycle\nfrom bright to dark, pixels that image a flickering light source generate many\nevents that provide little or no useful information for a robot, swamping the\nuseful data in the scene. In this paper, we propose a novel linear filter to\npreprocess event data to remove unwanted flicker events from an event stream.\nThe proposed algorithm achieves over 4.6 times relative improvement in the\nsignal-to-noise ratio when compared to the raw event stream due to the\neffective removal of flicker from fluorescent lighting. Thus, it is ideally\nsuited to robotics applications that operate in indoor settings or scenes\nilluminated by flickering light sources.\n", "rewritten_text": "Event cameras, bio-inspired sensors, capture per-pixel asynchronous intensity changes, unlike conventional cameras, which capture synchronous absolute intensity frames.  Their high temporal resolution, high dynamic range, and low latency make them ideal for robotics. However, this high temporal resolution also renders them highly susceptible to flicker from sources like fluorescent or LED lights.  Flickering light sources cause affected pixels to generate numerous events during each brightness cycle, overwhelming useful data with largely irrelevant information.  This paper presents a novel linear filter for preprocessing event data to mitigate this flicker.  Our algorithm demonstrates a greater than 4.6-fold improvement in signal-to-noise ratio compared to unfiltered event streams, effectively removing flicker from fluorescent lighting.  This makes it particularly well-suited for robotics applications in indoor environments or scenes with flickering light sources.\n"}, "2205.00627": {"original_text": "  White matter fiber clustering is an important strategy for white matter\nparcellation, which enables quantitative analysis of brain connections in\nhealth and disease. In combination with expert neuroanatomical labeling,\ndata-driven white matter fiber clustering is a powerful tool for creating\natlases that can model white matter anatomy across individuals. While widely\nused fiber clustering approaches have shown good performance using classical\nunsupervised machine learning techniques, recent advances in deep learning\nreveal a promising direction toward fast and effective fiber clustering. In\nthis work, we propose a novel deep learning framework for white matter fiber\nclustering, Deep Fiber Clustering (DFC), which solves the unsupervised\nclustering problem as a self-supervised learning task with a domain-specific\npretext task to predict pairwise fiber distances. This process learns a\nhigh-dimensional embedding feature representation for each fiber, regardless of\nthe order of fiber points reconstructed during tractography. We design a novel\nnetwork architecture that represents input fibers as point clouds and allows\nthe incorporation of additional sources of input information from gray matter\nparcellation to improve anatomical coherence of clusters. In addition, DFC\nconducts outlier removal naturally by rejecting fibers with low cluster\nassignment probability. We evaluate DFC on three independently acquired\ncohorts, including data from 220 individuals across genders, ages (young and\nelderly adults), and different health conditions (healthy control and multiple\nneuropsychiatric disorders). We compare DFC to several state-of-the-art white\nmatter fiber clustering algorithms. Experimental results demonstrate superior\nperformance of DFC in terms of cluster compactness, generalization ability,\nanatomical coherence, and computational efficiency.\n", "rewritten_text": "White matter fiber clustering is a crucial strategy for white matter parcellation, enabling quantitative analysis of brain connectivity in both health and disease.  Combining data-driven clustering with expert neuroanatomical labeling creates powerful atlases that model white matter anatomy across individuals. While classical unsupervised machine learning techniques have yielded good results, recent advances in deep learning offer a promising path toward faster and more effective fiber clustering.  This work introduces Deep Fiber Clustering (DFC), a novel deep learning framework that frames unsupervised clustering as a self-supervised learning problem.  DFC employs a domain-specific pretext task: predicting pairwise fiber distances. This approach learns a high-dimensional embedding for each fiber, irrespective of the tractography-derived fiber point order.  Our novel network architecture represents input fibers as point clouds and integrates additional information from gray matter parcellation to enhance anatomical cluster coherence.  Furthermore, DFC inherently removes outliers by rejecting fibers with low cluster assignment probabilities.  We evaluated DFC on three independent cohorts comprising 220 individuals of varying gender, age (young and elderly adults), and health status (healthy controls and individuals with multiple neuropsychiatric disorders).  Comparison with state-of-the-art white matter fiber clustering algorithms demonstrates DFC's superior performance in terms of cluster compactness, generalization, anatomical coherence, and computational efficiency.\n"}, "1707.08401": {"original_text": "  In the last two decades Computer Aided Diagnostics (CAD) systems were\ndeveloped to help radiologists analyze screening mammograms. The benefits of\ncurrent CAD technologies appear to be contradictory and they should be improved\nto be ultimately considered useful. Since 2012 deep convolutional neural\nnetworks (CNN) have been a tremendous success in image recognition, reaching\nhuman performance. These methods have greatly surpassed the traditional\napproaches, which are similar to currently used CAD solutions. Deep CNN-s have\nthe potential to revolutionize medical image analysis. We propose a CAD system\nbased on one of the most successful object detection frameworks, Faster R-CNN.\nThe system detects and classifies malignant or benign lesions on a mammogram\nwithout any human intervention. The proposed method sets the state of the art\nclassification performance on the public INbreast database, AUC = 0.95 . The\napproach described here has achieved the 2nd place in the Digital Mammography\nDREAM Challenge with AUC = 0.85 . When used as a detector, the system reaches\nhigh sensitivity with very few false positive marks per image on the INbreast\ndataset. Source code, the trained model and an OsiriX plugin are availaible\nonline at https://github.com/riblidezso/frcnn_cad .\n", "rewritten_text": "Over the past two decades, computer-aided diagnosis (CAD) systems have been developed to assist radiologists in analyzing screening mammograms.  However, the benefits of current CAD technologies are inconsistent and require improvement to achieve widespread clinical utility.  Since 2012, deep convolutional neural networks (CNNs) have achieved remarkable success in image recognition, surpassing human performance in some tasks.  These methods significantly outperform traditional approaches, which are similar to existing CAD solutions.  Deep CNNs hold the potential to revolutionize medical image analysis.\n\nThis paper proposes a CAD system based on Faster R-CNN, a highly successful object detection framework.  The system autonomously detects and classifies malignant and benign lesions in mammograms.  This method achieved state-of-the-art classification performance on the public INbreast database (AUC = 0.95).  Furthermore, it secured second place in the Digital Mammography DREAM Challenge (AUC = 0.85).  Used as a detector, the system exhibits high sensitivity with a low false-positive rate per image on the INbreast dataset.  The source code, trained model, and an OsiriX plugin are available online at https://github.com/riblidezso/frcnn_cad.\n"}, "1604.02975": {"original_text": "  We propose a novel Coupled Projection multi-task Metric Learning (CP-mtML)\nmethod for large scale face retrieval. In contrast to previous works which were\nlimited to low dimensional features and small datasets, the proposed method\nscales to large datasets with high dimensional face descriptors. It utilises\npairwise (dis-)similarity constraints as supervision and hence does not require\nexhaustive class annotation for every training image. While, traditionally,\nmulti-task learning methods have been validated on same dataset but different\ntasks, we work on the more challenging setting with heterogeneous datasets and\ndifferent tasks. We show empirical validation on multiple face image datasets\nof different facial traits, e.g. identity, age and expression. We use classic\nLocal Binary Pattern (LBP) descriptors along with the recent Deep Convolutional\nNeural Network (CNN) features. The experiments clearly demonstrate the\nscalability and improved performance of the proposed method on the tasks of\nidentity and age based face image retrieval compared to competitive existing\nmethods, on the standard datasets and with the presence of a million distractor\nface images.\n", "rewritten_text": "We propose Coupled Projection multi-task Metric Learning (CP-mtML), a novel method for large-scale face retrieval. Unlike previous methods limited to low-dimensional features and small datasets, CP-mtML scales effectively to high-dimensional face descriptors and large datasets.  It leverages pairwise (dis)similarity constraints for supervision, eliminating the need for exhaustive class annotation of training images.  Departing from traditional multi-task learning, which typically uses the same dataset for different tasks, we address the more challenging scenario of heterogeneous datasets and diverse tasks.  We empirically validate CP-mtML on multiple face image datasets exhibiting variations in facial traits such as identity, age, and expression.  We utilize both classic Local Binary Pattern (LBP) descriptors and state-of-the-art Deep Convolutional Neural Network (CNN) features.  Experiments demonstrate CP-mtML's superior scalability and performance in identity and age-based face retrieval compared to existing methods, even with a million distractor images on standard benchmark datasets.\n"}, "1705.10872": {"original_text": "  We introduce a novel loss for learning local feature descriptors which is\ninspired by the Lowe's matching criterion for SIFT. We show that the proposed\nloss that maximizes the distance between the closest positive and closest\nnegative patch in the batch is better than complex regularization methods; it\nworks well for both shallow and deep convolution network architectures.\nApplying the novel loss to the L2Net CNN architecture results in a compact\ndescriptor -- it has the same dimensionality as SIFT (128) that shows\nstate-of-art performance in wide baseline stereo, patch verification and\ninstance retrieval benchmarks. It is fast, computing a descriptor takes about 1\nmillisecond on a low-end GPU.\n", "rewritten_text": "We introduce a novel loss function for learning local feature descriptors, inspired by Lowe's SIFT matching criterion.  This loss maximizes the distance between the nearest positive and nearest negative patch within a batch, outperforming complex regularization methods.  It is effective with both shallow and deep convolutional neural network architectures.  Applied to the L2Net CNN architecture, it yields a compact 128-dimensional descriptor\u2014matching the dimensionality of SIFT\u2014that achieves state-of-the-art performance on wide-baseline stereo, patch verification, and instance retrieval benchmarks.  Descriptor computation is fast, requiring approximately 1 millisecond on a low-end GPU.\n"}, "2103.17107": {"original_text": "  In this paper, the multi-task learning of lightweight convolutional neural\nnetworks is studied for face identification and classification of facial\nattributes (age, gender, ethnicity) trained on cropped faces without margins.\nThe necessity to fine-tune these networks to predict facial expressions is\nhighlighted. Several models are presented based on MobileNet, EfficientNet and\nRexNet architectures. It was experimentally demonstrated that they lead to near\nstate-of-the-art results in age, gender and race recognition on the UTKFace\ndataset and emotion classification on the AffectNet dataset. Moreover, it is\nshown that the usage of the trained models as feature extractors of facial\nregions in video frames leads to 4.5% higher accuracy than the previously known\nstate-of-the-art single models for the AFEW and the VGAF datasets from the\nEmotiW challenges. The models and source code are publicly available at\nhttps://github.com/HSE-asavchenko/face-emotion-recognition.\n", "rewritten_text": "This paper investigates multi-task learning with lightweight convolutional neural networks (CNNs) for simultaneous face identification and attribute classification (age, gender, ethnicity).  Trained on cropped faces without margins, these models, based on MobileNet, EfficientNet, and RexNet architectures, are subsequently fine-tuned for facial expression prediction.  Experimental results demonstrate near state-of-the-art performance on the UTKFace dataset (for age, gender, and ethnicity recognition) and the AffectNet dataset (for emotion classification).  Furthermore, using these trained models as feature extractors for facial regions in video frames yielded a 4.5% accuracy improvement over previous state-of-the-art single models on the AFEW and VGAF datasets from the EmotiW challenges. The models and source code are publicly available at https://github.com/HSE-asavchenko/face-emotion-recognition.\n"}, "2310.06474": {"original_text": "  While large language models (LLMs) exhibit remarkable capabilities across a\nwide range of tasks, they pose potential safety concerns, such as the\n``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to\nexhibit undesirable behavior. Although several preventive measures have been\ndeveloped to mitigate the potential risks associated with LLMs, they have\nprimarily focused on English. In this study, we reveal the presence of\nmultilingual jailbreak challenges within LLMs and consider two potential risky\nscenarios: unintentional and intentional. The unintentional scenario involves\nusers querying LLMs using non-English prompts and inadvertently bypassing the\nsafety mechanisms, while the intentional scenario concerns malicious users\ncombining malicious instructions with multilingual prompts to deliberately\nattack LLMs. The experimental results reveal that in the unintentional\nscenario, the rate of unsafe content increases as the availability of languages\ndecreases. Specifically, low-resource languages exhibit about three times the\nlikelihood of encountering harmful content compared to high-resource languages,\nwith both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts\ncan exacerbate the negative impact of malicious instructions, with\nastonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for\nGPT-4. To handle such a challenge in the multilingual context, we propose a\nnovel \\textsc{Self-Defense} framework that automatically generates multilingual\ntraining data for safety fine-tuning. Experimental results show that ChatGPT\nfine-tuned with such data can achieve a substantial reduction in unsafe content\ngeneration. Data is available at\n\\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.\n", "rewritten_text": "Large language models (LLMs) demonstrate remarkable capabilities across diverse tasks, but also present significant safety concerns.  One such concern is the \"jailbreak\" problem, where malicious instructions can elicit undesirable behavior. While mitigation strategies exist, they primarily focus on English.  This study reveals multilingual vulnerabilities in LLMs, exploring two scenarios: unintentional and intentional misuse.  Unintentional misuse occurs when non-English prompts inadvertently bypass safety mechanisms; intentional misuse involves malicious actors combining multilingual prompts with harmful instructions.\n\nOur experiments demonstrate that in unintentional scenarios, unsafe content generation increases with decreasing language resource availability.  Low-resource languages show approximately triple the rate of harmful content compared to high-resource languages, affecting both ChatGPT and GPT-4.  Intentionally malicious multilingual prompts significantly exacerbate the problem, resulting in alarmingly high rates of unsafe output: 80.92% for ChatGPT and 40.71% for GPT-4.\n\nTo address this multilingual challenge, we propose a novel Self-Defense framework that automatically generates multilingual training data for safety fine-tuning.  Experiments show that ChatGPT fine-tuned with this data achieves a substantial reduction in unsafe content generation.  The data is available at [https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs](https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs).\n"}, "2110.13793": {"original_text": "  With camera resolution ever increasing and the need to rapidly recalibrate\nrobotic platforms in less than ideal environments, there is a need for faster\nand more robust chessboard fiducial marker detectors. A new chessboard detector\nis proposed that is specifically designed for: high resolution images,\nfocus/motion blur, harsh lighting conditions, and background clutter. This is\naccomplished using a new x-corner detector, where for the first time blur is\nestimated and used in a novel way to enhance corner localization, edge\nvalidation, and connectivity. Performance is measured and compared against\nother libraries using a diverse set of images created by combining multiple\nthird party datasets and including new specially crafted scenarios designed to\nstress the state-of-the-art. The proposed detector has the best F1- Score of\n0.97, runs 1.9x faster than next fastest, and is a top performer for corner\naccuracy, while being the only detector to have consistent good performance in\nall scenarios.\n", "rewritten_text": "High-resolution cameras and the demand for rapid recalibration of robotic platforms in challenging environments necessitate faster and more robust chessboard fiducial marker detectors.  This paper proposes a novel chessboard detector optimized for high-resolution images, handling focus/motion blur, harsh lighting, and cluttered backgrounds.  This is achieved through a new x-corner detector that uniquely leverages blur estimation to enhance corner localization, edge validation, and connectivity.  Performance is evaluated against other leading libraries using a diverse image dataset, combining multiple third-party sources with newly generated scenarios designed to stress-test state-of-the-art detectors.  The proposed detector achieves a best-in-class F1-score of 0.97, runs 1.9 times faster than the next fastest competitor, exhibits superior corner accuracy, and demonstrates consistently strong performance across all test scenarios.\n"}, "2309.13492": {"original_text": "  Today's image style transfer methods have difficulty retaining humans face\nindividual features after the whole stylizing process. This occurs because the\nfeatures like face geometry and people's expressions are not captured by the\ngeneral-purpose image classifiers like the VGG-19 pre-trained models. This\npaper proposes the use of embeddings from an auxiliary pre-trained face\nrecognition model to encourage the algorithm to propagate human face features\nfrom the content image to the final stylized result.\n", "rewritten_text": "Current image style transfer methods struggle to preserve individual facial features throughout the stylization process.  This limitation arises because general-purpose image classifiers, such as pre-trained VGG-19 models, fail to adequately capture crucial facial attributes like geometry and expression.  This paper proposes leveraging embeddings from an auxiliary pre-trained face recognition model to guide the algorithm, ensuring the propagation of facial features from the content image to the stylized output.\n"}, "2206.04242": {"original_text": "  Despite advances in image classification methods, detecting the samples not\nbelonging to the training classes is still a challenging problem. There has\nbeen a burst of interest in this subject recently, which is called Open-Set\nRecognition (OSR). In OSR, the goal is to achieve both the classification and\ndetecting out-of-distribution (OOD) samples. Several ideas have been proposed\nto push the empirical result further through complicated techniques. We believe\nthat such complication is indeed not necessary. To this end, we have shown that\nMaximum Softmax Probability (MSP), as the simplest baseline for OSR, applied on\nVision Transformers (ViTs) as the base classifier that is trained with non-OOD\naugmentations can surprisingly outperform many recent methods. Non-OOD\naugmentations are the ones that do not alter the data distribution by much. Our\nresults outperform state-of-the-art in CIFAR-10 datasets, and is also better\nthan most of the current methods in SVHN and MNIST. We show that training\naugmentation has a significant effect on the performance of ViTs in the OSR\ntasks, and while they should produce significant diversity in the augmented\nsamples, the generated sample OOD-ness must remain limited.\n", "rewritten_text": "While image classification has advanced significantly, detecting samples outside the training distribution remains a challenge.  This has spurred recent interest in Open-Set Recognition (OSR), which aims to classify in-distribution samples while simultaneously detecting out-of-distribution (OOD) samples.  Although complex techniques have been proposed to improve empirical results, we demonstrate that this complexity is unnecessary.  We show that Maximum Softmax Probability (MSP), a simple OSR baseline, applied to Vision Transformers (ViTs) trained with minimally-distorting, non-OOD augmentations, surprisingly outperforms many state-of-the-art methods.  Our results surpass the state-of-the-art on CIFAR-10 and achieve performance superior to most existing methods on SVHN and MNIST.  This highlights the significant impact of training augmentations on ViT performance in OSR tasks;  while these augmentations should introduce diversity, they must maintain limited OOD characteristics.\n"}, "2010.01041": {"original_text": "  Planar homography estimation is foundational to many computer vision\nproblems, such as Simultaneous Localization and Mapping (SLAM) and Augmented\nReality (AR). However, conditions of high variance confound even the\nstate-of-the-art algorithms. In this report, we analyze the performance of two\nrecently published methods using Convolutional Neural Networks (CNNs) that are\nmeant to replace the more traditional feature-matching based approaches to the\nestimation of homography. Our evaluation of the CNN based methods focuses\nparticularly on measuring the performance under conditions of significant\nnoise, illumination shift, and occlusion. We also measure the benefits of\ntraining CNNs to varying degrees of noise. Additionally, we compare the effect\nof using color images instead of grayscale images for inputs to CNNs. Finally,\nwe compare the results against baseline feature-matching based homography\nestimation methods using SIFT, SURF, and ORB. We find that CNNs can be trained\nto be more robust against noise, but at a small cost to accuracy in the\nnoiseless case. Additionally, CNNs perform significantly better in conditions\nof extreme variance than their feature-matching based counterparts. With regard\nto color inputs, we conclude that with no change in the CNN architecture to\ntake advantage of the additional information in the color planes, the\ndifference in performance using color inputs or grayscale inputs is negligible.\nAbout the CNNs trained with noise-corrupted inputs, we show that training a CNN\nto a specific magnitude of noise leads to a \"Goldilocks Zone\" with regard to\nthe noise levels where that CNN performs best.\n", "rewritten_text": "Planar homography estimation is fundamental to many computer vision problems, including simultaneous localization and mapping (SLAM) and augmented reality (AR).  However, high-variance conditions challenge even state-of-the-art algorithms. This report analyzes the performance of two recently published convolutional neural network (CNN)-based methods designed to replace traditional feature-matching approaches to homography estimation.  Our evaluation focuses on performance under significant noise, illumination shifts, and occlusions. We also assess the benefits of training CNNs with varying levels of noise and compare the use of color versus grayscale images as CNN inputs.  Finally, we benchmark these CNN methods against baseline feature-matching techniques using SIFT, SURF, and ORB.\n\nOur results indicate that CNNs can be trained to exhibit greater robustness to noise, albeit at a slight cost to accuracy in noiseless conditions.  Furthermore, CNNs significantly outperform feature-matching methods under extreme variance.  Regarding color inputs, we found that without architectural modifications to leverage color information, performance differences between color and grayscale inputs were negligible.  Finally, our analysis of noise-corrupted training data reveals a \"Goldilocks zone\": training a CNN to a specific noise level optimizes its performance within a particular noise range.\n"}, "2308.10168": {"original_text": "  Since the recent prosperity of Large Language Models (LLMs), there have been\ninterleaved discussions regarding how to reduce hallucinations from LLM\nresponses, how to increase the factuality of LLMs, and whether Knowledge Graphs\n(KGs), which store the world knowledge in a symbolic form, will be replaced\nwith LLMs. In this paper, we try to answer these questions from a new angle:\nHow knowledgeable are LLMs?\n  To answer this question, we constructed Head-to-Tail, a benchmark that\nconsists of 18K question-answer (QA) pairs regarding head, torso, and tail\nfacts in terms of popularity. We designed an automated evaluation method and a\nset of metrics that closely approximate the knowledge an LLM confidently\ninternalizes. Through a comprehensive evaluation of 16 publicly available LLMs,\nwe show that existing LLMs are still far from being perfect in terms of their\ngrasp of factual knowledge, especially for facts of torso-to-tail entities.\n", "rewritten_text": "The recent proliferation of Large Language Models (LLMs) has spurred considerable debate regarding hallucination reduction, factual accuracy improvement, and the potential obsolescence of Knowledge Graphs (KGs)\u2014symbolic repositories of world knowledge\u2014in favor of LLMs.  This paper addresses these issues from a novel perspective:  How knowledgeable are LLMs, truly?\n\nTo investigate, we developed Head-to-Tail, a benchmark comprising 18,000 question-answer (QA) pairs focusing on facts varying in popularity (head, torso, and tail entities).  We devised an automated evaluation method and associated metrics to accurately assess the factual knowledge confidently internalized by LLMs.  A comprehensive evaluation of 16 publicly available LLMs reveals that their grasp of factual knowledge remains significantly imperfect, particularly concerning torso- and tail-level entities.\n"}, "1912.06683": {"original_text": "  Semantic image segmentation plays a pivotal role in many vision applications\nincluding autonomous driving and medical image analysis. Most of the former\napproaches move towards enhancing the performance in terms of accuracy with a\nlittle awareness of computational efficiency. In this paper, we introduce\nLiteSeg, a lightweight architecture for semantic image segmentation. In this\nwork, we explore a new deeper version of Atrous Spatial Pyramid Pooling module\n(ASPP) and apply short and long residual connections, and depthwise separable\nconvolution, resulting in a faster and efficient model. LiteSeg architecture is\nintroduced and tested with multiple backbone networks as Darknet19, MobileNet,\nand ShuffleNet to provide multiple trade-offs between accuracy and\ncomputational cost. The proposed model LiteSeg, with MobileNetV2 as a backbone\nnetwork, achieves an accuracy of 67.81% mean intersection over union at 161\nframes per second with $640 \\times 360$ resolution on the Cityscapes dataset.\n", "rewritten_text": "Semantic image segmentation is pivotal in numerous vision applications, including autonomous driving and medical image analysis.  However, most existing approaches prioritize accuracy at the expense of computational efficiency. This paper introduces LiteSeg, a lightweight architecture for semantic image segmentation.  We propose a novel, deeper Atrous Spatial Pyramid Pooling (ASPP) module, incorporating short and long residual connections and depthwise separable convolutions to create a faster and more efficient model.  LiteSeg's architecture is evaluated with multiple backbone networks\u2014Darknet19, MobileNet, and ShuffleNet\u2014offering various accuracy-efficiency trade-offs.  Using MobileNetV2 as the backbone, LiteSeg achieves a mean Intersection over Union (IoU) of 67.81% at 161 frames per second with a resolution of 640 \u00d7 360 on the Cityscapes dataset.\n"}, "2402.14281": {"original_text": "  Map representation learned by expert demonstrations has shown promising\nresearch value. However, recent advancements in the visual navigation field\nface challenges due to the lack of human datasets in the real world for\nefficient supervised representation learning of the environments. We present a\nLandmark-Aware Visual Navigation (LAVN) dataset to allow for supervised\nlearning of human-centric exploration policies and map building. We collect RGB\nobservation and human point-click pairs as a human annotator explores virtual\nand real-world environments with the goal of full coverage exploration of the\nspace. The human annotators also provide distinct landmark examples along each\ntrajectory, which we intuit will simplify the task of map or graph building and\nlocalization. These human point-clicks serve as direct supervision for waypoint\nprediction when learning to explore in environments. Our dataset covers a wide\nspectrum of scenes, including rooms in indoor environments, as well as walkways\noutdoors. Dataset is available at DOI: 10.5281/zenodo.10608067.\n", "rewritten_text": "Expert demonstrations have shown the promising research value of learned map representations.  However, recent advancements in visual navigation are hampered by a lack of real-world human datasets suitable for efficient supervised learning of environmental representations.  To address this, we introduce the Landmark-Aware Visual Navigation (LAVN) dataset, enabling supervised learning of human-centric exploration policies and map building.  The dataset comprises RGB observations and human point-click pairs collected as annotators explored virtual and real-world environments, aiming for complete coverage.  Annotators also identified distinct landmarks along each trajectory, facilitating map/graph building and localization.  These point-clicks provide direct supervision for waypoint prediction during exploration learning.  The dataset encompasses a diverse range of scenes, including indoor rooms and outdoor walkways.  The dataset is available at DOI: 10.5281/zenodo.10608067.\n"}, "2408.05184": {"original_text": "  This paper describes our solution of the first subtask from the AXOLOTL-24\nshared task on Semantic Change Modeling. The goal of this subtask is to\ndistribute a given set of usages of a polysemous word from a newer time period\nbetween senses of this word from an older time period and clusters representing\ngained senses of this word. We propose and experiment with three new methods\nsolving this task. Our methods achieve SOTA results according to both official\nmetrics of the first substask. Additionally, we develop a model that can tell\nif a given word usage is not described by any of the provided sense\ndefinitions. This model serves as a component in one of our methods, but can\npotentially be useful on its own.\n", "rewritten_text": "This paper presents our solution to the first subtask of the AXOLOTL-24 shared task on Semantic Change Modeling.  This subtask involves assigning a set of modern word usages of a polysemous word to either older senses of that word or to clusters representing newly acquired senses. We propose and evaluate three novel methods for this task, achieving state-of-the-art (SOTA) results according to the subtask's official metrics.  Furthermore, we developed a model to identify word usages not described by any provided sense definitions. This model, a component of one of our methods, also holds independent potential utility.\n"}, "2312.00335": {"original_text": "  Self-supervised learning (SSL) approaches have recently shown substantial\nsuccess in learning visual representations from unannotated images. Compared\nwith photographic images, medical images acquired with the same imaging\nprotocol exhibit high consistency in anatomy. To exploit this anatomical\nconsistency, this paper introduces a novel SSL approach, called PEAC (patch\nembedding of anatomical consistency), for medical image analysis. Specifically,\nin this paper, we propose to learn global and local consistencies via stable\ngrid-based matching, transfer pre-trained PEAC models to diverse downstream\ntasks, and extensively demonstrate that (1) PEAC achieves significantly better\nperformance than the existing state-of-the-art fully/self-supervised methods,\nand (2) PEAC captures the anatomical structure consistency across views of the\nsame patient and across patients of different genders, weights, and healthy\nstatuses, which enhances the interpretability of our method for medical image\nanalysis.\n", "rewritten_text": "Self-supervised learning (SSL) has recently demonstrated significant success in learning visual representations from unlabeled images.  Medical images acquired under consistent imaging protocols exhibit high anatomical consistency, unlike photographic images.  This paper introduces PEAC (Patch Embedding of Anatomical Consistency), a novel SSL approach leveraging this consistency for medical image analysis.  Specifically, we propose a method that learns both global and local consistencies through stable grid-based matching.  We further demonstrate the efficacy of transferring pre-trained PEAC models to diverse downstream tasks.  Our results show that: (1) PEAC significantly outperforms existing state-of-the-art fully and self-supervised methods; and (2) PEAC captures anatomical structure consistency across different views of the same patient and across patients with varying gender, weight, and health status, thereby enhancing the interpretability of our approach for medical image analysis.\n"}, "2408.15159": {"original_text": "  Translating written sentences from oral languages to a sequence of manual and\nnon-manual gestures plays a crucial role in building a more inclusive society\nfor deaf and hard-of-hearing people. Facial expressions (non-manual), in\nparticular, are responsible for encoding the grammar of the sentence to be\nspoken, applying punctuation, pronouns, or emphasizing signs. These non-manual\ngestures are closely related to the semantics of the sentence being spoken and\nalso to the utterance of the speaker's emotions. However, most Sign Language\nProduction (SLP) approaches are centered on synthesizing manual gestures and do\nnot focus on modeling the speakers expression. This paper introduces a new\nmethod focused in synthesizing facial expressions for sign language. Our goal\nis to improve sign language production by integrating sentiment information in\nfacial expression generation. The approach leverages a sentence sentiment and\nsemantic features to sample from a meaningful representation space, integrating\nthe bias of the non-manual components into the sign language production\nprocess. To evaluate our method, we extend the Frechet Gesture Distance (FGD)\nand propose a new metric called Frechet Expression Distance (FED) and apply an\nextensive set of metrics to assess the quality of specific regions of the face.\nThe experimental results showed that our method achieved state of the art,\nbeing superior to the competitors on How2Sign and PHOENIX14T datasets.\nMoreover, our architecture is based on a carefully designed graph pyramid that\nmakes it simpler, easier to train, and capable of leveraging emotions to\nproduce facial expressions.\n", "rewritten_text": "Translating spoken language into a sequence of manual and non-manual signs is crucial for building a more inclusive society for deaf and hard-of-hearing individuals.  Non-manual elements, such as facial expressions, are essential for encoding grammatical information, punctuation, pronouns, and emphasis.  These non-manual gestures are closely tied to both the semantics and emotional content of the utterance. However, most Sign Language Production (SLP) approaches primarily focus on synthesizing manual gestures, neglecting the crucial role of facial expressions.\n\nThis paper introduces a novel method for synthesizing facial expressions in sign language.  Our goal is to enhance sign language production by integrating sentiment information into facial expression generation.  The approach leverages sentence sentiment and semantic features to sample from a meaningful representation space, incorporating the influence of non-manual components into the SLP process.\n\nTo evaluate our method, we extended the Frechet Gesture Distance (FGD) metric, proposing a new Frechet Expression Distance (FED) metric, and applied a comprehensive set of metrics to assess the quality of specific facial regions.  Experimental results demonstrate state-of-the-art performance, surpassing existing methods on the How2Sign and PHOENIX-14T datasets.  Furthermore, our architecture, based on a carefully designed graph pyramid, is simpler, easier to train, and effectively leverages emotional information to generate realistic facial expressions.\n"}, "2011.08894": {"original_text": "  Medical image segmentation is a relevant task as it serves as the first step\nfor several diagnosis processes, thus it is indispensable in clinical usage.\nWhilst major success has been reported using supervised techniques, they assume\na large and well-representative labelled set. This is a strong assumption in\nthe medical domain where annotations are expensive, time-consuming, and\ninherent to human bias. To address this problem, unsupervised techniques have\nbeen proposed in the literature yet it is still an open problem due to the\ndifficulty of learning any transformation pattern. In this work, we present a\nnovel optimisation model framed into a new CNN-based contrastive registration\narchitecture for unsupervised medical image segmentation. The core of our\napproach is to exploit image-level registration and feature-level from a\ncontrastive learning mechanism, to perform registration-based segmentation.\nFirstly, we propose an architecture to capture the image-to-image\ntransformation pattern via registration for unsupervised medical image\nsegmentation. Secondly, we embed a contrastive learning mechanism into the\nregistration architecture to enhance the discriminating capacity of the network\nin the feature-level. We show that our proposed technique mitigates the major\ndrawbacks of existing unsupervised techniques. We demonstrate, through\nnumerical and visual experiments, that our technique substantially outperforms\nthe current state-of-the-art unsupervised segmentation methods on two major\nmedical image datasets.\n", "rewritten_text": "Medical image segmentation is crucial for numerous diagnostic processes and is indispensable in clinical practice. While supervised techniques have achieved significant success, they rely on large, well-representative labeled datasets\u2014a significant limitation in the medical domain, where annotation is expensive, time-consuming, and prone to human bias.  To overcome this, unsupervised techniques have been explored, but remain challenging due to the difficulty of learning effective transformation patterns.  This work presents a novel optimization model within a new CNN-based contrastive registration architecture for unsupervised medical image segmentation.  Our approach leverages both image-level registration and feature-level contrastive learning to achieve registration-based segmentation.  Specifically, we introduce an architecture that captures image-to-image transformation patterns via registration for unsupervised segmentation and embed a contrastive learning mechanism to enhance the network's discriminative capacity at the feature level.  Our results demonstrate that this technique mitigates key limitations of existing unsupervised methods, substantially outperforming state-of-the-art unsupervised segmentation techniques on two major medical image datasets, as shown through both numerical and visual experiments.\n"}, "2301.02307": {"original_text": "  Narrated ''how-to'' videos have emerged as a promising data source for a wide\nrange of learning problems, from learning visual representations to training\nrobot policies. However, this data is extremely noisy, as the narrations do not\nalways describe the actions demonstrated in the video. To address this problem\nwe introduce the novel task of visual narration detection, which entails\ndetermining whether a narration is visually depicted by the actions in the\nvideo. We propose What You Say is What You Show (WYS^2), a method that\nleverages multi-modal cues and pseudo-labeling to learn to detect visual\nnarrations with only weakly labeled data. Our model successfully detects visual\nnarrations in in-the-wild videos, outperforming strong baselines, and we\ndemonstrate its impact for state-of-the-art summarization and temporal\nalignment of instructional videos.\n", "rewritten_text": "Narrated how-to videos are a promising data source for diverse learning tasks, ranging from visual representation learning to robot policy training.  However, this data is extremely noisy due to inconsistencies between the narration and the demonstrated actions.  To address this, we introduce the novel task of visual narration detection: determining whether a video's narration accurately depicts its visual content.  We propose What You Say is What You Show (WYS\u00b2), a method that leverages multimodal cues and pseudo-labeling to learn this detection from weakly labeled data.  Our model effectively detects visual narrations in real-world videos, surpassing strong baselines and improving state-of-the-art performance in instructional video summarization and temporal alignment.\n"}, "2403.05170": {"original_text": "  This paper proposes a new pipeline for long-tail (LT) recognition. Instead of\nre-weighting or re-sampling, we utilize the long-tailed dataset itself to\ngenerate a balanced proxy that can be optimized through cross-entropy (CE).\nSpecifically, a randomly initialized diffusion model, trained exclusively on\nthe long-tailed dataset, is employed to synthesize new samples for\nunderrepresented classes. Then, we utilize the inherent information in the\noriginal dataset to filter out harmful samples and keep the useful ones. Our\nstrategy, Diffusion model for Long-Tail recognition (DiffuLT), represents a\npioneering utilization of generative models in long-tail recognition. DiffuLT\nachieves state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT,\nsurpassing the best competitors with non-trivial margins. Abundant ablations\nmake our pipeline interpretable, too. The whole generation pipeline is done\nwithout any external data or pre-trained model weights, making it highly\ngeneralizable to real-world long-tailed settings.\n", "rewritten_text": "This paper introduces DiffuLT, a novel pipeline for long-tail (LT) recognition that avoids re-weighting or re-sampling techniques.  Instead, it leverages the long-tailed dataset itself to generate a balanced proxy dataset suitable for cross-entropy (CE) optimization.  Specifically, a randomly initialized diffusion model, trained solely on the long-tailed data, synthesizes new samples for underrepresented classes.  A subsequent filtering step, utilizing inherent information within the original dataset, removes harmful synthetic samples, retaining only beneficial ones.  This approach represents a pioneering application of generative models to long-tail recognition.  DiffuLT achieves state-of-the-art results on CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT, significantly outperforming existing methods.  Comprehensive ablation studies enhance the interpretability of our pipeline.  Furthermore, the entire process requires no external data or pre-trained models, ensuring high generalizability to real-world long-tailed scenarios.\n"}, "2404.14779": {"original_text": "  This study presents a comprehensive analysis and comparison of two\npredominant fine-tuning methodologies - full-parameter fine-tuning and\nparameter-efficient tuning - within the context of medical Large Language\nModels (LLMs). We developed and refined a series of LLMs, based on the Llama-2\narchitecture, specifically designed to enhance medical knowledge retrieval,\nreasoning, and question-answering capabilities. Our experiments systematically\nevaluate the effectiveness of these tuning strategies across various well-known\nmedical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of\n72% on the US Medical Licensing Examination (USMLE) datasets, setting a new\nstandard in performance for openly available medical LLMs. Through this\ncomparative analysis, we aim to identify the most effective and efficient\nmethod for fine-tuning LLMs in the medical domain, thereby contributing\nsignificantly to the advancement of AI-driven healthcare applications.\n", "rewritten_text": "This study comprehensively analyzes and compares two prevalent fine-tuning methodologies\u2014full-parameter fine-tuning and parameter-efficient fine-tuning\u2014for medical large language models (LLMs).  We developed a series of Llama-2-based LLMs optimized for medical knowledge retrieval, reasoning, and question answering.  These models were systematically evaluated on established medical benchmarks.  Our medical LLM, Med42, achieved 72% accuracy on the US Medical Licensing Examination (USMLE) dataset, establishing a new benchmark for publicly available medical LLMs.  This comparative analysis aims to identify the most effective and efficient fine-tuning method for medical LLMs, thereby advancing AI-driven healthcare applications.\n"}, "2411.09694": {"original_text": "  Reranking a list of candidates from a machine translation system with an\nexternal scoring model and returning the highest-scoring candidate remains a\nsimple and effective method for improving the overall output quality.\nTranslation scoring models continue to grow in size, with the best models being\ncomparable to generation models. Thus, reranking can add substantial\ncomputational cost to the translation pipeline. In this work, we pose reranking\nas a Bayesian optimization (BayesOpt) problem. By strategically selecting\ncandidates to score based on a balance of exploration and exploitation, we show\nthat it is possible to find top-scoring candidates when scoring only a fraction\nof the candidate list. For instance, our method achieves the same CometKiwi\nscore using only 70 scoring evaluations compared a baseline system using 180.\nWe present a multi-fidelity setting for BayesOpt, where the candidates are\nfirst scored with a cheaper but noisier proxy scoring model, which further\nimproves the cost-performance tradeoff when using smaller but well-trained\ndistilled proxy scorers.\n", "rewritten_text": "Reranking machine translation candidates using an external scoring model and selecting the highest-scoring candidate remains a simple yet effective method for improving output quality.  However, as translation scoring models grow in size\u2014approaching the complexity of generation models\u2014reranking introduces significant computational overhead.  This work frames reranking as a Bayesian optimization (BayesOpt) problem. By strategically selecting candidates for scoring, balancing exploration and exploitation, we demonstrate that top-scoring candidates can be identified while evaluating only a fraction of the candidate list.  For example, our method achieves a CometKiwi score comparable to a baseline system that evaluates 180 candidates, using only 70 evaluations.  Furthermore, we introduce a multi-fidelity BayesOpt setting.  This employs a cheaper, albeit noisier, proxy scoring model for initial candidate assessment, further enhancing cost-performance by leveraging smaller, well-trained distilled proxy scorers.\n"}, "2407.07080": {"original_text": "  Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.\n", "rewritten_text": "Training large language models (LLMs) in low-resource languages like Hebrew presents unique challenges.  This paper introduces DictaLM2.0 and DictaLM2.0-Instruct, two LLMs derived from the Mistral model and trained on a substantial corpus of approximately 200 billion Hebrew and English tokens.  Adapting a pre-trained model to a new language requires specialized techniques that differ significantly from training a model from scratch or fine-tuning one already trained on high-resource languages such as English. We detail these novel training methodologies, which enable effective learning and adaptation to Hebrew's linguistic properties.  Furthermore, we fine-tuned DictaLM2.0-Instruct on a comprehensive instruction dataset to improve its performance on task-specific instructions.  To rigorously evaluate our models, we introduce a new benchmark suite for Hebrew LLMs, encompassing diverse tasks such as question answering, sentiment analysis, the Winograd Schema Challenge, translation, and summarization.  Our work addresses the complexities of LLM training in low-resource settings and proposes a framework adaptable to other non-English languages, contributing significantly to multilingual natural language processing (NLP).\n"}, "2211.08544": {"original_text": "  Quantization-aware training (QAT) receives extensive popularity as it well\nretains the performance of quantized networks. In QAT, the contemporary\nexperience is that all quantized weights are updated for an entire training\nprocess. In this paper, this experience is challenged based on an interesting\nphenomenon we observed. Specifically, a large portion of quantized weights\nreaches the optimal quantization level after a few training epochs, which we\nrefer to as the partly scratch-off lottery ticket. This\nstraightforward-yet-valuable observation naturally inspires us to zero out\ngradient calculations of these weights in the remaining training period to\navoid meaningless updating. To effectively find the ticket, we develop a\nheuristic method, dubbed lottery ticket scratcher (LTS), which freezes a weight\nonce the distance between the full-precision one and its quantization level is\nsmaller than a controllable threshold. Surprisingly, the proposed LTS typically\neliminates 50%-70% weight updating and 25%-35% FLOPs of the backward pass,\nwhile still resulting on par with or even better performance than the compared\nbaseline. For example, compared with the baseline, LTS improves 2-bit\nMobileNetV2 by 5.05%, eliminating 46% weight updating and 23% FLOPs of the\nbackward pass. Code is at url{https://github.com/zysxmu/LTS}.\n", "rewritten_text": "Quantization-aware training (QAT) is widely popular due to its ability to effectively preserve the performance of quantized networks.  Conventionally, all quantized weights are updated throughout the entire QAT process.  This paper challenges this convention based on an observed phenomenon: a significant portion of quantized weights reach their optimal quantization level after only a few training epochs. We refer to this as the \"partly scratch-off lottery ticket.\"  This observation suggests that gradient calculations for these weights can be eliminated during the remaining training, preventing meaningless updates.  To identify these weights effectively, we propose a heuristic method, Lottery Ticket Scratcher (LTS), which freezes a weight when the distance between its full-precision value and its quantization level falls below a controllable threshold.  Surprisingly, LTS typically eliminates 50\u201370% of weight updates and 25\u201335% of backward pass floating-point operations (FLOPs), while achieving comparable or even superior performance to baseline methods. For example, compared to the baseline, LTS improves the performance of a 2-bit MobileNetV2 by 5.05%, eliminating 46% of weight updates and 23% of backward pass FLOPs.  The code is available at [https://github.com/zysxmu/LTS](https://github.com/zysxmu/LTS).\n"}, "2001.04642": {"original_text": "  We address the dual problems of novel view synthesis and environment\nreconstruction from hand-held RGBD sensors. Our contributions include 1)\nmodeling highly specular objects, 2) modeling inter-reflections and Fresnel\neffects, and 3) enabling surface light field reconstruction with the same input\nneeded to reconstruct shape alone. In cases where scene surface has a strong\nmirror-like material component, we generate highly detailed environment images,\nrevealing room composition, objects, people, buildings, and trees visible\nthrough windows. Our approach yields state of the art view synthesis\ntechniques, operates on low dynamic range imagery, and is robust to geometric\nand calibration errors.\n", "rewritten_text": "This work addresses the dual challenges of novel view synthesis and environment reconstruction using handheld RGB-D sensors.  Our contributions are threefold: (1) a model for highly specular objects; (2) a model incorporating inter-reflections and Fresnel effects; and (3) the ability to reconstruct surface light fields using the same input data required for shape reconstruction.  For scenes containing strongly specular surfaces, our method generates highly detailed environment maps, revealing the composition of the room, including objects, people, buildings, and trees visible through windows.  Our approach achieves state-of-the-art view synthesis, operates on low dynamic range imagery, and is robust to geometric and calibration errors.\n"}, "1601.00199": {"original_text": "  Active Appearance Models (AAMs) are one of the most popular and\nwell-established techniques for modeling deformable objects in computer vision.\nIn this paper, we study the problem of fitting AAMs using Compositional\nGradient Descent (CGD) algorithms. We present a unified and complete view of\nthese algorithms and classify them with respect to three main characteristics:\ni) cost function; ii) type of composition; and iii) optimization method.\nFurthermore, we extend the previous view by: a) proposing a novel Bayesian cost\nfunction that can be interpreted as a general probabilistic formulation of the\nwell-known project-out loss; b) introducing two new types of composition,\nasymmetric and bidirectional, that combine the gradients of both image and\nappearance model to derive better conver- gent and more robust CGD algorithms;\nand c) providing new valuable insights into existent CGD algorithms by\nreinterpreting them as direct applications of the Schur complement and the\nWiberg method. Finally, in order to encourage open research and facilitate\nfuture comparisons with our work, we make the implementa- tion of the\nalgorithms studied in this paper publicly available as part of the Menpo\nProject.\n", "rewritten_text": "Active Appearance Models (AAMs) are a widely used and established technique for modeling deformable objects in computer vision. This paper investigates AAM fitting using Compositional Gradient Descent (CGD) algorithms.  We provide a unified framework for these algorithms, classifying them based on three key characteristics: (i) cost function; (ii) composition type; and (iii) optimization method.  Furthermore, we extend this framework by: (a) proposing a novel Bayesian cost function, generalizing the well-known project-out loss; (b) introducing two new composition types\u2014asymmetric and bidirectional\u2014which combine image and appearance model gradients for improved convergence and robustness; and (c) offering new insights into existing CGD algorithms by reinterpreting them as direct applications of the Schur complement and the Wiberg method.  Finally, to foster open research and facilitate future comparisons, we have made the implementation of these algorithms publicly available as part of the Menpo Project.\n"}, "1709.01695": {"original_text": "  3D action recognition was shown to benefit from a covariance representation\nof the input data (joint 3D positions). A kernel machine feed with such feature\nis an effective paradigm for 3D action recognition, yielding state-of-the-art\nresults. Yet, the whole framework is affected by the well-known scalability\nissue. In fact, in general, the kernel function has to be evaluated for all\npairs of instances inducing a Gram matrix whose complexity is quadratic in the\nnumber of samples. In this work we reduce such complexity to be linear by\nproposing a novel and explicit feature map to approximate the kernel function.\nThis allows to train a linear classifier with an explicit feature encoding,\nwhich implicitly implements a Log-Euclidean machine in a scalable fashion. Not\nonly we prove that the proposed approximation is unbiased, but also we work out\nan explicit strong bound for its variance, attesting a theoretical superiority\nof our approach with respect to existing ones. Experimentally, we verify that\nour representation provides a compact encoding and outperforms other\napproximation schemes on a number of publicly available benchmark datasets for\n3D action recognition.\n", "rewritten_text": "Covariance representations of input joint 3D positions have proven beneficial for 3D action recognition.  Kernel machines utilizing these features constitute an effective paradigm, achieving state-of-the-art results. However, this framework suffers from well-known scalability issues due to the quadratic complexity of the Gram matrix, requiring kernel function evaluations for all pairs of instances.  This work addresses this limitation by proposing a novel, explicit feature map that linearly approximates the kernel function. This enables training a linear classifier with an explicit feature encoding, implicitly implementing a scalable Log-Euclidean machine.  We demonstrate that this approximation is unbiased and derive a strong bound on its variance, theoretically establishing its superiority over existing methods.  Experimental results on several publicly available benchmark datasets for 3D action recognition confirm that our representation provides a compact encoding and outperforms other approximation schemes.\n"}, "2212.0583": {"original_text": "  Directly training a document-to-document (Doc2Doc) neural machine translation\n(NMT) via Transformer from scratch, especially on small datasets usually fails\nto converge. Our dedicated probing tasks show that 1) both the absolute\nposition and relative position information gets gradually weakened or even\nvanished once it reaches the upper encoder layers, and 2) the vanishing of\nabsolute position information in encoder output causes the training failure of\nDoc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer\n(P-Transformer) to enhance both the absolute and relative position information\nin both self-attention and cross-attention. Specifically, we integrate absolute\npositional information, i.e., position embeddings, into the query-key pairs\nboth in self-attention and cross-attention through a simple yet effective\naddition operation. Moreover, we also integrate relative position encoding in\nself-attention. The proposed P-Transformer utilizes sinusoidal position\nencoding and does not require any task-specified position embedding, segment\nembedding, or attention mechanism. Through the above methods, we build a\nDoc2Doc NMT model with P-Transformer, which ingests the source document and\ncompletely generates the target document in a sequence-to-sequence (seq2seq)\nway. In addition, P-Transformer can be applied to seq2seq-based\ndocument-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)\ntranslation. Extensive experimental results of Doc2Doc NMT show that\nP-Transformer significantly outperforms strong baselines on widely-used 9\ndocument-level datasets in 7 language pairs, covering small-, middle-, and\nlarge-scales, and achieves a new state-of-the-art. Experimentation on discourse\nphenomena shows that our Doc2Doc NMT models improve the translation quality in\nboth BLEU and discourse coherence. We make our code available on Github.\n", "rewritten_text": "Training document-to-document (Doc2Doc) neural machine translation (NMT) models using Transformers from scratch often fails to converge, especially with small datasets.  Our analysis reveals that: 1) absolute and relative positional information weakens or vanishes in upper encoder layers; and 2) this vanishing of absolute positional information causes training failure.  To address this, we propose a Position-Aware Transformer (P-Transformer) that enhances both absolute and relative positional information in self-attention and cross-attention mechanisms.  Specifically, we integrate absolute positional embeddings into query-key pairs via simple addition in both self-attention and cross-attention, and incorporate relative positional encoding into self-attention.  Our P-Transformer uses sinusoidal positional encoding, requiring no task-specific positional, segmental embeddings, or specialized attention mechanisms.\n\nThis P-Transformer forms the basis of our Doc2Doc NMT model, which translates source documents to target documents in a sequence-to-sequence (seq2seq) manner.  Furthermore, P-Transformer is applicable to seq2seq-based document-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent) translation.  Extensive experiments on nine widely used document-level datasets across seven language pairs\u2014encompassing small, medium, and large-scale datasets\u2014demonstrate that our P-Transformer significantly outperforms strong baselines, achieving state-of-the-art results.  Analysis of discourse phenomena shows improvements in both BLEU scores and discourse coherence.  Our code is publicly available on GitHub.\n"}, "2308.05430": {"original_text": "  In this work, we propose an ensemble modeling approach for multimodal action\nrecognition. We independently train individual modality models using a variant\nof focal loss tailored to handle the long-tailed distribution of the MECCANO\n[21] dataset. Based on the underlying principle of focal loss, which captures\nthe relationship between tail (scarce) classes and their prediction\ndifficulties, we propose an exponentially decaying variant of focal loss for\nour current task. It initially emphasizes learning from the hard misclassified\nexamples and gradually adapts to the entire range of examples in the dataset.\nThis annealing process encourages the model to strike a balance between\nfocusing on the sparse set of hard samples, while still leveraging the\ninformation provided by the easier ones. Additionally, we opt for the late\nfusion strategy to combine the resultant probability distributions from RGB and\nDepth modalities for final action prediction. Experimental evaluations on the\nMECCANO dataset demonstrate the effectiveness of our approach.\n", "rewritten_text": "This work proposes an ensemble modeling approach for multimodal action recognition.  We independently train individual modality models using a modified focal loss function designed to address the long-tailed distribution of the MECCANO dataset [21].  This modified focal loss, inspired by the original's focus on the relationship between rare classes and their prediction difficulty, employs an exponentially decaying weighting scheme.  This allows the model to initially prioritize learning from misclassified examples, gradually transitioning to a balanced consideration of all data points. This annealing process balances the benefits of focusing on hard samples with the information contained in easier ones.  Finally, we employ late fusion to combine the probability distributions from RGB and depth modalities for final action prediction.  Experiments on the MECCANO dataset demonstrate the effectiveness of our approach.\n"}, "1810.1161": {"original_text": "  Despite remarkable advances in image synthesis research, existing works often\nfail in manipulating images under the context of large geometric\ntransformations. Synthesizing person images conditioned on arbitrary poses is\none of the most representative examples where the generation quality largely\nrelies on the capability of identifying and modeling arbitrary transformations\non different body parts. Current generative models are often built on local\nconvolutions and overlook the key challenges (e.g. heavy occlusions, different\nviews or dramatic appearance changes) when distinct geometric changes happen\nfor each part, caused by arbitrary pose manipulations. This paper aims to\nresolve these challenges induced by geometric variability and spatial\ndisplacements via a new Soft-Gated Warping Generative Adversarial Network\n(Warping-GAN), which is composed of two stages: 1) it first synthesizes a\ntarget part segmentation map given a target pose, which depicts the\nregion-level spatial layouts for guiding image synthesis with higher-level\nstructure constraints; 2) the Warping-GAN equipped with a soft-gated\nwarping-block learns feature-level mapping to render textures from the original\nimage into the generated segmentation map. Warping-GAN is capable of\ncontrolling different transformation degrees given distinct target poses.\nMoreover, the proposed warping-block is light-weight and flexible enough to be\ninjected into any networks. Human perceptual studies and quantitative\nevaluations demonstrate the superiority of our Warping-GAN that significantly\noutperforms all existing methods on two large datasets.\n", "rewritten_text": "While image synthesis has made remarkable progress, existing methods struggle with large geometric transformations.  A prime example is synthesizing person images conditioned on arbitrary poses, where generation quality hinges on accurately identifying and modeling transformations across different body parts. Current generative models, often based on local convolutions, overlook significant challenges\u2014heavy occlusions, varying viewpoints, and dramatic appearance changes\u2014resulting from the part-specific geometric changes inherent in arbitrary pose manipulation.\n\nThis paper addresses these challenges, arising from geometric variability and spatial displacement, by introducing a novel Soft-Gated Warping Generative Adversarial Network (Warping-GAN).  Warping-GAN operates in two stages:  1) it first generates a target part segmentation map from a target pose, providing region-level spatial layouts to guide image synthesis with higher-level structural constraints; and 2) a soft-gated warping block within Warping-GAN learns a feature-level mapping to transfer textures from the source image onto this generated segmentation map.  Warping-GAN can control the degree of transformation based on the target pose. Furthermore, the proposed warping block is lightweight and easily integrated into various networks.  Human perceptual studies and quantitative evaluations on two large datasets demonstrate Warping-GAN's significant superiority over existing methods.\n"}, "2410.10407": {"original_text": "  The widespread dissemination of false information through manipulative\ntactics that combine deceptive text and images threatens the integrity of\nreliable sources of information. While there has been research on detecting\nfake news in high resource languages using multimodal approaches, methods for\nlow resource Indic languages primarily rely on textual analysis. This\ndifference highlights the need for robust methods that specifically address\nmultimodal fake news in Indic languages, where the lack of extensive datasets\nand tools presents a significant obstacle to progress. To this end, we\nintroduce the Multimodal Multilingual dataset for Indic Fake News Detection\n(MMIFND). This meticulously curated dataset consists of 28,085 instances\ndistributed across Hindi, Bengali, Marathi, Malayalam, Tamil, Gujarati and\nPunjabi. We further propose the Multimodal Multilingual Caption-aware framework\nfor Fake News Detection (MMCFND). MMCFND utilizes pre-trained unimodal encoders\nand pairwise encoders from a foundational model that aligns vision and\nlanguage, allowing for extracting deep representations from visual and textual\ncomponents of news articles. The multimodal fusion encoder in the foundational\nmodel integrates text and image representations derived from its pairwise\nencoders to generate a comprehensive cross modal representation. Furthermore,\nwe generate descriptive image captions that provide additional context to\ndetect inconsistencies and manipulations. The retrieved features are then fused\nand fed into a classifier to determine the authenticity of news articles. The\ncurated dataset can potentially accelerate research and development in low\nresource environments significantly. Thorough experimentation on MMIFND\ndemonstrates that our proposed framework outperforms established methods for\nextracting relevant fake news detection features.\n", "rewritten_text": "The proliferation of misinformation through manipulative tactics combining deceptive text and images undermines the credibility of reliable information sources. While high-resource languages benefit from multimodal fake news detection research, low-resource Indic languages primarily rely on textual analysis. This disparity necessitates robust methods specifically addressing multimodal fake news in these languages, hampered by a scarcity of datasets and tools.  To address this, we introduce the Multimodal Multilingual Dataset for Indic Fake News Detection (MMIFND), a meticulously curated dataset comprising 28,085 instances across seven Indic languages: Hindi, Bengali, Marathi, Malayalam, Tamil, Gujarati, and Punjabi.  We further propose the Multimodal Multilingual Caption-aware framework for Fake News Detection (MMCFND).  MMCFND leverages pre-trained unimodal and pairwise encoders from a foundational vision-language model to extract deep representations from the visual and textual components of news articles.  The foundational model's multimodal fusion encoder integrates these representations to generate a comprehensive cross-modal representation.  Furthermore, we generate descriptive image captions to provide additional contextual information for detecting inconsistencies and manipulations.  These fused features are then fed into a classifier to determine authenticity.  This curated dataset has the potential to significantly accelerate research and development in low-resource settings.  Extensive experimentation on MMIFND demonstrates that our proposed framework surpasses existing methods in extracting relevant features for fake news detection.\n"}, "2401.17207": {"original_text": "  A comprehensive understanding of the organizational principles in the human\nbrain requires, among other factors, well-quantifiable descriptors of nerve\nfiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a\nmicroscopic imaging technique that enables insights into the fine-grained\norganization of myelinated nerve fibers with high resolution. Descriptors\ncharacterizing the fiber architecture observed in 3D-PLI would enable\ndownstream analysis tasks such as multimodal correlation studies, clustering,\nand mapping. However, best practices for observer-independent characterization\nof fiber architecture in 3D-PLI are not yet available. To this end, we propose\nthe application of a fully data-driven approach to characterize nerve fiber\narchitecture in 3D-PLI images using self-supervised representation learning. We\nintroduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the\nspatial neighborhood of texture examples across histological brain sections of\na 3D reconstructed volume to sample positive pairs for contrastive learning. We\ncombine this sampling strategy with specifically designed image augmentations\nto gain robustness to typical variations in 3D-PLI parameter maps. The approach\nis demonstrated for the 3D reconstructed occipital lobe of a vervet monkey\nbrain. We show that extracted features are highly sensitive to different\nconfigurations of nerve fibers, yet robust to variations between consecutive\nbrain sections arising from histological processing. We demonstrate their\npractical applicability for retrieving clusters of homogeneous fiber\narchitecture and performing data mining for interactively selected templates of\nspecific components of fiber architecture such as U-fibers.\n", "rewritten_text": "Understanding the organizational principles of the human brain requires, among other factors, quantifiable descriptors of nerve fiber architecture. Three-dimensional polarized light imaging (3D-PLI) offers high-resolution microscopic insights into the fine-grained organization of myelinated nerve fibers.  Quantitative descriptors derived from 3D-PLI would facilitate downstream analyses such as multimodal correlation studies, clustering, and mapping. However, observer-independent characterization of 3D-PLI fiber architecture lacks established best practices.  Therefore, we propose a fully data-driven approach using self-supervised representation learning to characterize nerve fiber architecture in 3D-PLI images.  We introduce a 3D-Context Contrastive Learning (CL-3D) objective that leverages the spatial neighborhood of textural features across histological sections within a 3D-reconstructed volume to generate positive pairs for contrastive learning.  This sampling strategy, combined with specifically designed image augmentations, enhances robustness to typical variations in 3D-PLI parameter maps.  We demonstrate this approach on a 3D-reconstructed vervet monkey occipital lobe.  The extracted features exhibit high sensitivity to different nerve fiber configurations while remaining robust to inter-section variations resulting from histological processing.  We further demonstrate their practical utility in clustering homogeneous fiber architectures and performing data mining using interactively selected templates of specific fiber components, such as U-fibers.\n"}, "1703.04454": {"original_text": "  We address the problem of estimating human pose and body shape from 3D scans\nover time. Reliable estimation of 3D body shape is necessary for many\napplications including virtual try-on, health monitoring, and avatar creation\nfor virtual reality. Scanning bodies in minimal clothing, however, presents a\npractical barrier to these applications. We address this problem by estimating\nbody shape under clothing from a sequence of 3D scans. Previous methods that\nhave exploited body models produce smooth shapes lacking personalized details.\nWe contribute a new approach to recover a personalized shape of the person. The\nestimated shape deviates from a parametric model to fit the 3D scans. We\ndemonstrate the method using high quality 4D data as well as sequences of\nvisual hulls extracted from multi-view images. We also make available BUFF, a\nnew 4D dataset that enables quantitative evaluation\n(http://buff.is.tue.mpg.de). Our method outperforms the state of the art in\nboth pose estimation and shape estimation, qualitatively and quantitatively.\n", "rewritten_text": "This paper addresses the challenge of estimating human pose and body shape from temporal 3D scan data. Accurate 3D body shape estimation is crucial for numerous applications, including virtual try-on, health monitoring, and virtual reality avatar creation.  However, the practical limitations of scanning bodies in minimal clothing hinder these applications.  We overcome this limitation by estimating body shape *under* clothing from sequences of 3D scans.  Previous methods utilizing body models often produce overly smooth shapes lacking individual detail.  Our contribution is a novel approach that recovers personalized body shapes by allowing the estimated shape to deviate from a parametric model to better fit the 3D scan data.  We demonstrate our method using high-quality 4D data and sequences of visual hulls extracted from multi-view images.  Furthermore, we introduce BUFF, a new 4D dataset for quantitative evaluation (http://buff.is.tue.mpg.de).  Our method achieves state-of-the-art performance in both pose and shape estimation, demonstrating superior results both qualitatively and quantitatively.\n"}, "2002.01359": {"original_text": "  This paper gives an overview of the Schema-Guided Dialogue State Tracking\ntask of the 8th Dialogue System Technology Challenge. The goal of this task is\nto develop dialogue state tracking models suitable for large-scale virtual\nassistants, with a focus on data-efficient joint modeling across domains and\nzero-shot generalization to new APIs. This task provided a new dataset\nconsisting of over 16000 dialogues in the training set spanning 16 domains to\nhighlight these challenges, and a baseline model capable of zero-shot\ngeneralization to new APIs. Twenty-five teams participated, developing a range\nof neural network models, exceeding the performance of the baseline model by a\nvery high margin. The submissions incorporated a variety of pre-trained\nencoders and data augmentation techniques. This paper describes the task\ndefinition, dataset and evaluation methodology. We also summarize the approach\nand results of the submitted systems to highlight the overall trends in the\nstate-of-the-art.\n", "rewritten_text": "This paper provides an overview of the schema-guided dialogue state tracking task from the 8th Dialogue System Technology Challenge (DSTC8).  The task aimed to develop dialogue state tracking models suitable for large-scale virtual assistants, emphasizing data-efficient joint modeling across domains and zero-shot generalization to new APIs.  To address these challenges, the challenge provided a new dataset comprising over 16,000 dialogues across 16 domains, along with a baseline model capable of zero-shot generalization.  Twenty-five teams participated, developing diverse neural network models that significantly outperformed the baseline.  Submissions incorporated various pre-trained encoders and data augmentation techniques. This paper details the task definition, dataset, and evaluation methodology, and summarizes the approaches and results of the submitted systems to identify key trends in the state-of-the-art.\n"}, "2403.10574": {"original_text": "  The rich spatio-temporal information is crucial to capture the complicated\ntarget appearance variations in visual tracking. However, most top-performing\ntracking algorithms rely on many hand-crafted components for spatio-temporal\ninformation aggregation. Consequently, the spatio-temporal information is far\naway from being fully explored. To alleviate this issue, we propose an adaptive\ntracker with spatio-temporal transformers (named AQATrack), which adopts simple\nautoregressive queries to effectively learn spatio-temporal information without\nmany hand-designed components. Firstly, we introduce a set of learnable and\nautoregressive queries to capture the instantaneous target appearance changes\nin a sliding window fashion. Then, we design a novel attention mechanism for\nthe interaction of existing queries to generate a new query in current frame.\nFinally, based on the initial target template and learnt autoregressive\nqueries, a spatio-temporal information fusion module (STM) is designed for\nspatiotemporal formation aggregation to locate a target object. Benefiting from\nthe STM, we can effectively combine the static appearance and instantaneous\nchanges to guide robust tracking. Extensive experiments show that our method\nsignificantly improves the tracker's performance on six popular tracking\nbenchmarks: LaSOT, LaSOText, TrackingNet, GOT-10k, TNL2K, and UAV123.\n", "rewritten_text": "Rich spatio-temporal information is crucial for capturing the complex variations in target appearance during visual tracking.  However, most high-performing tracking algorithms rely heavily on hand-crafted components for spatio-temporal aggregation, leaving this information largely unexplored.  To address this, we propose AQATrack, an adaptive tracker employing spatio-temporal transformers.  AQATrack uses simple autoregressive queries to efficiently learn spatio-temporal information without extensive hand-designed components.  First, learnable, autoregressive queries capture instantaneous target appearance changes using a sliding window approach.  Next, a novel attention mechanism facilitates interaction between existing queries to generate a new query for the current frame.  Finally, a spatio-temporal information fusion module (STM) combines the initial target template and learned autoregressive queries to aggregate spatio-temporal information and locate the target.  The STM effectively integrates static appearance and instantaneous changes for robust tracking.  Extensive experiments on six benchmark datasets\u2014LaSOT, LaSOText, TrackingNet, GOT-10k, TNL2K, and UAV123\u2014demonstrate significant performance improvements.\n"}, "2206.1503": {"original_text": "  Question Answering (QA) is one of the most important natural language\nprocessing (NLP) tasks. It aims using NLP technologies to generate a\ncorresponding answer to a given question based on the massive unstructured\ncorpus. With the development of deep learning, more and more challenging QA\ndatasets are being proposed, and lots of new methods for solving them are also\nemerging. In this paper, we investigate influential QA datasets that have been\nreleased in the era of deep learning. Specifically, we begin with introducing\ntwo of the most common QA tasks - textual question answer and visual question\nanswering - separately, covering the most representative datasets, and then\ngive some current challenges of QA research.\n", "rewritten_text": "Question answering (QA) is a crucial natural language processing (NLP) task.  It leverages NLP technologies to generate answers to questions based on large, unstructured corpora.  The rise of deep learning has spurred the creation of increasingly challenging QA datasets and innovative solution methods. This paper examines influential QA datasets developed during the deep learning era.  We begin by introducing two prevalent QA tasks\u2014textual question answering and visual question answering\u2014separately, highlighting representative datasets for each.  Finally, we discuss current challenges in QA research.\n"}, "1604.04333": {"original_text": "  Deep Convolutional Neural Networks (CNN) have exhibited superior performance\nin many visual recognition tasks including image classification, object\ndetection, and scene label- ing, due to their large learning capacity and\nresistance to overfit. For the image classification task, most of the current\ndeep CNN- based approaches take the whole size-normalized image as input and\nhave achieved quite promising results. Compared with the previously dominating\napproaches based on feature extraction, pooling, and classification, the deep\nCNN-based approaches mainly rely on the learning capability of deep CNN to\nachieve superior results: the burden of minimizing intra-class variation while\nmaximizing inter-class difference is entirely dependent on the implicit feature\nlearning component of deep CNN; we rely upon the implicitly learned filters and\npooling component to select the discriminative regions, which correspond to the\nactivated neurons. However, if the irrelevant regions constitute a large\nportion of the image of interest, the classification performance of the deep\nCNN, which takes the whole image as input, can be heavily affected. To solve\nthis issue, we propose a novel latent CNN framework, which treats the most\ndiscriminate region as a latent variable. We can jointly learn the global CNN\nwith the latent CNN to avoid the aforementioned big irrelevant region issue,\nand our experimental results show the evident advantage of the proposed latent\nCNN over traditional deep CNN: latent CNN outperforms the state-of-the-art\nperformance of deep CNN on standard benchmark datasets including the CIFAR-10,\nCIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset.\n", "rewritten_text": "Deep convolutional neural networks (CNNs) have demonstrated superior performance in numerous visual recognition tasks, including image classification, object detection, and scene labeling.  This success stems from their high learning capacity and resistance to overfitting.  In image classification, most current deep CNN-based approaches utilize size-normalized images as input, achieving promising results.  Compared to previous dominant approaches relying on handcrafted feature extraction, pooling, and classification, deep CNN-based methods leverage the network's inherent learning capabilities.  The burden of minimizing intra-class variation while maximizing inter-class difference is entirely entrusted to the CNN's implicit feature learning.  Discriminative regions are selected through implicitly learned filters and pooling, corresponding to activated neurons.  However, if irrelevant regions constitute a significant portion of the image, the performance of a CNN processing the entire image can be severely impacted.  To address this, we propose a novel latent CNN framework that treats the most discriminative region as a latent variable.  Jointly learning this latent CNN with a global CNN mitigates the problem of irrelevant regions.  Our experimental results demonstrate the significant advantage of the proposed latent CNN over traditional deep CNNs, achieving state-of-the-art performance on standard benchmark datasets including CIFAR-10, CIFAR-100, MNIST, and PASCAL VOC 2007 Classification.\n"}, "2301.0734": {"original_text": "  Semi-Supervised Semantic Segmentation aims at training the segmentation model\nwith limited labeled data and a large amount of unlabeled data. To effectively\nleverage the unlabeled data, pseudo labeling, along with the teacher-student\nframework, is widely adopted in semi-supervised semantic segmentation. Though\nproved to be effective, this paradigm suffers from incorrect pseudo labels\nwhich inevitably exist and are taken as auxiliary training data. To alleviate\nthe negative impact of incorrect pseudo labels, we delve into the current\nSemi-Supervised Semantic Segmentation frameworks. We argue that the unlabeled\ndata with pseudo labels can facilitate the learning of representative features\nin the feature extractor, but it is unreliable to supervise the mask predictor.\nMotivated by this consideration, we propose a novel framework, Gentle Teaching\nAssistant (GTA-Seg) to disentangle the effects of pseudo labels on feature\nextractor and mask predictor of the student model. Specifically, in addition to\nthe original teacher-student framework, our method introduces a teaching\nassistant network which directly learns from pseudo labels generated by the\nteacher network. The gentle teaching assistant (GTA) is coined gentle since it\nonly transfers the beneficial feature representation knowledge in the feature\nextractor to the student model in an Exponential Moving Average (EMA) manner,\nprotecting the student model from the negative influences caused by unreliable\npseudo labels in the mask predictor. The student model is also supervised by\nreliable labeled data to train an accurate mask predictor, further facilitating\nfeature representation. Extensive experiment results on benchmark datasets\nvalidate that our method shows competitive performance against previous\nmethods. Code is available at https://github.com/Jin-Ying/GTA-Seg.\n", "rewritten_text": "Semi-supervised semantic segmentation trains segmentation models using limited labeled and abundant unlabeled data.  Pseudo-labeling, often within a teacher-student framework, is widely used to leverage unlabeled data. However, this approach suffers from the inevitable presence of incorrect pseudo-labels, which are treated as training data.  To mitigate this issue, we analyze existing semi-supervised semantic segmentation frameworks. We hypothesize that while pseudo-labeled data can improve feature extraction, it is unreliable for directly supervising mask prediction.\n\nTo address this, we propose Gentle Teaching Assistant (GTA-Seg), a novel framework that disentangles the effects of pseudo-labels on the student model's feature extractor and mask predictor.  In addition to the standard teacher-student paradigm, GTA-Seg introduces a teaching assistant network that directly learns from teacher-generated pseudo-labels.  This \"gentle\" assistant uses an exponential moving average (EMA) to transfer only beneficial feature representations to the student's feature extractor, protecting it from the negative influence of unreliable pseudo-labels in the mask predictor.  The student model is also supervised by reliable labeled data to ensure accurate mask prediction, further enhancing feature representation learning.\n\nExtensive experiments on benchmark datasets demonstrate that GTA-Seg achieves competitive performance.  The code is available at https://github.com/Jin-Ying/GTA-Seg.\n"}, "2108.03886": {"original_text": "  A meme is an part of media created to share an opinion or emotion across the\ninternet. Due to its popularity, memes have become the new forms of\ncommunication on social media. However, due to its nature, they are being used\nin harmful ways such as trolling and cyberbullying progressively. Various data\nmodelling methods create different possibilities in feature extraction and\nturning them into beneficial information. The variety of modalities included in\ndata plays a significant part in predicting the results. We try to explore the\nsignificance of visual features of images in classifying memes. Memes are a\nblend of both image and text, where the text is embedded into the image. We try\nto incorporate the memes as troll and non-trolling memes based on the images\nand the text on them. However, the images are to be analysed and combined with\nthe text to increase performance. Our work illustrates different textual\nanalysis methods and contrasting multimodal methods ranging from simple merging\nto cross attention to utilising both worlds' - best visual and textual\nfeatures. The fine-tuned cross-lingual language model, XLM, performed the best\nin textual analysis, and the multimodal transformer performs the best in\nmultimodal analysis.\n", "rewritten_text": "Memes are media created to share opinions or emotions online.  Their popularity has established them as a significant form of social media communication. However, this very nature also facilitates harmful uses, such as escalating trolling and cyberbullying.  Various data modeling methods offer diverse approaches to feature extraction and the transformation of this data into beneficial information.  The modality of the data significantly influences predictive accuracy.  This research explores the importance of visual features in meme classification.  Because memes typically combine images and embedded text, our approach incorporates both to classify memes as trolling or non-trolling.  Image and text analysis are combined to enhance performance.  We investigate various textual analysis methods and multimodal approaches, ranging from simple concatenation to cross-attention mechanisms that leverage the best visual and textual features.  The XLM (cross-lingual language model) achieved the best performance in textual analysis, while a multimodal transformer excelled in multimodal analysis.\n"}, "2305.03973": {"original_text": "  Implicit Discourse Relation Recognition (IDRR) is a sophisticated and\nchallenging task to recognize the discourse relations between the arguments\nwith the absence of discourse connectives. The sense labels for each discourse\nrelation follow a hierarchical classification scheme in the annotation process\n(Prasad et al., 2008), forming a hierarchy structure. Most existing works do\nnot well incorporate the hierarchy structure but focus on the syntax features\nand the prior knowledge of connectives in the manner of pure text\nclassification. We argue that it is more effective to predict the paths inside\nthe hierarchical tree (e.g., \"Comparison -> Contrast -> however\") rather than\nflat labels (e.g., Contrast) or connectives (e.g., however). We propose a\nprompt-based path prediction method to utilize the interactive information and\nintrinsic senses among the hierarchy in IDRR. This is the first work that\ninjects such structure information into pre-trained language models via prompt\ntuning, and the performance of our solution shows significant and consistent\nimprovement against competitive baselines.\n", "rewritten_text": "Implicit Discourse Relation Recognition (IDRR) is a challenging task: identifying discourse relations between arguments in the absence of explicit connectives.  The annotation process employs a hierarchical classification scheme for discourse relation sense labels (Prasad et al., 2008), creating a hierarchical structure.  Most existing methods neglect this hierarchy, focusing instead on syntactic features and connective prior knowledge, treating the task as pure text classification.  We argue that predicting paths within this hierarchical tree (e.g., \"Comparison -> Contrast -> however\") is more effective than predicting flat labels (e.g., \"Contrast\") or connectives (e.g., \"however\").  We propose a prompt-based path prediction method to leverage the interactive information and inherent semantic relationships within the hierarchy for IDRR. This is the first work to integrate this structural information into pre-trained language models using prompt tuning, achieving significant and consistent performance improvements over competing baselines.\n"}, "1908.06665": {"original_text": "  Recently, significant progresses have been made in object detection on common\nbenchmarks (i.e., Pascal VOC). However, object detection in real world is still\nchallenging due to the serious data imbalance. Images in real world are\ndominated by easy samples like the wide range of background and some easily\nrecognizable objects, for example. Although two-stage detectors like Faster\nR-CNN achieved big successes in object detection due to the strategy of\nextracting region proposals by region proposal network, they show their poor\nadaption in real-world object detection as a result of without considering\nmining hard samples during extracting region proposals. To address this issue,\nwe propose a Cascade framework of Region Proposal Networks, referred to as\nC-RPNs. The essence of C-RPNs is adopting multiple stages to mine hard samples\nwhile extracting region proposals and learn stronger classifiers. Meanwhile, a\nfeature chain and a score chain are proposed to help learning more\ndiscriminative representations for proposals. Moreover, a loss function of\ncascade stages is designed to train cascade classifiers through\nbackpropagation. Our proposed method has been evaluated on Pascal VOC and\nseveral challenging datasets like BSBDV 2017, CityPersons, etc. Our method\nachieves competitive results compared with the current state-of-the-arts and\nall-sided improvements in error analysis, validating its efficacy for detection\nin real world.\n", "rewritten_text": "Recent advancements in object detection have yielded significant improvements on common benchmarks like Pascal VOC.  However, real-world object detection remains challenging due to severe data imbalance. Real-world images are dominated by easy samples, such as extensive background and easily recognizable objects. While two-stage detectors such as Faster R-CNN have achieved considerable success through region proposal networks (RPNs), their performance suffers in real-world scenarios because they neglect hard sample mining during proposal extraction.  To address this, we propose a Cascade Region Proposal Network (C-RPN) framework.  C-RPNs employ multiple stages to progressively mine hard samples while extracting region proposals and learning more robust classifiers.  A feature chain and a score chain are introduced to learn more discriminative proposal representations.  Furthermore, a cascaded loss function enables training the cascade classifiers via backpropagation.  Our method was evaluated on Pascal VOC and several challenging datasets, including BSBDV 2017 and CityPersons, achieving competitive results compared to state-of-the-art methods and demonstrating all-around improvements in error analysis, thus validating its effectiveness for real-world object detection.\n"}, "2001.04388": {"original_text": "  The efficient fusion of depth maps is a key part of most state-of-the-art 3D\nreconstruction methods. Besides requiring high accuracy, these depth fusion\nmethods need to be scalable and real-time capable. To this end, we present a\nnovel real-time capable machine learning-based method for depth map fusion.\nSimilar to the seminal depth map fusion approach by Curless and Levoy, we only\nupdate a local group of voxels to ensure real-time capability. Instead of a\nsimple linear fusion of depth information, we propose a neural network that\npredicts non-linear updates to better account for typical fusion errors. Our\nnetwork is composed of a 2D depth routing network and a 3D depth fusion network\nwhich efficiently handle sensor-specific noise and outliers. This is especially\nuseful for surface edges and thin objects for which the original approach\nsuffers from thickening artifacts. Our method outperforms the traditional\nfusion approach and related learned approaches on both synthetic and real data.\nWe demonstrate the performance of our method in reconstructing fine geometric\ndetails from noise and outlier contaminated data on various scenes.\n", "rewritten_text": "Efficient depth map fusion is crucial for state-of-the-art 3D reconstruction.  These methods must be highly accurate, scalable, and real-time capable.  We present a novel, real-time, machine-learning-based approach to depth map fusion.  Like the seminal Curless and Levoy method, we update only a local voxel group to maintain real-time performance. However, instead of simple linear fusion, we employ a neural network to predict non-linear updates, thereby mitigating common fusion errors.  Our network comprises a 2D depth routing network and a 3D depth fusion network, which effectively handle sensor-specific noise and outliers. This is particularly beneficial for surface edges and thin objects, where traditional methods often produce thickening artifacts.  Our method surpasses both traditional and other learned approaches on synthetic and real datasets, demonstrating superior reconstruction of fine geometric details from noisy and outlier-contaminated data across various scenes.\n"}, "1810.08768": {"original_text": "  Motion estimation (ME) and motion compensation (MC) have been widely used for\nclassical video frame interpolation systems over the past decades. Recently, a\nnumber of data-driven frame interpolation methods based on convolutional neural\nnetworks have been proposed. However, existing learning based methods typically\nestimate either flow or compensation kernels, thereby limiting performance on\nboth computational efficiency and interpolation accuracy. In this work, we\npropose a motion estimation and compensation driven neural network for video\nframe interpolation. A novel adaptive warping layer is developed to integrate\nboth optical flow and interpolation kernels to synthesize target frame pixels.\nThis layer is fully differentiable such that both the flow and kernel\nestimation networks can be optimized jointly. The proposed model benefits from\nthe advantages of motion estimation and compensation methods without using\nhand-crafted features. Compared to existing methods, our approach is\ncomputationally efficient and able to generate more visually appealing results.\nFurthermore, the proposed MEMC-Net can be seamlessly adapted to several video\nenhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive\nquantitative and qualitative evaluations demonstrate that the proposed method\nperforms favorably against the state-of-the-art video frame interpolation and\nenhancement algorithms on a wide range of datasets.\n", "rewritten_text": "Classical video frame interpolation has relied heavily on motion estimation (ME) and motion compensation (MC) for decades.  Recently, data-driven methods based on convolutional neural networks (CNNs) have emerged. However, these learning-based approaches typically estimate either optical flow or compensation kernels separately, limiting both computational efficiency and interpolation accuracy.  This work introduces MEMC-Net, a neural network that integrates ME and MC for video frame interpolation.  A novel, fully differentiable adaptive warping layer combines optical flow and interpolation kernels to synthesize target frame pixels, enabling joint optimization of both flow and kernel estimation networks.  This approach leverages the strengths of ME and MC without relying on handcrafted features.  Compared to existing methods, MEMC-Net achieves superior computational efficiency and generates visually more appealing results.  Moreover, it readily adapts to various video enhancement tasks, including super-resolution, denoising, and deblocking.  Extensive quantitative and qualitative evaluations demonstrate state-of-the-art performance across a range of datasets.\n"}, "2303.12793": {"original_text": "  This work focuses on sign language retrieval-a recently proposed task for\nsign language understanding. Sign language retrieval consists of two sub-tasks:\ntext-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval.\nDifferent from traditional video-text retrieval, sign language videos, not only\ncontain visual signals but also carry abundant semantic meanings by themselves\ndue to the fact that sign languages are also natural languages. Considering\nthis character, we formulate sign language retrieval as a cross-lingual\nretrieval problem as well as a video-text retrieval task. Concretely, we take\ninto account the linguistic properties of both sign languages and natural\nlanguages, and simultaneously identify the fine-grained cross-lingual (i.e.,\nsign-to-word) mappings while contrasting the texts and the sign videos in a\njoint embedding space. This process is termed as cross-lingual contrastive\nlearning. Another challenge is raised by the data scarcity issue-sign language\ndatasets are orders of magnitude smaller in scale than that of speech\nrecognition. We alleviate this issue by adopting a domain-agnostic sign encoder\npre-trained on large-scale sign videos into the target domain via\npseudo-labeling. Our framework, termed as domain-aware sign language retrieval\nvia Cross-lingual Contrastive learning or CiCo for short, outperforms the\npioneering method by large margins on various datasets, e.g., +22.4 T2V and\n+28.0 V2T R@1 improvements on How2Sign dataset, and +13.7 T2V and +17.1 V2T R@1\nimprovements on PHOENIX-2014T dataset. Code and models are available at:\nhttps://github.com/FangyunWei/SLRT.\n", "rewritten_text": "This work addresses sign language retrieval, a newly proposed task in sign language understanding.  Sign language retrieval comprises two sub-tasks: text-to-sign-video (T2V) and sign-video-to-text (V2T) retrieval. Unlike traditional video-text retrieval, sign language videos convey rich semantic meaning intrinsically, as sign languages are natural languages themselves.  Therefore, we formulate sign language retrieval as both a cross-lingual retrieval problem and a video-text retrieval task.  Specifically, we leverage the linguistic properties of both sign and natural languages, simultaneously identifying fine-grained cross-lingual (sign-to-word) mappings while jointly embedding texts and sign videos. This process employs cross-lingual contrastive learning.\n\nA significant challenge is the scarcity of sign language data, which is considerably smaller than speech recognition datasets. We mitigate this by using a domain-agnostic sign encoder pre-trained on large-scale sign videos and adapting it to the target domain via pseudo-labeling.  Our framework, CiCo (Cross-lingual Contrastive learning for Domain-aware Sign Language Retrieval), significantly outperforms existing methods.  For example, we achieve +22.4% T2V and +28.0% V2T R@1 improvements on the How2Sign dataset, and +13.7% T2V and +17.1% V2T R@1 improvements on the PHOENIX-2014T dataset.  Code and models are available at: https://github.com/FangyunWei/SLRT.\n"}, "2410.01928": {"original_text": "  In the domain of battery research, the processing of high-resolution\nmicroscopy images is a challenging task, as it involves dealing with complex\nimages and requires a prior understanding of the components involved. The\nutilization of deep learning methodologies for image analysis has attracted\nconsiderable interest in recent years, with multiple investigations employing\nsuch techniques for image segmentation and analysis within the realm of battery\nresearch. However, the automated analysis of high-resolution microscopy images\nfor detecting phases and components in composite materials is still an\nunderexplored area. This work proposes a novel workflow for detecting\ncomponents and phase segmentation from raw high resolution transmission\nelectron microscopy (TEM) images using a trained U-Net segmentation model. The\ndeveloped model can expedite the detection of components and phase\nsegmentation, diminishing the temporal and cognitive demands associated with\nscrutinizing an extensive array of TEM images, thereby mitigating the potential\nfor human errors. This approach presents a novel and efficient image analysis\napproach with broad applicability beyond the battery field and holds potential\nfor application in other related domains characterized by phase and composition\ndistribution, such as alloy production.\n", "rewritten_text": "High-resolution microscopy image processing in battery research is challenging due to image complexity and the need for prior component knowledge.  While deep learning has shown promise for image segmentation and analysis in this field, automated phase and component detection in composite materials from high-resolution images remains underexplored.  This work presents a novel workflow for segmenting phases and detecting components in raw high-resolution transmission electron microscopy (TEM) images using a trained U-Net model.  This automated approach accelerates component detection and phase segmentation, reducing the time and cognitive load associated with manual analysis of numerous TEM images and minimizing human error.  Its efficiency and novel approach offer broad applicability beyond battery research, with potential uses in other fields involving phase and compositional distribution, such as alloy production.\n"}, "2103.15467": {"original_text": "  Unsupervised domain adaptation (UDA) becomes more and more popular in\ntackling real-world problems without ground truth of the target domain. Though\ntedious annotation work is not required, UDA unavoidably faces two problems: 1)\nhow to narrow the domain discrepancy to boost the transferring performance; 2)\nhow to improve pseudo annotation producing mechanism for self-supervised\nlearning (SSL). In this paper, we focus on UDA for semantic segmentation task.\nFirstly, we introduce adversarial learning into style gap bridging mechanism to\nkeep the style information from two domains in the similar space. Secondly, to\nkeep the balance of pseudo labels on each category, we propose a\ncategory-adaptive threshold mechanism to choose category-wise pseudo labels for\nSSL. The experiments are conducted using GTA5 as the source domain, Cityscapes\nas the target domain. The results show that our model outperforms the\nstate-of-the-arts with a noticeable gain on cross-domain adaptation tasks.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) is increasingly popular for addressing real-world problems lacking target domain ground truth. While eliminating the need for tedious annotation, UDA faces two key challenges: (1) mitigating domain discrepancy to improve transfer performance, and (2) enhancing pseudo-annotation mechanisms for self-supervised learning (SSL).  This paper focuses on UDA for semantic segmentation.  We introduce adversarial learning into a style gap bridging mechanism to align the style information of the source and target domains.  Furthermore, to balance pseudo-label distribution across categories, we propose a category-adaptive threshold mechanism for selecting category-wise pseudo-labels for SSL.  Experiments using GTA5 as the source domain and Cityscapes as the target domain demonstrate that our model significantly outperforms state-of-the-art methods in cross-domain adaptation tasks.\n"}, "2203.17013": {"original_text": "  Video streams are utilised to guide minimally-invasive surgery and diagnostic\nprocedures in a wide range of procedures, and many computer assisted techniques\nhave been developed to automatically analyse them. These approaches can provide\nadditional information to the surgeon such as lesion detection, instrument\nnavigation, or anatomy 3D shape modeling. However, the necessary image features\nto recognise these patterns are not always reliably detected due to the\npresence of irregular light patterns such as specular highlight reflections. In\nthis paper, we aim at removing specular highlights from endoscopic videos using\nmachine learning. We propose using a temporal generative adversarial network\n(GAN) to inpaint the hidden anatomy under specularities, inferring its\nappearance spatially and from neighbouring frames where they are not present in\nthe same location. This is achieved using in-vivo data of gastric endoscopy\n(Hyper-Kvasir) in a fully unsupervised manner that relies on automatic\ndetection of specular highlights. System evaluations show significant\nimprovements to traditional methods through direct comparison as well as other\nmachine learning techniques through an ablation study that depicts the\nimportance of the network's temporal and transfer learning components. The\ngeneralizability of our system to different surgical setups and procedures was\nalso evaluated qualitatively on in-vivo data of gastric endoscopy and ex-vivo\nporcine data (SERV-CT, SCARED). We also assess the effect of our method in\ncomputer vision tasks that underpin 3D reconstruction and camera motion\nestimation, namely stereo disparity, optical flow, and sparse point feature\nmatching. These are evaluated quantitatively and qualitatively and results show\na positive effect of specular highlight inpainting on these tasks in a novel\ncomprehensive analysis.\n", "rewritten_text": "Minimally invasive surgery and diagnostic procedures increasingly utilize video streams, prompting the development of numerous computer-assisted techniques for automated analysis.  These techniques offer surgeons valuable information, including lesion detection, instrument navigation, and 3D anatomical modeling. However, irregular lighting, particularly specular reflections, often hinders reliable feature detection.  This paper presents a machine learning approach to remove specular highlights from endoscopic videos.  We propose a temporal generative adversarial network (GAN) to inpaint the obscured anatomy under specularities, inferring appearance both spatially and from temporally adjacent frames lacking such reflections.  This unsupervised method uses in vivo gastric endoscopy data (Hyper-Kvasir) and relies on automatic specular highlight detection.  System evaluations, including direct comparisons with traditional methods and an ablation study highlighting the importance of temporal and transfer learning components, demonstrate significant improvements.  Furthermore, we qualitatively evaluated the system's generalizability using in vivo gastric endoscopy and ex vivo porcine data (SERV-CT, SCARED).  Finally, we quantitatively and qualitatively assessed the impact of our specular highlight inpainting on computer vision tasks crucial for 3D reconstruction and camera motion estimation\u2014specifically stereo disparity, optical flow, and sparse point feature matching\u2014revealing a positive effect in a novel, comprehensive analysis.\n"}, "2010.02808": {"original_text": "  We propose a method to learn image representations from uncurated videos. We\ncombine a supervised loss from off-the-shelf object detectors and\nself-supervised losses which naturally arise from the video-shot-frame-object\nhierarchy present in each video. We report competitive results on 19 transfer\nlearning tasks of the Visual Task Adaptation Benchmark (VTAB), and on 8\nout-of-distribution-generalization tasks, and discuss the benefits and\nshortcomings of the proposed approach. In particular, it improves over the\nbaseline on all 18/19 few-shot learning tasks and 8/8 out-of-distribution\ngeneralization tasks. Finally, we perform several ablation studies and analyze\nthe impact of the pretrained object detector on the performance across this\nsuite of tasks.\n", "rewritten_text": "We propose a method for learning image representations from uncurated videos.  This method combines a supervised loss derived from off-the-shelf object detectors with self-supervised losses inherent in the video-shot-frame-object hierarchy.  We achieve competitive results on 19 transfer learning tasks within the Visual Task Adaptation Benchmark (VTAB) and on 8 out-of-distribution generalization tasks.  Our approach surpasses the baseline on all 18 of 19 few-shot learning tasks and all 8 out-of-distribution generalization tasks.  Furthermore, we conduct ablation studies to analyze the pretrained object detector's impact on performance across this task suite.  We conclude by discussing the benefits and limitations of our proposed method.\n"}, "1903.12529": {"original_text": "  While deep neural networks (DNN) based single image super-resolution (SISR)\nmethods are rapidly gaining popularity, they are mainly designed for the\nwidely-used bicubic degradation, and there still remains the fundamental\nchallenge for them to super-resolve low-resolution (LR) image with arbitrary\nblur kernels. In the meanwhile, plug-and-play image restoration has been\nrecognized with high flexibility due to its modular structure for easy plug-in\nof denoiser priors. In this paper, we propose a principled formulation and\nframework by extending bicubic degradation based deep SISR with the help of\nplug-and-play framework to handle LR images with arbitrary blur kernels.\nSpecifically, we design a new SISR degradation model so as to take advantage of\nexisting blind deblurring methods for blur kernel estimation. To optimize the\nnew degradation induced energy function, we then derive a plug-and-play\nalgorithm via variable splitting technique, which allows us to plug any\nsuper-resolver prior rather than the denoiser prior as a modular part.\nQuantitative and qualitative evaluations on synthetic and real LR images\ndemonstrate that the proposed deep plug-and-play super-resolution framework is\nflexible and effective to deal with blurry LR images.\n", "rewritten_text": "Deep neural network (DNN)-based single image super-resolution (SISR) methods are rapidly gaining popularity, but they primarily address bicubic degradation.  A fundamental challenge remains: super-resolving low-resolution (LR) images with arbitrary blur kernels.  Plug-and-play image restoration offers high flexibility due to its modular structure, enabling easy integration of denoiser priors.  This paper proposes a novel framework that extends bicubic-degradation-based deep SISR using a plug-and-play approach to handle LR images with arbitrary blur kernels.  We introduce a new SISR degradation model leveraging existing blind deblurring methods for kernel estimation.  A plug-and-play algorithm, derived via variable splitting, optimizes the resulting energy function.  Crucially, this algorithm accommodates any super-resolution prior\u2014not just denoisers\u2014as a modular component.  Quantitative and qualitative evaluations on synthetic and real LR images demonstrate the framework's effectiveness and flexibility in handling blurry LR images.\n"}, "2407.05092": {"original_text": "  Computational and human perception are often considered separate approaches\nfor studying sound changes over time; few works have touched on the\nintersection of both. To fill this research gap, we provide a pioneering review\ncontrasting computational with human perception from the perspectives of\nmethods and tasks. Overall, computational approaches rely on computer-driven\nmodels to perceive historical sound changes on etymological datasets, while\nhuman approaches use listener-driven models to perceive ongoing sound changes\non recording corpora. Despite their differences, both approaches complement\neach other on phonetic and acoustic levels, showing the potential to achieve a\nmore comprehensive perception of sound change. Moreover, we call for a\ncomparative study on the datasets used by both approaches to investigate the\ninfluence of historical sound changes on ongoing changes. Lastly, we discuss\nthe applications of sound change in computational linguistics, and point out\nthat perceiving sound change alone is insufficient, as many processes of\nlanguage change are complex, with entangled changes at syntactic, semantic, and\nphonetic levels.\n", "rewritten_text": "Computational and human approaches to studying sound change are often treated as distinct.  However, few studies have explored their intersection.  This review bridges that gap by contrasting computational and human perception of sound change, focusing on methodologies and tasks.  Computational approaches typically employ computer-driven models analyzing historical sound changes in etymological datasets, while human approaches utilize listener-driven models to study ongoing changes in recorded corpora.  Despite their differences, these approaches are complementary at the phonetic and acoustic levels, offering the potential for a more holistic understanding of sound change.  Further research should comparatively analyze the datasets used by each approach to investigate the influence of historical changes on ongoing ones.  Finally, while applications of sound change in computational linguistics are promising, it's crucial to acknowledge that sound change alone is insufficient to explain the complexities of language evolution, which often involves entangled syntactic, semantic, and phonetic changes.\n"}, "1812.03621": {"original_text": "  The majority of Multi-Object Tracking (MOT) algorithms based on the\ntracking-by-detection scheme do not use higher order dependencies among objects\nor tracklets, which makes them less effective in handling complex scenarios. In\nthis work, we present a new near-online MOT algorithm based on non-uniform\nhypergraph, which can model different degrees of dependencies among tracklets\nin a unified objective. The nodes in the hypergraph correspond to the tracklets\nand the hyperedges with different degrees encode various kinds of dependencies\namong them. Specifically, instead of setting the weights of hyperedges with\ndifferent degrees empirically, they are learned automatically using the\nstructural support vector machine algorithm (SSVM). Several experiments are\ncarried out on various challenging datasets (i.e., PETS09, ParkingLot sequence,\nSubwayFace, and MOT16 benchmark), to demonstrate that our method achieves\nfavorable performance against the state-of-the-art MOT methods.\n", "rewritten_text": "Most tracking-by-detection Multi-Object Tracking (MOT) algorithms neglect higher-order dependencies between objects or tracklets, limiting their effectiveness in complex scenarios.  This work introduces a novel near-online MOT algorithm based on a non-uniform hypergraph, which models varying degrees of tracklet dependency within a unified objective function.  Hypergraph nodes represent tracklets, and hyperedges of varying cardinality encode diverse dependencies.  Instead of empirically assigning hyperedge weights, we learn them automatically using a Structural Support Vector Machine (SSVM).  Experiments on challenging datasets (PETS09, ParkingLot sequence, SubwayFace, and MOT16) demonstrate that our method achieves state-of-the-art performance.\n"}, "2211.08358": {"original_text": "  Few-shot classification has made great strides due to foundation models that,\nthrough priming and prompting, are highly effective few-shot learners. However,\nthis approach has high variance both across different sets of few shots (data\nselection) and across different finetuning runs (run variability). This is\nproblematic not only because it impedes the fair comparison of different\napproaches, but especially because it makes few-shot learning too unreliable\nfor many real-world applications. To alleviate these issues, we make two\ncontributions for more stable and effective few-shot learning: First, we\npropose novel ensembling methods and show that they substantially reduce run\nvariability. Second, we introduce a new active learning (AL) criterion for data\nselection and present the first AL-based approach specifically tailored towards\nprompt-based learning. In our experiments, we show that our combined method,\nMEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),\nimproves overall performance of prompt-based finetuning by 2.3 points on five\ndiverse tasks. We publicly share our code and data splits in\nhttps://github.com/akoksal/MEAL.\n", "rewritten_text": "Foundation models, leveraging priming and prompting, have significantly advanced few-shot classification.  However, this approach suffers from high variance, stemming from both data selection and finetuning run variability. This hinders fair comparison between methods and renders few-shot learning unreliable for many real-world applications.  To address this, we present a two-pronged approach for more stable and effective few-shot learning. First, we propose novel ensembling methods that substantially reduce run variability. Second, we introduce a novel active learning (AL) criterion specifically designed for prompt-based learning, representing the first such AL-based approach in this context.  Our combined method, MEAL (Multiprompt finetuning and prediction Ensembling with Active Learning), improves the overall performance of prompt-based finetuning by 2.3 points across five diverse tasks.  Code and data splits are publicly available at https://github.com/akoksal/MEAL.\n"}, "1909.09934": {"original_text": "  We propose methods to train convolutional neural networks (CNNs) with both\nbinarized weights and activations, leading to quantized models that are\nspecifically friendly to mobile devices with limited power capacity and\ncomputation resources. Previous works on quantizing CNNs often seek to\napproximate the floating-point information using a set of discrete values,\nwhich we call value approximation, typically assuming the same architecture as\nthe full-precision networks. Here we take a novel \"structure approximation\"\nview of quantization -- it is very likely that different architectures designed\nfor low-bit networks may be better for achieving good performance. In\nparticular, we propose a \"network decomposition\" strategy, termed Group-Net, in\nwhich we divide the network into groups. Thus, each full-precision group can be\neffectively reconstructed by aggregating a set of homogeneous binary branches.\nIn addition, we learn effective connections among groups to improve the\nrepresentation capability. Moreover, the proposed Group-Net shows strong\ngeneralization to other tasks. For instance, we extend Group-Net for accurate\nsemantic segmentation by embedding rich context into the binary structure.\nFurthermore, for the first time, we apply binary neural networks to object\ndetection. Experiments on both classification, semantic segmentation and object\ndetection tasks demonstrate the superior performance of the proposed methods\nover various quantized networks in the literature. Our methods outperform the\nprevious best binary neural networks in terms of accuracy and computation\nefficiency.\n", "rewritten_text": "We propose novel methods for training convolutional neural networks (CNNs) with binarized weights and activations, resulting in quantized models ideally suited for resource-constrained mobile devices.  Existing CNN quantization techniques typically approximate floating-point values with discrete representations (value approximation), often retaining the original network architecture.  We introduce a novel \"structure approximation\" approach, hypothesizing that architectures specifically designed for low-bit networks can achieve superior performance.  Specifically, we propose Group-Net, a network decomposition strategy that divides the network into groups. Each full-precision group is reconstructed by aggregating homogeneous binary branches.  Furthermore, we learn effective inter-group connections to enhance representational capacity.  Group-Net demonstrates strong generalization capabilities; we extend it to accurate semantic segmentation by incorporating rich contextual information into the binary structure and, for the first time, apply it to object detection.  Experiments on image classification, semantic segmentation, and object detection tasks demonstrate that Group-Net significantly outperforms existing quantized networks in both accuracy and computational efficiency, surpassing the state-of-the-art in binary neural networks.\n"}, "2407.04024": {"original_text": "  Deep unfolding methods and transformer architecture have recently shown\npromising results in hyperspectral image (HSI) reconstruction. However, there\nstill exist two issues: (1) in the data subproblem, most methods represents the\nstepsize utilizing a learnable parameter. Nevertheless, for different spectral\nchannel, error between features and ground truth is unequal. (2) Transformer\nstruggles to balance receptive field size with pixel-wise detail information.\nTo overcome the aforementioned drawbacks, We proposed an adaptive step-size\nperception unfolding network (ASPUN), a deep unfolding network based on FISTA\nalgorithm, which uses an adaptive step-size perception module to estimate the\nupdate step-size of each spectral channel. In addition, we design a Non-local\nHybrid Attention Transformer(NHAT) module for fully leveraging the receptive\nfield advantage of transformer. By plugging the NLHA into the Non-local\nInformation Aggregation (NLIA) module, the unfolding network can achieve better\nreconstruction results. Experimental results show that our ASPUN is superior to\nthe existing SOTA algorithms and achieves the best performance.\n", "rewritten_text": "Recent advancements in hyperspectral image (HSI) reconstruction have demonstrated the promise of deep unfolding methods and transformer architectures.  However, two key limitations remain: (1) most methods employ a learnable parameter to represent the step size in the data subproblem, neglecting the unequal error between features and ground truth across different spectral channels; and (2) transformers struggle to balance receptive field size with the preservation of pixel-wise detail.\n\nTo address these limitations, we propose the Adaptive Step-size Perception Unfolding Network (ASPUN), a deep unfolding network based on the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA).  ASPUN incorporates an adaptive step-size perception module to estimate the update step size for each spectral channel individually.  Furthermore, we introduce a Non-local Hybrid Attention Transformer (NHAT) module to fully exploit the advantages of the transformer's receptive field.  Integrating NHAT into the Non-local Information Aggregation (NLIA) module enhances the unfolding network's reconstruction capabilities.  Experimental results demonstrate that ASPUN outperforms state-of-the-art (SOTA) algorithms, achieving the best performance.\n"}, "1810.10519": {"original_text": "  With the increasing use of social networks and mobile devices, the number of\nvideos posted on the Internet is growing exponentially. Among the inappropriate\ncontents published on the Internet, pornography is one of the most worrying as\nit can be accessed by teens and children. Two spatiotemporal CNNs, VGG-C3D CNN\nand ResNet R(2+1)D CNN, were assessed for pornography detection in videos in\nthe present study. Experimental results using the Pornography-800 dataset\nshowed that these spatiotemporal CNNs performed better than some\nstate-of-the-art methods based on bag of visual words and are competitive with\nother CNN-based approaches, reaching accuracy of 95.1%.\n", "rewritten_text": "The proliferation of social media and mobile devices has led to an exponential increase in online videos.  A significant concern is the readily accessible pornography, posing a risk to children and teenagers. This study evaluated two spatiotemporal Convolutional Neural Networks (CNNs)\u2014VGG-C3D and ResNet R(2+1)D\u2014for video pornography detection.  Experiments using the Pornography-800 dataset demonstrated that these CNNs outperformed several state-of-the-art bag-of-visual-words methods and achieved competitive accuracy (95.1%) compared to other CNN-based approaches.\n"}, "2406.10209": {"original_text": "  Large language models can memorize and repeat their training data, causing\nprivacy and copyright risks. To mitigate memorization, we introduce a subtle\nmodification to the next-token training objective that we call the goldfish\nloss. During training, randomly sampled subsets of tokens are excluded from the\nloss computation. These dropped tokens are not memorized by the model, which\nprevents verbatim reproduction of a complete chain of tokens from the training\nset. We run extensive experiments training billion-scale Llama-2 models, both\npre-trained and trained from scratch, and demonstrate significant reductions in\nextractable memorization with little to no impact on downstream benchmarks.\n", "rewritten_text": "Large language models (LLMs) can memorize and reproduce their training data, posing significant privacy and copyright risks.  To mitigate this memorization, we introduce a novel training objective modification called the \"goldfish loss.\"  This approach randomly excludes subsets of tokens from the loss calculation during training.  The model, consequently, does not memorize these dropped tokens, preventing the verbatim reproduction of complete training data sequences.  Extensive experiments with billion-scale Llama-2 models, both pre-trained and trained from scratch, demonstrate substantial reductions in extractable memorization with negligible impact on downstream task performance.\n"}, "2110.14398": {"original_text": "  To consider Hawrami and Zaza (Zazaki) standalone languages or dialects of a\nlanguage have been discussed and debated for a while among linguists active in\nstudying Iranian languages. The question of whether those languages/dialects\nbelong to the Kurdish language or if they are independent descendants of\nIranian languages was answered by MacKenzie (1961). However, a majority of\npeople who speak the dialects are against that answer. Their disapproval mainly\nseems to be based on the sociological, cultural, and historical relationship\namong the speakers of the dialects. While the case of Hawrami and Zaza has\nremained unexplored and under-examined, an almost unanimous agreement exists\nabout the classification of Kurmanji and Sorani as Kurdish dialects. The\nrelated studies to address the mentioned cases are primarily qualitative.\nHowever, computational linguistics could approach the question from a\nquantitative perspective. In this research, we look into three questions from a\nlinguistic distance point of view. First, how similar or dissimilar Hawrami and\nZaza are, considering no common geographical coexistence between the two.\nSecond, what about Kurmanji and Sorani that have geographical overlap. Finally,\nwhat is the distance among all these dialects, pair by pair? We base our\ncomputation on phonetic presentations of these dialects (languages), and we\ncalculate various linguistic distances among the pairs. We analyze the data and\ndiscuss the results to conclude.\n", "rewritten_text": "The classification of Hawrami and Zaza (Zazaki) as independent languages or dialects of a larger language family has been a subject of ongoing debate among linguists specializing in Iranian languages.  MacKenzie (1961) addressed the question of their affiliation with Kurdish or their status as independent descendants of Iranian languages; however, this conclusion is largely rejected by the majority of Hawrami and Zaza speakers, primarily due to sociological, cultural, and historical factors. While Hawrami and Zaza remain relatively understudied, Kurmanji and Sorani are almost universally classified as Kurdish dialects.  Existing research on these classifications is predominantly qualitative.  This study, however, employs a quantitative computational linguistics approach to investigate three key questions regarding linguistic distance:  First, the degree of similarity or dissimilarity between Hawrami and Zaza, despite their geographical separation. Second, the linguistic distance between Kurmanji and Sorani, given their geographical overlap.  Third, the pairwise linguistic distances among all four dialects (Hawrami, Zaza, Kurmanji, and Sorani).  Our analysis utilizes phonetic data to calculate various linguistic distances between each pair, and the results are subsequently analyzed and discussed.\n"}, "2401.14718": {"original_text": "  Future Frame Synthesis (FFS) aims to enable models to generate sequences of\nfuture frames based on existing content. This survey comprehensively reviews\nhistorical and contemporary works in FFS, including widely used datasets and\nalgorithms. It scrutinizes the challenges and the evolving landscape of FFS\nwithin computer vision, with a focus on the transition from deterministic to\ngenerative synthesis methodologies. Our taxonomy highlights the significant\nadvancements and shifts in approach, underscoring the growing importance of\ngenerative models in achieving realistic and diverse future frame predictions.\n", "rewritten_text": "This survey comprehensively reviews the history and current state of Future Frame Synthesis (FFS), a field aiming to enable models to generate sequences of future frames from existing content.  We examine widely used datasets and algorithms, scrutinize the challenges facing the field, and analyze its evolution within computer vision, particularly the shift from deterministic to generative synthesis methodologies.  Our proposed taxonomy highlights significant advancements and changing approaches, underscoring the increasing importance of generative models in producing realistic and diverse future frame predictions.\n"}, "1808.0841": {"original_text": "  Training a good deep learning model often requires a lot of annotated data.\nAs a large amount of labeled data is typically difficult to collect and even\nmore difficult to annotate, data augmentation and data generation are widely\nused in the process of training deep neural networks. However, there is no\nclear common understanding on how much labeled data is needed to get\nsatisfactory performance. In this paper, we try to address such a question\nusing vehicle license plate character recognition as an example application. We\napply computer graphic scripts and Generative Adversarial Networks to generate\nand augment a large number of annotated, synthesized license plate images with\nrealistic colors, fonts, and character composition from a small number of real,\nmanually labeled license plate images. Generated and augmented data are mixed\nand used as training data for the license plate recognition network modified\nfrom DenseNet. The experimental results show that the model trained from the\ngenerated mixed training data has good generalization ability, and the proposed\napproach achieves a new state-of-the-art accuracy on Dataset-1 and AOLP, even\nwith a very limited number of original real license plates. In addition, the\naccuracy improvement caused by data generation becomes more significant when\nthe number of labeled images is reduced. Data augmentation also plays a more\nsignificant role when the number of labeled images is increased.\n", "rewritten_text": "Training effective deep learning models often requires extensive annotated data.  However, acquiring and annotating large datasets is challenging.  Consequently, data augmentation and generation are widely employed.  The optimal amount of labeled data for satisfactory performance remains unclear.  This paper addresses this issue using vehicle license plate character recognition as a case study.  We leverage computer graphics scripts and Generative Adversarial Networks (GANs) to generate and augment a large dataset of realistically rendered, annotated license plate images from a small set of manually labeled real images.  This synthetic data, combined with the original data, trains a DenseNet-based license plate recognition network.  Experimental results demonstrate that this approach achieves state-of-the-art accuracy on Dataset-1 and AOLP, even with limited real-world data.  Furthermore, the benefits of data generation are amplified with smaller initial datasets, while data augmentation becomes increasingly important with larger datasets.\n"}, "2401.03183": {"original_text": "  Defeasibility in causal reasoning implies that the causal relationship\nbetween cause and effect can be strengthened or weakened. Namely, the causal\nstrength between cause and effect should increase or decrease with the\nincorporation of strengthening arguments (supporters) or weakening arguments\n(defeaters), respectively. However, existing works ignore defeasibility in\ncausal reasoning and fail to evaluate existing causal strength metrics in\ndefeasible settings. In this work, we present $\\delta$-CAUSAL, the first\nbenchmark dataset for studying defeasibility in causal reasoning.\n$\\delta$-CAUSAL includes around 11K events spanning ten domains, featuring\ndefeasible causality pairs, i.e., cause-effect pairs accompanied by supporters\nand defeaters. We further show current causal strength metrics fail to reflect\nthe change of causal strength with the incorporation of supporters or defeaters\nin $\\delta$-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation\nwith Attention Rating), a metric that measures causal strength based on\ntoken-level causal relationships. CESAR achieves a significant 69.7% relative\nimprovement over existing metrics, increasing from 47.2% to 80.1% in capturing\nthe causal strength change brought by supporters and defeaters. We further\ndemonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and\n10.7 points behind humans in generating supporters and defeaters, emphasizing\nthe challenge posed by $\\delta$-CAUSAL.\n", "rewritten_text": "Defeasibility in causal reasoning means that the causal relationship between cause and effect can be strengthened or weakened.  Specifically, causal strength should increase with supporting arguments and decrease with counterarguments (defeaters).  However, existing work overlooks defeasibility and fails to evaluate causal strength metrics in defeasible contexts.  This work introduces $\\delta$-CAUSAL, the first benchmark dataset for studying defeasible causal reasoning.  $\\delta$-CAUSAL comprises approximately 11,000 events across ten domains, each featuring cause-effect pairs accompanied by supporting and counterarguments.  We demonstrate that current causal strength metrics fail to accurately reflect changes in causal strength when incorporating supporters or defeaters in $\\delta$-CAUSAL.  Therefore, we propose CESAR (Causal Embedding Association with Attention Rating), a metric measuring causal strength based on token-level relationships.  CESAR achieves a significant 69.7% relative improvement over existing metrics, increasing the accuracy of capturing causal strength changes from 47.2% to 80.1%.  Furthermore, we show that even large language models (LLMs) such as GPT-3.5 lag behind human performance by 4.5 and 10.7 percentage points in generating supporters and defeaters, respectively, highlighting the challenge posed by $\\delta$-CAUSAL.\n"}, "1310.5767": {"original_text": "  Salient object detection aims to locate objects that capture human attention\nwithin images. Previous approaches often pose this as a problem of image\ncontrast analysis. In this work, we model an image as a hypergraph that\nutilizes a set of hyperedges to capture the contextual properties of image\npixels or regions. As a result, the problem of salient object detection becomes\none of finding salient vertices and hyperedges in the hypergraph. The main\nadvantage of hypergraph modeling is that it takes into account each pixel's (or\nregion's) affinity with its neighborhood as well as its separation from image\nbackground. Furthermore, we propose an alternative approach based on\ncenter-versus-surround contextual contrast analysis, which performs salient\nobject detection by optimizing a cost-sensitive support vector machine (SVM)\nobjective function. Experimental results on four challenging datasets\ndemonstrate the effectiveness of the proposed approaches against the\nstate-of-the-art approaches to salient object detection.\n", "rewritten_text": "Salient object detection aims to identify image regions that attract human attention.  Previous methods often treated this as an image contrast analysis problem.  This work models an image as a hypergraph, using hyperedges to represent the contextual relationships between image pixels or regions.  Consequently, salient object detection becomes a problem of identifying salient vertices and hyperedges within this hypergraph.  This hypergraph representation effectively captures each pixel's (or region's) affinity with its neighbors and its distinction from the background.  Additionally, we propose a novel center-surround contextual contrast analysis approach, implemented by optimizing a cost-sensitive support vector machine (SVM) objective function.  Experiments on four challenging datasets demonstrate the superiority of our proposed methods over state-of-the-art salient object detection techniques.\n"}, "2111.02394": {"original_text": "  We propose an accurate and efficient scene text detection framework, termed\nFAST (i.e., faster arbitrarily-shaped text detector). Different from recent\nadvanced text detectors that used complicated post-processing and hand-crafted\nnetwork architectures, resulting in low inference speed, FAST has two new\ndesigns. (1) We design a minimalist kernel representation (only has 1-channel\noutput) to model text with arbitrary shape, as well as a GPU-parallel\npost-processing to efficiently assemble text lines with a negligible time\noverhead. (2) We search the network architecture tailored for text detection,\nleading to more powerful features than most networks that are searched for\nimage classification. Benefiting from these two designs, FAST achieves an\nexcellent trade-off between accuracy and efficiency on several challenging\ndatasets, including Total Text, CTW1500, ICDAR 2015, and MSRA-TD500. For\nexample, FAST-T yields 81.6% F-measure at 152 FPS on Total-Text, outperforming\nthe previous fastest method by 1.7 points and 70 FPS in terms of accuracy and\nspeed. With TensorRT optimization, the inference speed can be further\naccelerated to over 600 FPS. Code and models will be released at\nhttps://github.com/czczup/FAST.\n", "rewritten_text": "We propose FAST (Faster Arbitrarily-Shaped Text Detector), an accurate and efficient scene text detection framework.  Unlike recent advanced text detectors, which rely on complex post-processing and handcrafted architectures, resulting in slow inference speeds, FAST incorporates two novel designs: (1) a minimalist, single-channel kernel representation for modeling arbitrarily shaped text, coupled with GPU-parallel post-processing for efficient, low-overhead text line assembly; and (2) a tailored network architecture optimized for text detection, yielding more powerful features than architectures designed for image classification.  These designs enable FAST to achieve an excellent accuracy-efficiency trade-off on challenging datasets including Total-Text, CTW1500, ICDAR 2015, and MSRA-TD500.  For instance, FAST-T achieves an 81.6% F-measure at 152 FPS on Total-Text, surpassing the previous fastest method by 1.7 F-measure points and 70 FPS.  Further acceleration to over 600 FPS is achieved with TensorRT optimization. Code and models are available at https://github.com/czczup/FAST.\n"}, "2403.17856": {"original_text": "  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.\n", "rewritten_text": "English morphology is characterized by lexical-syntactic flexibility, particularly conversion (or zero-derivation).  Conversion involves using a word in a non-prototypical context, forcing it to function as a different part of speech.  Although this process significantly impacts the English lexicon, its representation in language models remains largely unexplored. This paper presents the first study investigating large language models' handling of conversion.  We designed a natural language inference task to assess lexical-syntactic flexibility\u2014specifically, the models' ability to generalize across words used in non-prototypical syntactic categories.  Five language models were evaluated: two proprietary models (GPT-3.5 and GPT-4) and three open-source models (Mistral 7B, Falcon 40B, and Llama 2 70B).  GPT-4 achieved the best performance, followed by GPT-3.5.  However, the open-source models also demonstrated proficiency, with the 7B parameter Mistral exhibiting a performance difference comparable to that of the significantly larger GPT-4 between its baseline natural language inference performance and its performance on the non-prototypical syntactic category task.\n"}, "1712.05277": {"original_text": "  Depth cameras allow to set up reliable solutions for people monitoring and\nbehavior understanding, especially when unstable or poor illumination\nconditions make unusable common RGB sensors. Therefore, we propose a complete\nframework for the estimation of the head and shoulder pose based on depth\nimages only. A head detection and localization module is also included, in\norder to develop a complete end-to-end system. The core element of the\nframework is a Convolutional Neural Network, called POSEidon+, that receives as\ninput three types of images and provides the 3D angles of the pose as output.\nMoreover, a Face-from-Depth component based on a Deterministic Conditional GAN\nmodel is able to hallucinate a face from the corresponding depth image. We\nempirically demonstrate that this positively impacts the system performances.\nWe test the proposed framework on two public datasets, namely Biwi Kinect Head\nPose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by\nthe automotive setup. Experimental results show that our method overcomes\nseveral recent state-of-art works based on both intensity and depth input data,\nrunning in real-time at more than 30 frames per second.\n", "rewritten_text": "Depth cameras enable reliable solutions for people monitoring and behavior understanding, particularly in unstable or poorly lit conditions where standard RGB sensors are unusable.  We propose a complete framework for head and shoulder pose estimation using only depth images.  This framework includes a head detection and localization module, creating a complete end-to-end system.  The core component is POSEidon+, a convolutional neural network (CNN) that takes three types of images as input and outputs 3D pose angles.  Furthermore, a Face-from-Depth component, based on a deterministic conditional generative adversarial network (DCGAN), hallucinates a face from the corresponding depth image, empirically improving system performance.  We evaluated the framework on two public datasets (Biwi Kinect Head Pose and ICT-3DHP) and Pandora, a new, challenging dataset inspired by automotive applications.  Experimental results demonstrate that our real-time method (operating at over 30 frames per second) outperforms several recent state-of-the-art approaches using both intensity and depth data.\n"}, "2404.19287": {"original_text": "  Pretrained vision-language models (VLMs) like CLIP exhibit exceptional\ngeneralization across diverse downstream tasks. While recent studies reveal\ntheir vulnerability to adversarial attacks, research to date has primarily\nfocused on enhancing the robustness of image encoders against image-based\nattacks, with defenses against text-based and multimodal attacks remaining\nlargely unexplored. To this end, this work presents the first comprehensive\nstudy on improving the adversarial robustness of VLMs against attacks targeting\nimage, text, and multimodal inputs. This is achieved by proposing multimodal\ncontrastive adversarial training (MMCoA). Such an approach strengthens the\nrobustness of both image and text encoders by aligning the clean text\nembeddings with adversarial image embeddings, and adversarial text embeddings\nwith clean image embeddings. The robustness of the proposed MMCoA is examined\nagainst existing defense methods over image, text, and multimodal attacks on\nthe CLIP model. Extensive experiments on 15 datasets across two tasks reveal\nthe characteristics of different adversarial defense methods under distinct\ndistribution shifts and dataset complexities across the three attack types.\nThis paves the way for a unified framework of adversarial robustness against\ndifferent modality attacks, opening up new possibilities for securing VLMs\nagainst multimodal attacks. The code is available at\nhttps://github.com/ElleZWQ/MMCoA.git.\n", "rewritten_text": "Pretrained vision-language models (VLMs) such as CLIP demonstrate exceptional generalization across diverse downstream tasks.  However, recent studies highlight their vulnerability to adversarial attacks.  Existing research has primarily focused on improving the robustness of image encoders against image-based attacks, neglecting defenses against text-based and multimodal attacks.  This work presents the first comprehensive study on enhancing the adversarial robustness of VLMs against attacks targeting image, text, and multimodal inputs.  We achieve this by proposing multimodal contrastive adversarial training (MMCoA), a novel approach that strengthens both image and text encoders by aligning clean text embeddings with adversarial image embeddings, and vice versa.  The robustness of MMCoA is evaluated against existing defense methods using image, text, and multimodal attacks on the CLIP model.  Extensive experiments across fifteen datasets and two tasks reveal the characteristics of various adversarial defense methods under different distribution shifts and dataset complexities for all three attack types.  This establishes a unified framework for adversarial robustness against multi-modal attacks, paving the way for more secure VLMs.  The code is available at https://github.com/ElleZWQ/MMCoA.git.\n"}, "2211.09469": {"original_text": "  Generating consecutive descriptions for videos, i.e., Video Captioning,\nrequires taking full advantage of visual representation along with the\ngeneration process. Existing video captioning methods focus on making an\nexploration of spatial-temporal representations and their relationships to\nproduce inferences. However, such methods only exploit the superficial\nassociation contained in the video itself without considering the intrinsic\nvisual commonsense knowledge that existed in a video dataset, which may hinder\ntheir capabilities of knowledge cognitive to reason accurate descriptions. To\naddress this problem, we propose a simple yet effective method, called Visual\nCommonsense-aware Representation Network (VCRN), for video captioning.\nSpecifically, we construct a Video Dictionary, a plug-and-play component,\nobtained by clustering all video features from the total dataset into multiple\nclustered centers without additional annotation. Each center implicitly\nrepresents a visual commonsense concept in the video domain, which is utilized\nin our proposed Visual Concept Selection (VCS) to obtain a video-related\nconcept feature. Next, a Conceptual Integration Generation (CIG) is proposed to\nenhance the caption generation. Extensive experiments on three publicly video\ncaptioning benchmarks: MSVD, MSR-VTT, and VATEX, demonstrate that our method\nreaches state-of-the-art performance, indicating the effectiveness of our\nmethod. In addition, our approach is integrated into the existing method of\nvideo question answering and improves this performance, further showing the\ngeneralization of our method. Source code has been released at\nhttps://github.com/zchoi/VCRN.\n", "rewritten_text": "Video captioning, the generation of consecutive descriptions for videos, requires leveraging both visual representation and the generation process.  Existing methods primarily explore spatial-temporal representations and their relationships to infer captions. However, these methods often overlook the intrinsic visual commonsense knowledge present within the video dataset, limiting their ability to reason and generate accurate descriptions.  To address this, we propose Visual Commonsense-aware Representation Network (VCRN), a simple yet effective method for video captioning.\n\nSpecifically, VCRN incorporates a Video Dictionary, a plug-and-play component created by clustering video features from the entire dataset into multiple cluster centers without requiring additional annotations. Each center implicitly represents a visual commonsense concept, which is then utilized by our Visual Concept Selection (VCS) module to extract relevant concept features.  These features are further integrated into the caption generation process via our Conceptual Integration Generation (CIG) module.\n\nExtensive experiments on three publicly available video captioning benchmarks\u2014MSVD, MSR-VTT, and VATEX\u2014demonstrate that VCRN achieves state-of-the-art performance, highlighting its effectiveness.  Furthermore, integrating VCRN into an existing video question answering method improves its performance, showcasing the generalizability of our approach.  The source code is available at https://github.com/zchoi/VCRN.\n"}, "2203.12788": {"original_text": "  A fundamental characteristic of natural language is the high rate at which\nspeakers produce novel expressions. Because of this novelty, a heavy-tail of\nrare events accounts for a significant amount of the total probability mass of\ndistributions in language (Baayen, 2001). Standard language modeling metrics\nsuch as perplexity quantify the performance of language models (LM) in\naggregate. As a result, we have relatively little understanding of whether\nneural LMs accurately estimate the probability of sequences in this heavy-tail\nof rare events. To address this gap, we develop a controlled evaluation scheme\nwhich uses generative models trained on natural data as artificial languages\nfrom which we can exactly compute sequence probabilities. Training LMs on\ngenerations from these artificial languages, we compare the sequence-level\nprobability estimates given by LMs to the true probabilities in the target\nlanguage. Our experiments reveal that LSTM and Transformer language models (i)\nsystematically underestimate the probability of sequences drawn from the target\nlanguage, and (ii) do so more severely for less-probable sequences.\nInvestigating where this probability mass went, (iii) we find that LMs tend to\noverestimate the probability of ill formed (perturbed) sequences. In addition,\nwe find that this underestimation behaviour (iv) is weakened, but not\neliminated by greater amounts of training data, and (v) is exacerbated for\ntarget distributions with lower entropy.\n", "rewritten_text": "Natural language is characterized by the high rate at which speakers generate novel expressions.  This novelty results in a heavy-tailed distribution of word and phrase frequencies, where rare events account for a significant portion of the total probability mass (Baayen, 2001). Standard language modeling metrics, such as perplexity, assess model performance in aggregate. Consequently, the accuracy of neural language models (LMs) in estimating the probabilities of rare events remains poorly understood.  To address this, we developed a controlled evaluation scheme using generative models trained on natural language data to create artificial languages with precisely calculable sequence probabilities.  By training LMs on these artificial languages and comparing their estimated sequence probabilities to the true probabilities, our experiments revealed that LSTM and Transformer LMs: (i) systematically underestimate the probability of sequences; (ii) exhibit more severe underestimation for less probable sequences; (iii) concomitantly overestimate the probability of ill-formed (perturbed) sequences; (iv) show reduced, but persistent, underestimation even with increased training data; and (v) demonstrate exacerbated underestimation for lower-entropy target distributions.\n"}, "2207.09884": {"original_text": "  We present a Momentum Re-identification (MoReID) framework that can leverage\na very large number of negative samples in training for general\nre-identification task. The design of this framework is inspired by Momentum\nContrast (MoCo), which uses a dictionary to store current and past batches to\nbuild a large set of encoded samples. As we find it less effective to use past\npositive samples which may be highly inconsistent to the encoded feature\nproperty formed with the current positive samples, MoReID is designed to use\nonly a large number of negative samples stored in the dictionary. However, if\nwe train the model using the widely used Triplet loss that uses only one sample\nto represent a set of positive/negative samples, it is hard to effectively\nleverage the enlarged set of negative samples acquired by the MoReID framework.\nTo maximize the advantage of using the scaled-up negative sample set, we newly\nintroduce Hard-distance Elastic loss (HE loss), which is capable of using more\nthan one hard sample to represent a large number of samples. Our experiments\ndemonstrate that a large number of negative samples provided by MoReID\nframework can be utilized at full capacity only with the HE loss, achieving the\nstate-of-the-art accuracy on three re-ID benchmarks, VeRi-776, Market-1501, and\nVeRi-Wild.\n", "rewritten_text": "We introduce Momentum Re-identification (MoReID), a framework that leverages a large number of negative samples during training for improved person re-identification.  Inspired by Momentum Contrast (MoCo), MoReID employs a dictionary to store encoded samples from current and past batches, creating an extensive negative sample set.  Unlike MoCo, we exclude past positive samples due to potential inconsistencies with the current positive sample encodings.  However, standard triplet loss, which uses only a single positive and a single negative sample, cannot effectively utilize this expanded negative set.  To fully exploit MoReID's large negative sample pool, we propose the Hard-distance Elastic (HE) loss, capable of incorporating multiple hard samples to represent numerous samples.  Our experiments on three re-identification benchmarks\u2014VeRi-776, Market-1501, and VeRi-Wild\u2014demonstrate that HE loss is crucial for achieving state-of-the-art accuracy by fully leveraging the extensive negative samples provided by the MoReID framework.\n"}, "2410.12158": {"original_text": "  Foundation models have significantly enhanced 2D task performance, and recent\nworks like Bridge3D have successfully applied these models to improve 3D scene\nunderstanding through knowledge distillation, marking considerable\nadvancements. Nonetheless, challenges such as the misalignment between 2D and\n3D representations and the persistent long-tail distribution in 3D datasets\nstill restrict the effectiveness of knowledge distillation from 2D to 3D using\nfoundation models. To tackle these issues, we introduce a novel SAM-guided\ntokenization method that seamlessly aligns 3D transformer structures with\nregion-level knowledge distillation, replacing the traditional KNN-based\ntokenization techniques. Additionally, we implement a group-balanced\nre-weighting strategy to effectively address the long-tail problem in knowledge\ndistillation. Furthermore, inspired by the recent success of masked feature\nprediction, our framework incorporates a two-stage masked token prediction\nprocess in which the student model predicts both the global embeddings and the\ntoken-wise local embeddings derived from the teacher models trained in the\nfirst stage. Our methodology has been validated across multiple datasets,\nincluding SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and\nsemantic segmentation. The results demonstrate significant improvements over\ncurrent State-of-the-art self-supervised methods, establishing new benchmarks\nin this field.\n", "rewritten_text": "Foundation models have significantly improved 2D task performance, and recent work, such as Bridge3D, has successfully applied these models to enhance 3D scene understanding through knowledge distillation.  However, challenges remain, including the misalignment between 2D and 3D representations and the persistent long-tailed distribution of 3D datasets, limiting the effectiveness of 2D-to-3D knowledge distillation.  To address these limitations, we introduce a novel SAM-guided tokenization method that seamlessly aligns 3D transformer architectures with region-level knowledge distillation, replacing traditional KNN-based methods.  We also incorporate a group-balanced re-weighting strategy to mitigate the long-tailed distribution problem.  Furthermore, inspired by masked feature prediction, our framework employs a two-stage masked token prediction process.  The student model predicts both global and token-wise local embeddings derived from a teacher model trained in the first stage.  Our approach has been evaluated on multiple datasets\u2014SUN RGB-D, ScanNet, and S3DIS\u2014for 3D object detection and semantic segmentation tasks.  Results demonstrate significant improvements over state-of-the-art self-supervised methods, establishing new benchmarks in the field.\n"}, "2307.09892": {"original_text": "  We propose 3Deformer, a general-purpose framework for interactive 3D shape\nediting. Given a source 3D mesh with semantic materials, and a user-specified\nsemantic image, 3Deformer can accurately edit the source mesh following the\nshape guidance of the semantic image, while preserving the source topology as\nrigid as possible. Recent studies of 3D shape editing mostly focus on learning\nneural networks to predict 3D shapes, which requires high-cost 3D training\ndatasets and is limited to handling objects involved in the datasets. Unlike\nthese studies, our 3Deformer is a non-training and common framework, which only\nrequires supervision of readily-available semantic images, and is compatible\nwith editing various objects unlimited by datasets. In 3Deformer, the source\nmesh is deformed utilizing the differentiable renderer technique, according to\nthe correspondences between semantic images and mesh materials. However,\nguiding complex 3D shapes with a simple 2D image incurs extra challenges, that\nis, the deform accuracy, surface smoothness, geometric rigidity, and global\nsynchronization of the edited mesh should be guaranteed. To address these\nchallenges, we propose a hierarchical optimization architecture to balance the\nglobal and local shape features, and propose further various strategies and\nlosses to improve properties of accuracy, smoothness, rigidity, and so on.\nExtensive experiments show that our 3Deformer is able to produce impressive\nresults and reaches the state-of-the-art level.\n", "rewritten_text": "We present 3Deformer, a general-purpose framework for interactive 3D shape editing.  Given a source 3D mesh with semantic material labels and a user-provided semantic image, 3Deformer accurately edits the source mesh according to the image's shape guidance while preserving its topology as rigidly as possible.  Existing 3D shape editing methods primarily rely on neural networks trained on expensive 3D datasets, limiting their applicability to the objects represented in those datasets.  In contrast, 3Deformer is a trainable framework requiring only readily available semantic images for supervision, enabling versatile object editing without dataset limitations.  3Deformer deforms the source mesh using a differentiable renderer, leveraging correspondences between semantic image labels and mesh materials.  However, guiding complex 3D shapes using a simple 2D image presents challenges: maintaining deformation accuracy, surface smoothness, geometric rigidity, and global consistency of the edited mesh.  To address these, we introduce a hierarchical optimization architecture that balances global and local shape features, along with several strategies and loss functions to improve accuracy, smoothness, and rigidity.  Extensive experiments demonstrate that 3Deformer achieves state-of-the-art results.\n"}, "2105.03571": {"original_text": "  Dialogue state tracking (DST) plays a key role in task-oriented dialogue\nsystems to monitor the user's goal. In general, there are two strategies to\ntrack a dialogue state: predicting it from scratch and updating it from\nprevious state. The scratch-based strategy obtains each slot value by inquiring\nall the dialogue history, and the previous-based strategy relies on the current\nturn dialogue to update the previous dialogue state. However, it is hard for\nthe scratch-based strategy to correctly track short-dependency dialogue state\nbecause of noise; meanwhile, the previous-based strategy is not very useful for\nlong-dependency dialogue state tracking. Obviously, it plays different roles\nfor the context information of different granularity to track different kinds\nof dialogue states. Thus, in this paper, we will study and discuss how the\ncontext information of different granularity affects dialogue state tracking.\nFirst, we explore how greatly different granularities affect dialogue state\ntracking. Then, we further discuss how to combine multiple granularities for\ndialogue state tracking. Finally, we apply the findings about context\ngranularity to few-shot learning scenario. Besides, we have publicly released\nall codes.\n", "rewritten_text": "Dialogue state tracking (DST) is crucial for task-oriented dialogue systems, enabling them to monitor user goals.  Two primary strategies exist: predicting the state from scratch and updating it from the previous state.  The former approach infers each slot value from the entire dialogue history, while the latter updates the previous state based solely on the current turn.  However, the \"from-scratch\" approach struggles with short-dependency dialogue states due to noise, whereas the update-based approach is less effective for long-dependency states.  The effectiveness of each strategy depends heavily on the granularity of the contextual information used to track different types of dialogue states.  This paper investigates the impact of context granularity on DST.  We first analyze the effect of varying granularities on DST performance.  Next, we explore methods for combining multiple granularities to improve tracking. Finally, we apply our findings to few-shot learning scenarios.  The code used in this research is publicly available.\n"}, "2405.10474": {"original_text": "  Over the last decade, a wide range of training and deployment strategies for\nLarge Language Models (LLMs) have emerged. Among these, the prompting paradigms\nof Auto-regressive LLMs (AR-LLMs) have catalyzed a significant surge in\nArtificial Intelligence (AI). This paper aims to emphasize the significance of\nutilizing free-form modalities (forms of input and output) and verbal free-form\ncontexts as user-directed channels (methods for transforming modalities) for\ndownstream deployment. Specifically, we analyze the structure of modalities\nwithin both two types of LLMs and six task-specific channels during deployment.\nFrom the perspective of users, our analysis introduces and applies the\nanalytical metrics of task customizability, transparency, and complexity to\ngauge their usability, highlighting the superior nature of AR-LLMs' prompting\nparadigms. Moreover, we examine the stimulation of diverse cognitive behaviors\nin LLMs through the adoption of free-form text and verbal contexts, mirroring\nhuman linguistic expressions of such behaviors. We then detail four common\ncognitive behaviors to underscore how AR-LLMs' prompting successfully imitate\nhuman-like behaviors using this free-form modality and channel. Lastly, the\npotential for improving LLM deployment, both as autonomous agents and within\nmulti-agent systems, is identified via cognitive behavior concepts and\nprinciples.\n", "rewritten_text": "The past decade has witnessed the emergence of diverse training and deployment strategies for Large Language Models (LLMs).  Autoregressive LLMs (AR-LLMs) and their prompting paradigms, in particular, have significantly advanced the field of Artificial Intelligence (AI). This paper argues for the importance of free-form input and output modalities, and their associated verbal contexts, as user-directed channels for LLM deployment.  We analyze the modal structure of two LLM types across six task-specific deployment channels.  From a user perspective, we introduce and apply the analytical metrics of task customizability, transparency, and complexity to evaluate usability, demonstrating the superior performance of AR-LLM prompting paradigms.  Furthermore, we examine how free-form text and verbal contexts elicit diverse cognitive behaviors in LLMs, mirroring human linguistic expression.  Four common cognitive behaviors are detailed to illustrate how AR-LLM prompting effectively mimics human-like behavior through this free-form modality and channel. Finally, we explore the potential for improving LLM deployment, both as autonomous agents and within multi-agent systems, leveraging principles of cognitive behavior.\n"}, "2212.04214": {"original_text": "  In a citation graph, adjacent paper nodes share related scientific terms and\ntopics. The graph thus conveys unique structure information of document-level\nrelatedness that can be utilized in the paper summarization task, for exploring\nbeyond the intra-document information. In this work, we focus on leveraging\ncitation graphs to improve scientific paper extractive summarization under\ndifferent settings. We first propose a Multi-granularity Unsupervised\nSummarization model (MUS) as a simple and low-cost solution to the task. MUS\nfinetunes a pre-trained encoder model on the citation graph by link prediction\ntasks. Then, the abstract sentences are extracted from the corresponding paper\nconsidering multi-granularity information. Preliminary results demonstrate that\ncitation graph is helpful even in a simple unsupervised framework. Motivated by\nthis, we next propose a Graph-based Supervised Summarization model (GSS) to\nachieve more accurate results on the task when large-scale labeled data are\navailable. Apart from employing the link prediction as an auxiliary task, GSS\nintroduces a gated sentence encoder and a graph information fusion module to\ntake advantage of the graph information to polish the sentence representation.\nExperiments on a public benchmark dataset show that MUS and GSS bring\nsubstantial improvements over the prior state-of-the-art model.\n", "rewritten_text": "Citation graphs, where adjacent paper nodes share related scientific terms and topics, encode unique structural information about document-level relatedness.  This information can be leveraged beyond intra-document analysis, particularly in scientific paper summarization.  This work explores using citation graphs to improve extractive summarization under various settings.  We first propose a Multi-granularity Unsupervised Summarization (MUS) model, a simple, low-cost approach.  MUS fine-tunes a pre-trained encoder on the citation graph using link prediction as an auxiliary task, then extracts abstract sentences considering multi-granularity information.  Preliminary results demonstrate the utility of citation graphs even within a simple unsupervised framework.  Building on this, we propose a Graph-based Supervised Summarization (GSS) model to achieve higher accuracy when large-scale labeled data are available.  In addition to link prediction, GSS incorporates a gated sentence encoder and a graph information fusion module to refine sentence representations.  Experiments on a public benchmark dataset show that MUS and GSS significantly outperform the prior state-of-the-art.\n"}, "2310.14561": {"original_text": "  Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by\nwell-designed perturbations. This could lead to disastrous results on critical\napplications such as self-driving cars, surveillance security, and medical\ndiagnosis. At present, adversarial training is one of the most effective\ndefenses against adversarial examples. However, traditional adversarial\ntraining makes it difficult to achieve a good trade-off between clean accuracy\nand robustness since spurious features are still learned by DNNs. The intrinsic\nreason is that traditional adversarial training makes it difficult to fully\nlearn core features from adversarial examples when adversarial noise and clean\nexamples cannot be disentangled. In this paper, we disentangle the adversarial\nexamples into natural and perturbed patterns by bit-plane slicing. We assume\nthe higher bit-planes represent natural patterns and the lower bit-planes\nrepresent perturbed patterns, respectively. We propose a Feature-Focusing\nAdversarial Training (F$^2$AT), which differs from previous work in that it\nenforces the model to focus on the core features from natural patterns and\nreduce the impact of spurious features from perturbed patterns. The\nexperimental results demonstrated that F$^2$AT outperforms state-of-the-art\nmethods in clean accuracy and adversarial robustness.\n", "rewritten_text": "Deep neural networks (DNNs) are vulnerable to adversarial examples generated by carefully designed perturbations, potentially leading to catastrophic consequences in critical applications such as self-driving cars, surveillance systems, and medical diagnosis.  Adversarial training is currently a highly effective defense; however, traditional methods struggle to balance clean accuracy and robustness due to the persistent learning of spurious features by DNNs. This difficulty stems from the inability to fully disentangle adversarial noise from clean examples during training.  This paper introduces Feature-Focusing Adversarial Training (F\u00b2AT), which addresses this limitation by disentangling adversarial examples into natural and perturbed patterns using bit-plane slicing.  We hypothesize that higher bit-planes represent natural patterns, while lower bit-planes represent perturbations.  F\u00b2AT uniquely enforces the model to focus on core features from natural patterns, thereby mitigating the influence of spurious features from perturbed patterns.  Experimental results demonstrate that F\u00b2AT surpasses state-of-the-art methods in both clean accuracy and adversarial robustness.\n"}, "2308.04782": {"original_text": "  Point cloud registration is a task to estimate the rigid transformation\nbetween two unaligned scans, which plays an important role in many computer\nvision applications. Previous learning-based works commonly focus on supervised\nregistration, which have limitations in practice. Recently, with the advance of\ninexpensive RGB-D sensors, several learning-based works utilize RGB-D data to\nachieve unsupervised registration. However, most of existing unsupervised\nmethods follow a cascaded design or fuse RGB-D data in a unidirectional manner,\nwhich do not fully exploit the complementary information in the RGB-D data. To\nleverage the complementary information more effectively, we propose a network\nimplementing multi-scale bidirectional fusion between RGB images and point\nclouds generated from depth images. By bidirectionally fusing visual and\ngeometric features in multi-scales, more distinctive deep features for\ncorrespondence estimation can be obtained, making our registration more\naccurate. Extensive experiments on ScanNet and 3DMatch demonstrate that our\nmethod achieves new state-of-the-art performance. Code will be released at\nhttps://github.com/phdymz/PointMBF\n", "rewritten_text": "Point cloud registration estimates the rigid transformation between two misaligned scans, a crucial task in many computer vision applications.  While previous learning-based approaches primarily relied on supervised registration\u2014a method with practical limitations\u2014recent advances in inexpensive RGB-D sensors have enabled unsupervised learning-based methods. However, most existing unsupervised methods employ a cascaded design or unidirectional fusion of RGB-D data, failing to fully exploit their complementary information.  To address this, we propose a network that performs multi-scale bidirectional fusion of RGB images and point clouds derived from depth images. This bidirectional fusion of visual and geometric features at multiple scales yields more distinctive deep features for correspondence estimation, resulting in improved registration accuracy.  Extensive experiments on ScanNet and 3DMatch datasets demonstrate state-of-the-art performance.  Code is available at https://github.com/phdymz/PointMBF.\n"}, "2404.02573": {"original_text": "  Knowledge distillation (KD) is a promising yet challenging model compression\ntechnique that transfers rich learning representations from a well-performing\nbut cumbersome teacher model to a compact student model. Previous methods for\nimage super-resolution (SR) mostly compare the feature maps directly or after\nstandardizing the dimensions with basic algebraic operations (e.g. average,\ndot-product). However, the intrinsic semantic differences among feature maps\nare overlooked, which are caused by the disparate expressive capacity between\nthe networks. This work presents MiPKD, a multi-granularity mixture of prior KD\nframework, to facilitate efficient SR model through the feature mixture in a\nunified latent space and stochastic network block mixture. Extensive\nexperiments demonstrate the effectiveness of the proposed MiPKD method.\n", "rewritten_text": "Knowledge distillation (KD) is a promising yet challenging model compression technique that transfers knowledge from a large, high-performing teacher model to a smaller, more efficient student model.  Existing image super-resolution (SR) methods primarily compare feature maps directly or after simple dimensionality standardization (e.g., averaging, dot product).  However, these methods neglect the inherent semantic differences between feature maps arising from the disparate representational capacities of the teacher and student networks.  This work introduces MiPKD, a multi-granularity mixture-of-priors KD framework, which facilitates efficient SR by mixing features in a unified latent space and employing stochastic network block mixing.  Extensive experiments demonstrate MiPKD's effectiveness.\n"}, "2307.08198": {"original_text": "  We introduce the notion of point affiliation into feature upsampling. By\nabstracting a feature map into non-overlapped semantic clusters formed by\npoints of identical semantic meaning, feature upsampling can be viewed as point\naffiliation -- designating a semantic cluster for each upsampled point. In the\nframework of kernel-based dynamic upsampling, we show that an upsampled point\ncan resort to its low-res decoder neighbors and high-res encoder point to\nreason the affiliation, conditioned on the mutual similarity between them. We\ntherefore present a generic formulation for generating similarity-aware\nupsampling kernels and prove that such kernels encourage not only semantic\nsmoothness but also boundary sharpness. This formulation constitutes a novel,\nlightweight, and universal upsampling solution, Similarity-Aware Point\nAffiliation (SAPA). We show its working mechanism via our preliminary designs\nwith window-shape kernel. After probing the limitations of the designs on\nobject detection, we reveal additional insights for upsampling, leading to SAPA\nwith the dynamic kernel shape. Extensive experiments demonstrate that SAPA\noutperforms prior upsamplers and invites consistent performance improvements on\na number of dense prediction tasks, including semantic segmentation, object\ndetection, instance segmentation, panoptic segmentation, image matting, and\ndepth estimation. Code is made available at: https://github.com/tiny-smart/sapa\n", "rewritten_text": "We introduce point affiliation to feature upsampling.  By abstracting a feature map into non-overlapping semantic clusters defined by points of identical semantic meaning, feature upsampling becomes point affiliation\u2014assigning a semantic cluster to each upsampled point.  Within a kernel-based dynamic upsampling framework, we demonstrate that an upsampled point can leverage its low-resolution decoder neighbors and high-resolution encoder point to infer its affiliation, conditioned on their mutual similarity.  We present a novel formulation for generating similarity-aware upsampling kernels and prove that these kernels promote both semantic smoothness and boundary sharpness. This formulation yields a lightweight, universal upsampling solution, Similarity-Aware Point Affiliation (SAPA).  We illustrate SAPA's mechanism through preliminary designs using window-shaped kernels.  Analysis of these designs's limitations in object detection revealed further insights, leading to SAPA with dynamic kernel shapes.  Extensive experiments demonstrate that SAPA outperforms existing upsamplers, consistently improving performance across various dense prediction tasks, including semantic segmentation, object detection, instance segmentation, panoptic segmentation, image matting, and depth estimation.  Code is available at: https://github.com/tiny-smart/sapa\n"}, "2410.16646": {"original_text": "  Diffusion models excel at creating visually impressive images but often\nstruggle to generate images with a specified topology. The Betti number, which\nrepresents the number of structures in an image, is a fundamental measure in\ntopology. Yet, diffusion models fail to satisfy even this basic constraint.\nThis limitation restricts their utility in applications requiring exact\ncontrol, like robotics and environmental modeling. To address this, we propose\nTopoDiffusionNet (TDN), a novel approach that enforces diffusion models to\nmaintain the desired topology. We leverage tools from topological data\nanalysis, particularly persistent homology, to extract the topological\nstructures within an image. We then design a topology-based objective function\nto guide the denoising process, preserving intended structures while\nsuppressing noisy ones. Our experiments across four datasets demonstrate\nsignificant improvements in topological accuracy. TDN is the first to integrate\ntopology with diffusion models, opening new avenues of research in this area.\n", "rewritten_text": "Diffusion models generate visually striking images but often fail to control their topology.  Betti numbers, quantifying image structures, are fundamental topological measures, yet diffusion models frequently violate even this basic constraint. This limitation hinders their applicability in precision-demanding fields such as robotics and environmental modeling.  To address this, we introduce TopoDiffusionNet (TDN), a novel method that enforces topological fidelity in diffusion models.  Leveraging persistent homology from topological data analysis, TDN extracts topological structures from images.  A topology-based objective function then guides the denoising process, preserving desired structures while mitigating noise.  Experiments across four datasets demonstrate significant improvements in topological accuracy.  TDN represents the first integration of topology into diffusion models, opening promising new research directions.\n"}, "1603.01684": {"original_text": "  In this paper, we propose an improved mechanism for saliency detection.\nFirstly,based on a neoteric background prior selecting four corners of an image\nas background,we use color and spatial contrast with each superpixel to obtain\na salinecy map(CBP). Inspired by reverse-measurement methods to improve the\naccuracy of measurement in Engineering,we employ the Objectness labels as\nforeground prior based on part of information of CBP to construct a\nmap(OFP).Further,an original energy function is applied to optimize both of\nthem respectively and a single-layer saliency map(SLP)is formed by merging the\nabove twos.Finally,to deal with the scale problem,we obtain our multi-layer\nmap(MLP) by presenting an integration algorithm to take advantage of multiple\nsaliency maps. Quantitative and qualitative experiments on three datasets\ndemonstrate that our method performs favorably against the state-of-the-art\nalgorithm.\n", "rewritten_text": "This paper proposes an improved saliency detection mechanism.  First, a novel background prior is established by selecting the four image corners as background regions.  Color and spatial contrast are then computed for each superpixel to generate a saliency map (CBP).  Inspired by reverse-measurement techniques from engineering, objectness labels, derived from the CBP, are used as a foreground prior to construct a second saliency map (OFP).  An original energy function independently optimizes both CBP and OFP, which are then merged to form a single-layer saliency map (SLP). Finally, a multi-layer saliency map (MLP) is generated by integrating multiple saliency maps at different scales using a novel integration algorithm.  Quantitative and qualitative evaluations on three datasets demonstrate that our method outperforms state-of-the-art algorithms.\n"}, "2403.16998": {"original_text": "  Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly.\n", "rewritten_text": "Recent LLM-based approaches have achieved excellent performance on long-video understanding benchmarks.  This success is attributed to the extensive world knowledge and strong reasoning capabilities of underlying Large Language Models (LLMs).  Surprisingly, we found that these approaches can achieve surprisingly high accuracy even with limited or no video-specific information.  Building on this observation, we explored augmenting an LLM-based framework with video-specific information.  We used off-the-shelf computer vision tools to extract three object-centric modalities from videos and leveraged natural language to fuse this information.  Our resulting Multimodal Video Understanding (MVU) framework achieves state-of-the-art performance across multiple video understanding benchmarks, demonstrating strong generalization capabilities, as evidenced by its performance on robotics tasks.  The code will be publicly released.\n"}, "2006.01372": {"original_text": "  In general, the labels used in sequence labeling consist of different types\nof elements. For example, IOB-format entity labels, such as B-Person and\nI-Person, can be decomposed into span (B and I) and type information (Person).\nHowever, while most sequence labeling models do not consider such label\ncomponents, the shared components across labels, such as Person, can be\nbeneficial for label prediction. In this work, we propose to integrate label\ncomponent information as embeddings into models. Through experiments on English\nand Japanese fine-grained named entity recognition, we demonstrate that the\nproposed method improves performance, especially for instances with\nlow-frequency labels.\n", "rewritten_text": "Sequence labeling typically employs labels composed of diverse elements.  For instance, IOB-format entity labels like B-Person and I-Person comprise both span (B and I) and type (Person) information.  Although most sequence labeling models ignore these label components, shared components (e.g., \"Person\") can improve label prediction.  This work proposes integrating label component information as embeddings within sequence labeling models.  Experiments on English and Japanese fine-grained named entity recognition demonstrate performance improvements, particularly for low-frequency labels.\n"}, "2410.09140": {"original_text": "  The remarkable development of text-to-image generation models has raised\nnotable security concerns, such as the infringement of portrait rights and the\ngeneration of inappropriate content. Concept erasure has been proposed to\nremove the model's knowledge about protected and inappropriate concepts.\nAlthough many methods have tried to balance the efficacy (erasing target\nconcepts) and specificity (retaining irrelevant concepts), they can still\ngenerate abundant erasure concepts under the steering of semantically related\ninputs. In this work, we propose RealEra to address this \"concept residue\"\nissue. Specifically, we first introduce the mechanism of neighbor-concept\nmining, digging out the associated concepts by adding random perturbation into\nthe embedding of erasure concept, thus expanding the erasing range and\neliminating the generations even through associated concept inputs.\nFurthermore, to mitigate the negative impact on the generation of irrelevant\nconcepts caused by the expansion of erasure scope, RealEra preserves the\nspecificity through the beyond-concept regularization. This makes irrelevant\nconcepts maintain their corresponding spatial position, thereby preserving\ntheir normal generation performance. We also employ the closed-form solution to\noptimize weights of U-Net for the cross-attention alignment, as well as the\nprediction noise alignment with the LoRA module. Extensive experiments on\nmultiple benchmarks demonstrate that RealEra outperforms previous concept\nerasing methods in terms of superior erasing efficacy, specificity, and\ngenerality. More details are available on our project page\nhttps://realerasing.github.io/RealEra/ .\n", "rewritten_text": "The remarkable development of text-to-image generation models has raised significant security concerns, including the infringement of portrait rights and the generation of inappropriate content.  Concept erasure, proposed to remove the model's knowledge of protected and inappropriate concepts, has proven challenging. While numerous methods aim to balance efficacy (erasing target concepts) and specificity (preserving irrelevant concepts), they often generate \"concept residue\"\u2014the unintended generation of related concepts from semantically similar inputs.\n\nThis work introduces RealEra to address this issue.  We achieve this through a novel neighbor-concept mining mechanism.  By adding random perturbations to the embedding of the erasure concept, we expand the erasure range, effectively eliminating generations even from associated concept inputs.  To mitigate the negative impact on irrelevant concepts resulting from this expanded scope, RealEra incorporates beyond-concept regularization. This preserves the spatial position of irrelevant concepts, maintaining their normal generation performance.  Furthermore, we utilize a closed-form solution to optimize the U-Net weights for cross-attention alignment and align prediction noise with the LoRA module.\n\nExtensive experiments on multiple benchmarks demonstrate that RealEra surpasses existing concept erasure methods in efficacy, specificity, and generality.  For further details, please visit our project page: https://realerasing.github.io/RealEra/\n"}, "2210.02843": {"original_text": "  Focusing on the issue of how to effectively capture and utilize\ncross-modality information in RGB-D salient object detection (SOD) task, we\npresent a convolutional neural network (CNN) model, named CIR-Net, based on the\nnovel cross-modality interaction and refinement. For the cross-modality\ninteraction, 1) a progressive attention guided integration unit is proposed to\nsufficiently integrate RGB-D feature representations in the encoder stage, and\n2) a convergence aggregation structure is proposed, which flows the RGB and\ndepth decoding features into the corresponding RGB-D decoding streams via an\nimportance gated fusion unit in the decoder stage. For the cross-modality\nrefinement, we insert a refinement middleware structure between the encoder and\nthe decoder, in which the RGB, depth, and RGB-D encoder features are further\nrefined by successively using a self-modality attention refinement unit and a\ncross-modality weighting refinement unit. At last, with the gradually refined\nfeatures, we predict the saliency map in the decoder stage. Extensive\nexperiments on six popular RGB-D SOD benchmarks demonstrate that our network\noutperforms the state-of-the-art saliency detectors both qualitatively and\nquantitatively.\n", "rewritten_text": "This paper presents CIR-Net, a convolutional neural network (CNN) for RGB-D salient object detection (SOD) that effectively captures and utilizes cross-modality information.  Our approach incorporates novel cross-modality interaction and refinement mechanisms.  Cross-modality interaction is achieved through: 1) a progressive attention-guided integration unit in the encoder, which effectively fuses RGB and depth feature representations; and 2) a convergence aggregation structure in the decoder, which integrates RGB and depth decoding features via an importance-gated fusion unit into corresponding RGB-D decoding streams.  Cross-modality refinement is performed by a middleware structure positioned between the encoder and decoder. This structure refines RGB, depth, and RGB-D encoder features using a self-modality attention refinement unit followed by a cross-modality weighting refinement unit.  Finally, the decoder generates the saliency map using these progressively refined features.  Extensive experiments on six popular RGB-D SOD benchmarks demonstrate that CIR-Net significantly outperforms state-of-the-art methods, both qualitatively and quantitatively.\n"}, "1703.01515": {"original_text": "  Temporal action localization is an important yet challenging problem. Given a\nlong, untrimmed video consisting of multiple action instances and complex\nbackground contents, we need not only to recognize their action categories, but\nalso to localize the start time and end time of each instance. Many\nstate-of-the-art systems use segment-level classifiers to select and rank\nproposal segments of pre-determined boundaries. However, a desirable model\nshould move beyond segment-level and make dense predictions at a fine\ngranularity in time to determine precise temporal boundaries. To this end, we\ndesign a novel Convolutional-De-Convolutional (CDC) network that places CDC\nfilters on top of 3D ConvNets, which have been shown to be effective for\nabstracting action semantics but reduce the temporal length of the input data.\nThe proposed CDC filter performs the required temporal upsampling and spatial\ndownsampling operations simultaneously to predict actions at the frame-level\ngranularity. It is unique in jointly modeling action semantics in space-time\nand fine-grained temporal dynamics. We train the CDC network in an end-to-end\nmanner efficiently. Our model not only achieves superior performance in\ndetecting actions in every frame, but also significantly boosts the precision\nof localizing temporal boundaries. Finally, the CDC network demonstrates a very\nhigh efficiency with the ability to process 500 frames per second on a single\nGPU server. We will update the camera-ready version and publish the source\ncodes online soon.\n", "rewritten_text": "Temporal action localization is a challenging yet important problem.  Given a long, untrimmed video containing multiple action instances and complex background content, accurate localization requires identifying not only the action category of each instance but also its precise start and end times. While many state-of-the-art systems rely on segment-level classifiers to rank pre-defined proposal segments, a superior model should perform dense, fine-grained temporal predictions to determine precise boundaries.  To this end, we propose a novel Convolutional-Deconvolutional (CDC) network.  This network incorporates CDC filters atop 3D convolutional neural networks (ConvNets), which, while effective at abstracting action semantics, often reduce temporal resolution.  Our CDC filters simultaneously upsample temporally and downsample spatially, enabling frame-level action prediction.  This approach uniquely combines spatiotemporal action semantic modeling with fine-grained temporal dynamics.  The CDC network is trained end-to-end efficiently.  Our results demonstrate superior performance in both frame-level action detection and temporal boundary localization precision.  Furthermore, the CDC network achieves high efficiency, processing 500 frames per second on a single GPU.  The camera-ready version and source code will be made publicly available shortly.\n"}, "2305.0541": {"original_text": "  The medical conversational question answering (CQA) system aims at providing\na series of professional medical services to improve the efficiency of medical\ncare. Despite the success of large language models (LLMs) in complex reasoning\ntasks in various fields, such as mathematics, logic, and commonsense QA, they\nstill need to improve with the increased complexity and specialization of the\nmedical field. This is because medical CQA tasks require not only strong\nmedical reasoning, but also the ability to think broadly and deeply. In this\npaper, to address these challenges in medical CQA tasks that need to be\nconsidered and understood in many aspects, we propose the Holistically Thought\n(HoT) method, which is designed to guide the LLMs to perform the diffused and\nfocused thinking for generating high-quality medical responses. The proposed\nHoT method has been evaluated through automated and manual assessments in three\ndifferent medical CQA datasets containing the English and Chinese languages.\nThe extensive experimental results show that our method can produce more\ncorrectness, professional, and considerate answers than several\nstate-of-the-art (SOTA) methods, manifesting its effectiveness. Our code in\nhttps://github.com/WENGSYX/HoT.\n", "rewritten_text": "This paper introduces the Holistically Thought (HoT) method, a novel approach to improve the performance of large language models (LLMs) in medical conversational question answering (CQA).  While LLMs have demonstrated success in complex reasoning tasks across diverse fields, their application to the highly specialized and nuanced domain of medical CQA remains challenging.  Medical CQA demands not only strong medical reasoning but also broad and deep contextual understanding.  To address this, HoT guides LLMs to perform both diffused and focused thinking, leading to higher-quality medical responses.  We evaluated HoT using automated and manual assessments across three diverse English and Chinese medical CQA datasets.  Extensive experimental results demonstrate that HoT significantly outperforms several state-of-the-art (SOTA) methods, generating more accurate, professional, and empathetic answers.  Our code is publicly available at https://github.com/WENGSYX/HoT.\n"}, "2110.02204": {"original_text": "  Contextualised word embeddings generated from Neural Language Models (NLMs),\nsuch as BERT, represent a word with a vector that considers the semantics of\nthe target word as well its context. On the other hand, static word embeddings\nsuch as GloVe represent words by relatively low-dimensional, memory- and\ncompute-efficient vectors but are not sensitive to the different senses of the\nword. We propose Context Derived Embeddings of Senses (CDES), a method that\nextracts sense related information from contextualised embeddings and injects\nit into static embeddings to create sense-specific static embeddings.\nExperimental results on multiple benchmarks for word sense disambiguation and\nsense discrimination tasks show that CDES can accurately learn sense-specific\nstatic embeddings reporting comparable performance to the current\nstate-of-the-art sense embeddings.\n", "rewritten_text": "Neural language models (NLMs) like BERT generate contextualized word embeddings, representing words as vectors that incorporate both semantics and context.  In contrast, static word embeddings such as GloVe use lower-dimensional, computationally efficient vectors but lack sensitivity to word sense.  We propose Context-Derived Embeddings of Senses (CDES), a method that leverages contextualized embeddings to enrich static embeddings with sense-related information, thereby creating sense-specific static embeddings.  Experimental results on multiple word sense disambiguation and sense discrimination benchmarks demonstrate that CDES accurately learns these sense-specific embeddings, achieving performance comparable to state-of-the-art sense embeddings.\n"}, "1804.00247": {"original_text": "  This article describes our experiments in neural machine translation using\nthe recent Tensor2Tensor framework and the Transformer sequence-to-sequence\nmodel (Vaswani et al., 2017). We examine some of the critical parameters that\naffect the final translation quality, memory usage, training stability and\ntraining time, concluding each experiment with a set of recommendations for\nfellow researchers. In addition to confirming the general mantra \"more data and\nlarger models\", we address scaling to multiple GPUs and provide practical tips\nfor improved training regarding batch size, learning rate, warmup steps,\nmaximum sentence length and checkpoint averaging. We hope that our observations\nwill allow others to get better results given their particular hardware and\ndata constraints.\n", "rewritten_text": "This article presents experiments in neural machine translation (NMT) using the Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017).  We investigate key parameters affecting translation quality, memory usage, training stability, and training time, offering practical recommendations for researchers.  Beyond confirming the benefits of larger models and more data, we explore multi-GPU scaling and provide practical guidance on optimizing training via batch size, learning rate, warmup steps, maximum sentence length, and checkpoint averaging.  Our findings aim to assist researchers in achieving optimal results given their specific hardware and data limitations.\n"}, "2402.08874": {"original_text": "  While large language models (LLMs) excel at understanding and generating\nplain text, they are not tailored to handle hierarchical text structures or\ndirectly predict task-specific properties such as text rating. In fact,\nselectively and repeatedly grasping the hierarchical structure of large-scale\ntext is pivotal for deciphering its essence. To this end, we propose a novel\nframework for hierarchical text rating utilizing LLMs, which incorporates\nRecurrent Alignment with Hard Attention (RAHA). Particularly, hard attention\nmechanism prompts a frozen LLM to selectively focus on pertinent leaf texts\nassociated with the root text and generate symbolic representations of their\nrelationships. Inspired by the gradual stabilization of the Markov Chain,\nrecurrent alignment strategy involves feeding predicted ratings iteratively\nback into the prompts of another trainable LLM, aligning it to progressively\napproximate the desired target. Experimental results demonstrate that RAHA\noutperforms existing state-of-the-art methods on three hierarchical text rating\ndatasets. Theoretical and empirical analysis confirms RAHA's ability to\ngradually converge towards the underlying target through multiple inferences.\nAdditional experiments on plain text rating datasets verify the effectiveness\nof this Markov-like alignment. Our data and code can be available in\nhttps://github.com/ECNU-Text-Computing/Markov-LLM.\n", "rewritten_text": "Large language models (LLMs) excel at processing plain text, but lack the inherent ability to handle hierarchical structures or directly predict task-specific properties like text ratings.  Effectively deciphering the essence of large-scale text requires selectively and repeatedly analyzing its hierarchical structure.  To address this, we propose RAHA (Recurrent Alignment with Hard Attention), a novel framework for hierarchical text rating using LLMs.  RAHA employs a hard attention mechanism to guide a frozen LLM to selectively focus on relevant leaf nodes within the hierarchical text, generating symbolic representations of their relationships.  A recurrent alignment strategy, inspired by the convergence of Markov chains, iteratively feeds predicted ratings back into the prompts of a trainable LLM, progressively aligning its predictions with the target ratings.  Experimental results on three hierarchical text rating datasets demonstrate that RAHA outperforms existing state-of-the-art methods.  Theoretical and empirical analyses confirm RAHA's ability to converge towards the target through iterative inference.  Further experiments on plain text rating datasets validate the effectiveness of this Markov-like alignment strategy.  Our data and code are available at https://github.com/ECNU-Text-Computing/Markov-LLM.\n"}, "2410.10227": {"original_text": "  Few-Shot Learning (FSL) aims to recognize new classes with limited labeled\ndata. Recent studies have attempted to address the challenge of rare samples\nwith textual prompts to modulate visual features. However, they usually\nstruggle to capture complex semantic relationships between textual and visual\nfeatures. Moreover, vanilla self-attention is heavily affected by useless\ninformation in images, severely constraining the potential of semantic priors\nin FSL due to the confusion of numerous irrelevant tokens during interaction.\nTo address these aforementioned issues, a K-NN Transformer with Pyramid Prompts\n(KTPP) is proposed to select discriminative information with K-NN Context\nAttention (KCA) and adaptively modulate visual features with Pyramid\nCross-modal Prompts (PCP). First, for each token, the KCA only selects the K\nmost relevant tokens to compute the self-attention matrix and incorporates the\nmean of all tokens as the context prompt to provide the global context in three\ncascaded stages. As a result, irrelevant tokens can be progressively\nsuppressed. Secondly, pyramid prompts are introduced in the PCP to emphasize\nvisual features via interactions between text-based class-aware prompts and\nmulti-scale visual features. This allows the ViT to dynamically adjust the\nimportance weights of visual features based on rich semantic information at\ndifferent scales, making models robust to spatial variations. Finally,\naugmented visual features and class-aware prompts are interacted via the KCA to\nextract class-specific features. Consequently, our model further enhances\nnoise-free visual representations via deep cross-modal interactions, extracting\ngeneralized visual representation in scenarios with few labeled samples.\nExtensive experiments on four benchmark datasets demonstrate the effectiveness\nof our method.\n", "rewritten_text": "Few-shot learning (FSL) aims to recognize novel classes with limited labeled data.  Recent approaches leverage textual prompts to modulate visual features, but often struggle to capture complex semantic relationships between these modalities.  Furthermore, standard self-attention mechanisms are susceptible to noise from irrelevant image information, hindering the effectiveness of semantic priors in FSL.  This is due to the interference of numerous irrelevant tokens during cross-modal interaction.\n\nTo address these limitations, we propose the K-NN Transformer with Pyramid Prompts (KTPP).  KTPP employs K-NN Context Attention (KCA) to select discriminative information and Pyramid Cross-modal Prompts (PCP) to adaptively modulate visual features.  KCA, implemented in three cascaded stages, selects the *k* most relevant tokens for each token's self-attention computation, incorporating the mean of all tokens as a global context prompt. This progressively suppresses irrelevant information.  PCP introduces pyramid prompts to enhance visual features through interactions between class-aware textual prompts and multi-scale visual features. This enables the Vision Transformer (ViT) to dynamically weight visual features based on rich, multi-scale semantic information, improving robustness to spatial variations.  Finally, augmented visual features and class-aware prompts interact via KCA to extract class-specific features, resulting in refined, noise-reduced visual representations through deep cross-modal interaction and improved generalization with limited labeled data.\n\nExtensive experiments on four benchmark datasets demonstrate the effectiveness of our method.\n"}, "2005.00692": {"original_text": "  Cross-lingual Entity Linking (XEL), the problem of grounding mentions of\nentities in a foreign language text into an English knowledge base such as\nWikipedia, has seen a lot of research in recent years, with a range of\npromising techniques. However, current techniques do not rise to the challenges\nintroduced by text in low-resource languages (LRL) and, surprisingly, fail to\ngeneralize to text not taken from Wikipedia, on which they are usually trained.\n  This paper provides a thorough analysis of low-resource XEL techniques,\nfocusing on the key step of identifying candidate English Wikipedia titles that\ncorrespond to a given foreign language mention. Our analysis indicates that\ncurrent methods are limited by their reliance on Wikipedia's interlanguage\nlinks and thus suffer when the foreign language's Wikipedia is small. We\nconclude that the LRL setting requires the use of outside-Wikipedia\ncross-lingual resources and present a simple yet effective zero-shot XEL\nsystem, QuEL, that utilizes search engines query logs. With experiments on 25\nlanguages, QuEL~shows an average increase of 25\\% in gold candidate recall and\nof 13\\% in end-to-end linking accuracy over state-of-the-art baselines.\n", "rewritten_text": "Cross-lingual entity linking (XEL), the task of mapping entity mentions in a foreign-language text to an English knowledge base such as Wikipedia, has been the focus of considerable recent research, yielding promising techniques.  However, current methods struggle with low-resource languages (LRLs) and, unexpectedly, exhibit poor generalization to text outside the Wikipedia corpus on which they are typically trained.\n\nThis paper presents a comprehensive analysis of low-resource XEL techniques, concentrating on the crucial step of identifying candidate English Wikipedia titles corresponding to foreign-language mentions.  Our analysis reveals that existing methods are constrained by their reliance on Wikipedia's interlanguage links, resulting in significant performance degradation when the foreign-language Wikipedia is small.  We conclude that effective LRL XEL requires leveraging external cross-lingual resources.  Therefore, we introduce QuEL, a simple yet effective zero-shot XEL system that utilizes search engine query logs.  Experiments across 25 languages demonstrate that QuEL achieves an average 25% increase in gold candidate recall and a 13% improvement in end-to-end linking accuracy compared to state-of-the-art baselines.\n"}, "2103.12462": {"original_text": "  Person ReID methods always learn through a stationary domain that is fixed by\nthe choice of a given dataset. In many contexts (e.g., lifelong learning),\nthose methods are ineffective because the domain is continually changing in\nwhich case incremental learning over multiple domains is required potentially.\nIn this work we explore a new and challenging ReID task, namely lifelong person\nre-identification (LReID), which enables to learn continuously across multiple\ndomains and even generalise on new and unseen domains. Following the cognitive\nprocesses in the human brain, we design an Adaptive Knowledge Accumulation\n(AKA) framework that is endowed with two crucial abilities: knowledge\nrepresentation and knowledge operation. Our method alleviates catastrophic\nforgetting on seen domains and demonstrates the ability to generalize to unseen\ndomains. Correspondingly, we also provide a new and large-scale benchmark for\nLReID. Extensive experiments demonstrate our method outperforms other\ncompetitors by a margin of 5.8% mAP in generalising evaluation.\n", "rewritten_text": "Person Re-identification (ReID) methods typically learn from a fixed, stationary domain defined by the chosen dataset.  This limits their effectiveness in dynamic contexts, such as lifelong learning, where the domain continually changes, necessitating incremental learning across multiple domains.  This work introduces a novel and challenging ReID task: lifelong person re-identification (LReID), enabling continuous learning across multiple domains and generalization to unseen domains.  Inspired by human cognitive processes, we propose an Adaptive Knowledge Accumulation (AKA) framework with two key components: knowledge representation and knowledge operation.  Our method mitigates catastrophic forgetting on previously seen domains while demonstrating strong generalization to unseen domains.  Furthermore, we introduce a new large-scale benchmark dataset for LReID.  Extensive experiments show our method outperforms competing approaches, achieving a 5.8% mAP improvement in generalization performance.\n"}, "1705.07426": {"original_text": "  While the research community appears to have developed a consensus on the\nmethods of acquiring annotated data, design and training of CNNs, many\nquestions still remain to be answered. In this paper, we explore the following\nquestions that are critical to face recognition research: (i) Can we train on\nstill images and expect the systems to work on videos? (ii) Are deeper datasets\nbetter than wider datasets? (iii) Does adding label noise lead to improvement\nin performance of deep networks? (iv) Is alignment needed for face recognition?\nWe address these questions by training CNNs using CASIA-WebFace, UMDFaces, and\na new video dataset and testing on YouTube- Faces, IJB-A and a disjoint portion\nof UMDFaces datasets. Our new data set, which will be made publicly available,\nhas 22,075 videos and 3,735,476 human annotated frames extracted from them.\n", "rewritten_text": "Although a consensus exists within the research community regarding methods for acquiring annotated data and designing and training Convolutional Neural Networks (CNNs), several critical questions remain unanswered in face recognition research.  This paper investigates the following: (i) Can CNNs trained on still images effectively process video data? (ii) Do deeper datasets outperform wider datasets? (iii) Does the introduction of label noise improve deep network performance? (iv) Is facial alignment necessary for accurate face recognition?  We address these questions by training CNNs on CASIA-WebFace, UMDFaces, and a novel video dataset, subsequently evaluating performance on YouTube Faces, IJB-A, and a disjoint subset of UMDFaces.  This new dataset, which will be publicly released, comprises 22,075 videos and 3,735,476 human-annotated frames.\n"}, "2010.06363": {"original_text": "  Lip motion reflects behavior characteristics of speakers, and thus can be\nused as a new kind of biometrics in speaker recognition. In the literature,\nlots of works used two-dimensional (2D) lip images to recognize speaker in a\ntextdependent context. However, 2D lip easily suffers from various face\norientations. To this end, in this work, we present a novel end-to-end 3D lip\nmotion Network (3LMNet) by utilizing the sentence-level 3D lip motion (S3DLM)\nto recognize speakers in both the text-independent and text-dependent contexts.\nA new regional feedback module (RFM) is proposed to obtain attentions in\ndifferent lip regions. Besides, prior knowledge of lip motion is investigated\nto complement RFM, where landmark-level and frame-level features are merged to\nform a better feature representation. Moreover, we present two methods, i.e.,\ncoordinate transformation and face posture correction to pre-process the LSD-AV\ndataset, which contains 68 speakers and 146 sentences per speaker. The\nevaluation results on this dataset demonstrate that our proposed 3LMNet is\nsuperior to the baseline models, i.e., LSTM, VGG-16 and ResNet-34, and\noutperforms the state-of-the-art using 2D lip image as well as the 3D face. The\ncode of this work is released at\nhttps://github.com/wutong18/Three-Dimensional-Lip-\nMotion-Network-for-Text-Independent-Speaker-Recognition.\n", "rewritten_text": "Lip motion reflects speaker behavior and can serve as a novel biometric for speaker recognition.  Existing literature predominantly uses two-dimensional (2D) lip images for text-dependent speaker recognition, but 2D approaches are highly susceptible to variations in face orientation.  This work introduces 3LMNet, a novel end-to-end three-dimensional (3D) lip motion network that leverages sentence-level 3D lip motion (S3DLM) for speaker recognition in both text-independent and text-dependent contexts.  A novel regional feedback module (RFM) is proposed to focus on different lip regions.  Furthermore, we incorporate prior knowledge of lip motion by merging landmark-level and frame-level features to enhance feature representation.  We also present two preprocessing methods\u2014coordinate transformation and face posture correction\u2014applied to the LSD-AV dataset, which comprises 68 speakers with 146 sentences per speaker.  Evaluation results demonstrate that 3LMNet significantly outperforms baseline models (LSTM, VGG-16, and ResNet-34) and achieves state-of-the-art performance compared to methods using 2D lip images and 3D facial data. The code is available at https://github.com/wutong18/Three-Dimensional-Lip-Motion-Network-for-Text-Independent-Speaker-Recognition.\n"}, "2210.09345": {"original_text": "  Relation Extraction (RE) has attracted increasing attention, but current RE\nevaluation is limited to in-domain evaluation setups. Little is known on how\nwell a RE system fares in challenging, but realistic out-of-distribution\nevaluation setups. To address this gap, we propose CrossRE, a new,\nfreely-available cross-domain benchmark for RE, which comprises six distinct\ntext domains and includes multi-label annotations. An additional innovation is\nthat we release meta-data collected during annotation, to include explanations\nand flags of difficult instances. We provide an empirical evaluation with a\nstate-of-the-art model for relation classification. As the meta-data enables us\nto shed new light on the state-of-the-art model, we provide a comprehensive\nanalysis on the impact of difficult cases and find correlations between model\nand human annotations. Overall, our empirical investigation highlights the\ndifficulty of cross-domain RE. We release our dataset, to spur more research in\nthis direction.\n", "rewritten_text": "Relation extraction (RE) has garnered increasing attention, yet current RE evaluation is limited to in-domain settings.  The performance of RE systems in challenging, yet realistic, out-of-distribution scenarios remains largely unexplored. To address this gap, we introduce CrossRE, a new, freely available cross-domain benchmark for RE.  CrossRE comprises six distinct text domains and features multi-label annotations.  Furthermore, we release the metadata collected during annotation, including explanations and flags for difficult instances.  We evaluate a state-of-the-art relation classification model on CrossRE, leveraging the metadata to analyze the model's performance. This analysis sheds light on the impact of difficult instances and reveals correlations between model and human annotations. Our empirical investigation highlights the significant challenges of cross-domain RE.  We release our dataset to foster further research in this area.\n"}, "1006.2734": {"original_text": "  A difficult problem in clustering is how to handle data with a manifold\nstructure, i.e. data that is not shaped in the form of compact clouds of\npoints, forming arbitrary shapes or paths embedded in a high-dimensional space.\nIn this work we introduce the Penalized k-Nearest-Neighbor-Graph (PKNNG) based\nmetric, a new tool for evaluating distances in such cases. The new metric can\nbe used in combination with most clustering algorithms. The PKNNG metric is\nbased on a two-step procedure: first it constructs the k-Nearest-Neighbor-Graph\nof the dataset of interest using a low k-value and then it adds edges with an\nexponentially penalized weight for connecting the sub-graphs produced by the\nfirst step. We discuss several possible schemes for connecting the different\nsub-graphs. We use three artificial datasets in four different embedding\nsituations to evaluate the behavior of the new metric, including a comparison\namong different clustering methods. We also evaluate the new metric in a real\nworld application, clustering the MNIST digits dataset. In all cases the PKNNG\nmetric shows promising clustering results.\n", "rewritten_text": "Clustering data with manifold structure\u2014data forming arbitrary shapes or paths embedded in high-dimensional space rather than compact point clouds\u2014presents a significant challenge.  This work introduces the Penalized k-Nearest-Neighbor-Graph (PKNNG) metric, a novel tool for evaluating distances in such datasets.  Compatible with most clustering algorithms, the PKNNG metric employs a two-step procedure: first, it constructs a k-Nearest-Neighbor-Graph (k-NN graph) using a low k-value; second, it connects the resulting sub-graphs with exponentially penalized edge weights.  Several schemes for connecting these sub-graphs are discussed.  The metric's performance is evaluated using three artificial datasets in four embedding scenarios, comparing various clustering methods.  Furthermore, we demonstrate its efficacy on a real-world application: clustering the MNIST digits dataset.  In all cases, the PKNNG metric yields promising clustering results.\n"}, "1804.01422": {"original_text": "  In this paper, we propose a simple but effective semantic-based aggregation\n(SBA) method. The proposed SBA utilizes the discriminative filters of deep\nconvolutional layers as semantic detectors. Moreover, we propose the effective\nunsupervised strategy to select some semantic detectors to generate the\n\"probabilistic proposals\", which highlight certain discriminative pattern of\nobjects and suppress the noise of background. The final global SBA\nrepresentation could then be acquired by aggregating the regional\nrepresentations weighted by the selected \"probabilistic proposals\"\ncorresponding to various semantic content. Our unsupervised SBA is easy to\ngeneralize and achieves excellent performance on various tasks. We conduct\ncomprehensive experiments and show that our unsupervised SBA outperforms the\nstate-of-the-art unsupervised and supervised aggregation methods on image\nretrieval, place recognition and cloud classification.\n", "rewritten_text": "This paper presents a simple yet effective semantic-based aggregation (SBA) method.  Our SBA leverages the discriminative filters of deep convolutional layers as semantic detectors.  Furthermore, we introduce an effective unsupervised strategy for selecting these detectors to generate probabilistic proposals, which highlight object patterns while suppressing background noise.  The final global SBA representation is obtained by aggregating regional representations weighted by the selected probabilistic proposals, each corresponding to distinct semantic content.  This unsupervised SBA method is readily generalizable and achieves excellent performance across diverse tasks.  Comprehensive experiments demonstrate that our approach surpasses state-of-the-art unsupervised and supervised aggregation methods in image retrieval, place recognition, and cloud classification.\n"}, "2109.00881": {"original_text": "  There is a growing interest in product aesthetics analytics and design.\nHowever, the lack of available large-scale data that covers various variables\nand information is one of the biggest challenges faced by analysts and\nresearchers. In this paper, we present our multidisciplinary initiative of\ndeveloping a comprehensive automotive dataset from different online sources and\nformats. Specifically, the created dataset contains 1.4 million images from 899\ncar models and their corresponding model specifications and sales information\nover more than ten years in the UK market. Our work makes significant\ncontributions to: (i) research and applications in the automotive industry;\n(ii) big data creation and sharing; (iii) database design; and (iv) data\nfusion. Apart from our motivation, technical details and data structure, we\nfurther present three simple examples to demonstrate how our data can be used\nin business research and applications.\n", "rewritten_text": "Interest in product aesthetics analytics and design is growing rapidly.  However, a significant challenge for analysts and researchers is the scarcity of large-scale datasets encompassing diverse variables and information. This paper presents a multidisciplinary initiative to develop a comprehensive automotive dataset compiled from various online sources and formats.  This dataset contains 1.4 million images of 899 car models, along with their specifications and sales data spanning over a decade in the UK market.  Our work contributes significantly to: (i) automotive industry research and applications; (ii) big data creation and sharing; (iii) database design; and (iv) data fusion.  Beyond outlining our motivation and technical details, including data structure, we illustrate the dataset's utility with three examples of its application in business research.\n"}, "1611.09007": {"original_text": "  Hyperspectral imaging sensors are becoming increasingly popular in robotics\napplications such as agriculture and mining, and allow per-pixel thematic\nclassification of materials in a scene based on their unique spectral\nsignatures. Recently, convolutional neural networks have shown remarkable\nperformance for classification tasks, but require substantial amounts of\nlabelled training data. This data must sufficiently cover the variability\nexpected to be encountered in the environment. For hyperspectral data, one of\nthe main variations encountered outdoors is due to incident illumination, which\ncan change in spectral shape and intensity depending on the scene geometry. For\nexample, regions occluded from the sun have a lower intensity and their\nincident irradiance skewed towards shorter wavelengths.\n  In this work, a data augmentation strategy based on relighting is used during\ntraining of a hyperspectral convolutional neural network. It allows training to\noccur in the outdoor environment given only a small labelled region, which does\nnot need to sufficiently represent the geometric variability of the entire\nscene. This is important for applications where obtaining large amounts of\ntraining data is labourious, hazardous or difficult, such as labelling pixels\nwithin shadows. Radiometric normalisation approaches for pre-processing the\nhyperspectral data are analysed and it is shown that methods based on the raw\npixel data are sufficient to be used as input for the classifier. This removes\nthe need for external hardware such as calibration boards, which can restrict\nthe application of hyperspectral sensors in robotics applications. Experiments\nto evaluate the classification system are carried out on two datasets captured\nfrom a field-based platform.\n", "rewritten_text": "Hyperspectral imaging sensors are increasingly popular in robotics applications, such as agriculture and mining, enabling per-pixel thematic classification of materials based on their unique spectral signatures.  While convolutional neural networks (CNNs) have demonstrated remarkable performance in classification tasks, they require substantial labeled training data that adequately represents the environmental variability.  A significant source of variability in outdoor hyperspectral data is incident illumination, which varies in both spectral shape and intensity depending on scene geometry.  For instance, shaded regions exhibit lower intensity and a spectral shift toward shorter wavelengths.\n\nThis work presents a data augmentation strategy employing relighting during CNN training. This allows training with only a small labeled region, eliminating the need for comprehensive representation of the scene's geometric variability. This is crucial for applications where acquiring extensive labeled data is laborious, hazardous, or impractical, such as pixel-level labeling in shadowed areas.  We analyze various radiometric normalization preprocessing techniques and demonstrate that methods operating directly on raw pixel data are sufficient for classifier input. This eliminates the need for external hardware, such as calibration boards, thereby enhancing the applicability of hyperspectral sensors in robotics.  The classification system's performance is evaluated using two datasets acquired from a field-based platform.\n"}, "1905.00996": {"original_text": "  In this work, we propose a novel framework named Region-Aware Network\n(RANet), which learns the ability of anti-confusing in case of heavy occlusion,\nnearby person and symmetric appearance, for human pose estimation.\nSpecifically, the proposed method addresses three key aspects, i.e., data\naugmentation, feature learning and prediction fusion, respectively. First, we\npropose Parsing-based Data Augmentation (PDA) to generate abundant data that\nsynthesizes confusing textures. Second, we not only propose a Feature Pyramid\nStem (FPS) to learn stronger low-level features in lower stage; but also\nincorporate an Effective Region Extraction (ERE) module to excavate better\ntarget-specific features. Third, we introduce Cascade Voting Fusion (CVF) to\nexplicitly exclude the inferior predictions and fuse the rest effective\npredictions for the final pose estimation. Extensive experimental results on\ntwo popular benchmarks, i.e. MPII and LSP, demonstrate the effectiveness of our\nmethod against the state-of-the-art competitors. Especially on\neasily-confusable joints, our method makes significant improvement.\n", "rewritten_text": "This work proposes Region-Aware Network (RANet), a novel framework for human pose estimation that mitigates the challenges of heavy occlusion, nearby persons, and symmetric appearances.  The method addresses three key aspects: data augmentation, feature learning, and prediction fusion.  First, we introduce Parsing-based Data Augmentation (PDA) to generate synthetic data containing confusing textures. Second, we employ a Feature Pyramid Stem (FPS) to enhance low-level feature learning and an Effective Region Extraction (ERE) module to improve target-specific feature extraction. Third, Cascade Voting Fusion (CVF) explicitly filters out poor predictions and fuses the remaining predictions for improved pose estimation accuracy.  Extensive experiments on the MPII and LSP benchmarks demonstrate state-of-the-art performance, with significant improvements observed, particularly on easily confusable joints.\n"}, "2304.00025": {"original_text": "  After the pandemic, artificial intelligence (AI) powered support for mental\nhealth care has become increasingly important. The breadth and complexity of\nsignificant challenges required to provide adequate care involve: (a)\nPersonalized patient understanding, (b) Safety-constrained and medically\nvalidated chatbot patient interactions, and (c) Support for continued\nfeedback-based refinements in design using chatbot-patient interactions. We\npropose Alleviate, a chatbot designed to assist patients suffering from mental\nhealth challenges with personalized care and assist clinicians with\nunderstanding their patients better. Alleviate draws from an array of publicly\navailable clinically valid mental-health texts and databases, allowing\nAlleviate to make medically sound and informed decisions. In addition,\nAlleviate's modular design and explainable decision-making lends itself to\nrobust and continued feedback-based refinements to its design. In this paper,\nwe explain the different modules of Alleviate and submit a short video\ndemonstrating Alleviate's capabilities to help patients and clinicians\nunderstand each other better to facilitate optimal care strategies.\n", "rewritten_text": "The COVID-19 pandemic underscored the critical need for AI-powered support in mental healthcare.  Addressing this need requires overcoming significant challenges, including: (a) personalized patient understanding; (b) safe and medically validated chatbot interactions; and (c) continuous feedback-driven design refinement based on chatbot-patient interactions.  This paper introduces Alleviate, a chatbot designed to provide personalized care for patients with mental health challenges and to enhance clinician understanding of their patients.  Alleviate leverages publicly available, clinically validated mental health texts and databases to inform its medically sound decision-making.  Furthermore, its modular design and explainable AI facilitate robust, ongoing feedback-driven refinement.  This paper details Alleviate's modular architecture and includes a supplementary video demonstrating its capabilities in improving patient-clinician communication and facilitating optimal care strategies.\n"}, "1204.2294": {"original_text": "  This paper present our new intensity chromaticity space-based feature\ndetection and matching algorithm. This approach utilizes hybridization of\nwireless local area network and camera internal sensor which to receive signal\nstrength from a access point and the same time retrieve interest point\ninformation from hallways. This information is combined by model fitting\napproach in order to find the absolute of user target position. No conventional\nsearching algorithm is required, thus it is expected reducing the computational\ncomplexity. Finally we present pre-experimental results to illustrate the\nperformance of the localization system for an indoor environment set-up.\n", "rewritten_text": "This paper presents a novel intensity chromaticity space-based feature detection and matching algorithm for indoor localization.  The approach hybridizes wireless local area network (WLAN) signal strength data from an access point with interest point information from a camera's internal sensor, acquired simultaneously within hallways.  A model-fitting approach combines this information to determine the absolute position of the target user.  This eliminates the need for conventional search algorithms, thereby reducing computational complexity.  Preliminary experimental results illustrate the system's performance in an indoor environment.\n"}, "2410.09595": {"original_text": "  Deformable image registration remains a fundamental task in clinical\npractice, yet solving registration problems involving complex deformations\nremains challenging. Current deep learning-based registration methods employ\ncontinuous deformation to model large deformations, which often suffer from\naccumulated registration errors and interpolation inaccuracies. Moreover,\nachieving satisfactory results with these frameworks typically requires a large\nnumber of cascade stages, demanding substantial computational resources.\nTherefore, we propose a novel approach, the field refinement framework\n(FiRework), tailored for unsupervised deformable registration, aiming to\naddress these challenges. In FiRework, we redesign the continuous deformation\nframework to mitigate the aforementioned errors. Notably, our FiRework requires\nonly one level of recursion during training and supports continuous inference,\noffering improved efficacy compared to continuous deformation frameworks. We\nconducted experiments on two brain MRI datasets, enhancing two existing\ndeformable registration networks with FiRework. The experimental results\ndemonstrate the superior performance of our proposed framework in deformable\nregistration. The code is publicly available at\nhttps://github.com/ZAX130/FiRework.\n", "rewritten_text": "Deformable image registration is a fundamental task in clinical practice, but registering images with complex deformations remains challenging.  Current deep learning-based methods often use continuous deformation models for large deformations, leading to accumulated registration errors and interpolation inaccuracies.  Furthermore, these methods typically require numerous cascaded stages, demanding significant computational resources.  To address these limitations, we propose FiRework, a novel unsupervised deformable registration framework.  FiRework re-designs the continuous deformation framework to mitigate accumulated errors.  Significantly, FiRework requires only a single recursion level during training and supports continuous inference, resulting in improved efficiency compared to existing continuous deformation methods.  We evaluated FiRework on two brain MRI datasets by enhancing two existing deformable registration networks.  Experimental results demonstrate FiRework's superior performance. The code is publicly available at https://github.com/ZAX130/FiRework.\n"}, "2401.07770": {"original_text": "  Computer vision tasks typically involve describing what is present in an\nimage (e.g. classification, detection, segmentation, and captioning). We study\na visual common sense task that requires understanding what is not present.\nSpecifically, given an image (e.g. of a living room) and name of an object\n(\"cushion\"), a vision system is asked to predict semantically-meaningful\nregions (masks or bounding boxes) in the image where that object could be\nplaced or is likely be placed by humans (e.g. on the sofa). We call this task:\nSemantic Placement (SP) and believe that such common-sense visual understanding\nis critical for assitive robots (tidying a house), and AR devices\n(automatically rendering an object in the user's space). Studying the invisible\nis hard. Datasets for image description are typically constructed by curating\nrelevant images and asking humans to annotate the contents of the image;\nneither of those two steps are straightforward for objects not present in the\nimage. We overcome this challenge by operating in the opposite direction: we\nstart with an image of an object in context from web, and then remove that\nobject from the image via inpainting. This automated pipeline converts\nunstructured web data into a dataset comprising pairs of images with/without\nthe object. Using this, we collect a novel dataset, with ${\\sim}1.3$M images\nacross $9$ object categories, and train a SP prediction model called CLIP-UNet.\nCLIP-UNet outperforms existing VLMs and baselines that combine semantic priors\nwith object detectors on real-world and simulated images. In our user studies,\nwe find that the SP masks predicted by CLIP-UNet are favored $43.7\\%$ and\n$31.3\\%$ times when comparing against the $4$ SP baselines on real and\nsimulated images. In addition, we demonstrate leveraging SP mask predictions\nfrom CLIP-UNet enables downstream applications like building tidying robots in\nindoor environments.\n", "rewritten_text": "Computer vision typically focuses on identifying what *is* present in an image (e.g., classification, detection, segmentation, captioning).  This work explores a novel visual common sense task: understanding what is *not* present.  Specifically, given an image (e.g., a living room) and an object name (\"cushion\"), the system predicts semantically meaningful regions (masks or bounding boxes) where that object could plausibly be placed by a human (e.g., on a sofa). We call this task Semantic Placement (SP) and posit that this common-sense understanding is crucial for applications such as assistive robots (e.g., tidying a house) and augmented reality (AR) devices (e.g., automatically rendering objects in a user's space).\n\nCreating datasets for this task is challenging.  Traditional image description datasets rely on human annotation of *present* objects, a process unsuitable for objects that are absent.  We address this by employing a reverse approach: starting with web images showing an object in context, we then remove the object using inpainting. This automated pipeline transforms unstructured web data into a dataset of image pairs\u2014one with, and one without, the object.  This yielded a novel dataset containing approximately 1.3 million images across nine object categories.\n\nWe trained a SP prediction model, CLIP-UNet, on this dataset.  CLIP-UNet outperforms existing vision-language models (VLMs) and baselines that combine semantic priors with object detectors on both real-world and simulated images.  User studies revealed that CLIP-UNet's SP mask predictions were preferred 43.7% and 31.3% of the time compared to four baselines on real and simulated images, respectively.  Furthermore, we demonstrate the application of CLIP-UNet's SP mask predictions to downstream tasks, such as building tidying robots for indoor environments.\n"}, "2112.11454": {"original_text": "  Generating digital humans that move realistically has many applications and\nis widely studied, but existing methods focus on the major limbs of the body,\nignoring the hands and head. Hands have been separately studied, but the focus\nhas been on generating realistic static grasps of objects. To synthesize\nvirtual characters that interact with the world, we need to generate full-body\nmotions and realistic hand grasps simultaneously. Both sub-problems are\nchallenging on their own and, together, the state-space of poses is\nsignificantly larger, the scales of hand and body motions differ, and the\nwhole-body posture and the hand grasp must agree, satisfy physical constraints,\nand be plausible. Additionally, the head is involved because the avatar must\nlook at the object to interact with it. For the first time, we address the\nproblem of generating full-body, hand and head motions of an avatar grasping an\nunknown object. As input, our method, called GOAL, takes a 3D object, its\nposition, and a starting 3D body pose and shape. GOAL outputs a sequence of\nwhole-body poses using two novel networks. First, GNet generates a goal\nwhole-body grasp with a realistic body, head, arm, and hand pose, as well as\nhand-object contact. Second, MNet generates the motion between the starting and\ngoal pose. This is challenging, as it requires the avatar to walk towards the\nobject with foot-ground contact, orient the head towards it, reach out, and\ngrasp it with a realistic hand pose and hand-object contact. To achieve this,\nthe networks exploit a representation that combines SMPL-X body parameters and\n3D vertex offsets. We train and evaluate GOAL, both qualitatively and\nquantitatively, on the GRAB dataset. Results show that GOAL generalizes well to\nunseen objects, outperforming baselines. GOAL takes a step towards synthesizing\nrealistic full-body object grasping.\n", "rewritten_text": "Realistic digital human motion synthesis, while extensively studied, currently focuses primarily on major limbs, neglecting the nuanced movements of hands and head.  While hand motion has been investigated separately, the emphasis has been on static grasps.  Synthesizing virtual characters interacting realistically with their environment requires simultaneous generation of full-body motion and dynamic hand grasps. This is challenging because the combined pose state-space is significantly larger, the scales of hand and body movements differ, and whole-body posture, hand grasp, and physical constraints must be mutually consistent and plausible.  Furthermore, head orientation is crucial for realistic interaction, requiring the avatar to visually attend to the manipulated object.\n\nThis paper introduces GOAL, a novel method for generating full-body, hand, and head motion sequences for an avatar grasping an unknown object.  Given a 3D object, its position, and an initial 3D body pose and shape, GOAL outputs a sequence of whole-body poses using two neural networks:  GNet and MNet. GNet generates a goal pose encompassing a realistic body, head, arm, and hand configuration, including accurate hand-object contact. MNet then generates the motion sequence connecting the initial and goal poses. This is computationally demanding, requiring the avatar to navigate towards the object (maintaining foot-ground contact), orient its head, reach, and grasp the object with a realistic hand pose and contact.  The networks leverage a representation combining SMPL-X body parameters and 3D vertex offsets.\n\nGOAL was trained and evaluated qualitatively and quantitatively on the GRAB dataset.  Results demonstrate strong generalization to unseen objects, surpassing existing baselines.  This work represents a significant advance towards synthesizing realistic full-body object grasping in digital humans.\n"}, "2103.05861": {"original_text": "  Neural network pruning is an essential approach for reducing the\ncomputational complexity of deep models so that they can be well deployed on\nresource-limited devices. Compared with conventional methods, the recently\ndeveloped dynamic pruning methods determine redundant filters variant to each\ninput instance which achieves higher acceleration. Most of the existing methods\ndiscover effective sub-networks for each instance independently and do not\nutilize the relationship between different inputs. To maximally excavate\nredundancy in the given network architecture, this paper proposes a new\nparadigm that dynamically removes redundant filters by embedding the manifold\ninformation of all instances into the space of pruned networks (dubbed as\nManiDP). We first investigate the recognition complexity and feature similarity\nbetween images in the training set. Then, the manifold relationship between\ninstances and the pruned sub-networks will be aligned in the training\nprocedure. The effectiveness of the proposed method is verified on several\nbenchmarks, which shows better performance in terms of both accuracy and\ncomputational cost compared to the state-of-the-art methods. For example, our\nmethod can reduce 55.3% FLOPs of ResNet-34 with only 0.57% top-1 accuracy\ndegradation on ImageNet.\n", "rewritten_text": "Neural network pruning is crucial for deploying deep models on resource-constrained devices by reducing computational complexity.  Unlike conventional methods, recently developed dynamic pruning techniques achieve greater acceleration by identifying redundant filters that vary for each input instance.  However, most existing methods independently discover effective sub-networks for each instance, neglecting inter-instance relationships.  To maximize redundancy exploitation, this paper proposes ManiDP, a novel paradigm that dynamically removes redundant filters by embedding the manifold information of all instances into the pruned network space.  We first analyze the recognition complexity and feature similarity among training images.  The training procedure then aligns the manifold relationship between instances and the pruned sub-networks.  Experiments on several benchmark datasets demonstrate superior performance, achieving both higher accuracy and lower computational cost than state-of-the-art methods.  For instance, ManiDP reduces ResNet-34's FLOPs by 55.3% with only a 0.57% top-1 accuracy drop on ImageNet.\n"}, "2002.01127": {"original_text": "  How to generate descriptions from structured data organized in tables?\nExisting approaches using neural encoder-decoder models often suffer from\nlacking diversity. We claim that an open set of templates is crucial for\nenriching the phrase constructions and realizing varied generations. Learning\nsuch templates is prohibitive since it often requires a large paired <table,\ndescription> corpus, which is seldom available. This paper explores the problem\nof automatically learning reusable \"templates\" from paired and non-paired data.\nWe propose the variational template machine (VTM), a novel method to generate\ntext descriptions from data tables. Our contributions include: a) we carefully\ndevise a specific model architecture and losses to explicitly disentangle text\ntemplate and semantic content information, in the latent spaces, and b)we\nutilize both small parallel data and large raw text without aligned tables to\nenrich the template learning. Experiments on datasets from a variety of\ndifferent domains show that VTM is able to generate more diversely while\nkeeping a good fluency and quality.\n", "rewritten_text": "Generating diverse descriptions from tabular structured data remains a challenge.  Existing neural encoder-decoder approaches often lack descriptive variety.  We hypothesize that an open set of text templates is crucial for enriching phrase construction and generating diverse outputs.  However, learning such templates is typically infeasible due to the scarcity of large paired <table, description> corpora.  This paper addresses the problem of automatically learning reusable templates from both paired and unpaired data.  We propose the Variational Template Machine (VTM), a novel method for generating text descriptions from data tables.  Our contributions are twofold: (a) we design a model architecture and loss function to explicitly disentangle textual templates and semantic content in the latent space; and (b) we leverage both small parallel corpora and large unaligned text corpora to enhance template learning.  Experiments across diverse domains demonstrate that VTM generates more diverse descriptions while maintaining high fluency and quality.\n"}, "2010.03725": {"original_text": "  We present a new benchmark dataset called PARADE for paraphrase\nidentification that requires specialized domain knowledge. PARADE contains\nparaphrases that overlap very little at the lexical and syntactic level but are\nsemantically equivalent based on computer science domain knowledge, as well as\nnon-paraphrases that overlap greatly at the lexical and syntactic level but are\nnot semantically equivalent based on this domain knowledge. Experiments show\nthat both state-of-the-art neural models and non-expert human annotators have\npoor performance on PARADE. For example, BERT after fine-tuning achieves an F1\nscore of 0.709, which is much lower than its performance on other paraphrase\nidentification datasets. PARADE can serve as a resource for researchers\ninterested in testing models that incorporate domain knowledge. We make our\ndata and code freely available.\n", "rewritten_text": "We introduce PARADE, a new benchmark dataset for paraphrase identification requiring specialized computer science domain knowledge.  PARADE comprises semantically equivalent paraphrases with minimal lexical and syntactic overlap, and non-paraphrases with significant lexical and syntactic overlap but lacking semantic equivalence (as judged by domain experts).  Experiments demonstrate that both state-of-the-art neural models and non-expert human annotators exhibit poor performance on PARADE. For instance, fine-tuned BERT achieves a relatively low F1 score of 0.709, significantly underperforming on other paraphrase identification datasets.  PARADE thus serves as a valuable resource for evaluating models incorporating domain-specific knowledge.  Our data and code are freely available.\n"}, "2307.12067": {"original_text": "  We introduce Replay, a collection of multi-view, multi-modal videos of humans\ninteracting socially. Each scene is filmed in high production quality, from\ndifferent viewpoints with several static cameras, as well as wearable action\ncameras, and recorded with a large array of microphones at different positions\nin the room. Overall, the dataset contains over 4000 minutes of footage and\nover 7 million timestamped high-resolution frames annotated with camera poses\nand partially with foreground masks. The Replay dataset has many potential\napplications, such as novel-view synthesis, 3D reconstruction, novel-view\nacoustic synthesis, human body and face analysis, and training generative\nmodels. We provide a benchmark for training and evaluating novel-view\nsynthesis, with two scenarios of different difficulty. Finally, we evaluate\nseveral baseline state-of-the-art methods on the new benchmark.\n", "rewritten_text": "We introduce Replay, a multi-view, multi-modal video dataset of human social interactions.  Each scene is professionally filmed from multiple viewpoints using static and wearable cameras, and recorded with a spatially diverse microphone array.  The dataset comprises over 4000 minutes of footage, exceeding 7 million high-resolution, timestamped frames annotated with camera poses and partial foreground masks.  Replay's potential applications include novel-view synthesis, 3D reconstruction, novel-view acoustic synthesis, human body and face analysis, and generative model training.  We provide a benchmark, featuring two scenarios of varying difficulty, for training and evaluating novel-view synthesis methods.  Finally, we evaluate several state-of-the-art baselines on this new benchmark.\n"}, "1703.10645": {"original_text": "  We propose a novel method called the Relevance Subject Machine (RSM) to solve\nthe person re-identification (re-id) problem. RSM falls under the category of\nBayesian sparse recovery algorithms and uses the sparse representation of the\ninput video under a pre-defined dictionary to identify the subject in the\nvideo. Our approach focuses on the multi-shot re-id problem, which is the\nprevalent problem in many video analytics applications. RSM captures the\nessence of the multi-shot re-id problem by constraining the support of the\nsparse codes for each input video frame to be the same. Our proposed approach\nis also robust enough to deal with time varying outliers and occlusions by\nintroducing a sparse, non-stationary noise term in the model error. We provide\na novel Variational Bayesian based inference procedure along with an intuitive\ninterpretation of the proposed update rules. We evaluate our approach over\nseveral commonly used re-id datasets and show superior performance over current\nstate-of-the-art algorithms. Specifically, for ILIDS-VID, a recent large scale\nre-id dataset, RSM shows significant improvement over all published approaches,\nachieving an 11.5% (absolute) improvement in rank 1 accuracy over the closest\ncompeting algorithm considered.\n", "rewritten_text": "We propose a novel Bayesian sparse recovery algorithm, the Relevance Subject Machine (RSM), to address the person re-identification (re-ID) problem.  RSM leverages sparse representation of input video frames under a pre-defined dictionary to identify subjects.  Focusing on the prevalent multi-shot re-ID problem common in video analytics, RSM enforces consistent sparse code support across all frames of an input video.  The model's robustness to time-varying outliers and occlusions is ensured by incorporating a sparse, non-stationary noise term.  We present a novel variational Bayesian inference procedure, providing an intuitive interpretation of the update rules.  Evaluated on several standard re-ID datasets, RSM demonstrates superior performance compared to state-of-the-art algorithms. Notably, on the large-scale ILIDS-VID dataset, RSM achieves a significant 11.5% absolute improvement in rank-1 accuracy over the best competing algorithm.\n"}, "1908.08498": {"original_text": "  We focus on multi-modal fusion for egocentric action recognition, and propose\na novel architecture for multi-modal temporal-binding, i.e. the combination of\nmodalities within a range of temporal offsets. We train the architecture with\nthree modalities -- RGB, Flow and Audio -- and combine them with mid-level\nfusion alongside sparse temporal sampling of fused representations. In contrast\nwith previous works, modalities are fused before temporal aggregation, with\nshared modality and fusion weights over time. Our proposed architecture is\ntrained end-to-end, outperforming individual modalities as well as late-fusion\nof modalities.\n  We demonstrate the importance of audio in egocentric vision, on per-class\nbasis, for identifying actions as well as interacting objects. Our method\nachieves state of the art results on both the seen and unseen test sets of the\nlargest egocentric dataset: EPIC-Kitchens, on all metrics using the public\nleaderboard.\n", "rewritten_text": "This paper introduces a novel architecture for multi-modal temporal binding in egocentric action recognition.  We focus on fusing RGB, optical flow, and audio modalities, employing mid-level fusion with sparse temporal sampling of the fused representations.  Unlike previous approaches, we fuse modalities *before* temporal aggregation, sharing modality and fusion weights across time.  This end-to-end trained architecture significantly outperforms both individual modalities and late fusion methods.  We demonstrate the importance of audio, particularly in per-class action and object interaction identification within egocentric vision.  Our method achieves state-of-the-art results on both seen and unseen test sets of the EPIC-Kitchens dataset, surpassing all metrics on the public leaderboard.\n"}, "1611.09813": {"original_text": "  We propose a CNN-based approach for 3D human body pose estimation from single\nRGB images that addresses the issue of limited generalizability of models\ntrained solely on the starkly limited publicly available 3D pose data. Using\nonly the existing 3D pose data and 2D pose data, we show state-of-the-art\nperformance on established benchmarks through transfer of learned features,\nwhile also generalizing to in-the-wild scenes. We further introduce a new\ntraining set for human body pose estimation from monocular images of real\nhumans that has the ground truth captured with a multi-camera marker-less\nmotion capture system. It complements existing corpora with greater diversity\nin pose, human appearance, clothing, occlusion, and viewpoints, and enables an\nincreased scope of augmentation. We also contribute a new benchmark that covers\noutdoor and indoor scenes, and demonstrate that our 3D pose dataset shows\nbetter in-the-wild performance than existing annotated data, which is further\nimproved in conjunction with transfer learning from 2D pose data. All in all,\nwe argue that the use of transfer learning of representations in tandem with\nalgorithmic and data contributions is crucial for general 3D body pose\nestimation.\n", "rewritten_text": "We propose a CNN-based approach for 3D human body pose estimation from single RGB images, addressing the limited generalizability of models trained solely on the scarce publicly available 3D pose data.  Leveraging existing 2D and 3D pose datasets, we achieve state-of-the-art performance on established benchmarks via transfer learning, demonstrating strong generalization to in-the-wild scenes.  Furthermore, we introduce a new training dataset for monocular 3D human pose estimation, captured using a markerless multi-camera motion capture system. This dataset complements existing corpora with increased diversity in pose, appearance, clothing, occlusion, and viewpoint, enabling broader data augmentation.  We also contribute a new benchmark encompassing indoor and outdoor scenes.  Our results demonstrate that this 3D pose dataset, further enhanced by transfer learning from 2D pose data, yields superior in-the-wild performance compared to existing annotated datasets.  In conclusion, we argue that combining algorithmic advancements with novel datasets and transfer learning is crucial for robust 3D human body pose estimation.\n"}, "2109.12338": {"original_text": "  Model binarization is an effective method of compressing neural networks and\naccelerating their inference process. However, a significant performance gap\nstill exists between the 1-bit model and the 32-bit one. The empirical study\nshows that binarization causes a great loss of information in the forward and\nbackward propagation. We present a novel Distribution-sensitive Information\nRetention Network (DIR-Net) that retains the information in the forward and\nbackward propagation by improving internal propagation and introducing external\nrepresentations. The DIR-Net mainly relies on three technical contributions:\n(1) Information Maximized Binarization (IMB): minimizing the information loss\nand the binarization error of weights/activations simultaneously by weight\nbalance and standardization; (2) Distribution-sensitive Two-stage Estimator\n(DTE): retaining the information of gradients by distribution-sensitive soft\napproximation by jointly considering the updating capability and accurate\ngradient; (3) Representation-align Binarization-aware Distillation (RBD):\nretaining the representation information by distilling the representations\nbetween full-precision and binarized networks. The DIR-Net investigates both\nforward and backward processes of BNNs from the unified information\nperspective, thereby providing new insight into the mechanism of network\nbinarization. The three techniques in our DIR-Net are versatile and effective\nand can be applied in various structures to improve BNNs. Comprehensive\nexperiments on the image classification and objective detection tasks show that\nour DIR-Net consistently outperforms the state-of-the-art binarization\napproaches under mainstream and compact architectures, such as ResNet, VGG,\nEfficientNet, DARTS, and MobileNet. Additionally, we conduct our DIR-Net on\nreal-world resource-limited devices which achieves 11.1x storage saving and\n5.4x speedup.\n", "rewritten_text": "Model binarization is an effective method for compressing neural networks and accelerating inference.  However, a significant performance gap remains between 1-bit and 32-bit models.  Empirical studies show that binarization causes substantial information loss during both forward and backward propagation.  To address this, we introduce Distribution-sensitive Information Retention Network (DIR-Net), which preserves information during both propagation phases by enhancing internal propagation and incorporating external representations.  DIR-Net's key contributions are threefold: (1) Information Maximized Binarization (IMB), which simultaneously minimizes information loss and binarization error in weights and activations through weight balancing and standardization; (2) Distribution-sensitive Two-stage Estimator (DTE), which preserves gradient information via a distribution-sensitive soft approximation that balances update capability and gradient accuracy; and (3) Representation-align Binarization-aware Distillation (RBD), which preserves representational information by distilling knowledge between full-precision and binarized networks.  DIR-Net offers a unified information-theoretic perspective on the forward and backward passes of binary neural networks (BNNs), providing novel insights into the binarization mechanism.  Its three techniques are versatile and effective, applicable to various network architectures to improve BNN performance.  Comprehensive experiments on image classification and object detection tasks demonstrate that DIR-Net consistently outperforms state-of-the-art binarization methods across mainstream and compact architectures, including ResNet, VGG, EfficientNet, DARTS, and MobileNet.  Furthermore, deployment on resource-constrained devices yielded an 11.1x storage reduction and a 5.4x speedup.\n"}, "2310.07343": {"original_text": "  Although large language models (LLMs) are impressive in solving various\ntasks, they can quickly be outdated after deployment. Maintaining their\nup-to-date status is a pressing concern in the current era. This paper provides\na comprehensive review of recent advances in aligning LLMs with the\never-changing world knowledge without re-training from scratch. We categorize\nresearch works systemically and provide in-depth comparisons and discussion. We\nalso discuss existing challenges and highlight future directions to facilitate\nresearch in this field. We release the paper list at\nhttps://github.com/hyintell/awesome-refreshing-llms\n", "rewritten_text": "Large language models (LLMs), while impressive in their ability to solve diverse tasks, rapidly become outdated after deployment.  Maintaining their currency is a critical challenge. This paper comprehensively reviews recent advancements in aligning LLMs with evolving world knowledge without requiring complete retraining.  We systematically categorize relevant research, providing in-depth comparisons and analysis.  Furthermore, we discuss existing challenges and identify promising future research directions.  A curated list of papers is available at https://github.com/hyintell/awesome-refreshing-llms.\n"}, "2301.05792": {"original_text": "  Class-Incremental Learning (CIL) [40] trains classifiers under a strict\nmemory budget: in each incremental phase, learning is done for new data, most\nof which is abandoned to free space for the next phase. The preserved data are\nexemplars used for replaying. However, existing methods use a static and ad hoc\nstrategy for memory allocation, which is often sub-optimal. In this work, we\npropose a dynamic memory management strategy that is optimized for the\nincremental phases and different object classes. We call our method reinforced\nmemory management (RMM), leveraging reinforcement learning. RMM training is not\nnaturally compatible with CIL as the past, and future data are strictly\nnon-accessible during the incremental phases. We solve this by training the\npolicy function of RMM on pseudo CIL tasks, e.g., the tasks built on the data\nof the 0-th phase, and then applying it to target tasks. RMM propagates two\nlevels of actions: Level-1 determines how to split the memory between old and\nnew classes, and Level-2 allocates memory for each specific class. In essence,\nit is an optimizable and general method for memory management that can be used\nin any replaying-based CIL method. For evaluation, we plug RMM into two\ntop-performing baselines (LUCIR+AANets and POD+AANets [30]) and conduct\nexperiments on three benchmarks (CIFAR-100, ImageNet-Subset, and\nImageNet-Full). Our results show clear improvements, e.g., boosting POD+AANets\nby 3.6%, 4.4%, and 1.9% in the 25-Phase settings of the above benchmarks,\nrespectively.\n", "rewritten_text": "Class-incremental learning (CIL) [40] trains classifiers under a strict memory budget.  Each incremental phase involves learning from new data, most of which is discarded to free space for subsequent phases.  The retained data\u2014exemplars\u2014are used for replay. However, existing methods employ a static and ad hoc memory allocation strategy, often resulting in suboptimal performance.  This work proposes a dynamic memory management strategy, reinforced memory management (RMM), optimized for incremental phases and individual object classes.  RMM leverages reinforcement learning.\n\nBecause RMM training is inherently incompatible with the sequential, non-accessible nature of past and future data in CIL, we train RMM's policy function on pseudo-CIL tasks\u2014tasks constructed from the data of the initial (0th) phase\u2014before applying it to target tasks.  RMM operates at two action levels: Level 1 determines the memory allocation split between old and new classes, while Level 2 allocates memory to individual classes.  Essentially, RMM provides a general, optimizable method for memory management applicable to any replay-based CIL method.\n\nWe evaluated RMM by integrating it into two high-performing baselines (LUCIR+AANets and POD+AANets [30]) and conducting experiments on three benchmark datasets (CIFAR-100, ImageNet-Subset, and ImageNet-Full).  Results demonstrate significant improvements; for example, in the 25-phase setting, POD+AANets performance increased by 3.6%, 4.4%, and 1.9% on CIFAR-100, ImageNet-Subset, and ImageNet-Full, respectively.\n"}, "1711.01362": {"original_text": "  An Unreliable news is any piece of information which is false or misleading,\ndeliberately spread to promote political, ideological and financial agendas.\nRecently the problem of unreliable news has got a lot of attention as the\nnumber instances of using news and social media outlets for propaganda have\nincreased rapidly. This poses a serious threat to society, which calls for\ntechnology to automatically and reliably identify unreliable news sources. This\npaper is an effort made in this direction to build systems for detecting\nunreliable news articles. In this paper, various NLP algorithms were built and\nevaluated on Unreliable News Data 2017 dataset. Variants of hierarchical\nattention networks (HAN) are presented for encoding and classifying news\narticles which achieve the best results of 0.944 ROC-AUC. Finally, Attention\nlayer weights are visualized to understand and give insight into the decisions\nmade by HANs. The results obtained are very promising and encouraging to deploy\nand use these systems in the real world to mitigate the problem of unreliable\nnews.\n", "rewritten_text": "Unreliable news, defined as false or misleading information deliberately disseminated to advance political, ideological, or financial agendas, has recently garnered significant attention.  The rapid proliferation of propaganda via news and social media outlets poses a serious societal threat, necessitating the development of technology capable of automatically and reliably identifying unreliable news sources.  This paper contributes to this effort by building and evaluating systems for detecting unreliable news articles.  We developed and evaluated various NLP algorithms on the Unreliable News Data 2017 dataset.  Variants of hierarchical attention networks (HANs) are presented for encoding and classifying news articles, achieving a best result of 0.944 ROC-AUC.  Finally, we visualize attention layer weights to gain insight into the decision-making process of the HANs.  The promising results encourage the deployment of these systems to mitigate the problem of unreliable news in real-world applications.\n"}, "1803.10464": {"original_text": "  The deficiency of segmentation labels is one of the main obstacles to\nsemantic segmentation in the wild. To alleviate this issue, we present a novel\nframework that generates segmentation labels of images given their image-level\nclass labels. In this weakly supervised setting, trained models have been known\nto segment local discriminative parts rather than the entire object area. Our\nsolution is to propagate such local responses to nearby areas which belong to\nthe same semantic entity. To this end, we propose a Deep Neural Network (DNN)\ncalled AffinityNet that predicts semantic affinity between a pair of adjacent\nimage coordinates. The semantic propagation is then realized by random walk\nwith the affinities predicted by AffinityNet. More importantly, the supervision\nemployed to train AffinityNet is given by the initial discriminative part\nsegmentation, which is incomplete as a segmentation annotation but sufficient\nfor learning semantic affinities within small image areas. Thus the entire\nframework relies only on image-level class labels and does not require any\nextra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with\nsegmentation labels generated by our method outperforms previous models trained\nwith the same level of supervision, and is even as competitive as those relying\non stronger supervision.\n", "rewritten_text": "Semantic segmentation in unconstrained environments is significantly hampered by a scarcity of segmentation labels.  To address this, we propose a novel framework that generates image segmentation labels from image-level class labels.  Weakly supervised models trained in this manner typically segment only locally discriminative parts of objects, rather than the entire object.  Our approach propagates these local responses to neighboring regions belonging to the same semantic entity.  To achieve this, we introduce AffinityNet, a deep neural network (DNN) that predicts the semantic affinity between pairs of adjacent image coordinates.  Semantic propagation is then performed using a random walk guided by AffinityNet's affinity predictions.  Crucially, AffinityNet's training is supervised by the initial, incomplete discriminative part segmentation; this imperfect segmentation is nonetheless sufficient for learning semantic affinities within small image regions.  Therefore, the entire framework requires only image-level class labels, eliminating the need for additional data or annotations.  On the PASCAL VOC 2012 dataset, a DNN trained using our generated segmentation labels outperforms previous weakly supervised models and achieves competitive results compared to those trained with stronger supervision.\n"}, "1902.07938": {"original_text": "  Named entity recognition (NER) is an important task in NLP, which is all the\nmore challenging in conversational domain with their noisy facets. Moreover,\nconversational texts are often available in limited amount, making supervised\ntasks infeasible. To learn from small data, strong inductive biases are\nrequired. Previous work relied on hand-crafted features to encode these biases\nuntil transfer learning emerges. Here, we explore a transfer learning method,\nnamely language model pretraining, on NER task in Indonesian conversational\ntexts. We utilize large unlabeled data (generic domain) to be transferred to\nconversational texts, enabling supervised training on limited in-domain data.\nWe report two transfer learning variants, namely supervised model fine-tuning\nand unsupervised pretrained LM fine-tuning. Our experiments show that both\nvariants outperform baseline neural models when trained on small data (100\nsentences), yielding an absolute improvement of 32 points of test F1 score.\nFurthermore, we find that the pretrained LM encodes part-of-speech information\nwhich is a strong predictor for NER.\n", "rewritten_text": "Named entity recognition (NER) is a crucial task in natural language processing (NLP), particularly challenging in the noisy conversational domain.  The limited availability of conversational text data often renders supervised training infeasible.  To address this data scarcity, strong inductive biases are necessary. While previous work relied on hand-crafted features to incorporate these biases, transfer learning offers a promising alternative.  This paper explores a transfer learning approach using language model pre-training for NER in Indonesian conversational text.  We leverage a large corpus of unlabeled generic-domain data to pre-train a language model, subsequently fine-tuning it on a limited amount of in-domain conversational data for supervised training.  We investigate two transfer learning variants: supervised model fine-tuning and unsupervised pre-trained language model (LM) fine-tuning.  Our experiments demonstrate that both variants significantly outperform baseline neural models trained on a small dataset (100 sentences), achieving an absolute improvement of 32 F1 points on the test set.  Furthermore, our analysis reveals that the pre-trained LM effectively encodes part-of-speech information, a strong predictor for NER performance.\n"}, "1504.0234": {"original_text": "  In this paper, we focus on the two key aspects of multiple target tracking\nproblem: 1) designing an accurate affinity measure to associate detections and\n2) implementing an efficient and accurate (near) online multiple target\ntracking algorithm. As the first contribution, we introduce a novel Aggregated\nLocal Flow Descriptor (ALFD) that encodes the relative motion pattern between a\npair of temporally distant detections using long term interest point\ntrajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust\naffinity measure for estimating the likelihood of matching detections\nregardless of the application scenarios. As another contribution, we present a\nNear-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is\nformulated as a data-association between targets and detections in a temporal\nwindow, that is performed repeatedly at every frame. While being efficient,\nNOMT achieves robustness via integrating multiple cues including ALFD metric,\ntarget dynamics, appearance similarity, and long term trajectory regularization\ninto the model. Our ablative analysis verifies the superiority of the ALFD\nmetric over the other conventional affinity metrics. We run a comprehensive\nexperimental evaluation on two challenging tracking datasets, KITTI and MOT\ndatasets. The NOMT method combined with ALFD metric achieves the best accuracy\nin both datasets with significant margins (about 10% higher MOTA) over the\nstate-of-the-arts.\n", "rewritten_text": "This paper addresses two key challenges in multiple target tracking: designing an accurate affinity measure for associating detections and developing an efficient and accurate near-online tracking algorithm.  First, we introduce a novel Aggregated Local Flow Descriptor (ALFD) that encodes the relative motion pattern between temporally distant detections using long-term interest point trajectories (IPTs).  Leveraging IPTs, ALFD provides a robust affinity measure for matching detections across diverse application scenarios.  Second, we present a Near-Online Multi-target Tracking (NOMT) algorithm.  Formulated as a data association problem within a temporal window, repeatedly solved at each frame, NOMT efficiently integrates multiple cues\u2014including the ALFD metric, target dynamics, appearance similarity, and long-term trajectory regularization\u2014to achieve robustness.  Ablative analysis demonstrates ALFD's superiority over conventional affinity metrics.  Comprehensive experiments on the challenging KITTI and MOT datasets show that NOMT, combined with ALFD, achieves state-of-the-art accuracy, exceeding existing methods by approximately 10% in MOTA.\n"}, "2405.07399": {"original_text": "  Weeds present a significant challenge in agriculture, causing yield loss and\nrequiring expensive control measures. Automatic weed detection using computer\nvision and deep learning offers a promising solution. However, conventional\ndeep learning methods often require large amounts of labelled training data,\nwhich can be costly and time-consuming to acquire. This paper introduces a\nnovel method for semi-supervised weed detection, comprising two main\ncomponents. Firstly, a multi-scale feature representation technique is employed\nto capture distinctive weed features across different scales. Secondly, we\npropose an adaptive pseudo-label assignment strategy, leveraging a small set of\nlabelled images during training. This strategy dynamically assigns confidence\nscores to pseudo-labels generated from unlabeled data. Additionally, our\napproach integrates epoch-corresponding and mixed pseudo-labels to further\nenhance the learning process. Experimental results on the COCO dataset and five\nprominent weed datasets -- CottonWeedDet12, CropAndWeed, Palmer amaranth,\nRadishWheat, and RoboWeedMap -- illustrate that our method achieves\nstate-of-the-art performance in weed detection, even with significantly less\nlabelled data compared to existing techniques. This approach holds the\npotential to alleviate the labelling burden and enhance the feasibility and\ndeployment speed of deep learning for weed detection in real-world agricultural\nscenarios.\n", "rewritten_text": "Weed control is a significant challenge in agriculture, leading to yield losses and expensive management practices.  Computer vision and deep learning offer a promising automated solution for weed detection. However, conventional deep learning approaches typically require extensive labeled training data, which is costly and time-consuming to obtain. This paper presents a novel semi-supervised method for weed detection, incorporating two key components: a multi-scale feature representation technique to capture distinctive weed characteristics at various scales, and an adaptive pseudo-labeling strategy that leverages a small set of labeled images. This strategy dynamically assigns confidence scores to pseudo-labels generated from unlabeled data, integrating both epoch-corresponding and mixed pseudo-labels to optimize the learning process.  Experiments conducted on the COCO dataset and five prominent weed datasets (CottonWeedDet12, CropAndWeed, Palmer amaranth, RadishWheat, and RoboWeedMap) demonstrate state-of-the-art weed detection performance, even with substantially reduced labeled data compared to existing methods. This approach significantly mitigates the data labeling burden, enhancing the feasibility and deployment of deep learning for real-world agricultural weed detection.\n"}, "2204.12785": {"original_text": "  Language models (LMs) have shown great potential as implicit knowledge bases\n(KBs). And for their practical use, knowledge in LMs need to be updated\nperiodically. However, existing tasks to assess LMs' efficacy as KBs do not\nadequately consider multiple large-scale updates. To this end, we first propose\na novel task--Continuously-updated QA (CuQA)--in which multiple large-scale\nupdates are made to LMs, and the performance is measured with respect to the\nsuccess in adding and updating knowledge while retaining existing knowledge. We\nthen present LMs with plug-in modules that effectively handle the updates.\nExperiments conducted on zsRE QA and NQ datasets show that our method\noutperforms existing approaches. We find that our method is 4x more effective\nin terms of updates/forgets ratio, compared to a fine-tuning baseline.\n", "rewritten_text": "Language models (LMs) show great potential as implicit knowledge bases (KBs).  However, their practical application requires periodic knowledge updates, a factor inadequately addressed by existing LM-as-KB evaluation tasks.  To address this, we introduce Continuously-updated Question Answering (CuQA), a novel task involving multiple large-scale LM updates.  CuQA measures performance by assessing the success of knowledge addition and update while preserving existing knowledge. We then present a method employing plug-in modules to efficiently manage these updates. Experiments on the zsRE QA and Natural Questions (NQ) datasets demonstrate that our method outperforms existing approaches, achieving a fourfold improvement in the updates/forgetting ratio compared to a fine-tuning baseline.\n"}, "2406.08215": {"original_text": "  Extractive summarization is a task of highlighting the most important parts\nof the text. We introduce a new approach to extractive summarization task using\nhidden clustering structure of the text. Experimental results on CNN/DailyMail\ndemonstrate that our approach generates more accurate summaries than both\nextractive and abstractive methods, achieving state-of-the-art results in terms\nof ROUGE-2 metric exceeding the previous approaches by 10%. Additionally, we\nshow that hidden structure of the text could be interpreted as aspects.\n", "rewritten_text": "This paper introduces a novel approach to extractive summarization that leverages the text's hidden clustering structure.  Experiments on the CNN/DailyMail dataset demonstrate that our method produces more accurate summaries than both existing extractive and abstractive methods, achieving state-of-the-art results.  Specifically, we surpass previous approaches by 10% in ROUGE-2 score.  Furthermore, we show that this hidden text structure can be interpreted as representing different aspects of the topic.\n"}, "1205.3183": {"original_text": "  Existing probabilistic scanners and parsers impose hard constraints on the\nway lexical and syntactic ambiguities can be resolved. Furthermore, traditional\ngrammar-based parsing tools are limited in the mechanisms they allow for taking\ncontext into account. In this paper, we propose a model-driven tool that allows\nfor statistical language models with arbitrary probability estimators. Our work\non model-driven probabilistic parsing is built on top of ModelCC, a model-based\nparser generator, and enables the probabilistic interpretation and resolution\nof anaphoric, cataphoric, and recursive references in the disambiguation of\nabstract syntax graphs. In order to prove the expression power of ModelCC, we\ndescribe the design of a general-purpose natural language parser.\n", "rewritten_text": "Existing probabilistic scanners and parsers impose rigid constraints on lexical and syntactic ambiguity resolution.  Traditional grammar-based parsing tools also offer limited contextual mechanisms. This paper presents a model-driven tool supporting statistical language models with arbitrary probability estimators.  Building upon ModelCC, a model-based parser generator, our approach enables probabilistic interpretation and resolution of anaphoric, cataphoric, and recursive references during abstract syntax graph disambiguation.  We demonstrate ModelCC's expressive power by describing the design of a general-purpose natural language parser.\n"}, "1607.06408": {"original_text": "  Action recognition has received increasing attention from the computer vision\nand machine learning communities in the last decade. To enable the study of\nthis problem, there exist a vast number of action datasets, which are recorded\nunder controlled laboratory settings, real-world surveillance environments, or\ncrawled from the Internet. Apart from the \"in-the-wild\" datasets, the training\nand test split of conventional datasets often possess similar environments\nconditions, which leads to close to perfect performance on constrained\ndatasets. In this paper, we introduce a new dataset, namely Multi-Camera Action\nDataset (MCAD), which is designed to evaluate the open view classification\nproblem under the surveillance environment. In total, MCAD contains 14,298\naction samples from 18 action categories, which are performed by 20 subjects\nand independently recorded with 5 cameras. Inspired by the well received\nevaluation approach on the LFW dataset, we designed a standard evaluation\nprotocol and benchmarked MCAD under several scenarios. The benchmark shows that\nwhile an average of 85% accuracy is achieved under the closed-view scenario,\nthe performance suffers from a significant drop under the cross-view scenario.\nIn the worst case scenario, the performance of 10-fold cross validation drops\nfrom 87.0% to 47.4%.\n", "rewritten_text": "Action recognition has garnered increasing attention from the computer vision and machine learning communities over the past decade.  Numerous action datasets exist, recorded in controlled laboratory settings, real-world surveillance environments, and collected from the internet.  However, excluding \"in-the-wild\" datasets, conventional datasets often exhibit similar training and testing environments, leading to near-perfect performance on constrained benchmarks.  This paper introduces a new dataset, the Multi-Camera Action Dataset (MCAD), designed to evaluate open-view classification in surveillance environments.  MCAD comprises 14,298 action samples from 18 categories, performed by 20 subjects and independently recorded by 5 cameras.  Inspired by the successful evaluation protocol of the LFW dataset, we developed a standardized evaluation protocol and benchmarked MCAD under various scenarios.  Results show that while an average accuracy of 85% is achieved in closed-view scenarios, performance significantly degrades in cross-view scenarios.  In the most challenging scenario (10-fold cross-validation), performance drops from 87.0% to 47.4%.\n"}, "2007.06233": {"original_text": "  In the majority of object detection frameworks, the confidence of instance\nclassification is used as the quality criterion of predicted bounding boxes,\nlike the confidence-based ranking in non-maximum suppression (NMS). However,\nthe quality of bounding boxes, indicating the spatial relations, is not only\ncorrelated with the classification scores. Compared with the region proposal\nnetwork (RPN) based detectors, single-shot object detectors suffer the box\nquality as there is a lack of pre-selection of box proposals. In this paper, we\naim at single-shot object detectors and propose a location-aware anchor-based\nreasoning (LAAR) for the bounding boxes. LAAR takes both the location and\nclassification confidences into consideration for the quality evaluation of\nbounding boxes. We introduce a novel network block to learn the relative\nlocation between the anchors and the ground truths, denoted as a localization\nscore, which acts as a location reference during the inference stage. The\nproposed localization score leads to an independent regression branch and\ncalibrates the bounding box quality by scoring the predicted localization score\nso that the best-qualified bounding boxes can be picked up in NMS. Experiments\non MS COCO and PASCAL VOC benchmarks demonstrate that the proposed\nlocation-aware framework enhances the performances of current anchor-based\nsingle-shot object detection frameworks and yields consistent and robust\ndetection results.\n", "rewritten_text": "Most object detection frameworks use instance classification confidence as the primary criterion for evaluating predicted bounding box quality, as exemplified by confidence-based ranking in non-maximum suppression (NMS).  However, bounding box quality, reflecting spatial relationships, is not solely determined by classification scores.  Single-shot detectors, unlike region proposal network (RPN)-based detectors, often suffer from lower box quality due to the absence of pre-selected proposals.  This paper proposes Location-Aware Anchor-based Reasoning (LAAR) for single-shot object detectors.  LAAR incorporates both location and classification confidences to evaluate bounding box quality.  We introduce a novel network block that learns the relative location between anchors and ground truths, generating a *localization score* that serves as a spatial reference during inference. This score enables an independent regression branch, calibrating bounding box quality by scoring predicted localization. This allows NMS to select higher-quality bounding boxes. Experiments on the MS COCO and PASCAL VOC benchmarks demonstrate that our location-aware framework improves the performance of existing anchor-based single-shot object detectors, yielding consistently robust detection results.\n"}, "2308.09475": {"original_text": "  Robot-assisted surgery has made significant progress, with instrument\nsegmentation being a critical factor in surgical intervention quality. It\nserves as the building block to facilitate surgical robot navigation and\nsurgical education for the next generation of operating intelligence. Although\nexisting methods have achieved accurate instrument segmentation results, they\nsimultaneously generate segmentation masks for all instruments, without the\ncapability to specify a target object and allow an interactive experience. This\nwork explores a new task of Referring Surgical Video Instrument Segmentation\n(RSVIS), which aims to automatically identify and segment the corresponding\nsurgical instruments based on the given language expression. To achieve this,\nwe devise a novel Video-Instrument Synergistic Network (VIS-Net) to learn both\nvideo-level and instrument-level knowledge to boost performance, while previous\nwork only used video-level information. Meanwhile, we design a Graph-based\nRelation-aware Module (GRM) to model the correlation between multi-modal\ninformation (i.e., textual description and video frame) to facilitate the\nextraction of instrument-level information. We are also the first to produce\ntwo RSVIS datasets to promote related research. Our method is verified on these\ndatasets, and experimental results exhibit that the VIS-Net can significantly\noutperform existing state-of-the-art referring segmentation methods. Our code\nand our datasets will be released upon the publication of this work.\n", "rewritten_text": "Robot-assisted surgery has advanced significantly, with accurate instrument segmentation crucial for high-quality surgical interventions.  This process underpins surgical robot navigation and the training of future surgeons. While existing methods achieve accurate segmentation, they simultaneously segment all instruments, lacking the ability to target specific objects interactively.  This work introduces a novel task: Referring Surgical Video Instrument Segmentation (RSVIS), which aims to automatically identify and segment surgical instruments based on natural language descriptions.  To address this, we propose a novel Video-Instrument Synergistic Network (VIS-Net) that leverages both video-level and instrument-level information, unlike previous methods which relied solely on video-level data.  Furthermore, we introduce a Graph-based Relation-aware Module (GRM) to model the relationships between multimodal information (textual descriptions and video frames), thereby enhancing instrument-level information extraction.  We also present two new RSVIS datasets to foster further research.  Our method, evaluated on these datasets, significantly outperforms state-of-the-art referring segmentation methods.  The code and datasets will be publicly released upon publication.\n"}, "1401.119": {"original_text": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n", "rewritten_text": "Extracting and recognizing Bangla text from video frame images is challenging due to complex color backgrounds and low resolution. This paper proposes a two-step algorithm for extracting and recognizing Bangla text from such video frames.  First, text lines are segmented into words using line contour information. Word gaps are identified using the first-order gradient values of the text blocks.  A local binarization technique is then applied to each word, and the text line is reconstructed from these binarized words. Second, this binarized text block is passed to an optical character recognition (OCR) system for recognition.\n"}, "1502.06807": {"original_text": "  We introduce and evaluate several architectures for Convolutional Neural\nNetworks to predict the 3D joint locations of a hand given a depth map. We\nfirst show that a prior on the 3D pose can be easily introduced and\nsignificantly improves the accuracy and reliability of the predictions. We also\nshow how to use context efficiently to deal with ambiguities between fingers.\nThese two contributions allow us to significantly outperform the\nstate-of-the-art on several challenging benchmarks, both in terms of accuracy\nand computation times.\n", "rewritten_text": "This paper introduces and evaluates several convolutional neural network (CNN) architectures for predicting 3D hand joint locations from depth maps.  We demonstrate that incorporating a 3D pose prior significantly improves prediction accuracy and reliability.  Furthermore, we show how to effectively leverage contextual information to resolve ambiguities between fingers.  These contributions enable us to achieve state-of-the-art performance on several challenging benchmarks, surpassing existing methods in both accuracy and computational efficiency.\n"}, "1812.01855": {"original_text": "  We aim to dismantle the prevalent black-box neural architectures used in\ncomplex visual reasoning tasks, into the proposed eXplainable and eXplicit\nNeural Modules (XNMs), which advance beyond existing neural module networks\ntowards using scene graphs --- objects as nodes and the pairwise relationships\nas edges --- for explainable and explicit reasoning with structured knowledge.\nXNMs allow us to pay more attention to teach machines how to \"think\",\nregardless of what they \"look\". As we will show in the paper, by using scene\ngraphs as an inductive bias, 1) we can design XNMs in a concise and flexible\nfashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce\nthe number of parameters by 10 to 100 times, and 2) we can explicitly trace the\nreasoning-flow in terms of graph attentions. XNMs are so generic that they\nsupport a wide range of scene graph implementations with various qualities. For\nexample, when the graphs are detected perfectly, XNMs achieve 100% accuracy on\nboth CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound\nfor visual reasoning; when the graphs are noisily detected from real-world\nimages, XNMs are still robust to achieve a competitive 67.5% accuracy on\nVQAv2.0, surpassing the popular bag-of-objects attention models without graph\nstructures.\n", "rewritten_text": "This paper introduces eXplainable and eXplicit Neural Modules (XNMs), a novel architecture designed to replace the opaque neural networks commonly used in complex visual reasoning tasks.  Unlike existing neural module networks, XNMs leverage scene graphs\u2014representing objects as nodes and their relationships as edges\u2014to enable explainable and explicit reasoning with structured knowledge.  This approach shifts the focus from visual appearance to the underlying reasoning process, teaching machines *how* to think rather than simply *what* to see.\n\nWe demonstrate that using scene graphs as an inductive bias offers two key advantages:  (1) XNMs achieve remarkable conciseness and flexibility, employing only four meta-types and reducing the number of parameters by a factor of 10 to 100; and (2) the reasoning flow can be explicitly traced through graph attentions.  The generality of XNMs allows them to accommodate a wide range of scene graph implementations.  Specifically, with perfectly detected graphs, XNMs achieve 100% accuracy on both CLEVR and CLEVR-CoGenT, establishing an empirical upper bound for visual reasoning performance.  Even with noisy graph detection from real-world images, XNMs maintain robustness, achieving a competitive 67.5% accuracy on VQAv2.0, surpassing popular bag-of-objects attention models that lack graph structures.\n"}, "2401.03340": {"original_text": "  This paper introduces the CowStallNumbers dataset, a collection of images\nextracted from videos focusing on cow teats, designed to advance the field of\ncow stall number detection. The dataset comprises 1042 training images and 261\ntest images, featuring stall numbers ranging from 0 to 60. To enhance the\ndataset, we performed fine-tuning on a YOLO model and applied data augmentation\ntechniques, including random crop, center crop, and random rotation. The\nexperimental outcomes demonstrate a notable 95.4\\% accuracy in recognizing\nstall numbers.\n", "rewritten_text": "This paper introduces CowStallNumbers, a new dataset of images extracted from cow teat videos for advancing cow stall number detection.  The dataset contains 1042 training images and 261 test images, representing stall numbers 0\u201360.  To improve dataset quality, we fine-tuned a YOLO model and applied data augmentation techniques such as random cropping, center cropping, and random rotation.  Experiments achieved a 95.4% accuracy in stall number recognition.\n"}, "2307.05158": {"original_text": "  Predicting where a person is looking is a complex task, requiring to\nunderstand not only the person's gaze and scene content, but also the 3D scene\nstructure and the person's situation (are they manipulating? interacting or\nobserving others? attentive?) to detect obstructions in the line of sight or\napply attention priors that humans typically have when observing others. In\nthis paper, we hypothesize that identifying and leveraging such priors can be\nbetter achieved through the exploitation of explicitly derived multimodal cues\nsuch as depth and pose. We thus propose a modular multimodal architecture\nallowing to combine these cues using an attention mechanism. The architecture\ncan naturally be exploited in privacy-sensitive situations such as surveillance\nand health, where personally identifiable information cannot be released. We\nperform extensive experiments on the GazeFollow and VideoAttentionTarget public\ndatasets, obtaining state-of-the-art performance and demonstrating very\ncompetitive results in the privacy setting case.\n", "rewritten_text": "Predicting gaze direction is a complex task requiring understanding not only the person's gaze and scene content, but also the 3D scene structure and the observer's context (e.g., manipulation, interaction, observation, attentiveness).  This necessitates accounting for line-of-sight obstructions and incorporating the attentional priors humans naturally employ when observing others.  This paper hypothesizes that leveraging these priors can be best achieved by explicitly exploiting multimodal cues such as depth and pose.  We therefore propose a modular multimodal architecture that combines these cues using an attention mechanism.  This architecture is particularly well-suited for privacy-sensitive applications like surveillance and healthcare, where personally identifiable information cannot be disclosed.  Extensive experiments on the GazeFollow and VideoAttentionTarget public datasets demonstrate state-of-the-art performance and highly competitive results in privacy-preserving settings.\n"}, "2302.07027": {"original_text": "  Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.\n", "rewritten_text": "Pretrained language models (PLMs), while trained on massive corpora, often require domain-specific adaptation.  A parameter-efficient approach involves training a separate adapter for each domain on a language modeling task.  Although this yields strong in-domain performance, it's impractical for resource-constrained settings.  To address this, we propose AdapterSoup, a method that leverages weight-space averaging of adapters trained on diverse domains.  Our approach is embarrassingly parallel:  we first train a set of domain-specific adapters; then, for each unseen domain, we select and average the most relevant adapters at test time.  Extensive experiments demonstrate that AdapterSoup consistently improves performance on novel domains without requiring additional training.  Furthermore, we investigate averaging adapters trained on the same domain with varying hyperparameters, showing that this maintains strong performance on unseen domains while achieving robust in-domain results.  We explore several adapter selection methods, including text clustering and semantic similarity, finding that clustering yields the most competitive results on novel domains.\n"}, "1706.09147": {"original_text": "  We address the task of Named Entity Disambiguation (NED) for noisy text. We\npresent WikilinksNED, a large-scale NED dataset of text fragments from the web,\nwhich is significantly noisier and more challenging than existing news-based\ndatasets. To capture the limited and noisy local context surrounding each\nmention, we design a neural model and train it with a novel method for sampling\ninformative negative examples. We also describe a new way of initializing word\nand entity embeddings that significantly improves performance. Our model\nsignificantly outperforms existing state-of-the-art methods on WikilinksNED\nwhile achieving comparable performance on a smaller newswire dataset.\n", "rewritten_text": "This paper addresses named entity disambiguation (NED) in noisy text.  We introduce WikilinksNED, a large-scale NED dataset comprising web text fragments that are significantly noisier and more challenging than existing news-based datasets.  To address the limited and noisy local context surrounding each mention, we propose a neural model trained with a novel negative sampling method designed to select informative examples.  Furthermore, we introduce a new word and entity embedding initialization technique that substantially improves performance.  Our model significantly outperforms state-of-the-art methods on WikilinksNED while achieving comparable performance on a smaller newswire dataset.\n"}, "2312.16274": {"original_text": "  Recent progress in multi-modal conditioned face synthesis has enabled the\ncreation of visually striking and accurately aligned facial images. Yet,\ncurrent methods still face issues with scalability, limited flexibility, and a\none-size-fits-all approach to control strength, not accounting for the\ndiffering levels of conditional entropy, a measure of unpredictability in data\ngiven some condition, across modalities. To address these challenges, we\nintroduce a novel uni-modal training approach with modal surrogates, coupled\nwith an entropy-aware modal-adaptive modulation, to support flexible, scalable,\nand scalable multi-modal conditioned face synthesis network. Our uni-modal\ntraining with modal surrogate that only leverage uni-modal data, use modal\nsurrogate to decorate condition with modal-specific characteristic and serve as\nlinker for inter-modal collaboration , fully learns each modality control in\nface synthesis process as well as inter-modal collaboration. The entropy-aware\nmodal-adaptive modulation finely adjust diffusion noise according to\nmodal-specific characteristics and given conditions, enabling well-informed\nstep along denoising trajectory and ultimately leading to synthesis results of\nhigh fidelity and quality. Our framework improves multi-modal face synthesis\nunder various conditions, surpassing current methods in image quality and\nfidelity, as demonstrated by our thorough experimental results.\n", "rewritten_text": "Recent advances in multi-modal conditioned face synthesis have yielded visually striking and accurately aligned facial images. However, current methods suffer from scalability limitations, inflexible control, and a uniform approach to control strength, neglecting the varying conditional entropy\u2014a measure of data unpredictability given a condition\u2014across modalities.  To address these limitations, we propose a novel uni-modal training approach employing modal surrogates, coupled with entropy-aware modal-adaptive modulation. This framework enables flexible and scalable multi-modal conditioned face synthesis.  Our uni-modal training, leveraging only uni-modal data, utilizes modal surrogates to enrich conditions with modality-specific characteristics, facilitating inter-modal collaboration. This approach thoroughly learns both individual modality control and inter-modal interactions within the face synthesis process.  The entropy-aware modal-adaptive modulation finely adjusts diffusion noise based on modality-specific characteristics and given conditions, guiding the denoising process towards high-fidelity synthesis.  Our framework significantly improves multi-modal face synthesis across diverse conditions, outperforming existing methods in image quality and fidelity, as demonstrated by our comprehensive experimental results.\n"}, "2407.06938": {"original_text": "  We present RodinHD, which can generate high-fidelity 3D avatars from a\nportrait image. Existing methods fail to capture intricate details such as\nhairstyles which we tackle in this paper. We first identify an overlooked\nproblem of catastrophic forgetting that arises when fitting triplanes\nsequentially on many avatars, caused by the MLP decoder sharing scheme. To\novercome this issue, we raise a novel data scheduling strategy and a weight\nconsolidation regularization term, which improves the decoder's capability of\nrendering sharper details. Additionally, we optimize the guiding effect of the\nportrait image by computing a finer-grained hierarchical representation that\ncaptures rich 2D texture cues, and injecting them to the 3D diffusion model at\nmultiple layers via cross-attention. When trained on 46K avatars with a noise\nschedule optimized for triplanes, the resulting model can generate 3D avatars\nwith notably better details than previous methods and can generalize to\nin-the-wild portrait input.\n", "rewritten_text": "RodinHD generates high-fidelity 3D avatars from single portrait images.  Unlike existing methods, which fail to capture intricate details such as hairstyles, RodinHD addresses this limitation through a novel approach.  We identify and solve the previously overlooked problem of catastrophic forgetting during sequential triplane fitting, caused by shared MLP decoder weights.  Our solution incorporates a novel data scheduling strategy and a weight consolidation regularization term, enhancing the decoder's ability to render sharper details.  Furthermore, we improve the guidance from the portrait image by computing a finer-grained hierarchical representation of 2D texture cues and injecting these cues into the 3D diffusion model at multiple layers via cross-attention.  Trained on 46,000 avatars with a noise schedule optimized for triplanes, RodinHD generates significantly more detailed 3D avatars than prior methods and generalizes well to diverse portrait inputs.\n"}, "2311.08107": {"original_text": "  Large Language Models (LLMs) can justify or critique their predictions\nthrough discussions with other models or humans, thereby enriching their\nintrinsic understanding of instances. While proactive discussions in the\ninference phase have been shown to boost performance, such interactions have\nnot been extensively explored during the training phase. We hypothesize that\nincorporating interactive discussions into the training process can enhance the\nmodels' understanding and improve their reasoning and verbal expression\nabilities during inference. This work introduces the SAIE framework, which\nfacilitates supportive and adversarial discussions between learner and partner\nmodels. The learner model receives responses from the partner, and its\nparameters are then updated based on this discussion. This dynamic adjustment\nprocess continues throughout the training phase, responding to the evolving\noutputs of the learner model. Our empirical evaluation across various tasks,\nincluding math problems, commonsense reasoning, and multi-domain knowledge,\ndemonstrates that models fine-tuned with the SAIE framework outperform those\ntrained with conventional fine-tuning approaches. Furthermore, our method\nenhances the models' reasoning capabilities, improving both individual and\nmulti-agent inference performance.\n", "rewritten_text": "Large language models (LLMs) can justify and critique their predictions through dialogue with other models or humans, thereby deepening their understanding. While interactive inference has demonstrably improved performance, interactive training remains largely unexplored.  We hypothesize that incorporating interactive discussions into training will enhance model understanding, reasoning, and verbal fluency during inference.  This work introduces the SAIE framework, which facilitates both supportive and adversarial dialogues between a learner model and a partner model. The learner model's parameters are updated based on the partner's responses, dynamically adapting throughout training.  Our empirical evaluation across diverse tasks\u2014including mathematics, commonsense reasoning, and multi-domain knowledge\u2014demonstrates that models fine-tuned with SAIE outperform those trained using conventional methods.  Moreover, SAIE enhances reasoning capabilities, improving both single-agent and multi-agent inference performance.\n"}, "1704.05831": {"original_text": "  We propose a hierarchical approach for making long-term predictions of future\nframes. To avoid inherent compounding errors in recursive pixel-level\nprediction, we propose to first estimate high-level structure in the input\nframes, then predict how that structure evolves in the future, and finally by\nobserving a single frame from the past and the predicted high-level structure,\nwe construct the future frames without having to observe any of the pixel-level\npredictions. Long-term video prediction is difficult to perform by recurrently\nobserving the predicted frames because the small errors in pixel space\nexponentially amplify as predictions are made deeper into the future. Our\napproach prevents pixel-level error propagation from happening by removing the\nneed to observe the predicted frames. Our model is built with a combination of\nLSTM and analogy based encoder-decoder convolutional neural networks, which\nindependently predict the video structure and generate the future frames,\nrespectively. In experiments, our model is evaluated on the Human3.6M and Penn\nAction datasets on the task of long-term pixel-level video prediction of humans\nperforming actions and demonstrate significantly better results than the\nstate-of-the-art.\n", "rewritten_text": "We propose a hierarchical approach for long-term video prediction to mitigate the compounding errors inherent in recursive pixel-level prediction.  Our method first estimates the high-level structure of input frames.  It then predicts the evolution of this structure into the future. Finally, using a single past frame and the predicted high-level structure, it constructs future frames without relying on intermediate pixel-level predictions.  Recurrently predicting frames suffers from exponential error amplification in pixel space, making long-term prediction challenging.  Our approach avoids this by eliminating the need to observe predicted intermediate frames.  Our model combines LSTM and analogy-based encoder-decoder convolutional neural networks.  These independently predict video structure and generate future frames, respectively.  Experiments on the Human3.6M and Penn Action datasets, focusing on long-term, pixel-level human action prediction, demonstrate significantly improved performance over state-of-the-art methods.\n"}, "1606.05706": {"original_text": "  We study the problem of agreement and disagreement detection in online\ndiscussions. An isotonic Conditional Random Fields (isotonic CRF) based\nsequential model is proposed to make predictions on sentence- or segment-level.\nWe automatically construct a socially-tuned lexicon that is bootstrapped from\nexisting general-purpose sentiment lexicons to further improve the performance.\nWe evaluate our agreement and disagreement tagging model on two disparate\nonline discussion corpora -- Wikipedia Talk pages and online debates. Our model\nis shown to outperform the state-of-the-art approaches in both datasets. For\nexample, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for\nagreement and disagreement detection, when a linear chain CRF obtains 0.58 and\n0.56 for the discussions on Wikipedia Talk pages.\n", "rewritten_text": "This paper addresses the problem of detecting agreement and disagreement in online discussions.  We propose a sequential model based on isotonic Conditional Random Fields (isotonic CRFs) for sentence- or segment-level prediction.  To enhance performance, we automatically construct a socially-tuned lexicon bootstrapped from existing general-purpose sentiment lexicons.  We evaluate our agreement/disagreement tagging model on two diverse corpora: Wikipedia Talk pages and online debate transcripts.  Our model outperforms state-of-the-art approaches on both datasets. For instance, on Wikipedia Talk pages, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for agreement and disagreement detection, respectively, compared to 0.58 and 0.56 achieved by a linear-chain CRF.\n"}, "2401.13011": {"original_text": "  This paper presents a novel generative model, Collaborative Competitive\nAgents (CCA), which leverages the capabilities of multiple Large Language\nModels (LLMs) based agents to execute complex tasks. Drawing inspiration from\nGenerative Adversarial Networks (GANs), the CCA system employs two equal-status\ngenerator agents and a discriminator agent. The generators independently\nprocess user instructions and generate results, while the discriminator\nevaluates the outputs, and provides feedback for the generator agents to\nfurther reflect and improve the generation results. Unlike the previous\ngenerative model, our system can obtain the intermediate steps of generation.\nThis allows each generator agent to learn from other successful executions due\nto its transparency, enabling a collaborative competition that enhances the\nquality and robustness of the system's results. The primary focus of this study\nis image editing, demonstrating the CCA's ability to handle intricate\ninstructions robustly. The paper's main contributions include the introduction\nof a multi-agent-based generative model with controllable intermediate steps\nand iterative optimization, a detailed examination of agent relationships, and\ncomprehensive experiments on image editing. Code is available at\n\\href{https://github.com/TiankaiHang/CCA}{https://github.com/TiankaiHang/CCA}.\n", "rewritten_text": "This paper introduces Collaborative Competitive Agents (CCA), a novel generative model that leverages multiple Large Language Model (LLM)-based agents to perform complex tasks.  Inspired by Generative Adversarial Networks (GANs), CCA employs two equally weighted generator agents and a discriminator agent.  The generators independently process user instructions and produce outputs, which the discriminator then evaluates, providing feedback to refine subsequent generations.  Unlike previous generative models, CCA's transparency allows access to intermediate generation steps. This enables each generator to learn from the successes of others, fostering collaborative competition and improving the quality and robustness of the results.  This study focuses on image editing, showcasing CCA's ability to robustly handle intricate instructions.  Our key contributions include: a multi-agent generative model with controllable intermediate steps and iterative optimization; a detailed analysis of agent interactions; and comprehensive image-editing experiments.  The code is available at [https://github.com/TiankaiHang/CCA](https://github.com/TiankaiHang/CCA).\n"}, "2309.08644": {"original_text": "  3D pose estimation is an invaluable task in computer vision with various\npractical applications. Especially, 3D pose estimation for multi-person from a\nmonocular video (3DMPPE) is particularly challenging and is still largely\nuncharted, far from applying to in-the-wild scenarios yet. We pose three\nunresolved issues with the existing methods: lack of robustness on unseen views\nduring training, vulnerability to occlusion, and severe jittering in the\noutput. As a remedy, we propose POTR-3D, the first realization of a\nsequence-to-sequence 2D-to-3D lifting model for 3DMPPE, powered by a novel\ngeometry-aware data augmentation strategy, capable of generating unbounded data\nwith a variety of views while caring about the ground plane and occlusions.\nThrough extensive experiments, we verify that the proposed model and data\naugmentation robustly generalizes to diverse unseen views, robustly recovers\nthe poses against heavy occlusions, and reliably generates more natural and\nsmoother outputs. The effectiveness of our approach is verified not only by\nachieving the state-of-the-art performance on public benchmarks, but also by\nqualitative results on more challenging in-the-wild videos. Demo videos are\navailable at https://www.youtube.com/@potr3d.\n", "rewritten_text": "3D multi-person pose estimation (3DMPPE) from monocular video is a crucial yet challenging computer vision task with significant practical applications.  Existing methods struggle with several key limitations: robustness to unseen viewpoints during training, vulnerability to occlusion, and significant output jittering, hindering their deployment in real-world scenarios.  To address these issues, we introduce POTR-3D, the first sequence-to-sequence 2D-to-3D lifting model for 3DMPPE.  POTR-3D leverages a novel geometry-aware data augmentation strategy that generates diverse, unbounded data encompassing various viewpoints while explicitly considering ground plane constraints and occlusions.  Extensive experiments demonstrate that POTR-3D robustly generalizes to unseen viewpoints, accurately recovers poses despite heavy occlusion, and produces smoother, more natural output.  Our approach achieves state-of-the-art performance on public benchmarks and exhibits superior qualitative results on challenging real-world videos.  Demo videos are available at https://www.youtube.com/@potr3d.\n"}, "2008.08735": {"original_text": "  Computer vision (CV) has achieved great success in interpreting semantic\nmeanings from images, yet CV algorithms can be brittle for tasks with adverse\nvision conditions and the ones suffering from data/label pair limitation. One\nof this tasks is in-bed human pose estimation, which has significant values in\nmany healthcare applications. In-bed pose monitoring in natural settings could\ninvolve complete darkness or full occlusion. Furthermore, the lack of publicly\navailable in-bed pose datasets hinders the use of many successful pose\nestimation algorithms for this task. In this paper, we introduce our\nSimultaneously-collected multimodal Lying Pose (SLP) dataset, which includes\nin-bed pose images from 109 participants captured using multiple imaging\nmodalities including RGB, long wave infrared, depth, and pressure map. We also\npresent a physical hyper parameter tuning strategy for ground truth pose label\ngeneration under extreme conditions such as lights off and being fully covered\nby a sheet/blanket. SLP design is compatible with the mainstream human pose\ndatasets, therefore, the state-of-the-art 2D pose estimation models can be\ntrained effectively with SLP data with promising performance as high as 95% at\nPCKh@0.5 on a single modality. The pose estimation performance can be further\nimproved by including additional modalities through collaboration.\n", "rewritten_text": "Computer vision (CV) has achieved significant success in interpreting semantic meanings from images. However, CV algorithms remain brittle when faced with adverse visual conditions or limited data/label pairs.  One such challenging task is in-bed human pose estimation, which holds significant value in numerous healthcare applications. In-bed pose monitoring in naturalistic settings often involves complete darkness or full occlusion.  Furthermore, the scarcity of publicly available in-bed pose datasets hinders the application of many successful pose estimation algorithms.\n\nThis paper introduces the Simultaneously-collected Multimodal Lying Pose (SLP) dataset, comprising in-bed pose images from 109 participants captured using multiple imaging modalities: RGB, long-wave infrared, depth, and pressure mapping. We also present a novel physical hyperparameter tuning strategy for generating ground truth pose labels under extreme conditions, such as complete darkness or full coverage by a sheet or blanket.  The SLP dataset's design is compatible with mainstream human pose datasets, enabling effective training of state-of-the-art 2D pose estimation models.  Using SLP data, we achieve promising performance, reaching 95% PCKh@0.5 accuracy on a single modality.  Moreover, performance can be further improved by multi-modal fusion.\n"}, "2206.04846": {"original_text": "  Deep neural networks are capable of learning powerful representations to\ntackle complex vision tasks but expose undesirable properties like the\nover-fitting issue. To this end, regularization techniques like image\naugmentation are necessary for deep neural networks to generalize well.\nNevertheless, most prevalent image augmentation recipes confine themselves to\noff-the-shelf linear transformations like scale, flip, and colorjitter. Due to\ntheir hand-crafted property, these augmentations are insufficient to generate\ntruly hard augmented examples. In this paper, we propose a novel perspective of\naugmentation to regularize the training process. Inspired by the recent success\nof applying masked image modeling to self-supervised learning, we adopt the\nself-supervised masked autoencoder to generate the distorted view of the input\nimages. We show that utilizing such model-based nonlinear transformation as\ndata augmentation can improve high-level recognition tasks. We term the\nproposed method as \\textbf{M}ask-\\textbf{R}econstruct \\textbf{A}ugmentation\n(MRA). The extensive experiments on various image classification benchmarks\nverify the effectiveness of the proposed augmentation. Specifically, MRA\nconsistently enhances the performance on supervised, semi-supervised as well as\nfew-shot classification. The code will be available at\n\\url{https://github.com/haohang96/MRA}.\n", "rewritten_text": "Deep neural networks (DNNs), while capable of learning powerful representations for complex vision tasks, are susceptible to overfitting.  To mitigate this, regularization techniques such as image augmentation are crucial for improved generalization. However, common augmentation methods typically rely on simple, hand-crafted linear transformations (e.g., scaling, flipping, color jittering), which are insufficient for generating truly challenging augmented examples.\n\nThis paper proposes a novel augmentation approach inspired by the success of masked image modeling in self-supervised learning.  We leverage a self-supervised masked autoencoder to generate non-linearly distorted views of input images.  We demonstrate that using this model-based augmentation improves performance on high-level recognition tasks.  We term this method Mask-Reconstruct Augmentation (MRA).\n\nExtensive experiments on various image classification benchmarks confirm MRA's effectiveness, consistently enhancing performance in supervised, semi-supervised, and few-shot learning scenarios.  The code is available at [https://github.com/haohang96/MRA](https://github.com/haohang96/MRA).\n"}, "1803.10547": {"original_text": "  Text articles with false claims, especially news, have recently become\naggravating for the Internet users. These articles are in wide circulation and\nreaders face difficulty discerning fact from fiction. Previous work on\ncredibility assessment has focused on factual analysis and linguistic features.\nThe task's main challenge is the distinction between the features of true and\nfalse articles. In this paper, we propose a novel approach called Credibility\nOutcome (CREDO) which aims at scoring the credibility of an article in an open\ndomain setting.\n  CREDO consists of different modules for capturing various features\nresponsible for the credibility of an article. These features includes\ncredibility of the article's source and author, semantic similarity between the\narticle and related credible articles retrieved from a knowledge base, and\nsentiments conveyed by the article. A neural network architecture learns the\ncontribution of each of these modules to the overall credibility of an article.\nExperiments on Snopes dataset reveals that CREDO outperforms the\nstate-of-the-art approaches based on linguistic features.\n", "rewritten_text": "The proliferation of online articles containing false claims, particularly in news contexts, has become increasingly problematic for internet users.  These widely circulated articles make it difficult to distinguish fact from fiction. While previous credibility assessment work has focused on factual analysis and linguistic features, the key challenge remains differentiating features of true and false articles.  This paper proposes a novel approach, Credibility Outcome (CREDO), designed to score article credibility in an open-domain setting.\n\nCREDO comprises several modules that capture various credibility-related features. These include the credibility of the article's source and author, the semantic similarity between the article and related credible articles retrieved from a knowledge base, and the sentiment expressed within the article. A neural network architecture learns the contribution of each module to the overall credibility score.  Experiments on the Snopes dataset demonstrate that CREDO outperforms state-of-the-art approaches based solely on linguistic features.\n"}, "1704.01926": {"original_text": "  This paper tackles the problem of semi-supervised video object segmentation,\nthat is, segmenting an object in a sequence given its mask in the first frame.\nOne of the main challenges in this scenario is the change of appearance of the\nobjects of interest. Their semantics, on the other hand, do not vary. This\npaper investigates how to take advantage of such invariance via the\nintroduction of a semantic prior that guides the appearance model.\nSpecifically, given the segmentation mask of the first frame of a sequence, we\nestimate the semantics of the object of interest, and propagate that knowledge\nthroughout the sequence to improve the results based on an appearance model. We\npresent Semantically-Guided Video Object Segmentation (SGV), which improves\nresults over previous state of the art on two different datasets using a\nvariety of evaluation metrics, while running in half a second per frame.\n", "rewritten_text": "This paper addresses semi-supervised video object segmentation: segmenting an object throughout a video sequence given its mask in the first frame.  A primary challenge is the object's changing appearance; however, its semantic content remains consistent.  This work leverages this semantic invariance by introducing a semantic prior to guide the appearance model.  Specifically, given the initial frame's segmentation mask, we estimate the object's semantics and propagate this information throughout the sequence to enhance an appearance-based segmentation model.  We present Semantically-Guided Video Object Segmentation (SGV), which outperforms state-of-the-art methods on two datasets across multiple evaluation metrics, achieving a processing speed of 0.5 seconds per frame.\n"}, "1902.06557": {"original_text": "  We propose a novel biophysical and dichromatic reflectance model that\nefficiently characterises spectral skin reflectance. We show how to fit the\nmodel to multispectral face images enabling high quality estimation of diffuse\nand specular shading as well as biophysical parameter maps (melanin and\nhaemoglobin). Our method works from a single image without requiring complex\ncontrolled lighting setups yet provides quantitatively accurate reconstructions\nand qualitatively convincing decomposition and editing.\n", "rewritten_text": "We propose a novel biophysical dichromatic reflectance model for efficient characterization of spectral skin reflectance.  This model, fitted to multispectral face images, enables high-quality estimation of diffuse and specular shading, as well as biophysical parameter maps (melanin and hemoglobin).  Our single-image method, requiring no complex controlled lighting, provides quantitatively accurate reconstructions and qualitatively convincing decompositions and editing capabilities.\n"}, "2211.09379": {"original_text": "  In dialogue state tracking (DST), labeling the dataset involves considerable\nhuman labor. We propose a new self-training framework for few-shot generative\nDST that utilize unlabeled data. Our self-training method iteratively improves\nthe model by pseudo labeling and employs Purpose Preserving Augmentation\n(PPAug) to prevent overfitting. We increaese the few-shot 10% performance by\napproximately 4% on MultiWOZ 2.1 and enhances the slot-recall 8.34% for unseen\nvalues compared to baseline.\n", "rewritten_text": "Dialogue state tracking (DST) datasets require significant human annotation effort.  To address this, we propose a novel self-training framework for few-shot generative DST that leverages unlabeled data.  Our method iteratively refines the model using pseudo-labeling and Purpose-Preserving Augmentation (PPAug) to mitigate overfitting.  On the MultiWOZ 2.1 benchmark, this approach improves few-shot performance by approximately 4% (a 10% relative increase) and enhances slot recall by 8.34% for unseen values compared to the baseline.\n"}, "1902.11268": {"original_text": "  Deep neural networks (DNNs), especially deep convolutional neural networks\n(CNNs), have emerged as the powerful technique in various machine learning\napplications. However, the large model sizes of DNNs yield high demands on\ncomputation resource and weight storage, thereby limiting the practical\ndeployment of DNNs. To overcome these limitations, this paper proposes to\nimpose the circulant structure to the construction of convolutional layers, and\nhence leads to circulant convolutional layers (CircConvs) and circulant CNNs.\nThe circulant structure and models can be either trained from scratch or\nre-trained from a pre-trained non-circulant model, thereby making it very\nflexible for different training environments. Through extensive experiments,\nsuch strong structure-imposing approach is proved to be able to substantially\nreduce the number of parameters of convolutional layers and enable significant\nsaving of computational cost by using fast multiplication of the circulant\ntensor.\n", "rewritten_text": "Deep convolutional neural networks (CNNs), a powerful technique in various machine learning applications, suffer from large model sizes that demand significant computational resources and storage.  This limitation hinders their practical deployment.  To address this, this paper proposes constructing convolutional layers with a circulant structure, resulting in circulant convolutional layers (CircConvs) and circulant CNNs.  These circulant models can be trained from scratch or fine-tuned from pre-trained non-circulant models, offering flexibility across different training environments.  Extensive experiments demonstrate that this structured approach substantially reduces the number of parameters in convolutional layers and significantly lowers computational cost through the use of fast circulant tensor multiplication.\n"}, "1704.00447": {"original_text": "  Purpose: To allow fast and high-quality reconstruction of clinical\naccelerated multi-coil MR data by learning a variational network that combines\nthe mathematical structure of variational models with deep learning.\n  Theory and Methods: Generalized compressed sensing reconstruction formulated\nas a variational model is embedded in an unrolled gradient descent scheme. All\nparameters of this formulation, including the prior model defined by filter\nkernels and activation functions as well as the data term weights, are learned\nduring an offline training procedure. The learned model can then be applied\nonline to previously unseen data.\n  Results: The variational network approach is evaluated on a clinical knee\nimaging protocol. The variational network reconstructions outperform standard\nreconstruction algorithms in terms of image quality and residual artifacts for\nall tested acceleration factors and sampling patterns.\n  Conclusion: Variational network reconstructions preserve the natural\nappearance of MR images as well as pathologies that were not included in the\ntraining data set. Due to its high computational performance, i.e.,\nreconstruction time of 193 ms on a single graphics card, and the omission of\nparameter tuning once the network is trained, this new approach to image\nreconstruction can easily be integrated into clinical workflow.\n", "rewritten_text": "This work presents a variational network for fast, high-quality reconstruction of accelerated multi-coil magnetic resonance (MR) data.  The method embeds a generalized compressed sensing reconstruction, formulated as a variational model, within an unrolled gradient descent scheme.  All parameters\u2014including prior model parameters (filter kernels and activation functions) and data term weights\u2014are learned offline. This learned model then enables rapid online reconstruction of unseen data.\n\nEvaluated on a clinical knee imaging protocol, the variational network significantly outperformed standard reconstruction algorithms in terms of image quality and artifact reduction across various acceleration factors and sampling patterns.  Importantly, the reconstructions preserved the natural appearance of MR images, including pathologies not present in the training data.  With a reconstruction time of 193 ms on a single graphics card and no requirement for post-training parameter tuning, this approach offers a computationally efficient and clinically viable solution readily integrable into existing workflows.\n"}, "2104.1269": {"original_text": "  Data is the engine of modern computer vision, which necessitates collecting\nlarge-scale datasets. This is expensive, and guaranteeing the quality of the\nlabels is a major challenge. In this paper, we investigate efficient annotation\nstrategies for collecting multi-class classification labels for a large\ncollection of images. While methods that exploit learnt models for labeling\nexist, a surprisingly prevalent approach is to query humans for a fixed number\nof labels per datum and aggregate them, which is expensive. Building on prior\nwork on online joint probabilistic modeling of human annotations and\nmachine-generated beliefs, we propose modifications and best practices aimed at\nminimizing human labeling effort. Specifically, we make use of advances in\nself-supervised learning, view annotation as a semi-supervised learning\nproblem, identify and mitigate pitfalls and ablate several key design choices\nto propose effective guidelines for labeling. Our analysis is done in a more\nrealistic simulation that involves querying human labelers, which uncovers\nissues with evaluation using existing worker simulation methods. Simulated\nexperiments on a 125k image subset of the ImageNet100 show that it can be\nannotated to 80% top-1 accuracy with 0.35 annotations per image on average, a\n2.7x and 6.7x improvement over prior work and manual annotation, respectively.\nProject page: https://fidler-lab.github.io/efficient-annotation-cookbook\n", "rewritten_text": "Large-scale datasets are crucial for modern computer vision, but their creation is expensive and ensuring label quality presents a significant challenge.  This paper investigates efficient annotation strategies for multi-class image classification. While learned models can assist labeling, a common\u2014and costly\u2014approach involves aggregating human-provided labels for a fixed number of annotations per image.  Building upon existing work on online joint probabilistic modeling of human and machine-generated labels, we propose refined methods and best practices to minimize human labeling effort.  Specifically, we leverage advances in self-supervised learning, frame annotation as a semi-supervised learning problem, identify and mitigate common pitfalls, and ablate key design choices to develop effective labeling guidelines.  Our analysis, conducted using a more realistic simulation involving actual human labelers, reveals limitations in existing worker simulation methods.  Simulated experiments on a 125,000-image subset of ImageNet-100 demonstrate that we can achieve 80% top-1 accuracy with an average of 0.35 annotations per image\u2014a 2.7x and 6.7x improvement over prior work and manual annotation, respectively.  Project page: https://fidler-lab.github.io/efficient-annotation-cookbook\n"}, "2206.00923": {"original_text": "  We present a method that achieves state-of-the-art results on challenging\n(few-shot) layout-to-image generation tasks by accurately modeling textures,\nstructures and relationships contained in a complex scene. After compressing\nRGB images into patch tokens, we propose the Transformer with Focal Attention\n(TwFA) for exploring dependencies of object-to-object, object-to-patch and\npatch-to-patch. Compared to existing CNN-based and Transformer-based generation\nmodels that entangled modeling on pixel-level&patch-level and\nobject-level&patch-level respectively, the proposed focal attention predicts\nthe current patch token by only focusing on its highly-related tokens that\nspecified by the spatial layout, thereby achieving disambiguation during\ntraining. Furthermore, the proposed TwFA largely increases the data efficiency\nduring training, therefore we propose the first few-shot complex scene\ngeneration strategy based on the well-trained TwFA. Comprehensive experiments\nshow the superiority of our method, which significantly increases both\nquantitative metrics and qualitative visual realism with respect to\nstate-of-the-art CNN-based and transformer-based methods. Code is available at\nhttps://github.com/JohnDreamer/TwFA.\n", "rewritten_text": "We present a method that achieves state-of-the-art results on challenging few-shot layout-to-image generation tasks.  Our approach accurately models the textures, structures, and relationships within complex scenes.  After compressing RGB images into patch tokens, we introduce the Transformer with Focal Attention (TwFA) to effectively capture object-to-object, object-to-patch, and patch-to-patch dependencies. Unlike existing CNN-based and Transformer-based generation models, which entangle pixel-level/patch-level and object-level/patch-level modeling, respectively, our focal attention mechanism predicts each patch token by focusing only on highly relevant tokens specified by the spatial layout. This targeted approach improves disambiguation during training.  Furthermore, TwFA significantly improves data efficiency, enabling the first few-shot complex scene generation strategy based on a well-trained TwFA model.  Comprehensive experiments demonstrate the superiority of our method, achieving significant improvements in both quantitative metrics and qualitative visual realism over state-of-the-art CNN-based and Transformer-based methods. Code is available at https://github.com/JohnDreamer/TwFA.\n"}, "2204.09841": {"original_text": "  Information from an image occurs over multiple and distinct spatial scales.\nImage pyramid multiresolution representations are a useful data structure for\nimage analysis and manipulation over a spectrum of spatial scales. This paper\nemploys the Gaussian-Laplacian pyramid to treat different spatial frequency\nbands of a texture separately. First, we generate three images corresponding to\nthree levels of the Gaussian-Laplacian pyramid for an input image to capture\nintrinsic details. Then we aggregate features extracted from gray and color\ntexture images using bio-inspired texture descriptors, information-theoretic\nmeasures, gray-level co-occurrence matrix features, and Haralick statistical\nfeatures into a single feature vector. Such an aggregation aims at producing\nfeatures that characterize textures to their maximum extent, unlike employing\neach descriptor separately, which may lose some relevant textural information\nand reduce the classification performance. The experimental results on texture\nand histopathologic image datasets have shown the advantages of the proposed\nmethod compared to state-of-the-art approaches. Such findings emphasize the\nimportance of multiscale image analysis and corroborate that the descriptors\nmentioned above are complementary.\n", "rewritten_text": "Image information exists across multiple, distinct spatial scales.  Image pyramids, particularly multiresolution representations, provide a valuable data structure for analyzing and manipulating images across this spatial frequency spectrum. This paper utilizes a Gaussian-Laplacian pyramid to process the different spatial frequency bands of a texture independently.  Initially, three images corresponding to three levels of the Gaussian-Laplacian pyramid are generated from an input image to capture its intrinsic details.  Features are then extracted from both grayscale and color texture images using a combination of bio-inspired texture descriptors, information-theoretic measures, gray-level co-occurrence matrix (GLCM) features, and Haralick statistical features, which are subsequently aggregated into a single feature vector. This aggregation aims to comprehensively characterize textures, unlike using individual descriptors, which may discard relevant textural information and compromise classification performance.  Experimental results on texture and histopathologic image datasets demonstrate the proposed method's superiority over state-of-the-art techniques. These findings highlight the importance of multiscale image analysis and confirm the complementary nature of the aforementioned descriptors.\n"}, "2207.00744": {"original_text": "  Spatial-Temporal Video Grounding (STVG) is a challenging task which aims to\nlocalize the spatio-temporal tube of the interested object semantically\naccording to a natural language query. Most previous works not only severely\nrely on the anchor boxes extracted by Faster R-CNN, but also simply regard the\nvideo as a series of individual frames, thus lacking their temporal modeling.\nInstead, in this paper, we are the first to propose an anchor-free framework\nfor STVG, called Gaussian Kernel-based Cross Modal Network (GKCMN).\nSpecifically, we utilize the learned Gaussian Kernel-based heatmaps of each\nvideo frame to locate the query-related object. A mixed serial and parallel\nconnection network is further developed to leverage both spatial and temporal\nrelations among frames for better grounding. Experimental results on VidSTG\ndataset demonstrate the effectiveness of our proposed GKCMN.\n", "rewritten_text": "Spatial-temporal video grounding (STVG) is a challenging task that aims to semantically localize the spatio-temporal tube of an object of interest based on a natural language query.  Most previous work heavily relies on anchor boxes extracted by Faster R-CNN and treats videos as sequences of independent frames, neglecting temporal modeling.  This paper introduces GKCMN (Gaussian Kernel-based Cross-Modal Network), the first anchor-free framework for STVG.  Specifically, we use learned Gaussian kernel-based heatmaps for each video frame to locate the query-relevant object.  A novel network architecture, combining serial and parallel connections, leverages both spatial and temporal relationships between frames to improve grounding accuracy.  Experiments on the VidSTG dataset demonstrate the effectiveness of our proposed GKCMN.\n"}, "2209.00232": {"original_text": "  Capsule networks (CapsNets) aim to parse images into a hierarchy of objects,\nparts, and their relations using a two-step process involving part-whole\ntransformation and hierarchical component routing. However, this hierarchical\nrelationship modeling is computationally expensive, which has limited the wider\nuse of CapsNet despite its potential advantages. The current state of CapsNet\nmodels primarily focuses on comparing their performance with capsule baselines,\nfalling short of achieving the same level of proficiency as deep CNN variants\nin intricate tasks. To address this limitation, we present an efficient\napproach for learning capsules that surpasses canonical baseline models and\neven demonstrates superior performance compared to high-performing convolution\nmodels. Our contribution can be outlined in two aspects: firstly, we introduce\na group of subcapsules onto which an input vector is projected. Subsequently,\nwe present the Hybrid Gromov-Wasserstein framework, which initially quantifies\nthe dissimilarity between the input and the components modeled by the\nsubcapsules, followed by determining their alignment degree through optimal\ntransport. This innovative mechanism capitalizes on new insights into defining\nalignment between the input and subcapsules, based on the similarity of their\nrespective component distributions. This approach enhances CapsNets' capacity\nto learn from intricate, high-dimensional data while retaining their\ninterpretability and hierarchical structure. Our proposed model offers two\ndistinct advantages: (i) its lightweight nature facilitates the application of\ncapsules to more intricate vision tasks, including object detection; (ii) it\noutperforms baseline approaches in these demanding tasks.\n", "rewritten_text": "Capsule networks (CapsNets) aim to parse images hierarchically into objects, parts, and their relationships using a two-step process of part-whole transformation and hierarchical routing.  However, this hierarchical modeling is computationally expensive, limiting CapsNet's widespread adoption despite its potential advantages. Current CapsNet models primarily focus on benchmarking against capsule baselines, often underperforming deep convolutional neural networks (CNNs) on complex tasks.  To address this, we propose an efficient capsule learning approach that surpasses canonical baselines and even outperforms high-performing CNNs.  Our contributions are twofold: first, we introduce a group of subcapsules onto which input vectors are projected; second, we introduce a novel Hybrid Gromov-Wasserstein framework. This framework first quantifies the dissimilarity between the input and subcapsule components, then determines their alignment via optimal transport. This innovative mechanism leverages insights into component distribution similarity to define input-subcapsule alignment.  This enhances CapsNets' ability to learn from complex, high-dimensional data while preserving interpretability and hierarchical structure. Our model offers two key advantages: (i) its efficiency enables application to more complex vision tasks, including object detection; and (ii) it achieves superior performance on these challenging tasks compared to baseline approaches.\n"}, "2311.12076": {"original_text": "  Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof open-world intelligent systems. Despite the notable advancements in existing\nOOD detection methodologies, our study identifies a significant performance\ndrop under the scarcity of training samples. In this context, we introduce a\nnovel few-shot OOD detection benchmark, carefully constructed to address this\ngap. Our empirical analysis reveals the superiority of ParameterEfficient\nFine-Tuning (PEFT) strategies, such as visual prompt tuning and visual adapter\ntuning, over conventional techniques, including fully fine-tuning and linear\nprobing tuning in the few-shot OOD detection task. Recognizing some crucial\ninformation from the pre-trained model, which is pivotal for OOD detection, may\nbe lost during the fine-tuning process, we propose a method termed\nDomainSpecific and General Knowledge Fusion (DSGF). This approach is designed\nto be compatible with diverse fine-tuning frameworks. Our experiments show that\nthe integration of DSGF significantly enhances the few-shot OOD detection\ncapabilities across various methods and fine-tuning methodologies, including\nfully fine-tuning, visual adapter tuning, and visual prompt tuning. The code\nwill be released.\n", "rewritten_text": "Out-of-distribution (OOD) detection is crucial for reliable open-world intelligent systems.  While existing OOD detection methods have advanced significantly, our study reveals a substantial performance degradation under data-scarce conditions.  To address this limitation, we introduce a novel few-shot OOD detection benchmark.  Our empirical analysis demonstrates the superiority of Parameter-Efficient Fine-Tuning (PEFT) strategies, such as visual prompt tuning and visual adapter tuning, over conventional methods like full fine-tuning and linear probing in few-shot OOD detection.  Recognizing that crucial information for OOD detection may be lost during fine-tuning of pre-trained models, we propose Domain-Specific and General Knowledge Fusion (DSGF), a method compatible with various fine-tuning frameworks.  Experiments show that DSGF significantly improves few-shot OOD detection performance across multiple methods and fine-tuning strategies, including full fine-tuning, visual adapter tuning, and visual prompt tuning. The code will be publicly released.\n"}, "2104.10117": {"original_text": "  We present a novel deep learning-based framework to generate embedding\nrepresentations of fine-grained emotions that can be used to computationally\ndescribe psychological models of emotions. Our framework integrates a\ncontextualized embedding encoder with a multi-head probing model that enables\nto interpret dynamically learned representations optimized for an emotion\nclassification task. Our model is evaluated on the Empathetic Dialogue dataset\nand shows the state-of-the-art result for classifying 32 emotions. Our layer\nanalysis can derive an emotion graph to depict hierarchical relations among the\nemotions. Our emotion representations can be used to generate an emotion wheel\ndirectly comparable to the one from Plutchik's\\LN model, and also augment the\nvalues of missing emotions in the PAD emotional state model.\n", "rewritten_text": "This paper introduces a novel deep learning framework for generating fine-grained emotion embeddings, enabling computational descriptions of psychological emotion models.  The framework integrates a contextualized embedding encoder with a multi-head probing classifier to interpret dynamically learned representations optimized for emotion classification.  Evaluated on the Empathetic Dialogues dataset, our model achieves state-of-the-art performance in classifying 32 emotions.  Furthermore, layer analysis allows us to derive an emotion graph illustrating hierarchical relationships between emotions.  These representations can generate an emotion wheel directly comparable to Plutchik's model and augment missing emotion values in the PAD emotional state model.\n"}, "1912.06842": {"original_text": "  The main requisite for fine-grained recognition task is to focus on subtle\ndiscriminative details that make the subordinate classes different from each\nother. We note that existing methods implicitly address this requirement and\nleave it to a data-driven pipeline to figure out what makes a subordinate class\ndifferent from the others. This results in two major limitations: First, the\nnetwork focuses on the most obvious distinctions between classes and overlooks\nmore subtle inter-class variations. Second, the chance of misclassifying a\ngiven sample in any of the negative classes is considered equal, while in fact,\nconfusions generally occur among only the most similar classes. Here, we\npropose to explicitly force the network to find the subtle differences among\nclosely related classes. In this pursuit, we introduce two key novelties that\ncan be easily plugged into existing end-to-end deep learning pipelines. On one\nhand, we introduce diversification block which masks the most salient features\nfor an input to force the network to use more subtle cues for its correct\nclassification. Concurrently, we introduce a gradient-boosting loss function\nthat focuses only on the confusing classes for each sample and therefore moves\nswiftly along the direction on the loss surface that seeks to resolve these\nambiguities. The synergy between these two blocks helps the network to learn\nmore effective feature representations. Comprehensive experiments are performed\non five challenging datasets. Our approach outperforms existing methods using\nsimilar experimental setting on all five datasets.\n", "rewritten_text": "Fine-grained recognition requires focusing on subtle discriminative details that distinguish subordinate classes.  Existing methods implicitly address this, relying on data-driven learning to identify these distinctions. This approach has two limitations: first, it prioritizes readily apparent inter-class differences, neglecting subtler variations; second, it treats all negative classes as equally likely sources of misclassification, ignoring the fact that confusion primarily arises among similar classes.\n\nTo address these limitations, we propose explicitly forcing the network to learn subtle differences between closely related classes.  We introduce two novel components readily integrated into existing end-to-end deep learning pipelines: a diversification block and a gradient-boosting loss function.  The diversification block masks salient features, compelling the network to rely on subtler cues for classification.  Simultaneously, the gradient-boosting loss function focuses solely on the most confusable classes for each sample, efficiently navigating the loss landscape to resolve ambiguities.  The synergy between these components enhances feature representation learning.\n\nComprehensive experiments on five challenging datasets demonstrate that our approach outperforms existing methods under comparable experimental conditions across all datasets.\n"}, "1310.7447": {"original_text": "  A new method for removing impulse noise from speech in the wavelet transform\ndomain is proposed. The method utilizes the multiresolution property of the\nwavelet transform, which provides finer time resolution at the higher\nfrequencies than the short-time Fourier transform (STFT), to effectively\nidentify and remove impulse noise. It uses two features of speech to\ndiscriminate speech from impulse noise: one is the slow time-varying nature of\nspeech and the other is the Lipschitz regularity of the speech components. On\nthe basis of these features, an algorithm has been developed to identify and\nsuppress wavelet coefficients that correspond to impulse noise. Experiment\nresults show that the new method is able to significantly reduce impulse noise\nwithout degrading the quality of the speech signal or introducing any audible\nartifacts.\n", "rewritten_text": "This paper proposes a novel method for impulse noise reduction in speech signals within the wavelet transform domain.  Leveraging the wavelet transform's multiresolution property\u2014offering superior time resolution at higher frequencies compared to the short-time Fourier transform (STFT)\u2014the method effectively identifies and removes impulse noise.  This is achieved by exploiting two key characteristics of speech: its slow time-varying nature and the Lipschitz regularity of its components.  An algorithm based on these features identifies and suppresses wavelet coefficients associated with impulse noise. Experimental results demonstrate that this method significantly reduces impulse noise without compromising speech quality or introducing audible artifacts.\n"}, "2108.06771": {"original_text": "  Quantification of uncertainty in deep-neural-networks (DNN) based image\nregistration algorithms plays a critical role in the deployment of image\nregistration algorithms for clinical applications such as surgical planning,\nintraoperative guidance, and longitudinal monitoring of disease progression or\ntreatment efficacy as well as in research-oriented processing pipelines.\nCurrently available approaches for uncertainty estimation in DNN-based image\nregistration algorithms may result in sub-optimal clinical decision making due\nto potentially inaccurate estimation of the uncertainty of the registration\nstems for the assumed parametric distribution of the registration latent space.\nWe introduce NPBDREG, a fully non-parametric Bayesian framework for uncertainty\nestimation in DNN-based deformable image registration by combining an Adam\noptimizer with stochastic gradient Langevin dynamics (SGLD) to characterize the\nunderlying posterior distribution through posterior sampling. Thus, it has the\npotential to provide uncertainty estimates that are highly correlated with the\npresence of out of distribution data. We demonstrated the added-value of\nNPBDREG, compared to the baseline probabilistic VoxelMorph model (PrVXM), on\nbrain MRI image registration using $390$ image pairs from four publicly\navailable databases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a\nbetter correlation of the predicted uncertainty with out-of-distribution data\n($r>0.95$ vs. $r<0.5$) as well as a 7.3%improvement in the registration\naccuracy (Dice score, $0.74$ vs. $0.69$, $p \\ll 0.01$), and 18% improvement in\nregistration smoothness (percentage of folds in the deformation field, 0.014\nvs. 0.017, $p \\ll 0.01$). Finally, NPBDREG demonstrated a better generalization\ncapability for data corrupted by a mixed structure noise (Dice score of $0.73$\nvs. $0.69$, $p \\ll 0.01$) compared to the baseline PrVXM approach.\n", "rewritten_text": "Accurate uncertainty quantification in deep neural network (DNN)-based image registration algorithms is crucial for their deployment in clinical applications (surgical planning, intraoperative guidance, longitudinal disease monitoring, and treatment efficacy assessment) and research pipelines.  Current uncertainty estimation methods for DNN-based image registration may lead to suboptimal clinical decisions due to potentially inaccurate uncertainty quantification stemming from assumptions about the parametric distribution of the registration latent space.\n\nWe introduce NPBDREG, a fully non-parametric Bayesian framework for uncertainty estimation in DNN-based deformable image registration.  NPBDREG combines an Adam optimizer with stochastic gradient Langevin dynamics (SGLD) to characterize the posterior distribution via posterior sampling.  This approach has the potential to provide uncertainty estimates highly correlated with the presence of out-of-distribution data.\n\nWe evaluated NPBDREG against a probabilistic VoxelMorph model (PrVXM) baseline using 390 brain MRI image pairs from four publicly available databases (MGH10, CMUC12, ISBR18, and LPBA40).  NPBDREG exhibited significantly better correlation between predicted uncertainty and out-of-distribution data (r > 0.95 vs. r < 0.5).  Furthermore, it achieved a 7.3% improvement in registration accuracy (Dice score: 0.74 vs. 0.69, *p* \u226a 0.01) and an 18% improvement in registration smoothness (percentage of folds in the deformation field: 0.014 vs. 0.017, *p* \u226a 0.01).  Finally, NPBDREG demonstrated superior generalization to data corrupted by mixed-structure noise (Dice score: 0.73 vs. 0.69, *p* \u226a 0.01) compared to PrVXM.\n"}, "2308.13678": {"original_text": "  Reconstructing and tracking deformable surface with little or no texture has\nposed long-standing challenges. Fundamentally, the challenges stem from\ntextureless surfaces lacking features for establishing cross-image\ncorrespondences. In this work, we present a novel type of markers to\nproactively enrich the object's surface features, and thereby ease the 3D\nsurface reconstruction and correspondence tracking. Our markers are made of\nfluorescent dyes, visible only under the ultraviolet (UV) light and invisible\nunder regular lighting condition. Leveraging the markers, we design a\nmulti-camera system that captures surface deformation under the UV light and\nthe visible light in a time multiplexing fashion. Under the UV light, markers\non the object emerge to enrich its surface texture, allowing high-quality 3D\nshape reconstruction and tracking. Under the visible light, markers become\ninvisible, allowing us to capture the object's original untouched appearance.\nWe perform experiments on various challenging scenes, including hand gestures,\nfacial expressions, waving cloth, and hand-object interaction. In all these\ncases, we demonstrate that our system is able to produce robust, high-quality\n3D reconstruction and tracking.\n", "rewritten_text": "Reconstructing and tracking deformable, textureless surfaces has presented longstanding challenges, primarily due to the lack of features for establishing cross-image correspondences.  This work introduces novel fluorescent markers, visible only under ultraviolet (UV) light, to proactively enrich surface features and facilitate 3D surface reconstruction and correspondence tracking.  These markers are incorporated into a multi-camera system that captures surface deformation using time-multiplexed UV and visible light illumination.  Under UV light, the markers enhance surface texture, enabling high-quality 3D shape reconstruction and tracking.  Under visible light, the markers are invisible, preserving the object's original appearance.  Experiments on challenging scenarios\u2014including hand gestures, facial expressions, waving cloth, and hand-object interactions\u2014demonstrate the system's ability to produce robust, high-quality 3D reconstructions and tracking results.\n"}, "2408.12100": {"original_text": "  In recent years Plug-and-Play (PnP) methods have achieved state-of-the-art\nperformance in inverse imaging problems by replacing proximal operators with\ndenoisers. Based on the proximal gradient method, some theoretical results of\nPnP have appeared, where appropriate step size is crucial for convergence\nanalysis. However, in practical applications, applying PnP methods with\ntheoretically guaranteed step sizes is difficult, and these algorithms are\nlimited to Gaussian noise. In this paper,from a perspective of split convex\nfeasibility problems (SCFP), an adaptive PnP algorithm with Projected Landweber\nOperator (PnP-PLO) is proposed to address these issues. Numerical experiments\non image deblurring, super-resolution, and compressed sensing MRI experiments\nillustrate that PnP-PLO with theoretical guarantees outperforms\nstate-of-the-art methods such as RED and RED-PRO.\n", "rewritten_text": "Recent years have witnessed plug-and-play (PnP) methods achieve state-of-the-art performance in inverse imaging problems by substituting proximal operators with denoisers.  While some theoretical results for PnP, based on the proximal gradient method, have emerged\u2014highlighting the crucial role of appropriate step size for convergence\u2014practical application remains challenging.  Theoretically guaranteed step sizes are difficult to implement, and existing algorithms are often limited to Gaussian noise.  This paper addresses these limitations by proposing an adaptive PnP algorithm with a projected Landweber operator (PnP-PLO), framed within the context of split convex feasibility problems (SCFPs).  Numerical experiments in image deblurring, super-resolution, and compressed sensing MRI demonstrate that PnP-PLO, with its theoretical guarantees, outperforms state-of-the-art methods such as RED and RED-PRO.\n"}, "2210.13077": {"original_text": "  Novel-view synthesis (NVS) can be tackled through different approaches,\ndepending on the general setting: a single source image to a short video\nsequence, exact or noisy camera pose information, 3D-based information such as\npoint clouds etc. The most challenging scenario, the one where we stand in this\nwork, only considers a unique source image to generate a novel one from another\nviewpoint. However, in such a tricky situation, the latest learning-based\nsolutions often struggle to integrate the camera viewpoint transformation.\nIndeed, the extrinsic information is often passed as-is, through a\nlow-dimensional vector. It might even occur that such a camera pose, when\nparametrized as Euler angles, is quantized through a one-hot representation.\nThis vanilla encoding choice prevents the learnt architecture from inferring\nnovel views on a continuous basis (from a camera pose perspective). We claim it\nexists an elegant way to better encode relative camera pose, by leveraging\n3D-related concepts such as the epipolar constraint. We, therefore, introduce\nan innovative method that encodes the viewpoint transformation as a 2D feature\nimage. Such a camera encoding strategy gives meaningful insights to the network\nregarding how the camera has moved in space between the two views. By encoding\nthe camera pose information as a finite number of coloured epipolar lines, we\ndemonstrate through our experiments that our strategy outperforms vanilla\nencoding.\n", "rewritten_text": "Novel-view synthesis (NVS) employs various approaches depending on factors such as the input (a single image versus a short video sequence), the accuracy of camera pose information (exact or noisy), and the availability of 3D data (e.g., point clouds).  This work focuses on the most challenging scenario: generating a novel view from a single source image.  Existing learning-based methods often struggle in this context, inadequately integrating camera viewpoint transformations.  Extrinsic information is frequently passed as a low-dimensional vector, sometimes even quantized as a one-hot encoding of Euler angles. This simplistic encoding hinders the network's ability to infer novel views continuously from a camera pose perspective.\n\nWe propose a novel method for encoding relative camera pose that leverages 3D concepts, specifically the epipolar constraint.  Our approach represents the viewpoint transformation as a 2D feature image. This encoding provides the network with meaningful spatial context regarding camera movement between views. By encoding camera pose as a finite set of colored epipolar lines, our experimental results demonstrate superior performance compared to conventional low-dimensional encodings.\n"}, "2106.00184": {"original_text": "  Encouraging progress in few-shot semantic segmentation has been made by\nleveraging features learned upon base classes with sufficient training data to\nrepresent novel classes with few-shot examples. However, this feature sharing\nmechanism inevitably causes semantic aliasing between novel classes when they\nhave similar compositions of semantic concepts. In this paper, we reformulate\nfew-shot segmentation as a semantic reconstruction problem, and convert base\nclass features into a series of basis vectors which span a class-level semantic\nspace for novel class reconstruction. By introducing contrastive loss, we\nmaximize the orthogonality of basis vectors while minimizing semantic aliasing\nbetween classes. Within the reconstructed representation space, we further\nsuppress interference from other classes by projecting query features to the\nsupport vector for precise semantic activation. Our proposed approach, referred\nto as anti-aliasing semantic reconstruction (ASR), provides a systematic yet\ninterpretable solution for few-shot learning problems. Extensive experiments on\nPASCAL VOC and MS COCO datasets show that ASR achieves strong results compared\nwith the prior works.\n", "rewritten_text": "Recent progress in few-shot semantic segmentation has leveraged features learned from base classes with ample training data to represent novel classes using only a few examples.  However, this feature-sharing mechanism often leads to semantic aliasing between novel classes with similar semantic compositions.  This paper addresses this limitation by reformulating few-shot segmentation as a semantic reconstruction problem. We transform base class features into a set of basis vectors spanning a class-level semantic space, enabling the reconstruction of novel classes.  A contrastive loss function maximizes the orthogonality of these basis vectors, thereby minimizing semantic aliasing.  Furthermore, we suppress interference from other classes by projecting query features onto the support vector, ensuring precise semantic activation within the reconstructed representation space.  Our proposed approach, Anti-Aliasing Semantic Reconstruction (ASR), offers a systematic and interpretable solution to few-shot learning challenges.  Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that ASR achieves state-of-the-art results.\n"}, "2310.00274": {"original_text": "  Africa has a very low doctor-to-patient ratio. At very busy clinics, doctors\ncould see 30+ patients per day -- a heavy patient burden compared with\ndeveloped countries -- but productivity tools such as clinical automatic speech\nrecognition (ASR) are lacking for these overworked clinicians. However,\nclinical ASR is mature, even ubiquitous, in developed nations, and\nclinician-reported performance of commercial clinical ASR systems is generally\nsatisfactory. Furthermore, the recent performance of general domain ASR is\napproaching human accuracy. However, several gaps exist. Several publications\nhave highlighted racial bias with speech-to-text algorithms and performance on\nminority accents lags significantly. To our knowledge, there is no publicly\navailable research or benchmark on accented African clinical ASR, and speech\ndata is non-existent for the majority of African accents. We release\nAfriSpeech, 200hrs of Pan-African English speech, 67,577 clips from 2,463\nunique speakers across 120 indigenous accents from 13 countries for clinical\nand general domain ASR, a benchmark test set, with publicly available\npre-trained models with SOTA performance on the AfriSpeech benchmark.\n", "rewritten_text": "Africa faces a critically low doctor-to-patient ratio.  In busy clinics, doctors may see 30 or more patients daily, a significantly heavier burden than in developed countries.  However, these overworked clinicians lack access to productivity tools such as clinical automatic speech recognition (ASR), despite its maturity and widespread use in developed nations, where commercial systems generally achieve satisfactory clinician-reported performance.  Moreover, general-domain ASR is nearing human-level accuracy.  Nevertheless, significant gaps remain.  Studies have documented racial bias in speech-to-text algorithms, with performance lagging considerably for minority accents.  To our knowledge, no publicly available research or benchmark exists for accented African clinical ASR, and speech data for most African accents are nonexistent.  To address this, we introduce AfriSpeech, a 200-hour corpus of Pan-African English speech comprising 67,577 clips from 2,463 unique speakers across 120 indigenous accents from 13 countries.  This resource includes a benchmark test set and publicly available pre-trained models demonstrating state-of-the-art (SOTA) performance on the AfriSpeech benchmark, intended for both clinical and general-domain ASR applications.\n"}, "2306.02351": {"original_text": "  We present the RSSOD-Bench dataset for salient object detection (SOD) in\noptical remote sensing imagery. While SOD has achieved success in natural scene\nimages with deep learning, research in SOD for remote sensing imagery (RSSOD)\nis still in its early stages. Existing RSSOD datasets have limitations in terms\nof scale, and scene categories, which make them misaligned with real-world\napplications. To address these shortcomings, we construct the RSSOD-Bench\ndataset, which contains images from four different cities in the USA. The\ndataset provides annotations for various salient object categories, such as\nbuildings, lakes, rivers, highways, bridges, aircraft, ships, athletic fields,\nand more. The salient objects in RSSOD-Bench exhibit large-scale variations,\ncluttered backgrounds, and different seasons. Unlike existing datasets,\nRSSOD-Bench offers uniform distribution across scene categories. We benchmark\n23 different state-of-the-art approaches from both the computer vision and\nremote sensing communities. Experimental results demonstrate that more research\nefforts are required for the RSSOD task.\n", "rewritten_text": "This paper introduces RSSOD-Bench, a new dataset for salient object detection (SOD) in optical remote sensing imagery.  While deep learning has significantly advanced SOD in natural scenes, research in remote sensing SOD (RSSOD) remains in its infancy.  Existing RSSOD datasets suffer from limitations in scale and scene category diversity, hindering their applicability to real-world scenarios.  To overcome these limitations, we created RSSOD-Bench, a dataset comprising imagery from four diverse US cities.  It provides annotations for a wide range of salient object categories, including buildings, lakes, rivers, highways, bridges, aircraft, ships, athletic fields, and others.  RSSOD-Bench features salient objects with significant scale variations, cluttered backgrounds, and seasonal diversity.  Unlike existing datasets, it offers a uniform distribution across scene categories.  We benchmark 23 state-of-the-art SOD methods from both computer vision and remote sensing, revealing that further research is needed to effectively address the challenges of RSSOD.\n"}, "2303.12946": {"original_text": "  We have made a dataset of camouflage object detection mainly for complex\nseabed scenes, and named it UnderWater RGB&Sonar,or UW-RS for short. The UW-RS\ndataset contains a total of 1972 image data. The dataset mainly consists of two\nparts, namely underwater optical data part (UW-R dataset) and underwater sonar\ndata part (UW-S dataset).\n", "rewritten_text": "We introduce UnderWater RGB&Sonar (UW-RS), a new dataset for complex seabed camouflage object detection.  UW-RS comprises 1972 images and consists of two parts: an underwater optical data subset (UW-R) and an underwater sonar data subset (UW-S).\n"}, "2308.02738": {"original_text": "  Recently, visual-language learning has shown great potential in enhancing\nvisual-based person re-identification (ReID). Existing visual-language\nlearning-based ReID methods often focus on whole-body scale image-text feature\nalignment, while neglecting supervisions on fine-grained part features. This\nchoice simplifies the learning process but cannot guarantee within-part feature\nsemantic consistency thus hindering the final performance. Therefore, we\npropose to enhance fine-grained visual features with part-informed language\nsupervision for ReID tasks. The proposed method, named Part-Informed\nVisual-language Learning ($\\pi$-VL), suggests that (i) a human parsing-guided\nprompt tuning strategy and (ii) a hierarchical fusion-based visual-language\nalignment paradigm play essential roles in ensuring within-part feature\nsemantic consistency. Specifically, we combine both identity labels and parsing\nmaps to constitute pixel-level text prompts and fuse multi-stage visual\nfeatures with a light-weight auxiliary head to perform fine-grained image-text\nalignment. As a plug-and-play and inference-free solution, our $\\pi$-VL\nachieves substantial improvements over previous state-of-the-arts on four\ncommon-used ReID benchmarks, especially reporting 90.3% Rank-1 and 76.5% mAP\nfor the most challenging MSMT17 database without bells and whistles.\n", "rewritten_text": "Visual-language learning has recently shown significant potential for improving person re-identification (ReID).  Existing methods primarily focus on aligning whole-body image-text features, neglecting fine-grained part-level supervision. This simplification, while facilitating training, compromises within-part semantic consistency and limits performance.  To address this, we propose Part-Informed Visual-Language Learning ($\\pi$-VL), which enhances fine-grained visual features using part-informed language supervision.  $\\pi$-VL employs (i) a human parsing-guided prompt tuning strategy and (ii) a hierarchical fusion-based visual-language alignment paradigm to ensure within-part semantic consistency.  Specifically, we generate pixel-level text prompts by combining identity labels and parsing maps, and fuse multi-stage visual features with a lightweight auxiliary head for fine-grained image-text alignment.  As a plug-and-play, inference-free solution, $\\pi$-VL achieves substantial improvements over state-of-the-art methods on four widely used ReID benchmarks. Notably, on the challenging MSMT17 dataset, $\\pi$-VL achieves 90.3% Rank-1 accuracy and 76.5% mean Average Precision (mAP) without employing any additional techniques.\n"}, "1812.05785": {"original_text": "  It is prohibitively expensive to annotate a large-scale video-based person\nre-identification (re-ID) dataset, which makes fully supervised methods\ninapplicable to real-world deployment. How to maximally reduce the annotation\ncost while retaining the re-ID performance becomes an interesting problem. In\nthis paper, we address this problem by integrating an active learning scheme\ninto a deep learning framework. Noticing that the truly matched tracklet-pairs,\nalso denoted as true positives (TP), are the most informative samples for our\nre-ID model, we propose a sampling criterion to choose the most TP-likely\ntracklet-pairs for annotation. A view-aware sampling strategy considering\nview-specific biases is designed to facilitate candidate selection, followed by\nan adaptive resampling step to leave out the selected candidates that are\nunnecessary to annotate. Our method learns the re-ID model and updates the\nannotation set iteratively. The re-ID model is supervised by the tracklets'\npesudo labels that are initialized by treating each tracklet as a distinct\nclass. With the gained annotations of the actively selected candidates, the\ntracklets' pesudo labels are updated by label merging and further used to\nre-train our re-ID model. While being simple, the proposed method demonstrates\nits effectiveness on three video-based person re-ID datasets. Experimental\nresults show that less than 3\\% pairwise annotations are needed for our method\nto reach comparable performance with the fully-supervised setting.\n", "rewritten_text": "The prohibitive cost of annotating large-scale video-based person re-identification (re-ID) datasets renders fully supervised methods impractical for real-world deployment.  Minimizing annotation costs while maintaining re-ID performance is a significant challenge. This paper addresses this challenge by integrating active learning into a deep learning framework.  Recognizing that truly matched tracklet pairs (true positives, or TP) are the most informative samples, we propose a novel sampling criterion to select TP-likely tracklet pairs for annotation.  A view-aware sampling strategy mitigates view-specific biases in candidate selection, followed by an adaptive resampling step to eliminate redundant candidates.  Our iterative method simultaneously learns the re-ID model and updates the annotation set.  Initially, the re-ID model is trained using pseudo-labels, assigning each tracklet to a distinct class.  Subsequently, the pseudo-labels are refined through label merging using the annotations from actively selected candidates, and the model is retrained.  Despite its simplicity, our method demonstrates effectiveness on three video-based person re-ID datasets.  Experimental results show that our method achieves performance comparable to fully supervised methods with less than 3% pairwise annotation.\n"}, "2202.05457": {"original_text": "  Sentiment Analysis typically refers to using natural language processing,\ntext analysis and computational linguistics to extract affect and emotion based\ninformation from text data. Our work explores how we can effectively use deep\nneural networks in transfer learning and joint dual input learning settings to\neffectively classify sentiments and detect hate speech in Hindi and Bengali\ndata. We start by training Word2Vec word embeddings for Hindi \\textbf{HASOC\ndataset} and Bengali hate speech and then train LSTM and subsequently, employ\nparameter sharing based transfer learning to Bengali sentiment classifiers by\nreusing and fine-tuning the trained weights of Hindi classifiers with both\nclassifier being used as baseline in our study. Finally, we use BiLSTM with\nself attention in joint dual input learning setting where we train a single\nneural network on Hindi and Bengali dataset simultaneously using their\nrespective embeddings.\n", "rewritten_text": "Sentiment analysis uses natural language processing, text analysis, and computational linguistics to extract affective and emotional information from text.  This work explores the effective use of deep neural networks, employing transfer learning and joint dual-input learning, to classify sentiment and detect hate speech in Hindi and Bengali data.  We begin by training Word2Vec word embeddings on the Hindi HASOC dataset and Bengali hate speech data.  We then train LSTMs, subsequently applying parameter-sharing transfer learning to Bengali sentiment classifiers by reusing and fine-tuning the weights of the trained Hindi classifiers.  Both classifiers serve as baselines. Finally, we utilize a BiLSTM with self-attention in a joint dual-input learning setting, training a single neural network simultaneously on the Hindi and Bengali datasets using their respective embeddings.\n"}, "1210.0115": {"original_text": "  A framework of demosaicing and superresolution for color filter array (CFA)\nvia residual image reconstruction and sparse representation is presented.Given\nthe intermediate image produced by certain demosaicing and interpolation\ntechnique, a residual image between the final reconstruction image and the\nintermediate image is reconstructed using sparse representation.The final\nreconstruction image has richer edges and details than that of the intermediate\nimage. Specifically, a generic dictionary is learned from a large set of\ncomposite training data composed of intermediate data and residual data. The\nlearned dictionary implies a mapping between the two data. A specific\ndictionary adaptive to the input CFA is learned thereafter. Using the adaptive\ndictionary, the sparse coefficients of intermediate data are computed and\ntransformed to predict residual image. The residual image is added back into\nthe intermediate image to obtain the final reconstruction image. Experimental\nresults demonstrate the state-of-the-art performance in terms of PSNR and\nsubjective visual perception.\n", "rewritten_text": "This paper presents a novel demosaicing and super-resolution framework for color filter array (CFA) images based on residual image reconstruction and sparse representation.  Given an intermediate image generated by a demosaicing and interpolation technique, a residual image\u2014representing the difference between the final reconstruction and the intermediate image\u2014is reconstructed using sparse representation. This residual image enhances the final reconstruction with richer edges and details.  Specifically, a generic dictionary is learned from a large training dataset comprising paired intermediate and residual images. This learned dictionary implicitly defines a mapping between these two image types. Subsequently, an input-adaptive dictionary is learned for each CFA image.  Using this adaptive dictionary, the sparse coefficients of the intermediate image are computed and transformed to predict the residual image.  Finally, the predicted residual image is added to the intermediate image to yield the final, high-resolution reconstruction. Experimental results demonstrate state-of-the-art performance, as measured by PSNR and subjective visual quality.\n"}, "2406.01300": {"original_text": "  Text-guided image generation enables the creation of visual content from\ntextual descriptions. However, certain visual concepts cannot be effectively\nconveyed through language alone. This has sparked a renewed interest in\nutilizing the CLIP image embedding space for more visually-oriented tasks\nthrough methods such as IP-Adapter. Interestingly, the CLIP image embedding\nspace has been shown to be semantically meaningful, where linear operations\nwithin this space yield semantically meaningful results. Yet, the specific\nmeaning of these operations can vary unpredictably across different images. To\nharness this potential, we introduce pOps, a framework that trains specific\nsemantic operators directly on CLIP image embeddings. Each pOps operator is\nbuilt upon a pretrained Diffusion Prior model. While the Diffusion Prior model\nwas originally trained to map between text embeddings and image embeddings, we\ndemonstrate that it can be tuned to accommodate new input conditions, resulting\nin a diffusion operator. Working directly over image embeddings not only\nimproves our ability to learn semantic operations but also allows us to\ndirectly use a textual CLIP loss as an additional supervision when needed. We\nshow that pOps can be used to learn a variety of photo-inspired operators with\ndistinct semantic meanings, highlighting the semantic diversity and potential\nof our proposed approach.\n", "rewritten_text": "Text-guided image generation, while powerful, struggles to convey certain visual concepts solely through textual descriptions.  This limitation has fueled renewed interest in leveraging the CLIP image embedding space for visually-oriented tasks, exemplified by methods like IP-Adapter.  Although linear operations within this semantically meaningful space often produce semantically meaningful results, their precise interpretation can vary unpredictably across images.\n\nTo address this, we introduce pOps, a framework that trains specific semantic operators directly on CLIP image embeddings.  Each pOps operator is built upon a pre-trained Diffusion Prior model.  While originally trained to map text embeddings to image embeddings, we demonstrate that this model can be effectively fine-tuned to handle novel input conditions, thereby creating a diffusion operator.  Operating directly on image embeddings enhances our ability to learn semantic operations and allows the direct application of a textual CLIP loss for additional supervision when necessary.\n\nOur results show that pOps can learn a diverse range of photo-inspired operators with distinct semantic meanings, highlighting the potential and semantic richness of our approach.\n"}, "1805.07477": {"original_text": "  Augmenting neural networks with skip connections, as introduced in the\nso-called ResNet architecture, surprised the community by enabling the training\nof networks of more than 1,000 layers with significant performance gains. This\npaper deciphers ResNet by analyzing the effect of skip connections, and puts\nforward new theoretical results on the advantages of identity skip connections\nin neural networks. We prove that the skip connections in the residual blocks\nfacilitate preserving the norm of the gradient, and lead to stable\nback-propagation, which is desirable from optimization perspective. We also\nshow that, perhaps surprisingly, as more residual blocks are stacked, the\nnorm-preservation of the network is enhanced. Our theoretical arguments are\nsupported by extensive empirical evidence. Can we push for extra\nnorm-preservation? We answer this question by proposing an efficient method to\nregularize the singular values of the convolution operator and making the\nResNet's transition layers extra norm-preserving. Our numerical investigations\ndemonstrate that the learning dynamics and the classification performance of\nResNet can be improved by making it even more norm preserving. Our results and\nthe introduced modification for ResNet, referred to as Procrustes ResNets, can\nbe used as a guide for training deeper networks and can also inspire new deeper\narchitectures.\n", "rewritten_text": "The introduction of skip connections in the ResNet architecture unexpectedly enabled the training of networks exceeding 1000 layers, yielding significant performance improvements. This paper analyzes the impact of skip connections in ResNets, presenting novel theoretical results on the benefits of identity skip connections. We prove that these connections facilitate gradient norm preservation, leading to stable backpropagation\u2014a crucial advantage for optimization.  Furthermore, we demonstrate that norm preservation is enhanced as more residual blocks are added.  These theoretical findings are supported by extensive empirical evidence.\n\nTo further enhance norm preservation, we propose an efficient method to regularize the singular values of the convolutional operators within ResNet's transition layers.  Our numerical experiments show that this modification, resulting in what we term \"Procrustes ResNets,\" improves both the learning dynamics and classification performance.  These results provide valuable guidance for training even deeper networks and may inspire the design of novel architectures.\n"}, "2104.08668": {"original_text": "  Communicating new research ideas involves highlighting similarities and\ndifferences with past work. Authors write fluent, often long sections to survey\nthe distinction of a new paper with related work. In this work we model\ngenerating related work sections while being cognisant of the motivation behind\nciting papers. Our content planning model generates a tree of cited papers\nbefore a surface realization model lexicalizes this skeleton. Our model\noutperforms several strong state-of-the-art summarization and multi-document\nsummarization models on generating related work on an ACL Anthology (AA) based\ndataset which we contribute.\n", "rewritten_text": "Communicating novel research requires highlighting the similarities and differences between the new work and prior research.  Authors often dedicate lengthy sections to this task, meticulously differentiating their contributions from related work.  This work presents a model for generating such \"related work\" sections, explicitly considering the motivations behind citing specific papers. Our approach employs a two-stage process: a content planning model generates a citation tree, which is then lexicalized into text by a surface realization model.  Our model outperforms several strong state-of-the-art summarization and multi-document summarization models on a new dataset derived from the ACL Anthology (AA), which we also contribute.\n"}, "1608.08021": {"original_text": "  This paper presents how we can achieve the state-of-the-art accuracy in\nmulti-category object detection task while minimizing the computational cost by\nadapting and combining recent technical innovations. Following the common\npipeline of \"CNN feature extraction + region proposal + RoI classification\", we\nmainly redesign the feature extraction part, since region proposal part is not\ncomputationally expensive and classification part can be efficiently compressed\nwith common techniques like truncated SVD. Our design principle is \"less\nchannels with more layers\" and adoption of some building blocks including\nconcatenated ReLU, Inception, and HyperNet. The designed network is deep and\nthin and trained with the help of batch normalization, residual connections,\nand learning rate scheduling based on plateau detection. We obtained solid\nresults on well-known object detection benchmarks: 83.8% mAP (mean average\nprecision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only\n750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA\nTitan X GPU. Theoretically, our network requires only 12.3% of the\ncomputational cost compared to ResNet-101, the winner on VOC2012.\n", "rewritten_text": "This paper presents a novel approach to achieving state-of-the-art accuracy in multi-category object detection while minimizing computational cost.  By adapting and combining recent innovations, we redesigned the feature extraction stage of the common \"CNN feature extraction + region proposal + RoI classification\" pipeline.  The region proposal stage is computationally inexpensive, and the classification stage can be efficiently compressed using techniques such as truncated SVD.  Our design prioritizes \"fewer channels with more layers,\" incorporating building blocks such as concatenated ReLU, Inception modules, and HyperNetworks.  The resulting deep, narrow network is trained using batch normalization, residual connections, and a plateau-based learning rate schedule.  We achieved strong results on established object detection benchmarks: 83.8% mean average precision (mAP) on VOC2007 and 82.5% mAP on VOC2012 (achieving second place).  Inference time was 750 ms/image on a single core of an Intel i7-6700K CPU and 46 ms/image on an NVIDIA Titan X GPU.  Theoretically, our network requires only 12.3% of the computational cost of ResNet-101, the VOC2012 winner.\n"}, "2210.05738": {"original_text": "  In this work, we propose to explicitly use the landmarks of prostate to guide\nthe MR-TRUS image registration. We first train a deep neural network to\nautomatically localize a set of meaningful landmarks, and then directly\ngenerate the affine registration matrix from the location of these landmarks.\nFor landmark localization, instead of directly training a network to predict\nthe landmark coordinates, we propose to regress a full-resolution distance map\nof the landmark, which is demonstrated effective in avoiding statistical bias\nto unsatisfactory performance and thus improving performance. We then use the\npredicted landmarks to generate the affine transformation matrix, which\noutperforms the clinicians' manual rigid registration by a significant margin\nin terms of TRE.\n", "rewritten_text": "This work proposes a novel approach to MR-TRUS image registration guided by prostate landmarks.  We first train a deep neural network to automatically localize a set of meaningful landmarks.  Instead of directly predicting landmark coordinates, we regress a full-resolution distance map for each landmark. This approach effectively mitigates statistical bias, leading to improved performance.  Finally, we generate an affine transformation matrix from the predicted landmark locations.  This automated method significantly outperforms manual rigid registration by clinicians, as measured by target registration error (TRE).\n"}, "2006.08844": {"original_text": "  We tackle the problem of establishing dense pixel-wise correspondences\nbetween a pair of images. In this work, we introduce Dual-Resolution\nCorrespondence Networks (DualRC-Net), to obtain pixel-wise correspondences in a\ncoarse-to-fine manner. DualRC-Net extracts both coarse- and fine- resolution\nfeature maps. The coarse maps are used to produce a full but coarse 4D\ncorrelation tensor, which is then refined by a learnable neighbourhood\nconsensus module. The fine-resolution feature maps are used to obtain the final\ndense correspondences guided by the refined coarse 4D correlation tensor. The\nselected coarse-resolution matching scores allow the fine-resolution features\nto focus only on a limited number of possible matches with high confidence. In\nthis way, DualRC-Net dramatically increases matching reliability and\nlocalisation accuracy, while avoiding to apply the expensive 4D convolution\nkernels on fine-resolution feature maps. We comprehensively evaluate our method\non large-scale public benchmarks including HPatches, InLoc, and Aachen\nDay-Night. It achieves the state-of-the-art results on all of them.\n", "rewritten_text": "This work addresses the problem of establishing dense pixel-wise correspondences between image pairs.  We introduce Dual-Resolution Correspondence Networks (DualRC-Net), a novel approach that refines correspondences in a coarse-to-fine manner.  DualRC-Net extracts both coarse and fine-resolution feature maps.  A full, albeit coarse, 4D correlation tensor is generated from the coarse maps and subsequently refined using a learnable neighborhood consensus module.  The refined tensor then guides the generation of final dense correspondences from the fine-resolution feature maps.  By using the coarse-resolution matching scores to select only high-confidence matches, the fine-resolution features process a limited number of candidates, significantly improving matching reliability and localization accuracy while avoiding computationally expensive 4D convolutions on the high-resolution features.  Comprehensive evaluation on large-scale public benchmarks\u2014including HPatches, InLoc, and Aachen Day-Night\u2014demonstrates state-of-the-art performance across all datasets.\n"}, "1704.00405": {"original_text": "  As for semantic role labeling (SRL) task, when it comes to utilizing parsing\ninformation, both traditional methods and recent recurrent neural network (RNN)\nbased methods use the feature engineering way. In this paper, we propose Syntax\nAware Long Short Time Memory(SA-LSTM). The structure of SA-LSTM modifies\naccording to dependency parsing information in order to model parsing\ninformation directly in an architecture engineering way instead of feature\nengineering way. We experimentally demonstrate that SA-LSTM gains more\nimprovement from the model architecture. Furthermore, SA-LSTM outperforms the\nstate-of-the-art on CPB 1.0 significantly according to Student t-test\n($p<0.05$).\n", "rewritten_text": "This paper introduces Syntax-Aware Long Short-Term Memory (SA-LSTM), a novel approach to semantic role labeling (SRL).  Unlike traditional methods and recent recurrent neural network (RNN)-based methods that rely on feature engineering to incorporate parsing information, SA-LSTM directly models this information through architectural modifications based on dependency parsing.  Our experimental results demonstrate that SA-LSTM achieves significant improvements over the state-of-the-art on the CPB 1.0 dataset (Student's t-test, *p* < 0.05).\n"}, "2304.08481": {"original_text": "  High-definition (HD) semantic maps are crucial in enabling autonomous\nvehicles to navigate urban environments. The traditional method of creating\noffline HD maps involves labor-intensive manual annotation processes, which are\nnot only costly but also insufficient for timely updates. Recent studies have\nproposed an alternative approach that generates local maps using online sensor\nobservations. However, this approach is limited by the sensor's perception\nrange and its susceptibility to occlusions. In this study, we propose Neural\nMap Prior (NMP), a neural representation of global maps. This representation\nautomatically updates itself and improves the performance of local map\ninference. Specifically, we utilize two approaches to achieve this. Firstly, to\nintegrate a strong map prior into local map inference, we apply\ncross-attention, a mechanism that dynamically identifies correlations between\ncurrent and prior features. Secondly, to update the global neural map prior, we\nutilize a learning-based fusion module that guides the network in fusing\nfeatures from previous traversals. Our experimental results, based on the\nnuScenes dataset, demonstrate that our framework is highly compatible with\nvarious map segmentation and detection architectures. It significantly improves\nmap prediction performance, even in challenging weather conditions and\nsituations with a longer perception range. To the best of our knowledge, this\nis the first learning-based system for creating a global map prior.\n", "rewritten_text": "High-definition (HD) semantic maps are crucial for autonomous vehicle navigation in urban environments.  Traditional offline HD map creation relies on labor-intensive manual annotation, resulting in high costs and slow updates.  While recent approaches generate local maps online using sensor observations, these are limited by sensor range and susceptibility to occlusion.  This study introduces Neural Map Prior (NMP), a novel neural representation of global maps that enables automatic updates and improved local map inference.  We achieve this through two key mechanisms:  first, cross-attention dynamically identifies correlations between current and prior features, integrating a strong map prior into local map inference; second, a learning-based fusion module updates the global NMP by intelligently fusing features from previous vehicle traversals.  Experiments on the nuScenes dataset demonstrate the framework's compatibility with various map segmentation and detection architectures, significantly improving map prediction performance even under challenging weather conditions and extended perception ranges.  To our knowledge, this is the first learning-based system for generating a global map prior.\n"}, "2202.10108": {"original_text": "  Vision transformers have shown great potential in various computer vision\ntasks owing to their strong capability to model long-range dependency using the\nself-attention mechanism. Nevertheless, they treat an image as a 1D sequence of\nvisual tokens, lacking an intrinsic inductive bias (IB) in modeling local\nvisual structures and dealing with scale variance, which is instead learned\nimplicitly from large-scale training data with longer training schedules. In\nthis paper, we propose a Vision Transformer Advanced by Exploring intrinsic IB\nfrom convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid\nreduction modules to downsample and embed the input image into tokens with rich\nmulti-scale context using multiple convolutions with different dilation rates.\nIn this way, it acquires an intrinsic scale invariance IB and can learn robust\nfeature representation for objects at various scales. Moreover, in each\ntransformer layer, ViTAE has a convolution block parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. The proposed two kinds\nof cells are stacked in both isotropic and multi-stage manners to formulate two\nfamilies of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on\nthe ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and\nAP10K datasets validate the superiority of our models over the baseline\ntransformer models and concurrent works. Besides, we scale up our ViTAE model\nto 644M parameters and obtain the state-of-the-art classification performance,\ni.e., 88.5% Top-1 classification accuracy on ImageNet validation set and the\nbest 91.2% Top-1 accuracy on ImageNet real validation set, without using extra\nprivate data.\n", "rewritten_text": "Vision transformers have demonstrated significant potential in various computer vision tasks due to their ability to model long-range dependencies using the self-attention mechanism.  However, their treatment of images as one-dimensional sequences of visual tokens lacks an intrinsic inductive bias (IB) for modeling local visual structures and handling scale variance.  This bias is instead learned implicitly from extensive training data and prolonged training schedules.  This paper introduces ViTAE (Vision Transformer Advanced by Exploring intrinsic IB from convolutions), which incorporates spatial pyramid reduction modules to downsample and embed the input image into tokens enriched with multi-scale context.  This is achieved using multiple convolutions with varying dilation rates, thereby imparting an intrinsic scale-invariance IB and enabling the learning of robust feature representations for objects at diverse scales.  Furthermore, each transformer layer in ViTAE includes a convolutional block parallel to the multi-head self-attention module.  The features from these blocks are fused and fed into the feed-forward network, introducing an intrinsic locality IB and enabling the collaborative learning of local features and global dependencies.  Two families of ViTAE models\u2014vanilla ViTAE and ViTAEv2\u2014are constructed by stacking these components isotropically and in a multi-stage manner.  Experiments on ImageNet, and downstream tasks on MS COCO, ADE20K, and AP10K datasets, demonstrate the superiority of our models over baseline transformer models and contemporary approaches.  Scaling our ViTAE model to 644M parameters achieves state-of-the-art classification performance: 88.5% Top-1 accuracy on the ImageNet validation set and 91.2% Top-1 accuracy on the ImageNet real validation set, without employing any external private data.\n"}, "1809.08371": {"original_text": "  Human pose estimation is an essential yet challenging task in computer\nvision. One of the reasons for this difficulty is that there are many redundant\nregions in the images. In this work, we proposed a convolutional network\narchitecture combined with the novel attention model. We named it attention\nconvolutional neural network (ACNN). ACNN learns to focus on specific regions\nof different input features. It's a multi-stage architecture. Early stages\nfiltrate the \"nothing-regions\", such as background and redundant body parts.\nAnd then, they submit the important regions which contain the joints of the\nhuman body to the following stages to get a more accurate result. What's more,\nit does not require extra manual annotations and self-learning is one of our\nintentions. We separately trained the network because the attention learning\ntask and the pose estimation task are not independent. State-of-the-art\nperformance is obtained on the MPII benchmarks.\n", "rewritten_text": "Human pose estimation is a crucial yet challenging computer vision task.  This difficulty arises partly from the presence of numerous redundant image regions.  This work proposes a novel convolutional neural network architecture incorporating an attention model, termed the Attention Convolutional Neural Network (ACNN).  ACNN learns to focus on specific regions of diverse input features through a multi-stage architecture.  Early stages filter out irrelevant regions, such as background and redundant body parts, forwarding only salient regions containing human joints to subsequent stages for improved accuracy.  Furthermore, ACNN requires no additional manual annotations; self-supervised learning was a design goal.  The network was trained separately for attention learning and pose estimation, given their interdependence.  The resulting model achieved state-of-the-art performance on the MPII benchmark.\n"}, "1310.0302": {"original_text": "  Surface registration is a technique that is used in various areas such as\nobject recognition and 3D model reconstruction. Problem of surface registration\ncan be analyzed as an optimization problem of seeking a rigid motion between\ntwo different views. Genetic algorithms can be used for solving this\noptimization problem, both for obtaining the robust parameter estimation and\nfor its fine-tuning. The main drawback of genetic algorithms is that they are\ntime consuming which makes them unsuitable for online applications. Modern\nacquisition systems enable the implementation of the solutions that would\nimmediately give the information on the rotational angles between the different\nviews, thus reducing the dimension of the optimization problem. The paper gives\nan analysis of the genetic algorithm implemented in the conditions when the\nrotation matrix is known and a comparison of these results with results when\nthis information is not available.\n", "rewritten_text": "Surface registration, a technique used in object recognition and 3D model reconstruction, can be formulated as an optimization problem seeking the rigid motion between two views.  Genetic algorithms (GAs) can solve this problem, providing robust parameter estimation and fine-tuning. However, GAs' computational cost renders them unsuitable for real-time applications.  Modern acquisition systems can directly provide rotational angles between views, thereby reducing the dimensionality of the optimization problem. This paper analyzes a GA implementation under the condition of a known rotation matrix and compares its performance to that of a GA where this information is unavailable.\n"}, "2008.0194": {"original_text": "  While the progress of machine translation of written text has come far in the\npast several years thanks to the increasing availability of parallel corpora\nand corpora-based training technologies, automatic translation of spoken text\nand dialogues remains challenging even for modern systems. In this paper, we\naim to boost the machine translation quality of conversational texts by\nintroducing a newly constructed Japanese-English business conversation parallel\ncorpus. A detailed analysis of the corpus is provided along with challenging\nexamples for automatic translation. We also experiment with adding the corpus\nin a machine translation training scenario and show how the resulting system\nbenefits from its use.\n", "rewritten_text": "Recent advancements in machine translation (MT) of written text, driven by readily available parallel corpora and corpus-based training methods, have been significant.  However, automatic translation of spoken language and dialogues remains a considerable challenge, even for state-of-the-art systems. This paper addresses this challenge by introducing a newly constructed Japanese-English parallel corpus of business conversations.  We present a detailed analysis of this corpus, highlighting examples that pose difficulties for automatic translation.  Furthermore, we evaluate the impact of incorporating this corpus into MT training, demonstrating the resulting system's improved performance.\n"}, "2204.08308": {"original_text": "  With the rapid development of multimedia technology, Augmented Reality (AR)\nhas become a promising next-generation mobile platform. The primary theory\nunderlying AR is human visual confusion, which allows users to perceive the\nreal-world scenes and augmented contents (virtual-world scenes) simultaneously\nby superimposing them together. To achieve good Quality of Experience (QoE), it\nis important to understand the interaction between two scenarios, and\nharmoniously display AR contents. However, studies on how this superimposition\nwill influence the human visual attention are lacking. Therefore, in this\npaper, we mainly analyze the interaction effect between background (BG) scenes\nand AR contents, and study the saliency prediction problem in AR. Specifically,\nwe first construct a Saliency in AR Dataset (SARD), which contains 450 BG\nimages, 450 AR images, as well as 1350 superimposed images generated by\nsuperimposing BG and AR images in pair with three mixing levels. A large-scale\neye-tracking experiment among 60 subjects is conducted to collect eye movement\ndata. To better predict the saliency in AR, we propose a vector quantized\nsaliency prediction method and generalize it for AR saliency prediction. For\ncomparison, three benchmark methods are proposed and evaluated together with\nour proposed method on our SARD. Experimental results demonstrate the\nsuperiority of our proposed method on both of the common saliency prediction\nproblem and the AR saliency prediction problem over benchmark methods. Our\ndataset and code are available at: https://github.com/DuanHuiyu/ARSaliency.\n", "rewritten_text": "Augmented reality (AR), fueled by rapid multimedia technology advancements, has emerged as a promising next-generation mobile platform.  AR's core principle leverages human visual perception, enabling the simultaneous perception of real-world scenes and superimposed augmented content (virtual scenes).  Achieving a high quality of experience (QoE) requires understanding the interaction between these scenarios and ensuring harmonious display of AR content. However, research on how this superposition affects human visual attention remains limited.  This paper addresses this gap by analyzing the interaction between background (BG) scenes and AR content, focusing on saliency prediction in AR.\n\nSpecifically, we introduce the Saliency in AR Dataset (SARD), comprising 450 BG images, 450 AR images, and 1350 superimposed images generated by pairing BG and AR images at three mixing levels.  Eye-tracking data from 60 participants in a large-scale experiment were collected to inform our analysis.  To improve AR saliency prediction, we propose a vector-quantized saliency prediction method and adapt it for AR applications.  Three benchmark methods are evaluated alongside our proposed method on SARD.  Experimental results demonstrate the superiority of our method for both general saliency prediction and AR saliency prediction compared to the benchmarks.  Our dataset and code are publicly available at: https://github.com/DuanHuiyu/ARSaliency.\n"}, "2210.16865": {"original_text": "  Explicit decomposition modeling, which involves breaking down complex tasks\ninto more straightforward and often more interpretable sub-tasks, has long been\na central theme in developing robust and interpretable NLU systems. However,\ndespite the many datasets and resources built as part of this effort, the\nmajority have small-scale annotations and limited scope, which is insufficient\nto solve general decomposition tasks. In this paper, we look at large-scale\nintermediate pre-training of decomposition-based transformers using distant\nsupervision from comparable texts, particularly large-scale parallel news. We\nshow that with such intermediate pre-training, developing robust\ndecomposition-based models for a diverse range of tasks becomes more feasible.\nFor example, on semantic parsing, our model, DecompT5, improves 20% to 30% on\ntwo datasets, Overnight and TORQUE, over the baseline language model. We\nfurther use DecompT5 to build a novel decomposition-based QA system named\nDecompEntail, improving over state-of-the-art models, including GPT-3, on both\nHotpotQA and StrategyQA by 8% and 4%, respectively.\n", "rewritten_text": "Explicit decomposition modeling, which breaks down complex tasks into simpler, more interpretable sub-tasks, has been central to developing robust and interpretable natural language understanding (NLU) systems.  However, existing datasets and resources suffer from small-scale annotations and limited scope, hindering the development of general-purpose decomposition models.  This paper introduces a large-scale intermediate pre-training approach for decomposition-based transformers using distant supervision from comparable texts, specifically large-scale parallel news corpora.  We demonstrate that this intermediate pre-training significantly improves the feasibility of developing robust decomposition-based models for diverse tasks.  For instance, our model, DecompT5, achieves a 20-30% improvement over baseline language models on two semantic parsing datasets, Overnight and TORQUE.  Furthermore, we leverage DecompT5 to create a novel decomposition-based question answering (QA) system, DecompEntail, which surpasses state-of-the-art models, including GPT-3, on HotpotQA and StrategyQA by 8% and 4%, respectively.\n"}, "2103.16364": {"original_text": "  Unsupervised person re-identification (ReID) aims at learning discriminative\nidentity features without annotations. Recently, self-supervised contrastive\nlearning has gained increasing attention for its effectiveness in unsupervised\nrepresentation learning. The main idea of instance contrastive learning is to\nmatch a same instance in different augmented views. However, the relationship\nbetween different instances has not been fully explored in previous contrastive\nmethods, especially for instance-level contrastive loss. To address this issue,\nwe propose Inter-instance Contrastive Encoding (ICE) that leverages\ninter-instance pairwise similarity scores to boost previous class-level\ncontrastive ReID methods. We first use pairwise similarity ranking as one-hot\nhard pseudo labels for hard instance contrast, which aims at reducing\nintra-class variance. Then, we use similarity scores as soft pseudo labels to\nenhance the consistency between augmented and original views, which makes our\nmodel more robust to augmentation perturbations. Experiments on several\nlarge-scale person ReID datasets validate the effectiveness of our proposed\nunsupervised method ICE, which is competitive with even supervised methods.\nCode is made available at https://github.com/chenhao2345/ICE.\n", "rewritten_text": "Unsupervised person re-identification (ReID) aims to learn discriminative identity features without labeled data.  Recently, self-supervised contrastive learning has gained prominence for its effectiveness in unsupervised representation learning.  While instance contrastive learning effectively matches augmented views of the same instance, the relationships between different instances remain largely unexplored, particularly within instance-level contrastive loss functions.  To address this, we propose Inter-instance Contrastive Encoding (ICE), which leverages inter-instance pairwise similarity scores to improve existing class-level contrastive ReID methods.  Specifically, we employ pairwise similarity ranking to generate one-hot hard pseudo-labels for hard instance contrast, thereby reducing intra-class variance.  Furthermore, we utilize similarity scores as soft pseudo-labels to enhance consistency between augmented and original views, improving robustness to augmentation variations.  Experiments on several large-scale person ReID datasets demonstrate the effectiveness of our unsupervised ICE method, achieving performance competitive with even supervised approaches.  Code is available at https://github.com/chenhao2345/ICE.\n"}, "1607.07604": {"original_text": "  The interpretation and analysis of the wireless capsule endoscopy recording\nis a complex task which requires sophisticated computer aided decision (CAD)\nsystems in order to help physicians with the video screening and, finally, with\nthe diagnosis. Most of the CAD systems in the capsule endoscopy share a common\nsystem design, but use very different image and video representations. As a\nresult, each time a new clinical application of WCE appears, new CAD system has\nto be designed from scratch. This characteristic makes the design of new CAD\nsystems a very time consuming. Therefore, in this paper we introduce a system\nfor small intestine motility characterization, based on Deep Convolutional\nNeural Networks, which avoids the laborious step of designing specific features\nfor individual motility events. Experimental results show the superiority of\nthe learned features over alternative classifiers constructed by using state of\nthe art hand-crafted features. In particular, it reaches a mean classification\naccuracy of 96% for six intestinal motility events, outperforming the other\nclassifiers by a large margin (a 14% relative performance increase).\n", "rewritten_text": "Analysis of wireless capsule endoscopy (WCE) recordings is a complex task, necessitating sophisticated computer-aided diagnosis (CAD) systems to assist physicians with video screening and diagnosis.  While most WCE CAD systems share a common design, they utilize vastly different image and video representations.  Consequently, each new WCE clinical application requires a CAD system to be developed from scratch, a time-consuming process.  This paper introduces a novel system for small intestinal motility characterization based on deep convolutional neural networks (DCNNs).  This system avoids the laborious task of designing features for individual motility events.  Experimental results demonstrate the superiority of learned features over those of state-of-the-art hand-crafted features.  Specifically, the system achieves a mean classification accuracy of 96% for six intestinal motility events, surpassing other classifiers by a significant margin (a 14% relative performance increase).\n"}, "2206.12262": {"original_text": "  Microblogs have become a social platform for people to express their emotions\nin real-time, and it is a trend to analyze user emotional tendencies from the\ninformation on Microblogs. The dynamic features of emojis can affect the\nsentiment polarity of microblog texts. Since existing models seldom consider\nthe diversity of emoji sentiment polarity,the paper propose a microblog\nsentiment classification model based on ALBERT-FAET. We obtain text embedding\nvia ALBERT pretraining model and learn the inter-emoji embedding with an\nattention-based LSTM network. In addition, a fine-grained attention mechanism\nis proposed to capture the word-level interactions between plain text and\nemoji. Finally, we concatenate these features and feed them into a CNN\nclassifier to predict the sentiment labels of the microblogs. To verify the\neffectiveness of the model and the fine-grained attention network, we conduct\ncomparison experiments and ablation experiments. The comparison experiments\nshow that the model outperforms previous methods in three evaluation indicators\n(accuracy, precision, and recall) and the model can significantly improve\nsentiment classification. The ablation experiments show that compared with\nALBERT-AET, the proposed model ALBERT-FAET is better in the metrics, indicating\nthat the fine-grained attention network can understand the diversified\ninformation of emoticons.\n", "rewritten_text": "Microblogs have become a popular platform for real-time emotional expression, leading to a growing trend in analyzing user sentiment from microblog data.  The dynamic nature of emojis significantly impacts the sentiment polarity of microblog text.  Because existing models rarely account for the diverse sentiment polarities of emojis, this paper proposes a microblog sentiment classification model, ALBERT-FAET.  This model utilizes a pre-trained ALBERT model for text embedding and an attention-based LSTM network to learn inter-emoji embeddings.  Furthermore, a novel fine-grained attention mechanism captures word-level interactions between text and emojis.  These features are then concatenated and fed into a CNN classifier to predict sentiment labels.  Comparative and ablation experiments validate the model's effectiveness and the contribution of the fine-grained attention network.  Results demonstrate that ALBERT-FAET outperforms previous methods across three evaluation metrics (accuracy, precision, and recall), significantly improving sentiment classification.  Ablation experiments, comparing ALBERT-FAET to ALBERT-AET, further confirm the superior performance of the proposed model, highlighting the fine-grained attention network's ability to effectively interpret the nuanced information conveyed by emojis.\n"}, "1910.07481": {"original_text": "  In Machine Translation, considering the document as a whole can help to\nresolve ambiguities and inconsistencies. In this paper, we propose a simple yet\npromising approach to add contextual information in Neural Machine Translation.\nWe present a method to add source context that capture the whole document with\naccurate boundaries, taking every word into account. We provide this additional\ninformation to a Transformer model and study the impact of our method on three\nlanguage pairs. The proposed approach obtains promising results in the\nEnglish-German, English-French and French-English document-level translation\ntasks. We observe interesting cross-sentential behaviors where the model learns\nto use document-level information to improve translation coherence.\n", "rewritten_text": "This paper proposes a novel approach to enhance neural machine translation (NMT) by incorporating document-level context.  Leveraging the global context of a document helps resolve ambiguities and inconsistencies inherent in machine translation.  Our method accurately captures the entire source document, including precise word boundaries, and provides this contextual information to a Transformer model.  We evaluate the impact of this approach on three language pairs\u2014English-German, English-French, and French-English\u2014achieving promising results on document-level translation tasks.  Furthermore, we observe interesting cross-sentential behavior, indicating that the model effectively utilizes document-level information to improve translation coherence.\n"}, "2007.02424": {"original_text": "  Recent advances in unsupervised domain adaptation for semantic segmentation\nhave shown great potentials to relieve the demand of expensive per-pixel\nannotations. However, most existing works address the domain discrepancy by\naligning the data distributions of two domains at a global image level whereas\nthe local consistencies are largely neglected. This paper presents an\ninnovative local contextual-relation consistent domain adaptation (CrCDA)\ntechnique that aims to achieve local-level consistencies during the\nglobal-level alignment. The idea is to take a closer look at region-wise\nfeature representations and align them for local-level consistencies.\nSpecifically, CrCDA learns and enforces the prototypical local\ncontextual-relations explicitly in the feature space of a labelled source\ndomain while transferring them to an unlabelled target domain via\nbackpropagation-based adversarial learning. An adaptive entropy max-min\nadversarial learning scheme is designed to optimally align these hundreds of\nlocal contextual-relations across domain without requiring discriminator or\nextra computation overhead. The proposed CrCDA has been evaluated extensively\nover two challenging domain adaptive segmentation tasks (e.g., GTA5 to\nCityscapes and SYNTHIA to Cityscapes), and experiments demonstrate its superior\nsegmentation performance as compared with state-of-the-art methods.\n", "rewritten_text": "Recent advances in unsupervised domain adaptation for semantic segmentation have shown great potential to reduce the need for expensive per-pixel annotations.  However, most existing methods address domain discrepancy by aligning data distributions at the global image level, largely neglecting local consistencies. This paper introduces a novel local contextual-relation consistent domain adaptation (CrCDA) technique that achieves local-level consistency alongside global-level alignment.  CrCDA focuses on region-wise feature representations, aligning them to ensure local consistency. Specifically, CrCDA explicitly learns and enforces prototypical local contextual relations in the feature space of a labeled source domain, transferring them to an unlabeled target domain via backpropagation-based adversarial learning.  An adaptive entropy max-min adversarial learning scheme optimally aligns these numerous local contextual relations across domains without requiring a discriminator or additional computational overhead.  Extensive evaluation on two challenging domain adaptation segmentation tasks (GTA5 to Cityscapes and SYNTHIA to Cityscapes) demonstrates CrCDA's superior segmentation performance compared to state-of-the-art methods.\n"}, "1805.10047": {"original_text": "  Neural machine translation (NMT) has a drawback in that can generate only\nhigh-frequency words owing to the computational costs of the softmax function\nin the output layer.\n  In Japanese-English NMT, Japanese predicate conjugation causes an increase in\nvocabulary size. For example, one verb can have as many as 19 surface\nvarieties. In this research, we focus on predicate conjugation for compressing\nthe vocabulary size in Japanese. The vocabulary list is filled with the various\nforms of verbs. We propose methods using predicate conjugation information\nwithout discarding linguistic information. The proposed methods can generate\nlow-frequency words and deal with unknown words. Two methods were considered to\nintroduce conjugation information: the first considers it as a token\n(conjugation token) and the second considers it as an embedded vector\n(conjugation feature).\n  The results using these methods demonstrate that the vocabulary size can be\ncompressed by approximately 86.1% (Tanaka corpus) and the NMT models can output\nthe words not in the training data set. Furthermore, BLEU scores improved by\n0.91 points in Japanese-to-English translation, and 0.32 points in\nEnglish-to-Japanese translation with ASPEC.\n", "rewritten_text": "Neural machine translation (NMT) suffers from a limitation: the computational cost of the softmax function in the output layer restricts it to generating primarily high-frequency words.  This problem is exacerbated in Japanese-English NMT by the extensive conjugation of Japanese predicates, which significantly increases vocabulary size\u2014a single verb can have up to 19 surface forms.  This research focuses on leveraging predicate conjugation information to compress the Japanese vocabulary without sacrificing linguistic information.  We propose two methods: representing conjugation as a token (conjugation token method) and as an embedded vector (conjugation feature method).  These methods enable the generation of low-frequency and out-of-vocabulary words.\n\nExperiments using these methods achieved approximately 86.1% vocabulary compression (Tanaka corpus) and enabled the NMT models to generate words unseen during training.  Furthermore, BLEU scores improved by 0.91 points for Japanese-to-English translation and 0.32 points for English-to-Japanese translation (using ASPEC).\n"}, "1703.01028": {"original_text": "  Outlier detection and cluster number estimation is an important issue for\nclustering real data. This paper focuses on spectral clustering, a time-tested\nclustering method, and reveals its important properties related to outliers.\nThe highlights of this paper are the following two mathematical observations:\nfirst, spectral clustering's intrinsic property of an outlier cluster\nformation, and second, the singularity of an outlier cluster with a valid\ncluster number. Based on these observations, we designed a function that\nevaluates clustering and outlier detection results. In experiments, we prepared\ntwo scenarios, face clustering in photo album and person re-identification in a\ncamera network. We confirmed that the proposed method detects outliers and\nestimates the number of clusters properly in both problems. Our method\noutperforms state-of-the-art methods in both the 128-dimensional sparse space\nfor face clustering and the 4,096-dimensional non-sparse space for person\nre-identification.\n", "rewritten_text": "Outlier detection and cluster number estimation are crucial for clustering real-world data. This paper investigates spectral clustering, a well-established clustering method, and reveals key properties related to outlier handling.  Our contributions are twofold: first, we demonstrate spectral clustering's inherent tendency to form outlier clusters; second, we show that these outlier clusters exhibit singularity, providing a basis for valid cluster number estimation.  Based on these observations, we developed a novel evaluation function for clustering and outlier detection.  Experiments on two distinct scenarios\u2014face clustering in a photo album and person re-identification in a camera network\u2014demonstrated the proposed method's effectiveness in accurately detecting outliers and estimating the number of clusters.  Our method outperformed state-of-the-art techniques in both a 128-dimensional sparse space (face clustering) and a 4096-dimensional non-sparse space (person re-identification).\n"}, "2402.19122": {"original_text": "  Gait recognition stands as one of the most pivotal remote identification\ntechnologies and progressively expands across research and industry\ncommunities. However, existing gait recognition methods heavily rely on\ntask-specific upstream driven by supervised learning to provide explicit gait\nrepresentations like silhouette sequences, which inevitably introduce expensive\nannotation costs and potential error accumulation. Escaping from this trend,\nthis work explores effective gait representations based on the all-purpose\nknowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a\nsimple yet efficient gait framework, termed BigGait. Specifically, the Gait\nRepresentation Extractor (GRE) within BigGait draws upon design principles from\nestablished gait representations, effectively transforming all-purpose\nknowledge into implicit gait representations without requiring third-party\nsupervision signals. Experiments on CCPG, CAISA-B* and SUSTech1K indicate that\nBigGait significantly outperforms the previous methods in both within-domain\nand cross-domain tasks in most cases, and provides a more practical paradigm\nfor learning the next-generation gait representation. Finally, we delve into\nprospective challenges and promising directions in LVMs-based gait recognition,\naiming to inspire future work in this emerging topic. The source code is\navailable at https://github.com/ShiqiYu/OpenGait.\n", "rewritten_text": "Gait recognition is a crucial remote identification technology, rapidly expanding in research and industry.  However, current methods heavily rely on supervised learning, producing task-specific gait representations (e.g., silhouette sequences) that incur high annotation costs and error accumulation.  This work departs from this trend, exploring effective gait representations derived from the general knowledge of task-agnostic Large Vision Models (LVMs). We propose BigGait, a simple yet efficient gait recognition framework.  BigGait's Gait Representation Extractor (GRE) leverages established gait representation design principles to implicitly extract gait features from LVM knowledge, eliminating the need for external supervision.  Experiments on CCPG, CAISA-B*, and SUSTech1K datasets demonstrate that BigGait significantly outperforms existing methods in most within-domain and cross-domain scenarios, offering a more practical approach to next-generation gait representation learning.  Finally, we discuss future challenges and promising directions for LVM-based gait recognition, encouraging further research in this field. The source code is available at https://github.com/ShiqiYu/OpenGait.\n"}, "1808.03959": {"original_text": "  Deep Learning based stereo matching methods have shown great successes and\nachieved top scores across different benchmarks. However, like most data-driven\nmethods, existing deep stereo matching networks suffer from some well-known\ndrawbacks such as requiring large amount of labeled training data, and that\ntheir performances are fundamentally limited by the generalization ability. In\nthis paper, we propose a novel Recurrent Neural Network (RNN) that takes a\ncontinuous (possibly previously unseen) stereo video as input, and directly\npredicts a depth-map at each frame without a pre-training process, and without\nthe need of ground-truth depth-maps as supervision. Thanks to the recurrent\nnature (provided by two convolutional-LSTM blocks), our network is able to\nmemorize and learn from its past experiences, and modify its inner parameters\n(network weights) to adapt to previously unseen or unfamiliar environments.\nThis suggests a remarkable generalization ability of the net, making it\napplicable in an {\\em open world} setting. Our method works robustly with\nchanges in scene content, image statistics, and lighting and season conditions\n{\\em etc}. By extensive experiments, we demonstrate that the proposed method\nseamlessly adapts between different scenarios. Equally important, in terms of\nthe stereo matching accuracy, it outperforms state-of-the-art deep stereo\napproaches on standard benchmark datasets such as KITTI and Middlebury stereo.\n", "rewritten_text": "Deep learning-based stereo matching methods have achieved state-of-the-art results on various benchmarks.  However, like most data-driven methods, they suffer from limitations such as requiring large amounts of labeled training data and being constrained by their generalization ability.  This paper proposes a novel recurrent neural network (RNN) that processes continuous stereo video input, directly predicting depth maps for each frame without pre-training or ground-truth depth supervision.  Two convolutional LSTM blocks enable the network to learn from past experiences, adapting its parameters to unseen environments. This results in remarkable generalization, making it suitable for open-world applications.  The method robustly handles variations in scene content, image statistics, lighting, and seasonal conditions.  Extensive experiments demonstrate seamless adaptation across diverse scenarios and superior stereo matching accuracy compared to state-of-the-art deep stereo approaches on benchmark datasets such as KITTI and Middlebury.\n"}, "2206.12571": {"original_text": "  This competition focus on Urban-Sense Segmentation based on the vehicle\ncamera view. Class highly unbalanced Urban-Sense images dataset challenge the\nexisting solutions and further studies. Deep Conventional neural network-based\nsemantic segmentation methods such as encoder-decoder architecture and\nmulti-scale and pyramid-based approaches become flexible solutions applicable\nto real-world applications. In this competition, we mainly review the\nliterature and conduct experiments on transformer-driven methods especially\nSegFormer, to achieve an optimal trade-off between performance and efficiency.\nFor example, SegFormer-B0 achieved 74.6% mIoU with the smallest FLOPS, 15.6G,\nand the largest model, SegFormer- B5 archived 80.2% mIoU. According to multiple\nfactors, including individual case failure analysis, individual class\nperformance, training pressure and efficiency estimation, the final candidate\nmodel for the competition is SegFormer- B2 with 50.6 GFLOPS and 78.5% mIoU\nevaluated on the testing set. Checkout our code implementation at\nhttps://vmv.re/cv3315.\n", "rewritten_text": "This competition focuses on urban scene segmentation using a vehicle-mounted camera.  The highly imbalanced Urban-Sense image dataset presents a significant challenge to existing solutions.  While deep convolutional neural networks, such as encoder-decoder and multi-scale/pyramid architectures, offer flexible solutions for real-world applications, this competition investigates transformer-based methods, specifically SegFormer, to optimize the balance between performance and efficiency.  SegFormer-B0 achieved 74.6% mean Intersection over Union (mIoU) with the lowest FLOPS (15.6G), while the largest model, SegFormer-B5, achieved 80.2% mIoU.  Considering factors such as individual case failure analysis, per-class performance, training efficiency, and computational cost, SegFormer-B2 was selected as the final model, achieving 78.5% mIoU on the test set with 50.6 GFLOPS.  Our code implementation is available at https://vmv.re/cv3315.\n"}, "2204.11994": {"original_text": "  Digital pathological analysis is run as the main examination used for cancer\ndiagnosis. Recently, deep learning-driven feature extraction from pathology\nimages is able to detect genetic variations and tumor environment, but few\nstudies focus on differential gene expression in tumor cells. In this paper, we\npropose a self-supervised contrastive learning framework, HistCode, to infer\ndifferential gene expressions from whole slide images (WSIs). We leveraged\ncontrastive learning on large-scale unannotated WSIs to derive slide-level\nhistopathological feature in latent space, and then transfer it to tumor\ndiagnosis and prediction of differentially expressed cancer driver genes. Our\nextensive experiments showed that our method outperformed other\nstate-of-the-art models in tumor diagnosis tasks, and also effectively\npredicted differential gene expressions. Interestingly, we found the higher\nfold-changed genes can be more precisely predicted. To intuitively illustrate\nthe ability to extract informative features from pathological images, we\nspatially visualized the WSIs colored by the attentive scores of image tiles.\nWe found that the tumor and necrosis areas were highly consistent with the\nannotations of experienced pathologists. Moreover, the spatial heatmap\ngenerated by lymphocyte-specific gene expression patterns was also consistent\nwith the manually labeled WSI.\n", "rewritten_text": "Digital pathology is the primary method for cancer diagnosis. While deep learning has enabled the extraction of genetic variation and tumor microenvironment features from pathology images, few studies have focused on inferring differential gene expression in tumor cells directly from these images.  This paper introduces HistCode, a self-supervised contrastive learning framework designed to infer differential gene expression from whole slide images (WSIs).  We leverage contrastive learning on a large-scale dataset of unannotated WSIs to learn slide-level histopathological features in a latent space. These features are then transferred to improve tumor diagnosis and predict differentially expressed cancer driver genes.  Extensive experiments demonstrate that HistCode outperforms state-of-the-art models in tumor diagnosis and accurately predicts differential gene expression, with higher fold-change genes exhibiting greater prediction accuracy.  To visualize the informative features extracted, we spatially mapped attentive scores of image tiles onto WSIs.  The resulting visualizations show strong agreement between areas highlighted as significant (tumor and necrosis) and the annotations of expert pathologists.  Furthermore, spatial heatmaps generated from lymphocyte-specific gene expression patterns also align well with manual WSI annotations.\n"}, "2309.07509": {"original_text": "  Generating realistic talking faces is a complex and widely discussed task\nwith numerous applications. In this paper, we present DiffTalker, a novel model\ndesigned to generate lifelike talking faces through audio and landmark\nco-driving. DiffTalker addresses the challenges associated with directly\napplying diffusion models to audio control, which are traditionally trained on\ntext-image pairs. DiffTalker consists of two agent networks: a\ntransformer-based landmarks completion network for geometric accuracy and a\ndiffusion-based face generation network for texture details. Landmarks play a\npivotal role in establishing a seamless connection between the audio and image\ndomains, facilitating the incorporation of knowledge from pre-trained diffusion\nmodels. This innovative approach efficiently produces articulate-speaking\nfaces. Experimental results showcase DiffTalker's superior performance in\nproducing clear and geometrically accurate talking faces, all without the need\nfor additional alignment between audio and image features.\n", "rewritten_text": "Generating realistic talking faces is a complex and widely studied problem with numerous applications.  This paper introduces DiffTalker, a novel model that generates lifelike talking faces using audio and landmark co-driving.  Unlike traditional diffusion models trained on text-image pairs, DiffTalker directly addresses the challenges of audio-driven face generation.  It comprises two networks: a transformer-based landmark completion network, ensuring geometric accuracy, and a diffusion-based face generation network, responsible for textural detail.  Landmarks are crucial for bridging the audio and image domains, leveraging knowledge from pre-trained diffusion models. This innovative approach efficiently produces highly articulate talking faces.  Experimental results demonstrate DiffTalker's superior performance in generating clear, geometrically accurate talking faces without requiring additional audio-image alignment.\n"}, "2210.09071": {"original_text": "  Monocular Depth Estimation (MDE) aims to predict pixel-wise depth given a\nsingle RGB image. For both, the convolutional as well as the recent\nattention-based models, encoder-decoder-based architectures have been found to\nbe useful due to the simultaneous requirement of global context and pixel-level\nresolution. Typically, a skip connection module is used to fuse the encoder and\ndecoder features, which comprises of feature map concatenation followed by a\nconvolution operation. Inspired by the demonstrated benefits of attention in a\nmultitude of computer vision problems, we propose an attention-based fusion of\nencoder and decoder features. We pose MDE as a pixel query refinement problem,\nwhere coarsest-level encoder features are used to initialize pixel-level\nqueries, which are then refined to higher resolutions by the proposed Skip\nAttention Module (SAM). We formulate the prediction problem as ordinal\nregression over the bin centers that discretize the continuous depth range and\nintroduce a Bin Center Predictor (BCP) module that predicts bins at the\ncoarsest level using pixel queries. Apart from the benefit of image adaptive\ndepth binning, the proposed design helps learn improved depth embedding in\ninitial pixel queries via direct supervision from the ground truth. Extensive\nexperiments on the two canonical datasets, NYUV2 and KITTI, show that our\narchitecture outperforms the state-of-the-art by 5.3% and 3.9%, respectively,\nalong with an improved generalization performance by 9.4% on the SUNRGBD\ndataset. Code is available at https://github.com/ashutosh1807/PixelFormer.git.\n", "rewritten_text": "Monocular depth estimation (MDE) predicts pixel-wise depth from a single RGB image.  Encoder-decoder architectures, including convolutional and recent attention-based models, have proven effective due to their ability to simultaneously capture global context and maintain pixel-level resolution.  These architectures typically employ skip connections\u2014concatenating encoder and decoder feature maps followed by a convolution\u2014to fuse features.  Inspired by the success of attention mechanisms in computer vision, we propose an attention-based feature fusion method.  We frame MDE as a pixel query refinement problem, initializing pixel-level queries with coarsest-level encoder features and refining them to higher resolutions using a novel Skip Attention Module (SAM).  We formulate depth prediction as ordinal regression over discretized depth bins and introduce a Bin Center Predictor (BCP) module that predicts bin centers at the coarsest level using these pixel queries.  This approach offers image-adaptive depth binning and improves depth embedding in initial pixel queries through direct ground truth supervision.  Extensive experiments on NYUV2 and KITTI demonstrate state-of-the-art performance, surpassing existing methods by 5.3% and 3.9%, respectively.  Furthermore, we observe a 9.4% improvement in generalization performance on the SUNRGBD dataset.  Code is available at https://github.com/ashutosh1807/PixelFormer.git.\n"}, "2312.11716": {"original_text": "  Demand for efficient onboard object detection is increasing due to its key\nrole in autonomous navigation. However, deploying object detection models such\nas YOLO on resource constrained edge devices is challenging due to the high\ncomputational requirements of such models. In this paper, an compressed object\ndetection model named Squeezed Edge YOLO is examined. This model is compressed\nand optimized to kilobytes of parameters in order to fit onboard such edge\ndevices. To evaluate Squeezed Edge YOLO, two use cases - human and shape\ndetection - are used to show the model accuracy and performance. Moreover, the\nmodel is deployed onboard a GAP8 processor with 8 RISC-V cores and an NVIDIA\nJetson Nano with 4GB of memory. Experimental results show Squeezed Edge YOLO\nmodel size is optimized by a factor of 8x which leads to 76% improvements in\nenergy efficiency and 3.3x faster throughout.\n", "rewritten_text": "The demand for efficient onboard object detection is increasing due to its crucial role in autonomous navigation.  However, deploying computationally intensive object detection models, such as YOLO, on resource-constrained edge devices presents significant challenges. This paper examines Squeezed Edge YOLO, a compressed and optimized object detection model reduced to kilobytes of parameters to enable deployment on such devices.  Its accuracy and performance are evaluated using two use cases: human and shape detection.  Furthermore, the model was deployed on a GAP8 processor (featuring eight RISC-V cores) and an NVIDIA Jetson Nano (4 GB RAM). Experimental results demonstrate an 8x reduction in model size, resulting in a 76% improvement in energy efficiency and a 3.3x speedup.\n"}, "2409.15939": {"original_text": "  3D shape completion is traditionally solved using supervised training or by\ndistribution learning on complete shape examples. Recently self-supervised\nlearning approaches that do not require any complete 3D shape examples have\ngained more interests. In this paper, we propose a non-adversarial\nself-supervised approach for the shape completion task. Our first finding is\nthat completion problems can be formulated as an involutory function trivially,\nwhich implies a special constraint on the completion function G, such that\nG(G(X)) = X. Our second constraint on self-supervised shape completion relies\non the fact that shape completion becomes easier to solve with correspondences\nand similarly, completion can simplify the correspondences problem. We\nformulate a consistency measure in the canonical space in order to supervise\nthe completion function. We efficiently optimize the completion and\ncorrespondence modules using \"freeze and alternate\" strategy. The overall\napproach performs well for rigid shapes in a category as well as dynamic\nnon-rigid shapes. We ablate our design choices and compare our solution against\nstate-of-the-art methods, showing remarkable accuracy approaching supervised\naccuracy in some cases.\n", "rewritten_text": "Traditional 3D shape completion relies on supervised training or distribution learning from complete shape examples.  Recently, self-supervised learning approaches, which require no complete 3D shape examples, have attracted increasing interest. This paper presents a novel non-adversarial self-supervised approach for shape completion.  Our approach leverages two key observations. First, the completion problem can be formulated as an involutory function, G, such that G(G(X)) = X, imposing a crucial constraint on the completion function. Second, shape completion and correspondence estimation are mutually beneficial; improved correspondences facilitate completion, and vice-versa.  We therefore formulate a consistency measure in canonical space to supervise the completion function.  The completion and correspondence modules are efficiently optimized using a \"freeze and alternate\" strategy.  Our method achieves strong performance on both rigid and non-rigid shapes, demonstrating remarkable accuracy\u2014approaching supervised learning performance in some cases.  We validate our design choices through ablation studies and comparisons with state-of-the-art methods.\n"}, "1807.10552": {"original_text": "  Histology imaging is an essential diagnosis method to finalize the grade and\nstage of cancer of different tissues, especially for breast cancer diagnosis.\nSpecialists often disagree on the final diagnosis on biopsy tissue due to the\ncomplex morphological variety. Although convolutional neural networks (CNN)\nhave advantages in extracting discriminative features in image classification,\ndirectly training a CNN on high resolution histology images is computationally\ninfeasible currently. Besides, inconsistent discriminative features often\ndistribute over the whole histology image, which incurs challenges in\npatch-based CNN classification method. In this paper, we propose a novel\narchitecture for automatic classification of high resolution histology images.\nFirst, an adapted residual network is employed to explore hierarchical features\nwithout attenuation. Second, we develop a robust deep fusion network to utilize\nthe spatial relationship between patches and learn to correct the prediction\nbias generated from inconsistent discriminative feature distribution. The\nproposed method is evaluated using 10-fold cross-validation on 400 high\nresolution breast histology images with balanced labels and reports 95%\naccuracy on 4-class classification and 98.5% accuracy, 99.6% AUC on 2-class\nclassification (carcinoma and non-carcinoma), which substantially outperforms\nprevious methods and close to pathologist performance.\n", "rewritten_text": "Histological imaging is an essential diagnostic method for determining the grade and stage of various cancers, particularly breast cancer.  However, inter-specialist disagreement on diagnoses from biopsy tissue is common due to the complex morphological variability. While convolutional neural networks (CNNs) excel at extracting discriminative features for image classification, training them directly on high-resolution histological images is currently computationally prohibitive.  Furthermore, the inconsistent distribution of these features across the entire image poses challenges for patch-based CNN classification methods.\n\nThis paper presents a novel architecture for the automated classification of high-resolution histological images.  First, an adapted residual network is employed to extract hierarchical features without signal attenuation. Second, a robust deep fusion network leverages the spatial relationships between image patches and corrects prediction biases arising from the inconsistent distribution of discriminative features.\n\nThe proposed method was evaluated using 10-fold cross-validation on a balanced dataset of 400 high-resolution breast histology images.  It achieved 95% accuracy in a four-class classification task and 98.5% accuracy with 99.6% AUC in a two-class classification task (carcinoma versus non-carcinoma).  These results significantly outperform previous methods and approach the performance of expert pathologists.\n"}, "2205.11395": {"original_text": "  Hyperspectral anomalous change detection has been a challenging task for its\nemphasis on the dynamics of small and rare objects against the prevalent\nchanges. In this paper, we have proposed a Multi-Temporal spatial-spectral\nComparison Network for hyperspectral anomalous change detection (MTC-NET). The\nwhole model is a deep siamese network, aiming at learning the prevalent\nspectral difference resulting from the complex imaging conditions from the\nhyperspectral images by contrastive learning. A three-dimensional spatial\nspectral attention module is designed to effectively extract the spatial\nsemantic information and the key spectral differences. Then the gaps between\nthe multi-temporal features are minimized, boosting the alignment of the\nsemantic and spectral features and the suppression of the multi-temporal\nbackground spectral difference. The experiments on the \"Viareggio 2013\"\ndatasets demonstrate the effectiveness of proposed MTC-NET.\n", "rewritten_text": "Hyperspectral anomalous change detection is challenging due to the dominance of prevalent changes over small, rare objects.  This paper proposes a Multi-Temporal Spatial-Spectral Comparison Network (MTC-Net) for this task.  MTC-Net, a deep Siamese network, leverages contrastive learning to identify prevalent spectral differences arising from complex hyperspectral imaging conditions.  A novel three-dimensional spatial-spectral attention module effectively extracts spatial semantic information and key spectral differences.  Subsequently, the network minimizes discrepancies between multi-temporal features, aligning semantic and spectral features while suppressing background spectral differences. Experiments on the Viareggio 2013 dataset demonstrate MTC-Net's effectiveness.\n"}, "2109.01211": {"original_text": "  Reproducibility has become an intensely debated topic in NLP and ML over\nrecent years, but no commonly accepted way of assessing reproducibility, let\nalone quantifying it, has so far emerged. The assumption has been that wider\nscientific reproducibility terminology and definitions are not applicable to\nNLP/ML, with the result that many different terms and definitions have been\nproposed, some diametrically opposed. In this paper, we test this assumption,\nby taking the standard terminology and definitions from metrology and applying\nthem directly to NLP/ML. We find that we are able to straightforwardly derive a\npractical framework for assessing reproducibility which has the desirable\nproperty of yielding a quantified degree of reproducibility that is comparable\nacross different reproduction studies.\n", "rewritten_text": "Reproducibility in natural language processing (NLP) and machine learning (ML) has become a hotly debated topic in recent years, yet a universally accepted method for assessing, let alone quantifying, it remains elusive.  The prevailing assumption has been that standard scientific reproducibility terminology and definitions are inapplicable to NLP/ML, leading to a proliferation of disparate and sometimes contradictory terms and definitions. This paper challenges this assumption by directly applying established metrological terminology and definitions to NLP/ML.  We demonstrate that this approach readily yields a practical framework for assessing reproducibility, providing a quantifiable and comparable measure across different reproduction studies.\n"}, "2101.1065": {"original_text": "  In machine translation field, in both academia and industry, there is a\ngrowing interest in increasingly powerful systems, using corpora of several\nhundred million to several billion examples. These systems represent the\nstate-of-the-art. Here we defend the idea of developing in parallel <<frugal>>\nbilingual translation systems, trained with relatively small corpora. Based on\nthe observation of a standard human professional translator, we estimate that\nthe corpora should be composed at maximum of a monolingual sub-corpus of 75\nmillion examples for the source language, a second monolingual sub-corpus of 6\nmillion examples for the target language, and an aligned bilingual sub-corpus\nof 6 million bi-examples. A less desirable alternative would be an aligned\nbilingual corpus of 47.5 million bi-examples.\n", "rewritten_text": "The machine translation field, in both academia and industry, shows increasing interest in high-capacity systems trained on corpora ranging from hundreds of millions to billions of examples.  These systems represent the state of the art.  However, we advocate for the parallel development of \"frugal\" bilingual translation systems trained on significantly smaller corpora.  Based on observations of professional human translators, we propose a maximum corpus size of 75 million examples for the source language monolingual sub-corpus, 6 million for the target language monolingual sub-corpus, and 6 million aligned bilingual examples.  A less optimal alternative would be a single aligned bilingual corpus of 47.5 million examples.\n"}, "2211.10104": {"original_text": "  Stereo images, containing left and right view images with disparity, are\nutilized in solving low-vision tasks recently, e.g., rain removal and\nsuper-resolution. Stereo image restoration methods usually obtain better\nperformance than monocular methods by learning the disparity between dual views\neither implicitly or explicitly. However, existing stereo rain removal methods\nstill cannot make full use of the complementary information between two views,\nand we find it is because: 1) the rain streaks have more complex distributions\nin directions and densities, which severely damage the complementary\ninformation and pose greater challenges; 2) the disparity estimation is not\naccurate enough due to the imperfect fusion mechanism for the features between\ntwo views. To overcome such limitations, we propose a new \\underline{Stereo}\n\\underline{I}mage \\underline{R}ain \\underline{R}emoval method (StereoIRR) via\nsufficient interaction between two views, which incorporates: 1) a new\nDual-view Mutual Attention (DMA) mechanism which generates mutual attention\nmaps by taking left and right views as key information for each other to\nfacilitate cross-view feature fusion; 2) a long-range and cross-view\ninteraction, which is constructed with basic blocks and dual-view mutual\nattention, can alleviate the adverse effect of rain on complementary\ninformation to help the features of stereo images to get long-range and\ncross-view interaction and fusion. Notably, StereoIRR outperforms other related\nmonocular and stereo image rain removal methods on several datasets. Our codes\nand datasets will be released.\n", "rewritten_text": "Recent advancements in low-vision tasks, such as rain removal and super-resolution, leverage stereo images\u2014containing left and right views with disparity\u2014to achieve superior performance compared to monocular methods.  This improvement stems from the ability to learn the inter-view disparity, either implicitly or explicitly. However, existing stereo rain removal methods underutilize the complementary information between the two views. This limitation arises from two key challenges: 1) the complex and variable distribution of rain streaks in terms of direction and density, which significantly degrades complementary information; and 2) inaccurate disparity estimation due to suboptimal feature fusion mechanisms between the views.\n\nTo address these challenges, we propose StereoIRR (Stereo Image Rain Removal), a novel method that fosters sufficient inter-view interaction.  StereoIRR incorporates: 1) a Dual-view Mutual Attention (DMA) mechanism, which generates mutual attention maps using each view as key information for the other, thereby facilitating effective cross-view feature fusion; and 2) a long-range, cross-view interaction module, constructed using basic blocks and DMA, to mitigate the detrimental effects of rain on complementary information and enable comprehensive long-range, cross-view interaction and fusion.  Our results demonstrate that StereoIRR significantly outperforms existing monocular and stereo image rain removal methods on several benchmark datasets.  The code and datasets will be publicly released.\n"}, "1704.07293": {"original_text": "  The accuracy of object detectors and trackers is most commonly evaluated by\nthe Intersection over Union (IoU) criterion. To date, most approaches are\nrestricted to axis-aligned or oriented boxes and, as a consequence, many\ndatasets are only labeled with boxes. Nevertheless, axis-aligned or oriented\nboxes cannot accurately capture an object's shape. To address this, a number of\ndensely segmented datasets has started to emerge in both the object detection\nand the object tracking communities. However, evaluating the accuracy of object\ndetectors and trackers that are restricted to boxes on densely segmented data\nis not straightforward. To close this gap, we introduce the relative\nIntersection over Union (rIoU) accuracy measure. The measure normalizes the IoU\nwith the optimal box for the segmentation to generate an accuracy measure that\nranges between 0 and 1 and allows a more precise measurement of accuracies.\nFurthermore, it enables an efficient and easy way to understand scenes and the\nstrengths and weaknesses of an object detection or tracking approach. We\ndisplay how the new measure can be efficiently calculated and present an\neasy-to-use evaluation framework. The framework is tested on the DAVIS and the\nVOT2016 segmentations and has been made available to the community.\n", "rewritten_text": "Object detector and tracker accuracy is most commonly evaluated using the Intersection over Union (IoU) criterion.  However, most existing approaches are limited to axis-aligned or oriented bounding boxes, resulting in datasets primarily labeled with these boxes.  These boxes inadequately capture object shapes.  Consequently, densely segmented datasets are increasingly prevalent in both object detection and tracking.  Evaluating box-based detectors and trackers on such data, however, is non-trivial.\n\nTo address this, we introduce the relative Intersection over Union (rIoU) metric.  rIoU normalizes the IoU with respect to the optimally fitting bounding box for the segmentation, yielding an accuracy score between 0 and 1. This provides a more precise accuracy measurement and offers an efficient means of scene understanding and assessing the strengths and weaknesses of object detection and tracking approaches.  We detail an efficient rIoU calculation and present a user-friendly evaluation framework.  This framework has been tested on the DAVIS and VOT2016 segmentation datasets and is publicly available.\n"}, "2306.02819": {"original_text": "  Natural language understanding (NLU) is an essential branch of natural\nlanguage processing, which relies on representations generated by pre-trained\nlanguage models (PLMs). However, PLMs primarily focus on acquiring\nlexico-semantic information, while they may be unable to adequately handle the\nmeaning of constructions. To address this issue, we introduce construction\ngrammar (CxG), which highlights the pairings of form and meaning, to enrich\nlanguage representation. We adopt usage-based construction grammar as the basis\nof our work, which is highly compatible with statistical models such as PLMs.\nThen a HyCxG framework is proposed to enhance language representation through a\nthree-stage solution. First, all constructions are extracted from sentences via\na slot-constraints approach. As constructions can overlap with each other,\nbringing redundancy and imbalance, we formulate the conditional max coverage\nproblem for selecting the discriminative constructions. Finally, we propose a\nrelational hypergraph attention network to acquire representation from\nconstructional information by capturing high-order word interactions among\nconstructions. Extensive experiments demonstrate the superiority of the\nproposed model on a variety of NLU tasks.\n", "rewritten_text": "Natural language understanding (NLU) critically relies on representations generated by pre-trained language models (PLMs).  However, while PLMs excel at acquiring lexico-semantic information, they may inadequately capture the meaning conveyed by grammatical constructions.  To address this limitation, we leverage construction grammar (CxG), which explicitly models the form-meaning pairings of linguistic constructions, to enrich language representation.  Our approach utilizes usage-based construction grammar, highly compatible with statistical models like PLMs.  We propose a novel HyCxG framework that enhances language representation through a three-stage process:  First, a slot-constraints approach extracts constructions from sentences.  Second, to address the redundancy and imbalance caused by overlapping constructions, we formulate a conditional maximum coverage problem to select discriminative constructions.  Finally, a relational hypergraph attention network captures high-order word interactions among these constructions to generate enriched representations.  Extensive experiments demonstrate the superior performance of our proposed model across various NLU tasks.\n"}, "1309.4573": {"original_text": "  This paper is based on an application of smoothing of 3D face images followed\nby feature detection i.e. detecting the nose tip. The present method uses a\nweighted mesh median filtering technique for smoothing. In this present\nsmoothing technique we have built the neighborhood surrounding a particular\npoint in 3D face and replaced that with the weighted value of the surrounding\npoints in 3D face image. After applying the smoothing technique to the 3D face\nimages our experimental results show that we have obtained considerable\nimprovement as compared to the algorithm without smoothing. We have used here\nthe maximum intensity algorithm for detecting the nose-tip and this method\ncorrectly detects the nose-tip in case of any pose i.e. along X, Y, and Z axes.\nThe present technique gave us worked successfully on 535 out of 542 3D face\nimages as compared to the method without smoothing which worked only on 521 3D\nface images out of 542 face images. Thus we have obtained a 98.70% performance\nrate over 96.12% performance rate of the algorithm without smoothing. All the\nexperiments have been performed on the FRAV3D database.\n", "rewritten_text": "This paper presents a method for 3D face image processing that improves nose-tip detection.  The method employs weighted mesh median filtering for smoothing the 3D face images. This filter constructs a neighborhood around each point and replaces it with a weighted average of its surrounding points.  Experimental results on the FRAV3D database demonstrate a significant improvement in nose-tip detection accuracy compared to a method without smoothing.  A maximum intensity algorithm was used for nose-tip detection, successfully identifying the nose-tip regardless of pose (along the X, Y, and Z axes).  The proposed method achieved a 98.70% success rate (535/542 images), compared to 96.12% (521/542 images) for the method without smoothing.\n"}, "2306.16682": {"original_text": "  Egocentric action anticipation aims to predict the future actions the camera\nwearer will perform from the observation of the past. While predictions about\nthe future should be available before the predicted events take place, most\napproaches do not pay attention to the computational time required to make such\npredictions. As a result, current evaluation schemes assume that predictions\nare available right after the input video is observed, i.e., presuming a\nnegligible runtime, which may lead to overly optimistic evaluations. We propose\na streaming egocentric action evaluation scheme which assumes that predictions\nare performed online and made available only after the model has processed the\ncurrent input segment, which depends on its runtime. To evaluate all models\nconsidering the same prediction horizon, we hence propose that slower models\nshould base their predictions on temporal segments sampled ahead of time. Based\non the observation that model runtime can affect performance in the considered\nstreaming evaluation scenario, we further propose a lightweight action\nanticipation model based on feed-forward 3D CNNs which is optimized using\nknowledge distillation techniques with a novel past-to-future distillation\nloss. Experiments on the three popular datasets EPIC-KITCHENS-55,\nEPIC-KITCHENS-100 and EGTEA Gaze+ show that (i) the proposed evaluation scheme\ninduces a different ranking on state-of-the-art methods as compared to classic\nevaluations, (ii) lightweight approaches tend to outmatch more computationally\nexpensive ones, and (iii) the proposed model based on feed-forward 3D CNNs and\nknowledge distillation outperforms current art in the streaming egocentric\naction anticipation scenario.\n", "rewritten_text": "Egocentric action anticipation predicts a camera wearer's future actions based on past observations.  However, most existing approaches neglect the computational cost of prediction, assuming predictions are instantaneously available after observing the input video. This assumption, inherent in current evaluation schemes, leads to overly optimistic performance assessments.  We propose a novel streaming egocentric action evaluation scheme that explicitly accounts for model runtime.  Predictions are considered available only after the model processes the current input segment, thus reflecting online processing. To ensure a fair comparison across models with varying prediction horizons, slower models predict from temporally advanced segments.  Recognizing the impact of runtime on streaming performance, we introduce a lightweight action anticipation model based on feed-forward 3D convolutional neural networks (CNNs), optimized using knowledge distillation with a novel past-to-future distillation loss.  Experiments on EPIC-KITCHENS-55, EPIC-KITCHENS-100, and EGTEA Gaze+ datasets demonstrate that: (i) our evaluation scheme yields a different ranking of state-of-the-art methods compared to traditional evaluations; (ii) lightweight approaches often outperform computationally expensive ones; and (iii) our proposed feed-forward 3D CNN model with knowledge distillation achieves state-of-the-art performance in streaming egocentric action anticipation.\n"}, "1508.04458": {"original_text": "  Three-dimensional x-ray CT image reconstruction in baggage scanning in\nsecurity applications is an important research field. The variety of materials\nto be reconstructed is broader than medical x-ray imaging. Presence of high\nattenuating materials such as metal may cause artifacts if analytical\nreconstruction methods are used. Statistical modeling and the resultant\niterative algorithms are known to reduce these artifacts and present good\nquantitative accuracy in estimates of linear attenuation coefficients. However,\niterative algorithms may require computations in order to achieve\nquantitatively accurate results. For the case of baggage scanning, in order to\nprovide fast accurate inspection throughput, they must be accelerated\ndrastically. There are many approaches proposed in the literature to increase\nspeed of convergence. This paper presents a new method that estimates the\nwavelet coefficients of the images in the discrete wavelet transform domain\ninstead of the image space itself. Initially, surrogate functions are created\naround approximation coefficients only. As the iterations proceed, the wavelet\ntree on which the updates are made is expanded based on a criterion and detail\ncoefficients at each level are updated and the tree is expanded this way. For\nexample, in the smooth regions of the image the detail coefficients are not\nupdated while the coefficients that represent the high-frequency component\naround edges are being updated, thus saving time by focusing computations where\nthey are needed. This approach is implemented on real data from a SureScan (TM)\nx1000 Explosive Detection System and compared to straightforward implementation\nof the unregularized alternating minimization of O'Sullivan and Benac [1].\n", "rewritten_text": "Three-dimensional X-ray computed tomography (CT) image reconstruction for security applications, specifically baggage scanning, is a significant research area.  The diversity of materials encountered in baggage scanning surpasses that of medical X-ray imaging.  The presence of high-attenuating materials, such as metals, can introduce artifacts when using analytical reconstruction methods.  Statistical modeling and associated iterative algorithms are known to mitigate these artifacts and provide accurate estimates of linear attenuation coefficients. However, iterative algorithms can be computationally expensive, requiring significant processing time to achieve quantitative accuracy.  For baggage scanning, where rapid and accurate inspection throughput is crucial, drastic acceleration is necessary.  Numerous approaches to improve convergence speed have been proposed. This paper presents a novel method that estimates wavelet coefficients directly in the discrete wavelet transform (DWT) domain, rather than in the image space.  Initially, surrogate functions are constructed around only the approximation coefficients.  As iterations proceed, the wavelet tree is expanded based on a defined criterion, updating detail coefficients at each level.  This adaptive approach focuses computations on regions requiring refinement; for example, detail coefficients in smooth image regions remain unchanged, while those representing high-frequency components near edges are updated, thereby improving computational efficiency.  This method was implemented using real data from a SureScan\u2122 x1000 Explosive Detection System and compared to a standard implementation of O'Sullivan and Benac's unregularized alternating minimization algorithm [1].\n"}, "2301.00184": {"original_text": "  Most existing text-video retrieval methods focus on cross-modal matching\nbetween the visual content of videos and textual query sentences. However, in\nreal-world scenarios, online videos are often accompanied by relevant text\ninformation such as titles, tags, and even subtitles, which can be utilized to\nmatch textual queries. This insight has motivated us to propose a novel\napproach to text-video retrieval, where we directly generate associated\ncaptions from videos using zero-shot video captioning with knowledge from\nweb-scale pre-trained models (e.g., CLIP and GPT-2). Given the generated\ncaptions, a natural question arises: what benefits do they bring to text-video\nretrieval? To answer this, we introduce Cap4Video, a new framework that\nleverages captions in three ways: i) Input data: video-caption pairs can\naugment the training data. ii) Intermediate feature interaction: we perform\ncross-modal feature interaction between the video and caption to produce\nenhanced video representations. iii) Output score: the Query-Caption matching\nbranch can complement the original Query-Video matching branch for text-video\nretrieval. We conduct comprehensive ablation studies to demonstrate the\neffectiveness of our approach. Without any post-processing, Cap4Video achieves\nstate-of-the-art performance on four standard text-video retrieval benchmarks:\nMSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%). The code is\navailable at https://github.com/whwu95/Cap4Video .\n", "rewritten_text": "Existing text-video retrieval methods primarily focus on cross-modal matching between video content and textual queries.  However, real-world online videos often include valuable textual metadata (titles, tags, subtitles), offering additional opportunities for query matching.  This observation motivated the development of Cap4Video, a novel text-video retrieval framework.  Cap4Video leverages zero-shot video captioning, employing web-scale pre-trained models (e.g., CLIP and GPT-2), to generate associated captions directly from videos.  These generated captions enhance retrieval in three key ways: (i) *Data Augmentation*: video-caption pairs augment the training data; (ii) *Enhanced Feature Interaction*: cross-modal interaction between video and caption features produces richer video representations; and (iii) *Complementary Scoring*: a Query-Caption matching branch complements the existing Query-Video matching branch.  Comprehensive ablation studies demonstrate Cap4Video's effectiveness.  Without post-processing, it achieves state-of-the-art performance on four standard benchmarks: MSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%).  The code is available at https://github.com/whwu95/Cap4Video.\n"}, "1907.08854": {"original_text": "  Document Grounded Conversations is a task to generate dialogue responses when\nchatting about the content of a given document. Obviously, document knowledge\nplays a critical role in Document Grounded Conversations, while existing\ndialogue models do not exploit this kind of knowledge effectively enough. In\nthis paper, we propose a novel Transformer-based architecture for multi-turn\ndocument grounded conversations. In particular, we devise an Incremental\nTransformer to encode multi-turn utterances along with knowledge in related\ndocuments. Motivated by the human cognitive process, we design a two-pass\ndecoder (Deliberation Decoder) to improve context coherence and knowledge\ncorrectness. Our empirical study on a real-world Document Grounded Dataset\nproves that responses generated by our model significantly outperform\ncompetitive baselines on both context coherence and knowledge relevance.\n", "rewritten_text": "Document Grounded Conversation (DGC) involves generating dialogue responses grounded in the content of a given document.  Existing dialogue models underutilize this crucial document knowledge.  This paper proposes a novel Transformer-based architecture for multi-turn DGC.  Specifically, we introduce an Incremental Transformer to encode multi-turn utterances alongside relevant document knowledge.  Inspired by human cognitive processes, we employ a two-pass Deliberation Decoder to enhance context coherence and knowledge accuracy.  Our empirical evaluation on a real-world DGC dataset demonstrates that our model significantly outperforms competitive baselines in both context coherence and knowledge relevance.\n"}, "2406.05023": {"original_text": "  Generative adversarial networks (GANs) are machine learning models that are\nused to estimate the underlying statistical structure of a given dataset and as\na result can be used for a variety of tasks such as image generation or anomaly\ndetection. Despite their initial simplicity, designing an effective loss\nfunction for training GANs remains challenging, and various loss functions have\nbeen proposed aiming to improve the performance and stability of the generative\nmodels. In this study, loss function design for GANs is presented as an\noptimization problem solved using the genetic programming (GP) approach.\nInitial experiments were carried out using small Deep Convolutional GAN (DCGAN)\nmodel and the MNIST dataset, in order to search experimentally for an improved\nloss function. The functions found were evaluated on CIFAR10, with the best\nfunction, named GANetic loss, showing exceptionally better performance and\nstability compared to the losses commonly used for GAN training. To further\nevalute its general applicability on more challenging problems, GANetic loss\nwas applied for two medical applications: image generation and anomaly\ndetection. Experiments were performed with histopathological, gastrointestinal\nor glaucoma images to evaluate the GANetic loss in medical image generation,\nresulting in improved image quality compared to the baseline models. The\nGANetic Loss used for polyp and glaucoma images showed a strong improvement in\nthe detection of anomalies. In summary, the GANetic loss function was evaluated\non multiple datasets and applications where it consistently outperforms\nalternative loss functions. Moreover, GANetic loss leads to stable training and\nreproducible results, a known weak spot of GANs.\n", "rewritten_text": "Generative adversarial networks (GANs) are machine learning models that estimate the underlying statistical structure of a given dataset, enabling applications such as image generation and anomaly detection.  Despite their inherent simplicity, designing effective loss functions for GAN training remains challenging.  This study addresses this challenge by formulating loss function design as an optimization problem solved using genetic programming (GP).\n\nInitial experiments, using a small deep convolutional GAN (DCGAN) and the MNIST dataset, yielded improved loss functions.  The best-performing function, termed GANetic loss, was subsequently evaluated on CIFAR-10, demonstrating significantly improved performance and stability compared to standard GAN loss functions.\n\nTo assess its generalizability, GANetic loss was applied to two medical image applications: generation and anomaly detection.  Experiments using histopathological, gastrointestinal, and glaucoma images showed improved image quality in generation tasks compared to baseline models.  Furthermore, GANetic loss significantly improved anomaly detection in polyp and glaucoma images.\n\nIn summary, GANetic loss consistently outperformed alternative loss functions across multiple datasets and applications.  Crucially, it also addressed the known instability and reproducibility issues often associated with GAN training.\n"}, "2208.01633": {"original_text": "  We present UnrealEgo, i.e., a new large-scale naturalistic dataset for\negocentric 3D human pose estimation. UnrealEgo is based on an advanced concept\nof eyeglasses equipped with two fisheye cameras that can be used in\nunconstrained environments. We design their virtual prototype and attach them\nto 3D human models for stereo view capture. We next generate a large corpus of\nhuman motions. As a consequence, UnrealEgo is the first dataset to provide\nin-the-wild stereo images with the largest variety of motions among existing\negocentric datasets. Furthermore, we propose a new benchmark method with a\nsimple but effective idea of devising a 2D keypoint estimation module for\nstereo inputs to improve 3D human pose estimation. The extensive experiments\nshow that our approach outperforms the previous state-of-the-art methods\nqualitatively and quantitatively. UnrealEgo and our source codes are available\non our project web page.\n", "rewritten_text": "We introduce UnrealEgo, a new large-scale, naturalistic dataset for egocentric 3D human pose estimation.  UnrealEgo leverages a novel design: virtual eyeglasses equipped with two fisheye cameras, enabling data capture in unconstrained environments.  These virtual glasses are attached to 3D human models to generate stereo image pairs depicting a large corpus of diverse human motions.  Consequently, UnrealEgo is the first egocentric dataset to offer in-the-wild stereo images with such a wide variety of motions.  Furthermore, we propose a novel benchmark method employing a simple yet effective 2D keypoint estimation module for stereo inputs, thereby improving 3D human pose estimation accuracy.  Extensive experiments demonstrate that our approach surpasses previous state-of-the-art methods both qualitatively and quantitatively.  UnrealEgo and its source code are available on our project webpage.\n"}, "2011.131": {"original_text": "  To assess the knowledge proficiency of a learner, multiple choice question is\nan efficient and widespread form in standard tests. However, the composition of\nthe multiple choice question, especially the construction of distractors is\nquite challenging. The distractors are required to both incorrect and plausible\nenough to confuse the learners who did not master the knowledge. Currently, the\ndistractors are generated by domain experts which are both expensive and\ntime-consuming. This urges the emergence of automatic distractor generation,\nwhich can benefit various standard tests in a wide range of domains. In this\npaper, we propose a question and answer guided distractor generation (EDGE)\nframework to automate distractor generation. EDGE consists of three major\nmodules: (1) the Reforming Question Module and the Reforming Passage Module\napply gate layers to guarantee the inherent incorrectness of the generated\ndistractors; (2) the Distractor Generator Module applies attention mechanism to\ncontrol the level of plausibility. Experimental results on a large-scale public\ndataset demonstrate that our model significantly outperforms existing models\nand achieves a new state-of-the-art.\n", "rewritten_text": "Multiple-choice questions (MCQs) are an efficient and widely used format in standardized tests for assessing learner knowledge proficiency.  However, composing effective MCQs, particularly crafting plausible distractors, is challenging.  Distractors must be both incorrect and sufficiently plausible to mislead learners lacking mastery of the subject matter.  Currently, distractors are typically generated by domain experts, a process that is both expensive and time-consuming. This necessitates the development of automated distractor generation methods, which can significantly benefit standardized testing across diverse fields.\n\nThis paper proposes a novel question-and-answer guided distractor generation (EDGE) framework to automate this process.  EDGE comprises three key modules: (1) a Reforming Question Module and a Reforming Passage Module, which employ gate layers to ensure the inherent incorrectness of generated distractors; and (2) a Distractor Generator Module, which utilizes an attention mechanism to control the plausibility of the distractors.  Experimental results on a large-scale public dataset demonstrate that our model significantly outperforms existing state-of-the-art methods.\n"}, "2308.14082": {"original_text": "  Reconstructing interacting hands from monocular images is indispensable in\nAR/VR applications. Most existing solutions rely on the accurate localization\nof each skeleton joint. However, these methods tend to be unreliable due to the\nsevere occlusion and confusing similarity among adjacent hand parts. This also\ndefies human perception because humans can quickly imitate an interaction\npattern without localizing all joints. Our key idea is to first construct a\ntwo-hand interaction prior and recast the interaction reconstruction task as\nthe conditional sampling from the prior. To expand more interaction states, a\nlarge-scale multimodal dataset with physical plausibility is proposed. Then a\nVAE is trained to further condense these interaction patterns as latent codes\nin a prior distribution. When looking for image cues that contribute to\ninteraction prior sampling, we propose the interaction adjacency heatmap (IAH).\nCompared with a joint-wise heatmap for localization, IAH assigns denser visible\nfeatures to those invisible joints. Compared with an all-in-one visible\nheatmap, it provides more fine-grained local interaction information in each\ninteraction region. Finally, the correlations between the extracted features\nand corresponding interaction codes are linked by the ViT module. Comprehensive\nevaluations on benchmark datasets have verified the effectiveness of this\nframework. The code and dataset are publicly available at\nhttps://github.com/binghui-z/InterPrior_pytorch\n", "rewritten_text": "Reconstructing interacting hands from monocular images is crucial for augmented and virtual reality (AR/VR) applications.  Existing methods typically rely on accurate individual joint localization, but this proves unreliable due to severe occlusions and the inherent similarity between adjacent hand parts.  This contrasts with human perception, where interaction patterns are readily understood without explicit joint localization.  Our approach leverages a learned prior of two-hand interactions, reformulating hand reconstruction as conditional sampling from this prior.  To enrich the range of interaction states, we introduce a large-scale, physically plausible multimodal dataset.  A variational autoencoder (VAE) then compresses these interaction patterns into latent codes representing the prior distribution.  To guide sampling from the interaction prior, we propose the Interaction Adjacency Heatmap (IAH).  Unlike joint-wise heatmaps, IAH assigns higher weights to visible features, even for occluded joints, and provides more fine-grained local interaction information within each interaction region compared to a single visible heatmap.  Finally, a Vision Transformer (ViT) module links extracted features to their corresponding interaction codes.  Comprehensive evaluations on benchmark datasets demonstrate the effectiveness of our framework. The code and dataset are publicly available at https://github.com/binghui-z/InterPrior_pytorch.\n"}, "1910.10223": {"original_text": "  Deep generative models come with the promise to learn an explainable\nrepresentation for visual objects that allows image sampling, synthesis, and\nselective modification. The main challenge is to learn to properly model the\nindependent latent characteristics of an object, especially its appearance and\npose. We present a novel approach that learns disentangled representations of\nthese characteristics and explains them individually. Training requires only\npairs of images depicting the same object appearance, but no pose annotations.\nWe propose an additional classifier that estimates the minimal amount of\nregularization required to enforce disentanglement. Thus both representations\ntogether can completely explain an image while being independent of each other.\nPrevious methods based on adversarial approaches fail to enforce this\nindependence, while methods based on variational approaches lead to\nuninformative representations. In experiments on diverse object categories, the\napproach successfully recombines pose and appearance to reconstruct and\nretarget novel synthesized images. We achieve significant improvements over\nstate-of-the-art methods which utilize the same level of supervision, and reach\nperformances comparable to those of pose-supervised approaches. However, we can\nhandle the vast body of articulated object classes for which no pose\nmodels/annotations are available.\n", "rewritten_text": "Deep generative models offer the potential to learn explainable representations of visual objects, enabling image sampling, synthesis, and selective modification.  A key challenge lies in accurately modeling the independent latent characteristics of an object, particularly its appearance and pose.  We introduce a novel approach that learns disentangled representations of these characteristics and provides individual explanations.  Training requires only image pairs depicting the same object appearance, eliminating the need for pose annotations.  We propose an auxiliary classifier that estimates the minimal regularization necessary to enforce disentanglement.  Consequently, the combined representations fully explain an image while remaining independent.  Previous methods, whether adversarial or variational, have fallen short: adversarial approaches fail to guarantee independence, while variational approaches yield uninformative representations.  Experiments across diverse object categories demonstrate our approach's ability to recombine pose and appearance to reconstruct and generate novel, retargeted images.  We achieve significant improvements over state-of-the-art methods using comparable supervision levels, reaching performance on par with pose-supervised approaches.  Crucially, our method addresses the large number of articulated object classes lacking pose models or annotations.\n"}, "2405.17765": {"original_text": "  Video quality assessment (VQA) is a challenging problem due to the numerous\nfactors that can affect the perceptual quality of a video, \\eg, content\nattractiveness, distortion type, motion pattern, and level. However, annotating\nthe Mean opinion score (MOS) for videos is expensive and time-consuming, which\nlimits the scale of VQA datasets, and poses a significant obstacle for deep\nlearning-based methods. In this paper, we propose a VQA method named PTM-VQA,\nwhich leverages PreTrained Models to transfer knowledge from models pretrained\non various pre-tasks, enabling benefits for VQA from different aspects.\n  Specifically, we extract features of videos from different pretrained models\nwith frozen weights and integrate them to generate representation. Since these\nmodels possess various fields of knowledge and are often trained with labels\nirrelevant to quality, we propose an Intra-Consistency and Inter-Divisibility\n(ICID) loss to impose constraints on features extracted by multiple pretrained\nmodels. The intra-consistency constraint ensures that features extracted by\ndifferent pretrained models are in the same unified quality-aware latent space,\nwhile the inter-divisibility introduces pseudo clusters based on the annotation\nof samples and tries to separate features of samples from different clusters.\nFurthermore, with a constantly growing number of pretrained models, it is\ncrucial to determine which models to use and how to use them. To address this\nproblem, we propose an efficient scheme to select suitable candidates. Models\nwith better clustering performance on VQA datasets are chosen to be our\ncandidates. Extensive experiments demonstrate the effectiveness of the proposed\nmethod.\n", "rewritten_text": "Video quality assessment (VQA) is a challenging problem due to numerous factors influencing perceptual quality, including content attractiveness, distortion type, motion patterns, and motion level.  The high cost and time commitment associated with annotating Mean Opinion Scores (MOS) for videos limits the scale of VQA datasets, significantly hindering deep learning-based approaches.  This paper proposes PTM-VQA, a novel VQA method leveraging pretrained models (PTMs) to transfer knowledge from various pre-training tasks, thereby benefiting VQA from multiple perspectives.\n\nSpecifically, we extract video features from multiple PTMs with frozen weights and integrate them to create a unified representation.  Because these models possess diverse knowledge and are often trained with quality-irrelevant labels, we introduce an Intra-Consistency and Inter-Divisibility (ICID) loss function.  This loss imposes constraints on the extracted features, ensuring intra-consistency (features from different PTMs reside within a unified quality-aware latent space) and inter-divisibility (pseudo-clusters based on sample annotations separate features from different clusters).\n\nGiven the ever-increasing number of PTMs, selecting and utilizing appropriate models is crucial.  To address this, we propose an efficient selection scheme prioritizing models demonstrating superior clustering performance on VQA datasets.  Extensive experiments validate the effectiveness of our proposed method.\n"}, "2306.00246": {"original_text": "  The monetary value of a given piece of real estate, a parcel, is often\nreadily available from a geographic information system. However, for many\napplications, such as insurance and urban planning, it is useful to have\nestimates of property value at much higher spatial resolutions. We propose a\nmethod to estimate the distribution over property value at the pixel level from\nremote sensing imagery. We evaluate on a real-world dataset of a major urban\narea. Our results show that the proposed approaches are capable of generating\nfine-level estimates of property values, significantly improving upon a diverse\ncollection of baseline approaches.\n", "rewritten_text": "Geographic information systems (GIS) readily provide the monetary value of individual parcels of real estate.  However, many applications, including insurance and urban planning, require property value estimates at significantly higher spatial resolutions.  We propose a method for estimating the distribution of property values at the pixel level using remote sensing imagery.  We evaluate our method on a real-world dataset from a major urban area.  Results demonstrate that our approach generates fine-grained property value estimates, substantially outperforming a range of baseline methods.\n"}, "1606.02147": {"original_text": "  The ability to perform pixel-wise semantic segmentation in real-time is of\nparamount importance in mobile applications. Recent deep neural networks aimed\nat this task have the disadvantage of requiring a large number of floating\npoint operations and have long run-times that hinder their usability. In this\npaper, we propose a novel deep neural network architecture named ENet\n(efficient neural network), created specifically for tasks requiring low\nlatency operation. ENet is up to 18$\\times$ faster, requires 75$\\times$ less\nFLOPs, has 79$\\times$ less parameters, and provides similar or better accuracy\nto existing models. We have tested it on CamVid, Cityscapes and SUN datasets\nand report on comparisons with existing state-of-the-art methods, and the\ntrade-offs between accuracy and processing time of a network. We present\nperformance measurements of the proposed architecture on embedded systems and\nsuggest possible software improvements that could make ENet even faster.\n", "rewritten_text": "Real-time pixel-wise semantic segmentation is crucial for mobile applications.  However, existing deep neural networks designed for this task often suffer from high computational cost and long runtimes, limiting their usability.  This paper introduces ENet (Efficient Neural Network), a novel architecture specifically designed for low-latency operation.  ENet achieves up to an 18\u00d7 speedup, a 75\u00d7 reduction in floating-point operations (FLOPs), and a 79\u00d7 reduction in parameters, while maintaining comparable or superior accuracy to state-of-the-art models.  We evaluate ENet on the CamVid, Cityscapes, and SUN datasets, comparing its performance against existing methods and analyzing the trade-off between accuracy and processing time.  Furthermore, we present performance measurements on embedded systems and suggest potential software optimizations for further speed improvements.\n"}, "2201.11374": {"original_text": "  In this work, we focus on low-resource dependency parsing for multiple\nlanguages. Several strategies are tailored to enhance performance in\nlow-resource scenarios. While these are well-known to the community, it is not\ntrivial to select the best-performing combination of these strategies for a\nlow-resource language that we are interested in, and not much attention has\nbeen given to measuring the efficacy of these strategies. We experiment with 5\nlow-resource strategies for our ensembled approach on 7 Universal Dependency\n(UD) low-resource languages. Our exhaustive experimentation on these languages\nsupports the effective improvements for languages not covered in pretrained\nmodels. We show a successful application of the ensembled system on a truly\nlow-resource language Sanskrit. The code and data are available at:\nhttps://github.com/Jivnesh/SanDP\n", "rewritten_text": "This work focuses on low-resource dependency parsing across multiple languages.  We explore several established strategies for improving performance in low-resource scenarios.  However, optimally combining these strategies for a given low-resource language remains a challenge, and their individual effectiveness has not been thoroughly investigated.  We experimentally evaluate five such strategies within an ensemble approach applied to seven low-resource Universal Dependencies (UD) languages.  Our exhaustive experiments demonstrate significant performance improvements, particularly for languages lacking pre-trained models.  We successfully apply this ensemble system to the truly low-resource language of Sanskrit.  The code and data are available at: https://github.com/Jivnesh/SanDP\n"}, "2407.03841": {"original_text": "  Large Language Models (LLMs) have showcased remarkable capabilities in\nvarious Natural Language Processing tasks. For automatic open-domain dialogue\nevaluation in particular, LLMs have been seamlessly integrated into evaluation\nframeworks, and together with human evaluation, compose the backbone of most\nevaluations. However, existing evaluation benchmarks often rely on outdated\ndatasets and evaluate aspects like Fluency and Relevance, which fail to\nadequately capture the capabilities and limitations of state-of-the-art chatbot\nmodels.\n  This paper critically examines current evaluation benchmarks, highlighting\nthat the use of older response generators and quality aspects fail to\naccurately reflect modern chatbot capabilities. A small annotation experiment\non a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as\nGPT-4 struggle to detect actual deficiencies in dialogues generated by current\nLLM chatbots.\n", "rewritten_text": "Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks.  In automatic open-domain dialogue evaluation, LLMs are now integral components of evaluation frameworks, forming the backbone of most evaluations alongside human assessment. However, existing benchmarks often rely on outdated datasets and focus on metrics like fluency and relevance, which inadequately capture the strengths and weaknesses of state-of-the-art chatbot models.\n\nThis paper critically examines current evaluation benchmarks, arguing that their reliance on older response generators and outdated quality metrics fails to accurately reflect the capabilities of modern chatbots.  A small annotation experiment using a recently generated LLM dataset (SODA) reveals that even advanced LLM evaluators, such as GPT-4, struggle to identify genuine deficiencies in dialogues produced by current LLM chatbots.\n"}, "1502.00377": {"original_text": "  In order to track the moving objects in long range against occlusion,\ninterruption, and background clutter, this paper proposes a unified approach\nfor global trajectory analysis. Instead of the traditional frame-by-frame\ntracking, our method recovers target trajectories based on a short sequence of\nvideo frames, e.g. $15$ frames. We initially calculate a foreground map at each\nframe, as obtained from a state-of-the-art background model. An attribute graph\nis then extracted from the foreground map, where the graph vertices are image\nprimitives represented by the composite features. With this graph\nrepresentation, we pose trajectory analysis as a joint task of spatial graph\npartitioning and temporal graph matching. The task can be formulated by\nmaximizing a posteriori under the Bayesian framework, in which we integrate the\nspatio-temporal contexts and the appearance models. The probabilistic inference\nis achieved by a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given a\nperoid of observed frames, the algorithm simulates a ergodic and aperiodic\nMarkov Chain, and it visits a sequence of solution states in the joint space of\nspatial graph partitioning and temporal graph matching. In the experiments, our\nmethod is tested on several challenging videos from the public datasets of\nvisual surveillance, and it outperforms the state-of-the-art methods.\n", "rewritten_text": "This paper proposes a unified approach for global trajectory analysis to track moving objects over long ranges, despite occlusion, interruption, and background clutter.  Unlike traditional frame-by-frame tracking methods, our approach recovers target trajectories from short video sequences (e.g., 15 frames).  We begin by generating a foreground map for each frame using a state-of-the-art background subtraction model.  An attribute graph is then constructed from this foreground map, with vertices representing image primitives characterized by composite features.  Trajectory analysis is formulated as a joint task of spatial graph partitioning and temporal graph matching, maximizing the a posteriori probability within a Bayesian framework. This framework integrates spatio-temporal context and appearance models.  Probabilistic inference is performed using a data-driven Markov Chain Monte Carlo (MCMC) algorithm.  Given a sequence of observed frames, the algorithm simulates an ergodic and aperiodic Markov chain, iteratively exploring the joint space of spatial graph partitioning and temporal graph matching.  Experiments on challenging videos from publicly available visual surveillance datasets demonstrate that our method outperforms state-of-the-art techniques.\n"}, "1607.02204": {"original_text": "  In this paper we introduce a method to overcome one of the main challenges of\nperson re-identification in multi-camera networks, namely cross-view appearance\nchanges. The proposed solution addresses the extreme variability of person\nappearance in different camera views by exploiting multiple feature\nrepresentations. For each feature, Kernel Canonical Correlation Analysis (KCCA)\nwith different kernels is exploited to learn several projection spaces in which\nthe appearance correlation between samples of the same person observed from\ndifferent cameras is maximized. An iterative logistic regression is finally\nused to select and weigh the contributions of each feature projections and\nperform the matching between the two views. Experimental evaluation shows that\nthe proposed solution obtains comparable performance on VIPeR and PRID 450s\ndatasets and improves on PRID and CUHK01 datasets with respect to the state of\nthe art.\n", "rewritten_text": "This paper presents a novel method to address a key challenge in multi-camera person re-identification: cross-view appearance variations.  Our solution leverages multiple feature representations to mitigate the significant appearance changes observed across different camera views.  For each feature, we employ Kernel Canonical Correlation Analysis (KCCA) with diverse kernels to learn multiple projection spaces.  These spaces maximize the appearance correlation between images of the same individual captured by different cameras.  Finally, iterative logistic regression selects and weights the contributions of each feature projection to perform view matching.  Experimental results demonstrate comparable performance to state-of-the-art methods on the VIPeR and PRID 450s datasets, and improved performance on the PRID and CUHK01 datasets.\n"}, "2208.03207": {"original_text": "  Learning with noisy labels (LNL) aims at designing strategies to improve\nmodel performance and generalization by mitigating the effects of model\noverfitting to noisy labels. The key success of LNL lies in identifying as many\nclean samples as possible from massive noisy data, while rectifying the wrongly\nassigned noisy labels. Recent advances employ the predicted label distributions\nof individual samples to perform noise verification and noisy label correction,\neasily giving rise to confirmation bias. To mitigate this issue, we propose\nNeighborhood Collective Estimation, in which the predictive reliability of a\ncandidate sample is re-estimated by contrasting it against its feature-space\nnearest neighbors. Specifically, our method is divided into two steps: 1)\nNeighborhood Collective Noise Verification to separate all training samples\ninto a clean or noisy subset, 2) Neighborhood Collective Label Correction to\nrelabel noisy samples, and then auxiliary techniques are used to assist further\nmodel optimization. Extensive experiments on four commonly used benchmark\ndatasets, i.e., CIFAR-10, CIFAR-100, Clothing-1M and Webvision-1.0, demonstrate\nthat our proposed method considerably outperforms state-of-the-art methods.\n", "rewritten_text": "Learning with Noisy Labels (LNL) aims to improve model performance and generalization by mitigating overfitting to noisy labels.  The key to successful LNL is identifying clean samples from noisy data and correcting mislabeled samples.  Recent approaches leverage predicted label distributions for noise verification and correction, but this can lead to confirmation bias.  To address this, we propose Neighborhood Collective Estimation (NCE), which re-estimates the predictive reliability of a sample by comparing it to its nearest neighbors in feature space.  NCE comprises two steps: 1) Neighborhood Collective Noise Verification, which separates training samples into clean and noisy subsets; and 2) Neighborhood Collective Label Correction, which relabels noisy samples.  Auxiliary techniques further optimize the model.  Extensive experiments on four benchmark datasets (CIFAR-10, CIFAR-100, Clothing-1M, and WebVision-1.0) demonstrate that NCE significantly outperforms state-of-the-art methods.\n"}, "2305.03112": {"original_text": "  A surge of interest has emerged in weakly supervised semantic segmentation\ndue to its remarkable efficiency in recent years. Existing approaches based on\ntransformers mainly focus on exploring the affinity matrix to boost CAMs with\nglobal relationships. While in this work, we first perform a scrupulous\nexamination towards the impact of successive affinity matrices and discover\nthat they possess an inclination toward sparsification as the network\napproaches convergence, hence disclosing a manifestation of over-smoothing.\nBesides, it has been observed that enhanced attention maps tend to evince a\nsubstantial amount of extraneous background noise in deeper layers. Drawing\nupon this, we posit a daring conjecture that the undisciplined over-smoothing\nphenomenon introduces a noteworthy quantity of semantically irrelevant\nbackground noise, causing performance degradation. To alleviate this issue, we\npropose a novel perspective that highlights the objects of interest by\ninvestigating the regions of the trait, thereby fostering an extensive\ncomprehension of the successive affinity matrix. Consequently, we suggest an\nadaptive re-activation mechanism (AReAM) that alleviates the issue of\nincomplete attention within the object and the unbounded background noise.\nAReAM accomplishes this by supervising high-level attention with shallow\naffinity matrices, yielding promising results. Exhaustive experiments conducted\non the commonly used dataset manifest that segmentation results can be greatly\nimproved through our proposed AReAM, which imposes restrictions on each\naffinity matrix in deep layers to make it attentive to semantic regions.\n", "rewritten_text": "Weakly supervised semantic segmentation has seen a surge in popularity due to its efficiency.  Existing transformer-based approaches primarily leverage affinity matrices to enhance class activation maps (CAMs) by capturing global relationships.  However, we observe that successive affinity matrices exhibit increasing sparsity as the network converges, indicating over-smoothing.  Furthermore, deeper layers show enhanced attention maps with significant background noise.  We hypothesize that this over-smoothing introduces semantically irrelevant background noise, degrading performance.\n\nTo address this, we propose a novel approach that focuses on regions of interest within the successive affinity matrices, leading to a better understanding of their evolution.  Specifically, we introduce an Adaptive Re-activation Mechanism (AReAM) to mitigate incomplete object attention and excessive background noise. AReAM achieves this by supervising high-level attention with shallow affinity matrices.\n\nExtensive experiments on standard datasets demonstrate that AReAM significantly improves segmentation results by constraining deeper-layer affinity matrices to focus on semantically relevant regions.\n"}, "2010.10894": {"original_text": "  This paper aims to enhance the few-shot relation classification especially\nfor sentences that jointly describe multiple relations. Due to the fact that\nsome relations usually keep high co-occurrence in the same context, previous\nfew-shot relation classifiers struggle to distinguish them with few annotated\ninstances. To alleviate the above relation confusion problem, we propose CTEG,\na model equipped with two mechanisms to learn to decouple these easily-confused\nrelations. On the one hand, an Entity-Guided Attention (EGA) mechanism, which\nleverages the syntactic relations and relative positions between each word and\nthe specified entity pair, is introduced to guide the attention to filter out\ninformation causing confusion. On the other hand, a Confusion-Aware Training\n(CAT) method is proposed to explicitly learn to distinguish relations by\nplaying a pushing-away game between classifying a sentence into a true relation\nand its confusing relation. Extensive experiments are conducted on the FewRel\ndataset, and the results show that our proposed model achieves comparable and\neven much better results to strong baselines in terms of accuracy. Furthermore,\nthe ablation test and case study verify the effectiveness of our proposed EGA\nand CAT, especially in addressing the relation confusion problem.\n", "rewritten_text": "This paper addresses the challenge of few-shot relation classification, particularly for sentences expressing multiple, co-occurring relations.  Existing few-shot classifiers struggle to distinguish between such relations due to their frequent co-occurrence within the same context, hindering performance with limited annotated data.  To mitigate this, we introduce CTEG, a model incorporating two novel mechanisms for decoupling easily confused relations.  First, an Entity-Guided Attention (EGA) mechanism leverages syntactic relations and relative word positions to entities, focusing attention on relevant information and filtering out noise. Second, a Confusion-Aware Training (CAT) method explicitly learns to distinguish relations by employing a \"pushing-away\" strategy:  the model is trained to classify a sentence into its true relation while simultaneously discouraging classification into confusing alternatives.  Extensive experiments on the FewRel dataset demonstrate that CTEG achieves comparable or superior accuracy to strong baselines.  Ablation studies and case analyses further validate the effectiveness of EGA and CAT, particularly in resolving relation confusion.\n"}, "1903.05396": {"original_text": "  This paper introduces improved methods for sub-event detection in social\nmedia streams, by applying neural sequence models not only on the level of\nindividual posts, but also directly on the stream level. Current approaches to\nidentify sub-events within a given event, such as a goal during a soccer match,\nessentially do not exploit the sequential nature of social media streams. We\naddress this shortcoming by framing the sub-event detection problem in social\nmedia streams as a sequence labeling task and adopt a neural sequence\narchitecture that explicitly accounts for the chronological order of posts.\nSpecifically, we (i) establish a neural baseline that outperforms a graph-based\nstate-of-the-art method for binary sub-event detection (2.7% micro-F1\nimprovement), as well as (ii) demonstrate superiority of a recurrent neural\nnetwork model on the posts sequence level for labeled sub-events (2.4%\nbin-level F1 improvement over non-sequential models).\n", "rewritten_text": "This paper presents improved methods for sub-event detection in social media streams by applying neural sequence models at both the individual post and stream levels.  Existing approaches to sub-event identification within a given event (e.g., goals in a soccer match) largely neglect the inherent sequential nature of social media data. We address this limitation by framing sub-event detection as a sequence labeling task and employing a neural sequence architecture that explicitly models the chronological order of posts.  Specifically, we (i) establish a neural baseline that surpasses a state-of-the-art graph-based method for binary sub-event detection (achieving a 2.7% improvement in micro-F1 score), and (ii) demonstrate the superiority of a recurrent neural network model operating on post sequences for labeled sub-events (yielding a 2.4% improvement in bin-level F1 score over non-sequential models).\n"}, "2212.03496": {"original_text": "  Script event prediction aims to predict the subsequent event given the\ncontext. This requires the capability to infer the correlations between events.\nRecent works have attempted to improve event correlation reasoning by using\npretrained language models and incorporating external knowledge~(e.g.,\ndiscourse relations). Though promising results have been achieved, some\nchallenges still remain. First, the pretrained language models adopted by\ncurrent works ignore event-level knowledge, resulting in an inability to\ncapture the correlations between events well. Second, modeling correlations\nbetween events with discourse relations is limited because it can only capture\nexplicit correlations between events with discourse markers, and cannot capture\nmany implicit correlations. To this end, we propose a novel generative approach\nfor this task, in which a pretrained language model is fine-tuned with an\nevent-centric pretraining objective and predicts the next event within a\ngenerative paradigm. Specifically, we first introduce a novel event-level blank\ninfilling strategy as the learning objective to inject event-level knowledge\ninto the pretrained language model, and then design a likelihood-based\ncontrastive loss for fine-tuning the generative model. Instead of using an\nadditional prediction layer, we perform prediction by using sequence\nlikelihoods generated by the generative model. Our approach models correlations\nbetween events in a soft way without any external knowledge. The\nlikelihood-based prediction eliminates the need to use additional networks to\nmake predictions and is somewhat interpretable since it scores each word in the\nevent. Experimental results on the multi-choice narrative cloze~(MCNC) task\ndemonstrate that our approach achieves better results than other\nstate-of-the-art baselines. Our code will be available at\nhttps://github.com/zhufq00/mcnc.\n", "rewritten_text": "Script event prediction aims to predict subsequent events given their context, requiring the ability to infer inter-event correlations.  While recent work leveraging pretrained language models (PLMs) and external knowledge (e.g., discourse relations) has shown promise, challenges remain.  First, current methods often neglect event-level knowledge, hindering accurate correlation capture. Second, relying solely on discourse relations limits correlation modeling to explicitly marked events, ignoring numerous implicit relationships.\n\nTo address these limitations, we propose a novel generative approach.  We fine-tune a PLM using an event-centric pretraining objective and a generative paradigm for next-event prediction.  Specifically, we introduce a novel event-level blank infilling strategy to inject event-level knowledge into the PLM and employ a likelihood-based contrastive loss for fine-tuning the generative model.  Prediction is performed directly using the generative model's sequence likelihoods, eliminating the need for an additional prediction layer.  This approach models correlations softly, without external knowledge, and offers some interpretability by scoring each word in the predicted event.\n\nExperiments on the Multi-choice Narrative Cloze (MCNC) task demonstrate state-of-the-art performance.  Our code is available at https://github.com/zhufq00/mcnc.\n"}, "1911.12982": {"original_text": "  Recently, Chinese word segmentation (CWS) methods using neural networks have\nmade impressive progress. Most of them regard the CWS as a sequence labeling\nproblem which construct models based on local features rather than considering\nglobal information of input sequence. In this paper, we cast the CWS as a\nsequence translation problem and propose a novel sequence-to-sequence CWS model\nwith an attention-based encoder-decoder framework. The model captures the\nglobal information from the input and directly outputs the segmented sequence.\nIt can also tackle other NLP tasks with CWS jointly in an end-to-end mode.\nExperiments on Weibo, PKU and MSRA benchmark datasets show that our approach\nhas achieved competitive performances compared with state-of-the-art methods.\nMeanwhile, we successfully applied our proposed model to jointly learning CWS\nand Chinese spelling correction, which demonstrates its applicability of\nmulti-task fusion.\n", "rewritten_text": "Recent neural network approaches to Chinese word segmentation (CWS) have achieved significant progress.  However, most treat CWS as a sequence labeling problem, relying on local features and neglecting global input sequence information.  This paper proposes a novel sequence-to-sequence CWS model using an attention-based encoder-decoder framework.  This approach treats CWS as a sequence translation problem, capturing global information and directly outputting the segmented sequence.  Furthermore, it enables end-to-end joint learning with other natural language processing (NLP) tasks.  Experiments on the Weibo, PKU, and MSRA benchmark datasets demonstrate competitive performance against state-of-the-art methods.  We also successfully applied our model to the joint learning of CWS and Chinese spelling correction, showcasing its effectiveness in multi-task learning.\n"}, "2308.14018": {"original_text": "  Few-shot font generation is challenging, as it needs to capture the\nfine-grained stroke styles from a limited set of reference glyphs, and then\ntransfer to other characters, which are expected to have similar styles.\nHowever, due to the diversity and complexity of Chinese font styles, the\nsynthesized glyphs of existing methods usually exhibit visible artifacts, such\nas missing details and distorted strokes. In this paper, we propose a\nVQGAN-based framework (i.e., VQ-Font) to enhance glyph fidelity through token\nprior refinement and structure-aware enhancement. Specifically, we pre-train a\nVQGAN to encapsulate font token prior within a codebook. Subsequently, VQ-Font\nrefines the synthesized glyphs with the codebook to eliminate the domain gap\nbetween synthesized and real-world strokes. Furthermore, our VQ-Font leverages\nthe inherent design of Chinese characters, where structure components such as\nradicals and character components are combined in specific arrangements, to\nrecalibrate fine-grained styles based on references. This process improves the\nmatching and fusion of styles at the structure level. Both modules collaborate\nto enhance the fidelity of the generated fonts. Experiments on a collected font\ndataset show that our VQ-Font outperforms the competing methods both\nquantitatively and qualitatively, especially in generating challenging styles.\n", "rewritten_text": "Few-shot font generation is challenging because it requires capturing fine-grained stroke styles from limited reference glyphs and transferring them to other characters with similar styles.  However, the diversity and complexity of Chinese font styles often lead to artifacts\u2014such as missing details and distorted strokes\u2014in glyphs synthesized by existing methods.  This paper introduces VQ-Font, a VQGAN-based framework that enhances glyph fidelity through token prior refinement and structure-aware enhancement.  Specifically, we pre-train a VQGAN to encode font token priors into a codebook.  VQ-Font then refines synthesized glyphs using this codebook to bridge the domain gap between synthesized and real-world strokes.  Furthermore, leveraging the inherent structural components of Chinese characters (radicals and character components), VQ-Font recalibrates fine-grained styles based on reference glyphs, improving style matching and fusion at the structural level.  These two modules collaboratively enhance generated font fidelity.  Experiments on a collected font dataset demonstrate that VQ-Font quantitatively and qualitatively outperforms competing methods, particularly in generating challenging styles.\n"}, "2403.19836": {"original_text": "  Identifying the targets of hate speech is a crucial step in grasping the\nnature of such speech and, ultimately, in improving the detection of offensive\nposts on online forums. Much harmful content on online platforms uses implicit\nlanguage especially when targeting vulnerable and protected groups such as\nusing stereotypical characteristics instead of explicit target names, making it\nharder to detect and mitigate the language. In this study, we focus on\nidentifying implied targets of hate speech, essential for recognizing subtler\nhate speech and enhancing the detection of harmful content on digital\nplatforms. We define a new task aimed at identifying the targets even when they\nare not explicitly stated. To address that task, we collect and annotate target\nspans in three prominent implicit hate speech datasets: SBIC, DynaHate, and\nIHC. We call the resulting merged collection Implicit-Target-Span. The\ncollection is achieved using an innovative pooling method with matching scores\nbased on human annotations and Large Language Models (LLMs). Our experiments\nindicate that Implicit-Target-Span provides a challenging test bed for target\nspan detection methods.\n", "rewritten_text": "Identifying the targets of hate speech is crucial for understanding its nature and improving the detection of offensive online content.  Much harmful online content uses implicit language, particularly when targeting vulnerable and protected groups.  This often involves employing stereotypical characteristics instead of explicit names, hindering detection and mitigation efforts.  This study focuses on identifying the implied targets of hate speech, a critical step in recognizing subtle forms of hate speech and enhancing the detection of harmful content on digital platforms.  We introduce a new task: identifying targets even when not explicitly stated.  To address this, we created the Implicit-Target-Span dataset by merging and annotating target spans from three prominent implicit hate speech datasets: SBIC, DynaHate, and IHC.  This merged collection was created using an innovative pooling method that incorporates matching scores from human annotations and Large Language Models (LLMs). Our experiments demonstrate that Implicit-Target-Span provides a challenging benchmark for evaluating target span detection methods.\n"}, "2303.05955": {"original_text": "  Remote photoplethysmography (rPPG) technology has drawn increasing attention\nin recent years. It can extract Blood Volume Pulse (BVP) from facial videos,\nmaking many applications like health monitoring and emotional analysis more\naccessible. However, as the BVP signal is easily affected by environmental\nchanges, existing methods struggle to generalize well for unseen domains. In\nthis paper, we systematically address the domain shift problem in the rPPG\nmeasurement task. We show that most domain generalization methods do not work\nwell in this problem, as domain labels are ambiguous in complicated\nenvironmental changes. In light of this, we propose a domain-label-free\napproach called NEuron STructure modeling (NEST). NEST improves the\ngeneralization capacity by maximizing the coverage of feature space during\ntraining, which reduces the chance for under-optimized feature activation\nduring inference. Besides, NEST can also enrich and enhance domain invariant\nfeatures across multi-domain. We create and benchmark a large-scale domain\ngeneralization protocol for the rPPG measurement task. Extensive experiments\nshow that our approach outperforms the state-of-the-art methods on both\ncross-dataset and intra-dataset settings.\n", "rewritten_text": "Remote photoplethysmography (rPPG) has gained significant attention in recent years due to its ability to extract Blood Volume Pulse (BVP) signals from facial videos, enabling applications such as health monitoring and emotion analysis.  However, the susceptibility of BVP signals to environmental changes hinders the generalization performance of existing methods across different domains.  This paper addresses this domain shift problem in rPPG measurement by demonstrating the limitations of existing domain generalization methods, which often struggle due to the ambiguity of domain labels in complex environmental conditions.  To overcome this, we propose a novel domain-label-free approach, NEuron STructure modeling (NEST).  NEST enhances generalization by maximizing feature space coverage during training, thereby mitigating under-optimized feature activation during inference.  Furthermore, NEST enriches and enhances domain-invariant features across multiple domains.  We introduce a large-scale domain generalization benchmark protocol for rPPG measurement.  Extensive experiments demonstrate that our approach surpasses state-of-the-art methods in both cross-dataset and intra-dataset evaluations.\n"}, "2401.06957": {"original_text": "  As virtual environments continue to advance, the demand for immersive and\nemotionally engaging experiences has grown. Addressing this demand, we\nintroduce Emotion enabled Virtual avatar mapping using Optimized KnowledgE\ndistillation (EVOKE), a lightweight emotion recognition framework designed for\nthe seamless integration of emotion recognition into 3D avatars within virtual\nenvironments. Our approach leverages knowledge distillation involving\nmulti-label classification on the publicly available DEAP dataset, which covers\nvalence, arousal, and dominance as primary emotional classes. Remarkably, our\ndistilled model, a CNN with only two convolutional layers and 18 times fewer\nparameters than the teacher model, achieves competitive results, boasting an\naccuracy of 87% while demanding far less computational resources. This\nequilibrium between performance and deployability positions our framework as an\nideal choice for virtual environment systems. Furthermore, the multi-label\nclassification outcomes are utilized to map emotions onto custom-designed 3D\navatars.\n", "rewritten_text": "The increasing demand for immersive and emotionally engaging virtual environments has driven the development of Emotion-enabled Virtual Avatar Mapping using Optimized Knowledge Distillation (EVOKE).  EVOKE is a lightweight emotion recognition framework designed for seamless integration into 3D avatars within virtual environments.  Our approach employs knowledge distillation with multi-label classification on the publicly available DEAP dataset, encompassing valence, arousal, and dominance.  Significantly, our distilled convolutional neural network (CNN), featuring only two convolutional layers and 18 times fewer parameters than its teacher model, achieves 87% accuracy while requiring substantially less computational power. This balance of performance and deployability makes EVOKE ideal for virtual environment applications.  Finally, the multi-label classification results are used to map emotions onto custom 3D avatars.\n"}, "2108.07638": {"original_text": "  Affective Computing is the study of how computers can recognize, interpret\nand simulate human affects. Sentiment Analysis is a common task inNLP related\nto this topic, but it focuses only on emotion valence (positive, negative,\nneutral). An emerging approach in NLP is Emotion Recognition, which relies on\nfined-grained classification. This research describes an approach to create a\nlexical-based weakly supervised corpus for fine-grained emotion in Portuguese.\nWe evaluated our dataset by fine-tuning a transformer-based language model\n(BERT) and validating it on a Gold Standard annotated validation set. Our\nresults (F1-score=.64) suggest lexical-based weak supervision as an appropriate\nstrategy for initial work in low resourced environment.\n", "rewritten_text": "Affective computing studies how computers can recognize, interpret, and simulate human emotions.  While sentiment analysis, a common natural language processing (NLP) task, focuses solely on emotion valence (positive, negative, or neutral), emotion recognition represents an emerging, more fine-grained approach. This research presents a method for creating a lexically-based, weakly supervised corpus for fine-grained emotion classification in Portuguese.  We evaluated this dataset by fine-tuning a transformer-based language model (BERT) and validating it against a gold-standard annotated validation set.  Our results (F1-score = 0.64) suggest that lexically-based weak supervision is a viable strategy for initial research in low-resource environments.\n"}, "2408.01653": {"original_text": "  We introduce Multi-Cylindrical Panoramic Depth Estimation (MCPDepth), a\ntwo-stage framework for omnidirectional depth estimation via stereo matching\nbetween multiple cylindrical panoramas. MCPDepth uses cylindrical panoramas for\ninitial stereo matching and then fuses the resulting depth maps across views. A\ncircular attention module is employed to overcome the distortion along the\nvertical axis. MCPDepth exclusively utilizes standard network components,\nsimplifying deployment to embedded devices and outperforming previous methods\nthat require custom kernels. We theoretically and experimentally compare\nspherical and cylindrical projections for stereo matching, highlighting the\nadvantages of the cylindrical projection. MCPDepth achieves state-of-the-art\nperformance with an 18.8% reduction in mean absolute error (MAE) for depth on\nthe outdoor synthetic dataset Deep360 and a 19.9% reduction on the indoor\nreal-scene dataset 3D60.\n", "rewritten_text": "This paper introduces Multi-Cylindrical Panoramic Depth Estimation (MCPDepth), a two-stage framework for omnidirectional depth estimation.  MCPDepth performs stereo matching between multiple cylindrical panoramas, initially matching within each panorama and then fusing the resulting depth maps across views.  A circular attention module mitigates vertical distortion.  Employing only standard network components simplifies deployment to embedded devices, surpassing previous methods reliant on custom kernels.  We present a theoretical and experimental comparison of spherical and cylindrical projections for stereo matching, demonstrating the superiority of the cylindrical projection.  MCPDepth achieves state-of-the-art performance, reducing mean absolute error (MAE) for depth by 18.8% on the Deep360 outdoor synthetic dataset and by 19.9% on the 3D60 indoor real-scene dataset.\n"}, "2111.09091": {"original_text": "  Monitoring behaviour in smart homes using sensors can offer insights into\nchanges in the independent ability and long-term health of residents. Passive\nInfrared motion sensors (PIRs) are standard, however may not accurately track\nthe full duration of movement. They also require line-of-sight to detect motion\nwhich can restrict performance and ensures they must be visible to residents.\nChannel State Information (CSI) is a low cost, unintrusive form of radio\nsensing which can monitor movement but also offers opportunities to generate\nrich data. We have developed a novel, self-calibrating motion detection system\nwhich uses CSI data collected and processed on a stock Raspberry Pi 4. This\nsystem exploits the correlation between CSI frames, on which we perform\nvariance analysis using our algorithm to accurately measure the full period of\na resident's movement. We demonstrate the effectiveness of this approach in\nseveral real-world environments. Experiments conducted demonstrate that\nactivity start and end time can be accurately detected for motion examples of\ndifferent intensities at different locations.\n", "rewritten_text": "Monitoring resident behavior in smart homes using sensors provides valuable insights into changes in their independent living abilities and long-term health. While passive infrared (PIR) motion sensors are commonly used, they may inaccurately track the duration of movement and require line-of-sight, limiting their effectiveness and potentially impacting resident acceptance.  Channel State Information (CSI), a low-cost, unobtrusive radio sensing technique, offers a superior alternative by monitoring movement and generating rich data.  We have developed a novel, self-calibrating motion detection system that utilizes CSI data collected and processed on a standard Raspberry Pi 4. This system leverages the correlation between CSI frames, employing a variance analysis algorithm to accurately measure the complete duration of resident movement.  Real-world experiments demonstrate the system's effectiveness in accurately detecting the start and end times of movements with varying intensities and locations.\n"}, "2405.02296": {"original_text": "  Perspective distortion (PD) causes unprecedented changes in shape, size,\norientation, angles, and other spatial relationships of visual concepts in\nimages. Precisely estimating camera intrinsic and extrinsic parameters is a\nchallenging task that prevents synthesizing perspective distortion.\nNon-availability of dedicated training data poses a critical barrier to\ndeveloping robust computer vision methods. Additionally, distortion correction\nmethods make other computer vision tasks a multi-step approach and lack\nperformance. In this work, we propose mitigating perspective distortion (MPD)\nby employing a fine-grained parameter control on a specific family of M\\\"obius\ntransform to model real-world distortion without estimating camera intrinsic\nand extrinsic parameters and without the need for actual distorted data. Also,\nwe present a dedicated perspectively distorted benchmark dataset, ImageNet-PD,\nto benchmark the robustness of deep learning models against this new dataset.\nThe proposed method outperforms existing benchmarks, ImageNet-E and ImageNet-X.\nAdditionally, it significantly improves performance on ImageNet-PD while\nconsistently performing on standard data distribution. Notably, our method\nshows improved performance on three PD-affected real-world applications crowd\ncounting, fisheye image recognition, and person re-identification and one\nPD-affected challenging CV task: object detection. The source code, dataset,\nand models are available on the project webpage at\nhttps://prakashchhipa.github.io/projects/mpd.\n", "rewritten_text": "Perspective distortion (PD) drastically alters the shape, size, orientation, angles, and spatial relationships within images.  Accurately estimating camera intrinsic and extrinsic parameters is challenging, hindering the synthesis of realistic PD.  The lack of dedicated training data further impedes the development of robust computer vision methods, while existing distortion correction techniques often introduce additional steps and compromise performance.\n\nThis work proposes Mitigating Perspective Distortion (MPD) by employing fine-grained parameter control of a specific M\u00f6bius transformation family. This approach models real-world distortion without requiring camera parameter estimation or distorted training data.  We also introduce ImageNet-PD, a new benchmark dataset of perspectively distorted images, to evaluate the robustness of deep learning models.\n\nOur method outperforms existing benchmarks, including ImageNet-E and ImageNet-X, and significantly improves performance on ImageNet-PD while maintaining consistent performance on standard datasets.  Furthermore, it demonstrates improved performance on three real-world applications affected by PD\u2014crowd counting, fisheye image recognition, and person re-identification\u2014and on the challenging computer vision task of object detection.\n\nThe source code, dataset, and pre-trained models are available at [https://prakashchhipa.github.io/projects/mpd](https://prakashchhipa.github.io/projects/mpd).\n"}, "1705.08479": {"original_text": "  This paper introduces a new architectural framework, known as input\nfast-forwarding, that can enhance the performance of deep networks. The main\nidea is to incorporate a parallel path that sends representations of input\nvalues forward to deeper network layers. This scheme is substantially different\nfrom \"deep supervision\" in which the loss layer is re-introduced to earlier\nlayers. The parallel path provided by fast-forwarding enhances the training\nprocess in two ways. First, it enables the individual layers to combine\nhigher-level information (from the standard processing path) with lower-level\ninformation (from the fast-forward path). Second, this new architecture reduces\nthe problem of vanishing gradients substantially because the fast-forwarding\npath provides a shorter route for gradient backpropagation. In order to\nevaluate the utility of the proposed technique, a Fast-Forward Network (FFNet),\nwith 20 convolutional layers along with parallel fast-forward paths, has been\ncreated and tested. The paper presents empirical results that demonstrate\nimproved learning capacity of FFNet due to fast-forwarding, as compared to\nGoogLeNet (with deep supervision) and CaffeNet, which are 4x and 18x larger in\nsize, respectively. All of the source code and deep learning models described\nin this paper will be made available to the entire research community\n", "rewritten_text": "This paper introduces a novel architectural framework, input fast-forwarding, to enhance deep network performance.  The core concept involves a parallel pathway that directly transmits input representations to deeper layers.  Unlike deep supervision, which adds loss layers to earlier layers, fast-forwarding provides a parallel path. This enhances training in two ways: first, by allowing individual layers to integrate higher-level information (from the standard processing path) with lower-level information (from the fast-forward path); and second, by mitigating the vanishing gradient problem through a shorter backpropagation route.\n\nTo evaluate this technique, we developed a Fast-Forward Network (FFNet), a 20-layer convolutional network incorporating parallel fast-forward paths.  Empirical results demonstrate FFNet's improved learning capacity compared to GoogLeNet (with deep supervision) and CaffeNet, which are four and eighteen times larger, respectively.  All source code and deep learning models are publicly available to the research community.\n"}, "2002.06515": {"original_text": "  Automatic analysis of highly crowded people has attracted extensive attention\nfrom computer vision research. Previous approaches for crowd counting have\nalready achieved promising performance across various benchmarks. However, to\ndeal with the real situation, we hope the model run as fast as possible while\nkeeping accuracy. In this paper, we propose a compact convolutional neural\nnetwork for crowd counting which learns a more efficient model with a small\nnumber of parameters. With three parallel filters executing the convolutional\noperation on the input image simultaneously at the front of the network, our\nmodel could achieve nearly real-time speed and save more computing resources.\nExperiments on two benchmarks show that our proposed method not only takes a\nbalance between performance and efficiency which is more suitable for actual\nscenes but also is superior to existing light-weight models in speed.\n", "rewritten_text": "Automated analysis of dense crowds has garnered significant attention in computer vision research.  While existing crowd counting approaches have demonstrated promising performance on various benchmarks, real-world applications demand both high accuracy and speed.  This paper presents a compact convolutional neural network (CNN) for crowd counting, designed for efficiency through a reduced parameter count.  Employing three parallel convolutional filters operating simultaneously on the input image, our model achieves near real-time performance while conserving computational resources.  Experiments on two benchmark datasets demonstrate that our method effectively balances accuracy and efficiency, outperforming existing lightweight models in speed and proving more suitable for real-world deployment.\n"}, "1711.03278": {"original_text": "  Convolution Neural Networks (CNN), known as ConvNets are widely used in many\nvisual imagery application, object classification, speech recognition. After\nthe implementation and demonstration of the deep convolution neural network in\nImagenet classification in 2012 by krizhevsky, the architecture of deep\nConvolution Neural Network is attracted many researchers. This has led to the\nmajor development in Deep learning frameworks such as Tensorflow, caffe, keras,\ntheno. Though the implementation of deep learning is quite possible by\nemploying deep learning frameworks, mathematical theory and concepts are harder\nto understand for new learners and practitioners. This article is intended to\nprovide an overview of ConvNets architecture and to explain the mathematical\ntheory behind it including activation function, loss function, feedforward and\nbackward propagation. In this article, grey scale image is taken as input\ninformation image, ReLU and Sigmoid activation function are considered for\ndeveloping the architecture and cross-entropy loss function are used for\ncomputing the difference between predicted value and actual value. The\narchitecture is developed in such a way that it can contain one convolution\nlayer, one pooling layer, and multiple dense layers\n", "rewritten_text": "Convolutional Neural Networks (CNNs), also known as ConvNets, are widely used in various visual imagery applications, including object classification and speech recognition.  Following the 2012 ImageNet classification demonstration by Krizhevsky et al. using a deep CNN, the architecture of deep convolutional neural networks attracted significant research interest. This spurred major developments in deep learning frameworks such as TensorFlow, Caffe, Keras, and Theano.  While deep learning frameworks simplify implementation, the underlying mathematical theory and concepts can be challenging for newcomers.  This article provides an overview of ConvNet architecture and its mathematical foundations, including activation functions, loss functions, and feedforward and backpropagation.  We use a grayscale image as input, ReLU and sigmoid activation functions, and a cross-entropy loss function to measure the difference between predicted and actual values. The architecture comprises one convolutional layer, one pooling layer, and multiple dense layers.\n"}, "1610.07995": {"original_text": "  In this paper, the task of recognizing signs made by hearing impaired people\nat sentence level has been addressed. A novel method of extracting spatial\nfeatures to capture hand movements of a signer has been proposed. Frames of a\ngiven video of a sign are preprocessed to extract face and hand components of a\nsigner. The local centroids of the extracted components along with the global\ncentroid are exploited to extract spatial features. The concept of interval\nvalued type symbolic data has been explored to capture variations in the same\nsign made by the different signers at different instances of time. A suitable\nsymbolic similarity measure is studied to establish matching between test and\nreference signs and a simple nearest neighbour classifier is used to recognize\nan unknown sign as one among the known signs by specifying a desired level of\nthreshold. An extensive experimentation is conducted on a considerably large\ndatabase of signs created by us during the course of research work in order to\nevaluate the performance of the proposed system\n", "rewritten_text": "This paper addresses sentence-level recognition of signs made by hearing-impaired individuals.  We propose a novel method for extracting spatial features to capture hand movements.  Signer videos are preprocessed to isolate face and hand components.  Spatial features are then derived from the local and global centroids of these components.  Interval-valued symbolic data are used to account for variations in sign execution across different signers and time instances.  A suitable symbolic similarity measure is employed to match test and reference signs, and a nearest-neighbor classifier, with a tunable threshold, identifies unknown signs.  The system's performance was evaluated through extensive experimentation on a large, purpose-built sign language database.\n"}, "1908.04501": {"original_text": "  When a deep neural network is trained on data with only image-level labeling,\nthe regions activated in each image tend to identify only a small region of the\ntarget object. We propose a method of using videos automatically harvested from\nthe web to identify a larger region of the target object by using temporal\ninformation, which is not present in the static image. The temporal variations\nin a video allow different regions of the target object to be activated. We\nobtain an activated region in each frame of a video, and then aggregate the\nregions from successive frames into a single image, using a warping technique\nbased on optical flow. The resulting localization maps cover more of the target\nobject, and can then be used as proxy ground-truth to train a segmentation\nnetwork. This simple approach outperforms existing methods under the same level\nof supervision, and even approaches relying on extra annotations. Based on\nVGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4,\nrespectively, on PASCAL VOC 2012 test images, which represents a new\nstate-of-the-art.\n", "rewritten_text": "Deep neural networks trained on image-level labeled data tend to activate only small regions of the target object.  We propose a novel method leveraging automatically harvested web videos to identify larger target object regions using temporal information absent in static images.  Temporal variations in videos activate different target object regions across frames.  We extract an activated region from each frame and aggregate these regions into a single image using optical flow-based warping.  The resulting localization maps, encompassing a greater portion of the target object, serve as proxy ground truth for training a segmentation network.  This simple approach surpasses existing methods with comparable supervision, even outperforming those using additional annotations.  Using VGG-16 and ResNet-101 backbones, our method achieves mean Intersection over Union (mIoU) scores of 65.0% and 67.4%, respectively, on the PASCAL VOC 2012 test set, establishing a new state-of-the-art.\n"}, "2203.01532": {"original_text": "  Recently, contrastive learning-based image translation methods have been\nproposed, which contrasts different spatial locations to enhance the spatial\ncorrespondence. However, the methods often ignore the diverse semantic relation\nwithin the images. To address this, here we propose a novel semantic relation\nconsistency (SRC) regularization along with the decoupled contrastive learning,\nwhich utilize the diverse semantics by focusing on the heterogeneous semantics\nbetween the image patches of a single image. To further improve the\nperformance, we present a hard negative mining by exploiting the semantic\nrelation. We verified our method for three tasks: single-modal and multi-modal\nimage translations, and GAN compression task for image translation.\nExperimental results confirmed the state-of-art performance of our method in\nall the three tasks.\n", "rewritten_text": "Recent contrastive learning methods for image translation contrast spatial locations to improve spatial correspondence.  However, these methods often neglect the diverse semantic relationships within images.  To address this, we propose a novel Semantic Relation Consistency (SRC) regularization coupled with decoupled contrastive learning.  SRC leverages diverse semantics by focusing on the heterogeneous relationships between image patches within a single image.  Furthermore, we introduce hard negative mining based on semantic relationships to further enhance performance.  We evaluated our method on three tasks: single-modal and multi-modal image translation, and GAN compression for image translation.  Experimental results demonstrate state-of-the-art performance across all three tasks.\n"}, "2310.03414": {"original_text": "  Multi-document summarization is a challenging task due to its inherent\nsubjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4\namong DUC-2004 reference summaries. In this work, we aim to enhance the\nobjectivity of news summarization by focusing on the main event of a group of\nrelated news documents and presenting it coherently with sufficient context.\nOur primary objective is to succinctly report the main event, ensuring that the\nsummary remains objective and informative. To achieve this, we employ an\nextract-rewrite approach that incorporates a main-event biased\nmonotone-submodular function for content selection. This enables us to extract\nthe most crucial information related to the main event from the document\ncluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for\nrewriting the extracted content into a coherent text. The evaluation using\nobjective metrics and human evaluators confirms the effectiveness of our\napproach, as it surpasses potential baselines, demonstrating excellence in both\ncontent coverage, coherence, and informativeness.\n", "rewritten_text": "Multi-document summarization is a challenging task, as evidenced by the low inter-annotator agreement (ROUGE-1 score of 0.4) on the DUC-2004 reference summaries.  This work addresses the inherent subjectivity of news summarization by focusing on the main event within a cluster of related documents and presenting it coherently with sufficient context.  Our objective is to create succinct, objective, and informative summaries by employing an extract-rewrite approach.  This approach leverages a main-event-biased monotone-submodular function for content selection, enabling the extraction of crucial information related to the main event.  A fine-tuned large language model (LLM) then rewrites the extracted content to ensure coherence.  Evaluation using objective metrics and human assessment demonstrates that our approach outperforms baselines, achieving superior content coverage, coherence, and informativeness.\n"}, "1608.04314": {"original_text": "  We present a technique for weakly supervised object localization (WSOL),\nbuilding on the observation that WSOL algorithms usually work better on images\nwith bigger objects. Instead of training the object detector on the entire\ntraining set at the same time, we propose a curriculum learning strategy to\nfeed training images into the WSOL learning loop in an order from images\ncontaining bigger objects down to smaller ones. To automatically determine the\norder, we train a regressor to estimate the size of the object given the whole\nimage as input. Furthermore, we use these size estimates to further improve the\nre-localization step of WSOL by assigning weights to object proposals according\nto how close their size matches the estimated object size. We demonstrate the\neffectiveness of using size order and size weighting on the challenging PASCAL\nVOC 2007 dataset, where we achieve a significant improvement over existing\nstate-of-the-art WSOL techniques.\n", "rewritten_text": "This paper presents a novel weakly supervised object localization (WSOL) technique leveraging the observation that WSOL algorithms perform better on images containing larger objects.  Instead of training on the entire dataset simultaneously, we propose a curriculum learning approach that orders training images by object size, progressing from largest to smallest.  This ordering is determined automatically by a regressor trained to estimate object size from the entire image.  Furthermore, these size estimates are used to refine the WSOL re-localization step by weighting object proposals based on their size similarity to the estimated object size.  Experiments on the challenging PASCAL VOC 2007 dataset demonstrate significant improvements over existing state-of-the-art WSOL methods, validating the effectiveness of our size-ordered training and size-weighted re-localization strategies.\n"}, "2402.00160": {"original_text": "  In this work, we introduce the Multiple Embedding Model for EHR (MEME), an\napproach that serializes multimodal EHR tabular data into text using\npseudo-notes, mimicking clinical text generation. This conversion not only\npreserves better representations of categorical data and learns contexts but\nalso enables the effective employment of pretrained foundation models for rich\nfeature representation. To address potential issues with context length, our\nframework encodes embeddings for each EHR modality separately. We demonstrate\nthe effectiveness of MEME by applying it to several decision support tasks\nwithin the Emergency Department across multiple hospital systems. Our findings\nindicate that MEME outperforms traditional machine learning, EHR-specific\nfoundation models, and general LLMs, highlighting its potential as a general\nand extendible EHR representation strategy.\n", "rewritten_text": "This work introduces the Multiple Embedding Model for Electronic Health Records (MEME), a novel approach that serializes multimodal EHR tabular data into textual pseudo-notes, mimicking clinical text generation.  This conversion improves the representation of categorical data, captures contextual information, and facilitates the effective use of pretrained foundation models for richer feature extraction. To mitigate context length limitations, our framework encodes embeddings for each EHR modality independently.  We demonstrate MEME's effectiveness on several emergency department decision support tasks across multiple hospital systems.  Our results show that MEME outperforms traditional machine learning methods, EHR-specific foundation models, and general large language models (LLMs), suggesting its potential as a generalizable and extensible EHR representation strategy.\n"}, "2305.11853": {"original_text": "  Large language models (LLMs) with in-context learning have demonstrated\nremarkable capability in the text-to-SQL task. Previous research has prompted\nLLMs with various demonstration-retrieval strategies and intermediate reasoning\nsteps to enhance the performance of LLMs. However, those works often employ\nvaried strategies when constructing the prompt text for text-to-SQL inputs,\nsuch as databases and demonstration examples. This leads to a lack of\ncomparability in both the prompt constructions and their primary contributions.\nFurthermore, selecting an effective prompt construction has emerged as a\npersistent problem for future research. To address this limitation, we\ncomprehensively investigate the impact of prompt constructions across various\nsettings and provide insights into prompt constructions for future text-to-SQL\nstudies.\n", "rewritten_text": "Large language models (LLMs) with in-context learning have demonstrated remarkable capabilities in text-to-SQL tasks.  Previous research has explored various demonstration-retrieval strategies and intermediate reasoning steps to improve LLM performance. However, inconsistencies in prompt construction for text-to-SQL inputs\u2014including databases and demonstration examples\u2014hinder comparability across studies and obscure the impact of individual contributions.  Effective prompt engineering remains a significant challenge.  To address this, we comprehensively investigate the effects of different prompt constructions across various settings, offering valuable insights for future text-to-SQL research.\n"}, "1007.0621": {"original_text": "  In this paper fusion of visual and thermal images in wavelet transformed\ndomain has been presented. Here, Daubechies wavelet transform, called as D2,\ncoefficients from visual and corresponding coefficients computed in the same\nmanner from thermal images are combined to get fused coefficients. After\ndecomposition up to fifth level (Level 5) fusion of coefficients is done.\nInverse Daubechies wavelet transform of those coefficients gives us fused face\nimages. The main advantage of using wavelet transform is that it is well-suited\nto manage different image resolution and allows the image decomposition in\ndifferent kinds of coefficients, while preserving the image information. Fused\nimages thus found are passed through Principal Component Analysis (PCA) for\nreduction of dimensions and then those reduced fused images are classified\nusing a multi-layer perceptron. For experiments IRIS Thermal/Visual Face\nDatabase was used. Experimental results show that the performance of the\napproach presented here achieves maximum success rate of 100% in many cases.\n", "rewritten_text": "This paper presents a method for fusing visual and thermal images in the wavelet domain.  Visual and thermal images are decomposed to level 5 using the Daubechies wavelet transform (Db2).  Corresponding wavelet coefficients from both images are then fused.  An inverse Db2 transform yields a fused face image.  The wavelet transform is advantageous because it handles varying image resolutions and decomposes images into different coefficient types while preserving image information.  Principal Component Analysis (PCA) reduces the dimensionality of the fused images, which are subsequently classified using a multilayer perceptron.  Experiments using the IRIS Thermal/Visual Face Database achieved a 100% success rate in many cases.\n"}, "2210.11725": {"original_text": "  We present AROS, a one-shot learning approach that uses an explicit\nrepresentation of interactions between highly-articulated human poses and 3D\nscenes. The approach is one-shot as the method does not require re-training to\nadd new affordance instances. Furthermore, only one or a small handful of\nexamples of the target pose are needed to describe the interaction. Given a 3D\nmesh of a previously unseen scene, we can predict affordance locations that\nsupport the interactions and generate corresponding articulated 3D human bodies\naround them. We evaluate on three public datasets of scans of real environments\nwith varied degrees of noise. Via rigorous statistical analysis of crowdsourced\nevaluations, results show that our one-shot approach outperforms data-intensive\nbaselines by up to 80\\%.\n", "rewritten_text": "AROS is a novel one-shot learning approach for predicting human-object interaction affordances.  It explicitly represents the interaction between highly articulated human poses and 3D scenes.  This one-shot method requires no retraining to incorporate new affordance instances; only one or a few examples of the target pose are needed to describe the interaction. Given a 3D mesh of an unseen scene, AROS predicts affordance locations and generates corresponding articulated 3D human body poses.  Evaluated on three publicly available datasets of real-world scans with varying noise levels, and using rigorous statistical analysis of crowdsourced evaluations, AROS outperforms data-intensive baselines by up to 80%.\n"}, "1604.06646": {"original_text": "  In this paper we introduce a new method for text detection in natural images.\nThe method comprises two contributions: First, a fast and scalable engine to\ngenerate synthetic images of text in clutter. This engine overlays synthetic\ntext to existing background images in a natural way, accounting for the local\n3D scene geometry. Second, we use the synthetic images to train a\nFully-Convolutional Regression Network (FCRN) which efficiently performs text\ndetection and bounding-box regression at all locations and multiple scales in\nan image. We discuss the relation of FCRN to the recently-introduced YOLO\ndetector, as well as other end-to-end object detection systems based on deep\nlearning. The resulting detection network significantly out performs current\nmethods for text detection in natural images, achieving an F-measure of 84.2%\non the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per\nsecond on a GPU.\n", "rewritten_text": "This paper introduces a novel method for text detection in natural images.  Our approach comprises two key contributions:  First, a fast and scalable engine for generating synthetic text images within cluttered backgrounds. This engine realistically overlays synthetic text onto existing images, considering local 3D scene geometry. Second, we train a Fully Convolutional Regression Network (FCRN) using these synthetic images.  The FCRN efficiently performs text detection and bounding-box regression across all locations and scales within an image. We analyze the relationship between the FCRN and the recently introduced YOLO detector, as well as other deep-learning-based end-to-end object detection systems.  The resulting detection network significantly outperforms existing methods for text detection in natural images, achieving an F-measure of 84.2% on the ICDAR 2013 benchmark.  Moreover, it processes 15 images per second on a GPU.\n"}, "2204.06028": {"original_text": "  In this paper, we describe our submission to the Simultaneous Speech\nTranslation at IWSLT 2022. We explore strategies to utilize an offline model in\na simultaneous setting without the need to modify the original model. In our\nexperiments, we show that our onlinization algorithm is almost on par with the\noffline setting while being $3\\times$ faster than offline in terms of latency\non the test set. We also show that the onlinized offline model outperforms the\nbest IWSLT2021 simultaneous system in medium and high latency regimes and is\nalmost on par in the low latency regime. We make our system publicly available.\n", "rewritten_text": "This paper describes our submission to the IWSLT 2022 Simultaneous Speech Translation (SST) shared task.  We present strategies for adapting an offline model to simultaneous translation without requiring model modifications.  Our experiments demonstrate that our onlinization algorithm achieves near-offline performance while exhibiting a three-fold latency reduction on the test set.  Furthermore, the onlinized offline model surpasses the best IWSLT 2021 simultaneous system in medium- and high-latency settings and achieves comparable performance in low-latency settings.  Our system is publicly available.\n"}, "2304.02715": {"original_text": "  This paper presents an unsupervised approach that leverages raw aerial videos\nto learn to estimate planar homographic transformation between consecutive\nvideo frames. Previous learning-based estimators work on pairs of images to\nestimate their planar homographic transformations but suffer from severe\nover-fitting issues, especially when applying over aerial videos. To address\nthis concern, we develop a sequential estimator that directly processes a\nsequence of video frames and estimates their pairwise planar homographic\ntransformations in batches. We also incorporate a set of spatial-temporal\nknowledge to regularize the learning of such a sequence-to-sequence model. We\ncollect a set of challenging aerial videos and compare the proposed method to\nthe alternative algorithms. Empirical studies suggest that our sequential model\nachieves significant improvement over alternative image-based methods and the\nknowledge-rich regularization further boosts our system performance. Our codes\nand dataset could be found at https://github.com/Paul-LiPu/DeepVideoHomography\n", "rewritten_text": "This paper presents an unsupervised approach for estimating planar homographic transformations between consecutive frames in raw aerial video.  Existing learning-based estimators, which operate on image pairs, suffer from significant overfitting, particularly when applied to aerial video.  To mitigate this, we propose a sequential estimator that processes video frames in batches, directly estimating pairwise planar homographic transformations.  Furthermore, we incorporate spatial-temporal constraints to regularize this sequence-to-sequence model.  Our method is evaluated on a challenging dataset of aerial videos, demonstrating significant improvements over alternative image-based methods.  The incorporation of knowledge-rich regularization further enhances performance.  The code and dataset are available at https://github.com/Paul-LiPu/DeepVideoHomography.\n"}, "2306.16925": {"original_text": "  Pretraining with large-scale 3D volumes has a potential for improving the\nsegmentation performance on a target medical image dataset where the training\nimages and annotations are limited. Due to the high cost of acquiring\npixel-level segmentation annotations on the large-scale pretraining dataset,\npretraining with unannotated images is highly desirable. In this work, we\npropose a novel self-supervised learning strategy named Volume Fusion (VF) for\npretraining 3D segmentation models. It fuses several random patches from a\nforeground sub-volume to a background sub-volume based on a predefined set of\ndiscrete fusion coefficients, and forces the model to predict the fusion\ncoefficient of each voxel, which is formulated as a self-supervised\nsegmentation task without manual annotations. Additionally, we propose a novel\nnetwork architecture based on parallel convolution and transformer blocks that\nis suitable to be transferred to different downstream segmentation tasks with\nvarious scales of organs and lesions. The proposed model was pretrained with\n110k unannotated 3D CT volumes, and experiments with different downstream\nsegmentation targets including head and neck organs, thoracic/abdominal organs\nshowed that our pretrained model largely outperformed training from scratch and\nseveral state-of-the-art self-supervised training methods and segmentation\nmodels. The code and pretrained model are available at\nhttps://github.com/openmedlab/MIS-FM.\n", "rewritten_text": "Pretraining 3D segmentation models with large-scale volumes holds significant potential for improving performance on target medical image datasets with limited training data and annotations.  The high cost of acquiring pixel-level annotations for large-scale datasets necessitates self-supervised pretraining.  This work introduces Volume Fusion (VF), a novel self-supervised learning strategy for pretraining 3D segmentation models.  VF fuses random patches from foreground and background sub-volumes using a predefined set of discrete fusion coefficients.  The model then predicts the fusion coefficient for each voxel, creating a self-supervised segmentation task requiring no manual annotations.  Furthermore, we propose a novel network architecture incorporating parallel convolutional and transformer blocks, enabling effective transfer learning to diverse downstream segmentation tasks involving organs and lesions of varying scales.  Pretrained on 110,000 unannotated 3D CT volumes, our model significantly outperformed both training from scratch and several state-of-the-art self-supervised methods and segmentation models on downstream tasks involving head and neck, and thoracic/abdominal organs.  The code and pretrained model are available at https://github.com/openmedlab/MIS-FM.\n"}, "2102.0932": {"original_text": "  Event cameras are novel vision sensors that report per-pixel brightness\nchanges as a stream of asynchronous \"events\". They offer significant advantages\ncompared to standard cameras due to their high temporal resolution, high\ndynamic range and lack of motion blur. However, events only measure the varying\ncomponent of the visual signal, which limits their ability to encode scene\ncontext. By contrast, standard cameras measure absolute intensity frames, which\ncapture a much richer representation of the scene. Both sensors are thus\ncomplementary. However, due to the asynchronous nature of events, combining\nthem with synchronous images remains challenging, especially for learning-based\nmethods. This is because traditional recurrent neural networks (RNNs) are not\ndesigned for asynchronous and irregular data from additional sensors. To\naddress this challenge, we introduce Recurrent Asynchronous Multimodal (RAM)\nnetworks, which generalize traditional RNNs to handle asynchronous and\nirregular data from multiple sensors. Inspired by traditional RNNs, RAM\nnetworks maintain a hidden state that is updated asynchronously and can be\nqueried at any time to generate a prediction. We apply this novel architecture\nto monocular depth estimation with events and frames where we show an\nimprovement over state-of-the-art methods by up to 30% in terms of mean\nabsolute depth error. To enable further research on multimodal learning with\nevents, we release EventScape, a new dataset with events, intensity frames,\nsemantic labels, and depth maps recorded in the CARLA simulator.\n", "rewritten_text": "Event cameras are novel vision sensors that report per-pixel brightness changes as a stream of asynchronous events.  They offer significant advantages over standard cameras due to their high temporal resolution, high dynamic range, and lack of motion blur. However, events only capture the changing component of the visual signal, limiting their ability to encode full scene context.  In contrast, standard cameras measure absolute intensity frames, providing a richer scene representation.  These sensor modalities are thus complementary.  However, the asynchronous nature of event data makes combining it with synchronous images challenging, particularly for learning-based methods. Traditional recurrent neural networks (RNNs) are ill-suited for the asynchronous and irregular data streams from event cameras.\n\nTo address this, we introduce Recurrent Asynchronous Multimodal (RAM) networks, which generalize traditional RNNs to handle asynchronous and irregular data from multiple sensors.  Inspired by RNNs, RAM networks maintain a hidden state updated asynchronously and readily available for prediction at any time.  We apply this architecture to monocular depth estimation using events and frames, achieving up to a 30% improvement over state-of-the-art methods in terms of mean absolute depth error.  To facilitate further research on multimodal learning with events, we release EventScape, a new dataset containing events, intensity frames, semantic labels, and depth maps recorded using the CARLA simulator.\n"}, "2410.05963": {"original_text": "  Existing perception models achieve great success by learning from large\namounts of labeled data, but they still struggle with open-world scenarios. To\nalleviate this issue, researchers introduce open-set perception tasks to detect\nor segment unseen objects in the training set. However, these models require\npredefined object categories as inputs during inference, which are not\navailable in real-world scenarios. Recently, researchers pose a new and more\npractical problem, \\textit{i.e.}, open-ended object detection, which discovers\nunseen objects without any object categories as inputs. In this paper, we\npresent VL-SAM, a training-free framework that combines the generalized object\nrecognition model (\\textit{i.e.,} Vision-Language Model) with the generalized\nobject localization model (\\textit{i.e.,} Segment-Anything Model), to address\nthe open-ended object detection and segmentation task. Without additional\ntraining, we connect these two generalized models with attention maps as the\nprompts. Specifically, we design an attention map generation module by\nemploying head aggregation and a regularized attention flow to aggregate and\npropagate attention maps across all heads and layers in VLM, yielding\nhigh-quality attention maps. Then, we iteratively sample positive and negative\npoints from the attention maps with a prompt generation module and send the\nsampled points to SAM to segment corresponding objects. Experimental results on\nthe long-tail instance segmentation dataset (LVIS) show that our method\nsurpasses the previous open-ended method on the object detection task and can\nprovide additional instance segmentation masks. Besides, VL-SAM achieves\nfavorable performance on the corner case object detection dataset (CODA),\ndemonstrating the effectiveness of VL-SAM in real-world applications. Moreover,\nVL-SAM exhibits good model generalization that can incorporate various VLMs and\nSAMs.\n", "rewritten_text": "Existing perception models, while highly successful with large labeled datasets, struggle in open-world scenarios.  To address this, researchers have introduced open-set perception tasks, focusing on detecting or segmenting unseen objects. However, these methods typically require predefined object categories during inference, a limitation absent in real-world applications.  A more practical approach, open-ended object detection, aims to discover unseen objects without such input categories.  This paper presents VL-SAM, a training-free framework that combines a generalized object recognition model (a Vision-Language Model, or VLM) with a generalized object localization model (a Segment-Anything Model, or SAM) to achieve open-ended object detection and segmentation.  Without retraining, we connect these models using attention maps as prompts.  Specifically, an attention map generation module, employing head aggregation and regularized attention flow, aggregates and propagates attention maps across all VLM heads and layers, producing high-quality maps.  A prompt generation module then iteratively samples positive and negative points from these maps, which are subsequently sent to SAM for object segmentation.  Experiments on the Long-Tail Instance Segmentation dataset (LVIS) demonstrate that VL-SAM outperforms previous open-ended methods in object detection and provides instance segmentation masks.  Furthermore, VL-SAM achieves strong performance on the Corner Case Object Detection dataset (CODA), highlighting its real-world applicability.  Finally, VL-SAM exhibits good model generalization, readily incorporating various VLMs and SAMs.\n"}, "1805.05421": {"original_text": "  Deep learning has made significant improvements at many image processing\ntasks in recent years, such as image classification, object recognition and\nobject detection. Convolutional neural networks (CNN), which is a popular deep\nlearning architecture designed to process data in multiple array form, show\ngreat success to almost all detection \\& recognition problems and computer\nvision tasks. However, the number of parameters in a CNN is too high such that\nthe computers require more energy and larger memory size. In order to solve\nthis problem, we propose a novel energy efficient model Binary Weight and\nHadamard-transformed Image Network (BWHIN), which is a combination of Binary\nWeight Network (BWN) and Hadamard-transformed Image Network (HIN). It is\nobserved that energy efficiency is achieved with a slight sacrifice at\nclassification accuracy. Among all energy efficient networks, our novel\nensemble model outperforms other energy efficient models.\n", "rewritten_text": "Deep learning has significantly advanced many image processing tasks in recent years, including image classification, object recognition, and object detection.  Convolutional neural networks (CNNs), a popular deep learning architecture designed to process multi-dimensional array data, have achieved considerable success in detection, recognition, and broader computer vision tasks. However, the large number of parameters in CNNs necessitates high energy consumption and substantial memory requirements.  To address this, we propose a novel energy-efficient model, the Binary Weight and Hadamard-transformed Image Network (BWHIN), which combines a Binary Weight Network (BWN) and a Hadamard-transformed Image Network (HIN).  While achieving significant energy efficiency, this approach results in a minor reduction in classification accuracy.  Nevertheless, our novel ensemble model outperforms other energy-efficient networks.\n"}, "2001.11561": {"original_text": "  We consider referring image segmentation. It is a problem at the intersection\nof computer vision and natural language understanding. Given an input image and\na referring expression in the form of a natural language sentence, the goal is\nto segment the object of interest in the image referred by the linguistic\nquery. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to\ntackle this problem. Our model consists of an encoder network and a decoder\nnetwork, where ConvLSTM is used in both encoder and decoder networks to capture\nspatial and sequential information. The encoder network extracts visual and\nlinguistic features for each word in the expression sentence, and adopts an\nattention mechanism to focus on words that are more informative in the\nmultimodal interaction. The decoder network integrates the features generated\nby the encoder network at multiple levels as its input and produces the final\nprecise segmentation mask. Experimental results on four challenging datasets\ndemonstrate that the proposed network achieves superior segmentation\nperformance compared with other state-of-the-art methods.\n", "rewritten_text": "Referring image segmentation, a problem bridging computer vision and natural language understanding, requires segmenting an object in an image based on a natural language description.  We propose a dual convolutional LSTM (ConvLSTM) network to address this.  Our model comprises an encoder and a decoder, both employing ConvLSTMs to capture spatial and sequential information. The encoder extracts visual and linguistic features for each word in the descriptive sentence, using an attention mechanism to prioritize informative words in the multimodal context. The decoder integrates multi-level encoder features to generate a precise segmentation mask.  Experiments on four challenging datasets demonstrate that our network achieves state-of-the-art segmentation performance.\n"}, "1903.07224": {"original_text": "  This work develops a novel end-to-end deep unsupervised learning method based\non convolutional neural network (CNN) with pseudo-classes for remote sensing\nscene representation. First, we introduce center points as the centers of the\npseudo classes and the training samples can be allocated with pseudo labels\nbased on the center points. Therefore, the CNN model, which is used to extract\nfeatures from the scenes, can be trained supervised with the pseudo labels.\nMoreover, a pseudo-center loss is developed to decrease the variance between\nthe samples and the corresponding pseudo center point. The pseudo-center loss\nis important since it can update both the center points with the training\nsamples and the CNN model with the center points in the training process\nsimultaneously. Finally, joint learning of the pseudo-center loss and the\npseudo softmax loss which is formulated with the samples and the pseudo labels\nis developed for unsupervised remote sensing scene representation to obtain\ndiscriminative representations from the scenes. Experiments are conducted over\ntwo commonly used remote sensing scene datasets to validate the effectiveness\nof the proposed method and the experimental results show the superiority of the\nproposed method when compared with other state-of-the-art methods.\n", "rewritten_text": "This work presents a novel end-to-end unsupervised deep learning method for remote sensing scene representation.  The method employs a convolutional neural network (CNN) trained with pseudo-classes.  Initially, center points defining these pseudo-classes are established, assigning pseudo-labels to training samples based on proximity to these centers.  The CNN then learns to extract features from the scenes using these pseudo-labels in a supervised manner.  Furthermore, a novel pseudo-center loss function is introduced to minimize the variance between samples and their assigned pseudo-centers. This loss function simultaneously updates both the pseudo-centers based on the training samples and the CNN weights based on the updated centers.  Finally, a joint learning framework combines this pseudo-center loss with a pseudo-softmax loss (formulated using samples and pseudo-labels) to achieve discriminative scene representations.  Experiments on two widely used remote sensing datasets demonstrate the effectiveness of the proposed method, outperforming state-of-the-art techniques.\n"}, "2203.073": {"original_text": "  Current mobile user authentication systems based on PIN codes, fingerprint,\nand face recognition have several shortcomings. Such limitations have been\naddressed in the literature by exploring the feasibility of passive\nauthentication on mobile devices through behavioral biometrics. In this line of\nresearch, this work carries out a comparative analysis of unimodal and\nmultimodal behavioral biometric traits acquired while the subjects perform\ndifferent activities on the phone such as typing, scrolling, drawing a number,\nand tapping on the screen, considering the touchscreen and the simultaneous\nbackground sensor data (accelerometer, gravity sensor, gyroscope, linear\naccelerometer, and magnetometer). Our experiments are performed over HuMIdb,\none of the largest and most comprehensive freely available mobile user\ninteraction databases to date. A separate Recurrent Neural Network (RNN) with\ntriplet loss is implemented for each single modality. Then, the weighted fusion\nof the different modalities is carried out at score level. In our experiments,\nthe most discriminative background sensor is the magnetometer, whereas among\ntouch tasks the best results are achieved with keystroke in a fixed-text\nscenario. In all cases, the fusion of modalities is very beneficial, leading to\nEqual Error Rates (EER) ranging from 4% to 9% depending on the modality\ncombination in a 3-second interval.\n", "rewritten_text": "Current mobile authentication systems, relying on PINs, fingerprints, and facial recognition, suffer from several limitations.  Existing literature addresses these limitations by exploring passive authentication via behavioral biometrics on mobile devices.  This work presents a comparative analysis of unimodal and multimodal behavioral biometric traits collected during various user activities\u2014typing, scrolling, number drawing, and screen tapping\u2014using touchscreen data and concurrent background sensor data (accelerometer, gravity sensor, gyroscope, linear accelerometer, and magnetometer).  Our experiments utilize HuMIdb, one of the largest publicly available mobile user interaction databases.  We trained a separate recurrent neural network (RNN) with triplet loss for each modality, subsequently performing score-level fusion of the modalities using weighted averaging.  The magnetometer proved the most discriminative background sensor, while keystroke dynamics in a fixed-text scenario yielded the best results among touch-based tasks.  Modality fusion consistently improved performance, achieving equal error rates (EERs) ranging from 4% to 9% across different combinations within a 3-second window.\n"}, "2305.14722": {"original_text": "  Most contemporary supervised Remote Sensing (RS) image Change Detection (CD)\napproaches are customized for equal-resolution bitemporal images. Real-world\napplications raise the need for cross-resolution change detection, aka, CD\nbased on bitemporal images with different spatial resolutions. Given training\nsamples of a fixed bitemporal resolution difference (ratio) between the\nhigh-resolution (HR) image and the low-resolution (LR) one, current\ncross-resolution methods may fit a certain ratio but lack adaptation to other\nresolution differences. Toward continuous cross-resolution CD, we propose\nscale-invariant learning to enforce the model consistently predicting HR\nresults given synthesized samples of varying resolution differences.\nConcretely, we synthesize blurred versions of the HR image by random\ndownsampled reconstructions to reduce the gap between HR and LR images. We\nintroduce coordinate-based representations to decode per-pixel predictions by\nfeeding the coordinate query and corresponding multi-level embedding features\ninto an MLP that implicitly learns the shape of land cover changes, therefore\nbenefiting recognizing blurred objects in the LR image. Moreover, considering\nthat spatial resolution mainly affects the local textures, we apply\nlocal-window self-attention to align bitemporal features during the early\nstages of the encoder. Extensive experiments on two synthesized and one\nreal-world different-resolution CD datasets verify the effectiveness of the\nproposed method. Our method significantly outperforms several vanilla CD\nmethods and two cross-resolution CD methods on the three datasets both in\nin-distribution and out-of-distribution settings. The empirical results suggest\nthat our method could yield relatively consistent HR change predictions\nregardless of varying bitemporal resolution ratios. Our code is available at\n\\url{https://github.com/justchenhao/SILI_CD}.\n", "rewritten_text": "Most contemporary supervised remote sensing (RS) image change detection (CD) approaches are designed for bitemporal images with equal spatial resolution.  However, real-world applications necessitate cross-resolution change detection\u2014CD using bitemporal images with differing resolutions.  Current cross-resolution methods, trained on bitemporal image pairs with a fixed resolution ratio (between high-resolution (HR) and low-resolution (LR) images), perform well only for that specific ratio and lack adaptability to others.  To address this limitation and enable continuous cross-resolution CD, we propose scale-invariant learning (SIL).  SIL enforces consistent high-resolution prediction by training on synthetically generated samples with varying resolution differences.  Specifically, we generate blurred, downsampled versions of the HR image to bridge the gap between HR and LR images.  We introduce coordinate-based representations, feeding coordinate queries and corresponding multi-level embedding features into a multi-layer perceptron (MLP). This implicitly learns the shape of land cover changes, improving the recognition of blurred objects in the LR image.  Furthermore, recognizing that spatial resolution primarily affects local textures, we employ local-window self-attention to align bitemporal features in the early encoder stages.  Extensive experiments on two synthetic and one real-world cross-resolution CD datasets demonstrate the effectiveness of our method.  It significantly outperforms several standard CD methods and two existing cross-resolution CD methods in both in-distribution and out-of-distribution settings.  These empirical results indicate that our method produces consistently accurate HR change predictions across varying bitemporal resolution ratios.  Our code is available at [https://github.com/justchenhao/SILI_CD](https://github.com/justchenhao/SILI_CD).\n"}, "1704.08763": {"original_text": "  We present GazeDirector, a new approach for eye gaze redirection that uses\nmodel-fitting. Our method first tracks the eyes by fitting a multi-part eye\nregion model to video frames using analysis-by-synthesis, thereby recovering\neye region shape, texture, pose, and gaze simultaneously. It then redirects\ngaze by 1) warping the eyelids from the original image using a model-derived\nflow field, and 2) rendering and compositing synthesized 3D eyeballs onto the\noutput image in a photorealistic manner. GazeDirector allows us to change where\npeople are looking without person-specific training data, and with full\narticulation, i.e. we can precisely specify new gaze directions in 3D.\nQuantitatively, we evaluate both model-fitting and gaze synthesis, with\nexperiments for gaze estimation and redirection on the Columbia gaze dataset.\nQualitatively, we compare GazeDirector against recent work on gaze redirection,\nshowing better results especially for large redirection angles. Finally, we\ndemonstrate gaze redirection on YouTube videos by introducing new 3D gaze\ntargets and by manipulating visual behavior.\n", "rewritten_text": "GazeDirector is a novel approach to eye-gaze redirection employing model fitting.  Our method first tracks the eyes by fitting a multi-part eye region model to video frames using analysis-by-synthesis, simultaneously recovering eye region shape, texture, pose, and gaze. Gaze redirection is then achieved by: 1) warping the eyelids from the original image using a model-derived flow field, and 2) photorealistically rendering and compositing synthesized 3D eyeballs onto the output image.  GazeDirector enables gaze alteration without person-specific training data, offering full 3D articulation\u2014precise specification of new gaze directions.  We quantitatively evaluate both model fitting and gaze synthesis through experiments on gaze estimation and redirection using the Columbia Gaze dataset.  Qualitative comparisons with recent gaze redirection methods demonstrate superior performance, particularly for large redirection angles.  Finally, we showcase GazeDirector's capabilities by redirecting gaze in YouTube videos, introducing novel 3D gaze targets and manipulating visual behavior.\n"}, "2110.13385": {"original_text": "  Recently, Transformer-based networks have shown great promise on\nskeleton-based action recognition tasks. The ability to capture global and\nlocal dependencies is the key to success while it also brings quadratic\ncomputation and memory cost. Another problem is that previous studies mainly\nfocus on the relationships among individual joints, which often suffers from\nthe noisy skeleton joints introduced by the noisy inputs of sensors or\ninaccurate estimations. To address the above issues, we propose a novel\nTransformer-based network (IIP-Transformer). Instead of exploiting interactions\namong individual joints, our IIP-Transformer incorporates body joints and parts\ninteractions simultaneously and thus can capture both joint-level (intra-part)\nand part-level (inter-part) dependencies efficiently and effectively. From the\ndata aspect, we introduce a part-level skeleton data encoding that\nsignificantly reduces the computational complexity and is more robust to\njoint-level skeleton noise. Besides, a new part-level data augmentation is\nproposed to improve the performance of the model. On two large-scale datasets,\nNTU-RGB+D 60 and NTU RGB+D 120, the proposed IIP-Transformer achieves\nthe-state-of-art performance with more than 8x less computational complexity\nthan DSTA-Net, which is the SOTA Transformer-based method.\n", "rewritten_text": "Recent Transformer-based networks have shown great promise for skeleton-based action recognition.  Their ability to capture both global and local dependencies is key to their success, but this also leads to quadratic computational and memory costs.  Furthermore, previous work primarily focuses on relationships between individual joints, often suffering from noisy skeleton data due to sensor noise or inaccurate estimations.\n\nTo address these limitations, we propose a novel Transformer-based network, IIP-Transformer.  Unlike previous methods, IIP-Transformer simultaneously incorporates interactions between body joints and parts, efficiently and effectively capturing both joint-level (intra-part) and part-level (inter-part) dependencies.  We also introduce a part-level skeleton data encoding that significantly reduces computational complexity and improves robustness to noisy joint-level data.  Additionally, a novel part-level data augmentation technique is proposed to further enhance model performance.\n\nOn the large-scale NTU-RGB+D 60 and NTU-RGB+D 120 datasets, IIP-Transformer achieves state-of-the-art performance with over eight times less computational complexity than DSTA-Net, the previous state-of-the-art Transformer-based method.\n"}, "2307.01200": {"original_text": "  Learning-based approaches to monocular motion capture have recently shown\npromising results by learning to regress in a data-driven manner. However, due\nto the challenges in data collection and network designs, it remains\nchallenging for existing solutions to achieve real-time full-body capture while\nbeing accurate in world space. In this work, we introduce ProxyCap, a\nhuman-centric proxy-to-motion learning scheme to learn world-space motions from\na proxy dataset of 2D skeleton sequences and 3D rotational motions. Such proxy\ndata enables us to build a learning-based network with accurate world-space\nsupervision while also mitigating the generalization issues. For more accurate\nand physically plausible predictions in world space, our network is designed to\nlearn human motions from a human-centric perspective, which enables the\nunderstanding of the same motion captured with different camera trajectories.\nMoreover, a contact-aware neural motion descent module is proposed in our\nnetwork so that it can be aware of foot-ground contact and motion misalignment\nwith the proxy observations. With the proposed learning-based solution, we\ndemonstrate the first real-time monocular full-body capture system with\nplausible foot-ground contact in world space even using hand-held moving\ncameras. Our project page is https://zhangyux15.github.io/ProxyCapV2.\n", "rewritten_text": "Recent learning-based approaches to monocular motion capture have yielded promising results through data-driven regression.  However, data acquisition and network architecture challenges hinder existing solutions from achieving accurate, real-time, full-body capture in world space.  This work introduces ProxyCap, a human-centric proxy-to-motion learning framework that learns world-space motions from a proxy dataset of 2D skeletal sequences and 3D rotational movements. This proxy data enables the training of a learning-based network with accurate world-space supervision, mitigating generalization issues.  To ensure accurate and physically plausible world-space predictions, our network adopts a human-centric perspective, enabling it to understand the same motion captured from varying camera trajectories.  Furthermore, a contact-aware neural motion descent module is incorporated to account for foot-ground contact and potential misalignments with proxy observations.  This learning-based solution demonstrates, for the first time, a real-time monocular full-body capture system with plausible foot-ground contact in world space, even using handheld, moving cameras.  See our project page for more details: https://zhangyux15.github.io/ProxyCapV2.\n"}, "1410.7484": {"original_text": "  Stochastic sampling based trackers have shown good performance for abrupt\nmotion tracking so that they have gained popularity in recent years. However,\nconventional methods tend to use a two-stage sampling paradigm, in which the\nsearch space needs to be uniformly explored with an inefficient preliminary\nsampling phase. In this paper, we propose a novel sampling-based method in the\nBayesian filtering framework to address the problem. Within the framework,\nnearest neighbor field estimation is utilized to compute the importance\nproposal probabilities, which guide the Markov chain search towards promising\nregions and thus enhance the sampling efficiency; given the motion priors, a\nsmoothing stochastic sampling Monte Carlo algorithm is proposed to approximate\nthe posterior distribution through a smoothing weight-updating scheme.\nMoreover, to track the abrupt and the smooth motions simultaneously, we develop\nan abrupt-motion detection scheme which can discover the presence of abrupt\nmotions during online tracking. Extensive experiments on challenging image\nsequences demonstrate the effectiveness and the robustness of our algorithm in\nhandling the abrupt motions.\n", "rewritten_text": "Stochastic sampling-based trackers have demonstrated strong performance in tracking abrupt motion, leading to increased popularity in recent years.  However, conventional methods typically employ a two-stage sampling paradigm, requiring inefficient uniform exploration of the search space during a preliminary sampling phase. This paper proposes a novel Bayesian filtering framework that addresses this limitation.  Within this framework, nearest-neighbor field estimation computes importance proposal probabilities, guiding the Markov chain search toward promising regions and improving sampling efficiency.  Leveraging motion priors, a smoothing stochastic sampling Monte Carlo algorithm, incorporating a smoothing weight-updating scheme, approximates the posterior distribution.  Furthermore, to handle both abrupt and smooth motions, we introduce an abrupt-motion detection scheme for online identification of abrupt motion events.  Extensive experiments on challenging image sequences demonstrate the effectiveness and robustness of our algorithm in handling abrupt motions.\n"}, "2010.06944": {"original_text": "  We present a formulation of the relative depth estimation from a single image\nproblem, as a ranking problem. By reformulating the problem this way, we were\nable to utilize literature on the ranking problem, and apply the existing\nknowledge to achieve better results. To this end, we have introduced a listwise\nranking loss borrowed from ranking literature, weighted ListMLE, to the\nrelative depth estimation problem. We have also brought a new metric which\nconsiders pixel depth ranking accuracy, on which our method is stronger.\n", "rewritten_text": "This paper formulates relative depth estimation from a single image as a ranking problem.  This reformulation allows us to leverage existing ranking literature and achieve improved results.  Specifically, we introduce a weighted ListMLE loss function, adapted from the ranking literature, to the relative depth estimation task.  Furthermore, we propose a novel metric that assesses pixel depth ranking accuracy, on which our method demonstrates superior performance.\n"}, "2012.02951": {"original_text": "  Visual scene understanding is the core task in making any crucial decision in\nany computer vision system. Although popular computer vision datasets like\nCityscapes, MS-COCO, PASCAL provide good benchmarks for several tasks (e.g.\nimage classification, segmentation, object detection), these datasets are\nhardly suitable for post disaster damage assessments. On the other hand,\nexisting natural disaster datasets include mainly satellite imagery which have\nlow spatial resolution and a high revisit period. Therefore, they do not have a\nscope to provide quick and efficient damage assessment tasks. Unmanned Aerial\nVehicle(UAV) can effortlessly access difficult places during any disaster and\ncollect high resolution imagery that is required for aforementioned tasks of\ncomputer vision. To address these issues we present a high resolution UAV\nimagery, FloodNet, captured after the hurricane Harvey. This dataset\ndemonstrates the post flooded damages of the affected areas. The images are\nlabeled pixel-wise for semantic segmentation task and questions are produced\nfor the task of visual question answering. FloodNet poses several challenges\nincluding detection of flooded roads and buildings and distinguishing between\nnatural water and flooded water. With the advancement of deep learning\nalgorithms, we can analyze the impact of any disaster which can make a precise\nunderstanding of the affected areas. In this paper, we compare and contrast the\nperformances of baseline methods for image classification, semantic\nsegmentation, and visual question answering on our dataset.\n", "rewritten_text": "Visual scene understanding is crucial for decision-making in computer vision systems. While popular datasets like Cityscapes, MS-COCO, and PASCAL provide benchmarks for tasks such as image classification, segmentation, and object detection, they are unsuitable for post-disaster damage assessment. Existing natural disaster datasets primarily consist of low-resolution satellite imagery with long revisit periods, hindering rapid and efficient damage assessment.  Unmanned Aerial Vehicles (UAVs) offer a solution, providing easy access to disaster-affected areas and collecting the high-resolution imagery needed for computer vision tasks.\n\nTo address this limitation, we present FloodNet, a high-resolution UAV imagery dataset captured after Hurricane Harvey.  This dataset depicts post-flood damage and features pixel-wise labels for semantic segmentation and associated questions for visual question answering (VQA). FloodNet presents unique challenges, including differentiating flooded roads and buildings from natural water bodies.  Advances in deep learning offer the potential for precise analysis of disaster impact and improved understanding of affected areas.  This paper compares and contrasts the performance of baseline methods for image classification, semantic segmentation, and VQA on the FloodNet dataset.\n"}, "1905.1062": {"original_text": "  Large-scale face recognition in-the-wild has been recently achieved matured\nperformance in many real work applications. However, such systems are built on\nGPU platforms and mostly deploy heavy deep network architectures. Given a\nhigh-performance heavy network as a teacher, this work presents a simple and\nelegant teacher-student learning paradigm, namely ShrinkTeaNet, to train a\nportable student network that has significantly fewer parameters and\ncompetitive accuracy against the teacher network. Far apart from prior\nteacher-student frameworks mainly focusing on accuracy and compression ratios\nin closed-set problems, our proposed teacher-student network is proved to be\nmore robust against open-set problem, i.e. large-scale face recognition. In\naddition, this work introduces a novel Angular Distillation Loss for distilling\nthe feature direction and the sample distributions of the teacher's hypersphere\nto its student. Then ShrinkTeaNet framework can efficiently guide the student's\nlearning process with the teacher's knowledge presented in both intermediate\nand last stages of the feature embedding. Evaluations on LFW, CFP-FP, AgeDB,\nIJB-B and IJB-C Janus, and MegaFace with one million distractors have\ndemonstrated the efficiency of the proposed approach to learn robust student\nnetworks which have satisfying accuracy and compact sizes. Our ShrinkTeaNet is\nable to support the light-weight architecture achieving high performance with\n99.77% on LFW and 95.64% on large-scale Megaface protocols.\n", "rewritten_text": "Recent advancements in large-scale, in-the-wild face recognition have yielded mature performance in many real-world applications.  However, these systems typically rely on GPU platforms and resource-intensive deep network architectures.  To address this, we present ShrinkTeaNet, a novel teacher-student learning paradigm that trains a compact, portable student network with significantly fewer parameters while maintaining competitive accuracy compared to a high-performance teacher network.  Unlike prior teacher-student frameworks primarily focused on accuracy and compression ratios in closed-set problems, ShrinkTeaNet demonstrates enhanced robustness in open-set scenarios, specifically large-scale face recognition.  Furthermore, we introduce a novel Angular Distillation Loss to effectively transfer the teacher's feature direction and hypersphere sample distribution to the student.  ShrinkTeaNet leverages this knowledge from both intermediate and final feature embedding stages to efficiently guide the student's learning process.  Evaluations on LFW, CFP-FP, AgeDB, IJB-B, IJB-C Janus, and MegaFace (with one million distractors) demonstrate the effectiveness of our approach in learning robust, compact student networks with high accuracy.  ShrinkTeaNet achieves impressive results, reaching 99.77% accuracy on LFW and 95.64% on the large-scale MegaFace benchmark.\n"}, "1601.0763": {"original_text": "  We propose a method that integrates two widely available data sources,\nbuilding footprints from 2D maps and street level images, to derive valuable\ninformation that is generally difficult to acquire -- building heights and\nbuilding facade masks in images. Building footprints are elevated in world\ncoordinates and projected onto images. Building heights are estimated by\nscoring projected footprints based on their alignment with building features in\nimages. Building footprints with estimated heights can be converted to simple\n3D building models, which are projected back to images to identify buildings.\nIn this procedure, accurate camera projections are critical. However, camera\nposition errors inherited from external sensors commonly exist, which adversely\naffect results. We derive a solution to precisely locate cameras on maps using\ncorrespondence between image features and building footprints. Experiments on\nreal-world datasets show the promise of our method.\n", "rewritten_text": "This paper presents a novel method for estimating building heights and generating facade masks from readily available 2D map data and street-level imagery.  We leverage building footprints from maps, elevating them to world coordinates and projecting them onto corresponding images.  Building height is then estimated by scoring the projected footprints based on their alignment with image features.  These height-estimated footprints are subsequently used to create simplified 3D building models, which are projected back onto the images for improved building identification.  Accurate camera projection is crucial for this process.  To address the common problem of camera position errors inherited from external sensors, we introduce a solution that precisely geolocates cameras by establishing correspondences between image features and building footprints.  Experiments on real-world datasets demonstrate the effectiveness of our approach.\n"}, "2112.12141": {"original_text": "  3D human pose estimation (HPE) in autonomous vehicles (AV) differs from other\nuse cases in many factors, including the 3D resolution and range of data,\nabsence of dense depth maps, failure modes for LiDAR, relative location between\nthe camera and LiDAR, and a high bar for estimation accuracy. Data collected\nfor other use cases (such as virtual reality, gaming, and animation) may\ntherefore not be usable for AV applications. This necessitates the collection\nand annotation of a large amount of 3D data for HPE in AV, which is\ntime-consuming and expensive. In this paper, we propose one of the first\napproaches to alleviate this problem in the AV setting. Specifically, we\npropose a multi-modal approach which uses 2D labels on RGB images as weak\nsupervision to perform 3D HPE. The proposed multi-modal architecture\nincorporates LiDAR and camera inputs with an auxiliary segmentation branch. On\nthe Waymo Open Dataset, our approach achieves a 22% relative improvement over\ncamera-only 2D HPE baseline, and 6% improvement over LiDAR-only model. Finally,\ncareful ablation studies and parts based analysis illustrate the advantages of\neach of our contributions.\n", "rewritten_text": "3D human pose estimation (HPE) for autonomous vehicles (AVs) differs significantly from other applications due to several factors: lower 3D resolution and range, the absence of dense depth maps, LiDAR failure modes, the varying relative positions of cameras and LiDAR, and stringent accuracy requirements.  Consequently, datasets collected for applications like virtual reality, gaming, and animation are generally unsuitable for AVs.  This necessitates the collection and annotation of extensive 3D data for AV HPE, a process that is both time-consuming and costly.  This paper presents a novel approach to mitigate this challenge.  Specifically, we propose a multi-modal method that leverages weakly supervised 2D labels from RGB images to perform 3D HPE.  Our architecture integrates LiDAR and camera inputs with an auxiliary segmentation branch.  Using the Waymo Open Dataset, our approach achieves a 22% relative improvement over a camera-only 2D HPE baseline and a 6% improvement over a LiDAR-only model.  Finally, comprehensive ablation studies and a part-based analysis demonstrate the effectiveness of our individual contributions.\n"}, "2306.02182": {"original_text": "  Indian court legal texts and processes are essential towards the integrity of\nthe judicial system and towards maintaining the social and political order of\nthe nation. Due to the increase in number of pending court cases, there is an\nurgent need to develop tools to automate many of the legal processes with the\nknowledge of artificial intelligence. In this paper, we employ knowledge\nextraction techniques, specially the named entity extraction of legal entities\nwithin court case judgements. We evaluate several state of the art\narchitectures in the realm of sequence labeling using models trained on a\ncurated dataset of legal texts. We observe that a Bi-LSTM model trained on\nFlair Embeddings achieves the best results, and we also publish the BIO\nformatted dataset as part of this paper.\n", "rewritten_text": "The integrity of India's judicial system and the nation's social and political order depend on the effective functioning of its legal texts and processes.  The burgeoning backlog of court cases necessitates the urgent development of AI-powered tools to automate many legal processes. This paper explores knowledge extraction techniques, specifically named entity recognition (NER) of legal entities within court judgments. We evaluate several state-of-the-art sequence labeling architectures using models trained on a curated dataset of legal texts.  Our results demonstrate that a Bi-LSTM model trained on Flair embeddings achieves superior performance.  The BIO-formatted dataset used in this study is also publicly released as part of this paper.\n"}, "2011.09563": {"original_text": "  Unsupervised domain adaptation (UDA) is widely used to transfer knowledge\nfrom a labeled source domain to an unlabeled target domain with different data\ndistribution. While extensive studies attested that deep learning models are\nvulnerable to adversarial attacks, the adversarial robustness of models in\ndomain adaptation application has largely been overlooked. This paper points\nout that the inevitable domain distribution deviation in UDA is a critical\nbarrier to model robustness on the target domain. To address the problem, we\npropose a novel Class-consistent Unsupervised Robust Domain Adaptation (CURDA)\nframework for training robust UDA models. With the introduced contrastive\nrobust training and source anchored adversarial contrastive losses, our\nproposed CURDA framework can effectively robustify UDA models by simultaneously\nminimizing the data distribution deviation and the distance between target\ndomain clean-adversarial pairs without creating classification confusion.\nExperiments on several public benchmarks show that CURDA can significantly\nimprove model robustness in the target domain with only minor cost of accuracy\non the clean samples.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) leverages knowledge from a labeled source domain to an unlabeled target domain with differing data distributions.  While deep learning models' vulnerability to adversarial attacks is well-documented, their robustness within domain adaptation applications remains largely unexplored.  This paper highlights the inherent domain distribution discrepancy in UDA as a significant obstacle to target domain robustness.  To address this, we propose Class-consistent Unsupervised Robust Domain Adaptation (CURDA), a novel framework for training robust UDA models.  CURDA incorporates contrastive robust training and source-anchored adversarial contrastive losses to simultaneously minimize data distribution discrepancies and the distance between clean and adversarial target domain examples, without compromising classification accuracy.  Experiments on several public benchmarks demonstrate that CURDA significantly enhances target domain robustness with minimal impact on clean sample accuracy.\n"}, "1907.00184": {"original_text": "  Since Bahdanau et al. [1] first introduced attention for neural machine\ntranslation, most sequence-to-sequence models made use of attention mechanisms\n[2, 3, 4]. While they produce soft-alignment matrices that could be interpreted\nas alignment between target and source languages, we lack metrics to quantify\ntheir quality, being unclear which approach produces the best alignments. This\npaper presents an empirical evaluation of 3 main sequence-to-sequence models\n(CNN, RNN and Transformer-based) for word discovery from unsegmented phoneme\nsequences. This task consists in aligning word sequences in a source language\nwith phoneme sequences in a target language, inferring from it word\nsegmentation on the target side [5]. Evaluating word segmentation quality can\nbe seen as an extrinsic evaluation of the soft-alignment matrices produced\nduring training. Our experiments in a low-resource scenario on Mboshi and\nEnglish languages (both aligned to French) show that RNNs surprisingly\noutperform CNNs and Transformer for this task. Our results are confirmed by an\nintrinsic evaluation of alignment quality through the use of Average Normalized\nEntropy (ANE). Lastly, we improve our best word discovery model by using an\nalignment entropy confidence measure that accumulates ANE over all the\noccurrences of a given alignment pair in the collection.\n", "rewritten_text": "Since Bahdanau et al. [1] introduced attention mechanisms for neural machine translation, most sequence-to-sequence models have incorporated them [2, 3, 4].  Although these models produce soft-alignment matrices interpretable as source-target language alignments,  a lack of quantitative metrics hinders the assessment of alignment quality and optimal approach selection. This paper presents an empirical evaluation of three main sequence-to-sequence architectures (CNN, RNN, and Transformer) for word discovery from unsegmented phoneme sequences.  This task involves aligning source-language word sequences with target-language phoneme sequences, thereby inferring word segmentation in the target language [5].  Evaluating word segmentation quality serves as an extrinsic evaluation of the soft-alignment matrices generated during training.  Our low-resource experiments using Mboshi and English (both aligned to French) surprisingly reveal that RNNs outperform CNNs and Transformers for this task.  These findings are corroborated by an intrinsic evaluation of alignment quality using Average Normalized Entropy (ANE). Finally, we enhance our best-performing word discovery model by incorporating an alignment entropy confidence measure that aggregates ANE across all occurrences of a given alignment pair within the corpus.\n"}, "1812.09877": {"original_text": "  In multimodal unsupervised image-to-image translation tasks, the goal is to\ntranslate an image from the source domain to many images in the target domain.\nWe present a simple method that produces higher quality images than current\nstate-of-the-art while maintaining the same amount of multimodal diversity.\nPrevious methods follow the unconditional approach of trying to map the latent\ncode directly to a full-size image. This leads to complicated network\narchitectures with several introduced hyperparameters to tune. By treating the\nlatent code as a modifier of the convolutional filters, we produce multimodal\noutput while maintaining the traditional Generative Adversarial Network (GAN)\nloss and without additional hyperparameters. The only tuning required by our\nmethod controls the tradeoff between variability and quality of generated\nimages. Furthermore, we achieve disentanglement between source domain content\nand target domain style for free as a by-product of our formulation. We perform\nqualitative and quantitative experiments showing the advantages of our method\ncompared with the state-of-the art on multiple benchmark image-to-image\ntranslation datasets.\n", "rewritten_text": "Multimodal unsupervised image-to-image translation aims to translate a single source image into multiple diverse target images.  We propose a novel method that generates higher-quality images than current state-of-the-art techniques while preserving comparable multimodal diversity.  Unlike previous methods, which employ a complex, unconditional approach of directly mapping latent codes to full-size images\u2014resulting in intricate network architectures and numerous hyperparameters\u2014our method treats the latent code as a modifier of convolutional filters. This approach yields multimodal outputs using only the traditional Generative Adversarial Network (GAN) loss, without introducing additional hyperparameters.  The sole tunable parameter controls the balance between image variability and quality.  Importantly, our formulation inherently achieves disentanglement between source content and target style.  Qualitative and quantitative experiments on multiple benchmark image-to-image translation datasets demonstrate the superiority of our method over existing state-of-the-art approaches.\n"}, "1807.04047": {"original_text": "  Document image quality assessment (DIQA) is an important and challenging\nproblem in real applications. In order to predict the quality scores of\ndocument images, this paper proposes a novel no-reference DIQA method based on\ncharacter gradient, where the OCR accuracy is used as a ground-truth quality\nmetric. Character gradient is computed on character patches detected with the\nmaximally stable extremal regions (MSER) based method. Character patches are\nessentially significant to character recognition and therefore suitable for use\nin estimating document image quality. Experiments on a benchmark dataset show\nthat the proposed method outperforms the state-of-the-art methods in estimating\nthe quality score of document images.\n", "rewritten_text": "Document image quality assessment (DIQA) is a significant challenge in real-world applications.  This paper proposes a novel no-reference DIQA method that leverages character gradients to predict document image quality scores.  Optical character recognition (OCR) accuracy serves as the ground truth metric.  Character gradients are computed on character patches detected using the maximally stable extremal regions (MSER) method.  These patches are crucial for character recognition and thus well-suited for estimating document image quality.  Experiments on a benchmark dataset demonstrate that our proposed method surpasses state-of-the-art techniques in accurately estimating document image quality scores.\n"}, "2406.01171": {"original_text": "  The concept of persona, originally adopted in dialogue literature, has\nre-surged as a promising framework for tailoring large language models (LLMs)\nto specific context (e.g., personalized search, LLM-as-a-judge). However, the\ngrowing research on leveraging persona in LLMs is relatively disorganized and\nlacks a systematic taxonomy. To close the gap, we present a comprehensive\nsurvey to categorize the current state of the field. We identify two lines of\nresearch, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and\n(2) LLM Personalization, where LLMs take care of user personas. Additionally,\nwe introduce existing methods for LLM personality evaluation. To the best of\nour knowledge, we present the first survey for role-playing and personalization\nin LLMs under the unified view of persona. We continuously maintain a paper\ncollection to foster future endeavors:\nhttps://github.com/MiuLab/PersonaLLM-Survey\n", "rewritten_text": "The concept of persona, initially employed in dialogue literature, has re-emerged as a promising framework for tailoring large language models (LLMs) to specific contexts (e.g., personalized search, LLM-as-a-judge).  However, existing research on leveraging persona in LLMs is relatively disorganized and lacks a systematic taxonomy.  To address this gap, we present a comprehensive survey categorizing the current state of the field. We identify two primary research directions: (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs adapt to user personas.  Furthermore, we review existing methods for evaluating LLM personality. To our knowledge, this is the first survey to unify the perspectives of role-playing and personalization in LLMs under the umbrella of persona.  We maintain a continuously updated paper collection to facilitate future research: [https://github.com/MiuLab/PersonaLLM-Survey](https://github.com/MiuLab/PersonaLLM-Survey)\n"}, "1804.02745": {"original_text": "  Dynamic contrast-enhanced (DCE) MRI is an evolving imaging technique that\nprovides a quantitative measure of pharmacokinetic (PK) parameters in body\ntissues, in which series of T1-weighted images are collected following the\nadministration of a paramagnetic contrast agent. Unfortunately, in many\napplications, conventional clinical DCE-MRI suffers from low spatiotemporal\nresolution and insufficient volume coverage. In this paper, we propose a novel\ndeep learning based approach to directly estimate the PK parameters from\nundersampled DCE-MRI data. Specifically, we design a custom loss function where\nwe incorporate a forward physical model that relates the PK parameters to\ncorrupted image-time series obtained due to subsampling in k-space. This allows\nthe network to directly exploit the knowledge of true contrast agent kinetics\nin the training phase, and hence provide more accurate restoration of PK\nparameters. Experiments on clinical brain DCE datasets demonstrate the efficacy\nof our approach in terms of fidelity of PK parameter reconstruction and\nsignificantly faster parameter inference compared to a model-based iterative\nreconstruction method.\n", "rewritten_text": "Dynamic contrast-enhanced (DCE) MRI is an evolving imaging technique providing quantitative measures of pharmacokinetic (PK) parameters in body tissues.  It involves acquiring a series of T1-weighted images after administering a paramagnetic contrast agent. However, conventional clinical DCE-MRI often suffers from low spatiotemporal resolution and inadequate volume coverage.  This paper proposes a novel deep learning approach to directly estimate PK parameters from undersampled DCE-MRI data.  We designed a custom loss function incorporating a forward physical model that relates PK parameters to the corrupted image time series resulting from k-space subsampling. This allows the network to leverage knowledge of true contrast agent kinetics during training, thereby enabling more accurate PK parameter restoration. Experiments on clinical brain DCE datasets demonstrate the efficacy of our approach, showing improved PK parameter reconstruction fidelity and significantly faster inference compared to a model-based iterative reconstruction method.\n"}, "2207.01164": {"original_text": "  Neural Radiance Field (NeRF) regresses a neural parameterized scene by\ndifferentially rendering multi-view images with ground-truth supervision.\nHowever, when interpolating novel views, NeRF often yields inconsistent and\nvisually non-smooth geometric results, which we consider as a generalization\ngap between seen and unseen views. Recent advances in convolutional neural\nnetworks have demonstrated the promise of advanced robust data augmentations,\neither random or learned, in enhancing both in-distribution and\nout-of-distribution generalization. Inspired by that, we propose Augmented NeRF\n(Aug-NeRF), which for the first time brings the power of robust data\naugmentations into regularizing the NeRF training. Particularly, our proposal\nlearns to seamlessly blend worst-case perturbations into three distinct levels\nof the NeRF pipeline with physical grounds, including (1) the input\ncoordinates, to simulate imprecise camera parameters at image capture; (2)\nintermediate features, to smoothen the intrinsic feature manifold; and (3)\npre-rendering output, to account for the potential degradation factors in the\nmulti-view image supervision. Extensive results demonstrate that Aug-NeRF\neffectively boosts NeRF performance in both novel view synthesis (up to 1.5dB\nPSNR gain) and underlying geometry reconstruction. Furthermore, thanks to the\nimplicit smooth prior injected by the triple-level augmentations, Aug-NeRF can\neven recover scenes from heavily corrupted images, a highly challenging setting\nuntackled before. Our codes are available in\nhttps://github.com/VITA-Group/Aug-NeRF.\n", "rewritten_text": "Neural Radiance Fields (NeRFs) regress a neural scene representation by differentially rendering multi-view images with ground-truth supervision.  However, NeRFs often exhibit inconsistent and visually unsmooth geometric results when interpolating novel views, a limitation we attribute to a generalization gap between seen and unseen viewpoints.  Recent advances in convolutional neural networks have shown the effectiveness of robust data augmentations, both random and learned, in improving both in-distribution and out-of-distribution generalization.  Inspired by this, we propose Augmented NeRF (Aug-NeRF), the first method to leverage robust data augmentations for NeRF regularization.  Specifically, Aug-NeRF learns to seamlessly integrate worst-case perturbations at three distinct levels of the NeRF pipeline, grounded in physical principles: (1) input coordinates, simulating imprecise camera parameters during image capture; (2) intermediate features, smoothing the intrinsic feature manifold; and (3) pre-rendering output, accounting for potential degradations in multi-view image supervision.  Extensive experiments demonstrate that Aug-NeRF significantly improves NeRF performance in novel view synthesis (up to a 1.5 dB PSNR gain) and underlying geometry reconstruction.  Moreover, the implicit smoothness prior introduced by the three-level augmentations enables Aug-NeRF to reconstruct scenes from heavily corrupted images\u2014a previously unsolved challenge.  Our code is available at https://github.com/VITA-Group/Aug-NeRF.\n"}, "1511.05296": {"original_text": "  In this paper, we propose a method for ranking fashion images to find the\nones which might be liked by more people. We collect two new datasets from\nimage sharing websites (Pinterest and Polyvore). We represent fashion images\nbased on attributes: semantic attributes and data-driven attributes. To learn\nsemantic attributes from limited training data, we use an algorithm on\nmulti-task convolutional neural networks to share visual knowledge among\ndifferent semantic attribute categories. To discover data-driven attributes\nunsupervisedly, we propose an algorithm to simultaneously discover visual\nclusters and learn fashion-specific feature representations. Given attributes\nas representations, we propose to learn a ranking SPN (sum product networks) to\nrank pairs of fashion images. The proposed ranking SPN can capture the\nhigh-order correlations of the attributes. We show the effectiveness of our\nmethod on our two newly collected datasets.\n", "rewritten_text": "This paper presents a novel method for ranking fashion images to predict popularity.  We introduce two new datasets collected from Pinterest and Polyvore.  Fashion image representation leverages both semantic and data-driven attributes.  Semantic attributes are learned from limited training data using a multi-task convolutional neural network (CNN) that shares visual knowledge across attribute categories.  Data-driven attributes are discovered unsupervisedly via a novel algorithm that simultaneously identifies visual clusters and learns fashion-specific feature representations.  Finally, a ranking Sum-Product Network (SPN) is trained on these attribute representations to rank image pairs, capturing high-order attribute correlations.  The effectiveness of our method is demonstrated on our newly collected datasets.\n"}, "1901.00049": {"original_text": "  We introduce a new silhouette-based representation for modeling clothed human\nbodies using deep generative models. Our method can reconstruct a complete and\ntextured 3D model of a person wearing clothes from a single input picture.\nInspired by the visual hull algorithm, our implicit representation uses 2D\nsilhouettes and 3D joints of a body pose to describe the immense shape\ncomplexity and variations of clothed people. Given a segmented 2D silhouette of\na person and its inferred 3D joints from the input picture, we first synthesize\nconsistent silhouettes from novel view points around the subject. The\nsynthesized silhouettes which are the most consistent with the input\nsegmentation are fed into a deep visual hull algorithm for robust 3D shape\nprediction. We then infer the texture of the subject's back view using the\nfrontal image and segmentation mask as input to a conditional generative\nadversarial network. Our experiments demonstrate that our silhouette-based\nmodel is an effective representation and the appearance of the back view can be\npredicted reliably using an image-to-image translation network. While classic\nmethods based on parametric models often fail for single-view images of\nsubjects with challenging clothing, our approach can still produce successful\nresults, which are comparable to those obtained from multi-view input.\n", "rewritten_text": "This paper introduces a novel silhouette-based representation for modeling clothed human bodies using deep generative models.  Our method reconstructs complete, textured 3D models of clothed individuals from a single input image.  Inspired by the visual hull algorithm, our implicit representation leverages 2D silhouettes and 3D joint positions to capture the significant shape complexity and variability of clothed humans.  Given a segmented 2D silhouette and inferred 3D joints from the input image, we synthesize consistent silhouettes from novel viewpoints.  The most consistent synthesized silhouettes are then fed into a deep visual hull algorithm for robust 3D shape prediction.  Finally, we infer the texture of the subject's back view using a conditional generative adversarial network (cGAN), with the frontal image and segmentation mask as input.  Our experiments demonstrate the effectiveness of this silhouette-based representation, reliably predicting back-view appearance using an image-to-image translation network. Unlike traditional parametric methods, which often fail with single-view images of subjects in complex clothing, our approach achieves comparable results to those obtained from multi-view input.\n"}, "2308.07837": {"original_text": "  In this paper, we present a novel shape reconstruction method leveraging\ndiffusion model to generate 3D sparse point cloud for the object captured in a\nsingle RGB image. Recent methods typically leverage global embedding or local\nprojection-based features as the condition to guide the diffusion model.\nHowever, such strategies fail to consistently align the denoised point cloud\nwith the given image, leading to unstable conditioning and inferior\nperformance. In this paper, we present CCD-3DR, which exploits a novel centered\ndiffusion probabilistic model for consistent local feature conditioning. We\nconstrain the noise and sampled point cloud from the diffusion model into a\nsubspace where the point cloud center remains unchanged during the forward\ndiffusion process and reverse process. The stable point cloud center further\nserves as an anchor to align each point with its corresponding local\nprojection-based features. Extensive experiments on synthetic benchmark\nShapeNet-R2N2 demonstrate that CCD-3DR outperforms all competitors by a large\nmargin, with over 40% improvement. We also provide results on real-world\ndataset Pix3D to thoroughly demonstrate the potential of CCD-3DR in real-world\napplications. Codes will be released soon\n", "rewritten_text": "This paper introduces CCD-3DR, a novel shape reconstruction method that leverages a centered diffusion probabilistic model to generate 3D sparse point clouds from single RGB images.  Existing methods typically condition diffusion models using global embeddings or local projection-based features, resulting in inconsistent alignment between the denoised point cloud and the input image, leading to unstable conditioning and suboptimal performance.  In contrast, CCD-3DR employs a novel approach that constrains both the noise and the sampled point cloud within a subspace where the point cloud's center remains invariant throughout the forward and reverse diffusion processes. This stable center acts as an anchor, facilitating accurate alignment between each point and its corresponding local projection-based features.  Extensive experiments on the ShapeNet-R2N2 benchmark demonstrate that CCD-3DR significantly outperforms existing state-of-the-art methods, achieving over a 40% improvement.  Further results on the real-world Pix3D dataset highlight CCD-3DR's potential for practical applications.  The code will be released shortly.\n"}, "2312.10610": {"original_text": "  A number of tasks have been proposed recently to facilitate easy access to\ncharts such as chart QA and summarization. The dominant paradigm to solve these\ntasks has been to fine-tune a pretrained model on the task data. However, this\napproach is not only expensive but also not generalizable to unseen tasks. On\nthe other hand, large language models (LLMs) have shown impressive\ngeneralization capabilities to unseen tasks with zero- or few-shot prompting.\nHowever, their application to chart-related tasks is not trivial as these tasks\ntypically involve considering not only the underlying data but also the visual\nfeatures in the chart image. We propose PromptChart, a multimodal few-shot\nprompting framework with LLMs for chart-related applications. By analyzing the\ntasks carefully, we have come up with a set of prompting guidelines for each\ntask to elicit the best few-shot performance from LLMs. We further propose a\nstrategy to inject visual information into the prompts. Our experiments on\nthree different chart-related information consumption tasks show that with\nproperly designed prompts LLMs can excel on the benchmarks, achieving\nstate-of-the-art.\n", "rewritten_text": "Recent work has proposed numerous tasks to simplify access to chart information, including chart question answering (QA) and summarization.  The prevailing approach involves fine-tuning pre-trained models, a method that is both computationally expensive and lacks generalizability to novel tasks.  In contrast, large language models (LLMs) demonstrate impressive zero-shot and few-shot capabilities. However, applying LLMs to chart-related tasks presents challenges, as these tasks require consideration of both the underlying data and the visual features of the chart image.\n\nThis paper introduces PromptChart, a multimodal few-shot prompting framework leveraging LLMs for chart-related applications.  We developed task-specific prompting guidelines to optimize LLM performance, along with a strategy for incorporating visual information into the prompts.  Experiments across three chart-related information consumption tasks demonstrate that, with carefully designed prompts, LLMs achieve state-of-the-art results on established benchmarks.\n"}, "1906.03657": {"original_text": "  Group convolution works well with many deep convolutional neural networks\n(CNNs) that can effectively compress the model by reducing the number of\nparameters and computational cost. Using this operation, feature maps of\ndifferent group cannot communicate, which restricts their representation\ncapability. To address this issue, in this work, we propose a novel operation\nnamed Hierarchical Group Convolution (HGC) for creating computationally\nefficient neural networks. Different from standard group convolution which\nblocks the inter-group information exchange and induces the severe performance\ndegradation, HGC can hierarchically fuse the feature maps from each group and\nleverage the inter-group information effectively. Taking advantage of the\nproposed method, we introduce a family of compact networks called HGCNets.\nCompared to networks using standard group convolution, HGCNets have a huge\nimprovement in accuracy at the same model size and complexity level. Extensive\nexperimental results on the CIFAR dataset demonstrate that HGCNets obtain\nsignificant reduction of parameters and computational cost to achieve\ncomparable performance over the prior CNN architectures designed for mobile\ndevices such as MobileNet and ShuffleNet.\n", "rewritten_text": "Hierarchical Group Convolution (HGC) improves upon standard group convolution for computationally efficient deep convolutional neural networks (CNNs). While standard group convolution reduces parameters and computational cost by limiting communication between feature maps of different groups, this restriction hinders representational capacity.  To overcome this limitation, we propose HGC, a novel operation that hierarchically fuses feature maps from each group, effectively leveraging inter-group information.  This leads to a family of compact networks, HGCNets, which significantly outperform networks using standard group convolution at comparable model size and complexity.  Extensive experiments on CIFAR demonstrate that HGCNets achieve comparable performance to state-of-the-art mobile CNN architectures like MobileNet and ShuffleNet, while significantly reducing parameters and computational cost.\n"}, "2304.01534": {"original_text": "  Bird's eye view (BEV) perception is becoming increasingly important in the\nfield of autonomous driving. It uses multi-view camera data to learn a\ntransformer model that directly projects the perception of the road environment\nonto the BEV perspective. However, training a transformer model often requires\na large amount of data, and as camera data for road traffic are often private,\nthey are typically not shared. Federated learning offers a solution that\nenables clients to collaborate and train models without exchanging data but\nmodel parameters. In this paper, we introduce FedBEVT, a federated transformer\nlearning approach for BEV perception. In order to address two common data\nheterogeneity issues in FedBEVT: (i) diverse sensor poses, and (ii) varying\nsensor numbers in perception systems, we propose two approaches -- Federated\nLearning with Camera-Attentive Personalization (FedCaP) and Adaptive\nMulti-Camera Masking (AMCM), respectively. To evaluate our method in real-world\nsettings, we create a dataset consisting of four typical federated use cases.\nOur findings suggest that FedBEVT outperforms the baseline approaches in all\nfour use cases, demonstrating the potential of our approach for improving BEV\nperception in autonomous driving.\n", "rewritten_text": "Bird's-eye view (BEV) perception is crucial for autonomous driving.  It leverages multi-view camera data to train a transformer model that directly projects road environment perception onto a BEV perspective.  However, training such models typically requires extensive data, which is often unavailable due to privacy concerns surrounding road traffic camera data. Federated learning offers a solution, enabling collaborative model training without direct data sharing, only exchanging model parameters.  This paper introduces FedBEVT, a federated transformer learning approach for BEV perception.  To address common data heterogeneity challenges\u2014specifically, diverse sensor poses and varying sensor numbers\u2014we propose two methods: Federated Learning with Camera-Attentive Personalization (FedCaP) and Adaptive Multi-Camera Masking (AMCM).  We evaluate FedBEVT using a newly created dataset representing four typical federated learning scenarios.  Results demonstrate that FedBEVT outperforms baseline approaches across all scenarios, highlighting its potential for enhancing BEV perception in autonomous driving.\n"}, "1911.10248": {"original_text": "  The rapid development of inexpensive commodity depth sensors has made\nkeypoint detection and matching in the depth image modality an important\nproblem in computer vision. Despite great improvements in recent RGB local\nfeature learning methods, adapting them directly in the depth modality leads to\nunsatisfactory performance. Most of these methods do not explicitly reason\nbeyond the visible pixels in the images. To address the limitations of these\nmethods, we propose a framework ViewSynth, to jointly learn: (1) viewpoint\ninvariant keypoint-descriptor from depth images using a proposed Contrastive\nMatching Loss, and (2) view synthesis of depth images from different viewpoints\nusing the proposed View Synthesis Module and View Synthesis Loss. By learning\nview synthesis, we explicitly encourage the feature extractor to encode\ninformation about not only the visible, but also the occluded parts of the\nscene. We demonstrate that in the depth modality, ViewSynth outperforms the\nstate-of-the-art depth and RGB local feature extraction techniques in the 3D\nkeypoint matching and camera localization tasks on the RGB-D datasets 7-Scenes,\nTUM RGBD and CoRBS in most scenarios. We also show the generalizability of\nViewSynth in 3D keypoint matching across different datasets.\n", "rewritten_text": "The proliferation of low-cost commodity depth sensors has made keypoint detection and matching in depth images a significant problem in computer vision.  While recent RGB local feature learning methods have shown substantial improvement, directly adapting them to the depth modality yields unsatisfactory results.  These methods typically fail to explicitly model information beyond visible pixels.  To address this limitation, we propose ViewSynth, a framework that jointly learns: (1) viewpoint-invariant keypoint descriptors from depth images using a novel contrastive matching loss, and (2) depth image view synthesis from different viewpoints using a proposed view synthesis module and loss function.  By learning view synthesis, we explicitly encourage the feature extractor to encode information about both visible and occluded scene regions.  We demonstrate that ViewSynth outperforms state-of-the-art depth and RGB local feature extraction techniques on 3D keypoint matching and camera localization tasks across multiple RGB-D datasets (7-Scenes, TUM RGB-D, and CoRBS), achieving superior performance in most scenarios.  Furthermore, we show ViewSynth's generalizability in 3D keypoint matching across diverse datasets.\n"}, "2211.10018": {"original_text": "  Relation extraction has the potential for large-scale knowledge graph\nconstruction, but current methods do not consider the qualifier attributes for\neach relation triplet, such as time, quantity or location. The qualifiers form\nhyper-relational facts which better capture the rich and complex knowledge\ngraph structure. For example, the relation triplet (Leonard Parker, Educated\nAt, Harvard University) can be factually enriched by including the qualifier\n(End Time, 1967). Hence, we propose the task of hyper-relational extraction to\nextract more specific and complete facts from text. To support the task, we\nconstruct HyperRED, a large-scale and general-purpose dataset. Existing models\ncannot perform hyper-relational extraction as it requires a model to consider\nthe interaction between three entities. Hence, we propose CubeRE, a\ncube-filling model inspired by table-filling approaches and explicitly\nconsiders the interaction between relation triplets and qualifiers. To improve\nmodel scalability and reduce negative class imbalance, we further propose a\ncube-pruning method. Our experiments show that CubeRE outperforms strong\nbaselines and reveal possible directions for future research. Our code and data\nare available at github.com/declare-lab/HyperRED.\n", "rewritten_text": "Relation extraction holds significant potential for large-scale knowledge graph construction. However, current methods neglect qualifier attributes\u2014such as time, quantity, or location\u2014associated with each relation triplet.  These qualifiers constitute hyper-relational facts, providing a richer and more nuanced representation of complex knowledge graph structures. For example, the relation triplet \"(Leonard Parker, Educated At, Harvard University)\" can be enhanced by adding the qualifier \"(End Time, 1967).\"  Therefore, we introduce the task of hyper-relational extraction to facilitate the extraction of more precise and comprehensive facts from text.  To support this task, we present HyperRED, a large-scale, general-purpose dataset.  Existing models are unsuitable for hyper-relational extraction because they lack the capacity to model the interaction between three entities (two entities and a relation).  Consequently, we propose CubeRE, a cube-filling model inspired by table-filling approaches that explicitly considers the interaction between relation triplets and qualifiers. To enhance model scalability and mitigate the negative class imbalance, we further introduce a cube-pruning method. Our experiments demonstrate that CubeRE surpasses strong baselines and highlight promising avenues for future research.  Our code and data are available at github.com/declare-lab/HyperRED.\n"}, "2210.06246": {"original_text": "  Recently, the community has achieved substantial progress on many commonsense\nreasoning benchmarks. However, it is still unclear what is learned from the\ntraining process: the knowledge, inference capability, or both? We argue that\ndue to the large scale of commonsense knowledge, it is infeasible to annotate a\nlarge enough training set for each task to cover all commonsense for learning.\nThus we should separate the commonsense knowledge acquisition and inference\nover commonsense knowledge as two separate tasks. In this work, we focus on\ninvestigating models' commonsense inference capabilities from two perspectives:\n(1) Whether models can know if the knowledge they have is enough to solve the\ntask; (2) Whether models can develop commonsense inference capabilities that\ngeneralize across commonsense tasks. We first align commonsense tasks with\nrelevant knowledge from commonsense knowledge bases and ask humans to annotate\nwhether the knowledge is enough or not. Then, we convert different commonsense\ntasks into a unified question answering format to evaluate models'\ngeneralization capabilities. We name the benchmark as Commonsense Inference\nwith Knowledge-in-the-loop Question Answering (CIKQA).\n", "rewritten_text": "Recent advancements in commonsense reasoning benchmarks have been substantial.  However, the nature of what models learn\u2014knowledge, inference capabilities, or both\u2014remains unclear.  The sheer scale of commonsense knowledge makes comprehensive annotation for diverse tasks infeasible.  Therefore, we propose separating commonsense knowledge acquisition and inference as distinct tasks.  This work investigates models' commonsense inference capabilities from two perspectives: (1) metacognitive awareness\u2014can models assess whether their existing knowledge suffices for a given task? and (2) generalization\u2014can models develop inference capabilities that transfer across diverse commonsense tasks?  We address these questions by aligning commonsense tasks with relevant knowledge from existing knowledge bases, obtaining human annotations on knowledge sufficiency, and unifying diverse tasks into a question-answering format to evaluate generalization.  This benchmark is termed Commonsense Inference with Knowledge-in-the-Loop Question Answering (CIKQA).\n"}, "2409.11819": {"original_text": "  6D object pose estimation is the problem of identifying the position and\norientation of an object relative to a chosen coordinate system, which is a\ncore technology for modern XR applications. State-of-the-art 6D object pose\nestimators directly predict an object pose given an object observation. Due to\nthe ill-posed nature of the pose estimation problem, where multiple different\nposes can correspond to a single observation, generating additional plausible\nestimates per observation can be valuable. To address this, we reformulate the\nstate-of-the-art algorithm GDRNPP and introduce EPRO-GDR (End-to-End\nProbabilistic Geometry-Guided Regression). Instead of predicting a single pose\nper detection, we estimate a probability density distribution of the pose.\nUsing the evaluation procedure defined by the BOP (Benchmark for 6D Object Pose\nEstimation) Challenge, we test our approach on four of its core datasets and\ndemonstrate superior quantitative results for EPRO-GDR on LM-O, YCB-V, and\nITODD. Our probabilistic solution shows that predicting a pose distribution\ninstead of a single pose can improve state-of-the-art single-view pose\nestimation while providing the additional benefit of being able to sample\nmultiple meaningful pose candidates.\n", "rewritten_text": "Six-degree-of-freedom (6DoF) object pose estimation\u2014the task of determining an object's position and orientation relative to a chosen coordinate system\u2014is a core technology for modern extended reality (XR) applications.  State-of-the-art 6DoF object pose estimators directly predict an object's pose from a single observation. However, because pose estimation is ill-posed (multiple poses can correspond to a single observation), generating multiple plausible pose estimates per observation is beneficial.  To address this, we reformulate the state-of-the-art GDRNPP algorithm and introduce EPRO-GDR (End-to-End Probabilistic Geometry-Guided Regression).  Instead of predicting a single pose per detection, EPRO-GDR estimates a probability density distribution of poses.  Evaluated using the Benchmark for 6D Object Pose Estimation (BOP) Challenge's procedure on four of its core datasets, EPRO-GDR demonstrates superior quantitative results on LM-O, YCB-V, and ITODD.  Our probabilistic approach shows that predicting a pose distribution, rather than a single pose, improves state-of-the-art single-view pose estimation while offering the advantage of readily sampling multiple meaningful pose candidates.\n"}, "2405.18111": {"original_text": "  Large language models (LLMs) are proven to benefit a lot from\nretrieval-augmented generation (RAG) in alleviating hallucinations confronted\nwith knowledge-intensive questions. RAG adopts information retrieval techniques\nto inject external knowledge from semantic-relevant documents as input\ncontexts. However, since today's Internet is flooded with numerous noisy and\nfabricating content, it is inevitable that RAG systems are vulnerable to these\nnoises and prone to respond incorrectly. To this end, we propose to optimize\nthe retrieval-augmented Generator with an Adversarial Tuning Multi-agent system\n(ATM). The ATM steers the Generator to have a robust perspective of useful\ndocuments for question answering with the help of an auxiliary Attacker agent\nthrough adversarially tuning the agents for several iterations. After rounds of\nmulti-agent iterative tuning, the Generator can eventually better discriminate\nuseful documents amongst fabrications. The experimental results verify the\neffectiveness of ATM and we also observe that the Generator can achieve better\nperformance compared to the state-of-the-art baselines.\n", "rewritten_text": "Retrieval-augmented generation (RAG) significantly improves large language models' (LLMs) performance on knowledge-intensive questions, mitigating the problem of hallucinations.  RAG incorporates external knowledge from semantically relevant documents via information retrieval techniques. However, the prevalence of noisy and fabricated content online renders RAG systems vulnerable to inaccuracies.  To address this, we propose optimizing RAG with an Adversarial Tuning Multi-agent system (ATM).  ATM employs an auxiliary Attacker agent to adversarially tune a Generator agent, fostering the Generator's ability to identify reliable documents for question answering.  Through iterative multi-agent training, the Generator learns to discriminate between credible and fabricated information.  Experimental results demonstrate ATM's effectiveness, showing that the Generator outperforms state-of-the-art baselines.\n"}, "2309.00468": {"original_text": "  Dietary assessment is essential to maintaining a healthy lifestyle. Automatic\nimage-based dietary assessment is a growing field of research due to the\nincreasing prevalence of image capturing devices (e.g. mobile phones). In this\nwork, we estimate food energy from a single monocular image, a difficult task\ndue to the limited hard-to-extract amount of energy information present in an\nimage. To do so, we employ an improved encoder-decoder framework for energy\nestimation; the encoder transforms the image into a representation embedded\nwith food energy information in an easier-to-extract format, which the decoder\nthen extracts the energy information from. To implement our method, we compile\na high-quality food image dataset verified by registered dietitians containing\neating scene images, food-item segmentation masks, and ground truth calorie\nvalues. Our method improves upon previous caloric estimation methods by over\n10\\% and 30 kCal in terms of MAPE and MAE respectively.\n", "rewritten_text": "Dietary assessment is crucial for maintaining a healthy lifestyle.  The increasing prevalence of image-capturing devices (e.g., mobile phones) has fueled research into automated, image-based dietary assessment. This work addresses the challenging task of estimating food energy from a single monocular image, where energy information is limited and difficult to extract.  We propose an improved encoder-decoder framework for energy estimation. The encoder transforms the image into a representation where food energy information is more readily accessible; the decoder then extracts this information.  To implement our method, we compiled a high-quality food image dataset, verified by registered dietitians, containing images of eating scenes, food-item segmentation masks, and ground-truth calorie values. Our method achieves over 10% and 30 kcal improvements over previous caloric estimation methods in terms of mean absolute percentage error (MAPE) and mean absolute error (MAE), respectively.\n"}, "2411.08196": {"original_text": "  Diffusion Transformers (DiTs) have recently achieved remarkable success in\ntext-guided image generation. In image editing, DiTs project text and image\ninputs to a joint latent space, from which they decode and synthesize new\nimages. However, it remains largely unexplored how multimodal information\ncollectively forms this joint space and how they guide the semantics of the\nsynthesized images. In this paper, we investigate the latent space of DiT\nmodels and uncover two key properties: First, DiT's latent space is inherently\nsemantically disentangled, where different semantic attributes can be\ncontrolled by specific editing directions. Second, consistent semantic editing\nrequires utilizing the entire joint latent space, as neither encoded image nor\ntext alone contains enough semantic information. We show that these editing\ndirections can be obtained directly from text prompts, enabling precise\nsemantic control without additional training or mask annotations. Based on\nthese insights, we propose a simple yet effective Encode-Identify-Manipulate\n(EIM) framework for zero-shot fine-grained image editing. Specifically, we\nfirst encode both the given source image and the text prompt that describes the\nimage, to obtain the joint latent embedding. Then, using our proposed Hessian\nScore Distillation Sampling (HSDS) method, we identify editing directions that\ncontrol specific target attributes while preserving other image features. These\ndirections are guided by text prompts and used to manipulate the latent\nembeddings. Moreover, we propose a new metric to quantify the disentanglement\ndegree of the latent space of diffusion models. Extensive experiment results on\nour new curated benchmark dataset and analysis demonstrate DiT's\ndisentanglement properties and effectiveness of the EIM framework.\n", "rewritten_text": "Diffusion Transformers (DiTs) have recently achieved remarkable success in text-guided image generation.  In image editing, DiTs project text and image inputs into a shared latent space, from which they decode and synthesize new images. However, the mechanisms by which multimodal information shapes this joint space and guides the semantics of synthesized images remain largely unexplored.  This paper investigates the latent space of DiT models, uncovering two key properties: First, the latent space exhibits inherent semantic disentanglement, allowing control over specific semantic attributes via targeted editing directions. Second, consistent semantic editing necessitates utilizing the entire joint latent space, as neither the encoded image nor the text alone provides sufficient semantic information.  We demonstrate that these editing directions can be derived directly from text prompts, enabling precise semantic control without requiring additional training or mask annotations.\n\nBased on these findings, we propose a simple yet effective Encode-Identify-Manipulate (EIM) framework for zero-shot, fine-grained image editing.  This framework first encodes both the source image and a descriptive text prompt to obtain a joint latent embedding.  Then, using our proposed Hessian Score Distillation Sampling (HSDS) method, we identify editing directions that control specific target attributes while preserving other image features. These directions, guided by text prompts, are used to manipulate the latent embedding.  Furthermore, we introduce a novel metric to quantify the disentanglement degree of diffusion model latent spaces.  Extensive experimental results on a new curated benchmark dataset demonstrate both DiT's disentanglement properties and the effectiveness of the EIM framework.\n"}, "2403.06444": {"original_text": "  Estimating reliable geometric model parameters from the data with severe\noutliers is a fundamental and important task in computer vision. This paper\nattempts to sample high-quality subsets and select model instances to estimate\nparameters in the multi-structural data. To address this, we propose an\neffective method called Latent Semantic Consensus (LSC). The principle of LSC\nis to preserve the latent semantic consensus in both data points and model\nhypotheses. Specifically, LSC formulates the model fitting problem into two\nlatent semantic spaces based on data points and model hypotheses, respectively.\nThen, LSC explores the distributions of points in the two latent semantic\nspaces, to remove outliers, generate high-quality model hypotheses, and\neffectively estimate model instances. Finally, LSC is able to provide\nconsistent and reliable solutions within only a few milliseconds for general\nmulti-structural model fitting, due to its deterministic fitting nature and\nefficiency. Compared with several state-of-the-art model fitting methods, our\nLSC achieves significant superiority for the performance of both accuracy and\nspeed on synthetic data and real images. The code will be available at\nhttps://github.com/guobaoxiao/LSC.\n", "rewritten_text": "Robustly estimating geometric model parameters from data containing severe outliers is a fundamental challenge in computer vision.  This paper presents Latent Semantic Consensus (LSC), an effective method for estimating parameters in multi-structural data by sampling high-quality subsets and selecting optimal model instances.  LSC preserves latent semantic consensus in both data points and model hypotheses.  Specifically, it formulates model fitting as a problem in two latent semantic spaces, one for data points and one for model hypotheses.  By analyzing the distributions within these spaces, LSC effectively removes outliers, generates high-quality model hypotheses, and estimates model parameters.  Its deterministic nature and efficiency enable consistent and reliable solutions within milliseconds for general multi-structural model fitting.  Compared to state-of-the-art methods, LSC demonstrates significantly superior accuracy and speed on both synthetic data and real images. The code is available at https://github.com/guobaoxiao/LSC.\n"}, "1708.0923": {"original_text": "  Named Entity Recognition and Disambiguation (NERD) systems have recently been\nwidely researched to deal with the significant growth of the Web. NERD systems\nare crucial for several Natural Language Processing (NLP) tasks such as\nsummarization, understanding, and machine translation. However, there is no\nstandard interface specification, i.e. these systems may vary significantly\neither for exporting their outputs or for processing the inputs. Thus, when a\ngiven company desires to implement more than one NERD system, the process is\nquite exhaustive and prone to failure. In addition, industrial solutions demand\ncritical requirements, e.g., large-scale processing, completeness, versatility,\nand licenses. Commonly, these requirements impose a limitation, making good\nNERD models to be ignored by companies. This paper presents TANKER, a\ndistributed architecture which aims to overcome scalability, reliability and\nfailure tolerance limitations related to industrial needs by combining NERD\nsystems. To this end, TANKER relies on a micro-services oriented architecture,\nwhich enables agile development and delivery of complex enterprise\napplications. In addition, TANKER provides a standardized API which makes\npossible to combine several NERD systems at once.\n", "rewritten_text": "Named Entity Recognition and Disambiguation (NERD) systems have become increasingly important with the growth of the web.  Crucial for numerous Natural Language Processing (NLP) tasks\u2014including summarization, comprehension, and machine translation\u2014NERD systems lack a standardized interface.  This variability in input and output processing makes integrating multiple NERD systems a complex and error-prone undertaking for companies.  Furthermore, industrial applications demand scalability, completeness, versatility, and licensing considerations, often leading to the rejection of otherwise effective NERD models.\n\nThis paper introduces TANKER, a distributed architecture designed to address the scalability, reliability, and fault tolerance challenges inherent in industrial NERD system deployment.  TANKER leverages a microservices architecture to facilitate agile development and deployment of complex enterprise applications.  Moreover, it provides a standardized API, enabling the seamless integration of multiple NERD systems.\n"}, "1708.01846": {"original_text": "  Low-rank decomposition (LRD) is a state-of-the-art method for visual data\nreconstruction and modelling. However, it is a very challenging problem when\nthe image data contains significant occlusion, noise, illumination variation,\nand misalignment from rotation or viewpoint changes. We leverage the specific\nstructure of data in order to improve the performance of LRD when the data are\nnot ideal. To this end, we propose a new framework that embeds manifold priors\ninto LRD. To implement the framework, we design an alternating direction method\nof multipliers (ADMM) method which efficiently integrates the manifold\nconstraints during the optimization process. The proposed approach is\nsuccessfully used to calculate low-rank models from face images, hand-written\ndigits and planar surface images. The results show a consistent increase of\nperformance when compared to the state-of-the-art over a wide range of\nrealistic image misalignments and corruptions.\n", "rewritten_text": "Low-rank decomposition (LRD) is a state-of-the-art method for visual data reconstruction and modeling.  However, its performance significantly degrades when image data suffer from substantial occlusion, noise, illumination variations, and misalignments due to rotation or viewpoint changes.  To address this, we propose a novel framework that incorporates manifold priors into LRD, thereby improving its robustness to non-ideal data.  We implement this framework using an alternating direction method of multipliers (ADMM) algorithm, which efficiently integrates manifold constraints into the optimization process.  The proposed approach demonstrates improved performance on face images, handwritten digits, and planar surface images, consistently outperforming state-of-the-art methods across a wide range of realistic image misalignments and corruptions.\n"}, "2001.0732": {"original_text": "  It's natural these days for people to know the local events from massive\ndocuments. Many texts contain location information, such as city name or road\nname, which is always incomplete or latent. It's significant to extract the\nadministrative area of the text and organize the hierarchy of area, called\nlocation normalization. Existing detecting location systems either exclude\nhierarchical normalization or present only a few specific regions. We propose a\nsystem named ROIBase that normalizes the text by the Chinese hierarchical\nadministrative divisions. ROIBase adopts a co-occurrence constraint as the\nbasic framework to score the hit of the administrative area, achieves the\ninference by special embeddings, and expands the recall by the ROI (region of\ninterest). It has high efficiency and interpretability because it mainly\nestablishes on the definite knowledge and has less complex logic than the\nsupervised models. We demonstrate that ROIBase achieves better performance\nagainst feasible solutions and is useful as a strong support system for\nlocation normalization.\n", "rewritten_text": "In today's digital landscape, individuals commonly access local event information from extensive textual sources.  However, many texts contain incomplete or implicit location information, such as city or road names.  Extracting and hierarchically organizing this information\u2014a process known as location normalization\u2014is crucial.  Existing location detection systems either lack hierarchical normalization capabilities or support only limited geographical regions.  This paper proposes ROIBase, a system that normalizes text based on the hierarchical administrative divisions of China.  ROIBase employs a co-occurrence constraint framework to score administrative area matches, leverages specialized embeddings for inference, and enhances recall using a region-of-interest (ROI) approach.  Its high efficiency and interpretability stem from its reliance on explicit knowledge and simpler logic compared to supervised models.  We demonstrate that ROIBase outperforms existing solutions and serves as a robust support system for location normalization.\n"}, "1908.02505": {"original_text": "  This paper presents a scoring system that has shown the top result on the\ntext subset of CALL v3 shared task. The presented system is based on text\nembeddings, namely NNLM~\\cite{nnlm} and BERT~\\cite{Bert}. The distinguishing\nfeature of the given approach is that it does not rely on the reference grammar\nfile for scoring. The model is compared against approaches that use the grammar\nfile and proves the possibility to achieve similar and even higher results\nwithout a predefined set of correct answers.\n  The paper describes the model itself and the data preparation process that\nplayed a crucial role in the model training.\n", "rewritten_text": "This paper presents a scoring system that achieved state-of-the-art results on the text subset of the CALL v3 shared task.  The system leverages text embeddings, specifically NNLM [1] and BERT [2].  Unlike competing approaches, it does not rely on a reference grammar file for scoring.  Our results demonstrate that comparable, and even superior, performance can be achieved without a predefined set of correct answers.  The paper details the model architecture and the crucial data preparation process employed during training.\n"}, "1604.0801": {"original_text": "  The purpose of this paper is the detection of salient areas in natural video\nby using the new deep learning techniques. Salient patches in video frames are\npredicted first. Then the predicted visual fixation maps are built upon them.\nWe design the deep architecture on the basis of CaffeNet implemented with Caffe\ntoolkit. We show that changing the way of data selection for optimisation of\nnetwork parameters, we can save computation cost up to 12 times. We extend deep\nlearning approaches for saliency prediction in still images with RGB values to\nspecificity of video using the sensitivity of the human visual system to\nresidual motion. Furthermore, we complete primary colour pixel values by\ncontrast features proposed in classical visual attention prediction models. The\nexperiments are conducted on two publicly available datasets. The first is\nIRCCYN video database containing 31 videos with an overall amount of 7300\nframes and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided\n2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, the\naccuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction of\nsaliency of patches show the improvement up to 2% with regard to RGB use only.\nThe resulting accuracy of 76, 6% is obtained. The AUC metric in comparison of\npredicted saliency maps with visual fixation maps shows the increase up to 16%\non a sample of video clips from this dataset.\n", "rewritten_text": "This paper presents a novel deep learning approach for detecting salient areas in natural videos.  We first predict salient patches within individual video frames, subsequently constructing visual fixation maps based on these predictions.  Our deep architecture, based on CaffeNet and implemented using the Caffe toolkit, achieves significant computational efficiency. By optimizing network parameter selection, we reduce computational cost by up to a factor of 12.  We extend existing deep learning methods for still image saliency prediction (using RGB values) to video by incorporating the human visual system's sensitivity to residual motion.  Furthermore, we augment RGB pixel values with contrast features, as proposed in classical visual attention models.\n\nExperiments were conducted on two publicly available datasets: the IRCCYN video database (31 videos, 7300 frames, 37 subjects' eye fixations) and the HOLLYWOOD2 dataset (2517 movie clips, 19 subjects' eye fixations).  On the IRCCYN dataset, we achieved an accuracy of 89.51%.  On the HOLLYWOOD2 dataset, patch saliency prediction improved by up to 2% compared to using RGB values alone, resulting in an accuracy of 76.6%.  The area under the curve (AUC) metric, comparing predicted saliency maps with visual fixation maps, showed an improvement of up to 16% on a subset of HOLLYWOOD2 video clips.\n"}, "2301.03182": {"original_text": "  Existing deep learning-based shadow removal methods still produce images with\nshadow remnants. These shadow remnants typically exist in homogeneous regions\nwith low-intensity values, making them untraceable in the existing\nimage-to-image mapping paradigm. We observe that shadows mainly degrade images\nat the image-structure level (in which humans perceive object shapes and\ncontinuous colors). Hence, in this paper, we propose to remove shadows at the\nimage structure level. Based on this idea, we propose a novel\nstructure-informed shadow removal network (StructNet) to leverage the\nimage-structure information to address the shadow remnant problem.\nSpecifically, StructNet first reconstructs the structure information of the\ninput image without shadows and then uses the restored shadow-free structure\nprior to guiding the image-level shadow removal. StructNet contains two main\nnovel modules: (1) a mask-guided shadow-free extraction (MSFE) module to\nextract image structural features in a non-shadow-to-shadow directional manner,\nand (2) a multi-scale feature & residual aggregation (MFRA) module to leverage\nthe shadow-free structure information to regularize feature consistency. In\naddition, we also propose to extend StructNet to exploit multi-level structure\ninformation (MStructNet), to further boost the shadow removal performance with\nminimum computational overheads. Extensive experiments on three shadow removal\nbenchmarks demonstrate that our method outperforms existing shadow removal\nmethods, and our StructNet can be integrated with existing methods to improve\nthem further.\n", "rewritten_text": "Existing deep learning-based shadow removal methods often leave residual artifacts.  These remnants typically appear in low-intensity, homogeneous regions, hindering their removal within the standard image-to-image translation framework.  Shadows primarily degrade image quality at the structural level, affecting human perception of object shapes and color continuity.  Therefore, this paper proposes a novel structure-informed shadow removal network, StructNet, that operates at this structural level.\n\nStructNet first reconstructs the shadow-free image structure and then uses this prior to guide shadow removal at the image level.  The network incorporates two key modules: (1) a mask-guided shadow-free extraction (MSFE) module, which extracts structural features directionally, avoiding shadow propagation; and (2) a multi-scale feature & residual aggregation (MFRA) module, which leverages the shadow-free structure to enforce feature consistency.  Furthermore, we extend StructNet to MStructNet, which exploits multi-level structural information to enhance performance with minimal computational overhead.\n\nExtensive experiments on three benchmark datasets demonstrate that our method surpasses existing state-of-the-art shadow removal techniques.  Moreover, StructNet can be integrated with other methods to further improve their performance.\n"}, "1709.0586": {"original_text": "  We present a novel method for cell segmentation in microscopy images which is\ninspired by the Generative Adversarial Neural Network (GAN) approach. Our\nframework is built on a pair of two competitive artificial neural networks,\nwith a unique architecture, termed Rib Cage, which are trained simultaneously\nand together define a min-max game resulting in an accurate segmentation of a\ngiven image. Our approach has two main strengths, similar to the GAN, the\nmethod does not require a formulation of a loss function for the optimization\nprocess. This allows training on a limited amount of annotated data in a weakly\nsupervised manner. Promising segmentation results on real fluorescent\nmicroscopy data are presented. The code is freely available at:\nhttps://github.com/arbellea/DeepCellSeg.git\n", "rewritten_text": "We present a novel cell segmentation method for microscopy images, inspired by generative adversarial neural networks (GANs).  Our framework employs a unique architecture, termed \"Rib Cage,\" comprising two competing neural networks trained simultaneously in a min-max game to achieve accurate image segmentation.  Like GANs, our approach obviates the need for an explicit loss function, enabling weakly supervised training with limited annotated data.  We demonstrate promising segmentation results on real fluorescent microscopy data. The code is freely available at: https://github.com/arbellea/DeepCellSeg.git\n"}, "2203.14565": {"original_text": "  Domain adaptation (DA) or domain generalization (DG) for face presentation\nattack detection (PAD) has attracted attention recently with its robustness\nagainst unseen attack scenarios. Existing DA/DG-based PAD methods, however,\nhave not yet fully explored the domain-specific style information that can\nprovide knowledge regarding attack styles (e.g., materials, background,\nillumination and resolution). In this paper, we introduce a novel Style-Guided\nDomain Adaptation (SGDA) framework for inference-time adaptive PAD.\nSpecifically, Style-Selective Normalization (SSN) is proposed to explore the\ndomain-specific style information within the high-order feature statistics. The\nproposed SSN enables the adaptation of the model to the target domain by\nreducing the style difference between the target and the source domains.\nMoreover, we carefully design Style-Aware Meta-Learning (SAML) to boost the\nadaptation ability, which simulates the inference-time adaptation with style\nselection process on virtual test domain. In contrast to previous domain\nadaptation approaches, our method does not require either additional auxiliary\nmodels (e.g., domain adaptors) or the unlabeled target domain during training,\nwhich makes our method more practical to PAD task. To verify our experiments,\nwe utilize the public datasets: MSU-MFSD, CASIA-FASD, OULU-NPU and Idiap\nREPLAYATTACK. In most assessments, the result demonstrates a notable gap of\nperformance compared to the conventional DA/DG-based PAD methods.\n", "rewritten_text": "Recent interest in domain adaptation (DA) and domain generalization (DG) for face presentation attack detection (PAD) stems from their robustness against unseen attack scenarios.  However, existing DA/DG-based PAD methods have not fully exploited domain-specific style information (e.g., materials, background, illumination, and resolution) to understand attack styles. This paper introduces a novel Style-Guided Domain Adaptation (SGDA) framework for inference-time adaptive PAD.  Specifically, we propose Style-Selective Normalization (SSN) to leverage domain-specific style information within high-order feature statistics, enabling model adaptation to the target domain by reducing style discrepancies between source and target domains.  Furthermore, we introduce Style-Aware Meta-Learning (SAML) to enhance adaptation capabilities by simulating inference-time adaptation with style selection on a virtual test domain. Unlike previous DA approaches, our method requires neither auxiliary models (e.g., domain adaptors) nor unlabeled target domain data during training, enhancing its practicality for PAD.  We evaluate our method using the public datasets MSU-MFSD, CASIA-FASD, OULU-NPU, and Idiap REPLAY-ATTACK.  Results demonstrate significant performance improvements over conventional DA/DG-based PAD methods in most evaluations.\n"}, "1905.08511": {"original_text": "  Question answering (QA) using textual sources for purposes such as reading\ncomprehension (RC) has attracted much attention. This study focuses on the task\nof explainable multi-hop QA, which requires the system to return the answer\nwith evidence sentences by reasoning and gathering disjoint pieces of the\nreference texts. It proposes the Query Focused Extractor (QFE) model for\nevidence extraction and uses multi-task learning with the QA model. QFE is\ninspired by extractive summarization models; compared with the existing method,\nwhich extracts each evidence sentence independently, it sequentially extracts\nevidence sentences by using an RNN with an attention mechanism on the question\nsentence. It enables QFE to consider the dependency among the evidence\nsentences and cover important information in the question sentence.\nExperimental results show that QFE with a simple RC baseline model achieves a\nstate-of-the-art evidence extraction score on HotpotQA. Although designed for\nRC, it also achieves a state-of-the-art evidence extraction score on FEVER,\nwhich is a recognizing textual entailment task on a large textual database.\n", "rewritten_text": "Explainable multi-hop question answering (QA) from textual sources, crucial for tasks like reading comprehension (RC), has garnered significant attention.  This study introduces the Query Focused Extractor (QFE) model for evidence extraction in multi-hop QA, a task requiring the system to reason over and synthesize disparate pieces of reference text to produce an answer with supporting evidence sentences.  QFE leverages multi-task learning with a QA model and draws inspiration from extractive summarization techniques. Unlike existing methods that extract evidence sentences independently, QFE employs a recurrent neural network (RNN) with an attention mechanism focused on the question, enabling sequential extraction and consideration of inter-sentence dependencies while ensuring crucial question information is incorporated.  Experimental results demonstrate that QFE, even with a simple RC baseline QA model, achieves state-of-the-art evidence extraction performance on the HotpotQA benchmark.  Remarkably, despite being designed for RC, QFE also attains state-of-the-art results on FEVER, a fact verification task involving textual entailment on a large-scale knowledge base.\n"}, "2310.18205": {"original_text": "  Claim span identification (CSI) is an important step in fact-checking\npipelines, aiming to identify text segments that contain a checkworthy claim or\nassertion in a social media post. Despite its importance to journalists and\nhuman fact-checkers, it remains a severely understudied problem, and the scarce\nresearch on this topic so far has only focused on English. Here we aim to\nbridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K\nreal-world claims collected from numerous social media platforms in five Indian\nlanguages and English. We report strong baselines with state-of-the-art\nencoder-only language models (e.g., XLM-R) and we demonstrate the benefits of\ntraining on multiple languages over alternative cross-lingual transfer methods\nsuch as zero-shot transfer, or training on translated data, from a\nhigh-resource language such as English. We evaluate generative large language\nmodels from the GPT series using prompting methods on the X-CLAIM dataset and\nwe find that they underperform the smaller encoder-only language models for\nlow-resource languages.\n", "rewritten_text": "Claim span identification (CSI) is a crucial step in fact-checking pipelines, identifying text segments containing verifiable claims or assertions within social media posts.  Despite its importance to journalists and human fact-checkers, CSI remains significantly understudied, with existing research largely confined to English.  This work addresses this gap by introducing X-CLAIM, a novel dataset comprising 7,000 real-world claims gathered from multiple social media platforms across five Indian languages and English.  We establish strong baselines using state-of-the-art encoder-only language models (e.g., XLM-R), demonstrating the superiority of multilingual training over alternative cross-lingual transfer methods such as zero-shot transfer or training on data translated from a high-resource language like English.  Evaluation of generative large language models (GPT series) using prompting techniques on X-CLAIM reveals their underperformance compared to smaller encoder-only models, particularly for low-resource languages.\n"}, "2306.08075": {"original_text": "  Current knowledge distillation approaches in semantic segmentation tend to\nadopt a holistic approach that treats all spatial locations equally. However,\nfor dense prediction, students' predictions on edge regions are highly\nuncertain due to contextual information leakage, requiring higher spatial\nsensitivity knowledge than the body regions. To address this challenge, this\npaper proposes a novel approach called boundary-privileged knowledge\ndistillation (BPKD). BPKD distills the knowledge of the teacher model's body\nand edges separately to the compact student model. Specifically, we employ two\ndistinct loss functions: (i) edge loss, which aims to distinguish between\nambiguous classes at the pixel level in edge regions; (ii) body loss, which\nutilizes shape constraints and selectively attends to the inner-semantic\nregions. Our experiments demonstrate that the proposed BPKD method provides\nextensive refinements and aggregation for edge and body regions. Additionally,\nthe method achieves state-of-the-art distillation performance for semantic\nsegmentation on three popular benchmark datasets, highlighting its\neffectiveness and generalization ability. BPKD shows consistent improvements\nacross a diverse array of lightweight segmentation structures, including both\nCNNs and transformers, underscoring its architecture-agnostic adaptability. The\ncode is available at \\url{https://github.com/AkideLiu/BPKD}.\n", "rewritten_text": "Existing knowledge distillation methods for semantic segmentation typically treat all spatial locations uniformly.  However,  student models exhibit higher uncertainty in predicting edge regions due to contextual information leakage, demanding more spatially sensitive knowledge than for interior regions.  To address this, we propose Boundary-Privileged Knowledge Distillation (BPKD), a novel approach that distills teacher knowledge separately for edge and body regions into a compact student model.  BPKD employs two distinct loss functions: (i) an edge loss, focusing on disambiguating classes at the pixel level within edge regions; and (ii) a body loss, incorporating shape constraints and selectively attending to semantically consistent interior regions.  Our experiments demonstrate that BPKD significantly refines predictions in both edge and body regions, achieving state-of-the-art distillation performance on three widely used benchmark datasets.  This highlights its effectiveness and generalizability across diverse lightweight segmentation architectures, including both convolutional neural networks (CNNs) and transformers.  The code is available at [https://github.com/AkideLiu/BPKD](https://github.com/AkideLiu/BPKD).\n"}, "2010.05762": {"original_text": "  Scene flow represents the 3D motion of every point in the dynamic\nenvironments. Like the optical flow that represents the motion of pixels in 2D\nimages, 3D motion representation of scene flow benefits many applications, such\nas autonomous driving and service robot. This paper studies the problem of\nscene flow estimation from two consecutive 3D point clouds. In this paper, a\nnovel hierarchical neural network with double attention is proposed for\nlearning the correlation of point features in adjacent frames and refining\nscene flow from coarse to fine layer by layer. The proposed network has a new\nmore-for-less hierarchical architecture. The more-for-less means that the\nnumber of input points is greater than the number of output points for scene\nflow estimation, which brings more input information and balances the precision\nand resource consumption. In this hierarchical architecture, scene flow of\ndifferent levels is generated and supervised respectively. A novel attentive\nembedding module is introduced to aggregate the features of adjacent points\nusing a double attention method in a patch-to-patch manner. The proper layers\nfor flow embedding and flow supervision are carefully considered in our network\ndesignment. Experiments show that the proposed network outperforms the\nstate-of-the-art performance of 3D scene flow estimation on the FlyingThings3D\nand KITTI Scene Flow 2015 datasets. We also apply the proposed network to\nrealistic LiDAR odometry task, which is an key problem in autonomous driving.\nThe experiment results demonstrate that our proposed network can outperform the\nICP-based method and shows the good practical application ability.\n", "rewritten_text": "Scene flow represents the 3D motion of every point in dynamic environments.  Analogous to optical flow, which represents 2D pixel motion in images, 3D scene flow benefits numerous applications, including autonomous driving and service robotics. This paper addresses the problem of estimating scene flow from two consecutive 3D point clouds.  We propose a novel hierarchical neural network with double attention to learn the correlation of point features in adjacent frames and refine scene flow iteratively from coarse to fine.  This network employs a novel \"more-for-less\" hierarchical architecture, where the number of input points exceeds the number of output points, enhancing input information while balancing precision and computational resources.  The architecture generates and supervises scene flow at different hierarchical levels.  A novel attentive embedding module aggregates features of adjacent points using a patch-to-patch double attention mechanism.  Careful consideration was given to the placement of flow embedding and supervision layers. Experiments demonstrate that our network surpasses state-of-the-art 3D scene flow estimation performance on the FlyingThings3D and KITTI Scene Flow 2015 datasets.  Furthermore, application to the crucial autonomous driving task of LiDAR odometry shows that our network outperforms ICP-based methods, highlighting its practical applicability.\n"}, "2303.14829": {"original_text": "  Generating grammatically and semantically correct captions in video\ncaptioning is a challenging task. The captions generated from the existing\nmethods are either word-by-word that do not align with grammatical structure or\nmiss key information from the input videos. To address these issues, we\nintroduce a novel global-local fusion network, with a Global-Local Fusion Block\n(GLFB) that encodes and fuses features from different parts of speech (POS)\ncomponents with visual-spatial features. We use novel combinations of different\nPOS components - 'determinant + subject', 'auxiliary verb', 'verb', and\n'determinant + object' for supervision of the POS blocks - Det + Subject, Aux\nVerb, Verb, and Det + Object respectively. The novel global-local fusion\nnetwork together with POS blocks helps align the visual features with language\ndescription to generate grammatically and semantically correct captions.\nExtensive qualitative and quantitative experiments on benchmark MSVD and MSRVTT\ndatasets demonstrate that the proposed approach generates more grammatically\nand semantically correct captions compared to the existing methods, achieving\nthe new state-of-the-art. Ablations on the POS blocks and the GLFB demonstrate\nthe impact of the contributions on the proposed method.\n", "rewritten_text": "Generating grammatically and semantically correct video captions is a challenging task. Existing methods often produce captions that are either verbatim transcriptions lacking grammatical structure or omit crucial information.  To address this, we propose a novel global-local fusion network incorporating a Global-Local Fusion Block (GLFB).  This block encodes and fuses features from different parts-of-speech (POS) components with visual-spatial features.  We leverage novel POS component combinations\u2014'determiner + subject', 'auxiliary verb', 'verb', and 'determiner + object'\u2014to supervise the corresponding POS blocks (Det + Subject, Aux Verb, Verb, Det + Object).  This global-local fusion network, coupled with the POS blocks, effectively aligns visual features with linguistic descriptions, generating grammatically and semantically correct captions.  Extensive qualitative and quantitative experiments on the benchmark MSVD and MSRVTT datasets demonstrate that our approach surpasses existing methods, achieving state-of-the-art performance.  Ablation studies on the POS blocks and the GLFB confirm the significance of our contributions.\n"}, "1805.11788": {"original_text": "  Traditional Convolutional Neural Networks (CNNs) typically use the same\nactivation function (usually ReLU) for all neurons with non-linear mapping\noperations. For example, the deep convolutional architecture Inception-v4 uses\nReLU. To improve the classification performance of traditional CNNs, a new\n\"Multi-function Convolutional Neural Network\" (MCNN) is created by using\ndifferent activation functions for different neurons. For $n$ neurons and $m$\ndifferent activation functions, there are a total of $m^n-m$ MCNNs and only $m$\ntraditional CNNs. Therefore, the best model is very likely to be chosen from\nMCNNs because there are $m^n-2m$ more MCNNs than traditional CNNs. For\nperformance analysis, two different datasets for two applications (classifying\nhandwritten digits from the MNIST database and classifying brain MRI images\ninto one of the four stages of Alzheimer's disease (AD)) are used. For both\napplications, an activation function is randomly selected for each layer of a\nMCNN. For the AD diagnosis application, MCNNs using a newly created\nmulti-function Inception-v4 architecture are constructed. Overall, simulations\nshow that MCNNs can outperform traditional CNNs in terms of multi-class\nclassification accuracy for both applications. An important future research\nwork will be to efficiently select the best MCNN from $m^n-m$ candidate MCNNs.\nCurrent CNN software only provides users with partial functionality of MCNNs\nsince different layers can use different activation functions but not\nindividual neurons in the same layer. Thus, modifying current CNN software\nsystems such as ResNets, DenseNets, and Dual Path Networks by using multiple\nactivation functions and developing more effective and faster MCNN software\nsystems and tools would be very useful to solve difficult practical image\nclassification problems.\n", "rewritten_text": "Traditional convolutional neural networks (CNNs) typically employ the same activation function (usually ReLU) for all neurons.  For instance, Inception-v4 uses ReLU. To enhance classification performance, we propose a Multi-function Convolutional Neural Network (MCNN) that utilizes different activation functions for different neurons.  With *n* neurons and *m* activation functions, there are *m<sup>n</sup> - m* possible MCNNs, compared to only *m* traditional CNNs.  This significantly larger search space suggests that superior models are more likely to be found among the MCNNs (*m<sup>n</sup> - 2m* more than traditional CNNs).\n\nWe evaluated MCNN performance on two datasets: the MNIST handwritten digit database and a brain MRI dataset for Alzheimer's disease (AD) staging (four stages).  For both applications, we randomly assigned activation functions to each layer of the MCNN.  For AD diagnosis, we constructed MCNNs using a novel multi-function Inception-v4 architecture.  Our simulations demonstrate that MCNNs consistently outperform traditional CNNs in multi-class classification accuracy for both applications.\n\nFuture research will focus on efficiently selecting the optimal MCNN from the *m<sup>n</sup> - m* candidates.  Current CNN software offers only limited MCNN functionality, allowing different activation functions across layers but not within a single layer.  Therefore, modifying existing CNN software frameworks (e.g., ResNets, DenseNets, Dual Path Networks) to support multiple activation functions per layer and developing more efficient MCNN software tools would significantly benefit practical image classification tasks.\n"}, "2108.024": {"original_text": "  Gait data captured by inertial sensors have demonstrated promising results on\nuser authentication. However, most existing approaches stored the enrolled gait\npattern insecurely for matching with the validating pattern, thus, posed\ncritical security and privacy issues. In this study, we present a gait\ncryptosystem that generates from gait data the random key for user\nauthentication, meanwhile, secures the gait pattern. First, we propose a\nrevocable and random binary string extraction method using a deep neural\nnetwork followed by feature-wise binarization. A novel loss function for\nnetwork optimization is also designed, to tackle not only the intrauser\nstability but also the inter-user randomness. Second, we propose a new\nbiometric key generation scheme, namely Irreversible Error Correct and\nObfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme,\nto securely generate from the binary string the random and irreversible key.\nThe model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. We\nshowed that our model could generate the key of 139 bits from 5-second data\nsequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR)\nsmaller than 5.441%. In addition, the security and user privacy analyses showed\nthat our model was secure against existing attacks on biometric template\nprotection, and fulfilled irreversibility and unlinkability.\n", "rewritten_text": "Inertial sensor-based gait data has shown promise for user authentication.  However, most existing approaches store enrolled gait patterns insecurely, raising significant security and privacy concerns. This study presents a gait cryptosystem that generates a random authentication key directly from gait data while simultaneously securing the gait pattern itself.  First, we propose a novel method for extracting revocable, random binary strings using a deep neural network followed by feature-wise binarization.  A new loss function is designed to optimize the network, addressing both intra-user stability and inter-user randomness. Second, we introduce a novel biometric key generation scheme, Irreversible Error Correct and Obfuscate (IECO), an improvement on the Error Correct and Obfuscate (ECO) scheme, to securely generate irreversible, random keys from the binary string.  The model was evaluated using two benchmark datasets, OU-ISIR and whuGAIT.  Results demonstrate that our model generates 139-bit keys from 5-second gait sequences, achieving a zero False Acceptance Rate (FAR) and a False Rejection Rate (FRR) below 5.441%.  Furthermore, security and privacy analyses demonstrate resistance to existing attacks on biometric template protection, satisfying irreversibility and unlinkability requirements.\n"}, "2404.03443": {"original_text": "  The goal of occluded person re-identification (ReID) is to retrieve specific\npedestrians in occluded situations. However, occluded person ReID still suffers\nfrom background clutter and low-quality local feature representations, which\nlimits model performance. In our research, we introduce a new framework called\nPAB-ReID, which is a novel ReID model incorporating part-attention mechanisms\nto tackle the aforementioned issues effectively. Firstly, we introduce the\nhuman parsing label to guide the generation of more accurate human part\nattention maps. In addition, we propose a fine-grained feature focuser for\ngenerating fine-grained human local feature representations while suppressing\nbackground interference. Moreover, We also design a part triplet loss to\nsupervise the learning of human local features, which optimizes\nintra/inter-class distance. We conducted extensive experiments on specialized\nocclusion and regular ReID datasets, showcasing that our approach outperforms\nthe existing state-of-the-art methods.\n", "rewritten_text": "Occluded person re-identification (ReID) aims to retrieve specific pedestrians despite occlusions.  However, existing methods struggle with background clutter and low-quality local feature representations, hindering performance.  This research introduces PAB-ReID, a novel ReID model employing part-attention mechanisms to address these limitations.  Specifically, we leverage human parsing labels to generate more accurate human part attention maps.  Furthermore, a fine-grained feature focuser generates high-quality local features while suppressing background interference.  Finally, a part triplet loss optimizes intra- and inter-class distances by supervising the learning of human local features.  Extensive experiments on both occlusion-specific and standard ReID datasets demonstrate that PAB-ReID surpasses state-of-the-art methods.\n"}, "2310.05620": {"original_text": "  General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic.\n", "rewritten_text": "Existing evaluations of large language models (LLMs) in LegalAI, primarily conducted by computer scientists, lack alignment with legal practice, hindering accurate assessment of their practical capabilities.  While general and legal-domain LLMs have shown promise in various LegalAI tasks, their performance remains inconsistently measured.  To address this, we introduce LAiW, the first Chinese legal LLM benchmark grounded in the logic of legal practice.  LAiW employs a three-tiered evaluation framework, reflecting the increasing complexity of legal reasoning: basic information retrieval, legal foundation inference, and complex legal application.  Each tier comprises multiple tasks for comprehensive assessment.  Our automated evaluation of current LLMs reveals a surprising discrepancy:  while some models exhibit seemingly advanced capabilities in complex legal application, their performance on fundamental tasks is often deficient. This suggests potential obstacles to practical application and acceptance by legal professionals.  Further human evaluation by legal experts confirms these findings, indicating that despite strong performance in certain areas, LLMs require significant improvement in their adherence to legal logic.\n"}, "2408.12677": {"original_text": "  Traditional volumetric fusion algorithms preserve the spatial structure of 3D\nscenes, which is beneficial for many tasks in computer vision and robotics.\nHowever, they often lack realism in terms of visualization. Emerging 3D\nGaussian splatting bridges this gap, but existing Gaussian-based reconstruction\nmethods often suffer from artifacts and inconsistencies with the underlying 3D\nstructure, and struggle with real-time optimization, unable to provide users\nwith immediate feedback in high quality. One of the bottlenecks arises from the\nmassive amount of Gaussian parameters that need to be updated during\noptimization. Instead of using 3D Gaussian as a standalone map representation,\nwe incorporate it into a volumetric mapping system to take advantage of\ngeometric information and propose to use a quadtree data structure on images to\ndrastically reduce the number of splats initialized. In this way, we\nsimultaneously generate a compact 3D Gaussian map with fewer artifacts and a\nvolumetric map on the fly. Our method, GSFusion, significantly enhances\ncomputational efficiency without sacrificing rendering quality, as demonstrated\non both synthetic and real datasets. Code will be available at\nhttps://github.com/goldoak/GSFusion.\n", "rewritten_text": "Traditional volumetric fusion algorithms, while preserving the spatial structure of 3D scenes\u2014a benefit for many computer vision and robotics tasks\u2014often lack visual realism.  Although 3D Gaussian splatting offers improved realism, existing Gaussian-based reconstruction methods frequently suffer from artifacts, inconsistencies with the underlying 3D structure, and real-time optimization limitations, hindering immediate high-quality user feedback.  A major bottleneck stems from the large number of Gaussian parameters requiring updates during optimization.  To address this, we propose a novel volumetric mapping system that integrates 3D Gaussian splatting with a quadtree data structure applied to input images. This drastically reduces the number of initialized splats, enabling the simultaneous generation of a compact, artifact-reduced 3D Gaussian map and a volumetric map.  Our method, GSFusion, significantly improves computational efficiency without compromising rendering quality, as demonstrated on both synthetic and real datasets.  The code is available at https://github.com/goldoak/GSFusion.\n"}, "2309.14962": {"original_text": "  All tables can be represented as grids. Based on this observation, we propose\nGridFormer, a novel approach for interpreting unconstrained table structures by\npredicting the vertex and edge of a grid. First, we propose a flexible table\nrepresentation in the form of an MXN grid. In this representation, the vertexes\nand edges of the grid store the localization and adjacency information of the\ntable. Then, we introduce a DETR-style table structure recognizer to\nefficiently predict this multi-objective information of the grid in a single\nshot. Specifically, given a set of learned row and column queries, the\nrecognizer directly outputs the vertexes and edges information of the\ncorresponding rows and columns. Extensive experiments on five challenging\nbenchmarks which include wired, wireless, multi-merge-cell, oriented, and\ndistorted tables demonstrate the competitive performance of our model over\nother methods.\n", "rewritten_text": "All tables can be represented as grids.  Building on this observation, we propose GridFormer, a novel approach for interpreting unconstrained table structures by predicting grid vertices and edges.  We introduce a flexible table representation as an M\u00d7N grid, where vertices and edges encode the table's localization and adjacency information.  A DETR-style table structure recognizer then efficiently predicts this multi-objective grid information in a single pass.  Specifically, given learned row and column queries, the recognizer directly outputs the vertex and edge information for the corresponding rows and columns.  Extensive experiments on five challenging benchmarks\u2014including wired, wireless, multi-merged-cell, oriented, and distorted tables\u2014demonstrate GridFormer's competitive performance against other methods.\n"}, "2303.17658": {"original_text": "  Machine learning models deployed in the open world may encounter observations\nthat they were not trained to recognize, and they risk misclassifying such\nobservations with high confidence. Therefore, it is essential that these models\nare able to ascertain what is in-distribution (ID) and out-of-distribution\n(OOD), to avoid this misclassification. In recent years, huge strides have been\nmade in creating models that are robust to this distinction. As a result, the\ncurrent state-of-the-art has reached near perfect performance on relatively\ncoarse-grained OOD detection tasks, such as distinguishing horses from trucks,\nwhile struggling with finer-grained classification, like differentiating models\nof commercial aircraft. In this paper, we describe a new theoretical framework\nfor understanding fine- and coarse-grained OOD detection, we re-conceptualize\nfine grained classification into a three part problem, and we propose a new\nbaseline task for OOD models on two fine-grained hierarchical data sets, two\nnew evaluation methods to differentiate fine- and coarse-grained OOD\nperformance, along with a new loss function for models in this task.\n", "rewritten_text": "Open-world deployments of machine learning models risk misclassifying unseen observations with high confidence.  Robustly distinguishing in-distribution (ID) from out-of-distribution (OOD) data is crucial to mitigate this risk.  While recent advancements have yielded near-perfect performance on coarse-grained OOD detection tasks (e.g., distinguishing horses from trucks), finer-grained distinctions (e.g., differentiating commercial aircraft models) remain challenging.  This paper introduces a novel theoretical framework for understanding fine- and coarse-grained OOD detection. We re-conceptualize fine-grained classification as a three-part problem and propose a new baseline task using two fine-grained hierarchical datasets.  Furthermore, we introduce two new evaluation methods to differentiate fine- and coarse-grained OOD performance and a novel loss function tailored to this task.\n"}, "2109.00698": {"original_text": "  While conventional wisdom suggests that more aggressively filtering data from\nlow-quality sources like Common Crawl always monotonically improves the quality\nof training data, we find that aggressive filtering can in fact lead to a\ndecrease in model quality on a wide array of downstream tasks for a GPT-like\nlanguage model. We speculate that this is because optimizing sufficiently\nstrongly for a proxy metric harms performance on the true objective, suggesting\na need for more robust filtering objectives when attempting to filter more\naggressively. We hope this work leads to detailed analysis of the effects of\ndataset filtering design choices on downstream model performance in future\nwork.\n", "rewritten_text": "Conventional wisdom holds that aggressively filtering low-quality data sources, such as Common Crawl, monotonically improves training data quality.  However, we find that aggressive filtering can actually decrease the performance of GPT-like language models on a wide range of downstream tasks.  This suggests that optimizing too strongly for a proxy metric can harm performance on the true objective, highlighting the need for more robust filtering objectives when employing aggressive filtering strategies.  Future work should investigate the impact of various dataset filtering design choices on downstream model performance in greater detail.\n"}, "1501.03952": {"original_text": "  Domain adaptation techniques aim at adapting a classifier learnt on a source\ndomain to work on the target domain. Exploiting the subspaces spanned by\nfeatures of the source and target domains respectively is one approach that has\nbeen investigated towards solving this problem. These techniques normally\nassume the existence of a single subspace for the entire source / target\ndomain. In this work, we consider the hierarchical organization of the data and\nconsider multiple subspaces for the source and target domain based on the\nhierarchy. We evaluate different subspace based domain adaptation techniques\nunder this setting and observe that using different subspaces based on the\nhierarchy yields consistent improvement over a non-hierarchical baseline\n", "rewritten_text": "Domain adaptation techniques aim to adapt a classifier trained on a source domain to a target domain. One approach involves exploiting the feature subspaces of the source and target domains.  Existing techniques typically assume a single subspace for each domain.  This work explores a hierarchical data organization, employing multiple subspaces for both source and target domains based on this hierarchy.  We evaluate several subspace-based domain adaptation techniques within this hierarchical framework and demonstrate that using multiple subspaces consistently improves performance compared to a non-hierarchical baseline.\n"}, "1707.00569": {"original_text": "  This survey presents a deep analysis of the learning and inference\ncapabilities in nine popular trackers. It is neither intended to study the\nwhole literature nor is it an attempt to review all kinds of neural networks\nproposed for visual tracking. We focus instead on Siamese neural networks which\nare a promising starting point for studying the challenging problem of\ntracking. These networks integrate efficiently feature learning and the\ntemporal matching and have so far shown state-of-the-art performance. In\nparticular, the branches of Siamese networks, their layers connecting these\nbranches, specific aspects of training and the embedding of these networks into\nthe tracker are highlighted. Quantitative results from existing papers are\ncompared with the conclusion that the current evaluation methodology shows\nproblems with the reproducibility and the comparability of results. The paper\nproposes a novel Lisp-like formalism for a better comparison of trackers. This\nassumes a certain functional design and functional decomposition of trackers.\nThe paper tries to give foundation for tracker design by a formulation of the\nproblem based on the theory of machine learning and by the interpretation of a\ntracker as a decision function. The work concludes with promising lines of\nresearch and suggests future work.\n", "rewritten_text": "This survey analyzes the learning and inference capabilities of nine popular visual trackers.  It does not aim to be a comprehensive literature review of all neural networks used for visual tracking, but rather focuses on Siamese neural networks\u2014a promising approach to this challenging problem.  These networks efficiently integrate feature learning and temporal matching, and have demonstrated state-of-the-art performance.  Our analysis highlights key aspects of Siamese networks, including their branch architectures, inter-branch connections, training methodologies, and integration within the tracker.  A comparative analysis of quantitative results from existing literature reveals significant reproducibility and comparability issues in current evaluation methodologies.  To address this, we propose a novel Lisp-like formalism for improved tracker comparison, based on a functional design and decomposition of tracker components.  Furthermore, we provide a theoretical foundation for tracker design by formulating the tracking problem within a machine learning framework and interpreting the tracker as a decision function.  The survey concludes by outlining promising avenues for future research.\n"}, "2202.04897": {"original_text": "  Knowledge graph embedding (KGE) models learn the representation of entities\nand relations in knowledge graphs. Distance-based methods show promising\nperformance on link prediction task, which predicts the result by the distance\nbetween two entity representations. However, most of these methods represent\nthe head entity and tail entity separately, which limits the model capacity. We\npropose two novel distance-based methods named InterHT and InterHT+ that allow\nthe head and tail entities to interact better and get better entity\nrepresentation. Experimental results show that our proposed method achieves the\nbest results on ogbl-wikikg2 dataset.\n", "rewritten_text": "Knowledge graph embedding (KGE) models learn representations of entities and relations within knowledge graphs.  Distance-based methods have shown promising performance in link prediction, predicting relationships based on the distance between entity representations. However, most such methods represent head and tail entities independently, limiting model capacity.  We propose two novel distance-based methods, InterHT and InterHT+, which facilitate improved interaction between head and tail entities, leading to enhanced entity representations.  Experimental results demonstrate that our proposed methods achieve state-of-the-art performance on the OGBL-wikikg2 dataset.\n"}, "1907.0203": {"original_text": "  Factchecking has always been a part of the journalistic process. However with\nnewsroom budgets shrinking it is coming under increasing pressure just as the\namount of false information circulating is on the rise. We therefore propose a\nmethod to increase the efficiency of the factchecking process, using the latest\ndevelopments in Natural Language Processing (NLP). This method allows us to\ncompare incoming claims to an existing corpus and return similar, factchecked,\nclaims in a live system-allowing factcheckers to work simultaneously without\nduplicating their work.\n", "rewritten_text": "Fact-checking has always been integral to the journalistic process.  However, shrinking newsroom budgets are placing increasing pressure on fact-checking resources, even as the proliferation of misinformation intensifies.  We therefore propose a method to enhance fact-checking efficiency using the latest advancements in Natural Language Processing (NLP). This method enables the comparison of incoming claims against an existing corpus of fact-checked claims, returning similar results in a real-time system. This allows fact-checkers to collaborate simultaneously, avoiding redundant work.\n"}, "2311.06377": {"original_text": "  Heaps' law is an empirical relation in text analysis that predicts vocabulary\ngrowth as a function of corpus size. While this law has been validated in\ndiverse human-authored text corpora, its applicability to large language model\ngenerated text remains unexplored. This study addresses this gap, focusing on\nthe emulation of corpora using the suite of GPT-Neo large language models. To\nconduct our investigation, we emulated corpora of PubMed abstracts using three\ndifferent parameter sizes of the GPT-Neo model. Our emulation strategy involved\nusing the initial five words of each PubMed abstract as a prompt and\ninstructing the model to expand the content up to the original abstract's\nlength. Our findings indicate that the generated corpora adhere to Heaps' law.\nInterestingly, as the GPT-Neo model size grows, its generated vocabulary\nincreasingly adheres to Heaps' law as as observed in human-authored text. To\nfurther improve the richness and authenticity of GPT-Neo outputs, future\niterations could emphasize enhancing model size or refining the model\narchitecture to curtail vocabulary repetition.\n", "rewritten_text": "Heaps' law, an empirical relationship in text analysis predicting vocabulary growth as a function of corpus size, has been validated across diverse human-authored corpora.  However, its applicability to large language model (LLM)-generated text remains uninvestigated. This study addresses this gap by examining corpora emulated using the GPT-Neo suite of LLMs.  We emulated PubMed abstract corpora using three GPT-Neo models with varying parameter sizes.  Our emulation strategy involved prompting each model with the first five words of a PubMed abstract and instructing it to generate text matching the original abstract's length.  Results indicate that the generated corpora adhere to Heaps' law.  Furthermore, larger GPT-Neo models exhibited increasingly closer adherence to the Heaps' law relationship observed in human-authored text.  Future work could focus on increasing model size or refining the model architecture to reduce vocabulary repetition and enhance the richness and authenticity of GPT-Neo-generated text.\n"}, "1809.07099": {"original_text": "  Deep neural networks have exhibited promising performance in image\nsuper-resolution (SR) due to the power in learning the non-linear mapping from\nlow-resolution (LR) images to high-resolution (HR) images. However, most deep\nlearning methods employ feed-forward architectures, and thus the dependencies\nbetween LR and HR images are not fully exploited, leading to limited learning\nperformance. Moreover, most deep learning based SR methods apply the pixel-wise\nreconstruction error as the loss, which, however, may fail to capture\nhigh-frequency information and produce perceptually unsatisfying results,\nwhilst the recent perceptual loss relies on some pre-trained deep model and\nthey may not generalize well. In this paper, we introduce a mask to separate\nthe image into low- and high-frequency parts based on image gradient magnitude,\nand then devise a gradient sensitive loss to well capture the structures in the\nimage without sacrificing the recovery of low-frequency content. Moreover, by\ninvestigating the duality in SR, we develop a dual reconstruction network (DRN)\nto improve the SR performance. We provide theoretical analysis on the\ngeneralization performance of our method and demonstrate its effectiveness and\nsuperiority with thorough experiments.\n", "rewritten_text": "Deep neural networks have shown promising performance in image super-resolution (SR) by learning the complex non-linear mapping from low-resolution (LR) to high-resolution (HR) images.  However, most deep learning approaches utilize feed-forward architectures, which fail to fully exploit the dependencies between LR and HR images, limiting performance.  Furthermore, the prevalent pixel-wise reconstruction loss often neglects high-frequency information, resulting in perceptually unsatisfactory results.  While recent perceptual losses leverage pre-trained deep models, their generalization capabilities may be limited.\n\nThis paper introduces a novel approach. We employ a mask, based on image gradient magnitude, to separate the image into low- and high-frequency components.  A gradient-sensitive loss function is then devised to effectively capture image structures without compromising low-frequency content recovery.  Additionally, exploiting the duality inherent in SR, we propose a dual reconstruction network (DRN) to further enhance performance.  We provide a theoretical analysis of our method's generalization capabilities and demonstrate its effectiveness and superiority through comprehensive experiments.\n"}, "2310.15052": {"original_text": "  Dataset distillation plays a crucial role in creating compact datasets with\nsimilar training performance compared with original large-scale ones. This is\nessential for addressing the challenges of data storage and training costs.\nPrevalent methods facilitate knowledge transfer by matching the gradients,\nembedding distributions, or training trajectories of synthetic images with\nthose of the sampled original images. Although there are various matching\nobjectives, currently the strategy for selecting original images is limited to\nnaive random sampling. We argue that random sampling overlooks the evenness of\nthe selected sample distribution, which may result in noisy or biased matching\ntargets. Besides, the sample diversity is also not constrained by random\nsampling. Additionally, current methods predominantly focus on\nsingle-dimensional matching, where information is not fully utilized. To\naddress these challenges, we propose a novel matching strategy called Dataset\nDistillation by Bidirectional REpresentAtive Matching (DREAM+), which selects\nrepresentative original images for bidirectional matching. DREAM+ is applicable\nto a variety of mainstream dataset distillation frameworks and significantly\nreduces the number of distillation iterations by more than 15 times without\naffecting performance. Given sufficient training time, DREAM+ can further\nimprove the performance and achieve state-of-the-art results. We have released\nthe code at github.com/NUS-HPC-AI-Lab/DREAM+.\n", "rewritten_text": "Dataset distillation is crucial for creating compact datasets that maintain the training performance of their larger counterparts, thereby mitigating data storage and training costs.  Existing methods transfer knowledge by matching gradients, embedding distributions, or training trajectories of synthetic and original images. However, current strategies for selecting original images rely on naive random sampling, neglecting the importance of sample distribution evenness and diversity. This can lead to noisy or biased matching targets. Furthermore, these methods primarily employ single-dimensional matching, failing to fully utilize available information.\n\nTo address these limitations, we propose Dataset Distillation by Bidirectional REpresentAtive Matching (DREAM+), a novel matching strategy that selects representative original images for bidirectional matching.  DREAM+ is compatible with various dataset distillation frameworks and significantly reduces the number of distillation iterations\u2014by more than 15 times\u2014without compromising performance.  With sufficient training time, DREAM+ can further improve performance, achieving state-of-the-art results.  The code is available at github.com/NUS-HPC-AI-Lab/DREAM+.\n"}, "2207.09625": {"original_text": "  Given an image and a reference caption, the image caption editing task aims\nto correct the misalignment errors and generate a refined caption. However, all\nexisting caption editing works are implicit models, ie, they directly produce\nthe refined captions without explicit connections to the reference captions. In\nthis paper, we introduce a new task: Explicit Caption Editing (ECE). ECE models\nexplicitly generate a sequence of edit operations, and this edit operation\nsequence can translate the reference caption into a refined one. Compared to\nthe implicit editing, ECE has multiple advantages: 1) Explainable: it can trace\nthe whole editing path. 2) Editing Efficient: it only needs to modify a few\nwords. 3) Human-like: it resembles the way that humans perform caption editing,\nand tries to keep original sentence structures. To solve this new task, we\npropose the first ECE model: TIger. TIger is a non-autoregressive\ntransformer-based model, consisting of three modules: Tagger_del, Tagger_add,\nand Inserter. Specifically, Tagger_del decides whether each word should be\npreserved or not, Tagger_add decides where to add new words, and Inserter\npredicts the specific word for adding. To further facilitate ECE research, we\npropose two new ECE benchmarks by re-organizing two existing datasets, dubbed\nCOCO-EE and Flickr30K-EE, respectively. Extensive ablations on both two\nbenchmarks have demonstrated the effectiveness of TIger.\n", "rewritten_text": "Given an image and a reference caption, image caption editing aims to correct misalignments and generate a refined caption.  Existing methods are implicit models; they directly produce refined captions without explicitly connecting them to the reference captions.  This paper introduces Explicit Caption Editing (ECE), a new task where models explicitly generate a sequence of edit operations that transform the reference caption into a refined one.  Compared to implicit editing, ECE offers several advantages:  explainability (tracing the entire editing process), editing efficiency (modifying only a few words), and human-likeness (resembling human caption editing and preserving original sentence structure).\n\nTo address this task, we propose Tiger, the first ECE model.  Tiger is a non-autoregressive transformer-based model comprising three modules: Tagger<sub>del</sub> (deciding which words to preserve), Tagger<sub>add</sub> (deciding where to add words), and Inserter (predicting the words to add).  To facilitate ECE research, we introduce two new benchmarks, COCO-EE and Flickr30K-EE, by reorganizing existing datasets.  Extensive ablation studies on both benchmarks demonstrate Tiger's effectiveness.\n"}, "1805.05503": {"original_text": "  Human faces are one interesting object class with numerous applications.\nWhile significant progress has been made in the generic deblurring problem,\nexisting methods are less effective for blurry face images. The success of the\nstate-of-the-art image deblurring algorithms stems mainly from implicit or\nexplicit restoration of salient edges for kernel estimation. However, existing\nmethods are less effective as only few edges can be restored from blurry face\nimages for kernel estimation. In this paper, we address the problem of\ndeblurring face images by exploiting facial structures. We propose a deblurring\nalgorithm based on an exemplar dataset without using coarse-to-fine strategies\nor heuristic edge selections. In addition, we develop a convolutional neural\nnetwork to restore sharp edges from blurry images for deblurring. Extensive\nexperiments against the state-of-the-art methods demonstrate the effectiveness\nof the proposed algorithms for deblurring face images. In addition, we show the\nproposed algorithms can be applied to image deblurring for other object\nclasses.\n", "rewritten_text": "Human faces constitute a particularly interesting object class with numerous applications.  While significant progress has been made in generic image deblurring, existing methods remain less effective for blurry facial images.  The success of state-of-the-art image deblurring algorithms largely depends on the implicit or explicit restoration of salient edges for accurate kernel estimation. However, these methods often underperform on blurry faces due to the limited number of recoverable edges.  This paper addresses the problem of facial image deblurring by leveraging facial structural information. We propose a novel deblurring algorithm utilizing an exemplar dataset, eschewing coarse-to-fine strategies and heuristic edge selection.  Furthermore, we develop a convolutional neural network (CNN) to restore sharp edges from blurry images, thereby improving deblurring performance.  Extensive experiments demonstrate the superiority of our proposed algorithms compared to state-of-the-art methods in deblurring facial images.  Moreover, we show the generalizability of our approach to deblurring other object classes.\n"}, "2205.09299": {"original_text": "  Convolutional Neural Networks (CNNs) have achieved promising results in\nmedical image segmentation. However, CNNs require lots of training data and are\nincapable of handling pose and deformation of objects. Furthermore, their\npooling layers tend to discard important information such as positions as well\nas CNNs are sensitive to rotation and affine transformation. Capsule network is\na recent new architecture that has achieved better robustness in part-whole\nrepresentation learning by replacing pooling layers with dynamic routing and\nconvolutional strides, which has shown potential results on popular tasks such\nas digit classification and object segmentation. In this paper, we propose a 3D\nencoder-decoder network with Convolutional Capsule Encoder (called 3DConvCaps)\nto learn lower-level features (short-range attention) with convolutional layers\nwhile modeling the higher-level features (long-range dependence) with capsule\nlayers. Our experiments on multiple datasets including iSeg-2017, Hippocampus,\nand Cardiac demonstrate that our 3D 3DConvCaps network considerably outperforms\nprevious capsule networks and 3D-UNets. We further conduct ablation studies of\nnetwork efficiency and segmentation performance under various configurations of\nconvolution layers and capsule layers at both contracting and expanding paths.\n", "rewritten_text": "Convolutional Neural Networks (CNNs) have shown promising results in medical image segmentation.  However, their reliance on extensive training data and vulnerability to object pose and deformation limit their effectiveness.  Furthermore, pooling layers often discard crucial positional information, and CNNs are sensitive to rotation and affine transformations.  Capsule networks, a more recent architecture, offer improved robustness in part-whole representation learning by replacing pooling layers with dynamic routing and convolutional strides.  This approach has yielded promising results in tasks such as digit classification and object segmentation.\n\nThis paper proposes a 3D encoder-decoder network, termed 3DConvCaps, which leverages convolutional layers to learn low-level features (short-range attention) and capsule layers to model high-level features (long-range dependencies).  Experiments on the iSeg-2017, Hippocampus, and Cardiac datasets demonstrate that 3DConvCaps significantly outperforms existing capsule networks and 3D-UNets.  Ablation studies further analyze the network's efficiency and segmentation performance under varying configurations of convolutional and capsule layers in both the encoder and decoder pathways.\n"}, "2407.08290": {"original_text": "  Recent advances in mobile mapping systems have greatly enhanced the\nefficiency and convenience of acquiring urban 3D data. These systems utilize\nLiDAR sensors mounted on vehicles to capture vast cityscapes. However, a\nsignificant challenge arises due to occlusions caused by roadside parked\nvehicles, leading to the loss of scene information, particularly on the roads,\nsidewalks, curbs, and the lower sections of buildings. In this study, we\npresent a novel approach that leverages deep neural networks to learn a model\ncapable of filling gaps in urban scenes that are obscured by vehicle occlusion.\nWe have developed an innovative technique where we place virtual vehicle models\nalong road boundaries in the gap-free scene and utilize a ray-casting algorithm\nto create a new scene with occluded gaps. This allows us to generate diverse\nand realistic urban point cloud scenes with and without vehicle occlusion,\nsurpassing the limitations of real-world training data collection and\nannotation. Furthermore, we introduce the Scene Gap Completion Network\n(SGC-Net), an end-to-end model that can generate well-defined shape boundaries\nand smooth surfaces within occluded gaps. The experiment results reveal that\n97.66% of the filled points fall within a range of 5 centimeters relative to\nthe high-density ground truth point cloud scene. These findings underscore the\nefficacy of our proposed model in gap completion and reconstructing urban\nscenes affected by vehicle occlusions.\n", "rewritten_text": "Recent advances in mobile mapping systems have significantly improved the efficiency and convenience of acquiring urban 3D data.  These systems, employing LiDAR sensors mounted on vehicles, capture extensive cityscapes.  However, occlusions from roadside parked vehicles present a significant challenge, resulting in the loss of scene information, particularly along roads, sidewalks, curbs, and the lower portions of buildings.  This study introduces a novel approach using deep neural networks to fill these occlusion-induced gaps in urban scenes.\n\nOur method generates synthetic training data by placing virtual vehicle models along road boundaries in a gap-free scene and employing a ray-casting algorithm to simulate occlusions. This process creates diverse and realistic urban point cloud scenes, both with and without vehicle occlusions, overcoming limitations inherent in collecting and annotating real-world data.  We further introduce the Scene Gap Completion Network (SGC-Net), an end-to-end model that generates well-defined shape boundaries and smooth surfaces within the occluded regions.\n\nExperimental results demonstrate that 97.66% of the completed points lie within 5 centimeters of the high-density ground truth point cloud.  These findings highlight the effectiveness of our proposed model in completing gaps and reconstructing urban scenes affected by vehicle occlusions.\n"}, "2004.04699": {"original_text": "  Deep Neural Networks trained in a fully supervised fashion are the dominant\ntechnology in perception-based autonomous driving systems. While collecting\nlarge amounts of unlabeled data is already a major undertaking, only a subset\nof it can be labeled by humans due to the effort needed for high-quality\nannotation. Therefore, finding the right data to label has become a key\nchallenge. Active learning is a powerful technique to improve data efficiency\nfor supervised learning methods, as it aims at selecting the smallest possible\ntraining set to reach a required performance. We have built a scalable\nproduction system for active learning in the domain of autonomous driving. In\nthis paper, we describe the resulting high-level design, sketch some of the\nchallenges and their solutions, present our current results at scale, and\nbriefly describe the open problems and future directions.\n", "rewritten_text": "Deep neural networks trained with fully supervised learning dominate perception-based autonomous driving systems.  While acquiring large unlabeled datasets is challenging, human annotation of even a subset remains labor-intensive and costly.  This necessitates strategic data selection for labeling, making active learning a crucial technique.  Active learning optimizes data efficiency by identifying the minimal training set needed to achieve target performance.  This paper presents a scalable production-level active learning system for autonomous driving. We detail its high-level design, address key challenges and their solutions, report our large-scale results, and outline open problems and future research directions.\n"}, "1808.01491": {"original_text": "  Single image rain streaks removal has recently witnessed substantial progress\ndue to the development of deep convolutional neural networks. However, existing\ndeep learning based methods either focus on the entrance and exit of the\nnetwork by decomposing the input image into high and low frequency information\nand employing residual learning to reduce the mapping range, or focus on the\nintroduction of cascaded learning scheme to decompose the task of rain streaks\nremoval into multi-stages. These methods treat the convolutional neural network\nas an encapsulated end-to-end mapping module without deepening into the\nrationality and superiority of neural network design. In this paper, we delve\ninto an effective end-to-end neural network structure for stronger feature\nexpression and spatial correlation learning. Specifically, we propose a\nnon-locally enhanced encoder-decoder network framework, which consists of a\npooling indices embedded encoder-decoder network to efficiently learn\nincreasingly abstract feature representation for more accurate rain streaks\nmodeling while perfectly preserving the image detail. The proposed\nencoder-decoder framework is composed of a series of non-locally enhanced dense\nblocks that are designed to not only fully exploit hierarchical features from\nall the convolutional layers but also well capture the long-distance\ndependencies and structural information. Extensive experiments on synthetic and\nreal datasets demonstrate that the proposed method can effectively remove\nrain-streaks on rainy image of various densities while well preserving the\nimage details, which achieves significant improvements over the recent\nstate-of-the-art methods.\n", "rewritten_text": "Recent advancements in single-image rain streak removal have been driven by deep convolutional neural networks (CNNs).  However, existing deep learning approaches either pre- and post-process the input image by decomposing it into high- and low-frequency components and using residual learning to reduce the mapping complexity, or employ cascaded learning schemes to decompose the rain streak removal task into multiple stages.  These methods treat the CNN as a black box, neglecting the rationale and potential improvements in network architecture.\n\nThis paper presents an effective end-to-end CNN architecture designed for enhanced feature representation and spatial correlation learning.  Specifically, we propose a non-locally enhanced encoder-decoder network incorporating a pooling indices embedded encoder-decoder to efficiently learn increasingly abstract feature representations for more accurate rain streak modeling while preserving image detail.  This framework comprises a series of non-locally enhanced dense blocks, which fully exploit hierarchical features from all convolutional layers and effectively capture long-range dependencies and structural information.\n\nExtensive experiments on synthetic and real datasets demonstrate that our method effectively removes rain streaks from images with varying rain densities while preserving image detail, significantly outperforming state-of-the-art methods.\n"}, "2303.15698": {"original_text": "  Standard deep learning models such as convolutional neural networks (CNNs)\nlack the ability of generalizing to domains which have not been seen during\ntraining. This problem is mainly due to the common but often wrong assumption\nof such models that the source and target data come from the same i.i.d.\ndistribution. Recently, Vision Transformers (ViTs) have shown outstanding\nperformance for a broad range of computer vision tasks. However, very few\nstudies have investigated their ability to generalize to new domains. This\npaper presents a first Token-level Feature Stylization (TFS-ViT) approach for\ndomain generalization, which improves the performance of ViTs to unseen data by\nsynthesizing new domains. Our approach transforms token features by mixing the\nnormalization statistics of images from different domains. We further improve\nthis approach with a novel strategy for attention-aware stylization, which uses\nthe attention maps of class (CLS) tokens to compute and mix normalization\nstatistics of tokens corresponding to different image regions. The proposed\nmethod is flexible to the choice of backbone model and can be easily applied to\nany ViT-based architecture with a negligible increase in computational\ncomplexity. Comprehensive experiments show that our approach is able to achieve\nstate-of-the-art performance on five challenging benchmarks for domain\ngeneralization, and demonstrate its ability to deal with different types of\ndomain shifts. The implementation is available at:\nhttps://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.\n", "rewritten_text": "Standard deep learning models, such as convolutional neural networks (CNNs), struggle to generalize to unseen domains due to the often-incorrect assumption that source and target data are independently and identically distributed (i.i.d.).  While Vision Transformers (ViTs) have demonstrated exceptional performance across various computer vision tasks, their domain generalization capabilities remain largely unexplored.  This paper introduces Token-level Feature Stylization (TFS-ViT), a novel approach to domain generalization that enhances ViT performance on unseen data through synthetic domain creation.  Our method transforms token features by mixing the normalization statistics of images from diverse domains.  Furthermore, we introduce attention-aware stylization, leveraging class (CLS) token attention maps to selectively mix normalization statistics of tokens corresponding to different image regions.  TFS-ViT is readily adaptable to various ViT-based architectures with minimal computational overhead.  Extensive experiments on five challenging domain generalization benchmarks demonstrate state-of-the-art performance and robust handling of diverse domain shifts.  The implementation is available at: https://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.\n"}, "1804.09555": {"original_text": "  Outdoor vision-based systems suffer from atmospheric turbulences, and rain is\none of the worst factors for vision degradation. Current rain removal methods\nshow limitations either for complex dynamic scenes, or under torrential rain\nwith opaque occlusions. We propose a novel derain framework which applies\nsuperpixel (SP) segmentation to decompose the scene into depth consistent\nunits. Alignment of scene contents are done at the SP level, which proves to be\nrobust against rain occlusion interferences and fast camera motion. Two\nalignment output tensors, i.e., optimal temporal match tensor and sorted\nspatial-temporal match tensor, provide informative clues for the location of\nrain streaks and the occluded background contents. Different classical and\nnovel methods such as Robust Principle Component Analysis and Convolutional\nNeural Networks are applied and compared for their respective advantages in\nefficiently exploiting the rich spatial-temporal features provided by the two\ntensors. Extensive evaluations show that advantage of up to 5dB is achieved on\nthe scene restoration PSNR over state-of-the-art methods, and the advantage is\nespecially obvious with highly complex and dynamic scenes. Visual evaluations\nshow that the proposed framework is not only able to suppress heavy and opaque\noccluding rain streaks, but also large semi-transparent regional fluctuations\nand distortions.\n", "rewritten_text": "Outdoor vision-based systems are severely affected by atmospheric turbulence, with rain being a particularly significant cause of image degradation.  Existing rain removal methods struggle with complex dynamic scenes and torrential rain resulting in opaque occlusions.  This paper proposes a novel deraining framework that employs superpixel (SP) segmentation to decompose the scene into depth-consistent units.  Scene content alignment is performed at the SP level, proving robust to rain occlusion and fast camera motion.  Two alignment output tensors\u2014an optimal temporal match tensor and a sorted spatiotemporal match tensor\u2014provide valuable information regarding rain streak location and occluded background content.  We compare and apply various classical and novel methods, including Robust Principal Component Analysis (RPCA) and Convolutional Neural Networks (CNNs), to efficiently leverage the rich spatiotemporal features extracted from these tensors.  Extensive evaluations demonstrate a performance improvement of up to 5 dB in Peak Signal-to-Noise Ratio (PSNR) for scene restoration compared to state-of-the-art methods, particularly in highly complex and dynamic scenes.  Visual results confirm the framework's ability to effectively suppress heavy, opaque rain streaks, as well as large, semi-transparent regional fluctuations and distortions.\n"}, "2005.00975": {"original_text": "  In the deep learning (DL) era, parsing models are extremely simplified with\nlittle hurt on performance, thanks to the remarkable capability of multi-layer\nBiLSTMs in context representation. As the most popular graph-based dependency\nparser due to its high efficiency and performance, the biaffine parser directly\nscores single dependencies under the arc-factorization assumption, and adopts a\nvery simple local token-wise cross-entropy training loss. This paper for the\nfirst time presents a second-order TreeCRF extension to the biaffine parser.\nFor a long time, the complexity and inefficiency of the inside-outside\nalgorithm hinder the popularity of TreeCRF. To address this issue, we propose\nan effective way to batchify the inside and Viterbi algorithms for direct large\nmatrix operation on GPUs, and to avoid the complex outside algorithm via\nefficient back-propagation. Experiments and analysis on 27 datasets from 13\nlanguages clearly show that techniques developed before the DL era, such as\nstructural learning (global TreeCRF loss) and high-order modeling are still\nuseful, and can further boost parsing performance over the state-of-the-art\nbiaffine parser, especially for partially annotated training data. We release\nour code at https://github.com/yzhangcs/crfpar.\n", "rewritten_text": "In the deep learning (DL) era, the remarkable context representation capabilities of multi-layer BiLSTMs have enabled significant simplification of parsing models with minimal performance degradation.  The biaffine parser, a highly efficient and popular graph-based dependency parser, directly scores individual dependencies under the arc-factorization assumption, employing a simple local token-wise cross-entropy loss function. This paper introduces, for the first time, a second-order TreeCRF extension to the biaffine parser.  The complexity and inefficiency of the inside-outside algorithm have long hindered the widespread adoption of TreeCRFs.  To overcome this, we propose an efficient method for batching the inside and Viterbi algorithms, enabling direct large-matrix operations on GPUs and eliminating the need for the complex outside algorithm through efficient backpropagation.  Experiments and analyses across 27 datasets spanning 13 languages demonstrate that pre-DL techniques, such as structural learning (global TreeCRF loss) and higher-order modeling, remain valuable and can substantially improve parsing performance beyond state-of-the-art biaffine parsers, particularly with partially annotated training data.  Our code is publicly available at https://github.com/yzhangcs/crfpar.\n"}, "1812.00108": {"original_text": "  With vast amounts of video content being uploaded to the Internet every\nminute, video summarization becomes critical for efficient browsing, searching,\nand indexing of visual content. Nonetheless, the spread of social and\negocentric cameras creates an abundance of sparse scenarios captured by several\ndevices, and ultimately required to be jointly summarized. In this paper, we\ndiscuss the problem of summarizing videos recorded independently by several\ndynamic cameras that intermittently share the field of view. We present a\nrobust framework that (a) identifies a diverse set of important events among\nmoving cameras that often are not capturing the same scene, and (b) selects the\nmost representative view(s) at each event to be included in a universal\nsummary. Due to the lack of an applicable alternative, we collected a new\nmulti-view egocentric dataset, Multi-Ego. Our dataset is recorded\nsimultaneously by three cameras, covering a wide variety of real-life\nscenarios. The footage is annotated by multiple individuals under various\nsummarization configurations, with a consensus analysis ensuring a reliable\nground truth. We conduct extensive experiments on the compiled dataset in\naddition to three other standard benchmarks that show the robustness and the\nadvantage of our approach in both supervised and unsupervised settings.\nAdditionally, we show that our approach learns collectively from data of varied\nnumber-of-views and orthogonal to other summarization methods, deeming it\nscalable and generic.\n", "rewritten_text": "The exponential growth of online video necessitates efficient methods for browsing, searching, and indexing.  This is particularly challenging with the proliferation of social and egocentric cameras, generating abundant, sparsely overlapping footage from multiple devices requiring joint summarization.  This paper addresses the problem of summarizing videos independently recorded by several dynamic cameras with intermittently shared fields of view. We propose a robust framework that (a) identifies diverse key events across these moving, often disparate, camera perspectives, and (b) selects the most representative view(s) of each event for inclusion in a unified summary.  Lacking a suitable existing dataset, we created Multi-Ego, a new multi-view egocentric dataset.  Recorded simultaneously by three cameras, Multi-Ego encompasses a wide range of real-world scenarios and features annotations from multiple individuals under various summarization configurations, ensuring a reliable consensus-based ground truth.  Extensive experiments on Multi-Ego and three established benchmark datasets demonstrate the robustness and superior performance of our approach in both supervised and unsupervised settings.  Furthermore, our method's ability to learn effectively from datasets with varying numbers of views, and its orthogonality to existing summarization techniques, highlight its scalability and generalizability.\n"}, "2012.07791": {"original_text": "  We propose real-time, six degrees of freedom (6DoF), 3D face pose estimation\nwithout face detection or landmark localization. We observe that estimating the\n6DoF rigid transformation of a face is a simpler problem than facial landmark\ndetection, often used for 3D face alignment. In addition, 6DoF offers more\ninformation than face bounding box labels. We leverage these observations to\nmake multiple contributions: (a) We describe an easily trained, efficient,\nFaster R-CNN--based model which regresses 6DoF pose for all faces in the photo,\nwithout preliminary face detection. (b) We explain how pose is converted and\nkept consistent between the input photo and arbitrary crops created while\ntraining and evaluating our model. (c) Finally, we show how face poses can\nreplace detection bounding box training labels. Tests on AFLW2000-3D and BIWI\nshow that our method runs at real-time and outperforms state of the art (SotA)\nface pose estimators. Remarkably, our method also surpasses SotA models of\ncomparable complexity on the WIDER FACE detection benchmark, despite not been\noptimized on bounding box labels.\n", "rewritten_text": "We propose a real-time, six degrees of freedom (6DoF) 3D face pose estimation method that bypasses face detection and landmark localization.  Estimating the 6DoF rigid transformation of a face is inherently simpler than facial landmark detection, commonly used for 3D face alignment, and provides more information than face bounding box labels.  This observation underpins our contributions: (a) We present an easily trained, efficient Faster R-CNN-based model that directly regresses the 6DoF pose of all faces in an image without prior face detection. (b) We detail a method for consistent pose representation across the input image and arbitrary crops used during training and evaluation. (c) Finally, we demonstrate that face pose can replace detection bounding box training labels.  Experiments on AFLW2000-3D and BIWI datasets show our method achieves real-time performance and surpasses state-of-the-art (SOTA) face pose estimators.  Remarkably, it also outperforms SOTA models of comparable complexity on the WIDER FACE detection benchmark, despite not being optimized for bounding box labels.\n"}, "2208.05909": {"original_text": "  Preservation of domain knowledge from the source to target is crucial in any\ntranslation workflow. It is common in the translation industry to receive\nhighly specialized projects, where there is hardly any parallel in-domain data.\nIn such scenarios where there is insufficient in-domain data to fine-tune\nMachine Translation (MT) models, producing translations that are consistent\nwith the relevant context is challenging. In this work, we propose a novel\napproach to domain adaptation leveraging state-of-the-art pretrained language\nmodels (LMs) for domain-specific data augmentation for MT, simulating the\ndomain characteristics of either (a) a small bilingual dataset, or (b) the\nmonolingual source text to be translated. Combining this idea with\nback-translation, we can generate huge amounts of synthetic bilingual in-domain\ndata for both use cases. For our investigation, we use the state-of-the-art\nTransformer architecture. We employ mixed fine-tuning to train models that\nsignificantly improve translation of in-domain texts. More specifically, in\nboth scenarios, our proposed methods achieve improvements of approximately 5-6\nBLEU and 2-3 BLEU, respectively, on the Arabic-to-English and English-to-Arabic\nlanguage pairs. Furthermore, the outcome of human evaluation corroborates the\nautomatic evaluation results.\n", "rewritten_text": "Preserving domain knowledge during translation is crucial.  The translation industry frequently handles highly specialized projects lacking sufficient parallel in-domain data for effective machine translation (MT) model fine-tuning.  This lack of data makes producing contextually consistent translations challenging.  This work proposes a novel domain adaptation approach leveraging state-of-the-art pretrained language models (LMs) for domain-specific data augmentation in MT.  We simulate domain characteristics using either (a) a small bilingual dataset or (b) the monolingual source text.  Combining this with back-translation generates substantial synthetic bilingual in-domain data for both scenarios.  Our investigation utilizes the Transformer architecture and mixed fine-tuning to train models that significantly improve in-domain translation.  Specifically, we achieved approximately 5-6 BLEU improvement for Arabic-to-English and 2-3 BLEU improvement for English-to-Arabic translation in both scenarios.  Human evaluation confirms these automatic evaluation results.\n"}, "1811.09745": {"original_text": "  We propose a new geometric regularization principle for reconstructing vector\nfields based on prior knowledge about their divergence. As one important\nexample of this general idea, we focus on vector fields modelling blood flow\npattern that should be divergent in arteries and convergent in veins. We show\nthat this previously ignored regularization constraint can significantly\nimprove the quality of vessel tree reconstruction particularly around\nbifurcations where non-zero divergence is concentrated. Our divergence prior is\ncritical for resolving (binary) sign ambiguity in flow orientations produced by\nstandard vessel filters, e.g. Frangi. Our vessel tree centerline reconstruction\ncombines divergence constraints with robust curvature regularization. Our\nunsupervised method can reconstruct complete vessel trees with near-capillary\ndetails on synthetic and real 3D volumes.\n", "rewritten_text": "We propose a novel geometric regularization principle for reconstructing vector fields, leveraging prior knowledge of their divergence.  This principle is exemplified by its application to vector fields modeling blood flow, which should exhibit divergence in arteries and convergence in veins.  We demonstrate that this previously unconsidered regularization constraint significantly improves vessel tree reconstruction, particularly at bifurcations where non-zero divergence is concentrated.  Our divergence prior is crucial for resolving the binary sign ambiguity in flow orientations inherent in standard vessel filters (e.g., Frangi).  Our method combines divergence constraints with robust curvature regularization to reconstruct vessel tree centerlines. This unsupervised approach reconstructs complete vessel trees, including near-capillary details, from both synthetic and real 3D volumes.\n"}, "2104.08575": {"original_text": "  Super-resolution (SR) is an ill-posed problem, which means that infinitely\nmany high-resolution (HR) images can be degraded to the same low-resolution\n(LR) image. To study the one-to-many stochastic SR mapping, we implicitly\nrepresent the non-local self-similarity of natural images and develop a\nVariational Sparse framework for Super-Resolution (VSpSR) via neural networks.\nSince every small patch of a HR image can be well approximated by the sparse\nrepresentation of atoms in an over-complete dictionary, we design a two-branch\nmodule, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM\nextracts patch-level basis from the LR input, and the other branch infers\npixel-wise variational distributions with respect to the sparse coefficients.\nBy repeatedly sampling coefficients, we could obtain infinite sparse\nrepresentations, and thus generate diverse HR images. According to the\npreliminary results of NTIRE 2021 challenge on learning SR space, our team\n(FudanZmic21) ranks 7-th in terms of released scores. The implementation of\nVSpSR is released at https://zmiclab.github.io/.\n", "rewritten_text": "Super-resolution (SR) is an ill-posed problem; infinitely many high-resolution (HR) images can be degraded to the same low-resolution (LR) image.  To address the one-to-many stochastic mapping inherent in SR, we propose a Variational Sparse framework for Super-Resolution (VSpSR) using neural networks.  This framework implicitly leverages the non-local self-similarity of natural images.  Because small patches within an HR image can be sparsely represented using atoms from an overcomplete dictionary, we designed a two-branch module, the VSpM, to explore the SR solution space.  One branch extracts patch-level basis from the LR input, while the other infers pixel-wise variational distributions over the sparse coefficients.  Repeatedly sampling these coefficients generates diverse HR image reconstructions.  In the NTIRE 2021 challenge on learning SR space, our team (FudanZmic21) achieved a 7th-place ranking based on the released scores.  The VSpSR implementation is available at https://zmiclab.github.io/.\n"}, "2206.12505": {"original_text": "  We propose a novel semi-supervised learning approach for classification of\nhistopathology images. We employ strong supervision with patch-level\nannotations combined with a novel co-training loss to create a semi-supervised\nlearning framework. Co-training relies on multiple conditionally independent\nand sufficient views of the data. We separate the hematoxylin and eosin\nchannels in pathology images using color deconvolution to create two views of\neach slide that can partially fulfill these requirements. Two separate CNNs are\nused to embed the two views into a joint feature space. We use a contrastive\nloss between the views in this feature space to implement co-training. We\nevaluate our approach in clear cell renal cell and prostate carcinomas, and\ndemonstrate improvement over state-of-the-art semi-supervised learning methods.\n", "rewritten_text": "This paper presents a novel semi-supervised learning approach for classifying histopathology images.  We leverage strong patch-level annotations alongside a novel co-training loss function to build a robust semi-supervised framework.  This framework exploits the principle of co-training, which requires multiple conditionally independent and sufficient views of the data.  We generate these views by applying color deconvolution to separate the hematoxylin and eosin channels in the pathology images, creating two distinct representations of each slide.  Two separate convolutional neural networks (CNNs) then embed these views into a shared feature space.  A contrastive loss function is employed within this space to implement the co-training strategy.  We evaluate our method on clear cell renal cell carcinoma and prostate carcinoma datasets, demonstrating improved performance over existing state-of-the-art semi-supervised learning techniques.\n"}, "2303.08714": {"original_text": "  Adapting the Diffusion Probabilistic Model (DPM) for direct image\nsuper-resolution is wasteful, given that a simple Convolutional Neural Network\n(CNN) can recover the main low-frequency content. Therefore, we present\nResDiff, a novel Diffusion Probabilistic Model based on Residual structure for\nSingle Image Super-Resolution (SISR). ResDiff utilizes a combination of a CNN,\nwhich restores primary low-frequency components, and a DPM, which predicts the\nresidual between the ground-truth image and the CNN predicted image. In\ncontrast to the common diffusion-based methods that directly use LR images to\nguide the noise towards HR space, ResDiff utilizes the CNN's initial prediction\nto direct the noise towards the residual space between HR space and\nCNN-predicted space, which not only accelerates the generation process but also\nacquires superior sample quality. Additionally, a frequency-domain-based loss\nfunction for CNN is introduced to facilitate its restoration, and a\nfrequency-domain guided diffusion is designed for DPM on behalf of predicting\nhigh-frequency details. The extensive experiments on multiple benchmark\ndatasets demonstrate that ResDiff outperforms previous diffusion based methods\nin terms of shorter model convergence time, superior generation quality, and\nmore diverse samples.\n", "rewritten_text": "Directly applying Diffusion Probabilistic Models (DPMs) to image super-resolution is inefficient, as Convolutional Neural Networks (CNNs) effectively capture low-frequency content.  We therefore propose ResDiff, a novel residual-based DPM for single image super-resolution (SISR).  ResDiff combines a CNN, which reconstructs low-frequency components, with a DPM that predicts the residual between the ground truth and the CNN's output. Unlike existing diffusion-based methods that directly map low-resolution (LR) images to high-resolution (HR) space, ResDiff leverages the CNN's initial prediction to guide the noise towards the residual between the HR and CNN-predicted spaces. This approach accelerates generation and improves sample quality.  Furthermore, we introduce a frequency-domain loss function for the CNN to enhance its reconstruction capabilities and a frequency-domain guided diffusion process for the DPM to effectively predict high-frequency details.  Extensive experiments on multiple benchmark datasets demonstrate that ResDiff surpasses existing diffusion-based methods, achieving faster convergence, higher-quality generation, and greater sample diversity.\n"}, "1607.06797": {"original_text": "  In this paper we proposed an ordered patch based method using Conditional\nRandom Field (CRF) in order to encode local properties and their spatial\nrelationship in images to address texture classification, face recognition, and\nscene classification problems. Typical image classification approaches work\nwithout considering spatial causality among distinctive properties of an image\nfor image representation in feature space. In this method first, each image is\nencoded as a sequence of ordered patches, including local properties. Second,\nthe sequence of these ordered patches is modeled as a probabilistic feature\nvector by CRF to model spatial relationship of these local properties. And\nfinally, image classification is performed on such probabilistic image\nrepresentation. Experimental results on several standard image datasets\nindicate that proposed method outperforms some of existing image classification\nmethods.\n", "rewritten_text": "This paper proposes an ordered patch-based method using Conditional Random Fields (CRFs) to encode local image properties and their spatial relationships for texture, face, and scene classification.  Unlike typical image classification approaches, which ignore spatial causality among features, our method first encodes each image as a sequence of ordered patches representing local properties.  Second, a CRF models this sequence as a probabilistic feature vector, capturing the spatial relationships between these properties. Finally, image classification is performed using this probabilistic representation.  Experimental results on several standard datasets demonstrate that our method outperforms several existing image classification techniques.\n"}, "2305.15929": {"original_text": "  Current large language models, such as OpenAI's ChatGPT, have captured the\npublic's attention because how remarkable they are in the use of language.\nHere, I demonstrate that ChatGPT displays phonological biases that are a\nhallmark of human language processing. More concretely, just like humans,\nChatGPT has a consonant bias. That is, the chatbot has a tendency to use\nconsonants over vowels to identify words. This is observed across languages\nthat differ in their relative distribution of consonants and vowels such as\nEnglish and Spanish. Despite the differences in how current artificial\nintelligence language models are trained to process linguistic stimuli and how\nhuman infants acquire language, such training seems to be enough for the\nemergence of a phonological bias in ChatGPT\n", "rewritten_text": "Large language models (LLMs) like OpenAI's ChatGPT have captivated the public with their remarkable linguistic abilities.  This paper demonstrates that ChatGPT exhibits phonological biases characteristic of human language processing. Specifically, like humans, ChatGPT displays a consonant bias, exhibiting a preference for consonants over vowels in word identification. This bias is observed across languages with varying consonant-vowel ratios, including English and Spanish.  Despite differences between the training methods of current AI language models and human language acquisition,  ChatGPT's training appears sufficient to elicit this phonological bias.\n"}, "2008.13196": {"original_text": "  The task of spatial-temporal action detection has attracted increasing\nattention among researchers. Existing dominant methods solve this problem by\nrelying on short-term information and dense serial-wise detection on each\nindividual frames or clips. Despite their effectiveness, these methods showed\ninadequate use of long-term information and are prone to inefficiency. In this\npaper, we propose for the first time, an efficient framework that generates\naction tube proposals from video streams with a single forward pass in a\nsparse-to-dense manner. There are two key characteristics in this framework:\n(1) Both long-term and short-term sampled information are explicitly utilized\nin our spatiotemporal network, (2) A new dynamic feature sampling module (DTS)\nis designed to effectively approximate the tube output while keeping the system\ntractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 and\nUCFSports benchmark datasets, achieving promising results that are competitive\nto state-of-the-art methods. The proposed sparse-to-dense strategy rendered our\nframework about 7.6 times more efficient than the nearest competitor.\n", "rewritten_text": "Spatial-temporal action detection has attracted increasing research attention.  Existing dominant methods rely on short-term information and perform dense, serial detection on individual frames or clips. While effective, these methods underutilize long-term information and are computationally inefficient.  This paper introduces a novel, efficient framework that generates action tube proposals from video streams using a single, sparse-to-dense forward pass.  This framework has two key characteristics: (1) explicit utilization of both long-term and short-term sampled information within a spatiotemporal network, and (2) a novel dynamic feature sampling module (DTS) that efficiently approximates tube outputs while maintaining computational tractability.  Evaluated on the UCF101-24, JHMDB-21, and UCF Sports benchmark datasets, our model achieves state-of-the-art results.  Furthermore, our sparse-to-dense strategy yields a 7.6x speed improvement over the fastest competing method.\n"}, "2010.07574": {"original_text": "  Evaluation of grammatical error correction (GEC) systems has primarily\nfocused on essays written by non-native learners of English, which however is\nonly part of the full spectrum of GEC applications. We aim to broaden the\ntarget domain of GEC and release CWEB, a new benchmark for GEC consisting of\nwebsite text generated by English speakers of varying levels of proficiency.\nWebsite data is a common and important domain that contains far fewer\ngrammatical errors than learner essays, which we show presents a challenge to\nstate-of-the-art GEC systems. We demonstrate that a factor behind this is the\ninability of systems to rely on a strong internal language model in low error\ndensity domains. We hope this work shall facilitate the development of\nopen-domain GEC models that generalize to different topics and genres.\n", "rewritten_text": "Grammatical error correction (GEC) system evaluation has primarily focused on essays written by non-native English speakers.  However, this represents only a subset of GEC applications.  To broaden the target domain, we introduce CWEB, a new GEC benchmark comprising website text authored by English speakers with varying proficiency levels.  Website text, a common and important domain, typically contains far fewer grammatical errors than learner essays, posing a significant challenge to state-of-the-art GEC systems, as we demonstrate.  This challenge stems, in part, from the systems' inability to leverage strong internal language models in low-error-density domains.  We anticipate that this work will facilitate the development of open-domain GEC models capable of generalizing across diverse topics and genres.\n"}, "2306.0045": {"original_text": "  Semantic segmentation is a crucial task in computer vision that involves\nsegmenting images into semantically meaningful regions at the pixel level.\nHowever, existing approaches often rely on expensive human annotations as\nsupervision for model training, limiting their scalability to large, unlabeled\ndatasets. To address this challenge, we present ZeroSeg, a novel method that\nleverages the existing pretrained vision-language (VL) model (e.g. CLIP) to\ntrain open-vocabulary zero-shot semantic segmentation models. Although acquired\nextensive knowledge of visual concepts, it is non-trivial to exploit knowledge\nfrom these VL models to the task of semantic segmentation, as they are usually\ntrained at an image level. ZeroSeg overcomes this by distilling the visual\nconcepts learned by VL models into a set of segment tokens, each summarizing a\nlocalized region of the target image. We evaluate ZeroSeg on multiple popular\nsegmentation benchmarks, including PASCAL VOC 2012, PASCAL Context, and COCO,\nin a zero-shot manner (i.e., no training or adaption on target segmentation\ndatasets). Our approach achieves state-of-the-art performance when compared to\nother zero-shot segmentation methods under the same training data, while also\nperforming competitively compared to strongly supervised methods. Finally, we\nalso demonstrated the effectiveness of ZeroSeg on open-vocabulary segmentation,\nthrough both human studies and qualitative visualizations.\n", "rewritten_text": "Semantic segmentation, a crucial task in computer vision, involves partitioning images into semantically meaningful regions at the pixel level.  Existing approaches, however, often rely on expensive human annotations for training, limiting their scalability to large, unlabeled datasets.  To address this, we introduce ZeroSeg, a novel method leveraging pretrained vision-language (VL) models (e.g., CLIP) to train open-vocabulary, zero-shot semantic segmentation models.  While VL models possess extensive knowledge of visual concepts, adapting this knowledge to pixel-level segmentation\u2014typically trained at the image level\u2014is non-trivial.  ZeroSeg overcomes this limitation by distilling the visual concepts learned by VL models into segment tokens, each representing a localized image region.  We evaluate ZeroSeg on several popular segmentation benchmarks (PASCAL VOC 2012, PASCAL Context, and COCO) in a zero-shot setting (i.e., without training or adaptation on target segmentation datasets).  Our approach achieves state-of-the-art zero-shot performance, and also performs competitively against strongly supervised methods.  Finally, we demonstrate ZeroSeg's effectiveness in open-vocabulary segmentation through both quantitative evaluation and qualitative visualizations.\n"}, "2401.05166": {"original_text": "  In dyadic interactions, humans communicate their intentions and state of mind\nusing verbal and non-verbal cues, where multiple different facial reactions\nmight be appropriate in response to a specific speaker behaviour. Then, how to\ndevelop a machine learning (ML) model that can automatically generate multiple\nappropriate, diverse, realistic and synchronised human facial reactions from an\npreviously unseen speaker behaviour is a challenging task. Following the\nsuccessful organisation of the first REACT challenge (REACT 2023), this edition\nof the challenge (REACT 2024) employs a subset used by the previous challenge,\nwhich contains segmented 30-secs dyadic interaction clips originally recorded\nas part of the NOXI and RECOLA datasets, encouraging participants to develop\nand benchmark Machine Learning (ML) models that can generate multiple\nappropriate facial reactions (including facial image sequences and their\nattributes) given an input conversational partner's stimulus under various\ndyadic video conference scenarios. This paper presents: (i) the guidelines of\nthe REACT 2024 challenge; (ii) the dataset utilized in the challenge; and (iii)\nthe performance of the baseline systems on the two proposed sub-challenges:\nOffline Multiple Appropriate Facial Reaction Generation and Online Multiple\nAppropriate Facial Reaction Generation, respectively. The challenge baseline\ncode is publicly available at\nhttps://github.com/reactmultimodalchallenge/baseline_react2024.\n", "rewritten_text": "Human dyadic interactions involve communicating intentions and mental states through verbal and nonverbal cues, with multiple appropriate facial reactions often possible in response to a single speaker behavior.  Developing a machine learning (ML) model capable of automatically generating multiple appropriate, diverse, realistic, and synchronized human facial reactions to unseen speaker behavior is a significant challenge.  Building on the success of the first REACT challenge (REACT 2023), this year's challenge (REACT 2024) uses a subset of the previous challenge's data: 30-second segmented clips from dyadic interactions originally recorded as part of the NOXI and RECOLA datasets.  This encourages participants to develop and benchmark ML models that generate multiple appropriate facial reactions (including facial image sequences and attributes) in response to a conversational partner's stimulus across various video conferencing scenarios.  This paper presents: (i) the REACT 2024 challenge guidelines; (ii) the dataset; and (iii) baseline system performance on two sub-challenges: Offline and Online Multiple Appropriate Facial Reaction Generation. The challenge baseline code is publicly available at https://github.com/reactmultimodalchallenge/baseline_react2024.\n"}, "2406.10323": {"original_text": "  Most public instruction finetuning datasets are relatively small compared to\nthe closed source datasets used to train industry models. To study questions\nabout finetuning at scale, such as curricula and learning rate cooldown\nschedules, there is a need for industrial-scale datasets. However, this scale\nnecessitates a data generation process that is almost entirely automated. In\nthis work, we study methods for generating large instruction datasets from a\nsingle prompt. With little human oversight, we get LLMs to write diverse sets\nof instruction examples ranging from simple completion tasks to complex\nmulti-turn dialogs across a variety of subject areas. When finetuning a Llama-3\n8B base model, our dataset meets or exceeds both WizardLM and Ultrachat on both\nknowledge-intensive leaderboard tasks as well as conversational evaluations. We\nrelease our dataset, the \"generator\" prompts that created it, and our finetuned\nmodel checkpoints.\n", "rewritten_text": "Publicly available instruction-tuning datasets are significantly smaller than the proprietary datasets used to train large language models (LLMs) in industry.  To effectively study large-scale finetuning strategies, such as curriculum learning and learning rate scheduling, industrial-scale datasets are required.  This necessitates a highly automated data generation process.  This work explores methods for generating large instruction datasets from a single prompt.  With minimal human intervention, we leverage LLMs to produce diverse instruction examples, ranging from simple completion tasks to complex multi-turn dialogues across various subject domains.  When finetuned on a Llama-3 8B base model, our dataset achieves performance exceeding or matching both WizardLM and Ultrachat on knowledge-intensive benchmark tasks and conversational evaluations.  We publicly release our dataset, the generative prompts used, and the finetuned model checkpoints.\n"}, "2303.12776": {"original_text": "  One-to-one label assignment in object detection has successfully obviated the\nneed for non-maximum suppression (NMS) as postprocessing and makes the pipeline\nend-to-end. However, it triggers a new dilemma as the widely used sparse\nqueries cannot guarantee a high recall, while dense queries inevitably bring\nmore similar queries and encounter optimization difficulties. As both sparse\nand dense queries are problematic, then what are the expected queries in\nend-to-end object detection? This paper shows that the solution should be Dense\nDistinct Queries (DDQ). Concretely, we first lay dense queries like traditional\ndetectors and then select distinct ones for one-to-one assignments. DDQ blends\nthe advantages of traditional and recent end-to-end detectors and significantly\nimproves the performance of various detectors including FCN, R-CNN, and DETRs.\nMost impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12\nepochs using a ResNet-50 backbone, outperforming all existing detectors in the\nsame setting. DDQ also shares the benefit of end-to-end detectors in crowded\nscenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchers\nto consider the complementarity between traditional methods and end-to-end\ndetectors. The source code can be found at\n\\url{https://github.com/jshilong/DDQ}.\n", "rewritten_text": "One-to-one label assignment in object detection eliminates the need for non-maximum suppression (NMS) post-processing, enabling end-to-end pipelines.  However, this introduces a challenge: while sparse queries limit recall, dense queries lead to redundant predictions and optimization difficulties.  To address this inherent trade-off in end-to-end object detection, we propose Dense Distinct Queries (DDQ).  DDQ initially generates dense queries, similar to traditional detectors, and then selects a distinct subset for one-to-one assignment. This approach combines the strengths of traditional and end-to-end methods, significantly improving performance across various detectors, including FCN, R-CNN, and DETR.  Remarkably, DDQ-DETR achieves 52.1 AP on the MS-COCO dataset within 12 epochs using a ResNet-50 backbone, surpassing all existing detectors under the same conditions.  Furthermore, DDQ maintains the robustness of end-to-end detectors in crowded scenes, achieving 93.8 AP on CrowdHuman.  We believe DDQ offers a valuable approach that highlights the synergistic potential of traditional and end-to-end object detection methods.  The source code is available at [https://github.com/jshilong/DDQ](https://github.com/jshilong/DDQ).\n"}, "2303.1112": {"original_text": "  Positional reasoning is the process of ordering unsorted parts contained in a\nset into a consistent structure. We present Positional Diffusion, a\nplug-and-play graph formulation with Diffusion Probabilistic Models to address\npositional reasoning. We use the forward process to map elements' positions in\na set to random positions in a continuous space. Positional Diffusion learns to\nreverse the noising process and recover the original positions through an\nAttention-based Graph Neural Network. We conduct extensive experiments with\nbenchmark datasets including two puzzle datasets, three sentence ordering\ndatasets, and one visual storytelling dataset, demonstrating that our method\noutperforms long-lasting research on puzzle solving with up to +18% compared to\nthe second-best deep learning method, and performs on par against the\nstate-of-the-art methods on sentence ordering and visual storytelling. Our work\nhighlights the suitability of diffusion models for ordering problems and\nproposes a novel formulation and method for solving various ordering tasks.\nProject website at https://iit-pavis.github.io/Positional_Diffusion/\n", "rewritten_text": "Positional reasoning involves ordering the unsorted elements of a set into a consistent structure.  We introduce Positional Diffusion, a plug-and-play graph formulation leveraging Diffusion Probabilistic Models to address this challenge.  Our approach uses a forward process to map element positions within a set to random positions in a continuous space. Positional Diffusion then learns to reverse this \"noising\" process, recovering the original positions via an attention-based graph neural network.  Extensive experiments on benchmark datasets\u2014including two puzzle datasets, three sentence ordering datasets, and one visual storytelling dataset\u2014demonstrate that our method surpasses existing puzzle-solving research, achieving up to an 18% improvement over the second-best deep learning approach.  Furthermore, it performs comparably to state-of-the-art methods on sentence ordering and visual storytelling tasks.  This work highlights the effectiveness of diffusion models for ordering problems and proposes a novel formulation and method applicable to diverse ordering tasks.  See our project website for more details: https://iit-pavis.github.io/Positional_Diffusion/\n"}, "2311.13735": {"original_text": "  Recent advances in large language models (LLMs) show potential for clinical\napplications, such as clinical decision support and trial recommendations.\nHowever, the GPT-4 LLM predicts an excessive number of ICD codes for medical\ncoding tasks, leading to high recall but low precision. To tackle this\nchallenge, we introduce LLM-codex, a two-stage approach to predict ICD codes\nthat first generates evidence proposals using an LLM and then employs an\nLSTM-based verification stage. The LSTM learns from both the LLM's high recall\nand human expert's high precision, using a custom loss function. Our model is\nthe only approach that simultaneously achieves state-of-the-art results in\nmedical coding accuracy, accuracy on rare codes, and sentence-level evidence\nidentification to support coding decisions without training on human-annotated\nevidence according to experiments on the MIMIC dataset.\n", "rewritten_text": "Recent advances in large language models (LLMs) offer significant potential for clinical applications, including clinical decision support and trial recommendations.  However, LLMs like GPT-4 exhibit a tendency toward excessive ICD code prediction in medical coding tasks, resulting in high recall but low precision.  To address this, we introduce LLM-codex, a two-stage approach.  First, an LLM generates evidence proposals; second, an LSTM-based verification stage refines these proposals.  This LSTM leverages both the LLM's high recall and the high precision of human experts, employing a custom loss function.  Our experiments on the MIMIC dataset demonstrate that LLM-codex achieves state-of-the-art performance in medical coding accuracy, accuracy on rare codes, and sentence-level evidence identification to support coding decisions\u2014all without requiring training on human-annotated evidence.\n"}, "2209.14575": {"original_text": "  In this paper, we investigate the problem of bit allocation in Neural Video\nCompression (NVC). First, we reveal that a recent bit allocation approach\nclaimed to be optimal is, in fact, sub-optimal due to its implementation.\nSpecifically, we find that its sub-optimality lies in the improper application\nof semi-amortized variational inference (SAVI) on latent with non-factorized\nvariational posterior. Then, we show that the corrected version of SAVI on\nnon-factorized latent requires recursively applying back-propagating through\ngradient ascent, based on which we derive the corrected optimal bit allocation\nalgorithm. Due to the computational in-feasibility of the corrected bit\nallocation, we design an efficient approximation to make it practical.\nEmpirical results show that our proposed correction significantly improves the\nincorrect bit allocation in terms of R-D performance and bitrate error, and\noutperforms all other bit allocation methods by a large margin. The source code\nis provided in the supplementary material.\n", "rewritten_text": "This paper investigates bit allocation in Neural Video Compression (NVC). We demonstrate that a recently proposed, purportedly optimal bit allocation approach is suboptimal due to implementation flaws.  Specifically, its suboptimality stems from the improper application of semi-amortized variational inference (SAVI) to latent variables with a non-factorized variational posterior. We present a corrected SAVI approach for non-factorized latent variables, requiring recursive backpropagation through gradient ascent. This leads to a corrected optimal bit allocation algorithm.  However, due to its computational complexity, we develop an efficient approximation.  Empirical results demonstrate that our corrected algorithm significantly improves rate-distortion (R-D) performance and reduces bitrate error, substantially outperforming all other bit allocation methods.  The source code is available in the supplementary material.\n"}, "2302.03744": {"original_text": "  The ability to perceive and understand 3D scenes is crucial for many\napplications in computer vision and robotics. Inverse graphics is an appealing\napproach to 3D scene understanding that aims to infer the 3D scene structure\nfrom 2D images. In this paper, we introduce probabilistic modeling to the\ninverse graphics framework to quantify uncertainty and achieve robustness in 6D\npose estimation tasks. Specifically, we propose 3D Neural Embedding Likelihood\n(3DNEL) as a unified probabilistic model over RGB-D images, and develop\nefficient inference procedures on 3D scene descriptions. 3DNEL effectively\ncombines learned neural embeddings from RGB with depth information to improve\nrobustness in sim-to-real 6D object pose estimation from RGB-D images.\nPerformance on the YCB-Video dataset is on par with state-of-the-art yet is\nmuch more robust in challenging regimes. In contrast to discriminative\napproaches, 3DNEL's probabilistic generative formulation jointly models\nmultiple objects in a scene, quantifies uncertainty in a principled way, and\nhandles object pose tracking under heavy occlusion. Finally, 3DNEL provides a\nprincipled framework for incorporating prior knowledge about the scene and\nobjects, which allows natural extension to additional tasks like camera pose\ntracking from video.\n", "rewritten_text": "Accurate 3D scene perception is crucial for numerous computer vision and robotics applications.  Inverse graphics offers an attractive approach to 3D scene understanding by inferring 3D structure from 2D images. This paper introduces a probabilistic modeling framework to enhance the robustness and uncertainty quantification of inverse graphics, specifically for 6D pose estimation.  We propose 3D Neural Embedding Likelihood (3DNEL), a unified probabilistic model operating on RGB-D images, and develop efficient inference procedures for 3D scene descriptions.  3DNEL effectively integrates learned neural embeddings from RGB data with depth information, improving robustness in sim-to-real 6D object pose estimation from RGB-D images.  Our results on the YCB-Video dataset achieve state-of-the-art performance while exhibiting significantly improved robustness in challenging scenarios. Unlike discriminative methods, 3DNEL's probabilistic generative formulation jointly models multiple objects, provides principled uncertainty quantification, and handles object pose tracking even under heavy occlusion.  Furthermore, its framework readily incorporates prior scene and object knowledge, enabling extensions to tasks such as camera pose tracking from video.\n"}, "2005.04551": {"original_text": "  A common approach to localize 3D human joints in a synchronized and\ncalibrated multi-view setup consists of two-steps: (1) apply a 2D detector\nseparately on each view to localize joints in 2D, and (2) perform robust\ntriangulation on 2D detections from each view to acquire the 3D joint\nlocations. However, in step 1, the 2D detector is limited to solving\nchallenging cases which could potentially be better resolved in 3D, such as\nocclusions and oblique viewing angles, purely in 2D without leveraging any 3D\ninformation. Therefore, we propose the differentiable \"epipolar transformer\",\nwhich enables the 2D detector to leverage 3D-aware features to improve 2D pose\nestimation. The intuition is: given a 2D location p in the current view, we\nwould like to first find its corresponding point p' in a neighboring view, and\nthen combine the features at p' with the features at p, thus leading to a\n3D-aware feature at p. Inspired by stereo matching, the epipolar transformer\nleverages epipolar constraints and feature matching to approximate the features\nat p'. Experiments on InterHand and Human3.6M show that our approach has\nconsistent improvements over the baselines. Specifically, in the condition\nwhere no external data is used, our Human3.6M model trained with ResNet-50\nbackbone and image size 256 x 256 outperforms state-of-the-art by 4.23 mm and\nachieves MPJPE 26.9 mm.\n", "rewritten_text": "A common approach to localizing 3D human joints in a synchronized, calibrated multi-view setup involves two steps: (1) applying a 2D joint detector independently to each view, and (2) performing robust triangulation on the resulting 2D detections to estimate 3D joint locations.  However, this two-step approach limits the 2D detector's ability to handle challenging cases, such as occlusions and oblique viewing angles, which are often better resolved using 3D information.  To address this limitation, we propose a differentiable \"epipolar transformer\" that enables the 2D detector to leverage 3D-aware features, thereby improving 2D pose estimation.  Our approach identifies a 2D point *p* in the current view, finds its corresponding point *p'* in a neighboring view using epipolar constraints and feature matching, and then combines the features at *p* and *p'* to create a 3D-aware feature at *p*.  Experiments on the InterHand and Human3.6M datasets demonstrate consistent improvements over baseline methods.  Specifically, without using external data, our Human3.6M model (ResNet-50 backbone, 256x256 input images) outperforms the state-of-the-art by 4.23 mm, achieving a mean per-joint position error (MPJPE) of 26.9 mm.\n"}, "1912.0643": {"original_text": "  Annotating videos is cumbersome, expensive and not scalable. Yet, many strong\nvideo models still rely on manually annotated data. With the recent\nintroduction of the HowTo100M dataset, narrated videos now offer the\npossibility of learning video representations without manual supervision. In\nthis work we propose a new learning approach, MIL-NCE, capable of addressing\nmisalignments inherent to narrated videos. With this approach we are able to\nlearn strong video representations from scratch, without the need for any\nmanual annotation. We evaluate our representations on a wide range of four\ndownstream tasks over eight datasets: action recognition (HMDB-51, UCF-101,\nKinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization\n(YouTube-8M Segments, CrossTask) and action segmentation (COIN). Our method\noutperforms all published self-supervised approaches for these tasks as well as\nseveral fully supervised baselines.\n", "rewritten_text": "Video annotation is cumbersome, expensive, and lacks scalability.  However, many high-performing video models still rely on manually annotated data.  The recent introduction of the HowTo100M dataset offers a new possibility: learning video representations from narrated videos without manual supervision.  This work proposes MIL-NCE, a novel learning approach designed to address the inherent misalignments in narrated videos.  This approach enables the learning of robust video representations from scratch, eliminating the need for manual annotation.  We evaluate these representations on eight datasets across four downstream tasks: action recognition (HMDB-51, UCF-101, Kinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization (YouTube-8M Segments, CrossTask), and action segmentation (COIN).  Our method surpasses all published self-supervised approaches and several fully supervised baselines on these tasks.\n"}, "2211.09795": {"original_text": "  Diffusion models have become the go-to method for many generative tasks,\nparticularly for image-to-image generation tasks such as super-resolution and\ninpainting. Current diffusion-based methods do not provide statistical\nguarantees regarding the generated results, often preventing their use in\nhigh-stakes situations. To bridge this gap, we construct a confidence interval\naround each generated pixel such that the true value of the pixel is guaranteed\nto fall within the interval with a probability set by the user. Since diffusion\nmodels parametrize the data distribution, a straightforward way of constructing\nsuch intervals is by drawing multiple samples and calculating their bounds.\nHowever, this method has several drawbacks: i) slow sampling speeds ii)\nsuboptimal bounds iii) requires training a diffusion model per task. To\nmitigate these shortcomings we propose Conffusion, wherein we fine-tune a\npre-trained diffusion model to predict interval bounds in a single forward\npass. We show that Conffusion outperforms the baseline method while being three\norders of magnitude faster.\n", "rewritten_text": "Diffusion models have become the leading method for many generative tasks, especially image-to-image generation such as super-resolution and inpainting.  However, current diffusion-based methods lack statistical guarantees on their generated results, limiting their applicability in high-stakes scenarios.  To address this, we propose Conffusion, a method that constructs a confidence interval around each generated pixel, guaranteeing that the true pixel value falls within the interval with a user-specified probability.\n\nWhile diffusion models parameterize the data distribution, directly constructing these intervals by repeatedly sampling and calculating bounds is inefficient. This approach suffers from slow sampling speeds, suboptimal bounds, and the need to train a separate diffusion model for each task.\n\nConffusion overcomes these limitations by fine-tuning a pre-trained diffusion model to predict interval bounds in a single forward pass.  Our results demonstrate that Conffusion significantly outperforms the baseline sampling method, achieving a speed improvement of three orders of magnitude.\n"}, "2404.14248": {"original_text": "  This paper reviews the NTIRE 2024 low light image enhancement challenge,\nhighlighting the proposed solutions and results. The aim of this challenge is\nto discover an effective network design or solution capable of generating\nbrighter, clearer, and visually appealing results when dealing with a variety\nof conditions, including ultra-high resolution (4K and beyond), non-uniform\nillumination, backlighting, extreme darkness, and night scenes. A notable total\nof 428 participants registered for the challenge, with 22 teams ultimately\nmaking valid submissions. This paper meticulously evaluates the\nstate-of-the-art advancements in enhancing low-light images, reflecting the\nsignificant progress and creativity in this field.\n", "rewritten_text": "This paper reviews the NTIRE 2024 low-light image enhancement challenge, analyzing the proposed solutions and their results.  The challenge aimed to identify effective network designs capable of producing brighter, clearer, and more visually appealing images under diverse conditions, including ultra-high resolution (4K and above), non-uniform illumination, backlighting, extreme darkness, and night scenes.  A remarkable 428 participants registered, with 22 teams submitting valid entries.  This paper meticulously evaluates these submissions, showcasing significant advancements and creative approaches in state-of-the-art low-light image enhancement.\n"}, "2311.01907": {"original_text": "  Automatic simplification can help laypeople to comprehend complex scientific\ntext. Language models are frequently applied to this task by translating from\ncomplex to simple language. In this paper, we describe our system based on\nLlama 2, which ranked first in the PLABA shared task addressing the\nsimplification of biomedical text. We find that the large portion of shared\ntokens between input and output leads to weak training signals and\nconservatively editing models. To mitigate these issues, we propose\nsentence-level and token-level loss weights. They give higher weight to\nmodified tokens, indicated by edit distance and edit operations, respectively.\nWe conduct an empirical evaluation on the PLABA dataset and find that both\napproaches lead to simplifications closer to those created by human annotators\n(+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x /\n1.8x edit distance) compared to the same model fine-tuned with standard cross\nentropy. We furthermore show that the hyperparameter $\\lambda$ in token-level\nloss weights can be used to control the edit distance and the simplicity level\n(FKGL).\n", "rewritten_text": "Automatic simplification facilitates layperson comprehension of complex scientific texts.  Language models are frequently employed for this task, translating complex language into simpler forms. This paper describes our Llama 2-based system, which achieved first place in the PLABA shared task for biomedical text simplification.  We observed that the high proportion of shared tokens between input and output resulted in weak training signals and conservatively edited outputs. To address this, we propose sentence-level and token-level loss weighting schemes that assign greater weight to modified tokens, identified via edit distance and edit operations, respectively.  Empirical evaluation on the PLABA dataset demonstrates that both approaches yield simplifications closer to human annotations (+1.8%/+3.5% SARI), simpler language (\u22121/\u22121.1 FKGL), and a greater number of edits (1.6x/1.8x edit distance) compared to the same model fine-tuned with standard cross-entropy.  Furthermore, we show that the hyperparameter \u03bb in the token-level loss weighting scheme controls both edit distance and simplicity level (FKGL).\n"}, "2402.12923": {"original_text": "  In recent years, 3D point clouds (PCs) have gained significant attention due\nto their diverse applications across various fields, such as computer vision\n(CV), condition monitoring (CM), virtual reality, robotics, autonomous driving,\netc. Deep learning (DL) has proven effective in leveraging 3D PCs to address\nvarious challenges encountered in 2D vision. However, applying deep neural\nnetworks (DNNs) to process 3D PCs presents unique challenges. This paper\nprovides an in-depth review of recent advancements in DL-based industrial CM\nusing 3D PCs, with a specific focus on defect shape classification and\nsegmentation within industrial applications. Recognizing the crucial role of\nthese aspects in industrial maintenance, the paper offers insightful\nobservations on the strengths and limitations of the reviewed DL-based PC\nprocessing methods. This knowledge synthesis aims to contribute to\nunderstanding and enhancing CM processes, particularly within the framework of\nremaining useful life (RUL), in industrial systems.\n", "rewritten_text": "Three-dimensional point clouds (PCs) have gained significant traction in recent years due to their widespread application across diverse fields, including computer vision (CV), condition monitoring (CM), virtual reality, robotics, and autonomous driving.  While deep learning (DL) has proven effective in addressing challenges previously encountered in 2D vision, applying deep neural networks (DNNs) to 3D PCs presents unique complexities. This paper provides a comprehensive review of recent advancements in DL-based industrial CM using 3D PCs, focusing specifically on defect shape classification and segmentation within industrial applications.  Given the critical role of these aspects in industrial maintenance, the paper offers insightful analysis of the strengths and limitations of reviewed DL-based PC processing methods. This synthesis aims to improve understanding and enhance CM processes, particularly concerning remaining useful life (RUL) prediction in industrial systems.\n"}, "2402.02082": {"original_text": "  Speculative decoding is a relatively new decoding framework that leverages\nsmall and efficient draft models to reduce the latency of LLMs. In this study,\nwe introduce GliDe and CaPE, two low-hassle modifications to vanilla\nspeculative decoding to further improve the decoding speed of a frozen LLM.\nSpecifically, GliDe is a modified draft model architecture that reuses the\ncached keys and values from the target LLM, while CaPE is a proposal expansion\nmethod that uses the draft model's confidence scores to help select additional\ncandidate tokens for verification. Extensive experiments on different\nbenchmarks demonstrate that our proposed GliDe draft model significantly\nreduces the expected decoding latency. Additional evaluation using walltime\nreveals that GliDe can accelerate Vicuna models up to 2.17x and further extend\nthe improvement to 2.61x with CaPE. We will release our code, data, and the\ntrained draft models.\n", "rewritten_text": "Speculative decoding, a novel framework leveraging small, efficient draft models to reduce large language model (LLM) latency, is presented.  This study introduces GliDe and CaPE, two straightforward modifications to vanilla speculative decoding that further enhance the decoding speed of frozen LLMs.  GliDe is a modified draft model architecture that reuses cached keys and values from the target LLM, while CaPE is a proposal expansion method employing the draft model's confidence scores to select additional candidate tokens for verification.  Extensive benchmarking demonstrates that the proposed GliDe draft model significantly reduces expected decoding latency.  Wall-clock time evaluations reveal that GliDe accelerates Vicuna models by up to 2.17x, further increasing to 2.61x with the addition of CaPE.  The code, data, and trained draft models will be publicly released.\n"}, "2302.14354": {"original_text": "  The cultural heritage buildings (CHB), which are part of mankind's history\nand identity, are in constant danger of damage or in extreme situations total\ndestruction. That being said, it's of utmost importance to preserve them by\nidentifying the existent, or presumptive, defects using novel methods so that\nrenovation processes can be done in a timely manner and with higher accuracy.\nThe main goal of this research is to use new deep learning (DL) methods in the\nprocess of preserving CHBs (situated in Iran); a goal that has been neglected\nespecially in developing countries such as Iran, as these countries still\npreserve their CHBs using manual, and even archaic, methods that need direct\nhuman supervision. Having proven their effectiveness and performance when it\ncomes to processing images, the convolutional neural networks (CNN) are a\nstaple in computer vision (CV) literacy and this paper is not exempt. When\nlacking enough CHB images, training a CNN from scratch would be very difficult\nand prone to overfitting; that's why we opted to use a technique called\ntransfer learning (TL) in which we used pre-trained ResNet, MobileNet, and\nInception networks, for classification. Even more, the Grad-CAM was utilized to\nlocalize the defects to some extent. The final results were very favorable\nbased on those of similar research. The final proposed model can pave the way\nfor moving from manual to unmanned CHB conservation, hence an increase in\naccuracy and a decrease in human-induced errors.\n", "rewritten_text": "Cultural heritage buildings (CHBs), integral to humanity's history and identity, face constant threats of damage, and in extreme cases, complete destruction.  Preserving these structures requires timely and accurate identification of existing and potential defects using innovative methods. This research employs novel deep learning (DL) techniques to address this critical need, particularly within the context of Iran's CHBs.  Developing countries like Iran often rely on manual, even archaic, methods requiring extensive human oversight, a deficiency this research aims to overcome.\n\nConvolutional neural networks (CNNs), proven highly effective in image processing and a cornerstone of computer vision (CV), form the basis of our approach.  However, the scarcity of CHB images makes training a CNN from scratch impractical and prone to overfitting.  Therefore, we utilize transfer learning (TL), employing pre-trained ResNet, MobileNet, and Inception networks for defect classification.  Furthermore, Grad-CAM was implemented to aid in defect localization.  The results favorably compare to similar research.  Our proposed model offers a pathway towards automated CHB conservation, promising increased accuracy and reduced human error.\n"}, "2303.0149": {"original_text": "  Language identification is an important first step in many IR and NLP\napplications. Most publicly available language identification datasets,\nhowever, are compiled under the assumption that the gold label of each instance\nis determined by where texts are retrieved from. Research has shown that this\nis a problematic assumption, particularly in the case of very similar languages\n(e.g., Croatian and Serbian) and national language varieties (e.g., Brazilian\nand European Portuguese), where texts may contain no distinctive marker of the\nparticular language or variety. To overcome this important limitation, this\npaper presents DSL True Labels (DSL-TL), the first human-annotated multilingual\ndataset for language variety identification. DSL-TL contains a total of 12,900\ninstances in Portuguese, split between European Portuguese and Brazilian\nPortuguese; Spanish, split between Argentine Spanish and Castilian Spanish; and\nEnglish, split between American English and British English. We trained\nmultiple models to discriminate between these language varieties, and we\npresent the results in detail. The data and models presented in this paper\nprovide a reliable benchmark toward the development of robust and fairer\nlanguage variety identification systems. We make DSL-TL freely available to the\nresearch community.\n", "rewritten_text": "Accurate language identification is crucial for many information retrieval (IR) and natural language processing (NLP) applications.  However, most publicly available language identification datasets rely on the problematic assumption that a text's language is accurately reflected by its source. This assumption is particularly flawed for closely related languages (e.g., Croatian and Serbian) or national language varieties (e.g., Brazilian and European Portuguese), where distinguishing linguistic markers may be absent.  To address this limitation, we introduce DSL True Labels (DSL-TL), the first human-annotated multilingual dataset for language variety identification.  DSL-TL comprises 12,900 instances of Portuguese (European and Brazilian), Spanish (Argentine and Castilian), and English (American and British). We trained and evaluated several models on this dataset, presenting detailed results.  DSL-TL and the associated models provide a robust benchmark for developing fairer and more accurate language variety identification systems.  This dataset is freely available to the research community.\n"}, "2004.0827": {"original_text": "  In this paper, we tackle the task of automatically analyzing 3D volumetric\nscans obtained from computed tomography (CT) devices. In particular, we address\na particular task for which data is very limited: the segmentation of ancient\nEgyptian mummies CT scans. We aim at digitally unwrapping the mummy and\nidentify different segments such as body, bandages and jewelry. The problem is\ncomplex because of the lack of annotated data for the different semantic\nregions to segment, thus discouraging the use of strongly supervised\napproaches. We, therefore, propose a weakly supervised and efficient\ninteractive segmentation method to solve this challenging problem. After\nsegmenting the wrapped mummy from its exterior region using histogram analysis\nand template matching, we first design a voxel distance measure to find an\napproximate solution for the body and bandage segments. Here, we use geodesic\ndistances since voxel features as well as spatial relationship among voxels is\nincorporated in this measure. Next, we refine the solution using a GrabCut\nbased segmentation together with a tracking method on the slices of the scan\nthat assigns labels to different regions in the volume, using limited\nsupervision in the form of scribbles drawn by the user. The efficiency of the\nproposed method is demonstrated using visualizations and validated through\nquantitative measures and qualitative unwrapping of the mummy.\n", "rewritten_text": "This paper presents an automated method for analyzing three-dimensional (3D) volumetric scans from computed tomography (CT) devices, focusing on the challenging task of segmenting ancient Egyptian mummy CT scans.  Data scarcity for this task precludes the use of strongly supervised approaches.  Therefore, we propose a weakly supervised, efficient, interactive segmentation method.  Initially, we segment the mummy from its surroundings using histogram analysis and template matching.  We then employ a voxel distance measure\u2014specifically, geodesic distance\u2014to obtain an approximate segmentation of the body and bandages, leveraging both voxel features and spatial relationships.  This initial segmentation is refined using a GrabCut-based approach combined with a slice-wise tracking method guided by minimal user-provided scribbles.  The method's efficiency is demonstrated through visualizations and validated quantitatively and qualitatively by digitally unwrapping the mummy.\n"}, "1607.06871": {"original_text": "  The \"interpretation through synthesis\" approach to analyze face images,\nparticularly Active Appearance Models (AAMs) method, has become one of the most\nsuccessful face modeling approaches over the last two decades. AAM models have\nability to represent face images through synthesis using a controllable\nparameterized Principal Component Analysis (PCA) model. However, the accuracy\nand robustness of the synthesized faces of AAM are highly depended on the\ntraining sets and inherently on the generalizability of PCA subspaces. This\npaper presents a novel Deep Appearance Models (DAMs) approach, an efficient\nreplacement for AAMs, to accurately capture both shape and texture of face\nimages under large variations. In this approach, three crucial components\nrepresented in hierarchical layers are modeled using the Deep Boltzmann\nMachines (DBM) to robustly capture the variations of facial shapes and\nappearances. DAMs are therefore superior to AAMs in inferencing a\nrepresentation for new face images under various challenging conditions. The\nproposed approach is evaluated in various applications to demonstrate its\nrobustness and capabilities, i.e. facial super-resolution reconstruction,\nfacial off-angle reconstruction or face frontalization, facial occlusion\nremoval and age estimation using challenging face databases, i.e. Labeled Face\nParts in the Wild (LFPW), Helen and FG-NET. Comparing to AAMs and other deep\nlearning based approaches, the proposed DAMs achieve competitive results in\nthose applications, thus this showed their advantages in handling occlusions,\nfacial representation, and reconstruction.\n", "rewritten_text": "Active Appearance Models (AAMs), a prominent \"interpretation through synthesis\" approach to face image analysis, have been highly successful over the past two decades.  AAMs represent face images via a controllable, parameterized Principal Component Analysis (PCA) model. However, the accuracy and robustness of AAM-synthesized faces are heavily dependent on the training data and the inherent limitations of PCA subspaces' generalizability.  This paper introduces Deep Appearance Models (DAMs), an efficient alternative to AAMs, designed to accurately capture facial shape and texture under significant variations.  This approach models three crucial hierarchical components using Deep Boltzmann Machines (DBMs) to robustly capture variations in facial shapes and appearances.  DAMs thus outperform AAMs in inferring representations for novel face images under challenging conditions.\n\nThe proposed approach is evaluated on several applications\u2014facial super-resolution reconstruction, off-angle reconstruction (frontalization), occlusion removal, and age estimation\u2014using challenging datasets such as Labeled Face Parts in the Wild (LFPW), Helen, and FG-NET.  Compared to AAMs and other deep learning methods, DAMs achieve competitive results, demonstrating superior performance in handling occlusions, facial representation, and reconstruction.\n"}, "2105.11872": {"original_text": "  Despite recent advances, standard sequence labeling systems often fail when\nprocessing noisy user-generated text or consuming the output of an Optical\nCharacter Recognition (OCR) process. In this paper, we improve the noise-aware\ntraining method by proposing an empirical error generation approach that\nemploys a sequence-to-sequence model trained to perform translation from\nerror-free to erroneous text. Using an OCR engine, we generated a large\nparallel text corpus for training and produced several real-world noisy\nsequence labeling benchmarks for evaluation. Moreover, to overcome the data\nsparsity problem that exacerbates in the case of imperfect textual input, we\nlearned noisy language model-based embeddings. Our approach outperformed the\nbaseline noise generation and error correction techniques on the erroneous\nsequence labeling data sets. To facilitate future research on robustness, we\nmake our code, embeddings, and data conversion scripts publicly available.\n", "rewritten_text": "Standard sequence labeling systems, despite recent advancements, often fail when processing noisy user-generated text or Optical Character Recognition (OCR) output.  This paper introduces an improved noise-aware training method using an empirical error generation approach.  This approach leverages a sequence-to-sequence model trained to translate error-free text into erroneous text. We generated a large parallel text corpus for training using an OCR engine, and created several real-world noisy sequence labeling benchmarks for evaluation.  To address the data sparsity problem exacerbated by imperfect textual input, we incorporated noisy language model-based embeddings.  Our method outperformed baseline noise generation and error correction techniques on the erroneous sequence labeling datasets.  To foster further research into robustness, we have publicly released our code, embeddings, and data conversion scripts.\n"}, "2102.00062": {"original_text": "  In this paper, we present a method of clothes retargeting; generating the\npotential poses and deformations of a given 3D clothing template model to fit\nonto a person in a single RGB image. The problem is fundamentally ill-posed as\nattaining the ground truth data is impossible, i.e., images of people wearing\nthe different 3D clothing template model at exact same pose. We address this\nchallenge by utilizing large-scale synthetic data generated from physical\nsimulation, allowing us to map 2D dense body pose to 3D clothing deformation.\nWith the simulated data, we propose a semi-supervised learning framework that\nvalidates the physical plausibility of the 3D deformation by matching with the\nprescribed body-to-cloth contact points and clothing silhouette to fit onto the\nunlabeled real images. A new neural clothes retargeting network (CRNet) is\ndesigned to integrate the semi-supervised retargeting task in an end-to-end\nfashion. In our evaluation, we show that our method can predict the realistic\n3D pose and deformation field needed for retargeting clothes models in\nreal-world examples.\n", "rewritten_text": "This paper presents a novel method for clothing retargeting: generating plausible poses and deformations of a 3D clothing template to fit a person depicted in a single RGB image.  This is an inherently ill-posed problem, as obtaining ground truth data\u2014images of individuals wearing the same 3D clothing template in identical poses\u2014is infeasible. We address this limitation by leveraging a large-scale synthetic dataset generated through physically-based simulation, enabling us to map 2D dense body pose to 3D clothing deformation.  Using this simulated data, we propose a semi-supervised learning framework that enforces physical plausibility by matching predicted 3D deformations to prescribed body-to-cloth contact points and clothing silhouettes in unlabeled real images.  A new neural network, CRNet (Clothing Retargeting Network), is designed to perform this semi-supervised retargeting task end-to-end.  Our evaluation demonstrates that our method accurately predicts realistic 3D pose and deformation fields, enabling effective clothing model retargeting in real-world scenarios.\n"}, "1810.0578": {"original_text": "  Pose estimation is a widely explored problem, enabling many robotic tasks\nsuch as grasping and manipulation. In this paper, we tackle the problem of pose\nestimation for objects that exhibit rotational symmetry, which are common in\nman-made and industrial environments. In particular, our aim is to infer poses\nfor objects not seen at training time, but for which their 3D CAD models are\navailable at test time. Previous work has tackled this problem by learning to\ncompare captured views of real objects with the rendered views of their 3D CAD\nmodels, by embedding them in a joint latent space using neural networks. We\nshow that sidestepping the issue of symmetry in this scenario during training\nleads to poor performance at test time. We propose a model that reasons about\nrotational symmetry during training by having access to only a small set of\nsymmetry-labeled objects, whereby exploiting a large collection of unlabeled\nCAD models. We demonstrate that our approach significantly outperforms a\nnaively trained neural network on a new pose dataset containing images of tools\nand hardware.\n", "rewritten_text": "Pose estimation, crucial for numerous robotic tasks like grasping and manipulation, is a widely studied problem.  This paper addresses pose estimation for rotationally symmetric objects\u2014common in industrial and man-made environments\u2014specifically focusing on objects unseen during training but with available 3D CAD models at test time.  Prior work has addressed this by learning to compare real-object views with rendered CAD model views, embedding them in a shared latent space using neural networks.  However, we demonstrate that ignoring rotational symmetry during training leads to poor test-time performance.  We propose a model that explicitly reasons about rotational symmetry during training, leveraging a small set of symmetry-labeled objects to inform the processing of a large collection of unlabeled CAD models.  Our approach significantly outperforms a standard neural network on a novel pose estimation dataset comprising images of tools and hardware.\n"}, "2012.15832": {"original_text": "  Increasing the input length has been a driver of progress in language\nmodeling with transformers. We identify conditions where shorter inputs are not\nharmful, and achieve perplexity and efficiency improvements through two new\nmethods that decrease input length. First, we show that initially training a\nmodel on short subsequences before moving on to longer ones both reduces\noverall training time and, surprisingly, substantially improves perplexity.\nSecond, we show how to improve the efficiency of recurrence methods in\ntransformers, which let models condition on previously processed tokens when\ngenerating sequences that exceed the maximal length the transformer can handle\nat once. Existing methods require computationally expensive relative position\nembeddings; we introduce a simple alternative of adding absolute position\nembeddings to queries and keys instead of to word embeddings, which efficiently\nproduces superior results. We show that these recurrent models also benefit\nfrom short input lengths. Combining these techniques speeds up training by a\nfactor of 1.65, reduces memory usage, and substantially improves perplexity on\nWikiText-103, without adding any parameters.\n", "rewritten_text": "Progress in transformer-based language modeling has been driven by increasing input length.  However, we demonstrate that shorter inputs can be beneficial under certain conditions, achieving improvements in both perplexity and efficiency through two novel methods that reduce input length.  First, we show that pre-training a model on short subsequences before transitioning to longer ones significantly reduces training time and, unexpectedly, improves perplexity. Second, we introduce a more efficient approach to recurrent methods within transformers, enabling the handling of sequences exceeding the model's maximum input length.  Existing recurrent methods rely on computationally expensive relative position embeddings; our alternative uses absolute position embeddings added to query and key vectors instead of word embeddings, yielding superior results with improved efficiency.  We further demonstrate that these recurrent models also benefit from shorter input lengths.  Combining these techniques yields a 1.65x speedup in training, reduced memory consumption, and substantial perplexity improvements on WikiText-103, without increasing the model's parameter count.\n"}, "cs/0006021": {"original_text": "  Systems now exist which are able to compile unification grammars into\nlanguage models that can be included in a speech recognizer, but it is so far\nunclear whether non-trivial linguistically principled grammars can be used for\nthis purpose. We describe a series of experiments which investigate the\nquestion empirically, by incrementally constructing a grammar and discovering\nwhat problems emerge when successively larger versions are compiled into finite\nstate graph representations and used as language models for a medium-vocabulary\nrecognition task.\n", "rewritten_text": "Existing systems can compile unification grammars into language models for speech recognition. However, the feasibility of using non-trivial, linguistically principled grammars remains unclear.  We present a series of experiments that empirically investigate this question.  We incrementally construct a grammar, observing the problems that arise when successively larger versions are compiled into finite-state graph representations and used as language models in a medium-vocabulary speech recognition task.\n"}, "2209.13351": {"original_text": "  Accurately and timely detecting multiscale small objects that contain tens of\npixels from remote sensing images (RSI) remains challenging. Most of the\nexisting solutions primarily design complex deep neural networks to learn\nstrong feature representations for objects separated from the background, which\noften results in a heavy computation burden. In this article, we propose an\naccurate yet fast object detection method for RSI, named SuperYOLO, which fuses\nmultimodal data and performs high-resolution (HR) object detection on\nmultiscale objects by utilizing the assisted super resolution (SR) learning and\nconsidering both the detection accuracy and computation cost. First, we utilize\na symmetric compact multimodal fusion (MF) to extract supplementary information\nfrom various data for improving small object detection in RSI. Furthermore, we\ndesign a simple and flexible SR branch to learn HR feature representations that\ncan discriminate small objects from vast backgrounds with low-resolution (LR)\ninput, thus further improving the detection accuracy. Moreover, to avoid\nintroducing additional computation, the SR branch is discarded in the inference\nstage, and the computation of the network model is reduced due to the LR input.\nExperimental results show that, on the widely used VEDAI RS dataset, SuperYOLO\nachieves an accuracy of 75.09% (in terms of mAP50 ), which is more than 10%\nhigher than the SOTA large models, such as YOLOv5l, YOLOv5x, and RS designed\nYOLOrs. Meanwhile, the parameter size and GFLOPs of SuperYOLO are about 18\ntimes and 3.8 times less than YOLOv5x. Our proposed model shows a favorable\naccuracy and speed tradeoff compared to the state-of-the-art models. The code\nwill be open-sourced at https://github.com/icey-zhang/SuperYOLO.\n", "rewritten_text": "Accurate and timely detection of multiscale small objects (tens of pixels) in remote sensing images (RSIs) remains challenging.  Existing solutions often rely on complex deep neural networks, resulting in high computational costs. This article introduces SuperYOLO, a fast and accurate object detection method for RSIs that leverages multimodal data and high-resolution (HR) object detection on multiscale objects.  SuperYOLO employs assisted super-resolution (SR) learning, balancing detection accuracy and computational efficiency.  A symmetric, compact multimodal fusion (MF) module extracts supplementary information from diverse data sources to improve small object detection.  A simple, flexible SR branch learns HR feature representations from low-resolution (LR) input, enhancing discrimination between small objects and the background.  Crucially, the SR branch is discarded during inference, mitigating computational overhead due to the use of LR input.  Experiments on the widely used VEDAI RS dataset demonstrate that SuperYOLO achieves a mean Average Precision (mAP<sub>50</sub>) of 75.09%, exceeding state-of-the-art (SOTA) large models like YOLOv5l, YOLOv5x, and YOLOrs by over 10%.  Furthermore, SuperYOLO's parameter size and GFLOPs are approximately 18 and 3.8 times smaller than YOLOv5x, respectively.  Our results show a favorable accuracy-speed trade-off compared to SOTA methods. The code is publicly available at https://github.com/icey-zhang/SuperYOLO.\n"}, "2109.01958": {"original_text": "  Transformer-based pre-trained language models boost the performance of\nopen-domain dialogue systems. Prior works leverage Transformer-based\npre-trained language models to generate texts with desired attributes in two\ngeneral approaches: (1) gradient-based methods: updating all latent\nrepresentations of pre-trained models with gradients from attribute models; (2)\nweighted-decoding methods: re-ranking beam candidates from pre-trained models\nwith attribute functions. However, gradient-based methods lead to high\ncomputation cost and can easily get overfitted on small training sets, while\nweighted-decoding methods are inherently constrained by the low-variance\nhigh-bias pre-trained model. In this work, we propose a novel approach to\ncontrol the generation of Transformer-based pre-trained language models: the\nSideControl framework, which leverages a novel control attributes loss to\nincorporate useful control signals, and is shown to perform well with very\nlimited training samples. We evaluate our proposed method on two benchmark\nopen-domain dialogue datasets, and results show that the SideControl framework\nhas better controllability, higher generation quality and better\nsample-efficiency than existing gradient-based and weighted-decoding baselines.\n", "rewritten_text": "Pre-trained transformer-based language models significantly improve open-domain dialogue system performance.  Previous work has employed these models to generate text with desired attributes using two main approaches: (1) gradient-based methods, which update all latent representations of the pre-trained model using gradients from an attribute model; and (2) weighted-decoding methods, which rerank beam search candidates using attribute functions.  However, gradient-based methods are computationally expensive and prone to overfitting with limited training data, while weighted-decoding methods are inherently limited by the high bias and low variance of the pre-trained model.\n\nThis work introduces SideControl, a novel framework for controlling the generation of pre-trained transformer-based language models.  SideControl incorporates useful control signals via a novel control attribute loss function and demonstrates strong performance even with very limited training data.  Evaluations on two benchmark open-domain dialogue datasets show that SideControl achieves superior controllability, higher generation quality, and improved sample efficiency compared to existing gradient-based and weighted-decoding baselines.\n"}, "2205.02022": {"original_text": "  Recent advances in the pre-training of language models leverage large-scale\ndatasets to create multilingual models. However, low-resource languages are\nmostly left out in these datasets. This is primarily because many widely spoken\nlanguages are not well represented on the web and therefore excluded from the\nlarge-scale crawls used to create datasets. Furthermore, downstream users of\nthese models are restricted to the selection of languages originally chosen for\npre-training. This work investigates how to optimally leverage existing\npre-trained models to create low-resource translation systems for 16 African\nlanguages. We focus on two questions: 1) How can pre-trained models be used for\nlanguages not included in the initial pre-training? and 2) How can the\nresulting translation models effectively transfer to new domains? To answer\nthese questions, we create a new African news corpus covering 16 languages, of\nwhich eight languages are not part of any existing evaluation dataset. We\ndemonstrate that the most effective strategy for transferring both to\nadditional languages and to additional domains is to fine-tune large\npre-trained models on small quantities of high-quality translation data.\n", "rewritten_text": "Recent advances in pre-training language models leverage large-scale datasets to create multilingual models.  However, these datasets largely neglect low-resource languages, primarily due to the underrepresentation of many widely spoken languages on the web, excluding them from the large-scale data crawls used for model training.  Consequently, downstream applications are limited to the languages initially included in pre-training.  This work investigates the optimal leveraging of existing pre-trained models to build low-resource translation systems for sixteen African languages.  We address two key questions: (1) How can pre-trained models be adapted for languages absent from their original training data? and (2) How can the resulting translation models effectively transfer to new domains? To answer these questions, we created a new African news corpus encompassing sixteen languages, eight of which lack representation in existing evaluation datasets.  Our results demonstrate that fine-tuning large pre-trained models on small amounts of high-quality translation data is the most effective strategy for transferring both to new languages and new domains.\n"}, "2109.12028": {"original_text": "  Human knowledge is collectively encoded in the roughly 6500 languages spoken\naround the world, but it is not distributed equally across languages. Hence,\nfor information-seeking question answering (QA) systems to adequately serve\nspeakers of all languages, they need to operate cross-lingually. In this work\nwe investigate the capabilities of multilingually pre-trained language models\non cross-lingual QA. We find that explicitly aligning the representations\nacross languages with a post-hoc fine-tuning step generally leads to improved\nperformance. We additionally investigate the effect of data size as well as the\nlanguage choice in this fine-tuning step, also releasing a dataset for\nevaluating cross-lingual QA systems. Code and dataset are publicly available\nhere: https://github.com/ffaisal93/aligned_qa\n", "rewritten_text": "Human knowledge is encoded in the approximately 6,500 languages spoken worldwide, but this knowledge is not equally distributed.  Therefore, for information-seeking question answering (QA) systems to serve all language speakers effectively, cross-lingual operation is essential. This work investigates the capabilities of multilingually pre-trained language models for cross-lingual QA.  We find that post-hoc fine-tuning with explicit cross-lingual representation alignment generally improves performance.  We further investigate the effects of training data size and language selection during fine-tuning, and release a new dataset for evaluating cross-lingual QA systems.  The code and dataset are publicly available at: https://github.com/ffaisal93/aligned_qa\n"}, "2010.00363": {"original_text": "  Long Short-Term Memory recurrent neural network (LSTM) is widely used and\nknown to capture informative long-term syntactic dependencies. However, how\nsuch information are reflected in its internal vectors for natural text has not\nyet been sufficiently investigated. We analyze them by learning a language\nmodel where syntactic structures are implicitly given. We empirically show that\nthe context update vectors, i.e. outputs of internal gates, are approximately\nquantized to binary or ternary values to help the language model to count the\ndepth of nesting accurately, as Suzgun et al. (2019) recently show for\nsynthetic Dyck languages. For some dimensions in the context vector, we show\nthat their activations are highly correlated with the depth of phrase\nstructures, such as VP and NP. Moreover, with an $L_1$ regularization, we also\nfound that it can accurately predict whether a word is inside a phrase\nstructure or not from a small number of components of the context vector. Even\nfor the case of learning from raw text, context vectors are shown to still\ncorrelate well with the phrase structures. Finally, we show that natural\nclusters of the functional words and the part of speeches that trigger phrases\nare represented in a small but principal subspace of the context-update vector\nof LSTM.\n", "rewritten_text": "Long short-term memory (LSTM) recurrent neural networks are widely used for their ability to capture informative long-term syntactic dependencies. However, how this information is reflected in their internal vectors for natural language remains insufficiently investigated.  We analyze these vectors by training a language model on implicitly structured data.  Our empirical results demonstrate that the context update vectors (i.e., the outputs of the internal gates) are approximately quantized to binary or ternary values, enabling accurate nesting depth counting, mirroring recent findings for synthetic Dyck languages (Suzgun et al., 2019).  We show that the activations of certain dimensions in the context vector are highly correlated with the depth of phrase structures, such as verb phrases (VPs) and noun phrases (NPs). Furthermore, using L1 regularization, we demonstrate that a small subset of context vector components accurately predicts whether a word is within a phrase structure.  Even when trained on raw text, context vectors exhibit strong correlations with phrase structures. Finally, we show that natural clusters of functional words and parts of speech that trigger phrases are represented within a small, principal subspace of the LSTM's context-update vector.\n"}, "2005.02877": {"original_text": "  Task-oriented dialog systems rely on dialog state tracking (DST) to monitor\nthe user's goal during the course of an interaction. Multi-domain and\nopen-vocabulary settings complicate the task considerably and demand scalable\nsolutions. In this paper we present a new approach to DST which makes use of\nvarious copy mechanisms to fill slots with values. Our model has no need to\nmaintain a list of candidate values. Instead, all values are extracted from the\ndialog context on-the-fly. A slot is filled by one of three copy mechanisms:\n(1) Span prediction may extract values directly from the user input; (2) a\nvalue may be copied from a system inform memory that keeps track of the\nsystem's inform operations; (3) a value may be copied over from a different\nslot that is already contained in the dialog state to resolve coreferences\nwithin and across domains. Our approach combines the advantages of span-based\nslot filling methods with memory methods to avoid the use of value picklists\naltogether. We argue that our strategy simplifies the DST task while at the\nsame time achieving state of the art performance on various popular evaluation\nsets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55%.\n", "rewritten_text": "Task-oriented dialog systems rely on dialog state tracking (DST) to monitor user goals throughout an interaction.  Multi-domain and open-vocabulary settings significantly complicate this task, demanding scalable solutions. This paper presents a novel DST approach employing various copy mechanisms to fill slots with values.  Unlike methods requiring a candidate value list, our model extracts all values directly from the dialog context.  A slot is filled using one of three copy mechanisms: (1) span prediction, extracting values directly from user input; (2) copying from a system inform memory, tracking the system's informed values; and (3) copying from another slot in the dialog state to resolve coreferences within and across domains.  This approach combines the advantages of span-based and memory-based slot filling, eliminating the need for value picklists.  We demonstrate that our strategy simplifies DST while achieving state-of-the-art performance on several benchmark datasets, including MultiWOZ 2.1, where we achieved a joint goal accuracy exceeding 55%.\n"}, "2408.06899": {"original_text": "  We present a novel method for measuring the rate of periodic phenomena (e.g.,\nrotation, flicker, and vibration), by an event camera, a device asynchronously\nreporting brightness changes at independently operating pixels with high\ntemporal resolution. The approach assumes that for a periodic phenomenon, a\nhighly similar set of events is generated within a spatio-temporal window at a\ntime difference corresponding to its period. The sets of similar events are\ndetected by a correlation in the spatio-temporal event stream space. The\nproposed method, EEPPR, is evaluated on a dataset of 12 sequences of periodic\nphenomena, i.e. flashing light and vibration, and periodic motion, e.g.,\nrotation, ranging from 3.2 Hz to 2 kHz (equivalent to 192 - 120 000 RPM). EEPPR\nsignificantly outperforms published methods on this dataset, achieving a mean\nrelative error of 0.1%, setting new state-of-the-art. The dataset and codes are\npublicly available on GitHub.\n", "rewritten_text": "We present a novel method, EEPPR, for measuring the rate of periodic phenomena (e.g., rotation, flicker, vibration) using an event camera.  This asynchronous sensor reports brightness changes at independently operating pixels with high temporal resolution.  Our approach leverages the observation that a periodic phenomenon generates highly similar event sets within a spatio-temporal window, separated by a time difference corresponding to its period.  These similar event sets are detected via correlation in the spatio-temporal event stream.  EEPPR was evaluated on a dataset of 12 sequences of periodic phenomena (flashing lights, vibrations, and rotational motion) ranging from 3.2 Hz to 2 kHz (192\u2013120,000 RPM).  EEPPR significantly outperforms existing methods, achieving a mean relative error of 0.1% and establishing a new state-of-the-art. The dataset and code are publicly available on GitHub.\n"}, "1703.08448": {"original_text": "  We investigate a principle way to progressively mine discriminative object\nregions using classification networks to address the weakly-supervised semantic\nsegmentation problems. Classification networks are only responsive to small and\nsparse discriminative regions from the object of interest, which deviates from\nthe requirement of the segmentation task that needs to localize dense, interior\nand integral regions for pixel-wise inference. To mitigate this gap, we propose\na new adversarial erasing approach for localizing and expanding object regions\nprogressively. Starting with a single small object region, our proposed\napproach drives the classification network to sequentially discover new and\ncomplement object regions by erasing the current mined regions in an\nadversarial manner. These localized regions eventually constitute a dense and\ncomplete object region for learning semantic segmentation. To further enhance\nthe quality of the discovered regions by adversarial erasing, an online\nprohibitive segmentation learning approach is developed to collaborate with\nadversarial erasing by providing auxiliary segmentation supervision modulated\nby the more reliable classification scores. Despite its apparent simplicity,\nthe proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union\n(mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new\nstate-of-the-arts.\n", "rewritten_text": "This paper presents a novel approach to weakly supervised semantic segmentation.  We address the challenge that classification networks, while effective at identifying discriminative object regions, only focus on small, sparse areas, unlike segmentation tasks requiring dense, complete object localization for pixel-wise prediction.  To bridge this gap, we propose a progressive adversarial erasing method.  Beginning with a single, small object region, our approach iteratively discovers and expands object regions by adversarially erasing previously identified areas, forcing the classification network to locate complementary regions. This process culminates in a dense, comprehensive object representation suitable for semantic segmentation learning.  Furthermore, we introduce an online prohibitive segmentation learning module that collaborates with adversarial erasing, providing auxiliary segmentation supervision guided by reliable classification scores, thereby improving the quality of the discovered regions.  Despite its simplicity, our method achieves state-of-the-art results, attaining 55.0% and 55.7% mean Intersection over Union (mIoU) on the PASCAL VOC 2012 validation and test sets, respectively.\n"}, "1708.00284": {"original_text": "  Future frame prediction in videos is a promising avenue for unsupervised\nvideo representation learning. Video frames are naturally generated by the\ninherent pixel flows from preceding frames based on the appearance and motion\ndynamics in the video. However, existing methods focus on directly\nhallucinating pixel values, resulting in blurry predictions. In this paper, we\ndevelop a dual motion Generative Adversarial Net (GAN) architecture, which\nlearns to explicitly enforce future-frame predictions to be consistent with the\npixel-wise flows in the video through a dual-learning mechanism. The primal\nfuture-frame prediction and dual future-flow prediction form a closed loop,\ngenerating informative feedback signals to each other for better video\nprediction. To make both synthesized future frames and flows indistinguishable\nfrom reality, a dual adversarial training method is proposed to ensure that the\nfuture-flow prediction is able to help infer realistic future-frames, while the\nfuture-frame prediction in turn leads to realistic optical flows. Our dual\nmotion GAN also handles natural motion uncertainty in different pixel locations\nwith a new probabilistic motion encoder, which is based on variational\nautoencoders. Extensive experiments demonstrate that the proposed dual motion\nGAN significantly outperforms state-of-the-art approaches on synthesizing new\nvideo frames and predicting future flows. Our model generalizes well across\ndiverse visual scenes and shows superiority in unsupervised video\nrepresentation learning.\n", "rewritten_text": "Unsupervised video representation learning benefits significantly from accurate future frame prediction.  Video frames inherently arise from pixel flows driven by the appearance and motion dynamics of preceding frames.  However, existing methods, which directly hallucinate pixel values, often produce blurry predictions.  This paper introduces a novel dual motion Generative Adversarial Network (GAN) architecture that explicitly enforces consistency between future frame predictions and video pixel flows via a dual-learning mechanism.  A closed loop is formed by the primal future-frame prediction and the dual future-flow prediction, providing mutually informative feedback for improved video prediction.  To enhance realism, a dual adversarial training method ensures that the predicted flows aid in generating realistic future frames, and vice-versa.  Furthermore, our dual motion GAN incorporates a novel probabilistic motion encoder, based on variational autoencoders, to effectively handle the inherent uncertainty in motion across different pixel locations.  Extensive experiments demonstrate that our approach significantly surpasses state-of-the-art methods in synthesizing new video frames and predicting future flows, exhibiting strong generalization across diverse visual scenes and superior performance in unsupervised video representation learning.\n"}, "1405.6103": {"original_text": "  The Swiss avalanche bulletin is produced twice a day in four languages. Due\nto the lack of time available for manual translation, a fully automated\ntranslation system is employed, based on a catalogue of predefined phrases and\npredetermined rules of how these phrases can be combined to produce sentences.\nThe system is able to automatically translate such sentences from German into\nthe target languages French, Italian and English without subsequent\nproofreading or correction. Our catalogue of phrases is limited to a small\nsublanguage. The reduction of daily translation costs is expected to offset the\ninitial development costs within a few years. After being operational for two\nwinter seasons, we assess here the quality of the produced texts based on an\nevaluation where participants rate real danger descriptions from both origins,\nthe catalogue of phrases versus the manually written and translated texts. With\na mean recognition rate of 55%, users can hardly distinguish between the two\ntypes of texts, and give similar ratings with respect to their language\nquality. Overall, the output from the catalogue system can be considered\nvirtually equivalent to a text written by avalanche forecasters and then\nmanually translated by professional translators. Furthermore, forecasters\ndeclared that all relevant situations were captured by the system with\nsufficient accuracy and within the limited time available.\n", "rewritten_text": "The Swiss avalanche bulletin, produced twice daily in four languages (German, French, Italian, and English), utilizes a fully automated translation system.  Due to time constraints precluding manual translation, the system employs a catalogue of predefined phrases and rules governing their combination into sentences.  This system automatically translates German bulletins into the target languages without subsequent proofreading.  The phrase catalogue is limited to a specialized sublanguage.  The anticipated reduction in daily translation costs is projected to recoup initial development costs within a few years.\n\nAfter two winter seasons of operation, we assessed the translated text quality.  Participants rated avalanche danger descriptions generated by both the phrase catalogue system and manually translated texts.  With a mean recognition rate of 55%, users demonstrated little ability to distinguish between the two, assigning similar ratings to their linguistic quality.  The automated system's output is virtually indistinguishable from professionally translated texts written by avalanche forecasters.  Moreover, forecasters confirmed that the system accurately captured all relevant situations within the available timeframe.\n"}, "2408.15063": {"original_text": "  Although most existing multi-modal salient object detection (SOD) methods\ndemonstrate effectiveness through training models from scratch, the limited\nmulti-modal data hinders these methods from reaching optimality. In this paper,\nwe propose a novel framework to explore and exploit the powerful feature\nrepresentation and zero-shot generalization ability of the pre-trained Segment\nAnything Model (SAM) for multi-modal SOD. Despite serving as a recent vision\nfundamental model, driving the class-agnostic SAM to comprehend and detect\nsalient objects accurately is non-trivial, especially in challenging scenes. To\nthis end, we develop \\underline{SAM} with se\\underline{m}antic\nf\\underline{e}ature fu\\underline{s}ion guidanc\\underline{e} (Sammese), which\nincorporates multi-modal saliency-specific knowledge into SAM to adapt SAM to\nmulti-modal SOD tasks. However, it is difficult for SAM trained on single-modal\ndata to directly mine the complementary benefits of multi-modal inputs and\ncomprehensively utilize them to achieve accurate saliency prediction. To\naddress these issues, we first design a multi-modal complementary fusion module\nto extract robust multi-modal semantic features by integrating information from\nvisible and thermal or depth image pairs. Then, we feed the extracted\nmulti-modal semantic features into both the SAM image encoder and mask decoder\nfor fine-tuning and prompting, respectively. Specifically, in the image\nencoder, a multi-modal adapter is proposed to adapt the single-modal SAM to\nmulti-modal information. In the mask decoder, a semantic-geometric prompt\ngeneration strategy is proposed to produce corresponding embeddings with\nvarious saliency cues. Extensive experiments on both RGB-D and RGB-T SOD\nbenchmarks show the effectiveness of the proposed framework. The code will be\navailable at \\url{https://github.com/Angknpng/Sammese}.\n", "rewritten_text": "Most existing multi-modal salient object detection (SOD) methods, while effective when trained from scratch, are limited by the scarcity of multi-modal data.  This paper proposes Sammese, a novel framework leveraging the powerful feature representation and zero-shot generalization capabilities of the pre-trained Segment Anything Model (SAM) for multi-modal SOD.  Although SAM is a foundational vision model, adapting its class-agnostic nature for accurate salient object detection, particularly in challenging scenes, is non-trivial.  Sammese incorporates multi-modal saliency-specific knowledge into SAM to address this challenge.  However, SAM's single-modal training hinders its ability to effectively exploit the complementary information inherent in multi-modal inputs for accurate saliency prediction.\n\nTo overcome this limitation, Sammese employs a multi-modal complementary fusion module to extract robust semantic features from visible and thermal or depth image pairs.  These features are then fed into both SAM's image encoder and mask decoder for fine-tuning and prompting, respectively.  Specifically, a multi-modal adapter is introduced into the image encoder to adapt SAM to multi-modal input, while a semantic-geometric prompt generation strategy is used in the mask decoder to create embeddings incorporating diverse saliency cues.  Extensive experiments on RGB-D and RGB-T SOD benchmarks demonstrate Sammese's effectiveness.  The code is available at [https://github.com/Angknpng/Sammese](https://github.com/Angknpng/Sammese).\n"}, "2210.09563": {"original_text": "  With the continuous development of deep learning in the field of image\ngeneration models, a large number of vivid forged faces have been generated and\nspread on the Internet. These high-authenticity artifacts could grow into a\nthreat to society security. Existing face forgery detection methods directly\nutilize the obtained public shared or centralized data for training but ignore\nthe personal privacy and security issues when personal data couldn't be\ncentralizedly shared in real-world scenarios. Additionally, different\ndistributions caused by diverse artifact types would further bring adverse\ninfluences on the forgery detection task. To solve the mentioned problems, the\npaper proposes a novel generalized residual Federated learning for face Forgery\ndetection (FedForgery). The designed variational autoencoder aims to learn\nrobust discriminative residual feature maps to detect forgery faces (with\ndiverse or even unknown artifact types). Furthermore, the general federated\nlearning strategy is introduced to construct distributed detection model\ntrained collaboratively with multiple local decentralized devices, which could\nfurther boost the representation generalization. Experiments conducted on\npublicly available face forgery detection datasets prove the superior\nperformance of the proposed FedForgery. The designed novel generalized face\nforgery detection protocols and source code would be publicly available.\n", "rewritten_text": "The proliferation of realistic deepfake faces online, fueled by advancements in image generation models, poses a significant threat to societal security.  Existing face forgery detection methods, trained on centralized, publicly shared datasets, neglect crucial privacy and security concerns inherent in the real-world limitations of data centralization.  Furthermore, the diverse distributions of forgery artifacts negatively impact detection accuracy.  To address these challenges, this paper proposes FedForgery, a novel generalized residual federated learning framework for face forgery detection.  Our approach employs a variational autoencoder to learn robust, discriminative residual feature maps capable of detecting forgeries with diverse or unknown artifact types.  A federated learning strategy enables collaborative training across multiple decentralized devices, enhancing the model's generalization capabilities.  Experiments on publicly available datasets demonstrate FedForgery's superior performance.  The proposed methodology and source code will be publicly released.\n"}, "2304.05995": {"original_text": "  In recent years, the success of large-scale vision-language models (VLMs)\nsuch as CLIP has led to their increased usage in various computer vision tasks.\nThese models enable zero-shot inference through carefully crafted instructional\ntext prompts without task-specific supervision. However, the potential of VLMs\nfor generalization tasks in remote sensing (RS) has not been fully realized. To\naddress this research gap, we propose a novel image-conditioned prompt learning\nstrategy called the Visual Attention Parameterized Prompts Learning Network\n(APPLeNet). APPLeNet emphasizes the importance of multi-scale feature learning\nin RS scene classification and disentangles visual style and content primitives\nfor domain generalization tasks. To achieve this, APPLeNet combines visual\ncontent features obtained from different layers of the vision encoder and style\nproperties obtained from feature statistics of domain-specific batches. An\nattention-driven injection module is further introduced to generate visual\ntokens from this information. We also introduce an anti-correlation regularizer\nto ensure discrimination among the token embeddings, as this visual information\nis combined with the textual tokens. To validate APPLeNet, we curated four\navailable RS benchmarks and introduced experimental protocols and datasets for\nthree domain generalization tasks. Our results consistently outperform the\nrelevant literature and code is available at\nhttps://github.com/mainaksingha01/APPLeNet\n", "rewritten_text": "Recent successes of large-scale vision-language models (VLMs), such as CLIP, have spurred their increased adoption in various computer vision tasks.  These models enable zero-shot inference via carefully crafted textual prompts, eliminating the need for task-specific supervision. However, the potential of VLMs for generalization in remote sensing (RS) remains largely untapped.  To address this, we propose APPLeNet (Visual Attention Parameterized Prompts Learning Network), a novel image-conditioned prompt learning strategy.  APPLeNet leverages multi-scale feature learning for RS scene classification and disentangles visual style and content for improved domain generalization.  This is achieved by combining visual content features from multiple layers of a vision encoder with style features derived from domain-specific batch statistics.  An attention-driven injection module then generates visual tokens from this combined information.  Furthermore, an anti-correlation regularizer enhances discrimination among token embeddings before fusion with textual tokens.  We validated APPLeNet on four established RS benchmarks, introducing novel experimental protocols and datasets for three domain generalization tasks.  Our results consistently surpass state-of-the-art performance. Code is available at https://github.com/mainaksingha01/APPLeNet.\n"}, "1804.03287": {"original_text": "  Despite the noticeable progress in perceptual tasks like detection, instance\nsegmentation and human parsing, computers still perform unsatisfactorily on\nvisually understanding humans in crowded scenes, such as group behavior\nanalysis, person re-identification and autonomous driving, etc. To this end,\nmodels need to comprehensively perceive the semantic information and the\ndifferences between instances in a multi-human image, which is recently defined\nas the multi-human parsing task. In this paper, we present a new large-scale\ndatabase \"Multi-Human Parsing (MHP)\" for algorithm development and evaluation,\nand advances the state-of-the-art in understanding humans in crowded scenes.\nMHP contains 25,403 elaborately annotated images with 58 fine-grained semantic\ncategory labels, involving 2-26 persons per image and captured in real-world\nscenes from various viewpoints, poses, occlusion, interactions and background.\nWe further propose a novel deep Nested Adversarial Network (NAN) model for\nmulti-human parsing. NAN consists of three Generative Adversarial Network\n(GAN)-like sub-nets, respectively performing semantic saliency prediction,\ninstance-agnostic parsing and instance-aware clustering. These sub-nets form a\nnested structure and are carefully designed to learn jointly in an end-to-end\nway. NAN consistently outperforms existing state-of-the-art solutions on our\nMHP and several other datasets, and serves as a strong baseline to drive the\nfuture research for multi-human parsing.\n", "rewritten_text": "While significant progress has been made in perceptual tasks such as object detection, instance segmentation, and human parsing, computer vision systems still struggle to understand humans in crowded scenes, including group behavior analysis, person re-identification, and autonomous driving.  This limitation stems from the need for models to comprehensively perceive both semantic information and individual distinctions within multi-human images\u2014a challenge recently formalized as the multi-human parsing task.  This paper introduces a new large-scale dataset, Multi-Human Parsing (MHP), for algorithm development and evaluation, advancing the state-of-the-art in understanding humans in crowded environments.  MHP comprises 25,403 meticulously annotated images with 58 fine-grained semantic labels, depicting 2\u201326 individuals per image in diverse real-world scenes with varying viewpoints, poses, occlusions, interactions, and backgrounds.  We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing.  NAN comprises three Generative Adversarial Network (GAN)-like sub-networks, respectively performing semantic saliency prediction, instance-agnostic parsing, and instance-aware clustering.  These sub-networks are arranged in a nested architecture and are jointly trained end-to-end.  NAN consistently surpasses existing state-of-the-art methods on MHP and several other benchmark datasets, establishing a strong baseline for future research in multi-human parsing.\n"}, "2206.10779": {"original_text": "  We propose a large-scale dataset of real-world rainy and clean image pairs\nand a method to remove degradations, induced by rain streaks and rain\naccumulation, from the image. As there exists no real-world dataset for\nderaining, current state-of-the-art methods rely on synthetic data and thus are\nlimited by the sim2real domain gap; moreover, rigorous evaluation remains a\nchallenge due to the absence of a real paired dataset. We fill this gap by\ncollecting a real paired deraining dataset through meticulous control of\nnon-rain variations. Our dataset enables paired training and quantitative\nevaluation for diverse real-world rain phenomena (e.g. rain streaks and rain\naccumulation). To learn a representation robust to rain phenomena, we propose a\ndeep neural network that reconstructs the underlying scene by minimizing a\nrain-robust loss between rainy and clean images. Extensive experiments\ndemonstrate that our model outperforms the state-of-the-art deraining methods\non real rainy images under various conditions. Project website:\nhttps://visual.ee.ucla.edu/gt_rain.htm/.\n", "rewritten_text": "We introduce a large-scale dataset of real-world paired rainy and clean images, along with a novel method for removing rain-induced degradations (streaks and accumulation).  The lack of real-world datasets for deraining has limited current state-of-the-art methods to synthetic data, resulting in a significant sim-to-real gap and hindering rigorous evaluation.  Our meticulously collected dataset addresses this limitation by providing paired images with controlled non-rain variations, enabling paired training and quantitative evaluation of diverse real-world rain phenomena.  We propose a deep neural network that reconstructs the underlying scene by minimizing a rain-robust loss function between rainy and clean image pairs, learning a representation robust to various rain conditions.  Extensive experiments demonstrate that our model surpasses state-of-the-art deraining methods on real-world rainy images across diverse scenarios.  Project website: https://visual.ee.ucla.edu/gt_rain.htm/\n"}, "1812.08115": {"original_text": "  We introduce a model-based deep learning architecture termed MoDL-MUSSELS for\nthe correction of phase errors in multishot diffusion-weighted echo-planar MRI\nimages. The proposed algorithm is a generalization of existing MUSSELS\nalgorithm with similar performance but with significantly reduced computational\ncomplexity. In this work, we show that an iterative re-weighted least-squares\nimplementation of MUSSELS alternates between a multichannel filter bank and the\nenforcement of data consistency. The multichannel filter bank projects the data\nto the signal subspace thus exploiting the phase relations between shots. Due\nto the high computational complexity of self-learned filter bank, we propose to\nreplace it with a convolutional neural network (CNN) whose parameters are\nlearned from exemplary data. The proposed CNN is a hybrid model involving a\nmultichannel CNN in the k-space and another CNN in the image space. The k-space\nCNN exploits the phase relations between the shot images, while the image\ndomain network is used to project the data to an image manifold. The\nexperiments show that the proposed scheme can yield reconstructions that are\ncomparable to state of the art methods while offering several orders of\nmagnitude reduction in run-time.\n", "rewritten_text": "We introduce MoDL-MUSSELS, a model-based deep learning architecture for correcting phase errors in multishot diffusion-weighted echo-planar imaging (EPI) MRI.  This algorithm generalizes the existing MUSSELS algorithm, achieving comparable performance with significantly reduced computational complexity.  We demonstrate that an iterative reweighted least-squares implementation of MUSSELS alternates between a multichannel filter bank and data consistency enforcement. The multichannel filter bank projects the data onto the signal subspace, exploiting inter-shot phase relationships.  To address the high computational cost of a self-learned filter bank, we replace it with a convolutional neural network (CNN). This hybrid CNN comprises a multichannel k-space CNN, which leverages inter-shot phase relationships, and an image-space CNN, which projects the data onto an image manifold.  Experiments show that our method achieves state-of-the-art reconstruction quality with several orders of magnitude faster runtime.\n"}, "1912.10644": {"original_text": "  In spite of the recent progresses on classifying 3D point cloud with deep\nCNNs, large geometric transformations like rotation and translation remain\nchallenging problem and harm the final classification performance. To address\nthis challenge, we propose Geometry Sharing Network (GS-Net) which effectively\nlearns point descriptors with holistic context to enhance the robustness to\ngeometric transformations. Compared with previous 3D point CNNs which perform\nconvolution on nearby points, GS-Net can aggregate point features in a more\nglobal way. Specially, GS-Net consists of Geometry Similarity Connection (GSC)\nmodules which exploit Eigen-Graph to group distant points with similar and\nrelevant geometric information, and aggregate features from nearest neighbors\nin both Euclidean space and Eigenvalue space. This design allows GS-Net to\nefficiently capture both local and holistic geometric features such as\nsymmetry, curvature, convexity and connectivity. Theoretically, we show the\nnearest neighbors of each point in Eigenvalue space are invariant to rotation\nand translation. We conduct extensive experiments on public datasets,\nModelNet40, ShapeNet Part. Experiments demonstrate that GS-Net achieves the\nstate-of-the-art performances on major datasets, 93.3% on ModelNet40, and are\nmore robust to geometric transformations.\n", "rewritten_text": "Despite recent progress in 3D point cloud classification using deep convolutional neural networks (CNNs), large geometric transformations such as rotation and translation remain a significant challenge, hindering classification performance.  To address this, we propose the Geometry Sharing Network (GS-Net), which effectively learns point descriptors incorporating holistic context to enhance robustness to geometric transformations. Unlike previous 3D point CNNs that perform convolutions on only nearby points, GS-Net aggregates point features more globally.  Specifically, GS-Net incorporates Geometry Similarity Connection (GSC) modules that leverage Eigen-Graphs to group distant points with similar and relevant geometric information, aggregating features from nearest neighbors in both Euclidean and eigenvalue space. This design enables GS-Net to efficiently capture both local and global geometric features, including symmetry, curvature, convexity, and connectivity.  Theoretically, we demonstrate that nearest neighbors in eigenvalue space are invariant to rotation and translation.  Extensive experiments on the public ModelNet40 and ShapeNet Part datasets demonstrate that GS-Net achieves state-of-the-art performance, reaching 93.3% accuracy on ModelNet40, and exhibits significantly improved robustness to geometric transformations.\n"}, "2111.01515": {"original_text": "  The enormous amount of data being generated on the web and social media has\nincreased the demand for detecting online hate speech. Detecting hate speech\nwill reduce their negative impact and influence on others. A lot of effort in\nthe Natural Language Processing (NLP) domain aimed to detect hate speech in\ngeneral or detect specific hate speech such as religion, race, gender, or\nsexual orientation. Hate communities tend to use abbreviations, intentional\nspelling mistakes, and coded words in their communication to evade detection,\nadding more challenges to hate speech detection tasks. Thus, word\nrepresentation will play an increasingly pivotal role in detecting hate speech.\nThis paper investigates the feasibility of leveraging domain-specific word\nembedding in Bidirectional LSTM based deep model to automatically\ndetect/classify hate speech. Furthermore, we investigate the use of the\ntransfer learning language model (BERT) on hate speech problem as a binary\nclassification task. The experiments showed that domainspecific word embedding\nwith the Bidirectional LSTM based deep model achieved a 93% f1-score while BERT\nachieved up to 96% f1-score on a combined balanced dataset from available hate\nspeech datasets.\n", "rewritten_text": "The proliferation of online data from websites and social media has fueled the demand for effective hate speech detection.  Accurate detection mitigates the harmful impact and influence of such speech.  Significant research within Natural Language Processing (NLP) has focused on general hate speech detection and the identification of hate speech targeting specific attributes like religion, race, gender, or sexual orientation.  However, hate communities often employ obfuscation techniques, including abbreviations, deliberate misspellings, and coded language, to evade detection, posing significant challenges.  Consequently, effective word representation is crucial for robust hate speech detection.\n\nThis paper explores the feasibility of using domain-specific word embeddings within a Bidirectional Long Short-Term Memory (BiLSTM) deep learning model for automated hate speech detection and classification.  Additionally, we investigate the application of the transfer learning model BERT to this binary classification problem.  Experiments conducted on a combined, balanced dataset derived from existing hate speech datasets demonstrated that the BiLSTM model with domain-specific word embeddings achieved an F1-score of 93%, while BERT achieved an F1-score of up to 96%.\n"}, "2409.17432": {"original_text": "  Reducing the atmospheric haze and enhancing image clarity is crucial for\ncomputer vision applications. The lack of real-life hazy ground truth images\nnecessitates synthetic datasets, which often lack diverse haze types, impeding\neffective haze type classification and dehazing algorithm selection. This\nresearch introduces the HazeSpace2M dataset, a collection of over 2 million\nimages designed to enhance dehazing through haze type classification.\nHazeSpace2M includes diverse scenes with 10 haze intensity levels, featuring\nFog, Cloud, and Environmental Haze (EH). Using the dataset, we introduce a\ntechnique of haze type classification followed by specialized dehazers to clear\nhazy images. Unlike conventional methods, our approach classifies haze types\nbefore applying type-specific dehazing, improving clarity in real-life hazy\nimages. Benchmarking with state-of-the-art (SOTA) models, ResNet50 and AlexNet\nachieve 92.75\\% and 92.50\\% accuracy, respectively, against existing synthetic\ndatasets. However, these models achieve only 80% and 70% accuracy,\nrespectively, against our Real Hazy Testset (RHT), highlighting the challenging\nnature of our HazeSpace2M dataset. Additional experiments show that haze type\nclassification followed by specialized dehazing improves results by 2.41% in\nPSNR, 17.14% in SSIM, and 10.2\\% in MSE over general dehazers. Moreover, when\ntesting with SOTA dehazing models, we found that applying our proposed\nframework significantly improves their performance. These results underscore\nthe significance of HazeSpace2M and our proposed framework in addressing\natmospheric haze in multimedia processing. Complete code and dataset is\navailable on \\href{https://github.com/tanvirnwu/HazeSpace2M}\n{\\textcolor{blue}{\\textbf{GitHub}}}.\n", "rewritten_text": "Reducing atmospheric haze and enhancing image clarity is crucial for computer vision applications.  The scarcity of real-world hazy images for ground truth necessitates the use of synthetic datasets, which often lack diversity in haze types, hindering effective haze type classification and dehazing algorithm selection.  This research introduces HazeSpace2M, a dataset of over two million synthetic images designed to improve dehazing performance through accurate haze type classification.  HazeSpace2M features diverse scenes with ten intensity levels across three haze types: fog, cloud, and environmental haze (EH).  We propose a novel two-stage approach: first classifying the haze type, then applying a specialized dehazing algorithm.  Unlike conventional methods that use a single dehazing algorithm, our type-specific approach improves clarity in real-world hazy images.\n\nBenchmarking against existing synthetic datasets using state-of-the-art (SOTA) models, ResNet50 and AlexNet achieved 92.75% and 92.50% accuracy, respectively.  However, on our Real Hazy Testset (RHT), these models achieved only 80% and 70% accuracy, respectively, demonstrating the challenging nature of HazeSpace2M.  Further experiments showed that our proposed framework, combining haze type classification with specialized dehazing, improved results by 2.41% in PSNR, 17.14% in SSIM, and a 10.2% reduction in MSE compared to general-purpose dehazing methods.  Moreover, applying our framework significantly enhanced the performance of SOTA dehazing models.  These results highlight the importance of HazeSpace2M and our proposed framework for addressing atmospheric haze in multimedia processing.  The complete code and dataset are available at [GitHub](https://github.com/tanvirnwu/HazeSpace2M).\n"}, "1808.045": {"original_text": "  Medical images with specific pathologies are scarce, but a large amount of\ndata is usually required for a deep convolutional neural network (DCNN) to\nachieve good accuracy. We consider the problem of segmenting the left\nventricular (LV) myocardium on late gadolinium enhancement (LGE) cardiovascular\nmagnetic resonance (CMR) scans of which only some of the scans have scar\ntissue. We propose ScarGAN to simulate scar tissue on healthy myocardium using\nchained generative adversarial networks (GAN). Our novel approach factorizes\nthe simulation process into 3 steps: 1) a mask generator to simulate the shape\nof the scar tissue; 2) a domain-specific heuristic to produce the initial\nsimulated scar tissue from the simulated shape; 3) a refining generator to add\ndetails to the simulated scar tissue. Unlike other approaches that generate\nsamples from scratch, we simulate scar tissue on normal scans resulting in\nhighly realistic samples. We show that experienced radiologists are unable to\ndistinguish between real and simulated scar tissue. Training a U-Net with\nadditional scans with scar tissue simulated by ScarGAN increases the percentage\nof scar pixels correctly included in LV myocardium prediction from 75.9% to\n80.5%.\n", "rewritten_text": "The scarcity of medical images depicting specific pathologies presents a challenge for training deep convolutional neural networks (DCNNs), which typically require large datasets to achieve high accuracy.  This study addresses the problem of left ventricular (LV) myocardium segmentation in late gadolinium enhancement (LGE) cardiovascular magnetic resonance (CMR) scans, where scar tissue is present in only a subset of images. We propose ScarGAN, a novel approach using chained generative adversarial networks (GANs) to simulate scar tissue on healthy myocardium.  Our method factorizes the simulation into three steps: 1) a mask generator to simulate scar tissue shape; 2) a domain-specific heuristic to generate initial scar tissue from the simulated shape; and 3) a refining generator to add detail. Unlike methods generating samples de novo, ScarGAN simulates scar tissue on normal scans, yielding highly realistic results.  Experienced radiologists were unable to reliably distinguish between real and simulated scar tissue.  Augmenting a U-Net training dataset with ScarGAN-generated scans increased the precision of scar pixel inclusion in LV myocardium predictions from 75.9% to 80.5%.\n"}, "2409.00045": {"original_text": "  Colonoscopy is the primary method for examination, detection, and removal of\npolyps. Regular screening helps detect and prevent colorectal cancer at an\nearly curable stage. However, challenges such as variation among the\nendoscopists' skills, bowel quality preparation, and complex nature of the\nlarge intestine which cause large number of polyp miss-rate. These missed\npolyps can develop into cancer later on, which underscores the importance of\nimproving the detection methods. A computer-aided diagnosis system can support\nphysicians by assisting in detecting overlooked polyps. However, one of the\nimportant challenges for developing novel deep learning models for automatic\npolyp detection and segmentation is the lack of publicly available,\nmulti-center large and diverse datasets. To address this gap, we introduce\nPolypDB, a large scale publicly available dataset that contains 3934 still\npolyp images and their corresponding ground truth from real colonoscopy videos\nto design efficient polyp detection and segmentation architectures. The dataset\nhas been developed and verified by a team of 10 gastroenterologists. PolypDB\ncomprises of images from five modalities: Blue Light Imaging (BLI), Flexible\nImaging Color Enhancement (FICE), Linked Color Imaging (LCI), Narrow Band\nImaging (NBI), and White Light Imaging (WLI) and three medical centers from\nNorway, Sweden and Vietnam. Thus, we split the dataset based on modality and\nmedical center for modality-wise and center-wise analysis. We provide a\nbenchmark on each modality using eight popular segmentation methods and six\nstandard benchmark polyp detection methods. Furthermore, we also provide\nbenchmark on center-wise under federated learning settings. Our dataset is\npublic and can be downloaded at \\url{https://osf.io/pr7ms/}.\n", "rewritten_text": "Colonoscopy is the primary method for examining, detecting, and removing polyps.  Regular screening helps detect and prevent colorectal cancer at an early, curable stage. However, challenges such as inter-endoscopist skill variation, inadequate bowel preparation, and the complex anatomy of the large intestine contribute to a significant polyp miss rate.  These missed polyps can develop into cancer, highlighting the need for improved detection methods. Computer-aided diagnosis systems can assist physicians by identifying overlooked polyps.  A major challenge in developing novel deep learning models for automatic polyp detection and segmentation is the lack of publicly available, large, diverse, and multi-center datasets.\n\nTo address this, we introduce PolypDB, a large-scale, publicly available dataset containing 3934 polyp images and their corresponding ground truth annotations derived from real colonoscopy videos.  This dataset, developed and validated by a team of 10 gastroenterologists, facilitates the design of efficient polyp detection and segmentation architectures. PolypDB comprises images from five modalities\u2014Blue Light Imaging (BLI), Flexible Imaging Color Enhancement (FICE), Linked Color Imaging (LCI), Narrow Band Imaging (NBI), and White Light Imaging (WLI)\u2014obtained from three medical centers in Norway, Sweden, and Vietnam.  The dataset is split by modality and medical center to enable modality-wise and center-wise analysis. We provide benchmarks for each modality using eight popular segmentation methods and six standard polyp detection methods.  Furthermore, we provide center-wise benchmarks under federated learning settings.  Our dataset is publicly available at [https://osf.io/pr7ms/](https://osf.io/pr7ms/).\n"}, "2201.09724": {"original_text": "  The task of hot-refresh model upgrades of image retrieval systems plays an\nessential role in the industry but has never been investigated in academia\nbefore. Conventional cold-refresh model upgrades can only deploy new models\nafter the gallery is overall backfilled, taking weeks or even months for\nmassive data. In contrast, hot-refresh model upgrades deploy the new model\nimmediately and then gradually improve the retrieval accuracy by backfilling\nthe gallery on-the-fly. Compatible training has made it possible, however, the\nproblem of model regression with negative flips poses a great challenge to the\nstable improvement of user experience. We argue that it is mainly due to the\nfact that new-to-old positive query-gallery pairs may show less similarity than\nnew-to-new negative pairs. To solve the problem, we introduce a\nRegression-Alleviating Compatible Training (RACT) method to properly constrain\nthe feature compatibility while reducing negative flips. The core is to\nencourage the new-to-old positive pairs to be more similar than both the\nnew-to-old negative pairs and the new-to-new negative pairs. An efficient\nuncertainty-based backfilling strategy is further introduced to fasten accuracy\nimprovements. Extensive experiments on large-scale retrieval benchmarks (e.g.,\nGoogle Landmark) demonstrate that our RACT effectively alleviates the model\nregression for one more step towards seamless model upgrades. The code will be\navailable at https://github.com/binjiezhang/RACT_ICLR2022.\n", "rewritten_text": "Hot-refresh model upgrades are crucial for industrial image retrieval systems, yet remain unexplored in academia.  Conventional cold-refresh upgrades require complete gallery backfilling, delaying deployment for weeks or months with massive datasets.  In contrast, hot-refresh upgrades deploy immediately, incrementally improving retrieval accuracy through on-the-fly backfilling. While compatible training enables this, model regression and negative flips hinder consistent user experience improvements.  This regression stems from new-to-old positive query-gallery pairs exhibiting lower similarity than new-to-new negative pairs.\n\nTo address this, we propose Regression-Alleviating Compatible Training (RACT), a method that constrains feature compatibility while mitigating negative flips.  RACT prioritizes higher similarity between new-to-old positive pairs than both new-to-old and new-to-new negative pairs.  Furthermore, we introduce an efficient uncertainty-based backfilling strategy to accelerate accuracy gains.  Extensive experiments on large-scale benchmarks (e.g., Google Landmark) demonstrate RACT's effectiveness in alleviating model regression, advancing seamless model upgrades.  The code is available at https://github.com/binjiezhang/RACT_ICLR2022.\n"}, "2012.14345": {"original_text": "  Deep Learning (DL) based methods for object detection achieve remarkable\nperformance at the cost of computationally expensive training and extensive\ndata labeling. Robots embodiment can be exploited to mitigate this burden by\nacquiring automatically annotated training data via a natural interaction with\na human showing the object of interest, handheld. However, learning solely from\nthis data may introduce biases (the so-called domain shift), and prevents\nadaptation to novel tasks. While Weakly-supervised Learning (WSL) offers a\nwell-established set of techniques to cope with these problems in\ngeneral-purpose Computer Vision, its adoption in challenging robotic domains is\nstill at a preliminary stage. In this work, we target the scenario of a robot\ntrained in a teacher-learner setting to detect handheld objects. The aim is to\nimprove detection performance in different settings by letting the robot\nexplore the environment with a limited human labeling budget. We compare\nseveral techniques for WSL in detection pipelines to reduce model re-training\ncosts without compromising accuracy, proposing solutions which target the\nconsidered robotic scenario. We show that the robot can improve adaptation to\nnovel domains, either by interacting with a human teacher (Active Learning) or\nwith an autonomous supervision (Semi-supervised Learning). We integrate our\nstrategies into an on-line detection method, achieving efficient model update\ncapabilities with few labels. We experimentally benchmark our method on\nchallenging robotic object detection tasks under domain shift.\n", "rewritten_text": "Deep learning (DL)-based object detection methods achieve remarkable performance but require computationally expensive training and extensive data labeling.  Robotic embodiment offers a potential solution by enabling the acquisition of automatically annotated training data through natural human-robot interaction, where a human demonstrates objects of interest. However, learning solely from this data can introduce biases (domain shift) and hinder adaptation to novel tasks. While weakly supervised learning (WSL) provides established techniques to address these challenges in general-purpose computer vision, its application in demanding robotic domains remains underdeveloped.\n\nThis work addresses the problem of training a robot in a teacher-learner setting to detect handheld objects.  Our goal is to improve detection performance across diverse settings by enabling the robot to explore its environment with a limited human labeling budget. We compare several WSL techniques within detection pipelines to minimize model retraining costs without sacrificing accuracy, proposing solutions tailored to the robotic scenario.  We demonstrate that the robot can improve its adaptation to novel domains through either human-in-the-loop interaction (active learning) or autonomous supervision (semi-supervised learning).  Our strategies are integrated into an online detection method, enabling efficient model updates with minimal labeling.  We experimentally evaluate our method on challenging robotic object detection tasks subject to domain shift.\n"}, "cs/0310014": {"original_text": "  Spoken Language can be used to provide insights into organisational\nprocesses, unfortunately transcription and coding stages are very time\nconsuming and expensive. The concept of partial transcription and coding is\nproposed in which spoken language is indexed prior to any subsequent\nprocessing. The functional linguistic theory of texture is used to describe the\neffects of partial transcription on observational records. The standard used to\nencode transcript context and metadata is called CHAT, but a previous XML\nschema developed to implement it contains design assumptions that make it\ndifficult to support partial transcription for example. This paper describes a\nmore effective XML schema that overcomes many of these problems and is intended\nfor use in applications that support the rapid development of spoken language\ndeliverables.\n", "rewritten_text": "Spoken language offers valuable insights into organizational processes. However, full transcription and coding are time-consuming and expensive.  This paper proposes partial transcription and coding, indexing spoken language before further processing.  We utilize the functional linguistic theory of texture to analyze the impact of partial transcription on observational records. While the CHAT standard encodes transcript context and metadata, its existing XML schema presents limitations for partial transcription.  This paper presents a more efficient XML schema designed to overcome these limitations and facilitate the rapid development of spoken language deliverables.\n"}, "2007.12099": {"original_text": "  Object detection is one of the most important areas in computer vision, which\nplays a key role in various practical scenarios. Due to limitation of hardware,\nit is often necessary to sacrifice accuracy to ensure the infer speed of the\ndetector in practice. Therefore, the balance between effectiveness and\nefficiency of object detector must be considered. The goal of this paper is to\nimplement an object detector with relatively balanced effectiveness and\nefficiency that can be directly applied in actual application scenarios, rather\nthan propose a novel detection model. Considering that YOLOv3 has been widely\nused in practice, we develop a new object detector based on YOLOv3. We mainly\ntry to combine various existing tricks that almost not increase the number of\nmodel parameters and FLOPs, to achieve the goal of improving the accuracy of\ndetector as much as possible while ensuring that the speed is almost unchanged.\nSince all experiments in this paper are conducted based on PaddlePaddle, we\ncall it PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a better\nbalance between effectiveness (45.2% mAP) and efficiency (72.9 FPS), surpassing\nthe existing state-of-the-art detectors such as EfficientDet and YOLOv4.Source\ncode is at https://github.com/PaddlePaddle/PaddleDetection.\n", "rewritten_text": "Object detection is a crucial area in computer vision with widespread practical applications.  However, hardware limitations often necessitate a trade-off between detection accuracy and inference speed.  Therefore, achieving a balance between effectiveness and efficiency is paramount. This paper presents an object detector designed for practical deployment, focusing on this balance rather than introducing a novel architecture.  Leveraging the widely used YOLOv3 as a foundation, we developed PP-YOLO by incorporating several existing optimization techniques.  These techniques minimize increases in model parameters and FLOPs, maximizing accuracy while maintaining inference speed.  All experiments were conducted using PaddlePaddle.  PP-YOLO achieves a superior balance of effectiveness (45.2% mAP) and efficiency (72.9 FPS), outperforming state-of-the-art detectors like EfficientDet and YOLOv4. The source code is available at https://github.com/PaddlePaddle/PaddleDetection.\n"}, "2406.15823": {"original_text": "  Understanding the abilities of LLMs to reason about natural language plans,\nsuch as instructional text and recipes, is critical to reliably using them in\ndecision-making systems. A fundamental aspect of plans is the temporal order in\nwhich their steps needs to be executed, which reflects the underlying causal\ndependencies between them. We introduce CaT-Bench, a benchmark of Step Order\nPrediction questions, which test whether a step must necessarily occur before\nor after another in cooking recipe plans. We use this to evaluate how well\nfrontier LLMs understand causal and temporal dependencies. We find that SOTA\nLLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased\ntowards predicting dependence more often, perhaps relying on temporal order of\nsteps as a heuristic. While prompting for explanations and using few-shot\nexamples improve performance, the best F1 result is only 0.73. Further, human\nevaluation of explanations along with answer correctness show that, on average,\nhumans do not agree with model reasoning. Surprisingly, we also find that\nexplaining after answering leads to better performance than normal\nchain-of-thought prompting, and LLM answers are not consistent across questions\nabout the same step pairs. Overall, results show that LLMs' ability to detect\ndependence between steps has significant room for improvement.\n", "rewritten_text": "Understanding large language models' (LLMs) ability to reason about natural language plans, such as instructions and recipes, is crucial for their reliable integration into decision-making systems.  A fundamental aspect of plans is the temporal ordering of steps, reflecting underlying causal dependencies.  We introduce CaT-Bench, a benchmark for step order prediction, which assesses whether one step must precede another in cooking recipes.  We use CaT-Bench to evaluate the comprehension of causal and temporal dependencies in state-of-the-art (SOTA) LLMs.  Results reveal underwhelming performance; the best zero-shot F1 score is only 0.59, indicating a bias towards predicting dependencies, potentially relying on the temporal order as a heuristic.  While prompting with explanations and few-shot examples improves performance (best F1: 0.73), human evaluation reveals disagreement between model reasoning and human judgment regarding both answer correctness and explanations.  Surprisingly, explaining *after* answering yields better performance than standard chain-of-thought prompting.  Furthermore, LLM answers lack consistency across questions concerning the same step pairs.  Overall, our findings demonstrate significant room for improvement in LLMs' ability to detect step dependencies in plans.\n"}, "1907.06007": {"original_text": "  With the development of deep neural networks, the demand for a significant\namount of annotated training data becomes the performance bottlenecks in many\nfields of research and applications. Image synthesis can generate annotated\nimages automatically and freely, which gains increasing attention recently. In\nthis paper, we propose to synthesize scene text images from the 3D virtual\nworlds, where the precise descriptions of scenes, editable\nillumination/visibility, and realistic physics are provided. Different from the\nprevious methods which paste the rendered text on static 2D images, our method\ncan render the 3D virtual scene and text instances as an entirety. In this way,\nreal-world variations, including complex perspective transformations, various\nilluminations, and occlusions, can be realized in our synthesized scene text\nimages. Moreover, the same text instances with various viewpoints can be\nproduced by randomly moving and rotating the virtual camera, which acts as\nhuman eyes. The experiments on the standard scene text detection benchmarks\nusing the generated synthetic data demonstrate the effectiveness and\nsuperiority of the proposed method. The code and synthetic data is available\nat: https://github.com/MhLiao/SynthText3D\n", "rewritten_text": "The increasing demand for large annotated datasets, a significant bottleneck in many research areas and applications, has been exacerbated by the rise of deep neural networks.  Image synthesis, capable of automatically generating annotated images, has gained considerable attention. This paper proposes synthesizing scene text images from 3D virtual worlds, leveraging their precise scene descriptions, controllable illumination and visibility, and realistic physics. Unlike previous methods that paste rendered text onto static 2D images, our approach renders the 3D virtual scene and text instances holistically. This allows for the realistic generation of complex perspective transformations, diverse illumination conditions, and occlusions.  Furthermore, by randomly moving and rotating the virtual camera\u2014simulating human viewpoint variation\u2014we can generate multiple images of the same text instance from different perspectives. Experiments on standard scene text detection benchmarks using our synthetic data demonstrate the effectiveness and superiority of the proposed method. The code and synthetic data are available at: https://github.com/MhLiao/SynthText3D\n"}, "2006.13017": {"original_text": "  Recently, 3D convolutional networks (3D ConvNets) yield good performance in\naction recognition. However, optical flow stream is still needed to ensure\nbetter performance, the cost of which is very high. In this paper, we propose a\nfast but effective way to extract motion features from videos utilizing\nresidual frames as the input data in 3D ConvNets. By replacing traditional\nstacked RGB frames with residual ones, 35.6% and 26.6% points improvements over\ntop-1 accuracy can be obtained on the UCF101 and HMDB51 datasets when ResNet-18\nmodels are trained from scratch. And we achieved the state-of-the-art results\nin this training mode. Analysis shows that better motion features can be\nextracted using residual frames compared to RGB counterpart. By combining with\na simple appearance path, our proposal can be even better than some methods\nusing optical flow streams.\n", "rewritten_text": "Recent studies have shown that 3D convolutional networks (3D ConvNets) achieve promising performance in action recognition.  However, incorporating optical flow often proves necessary for optimal results, incurring significant computational costs. This paper proposes a fast and effective method for extracting motion features from videos by utilizing residual frames as input to 3D ConvNets.  Replacing stacked RGB frames with residual frames yielded a 35.6% and 26.6% improvement in top-1 accuracy on the UCF101 and HMDB51 datasets, respectively, when training ResNet-18 models from scratch.  This approach achieved state-of-the-art results under these training conditions.  Analysis indicates that residual frames enable the extraction of superior motion features compared to their RGB counterparts.  Furthermore, incorporating a simple appearance stream further enhances performance, surpassing some methods that rely on optical flow.\n"}, "2403.15615": {"original_text": "  Conversation is the subject of increasing interest in the social, cognitive,\nand computational sciences. And yet, as conversational datasets continue to\nincrease in size and complexity, researchers lack scalable methods to segment\nspeech-to-text transcripts into conversational turns--the basic building blocks\nof social interaction. We introduce \"NaturalTurn,\" a turn segmentation\nalgorithm designed to accurately capture the dynamics of naturalistic exchange.\nNaturalTurn operates by distinguishing speakers' primary conversational turns\nfrom listeners' secondary utterances, such as backchannels, brief\ninterjections, and other forms of parallel speech that characterize\nconversation. Using data from a large conversation corpus, we show how\nNaturalTurn-derived transcripts demonstrate favorable statistical and\ninferential characteristics compared to transcripts derived from existing\nmethods. The NaturalTurn algorithm represents an improvement in\nmachine-generated transcript processing methods, or \"turn models\" that will\nenable researchers to associate turn-taking dynamics with the broader outcomes\nthat result from social interaction, a central goal of conversation science.\n", "rewritten_text": "Conversation is a subject of growing interest across the social, cognitive, and computational sciences.  However, as conversational datasets expand in size and complexity, researchers lack scalable methods for segmenting speech-to-text transcripts into conversational turns\u2014the fundamental units of social interaction.  We introduce NaturalTurn, a novel turn segmentation algorithm designed to accurately capture the dynamics of naturalistic conversation.  NaturalTurn distinguishes speakers' primary turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of overlapping speech.  Using a large conversation corpus, we demonstrate that NaturalTurn-derived transcripts exhibit superior statistical and inferential properties compared to those generated by existing methods.  The NaturalTurn algorithm represents a significant advancement in machine-generated transcript processing (i.e., turn modeling), enabling researchers to link turn-taking dynamics to broader outcomes of social interaction\u2014a central goal of conversation science.\n"}, "1807.00502": {"original_text": "  The use of deep learning for medical imaging has seen tremendous growth in\nthe research community. One reason for the slow uptake of these systems in the\nclinical setting is that they are complex, opaque and tend to fail silently.\nOutside of the medical imaging domain, the machine learning community has\nrecently proposed several techniques for quantifying model uncertainty (i.e.~a\nmodel knowing when it has failed). This is important in practical settings, as\nwe can refer such cases to manual inspection or correction by humans. In this\npaper, we aim to bring these recent results on estimating uncertainty to bear\non two important outputs in deep learning-based segmentation. The first is\nproducing spatial uncertainty maps, from which a clinician can observe where\nand why a system thinks it is failing. The second is quantifying an image-level\nprediction of failure, which is useful for isolating specific cases and\nremoving them from automated pipelines. We also show that reasoning about\nspatial uncertainty, the first output, is a useful intermediate representation\nfor generating segmentation quality predictions, the second output. We propose\na two-stage architecture for producing these measures of uncertainty, which can\naccommodate any deep learning-based medical segmentation pipeline.\n", "rewritten_text": "Deep learning has seen tremendous growth in medical image analysis research. However, clinical adoption remains slow due to the complexity, opacity, and tendency of these systems to fail silently.  To address this, the machine learning community has developed techniques for quantifying model uncertainty\u2014essentially, enabling models to recognize their own limitations. This is crucial for practical applications, allowing for human review and correction of uncertain predictions.  This paper applies these uncertainty estimation techniques to two key outputs in deep learning-based medical image segmentation:  spatial uncertainty maps, visualizing where and why a system might be failing, and image-level failure predictions, facilitating the identification and removal of unreliable cases from automated workflows. We demonstrate that spatial uncertainty serves as a valuable intermediate representation for generating segmentation quality predictions.  We propose a two-stage architecture for producing these uncertainty measures, compatible with any deep learning-based medical segmentation pipeline.\n"}, "2112.15439": {"original_text": "  This paper aims to conduct a comprehensive study on facial-sketch synthesis\n(FSS). However, due to the high costs of obtaining hand-drawn sketch datasets,\nthere lacks a complete benchmark for assessing the development of FSS\nalgorithms over the last decade. We first introduce a high-quality dataset for\nFSS, named FS2K, which consists of 2,104 image-sketch pairs spanning three\ntypes of sketch styles, image backgrounds, lighting conditions, skin colors,\nand facial attributes. FS2K differs from previous FSS datasets in difficulty,\ndiversity, and scalability and should thus facilitate the progress of FSS\nresearch. Second, we present the largest-scale FSS investigation by reviewing\n89 classical methods, including 25 handcrafted feature-based facial-sketch\nsynthesis approaches, 29 general translation methods, and 35 image-to-sketch\napproaches. Besides, we elaborate comprehensive experiments on the existing 19\ncutting-edge models. Third, we present a simple baseline for FSS, named FSGAN.\nWith only two straightforward components, i.e., facial-aware masking and\nstyle-vector expansion, FSGAN surpasses the performance of all previous\nstate-of-the-art models on the proposed FS2K dataset by a large margin.\nFinally, we conclude with lessons learned over the past years and point out\nseveral unsolved challenges. Our code is available at\nhttps://github.com/DengPingFan/FSGAN.\n", "rewritten_text": "This paper presents a comprehensive study of facial-sketch synthesis (FSS).  The lack of a comprehensive benchmark, due to the high cost of acquiring hand-drawn sketch datasets, has hindered the assessment of FSS algorithm development over the past decade.  To address this, we introduce FS2K, a high-quality dataset comprising 2,104 image-sketch pairs exhibiting variations in sketch style, background, lighting, skin color, and facial attributes.  FS2K's superior difficulty, diversity, and scalability distinguish it from previous datasets and should significantly advance FSS research.\n\nFurthermore, we conduct the largest-scale FSS investigation to date, reviewing 89 classical methods\u2014including 25 handcrafted feature-based, 29 general translation, and 35 image-to-sketch approaches\u2014and comprehensively evaluating 19 state-of-the-art models.  We also propose FSGAN, a simple yet effective FSS baseline.  Employing only facial-aware masking and style-vector expansion, FSGAN significantly outperforms all previous state-of-the-art models on the FS2K dataset.\n\nFinally, we summarize key lessons learned and highlight remaining challenges in the field.  Our code is publicly available at https://github.com/DengPingFan/FSGAN.\n"}, "1804.09803": {"original_text": "  The inference structures and computational complexity of existing deep neural\nnetworks, once trained, are fixed and remain the same for all test images.\nHowever, in practice, it is highly desirable to establish a progressive\nstructure for deep neural networks which is able to adapt its inference process\nand complexity for images with different visual recognition complexity. In this\nwork, we develop a multi-stage progressive structure with integrated confidence\nanalysis and decision policy learning for deep neural networks. This new\nframework consists of a set of network units to be activated in a sequential\nmanner with progressively increased complexity and visual recognition power.\nOur extensive experimental results on the CIFAR-10 and ImageNet datasets\ndemonstrate that the proposed progressive deep neural network is able to obtain\nmore than 10 fold complexity scalability while achieving the state-of-the-art\nperformance using a single network model satisfying different\ncomplexity-accuracy requirements.\n", "rewritten_text": "Existing deep neural networks (DNNs) have fixed inference structures and computational complexities after training, regardless of the input image.  However, adaptive inference structures that adjust complexity based on image characteristics are highly desirable.  This work introduces a multi-stage progressive DNN architecture incorporating confidence analysis and learned decision policies.  This framework sequentially activates network units of increasing complexity and visual recognition capability.  Extensive experiments on CIFAR-10 and ImageNet demonstrate that our progressive DNN achieves over a tenfold increase in complexity scalability while maintaining state-of-the-art performance across varying complexity-accuracy requirements, all within a single model.\n"}, "2312.14988": {"original_text": "  Autoregressive and diffusion models drive the recent breakthroughs on\ntext-to-image generation. Despite their huge success of generating\nhigh-realistic images, a common shortcoming of these models is their high\ninference latency - autoregressive models run more than a thousand times\nsuccessively to produce image tokens and diffusion models convert Gaussian\nnoise into images with many hundreds of denoising steps. In this work, we\nexplore non-autoregressive text-to-image models that efficiently generate\nhundreds of image tokens in parallel. We develop many model variations with\ndifferent learning and inference strategies, initialized text encoders, etc.\nCompared with autoregressive baselines that needs to run one thousand times,\nour model only runs 16 times to generate images of competitive quality with an\norder of magnitude lower inference latency. Our non-autoregressive model with\n346M parameters generates an image of 256$\\times$256 with about one second on\none V100 GPU.\n", "rewritten_text": "Recent breakthroughs in text-to-image generation are driven by autoregressive and diffusion models.  While these models generate highly realistic images, a significant drawback is their high inference latency. Autoregressive models require thousands of sequential steps to produce image tokens, and diffusion models necessitate hundreds of denoising steps to transform Gaussian noise into images.  This work explores non-autoregressive text-to-image models capable of efficiently generating hundreds of image tokens in parallel. We developed several model variations employing diverse learning and inference strategies, including different text encoder initializations.  Compared to autoregressive baselines requiring thousands of iterations, our model generates images of comparable quality in only 16 iterations, achieving an order-of-magnitude reduction in inference latency.  Specifically, our 346M-parameter non-autoregressive model generates a 256\u00d7256 image in approximately one second on a single V100 GPU.\n"}, "2106.16038": {"original_text": "  Recent pretraining models in Chinese neglect two important aspects specific\nto the Chinese language: glyph and pinyin, which carry significant syntax and\nsemantic information for language understanding. In this work, we propose\nChineseBERT, which incorporates both the {\\it glyph} and {\\it pinyin}\ninformation of Chinese characters into language model pretraining. The glyph\nembedding is obtained based on different fonts of a Chinese character, being\nable to capture character semantics from the visual features, and the pinyin\nembedding characterizes the pronunciation of Chinese characters, which handles\nthe highly prevalent heteronym phenomenon in Chinese (the same character has\ndifferent pronunciations with different meanings). Pretrained on large-scale\nunlabeled Chinese corpus, the proposed ChineseBERT model yields significant\nperformance boost over baseline models with fewer training steps. The porpsoed\nmodel achieves new SOTA performances on a wide range of Chinese NLP tasks,\nincluding machine reading comprehension, natural language inference, text\nclassification, sentence pair matching, and competitive performances in named\nentity recognition. Code and pretrained models are publicly available at\nhttps://github.com/ShannonAI/ChineseBert.\n", "rewritten_text": "Recent Chinese pre-training models neglect two crucial aspects of the language: glyphs and pinyin, both of which carry significant syntactic and semantic information.  This work introduces ChineseBERT, a pre-trained language model that incorporates both glyph and pinyin information. Glyph embeddings are derived from variations in character fonts, capturing visual semantic features. Pinyin embeddings represent pronunciation, addressing the prevalent issue of heteronyms (characters with multiple pronunciations and meanings).  Pre-trained on a large-scale unlabeled Chinese corpus, ChineseBERT significantly outperforms baseline models, requiring fewer training steps.  It achieves state-of-the-art (SOTA) performance on various Chinese NLP tasks, including machine reading comprehension, natural language inference, text classification, and sentence pair matching, and demonstrates competitive performance in named entity recognition.  Code and pre-trained models are publicly available at https://github.com/ShannonAI/ChineseBert.\n"}, "2103.05342": {"original_text": "  In this paper, we propose a novel data augmentation strategy named\nCut-Thumbnail, that aims to improve the shape bias of the network. We reduce an\nimage to a certain size and replace the random region of the original image\nwith the reduced image. The generated image not only retains most of the\noriginal image information but also has global information in the reduced\nimage. We call the reduced image as thumbnail. Furthermore, we find that the\nidea of thumbnail can be perfectly integrated with Mixed Sample Data\nAugmentation, so we put one image's thumbnail on another image while the ground\ntruth labels are also mixed, making great achievements on various computer\nvision tasks. Extensive experiments show that Cut-Thumbnail works better than\nstate-of-the-art augmentation strategies across classification, fine-grained\nimage classification, and object detection. On ImageNet classification,\nResNet-50 architecture with our method achieves 79.21\\% accuracy, which is more\nthan 2.8\\% improvement on the baseline.\n", "rewritten_text": "This paper introduces Cut-Thumbnail, a novel data augmentation strategy designed to mitigate shape bias in neural networks.  The method reduces an image to a thumbnail and then replaces a random region of the original image with this thumbnail.  The resulting augmented image retains most of the original image information while incorporating global context from the thumbnail.  We demonstrate that Cut-Thumbnail integrates seamlessly with Mixed Sample Data Augmentation by overlaying one image's thumbnail onto another, while also mixing ground truth labels.  This approach yields significant improvements across various computer vision tasks, including image classification, fine-grained image classification, and object detection.  Extensive experiments show Cut-Thumbnail outperforms state-of-the-art augmentation strategies.  Specifically, on ImageNet classification using a ResNet-50 architecture, our method achieves 79.21% accuracy, a 2.8% improvement over the baseline.\n"}, "2307.15257": {"original_text": "  The complexity of learning problems, such as Generative Adversarial Network\n(GAN) and its variants, multi-task and meta-learning, hyper-parameter learning,\nand a variety of real-world vision applications, demands a deeper understanding\nof their underlying coupling mechanisms. Existing approaches often address\nthese problems in isolation, lacking a unified perspective that can reveal\ncommonalities and enable effective solutions. Therefore, in this work, we\nproposed a new framework, named Learning with Constraint Learning (LwCL), that\ncan holistically examine challenges and provide a unified methodology to tackle\nall the above-mentioned complex learning and vision problems. Specifically,\nLwCL is designed as a general hierarchical optimization model that captures the\nessence of these diverse learning and vision problems. Furthermore, we develop\na gradient-response based fast solution strategy to overcome optimization\nchallenges of the LwCL framework. Our proposed framework efficiently addresses\na wide range of applications in learning and vision, encompassing three\ncategories and nine different problem types. Extensive experiments on synthetic\ntasks and real-world applications verify the effectiveness of our approach. The\nLwCL framework offers a comprehensive solution for tackling complex machine\nlearning and computer vision problems, bridging the gap between theory and\npractice.\n", "rewritten_text": "The complexity of learning problems, including Generative Adversarial Networks (GANs) and their variants, multi-task and meta-learning, hyperparameter optimization, and various real-world vision applications, necessitates a deeper understanding of their underlying interdependencies.  Existing approaches often treat these problems in isolation, lacking a unified perspective that reveals commonalities and facilitates effective solutions.  This work proposes a novel framework, Learning with Constraint Learning (LwCL), to holistically address these challenges and provide a unified methodology.  Specifically, LwCL is a general hierarchical optimization model capturing the essence of diverse learning and vision problems.  We further develop a fast, gradient-response-based solution strategy to overcome LwCL's optimization challenges.  Our framework efficiently addresses a wide range of learning and vision applications, encompassing three categories and nine distinct problem types.  Extensive experiments on synthetic and real-world applications validate its effectiveness.  LwCL offers a comprehensive solution for complex machine learning and computer vision problems, bridging the gap between theory and practice.\n"}, "2309.11119": {"original_text": "  A recent sensor fusion in a Bird's Eye View (BEV) space has shown its utility\nin various tasks such as 3D detection, map segmentation, etc. However, the\napproach struggles with inaccurate camera BEV estimation, and a perception of\ndistant areas due to the sparsity of LiDAR points. In this paper, we propose a\nbroad BEV fusion (BroadBEV) that addresses the problems with a spatial\nsynchronization approach of cross-modality. Our strategy aims to enhance camera\nBEV estimation for a broad-sighted perception while simultaneously improving\nthe completion of LiDAR's sparsity in the entire BEV space. Toward that end, we\ndevise Point-scattering that scatters LiDAR BEV distribution to camera depth\ndistribution. The method boosts the learning of depth estimation of the camera\nbranch and induces accurate location of dense camera features in BEV space. For\nan effective BEV fusion between the spatially synchronized features, we suggest\nColFusion that applies self-attention weights of LiDAR and camera BEV features\nto each other. Our extensive experiments demonstrate that BroadBEV provides a\nbroad-sighted BEV perception with remarkable performance gains.\n", "rewritten_text": "Recent sensor fusion in Bird's Eye View (BEV) space has proven valuable for various tasks, including 3D object detection and map segmentation.  However, existing approaches suffer from inaccuracies in camera-based BEV estimation and limited perception of distant areas due to LiDAR point sparsity.  This paper introduces BroadBEV, a novel BEV fusion method addressing these limitations through a cross-modality spatial synchronization approach.  Our strategy enhances camera BEV estimation for wider-range perception while simultaneously mitigating LiDAR sparsity across the entire BEV space.  To this end, we propose Point-Scattering, which aligns the LiDAR BEV distribution with the camera's depth distribution. This improves camera depth estimation and accurately locates dense camera features within the BEV space.  For effective fusion of these spatially synchronized features, we introduce ColFusion, which applies self-attention weights between LiDAR and camera BEV features.  Extensive experiments demonstrate that BroadBEV achieves significantly improved performance and broader BEV perception.\n"}, "2403.17920": {"original_text": "  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.\n", "rewritten_text": "Recent text-to-4D generation techniques synthesize dynamic 3D scenes using supervision from pre-trained text-to-video models.  However, existing motion representations, such as deformation models or time-dependent neural representations, are limited in their capacity to generate extensive motion; they cannot synthesize motion significantly beyond the volume-rendering bounding box. This limitation in motion modeling contributes to the realism gap between current 4D generation methods and state-of-the-art, near-photorealistic video generation.\n\nTo address this, we propose TC4D: trajectory-conditioned text-to-4D generation.  TC4D factors motion into global and local components. Global scene motion, represented by the bounding box's rigid transformation, is parameterized by a spline-based trajectory.  Local deformations are learned to conform to this global trajectory using supervision from a text-to-video model.  Our approach enables the synthesis of scenes animated along arbitrary trajectories, facilitates compositional scene generation, and significantly improves both the realism and extent of generated motion. We evaluate these improvements qualitatively and through a user study.  Video results are available on our website: https://sherwinbahmani.github.io/tc4d.\n"}, "2407.11144": {"original_text": "  Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25.\n", "rewritten_text": "Even for well-studied sign languages like American Sign Language (ASL), data scarcity significantly limits machine learning research.  This limitation is even more pronounced for the numerous other sign languages used by Deaf and Hard of Hearing communities worldwide.  This paper introduces YouTube-SL-25, a large-scale, open-domain, multilingual corpus of sign language videos with accurately aligned captions sourced from YouTube.  Comprising over 3000 hours of video across more than 25 sign languages, YouTube-SL-25 is: a) more than three times larger than YouTube-ASL; b) the largest parallel sign language dataset to date; and c) the first or largest parallel dataset for many of its constituent languages. We establish baselines for sign-to-text tasks using a unified multilingual, multitask model based on T5, reporting benchmark scores across four sign languages.  Results demonstrate that multilingual transfer learning benefits both high- and low-resource sign languages within YouTube-SL-25.\n"}, "1702.01776": {"original_text": "  In aspect-based sentiment analysis, most existing methods either focus on\naspect/opinion terms extraction or aspect terms categorization. However, each\ntask by itself only provides partial information to end users. To generate more\ndetailed and structured opinion analysis, we propose a finer-grained problem,\nwhich we call category-specific aspect and opinion terms extraction. This\nproblem involves the identification of aspect and opinion terms within each\nsentence, as well as the categorization of the identified terms. To this end,\nwe propose an end-to-end multi-task attention model, where each task\ncorresponds to aspect/opinion terms extraction for a specific category. Our\nmodel benefits from exploring the commonalities and relationships among\ndifferent tasks to address the data sparsity issue. We demonstrate its\nstate-of-the-art performance on three benchmark datasets.\n", "rewritten_text": "Most existing aspect-based sentiment analysis (ABSA) methods focus either on aspect/opinion term extraction or categorization.  However, these approaches provide only partial information. To enable more detailed and structured opinion analysis, we propose a finer-grained task: category-specific aspect and opinion term extraction. This involves identifying and categorizing aspect and opinion terms within each sentence.  We address this with a novel end-to-end multi-task attention model, where each task extracts aspect/opinion terms for a specific category.  Our model leverages the commonalities and relationships between tasks to mitigate data sparsity.  We demonstrate state-of-the-art performance on three benchmark datasets.\n"}, "2301.00399": {"original_text": "  In the present paper, semantic parsing challenges are briefly introduced and\nQDMR formalism in semantic parsing is implemented using sequence to sequence\nmodel with attention but uses only part of speech(POS) as a representation of\nwords of a sentence to make the training as simple and as fast as possible and\nalso avoiding curse of dimensionality as well as overfitting. It is shown how\nsemantic operator prediction could be augmented with other models like the\nCopyNet model or the recursive neural net model.\n", "rewritten_text": "This paper briefly introduces the challenges of semantic parsing and presents an implementation of the QDMR formalism using a sequence-to-sequence model with attention.  To simplify and accelerate training, and to mitigate the curse of dimensionality and overfitting, the model utilizes only part-of-speech (POS) tags as word representations.  Furthermore, we explore how semantic operator prediction can be augmented with other models, such as CopyNet or recursive neural networks.\n"}, "2411.07075": {"original_text": "  To predict upcoming text, language models must in some cases retrieve\nin-context information verbatim. In this report, we investigated how the\nability of language models to retrieve arbitrary in-context nouns developed\nduring training (across time) and as language models trained on the same\ndataset increase in size (across scale). We then asked whether learning of\nin-context retrieval correlates with learning of more challenging zero-shot\nbenchmarks. Furthermore, inspired by semantic effects in human short-term\nmemory, we evaluated the retrieval with respect to a major semantic component\nof target nouns, namely whether they denote a concrete or abstract entity, as\nrated by humans. We show that verbatim in-context retrieval developed in a\nsudden transition early in the training process, after about 1% of the training\ntokens. This was observed across model sizes (from 14M and up to 12B\nparameters), and the transition occurred slightly later for the two smallest\nmodels. We further found that the development of verbatim in-context retrieval\nis positively correlated with the learning of zero-shot benchmarks. Around the\ntransition point, all models showed the advantage of retrieving concrete nouns\nas opposed to abstract nouns. In all but two smallest models, the advantage\ndissipated away toward the end of training.\n", "rewritten_text": "Language models sometimes require verbatim retrieval of in-context information to predict text.  This report investigates the development of this ability in language models during training, examining both its evolution over time (across training epochs) and its scaling behavior with model size. We explored the correlation between the acquisition of in-context retrieval and performance on more challenging zero-shot benchmarks.  Furthermore, drawing inspiration from the semantic effects observed in human short-term memory, we analyzed retrieval performance based on the concreteness/abstractness of target nouns, as rated by human annotators.  Our findings reveal a sharp transition in the development of verbatim in-context retrieval, occurring early in training\u2014after approximately 1% of training tokens had been processed. This transition was consistent across model sizes (14M to 12B parameters), although slightly delayed in the two smallest models.  We also observed a positive correlation between the development of verbatim in-context retrieval and improved zero-shot benchmark performance.  Around the transition point, all models exhibited a preference for retrieving concrete over abstract nouns. However, this preference diminished in all but the two smallest models by the end of training.\n"}, "1911.11351": {"original_text": "  Recently, Human Attribute Recognition (HAR) has become a hot topic due to its\nscientific challenges and application potentials, where localizing attributes\nis a crucial stage but not well handled. In this paper, we propose a novel deep\nlearning approach to HAR, namely Distraction-aware HAR (Da-HAR). It enhances\ndeep CNN feature learning by improving attribute localization through a\ncoarse-to-fine attention mechanism. At the coarse step, a self-mask block is\nbuilt to roughly discriminate and reduce distractions, while at the fine step,\na masked attention branch is applied to further eliminate irrelevant regions.\nThanks to this mechanism, feature learning is more accurate, especially when\nheavy occlusions and complex backgrounds exist. Extensive experiments are\nconducted on the WIDER-Attribute and RAP databases, and state-of-the-art\nresults are achieved, demonstrating the effectiveness of the proposed approach.\n", "rewritten_text": "Human Attribute Recognition (HAR) has recently gained prominence due to its scientific challenges and significant application potential.  Accurate attribute localization, however, remains a critical yet underdeveloped aspect. This paper introduces Distraction-aware HAR (Da-HAR), a novel deep learning approach that enhances deep convolutional neural network (CNN) feature learning through a coarse-to-fine attention mechanism.  This mechanism employs a self-mask block for initial distraction reduction and a masked attention branch for subsequent refinement of irrelevant regions.  This approach improves feature learning accuracy, particularly in the presence of significant occlusions and complex backgrounds.  Extensive experiments on the WIDER Attribute and RAP datasets demonstrate state-of-the-art performance, validating the effectiveness of Da-HAR.\n"}, "2310.08487": {"original_text": "  While multi-modal models have successfully integrated information from image,\nvideo, and audio modalities, integrating graph modality into large language\nmodels (LLMs) remains unexplored. This discrepancy largely stems from the\ninherent divergence between structured graph data and unstructured text data.\nIncorporating graph knowledge provides a reliable source of information,\nenabling potential solutions to address issues in text generation, e.g.,\nhallucination, and lack of domain knowledge. To evaluate the integration of\ngraph knowledge into language models, a dedicated dataset is needed. However,\nthere is currently no benchmark dataset specifically designed for multimodal\ngraph-language models. To address this gap, we propose GraphextQA, a question\nanswering dataset with paired subgraphs, retrieved from Wikidata, to facilitate\nthe evaluation and future development of graph-language models. Additionally,\nwe introduce a baseline model called CrossGNN, which conditions answer\ngeneration on the paired graphs by cross-attending question-aware graph\nfeatures at decoding. The proposed dataset is designed to evaluate\ngraph-language models' ability to understand graphs and make use of it for\nanswer generation. We perform experiments with language-only models and the\nproposed graph-language model to validate the usefulness of the paired graphs\nand to demonstrate the difficulty of the task.\n", "rewritten_text": "Although multimodal models effectively integrate image, video, and audio data, integrating graph data into large language models (LLMs) remains largely unexplored. This gap stems from the inherent difference between structured graph data and unstructured text.  Incorporating graph knowledge offers a reliable information source, potentially addressing issues like hallucination and insufficient domain expertise in text generation.  To evaluate such integration, a dedicated benchmark dataset is crucial; however, none currently exists for multimodal graph-language models.  To address this, we introduce GraphextQA, a question-answering dataset featuring paired subgraphs retrieved from Wikidata, designed to facilitate the evaluation and development of graph-language models.  We also present CrossGNN, a baseline model that conditions answer generation on these paired graphs by cross-attending question-aware graph features during decoding.  GraphextQA assesses a model's ability to understand and utilize graph information for answer generation.  Experiments comparing language-only models with CrossGNN demonstrate both the value of paired graphs and the inherent challenge of this task.\n"}, "cs/0010012": {"original_text": "  We describe a new framework for distilling information from word lattices to\nimprove the accuracy of speech recognition and obtain a more perspicuous\nrepresentation of a set of alternative hypotheses. In the standard MAP decoding\napproach the recognizer outputs the string of words corresponding to the path\nwith the highest posterior probability given the acoustics and a language\nmodel. However, even given optimal models, the MAP decoder does not necessarily\nminimize the commonly used performance metric, word error rate (WER). We\ndescribe a method for explicitly minimizing WER by extracting word hypotheses\nwith the highest posterior probabilities from word lattices. We change the\nstandard problem formulation by replacing global search over a large set of\nsentence hypotheses with local search over a small set of word candidates. In\naddition to improving the accuracy of the recognizer, our method produces a new\nrepresentation of the set of candidate hypotheses that specifies the sequence\nof word-level confusions in a compact lattice format. We study the properties\nof confusion networks and examine their use for other tasks, such as lattice\ncompression, word spotting, confidence annotation, and reevaluation of\nrecognition hypotheses using higher-level knowledge sources.\n", "rewritten_text": "This paper presents a novel framework for distilling information from word lattices to enhance speech recognition accuracy and create a more concise representation of alternative hypotheses.  Standard maximum a posteriori (MAP) decoding outputs the word sequence with the highest posterior probability, given the acoustic input and language model. However, even with optimal models, MAP decoding does not guarantee minimization of the word error rate (WER), a common performance metric.  Our method directly minimizes WER by extracting word hypotheses with the highest posterior probabilities from the word lattice.  We reformulate the problem, replacing a global search across a large set of sentence hypotheses with a local search across a smaller set of word candidates.  Beyond improved recognition accuracy, our approach generates a novel representation of candidate hypotheses: a compact lattice specifying word-level confusions. We analyze the properties of these confusion networks and explore their applications in lattice compression, word spotting, confidence annotation, and the re-evaluation of recognition hypotheses using higher-level knowledge sources.\n"}, "2111.03993": {"original_text": "  Skeleton data is of low dimension. However, there is a trend of using very\ndeep and complicated feedforward neural networks to model the skeleton sequence\nwithout considering the complexity in recent year. In this paper, a simple yet\neffective multi-scale semantics-guided neural network (MS-SGN) is proposed for\nskeleton-based action recognition. We explicitly introduce the high level\nsemantics of joints (joint type and frame index) into the network to enhance\nthe feature representation capability of joints. Moreover, a multi-scale\nstrategy is proposed to be robust to the temporal scale variations. In\naddition, we exploit the relationship of joints hierarchically through two\nmodules, i.e., a joint-level module for modeling the correlations of joints in\nthe same frame and a frame-level module for modeling the temporal dependencies\nof frames. With an order of magnitude smaller model size than most previous\nmethods, MSSGN achieves the state-of-the-art performance on the NTU60, NTU120,\nand SYSU datasets.\n", "rewritten_text": "Skeleton data is low-dimensional.  However, recent years have witnessed a trend toward using deep and complex feedforward neural networks for skeleton sequence modeling, without adequately addressing computational complexity. This paper proposes a simple yet effective multi-scale semantics-guided neural network (MS-SGN) for skeleton-based action recognition.  We explicitly incorporate high-level joint semantics (joint type and frame index) to enhance feature representation.  Furthermore, a multi-scale strategy improves robustness to temporal variations.  Finally, we leverage hierarchical joint relationships through two modules: a joint-level module modeling intra-frame correlations and a frame-level module modeling inter-frame dependencies.  With a model size an order of magnitude smaller than most prior methods, MS-SGN achieves state-of-the-art performance on the NTU60, NTU120, and SYSU datasets.\n"}, "1808.06428": {"original_text": "  This paper presents an approach for automatic detection of Munro's\nMicroabscess in stratum corneum (SC) of human skin biopsy in order to realize a\nmachine assisted diagnosis of Psoriasis. The challenge of detecting neutrophils\nin presence of nucleated cells is solved using the recent advances of deep\nlearning algorithms. Separation of SC layer, extraction of patches from the\nlayer followed by classification of patches with respect to presence or absence\nof neutrophils form the basis of the overall approach which is effected through\nan integration of a U-Net based segmentation network and a capsule network for\nclassification. The novel design of the present capsule net leads to a drastic\nreduction in the number of parameters without any noticeable compromise in the\noverall performance. The research further addresses the challenge of dealing\nwith Mega-pixel images (in 10X) vis-a-vis Giga-pixel ones (in 40X). The\npromising result coming out of an experiment on a dataset consisting of 273\nreal-life images shows that a practical system is possible based on the present\nresearch. The implementation of our system is available at\nhttps://github.com/Anabik/CapsDeMM.\n", "rewritten_text": "This paper presents an approach for the automated detection of Munro microabscesses in the stratum corneum (SC) of human skin biopsies to facilitate machine-assisted diagnosis of psoriasis.  We address the challenge of detecting neutrophils amidst nucleated cells using recent advances in deep learning.  Our approach involves SC layer separation, patch extraction, and patch classification based on neutrophil presence or absence. This is achieved through an integrated system comprising a U-Net-based segmentation network and a novel capsule network for classification.  This capsule network design significantly reduces the number of parameters without compromising performance.  Furthermore, the research addresses the challenges posed by both megapixel (10x) and gigapixel (40x) images.  Experiments on a dataset of 273 real-life images yielded promising results, demonstrating the feasibility of a practical system.  The implementation is available at https://github.com/Anabik/CapsDeMM.\n"}, "1302.1422": {"original_text": "  The variation of word meaning according to the context leads us to enrich the\ntype system of our syntactical and semantic analyser of French based on\ncategorial grammars and Montague semantics (or lambda-DRT). The main advantage\nof a deep semantic analyse is too represent meaning by logical formulae that\ncan be easily used e.g. for inferences. Determiners and quantifiers play a\nfundamental role in the construction of those formulae. But in our rich type\nsystem the usual semantic terms do not work. We propose a solution ins- pired\nby the tau and epsilon operators of Hilbert, kinds of generic elements and\nchoice functions. This approach unifies the treatment of the different determi-\nners and quantifiers as well as the dynamic binding of pronouns. Above all,\nthis fully computational view fits in well within the wide coverage parser\nGrail, both from a theoretical and a practical viewpoint.\n", "rewritten_text": "Contextual variation in word meaning necessitates a richer type system for our French syntactic and semantic analyzer, based on categorial grammars and Montague semantics (or lambda-DRT).  Deep semantic analysis offers the key advantage of representing meaning via logical formulae readily applicable to inference.  Determiners and quantifiers are fundamental to constructing these formulae; however, standard semantic terms prove inadequate within our enriched type system.  We propose a solution inspired by Hilbert's tau and epsilon operators\u2014generic elements and choice functions\u2014which unifies the treatment of diverse determiners and quantifiers, as well as dynamic pronoun binding.  Crucially, this fully computational approach integrates seamlessly with the Grail wide-coverage parser, both theoretically and practically.\n"}, "2012.07315": {"original_text": "  The categorical distribution is a natural representation of uncertainty in\nmulti-class segmentations. In the two-class case the categorical distribution\nreduces to the Bernoulli distribution, for which grayscale morphology provides\na range of useful operations. In the general case, applying morphological\noperations on uncertain multi-class segmentations is not straightforward as an\nimage of categorical distributions is not a complete lattice. Although\nmorphology on color images has received wide attention, this is not so for\ncolor-coded or categorical images and even less so for images of categorical\ndistributions. In this work, we establish a set of requirements for morphology\non categorical distributions by combining classic morphology with a\nprobabilistic view. We then define operators respecting these requirements,\nintroduce protected operations on categorical distributions and illustrate the\nutility of these operators on two example tasks: modeling annotator bias in\nbrain tumor segmentations and segmenting vesicle instances from the predictions\nof a multi-class U-Net.\n", "rewritten_text": "Categorical distributions naturally represent uncertainty in multi-class segmentations.  In the binary case, they reduce to Bernoulli distributions, for which grayscale morphological operations are readily available.  However, extending morphological operations to uncertain multi-class segmentations is challenging because an image of categorical distributions does not form a complete lattice. While morphology on color images is well-studied,  its application to color-coded or categorical images, and especially images of categorical distributions, remains largely unexplored.  This work addresses this gap by establishing requirements for morphology on categorical distributions, combining classical morphological principles with a probabilistic framework. We then define operators satisfying these requirements, introduce protected operations on categorical distributions, and demonstrate their utility through two applications: modeling annotator bias in brain tumor segmentations and segmenting vesicle instances from multi-class U-Net predictions.\n"}, "2410.24219": {"original_text": "  Despite advancements in Text-to-Video (T2V) generation, producing videos with\nrealistic motion remains challenging. Current models often yield static or\nminimally dynamic outputs, failing to capture complex motions described by\ntext. This issue stems from the internal biases in text encoding, which\noverlooks motions, and inadequate conditioning mechanisms in T2V generation\nmodels. To address this, we propose a novel framework called DEcomposed MOtion\n(DEMO), which enhances motion synthesis in T2V generation by decomposing both\ntext encoding and conditioning into content and motion components. Our method\nincludes a content encoder for static elements and a motion encoder for\ntemporal dynamics, alongside separate content and motion conditioning\nmechanisms. Crucially, we introduce text-motion and video-motion supervision to\nimprove the model's understanding and generation of motion. Evaluations on\nbenchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench\ndemonstrate DEMO's superior ability to produce videos with enhanced motion\ndynamics while maintaining high visual quality. Our approach significantly\nadvances T2V generation by integrating comprehensive motion understanding\ndirectly from textual descriptions. Project page:\nhttps://PR-Ryan.github.io/DEMO-project/\n", "rewritten_text": "While significant advancements have been made in text-to-video (T2V) generation, creating videos with realistic motion remains a challenge.  Existing models often produce static or minimally dynamic outputs, failing to accurately represent the complex movements described in text. This limitation arises from biases in text encoding that neglect motion information and insufficient conditioning mechanisms within T2V models.  To overcome this, we introduce DEMO (DEcomposed MOtion), a novel framework that enhances motion synthesis in T2V generation by decomposing both text encoding and conditioning into content and motion components.  Our method employs separate content and motion encoders for static elements and temporal dynamics, respectively, along with distinct content and motion conditioning mechanisms.  Critically, we incorporate text-motion and video-motion supervision to improve the model's comprehension and generation of motion.  Evaluations on established benchmarks\u2014MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench\u2014demonstrate DEMO's superior ability to generate videos with significantly improved motion dynamics while maintaining high visual fidelity.  Our approach represents a substantial advancement in T2V generation by directly integrating comprehensive motion understanding from textual descriptions.  Project page: https://PR-Ryan.github.io/DEMO-project/\n"}, "2406.09386": {"original_text": "  Controllable synthetic data generation can substantially lower the annotation\ncost of training data. Prior works use diffusion models to generate driving\nimages conditioned on the 3D object layout. However, those models are trained\non small-scale datasets like nuScenes, which lack appearance and layout\ndiversity. Moreover, overfitting often happens, where the trained models can\nonly generate images based on the layout data from the validation set of the\nsame dataset. In this work, we introduce a simulator-conditioned scene\ngeneration framework called SimGen that can learn to generate diverse driving\nscenes by mixing data from the simulator and the real world. It uses a novel\ncascade diffusion pipeline to address challenging sim-to-real gaps and\nmulti-condition conflicts. A driving video dataset DIVA is collected to enhance\nthe generative diversity of SimGen, which contains over 147.5 hours of\nreal-world driving videos from 73 locations worldwide and simulated driving\ndata from the MetaDrive simulator. SimGen achieves superior generation quality\nand diversity while preserving controllability based on the text prompt and the\nlayout pulled from a simulator. We further demonstrate the improvements brought\nby SimGen for synthetic data augmentation on the BEV detection and segmentation\ntask and showcase its capability in safety-critical data generation.\n", "rewritten_text": "Controllable synthetic data generation significantly reduces the annotation costs associated with training data.  Previous work has employed diffusion models to generate driving images conditioned on 3D object layouts. However, these models are typically trained on small-scale datasets, such as nuScenes, which lack sufficient appearance and layout diversity.  Consequently, overfitting frequently occurs, limiting generated images to layouts resembling those in the training or validation sets.\n\nThis work introduces SimGen, a simulator-conditioned scene generation framework that learns to generate diverse driving scenes by integrating simulator and real-world data.  A novel cascade diffusion pipeline addresses the challenging sim-to-real gap and multi-condition conflicts.  To enhance generative diversity, we collected DIVA, a driving video dataset comprising over 147.5 hours of real-world driving videos from 73 global locations, supplemented by simulated data from the MetaDrive simulator.\n\nSimGen achieves superior generation quality and diversity while maintaining controllability based on text prompts and simulator-derived layouts.  We demonstrate SimGen's effectiveness in synthetic data augmentation for bird's-eye-view (BEV) detection and segmentation tasks, and showcase its potential for generating safety-critical data.\n"}, "2311.02313": {"original_text": "  Large-scale semantic mapping is crucial for outdoor autonomous agents to\nfulfill high-level tasks such as planning and navigation. This paper proposes a\nnovel method for large-scale 3D semantic reconstruction through implicit\nrepresentations from posed LiDAR measurements alone. We first leverage an\noctree-based and hierarchical structure to store implicit features, then these\nimplicit features are decoded to semantic information and signed distance value\nthrough shallow Multilayer Perceptrons (MLPs). We adopt off-the-shelf\nalgorithms to predict the semantic labels and instance IDs of point clouds. We\nthen jointly optimize the feature embeddings and MLPs parameters with a\nself-supervision paradigm for point cloud geometry and a pseudo-supervision\nparadigm for semantic and panoptic labels. Subsequently, categories and\ngeometric structures for novel points are regressed, and marching cubes are\nexploited to subdivide and visualize the scenes in the inferring stage. For\nscenarios with memory constraints, a map stitching strategy is also developed\nto merge sub-maps into a complete map. Experiments on two real-world datasets,\nSemanticKITTI and SemanticPOSS, demonstrate the superior segmentation\nefficiency and mapping effectiveness of our framework compared to current\nstate-of-the-art 3D LiDAR mapping methods.\n", "rewritten_text": "Large-scale semantic mapping is crucial for outdoor autonomous agents to perform high-level tasks such as planning and navigation. This paper presents a novel method for large-scale 3D semantic reconstruction from posed LiDAR measurements using implicit representations.  We employ an octree-based hierarchical structure to store implicit features, which are then decoded into semantic information and signed distance values using shallow multilayer perceptrons (MLPs).  Off-the-shelf algorithms predict semantic labels and instance IDs for point clouds.  We jointly optimize feature embeddings and MLP parameters using a self-supervised paradigm for point cloud geometry and a pseudo-supervised paradigm for semantic and panoptic labels.  Novel point categories and geometric structures are regressed, and marching cubes are used to subdivide and visualize the reconstructed scene.  To address memory constraints, a map stitching strategy merges sub-maps into a complete map. Experiments on the SemanticKITTI and SemanticPOSS datasets demonstrate that our framework surpasses state-of-the-art 3D LiDAR mapping methods in segmentation efficiency and mapping effectiveness.\n"}, "2203.09333": {"original_text": "  Perceiving the similarity between images has been a long-standing and\nfundamental problem underlying various visual generation tasks. Predominant\napproaches measure the inter-image distance by computing pointwise absolute\ndeviations, which tends to estimate the median of instance distributions and\nleads to blurs and artifacts in the generated images. This paper presents\nMoNCE, a versatile metric that introduces image contrast to learn a calibrated\nmetric for the perception of multifaceted inter-image distances. Unlike vanilla\ncontrast which indiscriminately pushes negative samples from the anchor\nregardless of their similarity, we propose to re-weight the pushing force of\nnegative samples adaptively according to their similarity to the anchor, which\nfacilitates the contrastive learning from informative negative samples. Since\nmultiple patch-level contrastive objectives are involved in image distance\nmeasurement, we introduce optimal transport in MoNCE to modulate the pushing\nforce of negative samples collaboratively across multiple contrastive\nobjectives. Extensive experiments over multiple image translation tasks show\nthat the proposed MoNCE outperforms various prevailing metrics substantially.\nThe code is available at https://github.com/fnzhan/MoNCE.\n", "rewritten_text": "Perceptual similarity between images is a fundamental challenge in various visual generation tasks.  Existing approaches often rely on pointwise absolute deviations to measure inter-image distance, which tends to estimate the median of instance distributions, resulting in blurred and artifacted generated images.  This paper introduces MoNCE, a versatile metric that leverages image contrast to learn a calibrated measure of multifaceted inter-image distances. Unlike standard contrastive methods that indiscriminately repel all negative samples from the anchor image, MoNCE adaptively weights the repulsion force based on the negative sample's similarity to the anchor, thereby focusing contrastive learning on informative negative samples.  Furthermore, to effectively modulate the repulsion force across multiple patch-level contrastive objectives, MoNCE incorporates optimal transport.  Extensive experiments on multiple image translation tasks demonstrate that MoNCE significantly outperforms existing metrics. The code is available at https://github.com/fnzhan/MoNCE.\n"}, "2402.17292": {"original_text": "  Text-to-Avatar generation has recently made significant strides due to\nadvancements in diffusion models. However, most existing work remains\nconstrained by limited diversity, producing avatars with subtle differences in\nappearance for a given text prompt. We design DivAvatar, a novel framework that\ngenerates diverse avatars, empowering 3D creatives with a multitude of distinct\nand richly varied 3D avatars from a single text prompt. Different from most\nexisting work that exploits scene-specific 3D representations such as NeRF,\nDivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse\navatar generation from simply noise sampling in inference time. DivAvatar has\ntwo key designs that help achieve generation diversity and visual quality. The\nfirst is a noise sampling technique during training phase which is critical in\ngenerating diverse appearances. The second is a semantic-aware zoom mechanism\nand a novel depth loss, the former producing appearances of high textual\nfidelity by separate fine-tuning of specific body parts and the latter\nimproving geometry quality greatly by smoothing the generated mesh in the\nfeatures space. Extensive experiments show that DivAvatar is highly versatile\nin generating avatars of diverse appearances.\n", "rewritten_text": "Recent advancements in diffusion models have significantly improved text-to-avatar generation.  However, existing methods often lack diversity, producing avatars with only subtle variations from a single text prompt.  To address this, we introduce DivAvatar, a novel framework that generates diverse and richly varied 3D avatars from a single text prompt, empowering 3D creatives.  Unlike most existing approaches that rely on scene-specific 3D representations like NeRF, DivAvatar fine-tunes a 3D generative model (EVA3D), enabling diverse avatar generation through simple noise sampling during inference.  DivAvatar achieves both diversity and high visual quality through two key design choices:  First, a novel noise sampling technique during training is crucial for generating diverse appearances. Second, a semantic-aware zoom mechanism and a novel depth loss function improve both textual fidelity (by separately fine-tuning specific body parts) and geometric quality (by smoothing the generated mesh in feature space).  Extensive experiments demonstrate DivAvatar's versatility in generating avatars with diverse appearances.\n"}, "2208.13078": {"original_text": "  Owing to the lack of corpora for low-resource languages, current works on\ndialogue generation have mainly focused on English. In this paper, we present\nmDIA, the first large-scale multilingual benchmark for dialogue generation\nacross low- to high-resource languages. It covers real-life conversations in 46\nlanguages across 19 language families. We present baseline results obtained by\nfine-tuning the multilingual, non-dialogue-focused pre-trained model mT5 as\nwell as English-centric, dialogue-focused pre-trained chatbot DialoGPT. The\nresults show that mT5-based models perform better on sacreBLEU and BertScore\nbut worse on diversity. Even though promising results are found in few-shot and\nzero-shot scenarios, there is a large gap between the generation quality in\nEnglish and other languages. We hope that the release of mDIA could encourage\nmore works on multilingual dialogue generation to promote language diversity.\n", "rewritten_text": "Due to the scarcity of corpora for low-resource languages, research on dialogue generation has primarily focused on English.  This paper introduces mDIA, the first large-scale multilingual benchmark for dialogue generation encompassing low- to high-resource languages.  It comprises real-world conversations in 46 languages spanning 19 language families.  We present baseline results achieved by fine-tuning both the multilingual, non-dialogue-focused pre-trained model mT5 and the English-centric, dialogue-focused pre-trained chatbot DialoGPT.  Results indicate that mT5-based models outperform DialoGPT on sacreBLEU and BertScore metrics but exhibit lower diversity. While promising few-shot and zero-shot results were observed, a significant performance gap remains between English and other languages.  We anticipate that the release of mDIA will stimulate further research into multilingual dialogue generation, thereby fostering linguistic diversity.\n"}, "2310.08908": {"original_text": "  The large language model (LLM) has garnered significant attention due to its\nin-context learning mechanisms and emergent capabilities. The research\ncommunity has conducted several pilot studies to apply LLMs to machine\ntranslation tasks and evaluate their performance from diverse perspectives.\nHowever, previous research has primarily focused on the LLM itself and has not\nexplored human intervention in the inference process of LLM. The\ncharacteristics of LLM, such as in-context learning and prompt engineering,\nclosely mirror human cognitive abilities in language tasks, offering an\nintuitive solution for human-in-the-loop generation. In this study, we propose\na human-in-the-loop pipeline that guides LLMs to produce customized outputs\nwith revision instructions. The pipeline initiates by prompting the LLM to\nproduce a draft translation, followed by the utilization of automatic retrieval\nor human feedback as supervision signals to enhance the LLM's translation\nthrough in-context learning. The human-machine interactions generated in this\npipeline are also stored in an external database to expand the in-context\nretrieval database, enabling us to leverage human supervision in an offline\nsetting. We evaluate the proposed pipeline using GPT-3.5-turbo API on five\ndomain-specific benchmarks for German-English translation. The results\ndemonstrate the effectiveness of the pipeline in tailoring in-domain\ntranslations and improving translation performance compared to direct\ntranslation. Additionally, we discuss the results from the following\nperspectives: 1) the effectiveness of different in-context retrieval methods;\n2) the construction of a retrieval database under low-resource scenarios; 3)\nthe observed domains differences; 4) the quantitative analysis of linguistic\nstatistics; and 5) the qualitative analysis of translation cases. The code and\ndata are available at https://github.com/NLP2CT/HIL-MT/.\n", "rewritten_text": "Large language models (LLMs) have garnered significant attention due to their in-context learning and emergent capabilities.  While numerous pilot studies have explored LLM application to machine translation, evaluating performance from various perspectives, prior research has largely neglected the role of human intervention in the inference process.  Given the parallels between LLM characteristics\u2014such as in-context learning and prompt engineering\u2014and human cognitive abilities in language tasks, we propose a human-in-the-loop (HIL) pipeline for customized machine translation.\n\nThis pipeline guides the LLM to produce refined translations using revision instructions.  It begins by prompting the LLM for a draft translation, then employs automatic retrieval or human feedback as supervisory signals to enhance the translation via in-context learning.  The resulting human-machine interactions are stored in an external database, augmenting the in-context retrieval database and enabling offline utilization of human supervision.\n\nWe evaluated this pipeline using the GPT-3.5-turbo API on five domain-specific German-English translation benchmarks.  Results demonstrate the pipeline's effectiveness in generating tailored, in-domain translations and improving performance compared to direct translation.  Our analysis considers: 1) the effectiveness of different in-context retrieval methods; 2) the construction of a retrieval database under low-resource conditions; 3) observed domain differences; 4) quantitative linguistic statistics; and 5) qualitative analysis of translation examples.  The code and data are available at [https://github.com/NLP2CT/HIL-MT/](https://github.com/NLP2CT/HIL-MT/).\n"}, "1011.0519": {"original_text": "  It is usual to consider that standards generate mixed feelings among\nscientists. They are often seen as not really reflecting the state of the art\nin a given domain and a hindrance to scientific creativity. Still, scientists\nshould theoretically be at the best place to bring their expertise into\nstandard developments, being even more neutral on issues that may typically be\nrelated to competing industrial interests. Even if it could be thought of as\neven more complex to think about developping standards in the humanities, we\nwill show how this can be made feasible through the experience gained both\nwithin the Text Encoding Initiative consortium and the International\nOrganisation for Standardisation. By taking the specific case of lexical\nresources, we will try to show how this brings about new ideas for designing\nfuture research infrastructures in the human and social sciences.\n", "rewritten_text": "Scientists often view standards with mixed feelings, perceiving them as lagging behind the state of the art and potentially stifling creativity.  However, scientists are ideally positioned to contribute their expertise to standard development, offering a more neutral perspective than those with competing industrial interests. While developing standards in the humanities may seem more challenging, this paper demonstrates its feasibility using the experiences of the Text Encoding Initiative and the International Organization for Standardization.  Focusing on lexical resources, we illustrate how this approach fosters innovative designs for future research infrastructures in the humanities and social sciences.\n"}, "2305.04451": {"original_text": "  Virtual try-on attracts increasing research attention as a promising way for\nenhancing the user experience for online cloth shopping. Though existing\nmethods can generate impressive results, users need to provide a well-designed\nreference image containing the target fashion clothes that often do not exist.\nTo support user-friendly fashion customization in full-body portraits, we\npropose a multi-modal interactive setting by combining the advantages of both\ntext and texture for multi-level fashion manipulation. With the carefully\ndesigned fashion editing module and loss functions, FashionTex framework can\nsemantically control cloth types and local texture patterns without annotated\npairwise training data. We further introduce an ID recovery module to maintain\nthe identity of input portrait. Extensive experiments have demonstrated the\neffectiveness of our proposed pipeline.\n", "rewritten_text": "Virtual try-on is attracting increasing research attention as a promising method for enhancing the online clothing shopping experience. While existing methods can produce impressive results, they require users to provide high-quality reference images of the desired clothing, which are often unavailable.  To facilitate user-friendly, full-body fashion customization, we propose a multi-modal interactive framework, FashionTex, that leverages both text and texture for multi-level fashion manipulation.  Our carefully designed editing module and loss functions enable semantic control over clothing types and local texture patterns without requiring paired training data.  Furthermore, an identity recovery module preserves the identity of the input portrait.  Extensive experiments demonstrate the effectiveness of our proposed pipeline.\n"}, "2310.05989": {"original_text": "  3D object detection plays a pivotal role in autonomous driving and robotics,\ndemanding precise interpretation of Bird's Eye View (BEV) images. The dynamic\nnature of real-world environments necessitates the use of dynamic query\nmechanisms in 3D object detection to adaptively capture and process the complex\nspatio-temporal relationships present in these scenes. However, prior\nimplementations of dynamic queries have often faced difficulties in effectively\nleveraging these relationships, particularly when it comes to integrating\ntemporal information in a computationally efficient manner. Addressing this\nlimitation, we introduce a framework utilizing dynamic query evolution\nstrategy, harnesses K-means clustering and Top-K attention mechanisms for\nrefined spatio-temporal data processing. By dynamically segmenting the BEV\nspace and prioritizing key features through Top-K attention, our model achieves\na real-time, focused analysis of pertinent scene elements. Our extensive\nevaluation on the nuScenes and Waymo dataset showcases a marked improvement in\ndetection accuracy, setting a new benchmark in the domain of query-based BEV\nobject detection. Our dynamic query evolution strategy has the potential to\npush the boundaries of current BEV methods with enhanced adaptability and\ncomputational efficiency. Project page:\nhttps://github.com/Jiawei-Yao0812/QE-BEV\n", "rewritten_text": "Three-dimensional (3D) object detection is crucial for autonomous driving and robotics, requiring precise interpretation of Bird's Eye View (BEV) images.  The dynamic nature of real-world environments necessitates dynamic query mechanisms to adaptively capture and process complex spatio-temporal relationships.  However, previous dynamic query implementations have struggled to effectively leverage these relationships, especially integrating temporal information efficiently.  To address this, we introduce a framework employing a novel dynamic query evolution strategy. This strategy leverages K-means clustering and Top-K attention mechanisms for refined spatio-temporal data processing. By dynamically segmenting the BEV space and prioritizing key features via Top-K attention, our model achieves real-time, focused analysis of relevant scene elements.  Extensive evaluation on the nuScenes and Waymo datasets demonstrates significant improvements in detection accuracy, establishing a new benchmark for query-based BEV object detection. Our dynamic query evolution strategy promises to advance the state-of-the-art in BEV methods through enhanced adaptability and computational efficiency.  Project page: https://github.com/Jiawei-Yao0812/QE-BEV\n"}, "1907.03513": {"original_text": "  Keeping up to date on emerging entities that appear every day is\nindispensable for various applications, such as social-trend analysis and\nmarketing research. Previous studies have attempted to detect unseen entities\nthat are not registered in a particular knowledge base as emerging entities and\nconsequently find non-emerging entities since the absence of entities in\nknowledge bases does not guarantee their emergence. We therefore introduce a\nnovel task of discovering truly emerging entities when they have just been\nintroduced to the public through microblogs and propose an effective method\nbased on time-sensitive distant supervision, which exploits distinctive\nearly-stage contexts of emerging entities. Experimental results with a\nlarge-scale Twitter archive show that the proposed method achieves 83.2%\nprecision of the top 500 discovered emerging entities, which outperforms\nbaselines based on unseen entity recognition with burst detection. Besides\nnotable emerging entities, our method can discover massive long-tail and\nhomographic emerging entities. An evaluation of relative recall shows that the\nmethod detects 80.4% emerging entities newly registered in Wikipedia; 92.4% of\nthem are discovered earlier than their registration in Wikipedia, and the\naverage lead-time is more than one year (571 days).\n", "rewritten_text": "Identifying newly emerging entities is crucial for applications such as social trend analysis and marketing research.  Previous attempts to detect these entities by identifying those absent from existing knowledge bases have proven unreliable, as the absence of an entity does not guarantee its emergence.  This paper introduces a novel task: discovering truly emerging entities immediately upon their public introduction via microblogs. We propose a method based on time-sensitive distant supervision, leveraging the distinctive contextual features present in the early stages of an entity's emergence.  Experiments using a large-scale Twitter archive demonstrate that our method achieves 83.2% precision in the top 500 discovered emerging entities, significantly outperforming baselines that combine unseen entity recognition with burst detection.  Furthermore, our method effectively identifies not only prominent emerging entities but also a substantial number of long-tail and homographic entities.  A relative recall evaluation reveals that our method detects 80.4% of entities newly registered in Wikipedia, with 92.4% of these detected more than one year (571 days) prior to their Wikipedia registration.\n"}, "2312.04567": {"original_text": "  Recent significant advances in text-to-image models unlock the possibility of\ntraining vision systems using synthetic images, potentially overcoming the\ndifficulty of collecting curated data at scale. It is unclear, however, how\nthese models behave at scale, as more synthetic data is added to the training\nset. In this paper we study the scaling laws of synthetic images generated by\nstate of the art text-to-image models, for the training of supervised models:\nimage classifiers with label supervision, and CLIP with language supervision.\nWe identify several factors, including text prompts, classifier-free guidance\nscale, and types of text-to-image models, that significantly affect scaling\nbehavior. After tuning these factors, we observe that synthetic images\ndemonstrate a scaling trend similar to, but slightly less effective than, real\nimages in CLIP training, while they significantly underperform in scaling when\ntraining supervised image classifiers. Our analysis indicates that the main\nreason for this underperformance is the inability of off-the-shelf\ntext-to-image models to generate certain concepts, a limitation that\nsignificantly impairs the training of image classifiers. Our findings also\nsuggest that scaling synthetic data can be particularly effective in scenarios\nsuch as: (1) when there is a limited supply of real images for a supervised\nproblem (e.g., fewer than 0.5 million images in ImageNet), (2) when the\nevaluation dataset diverges significantly from the training data, indicating\nthe out-of-distribution scenario, or (3) when synthetic data is used in\nconjunction with real images, as demonstrated in the training of CLIP models.\n", "rewritten_text": "Recent advances in text-to-image models enable training vision systems using synthetic images, potentially mitigating the challenges of large-scale curated data collection.  However, the scaling behavior of these models with increasing synthetic data remains unclear.  This paper investigates the scaling laws of synthetic images generated by state-of-the-art text-to-image models for training supervised models: image classifiers (with label supervision) and CLIP (with language supervision).  We identify several key factors influencing scaling behavior, including text prompts, classifier-free guidance scale, and the choice of text-to-image model.  After optimizing these factors, we observe that synthetic images exhibit a scaling trend similar to, but slightly less efficient than, real images in CLIP training.  Conversely, they significantly underperform when training supervised image classifiers.  Our analysis attributes this underperformance to the inability of off-the-shelf text-to-image models to generate certain concepts, hindering effective image classifier training.  Our findings suggest that scaling synthetic data can be particularly beneficial in scenarios with: (1) limited real image availability for supervised learning (e.g., fewer than 0.5 million ImageNet images), (2) significant divergence between training and evaluation datasets (out-of-distribution scenarios), or (3) when combined with real images, as demonstrated in CLIP training.\n"}, "1909.1244": {"original_text": "  Recently, pre-trained language models have achieved remarkable success in a\nbroad range of natural language processing tasks. However, in multilingual\nsetting, it is extremely resource-consuming to pre-train a deep language model\nover large-scale corpora for each language. Instead of exhaustively\npre-training monolingual language models independently, an alternative solution\nis to pre-train a powerful multilingual deep language model over large-scale\ncorpora in hundreds of languages. However, the vocabulary size for each\nlanguage in such a model is relatively small, especially for low-resource\nlanguages. This limitation inevitably hinders the performance of these\nmultilingual models on tasks such as sequence labeling, wherein in-depth\ntoken-level or sentence-level understanding is essential.\n  In this paper, inspired by previous methods designed for monolingual\nsettings, we investigate two approaches (i.e., joint mapping and mixture\nmapping) based on a pre-trained multilingual model BERT for addressing the\nout-of-vocabulary (OOV) problem on a variety of tasks, including part-of-speech\ntagging, named entity recognition, machine translation quality estimation, and\nmachine reading comprehension. Experimental results show that using mixture\nmapping is more promising. To the best of our knowledge, this is the first work\nthat attempts to address and discuss the OOV issue in multilingual settings.\n", "rewritten_text": "Pre-trained language models have achieved remarkable success in numerous natural language processing (NLP) tasks.  However, training a deep language model on large-scale corpora for each language individually is extremely resource-intensive in multilingual settings.  A more efficient alternative is to pre-train a single, powerful multilingual model on corpora encompassing hundreds of languages.  Nevertheless, this approach often results in relatively small vocabularies for each language, particularly low-resource languages. This limitation significantly impacts performance on tasks like sequence labeling, which require fine-grained token- and sentence-level understanding.\n\nThis paper investigates two approaches\u2014joint mapping and mixture mapping\u2014adapted from monolingual methods, to address the out-of-vocabulary (OOV) problem in a pre-trained multilingual BERT model.  We evaluate these approaches on various tasks, including part-of-speech tagging, named entity recognition, machine translation quality estimation, and machine reading comprehension.  Our experimental results demonstrate that mixture mapping is the more effective strategy.  To our knowledge, this is the first work to explicitly address and analyze the OOV problem within a multilingual context.\n"}, "2311.05821": {"original_text": "  While recent advances have boosted LM proficiency in linguistic benchmarks,\nLMs consistently struggle to reason correctly on complex tasks like\nmathematics. We turn to Reinforcement Learning from Human Feedback (RLHF) as a\nmethod with which to shape model reasoning processes. In particular, we explore\ntwo reward schemes, outcome-supervised reward models (ORMs) and\nprocess-supervised reward models (PRMs), to optimize for logical reasoning. Our\nresults show that the fine-grained reward provided by PRM-based methods\nenhances accuracy on simple mathematical reasoning (GSM8K) while, unexpectedly,\nreducing performance in complex tasks (MATH). Furthermore, we show the critical\nrole reward aggregation functions play in model performance. Providing\npromising avenues for future research, our study underscores the need for\nfurther exploration into fine-grained reward modeling for more reliable\nlanguage models.\n", "rewritten_text": "Recent advancements have significantly improved large language model (LLM) performance on linguistic benchmarks. However, LLMs consistently struggle with complex reasoning tasks, such as mathematics.  We address this limitation using Reinforcement Learning from Human Feedback (RLHF), exploring two reward schemes: outcome-supervised reward models (ORMs) and process-supervised reward models (PRMs), to optimize for logical reasoning.  Our results demonstrate that the fine-grained rewards provided by PRMs improve accuracy on simpler mathematical reasoning tasks (GSM8K) but, unexpectedly, decrease performance on more complex tasks (MATH).  Furthermore, we highlight the crucial role of reward aggregation functions in model performance.  These findings suggest promising avenues for future research and underscore the need for further investigation into fine-grained reward modeling to enhance the reliability of LLMs.\n"}, "1309.6379": {"original_text": "  We propose a large deformation diffeomorphic metric mapping algorithm to\nalign multiple b-value diffusion weighted imaging (mDWI) data, specifically\nacquired via hybrid diffusion imaging (HYDI), denoted as LDDMM-HYDI. We then\npropose a Bayesian model for estimating the white matter atlas from HYDIs. We\nadopt the work given in Hosseinbor et al. (2012) and represent the q-space\ndiffusion signal with the Bessel Fourier orientation reconstruction (BFOR)\nsignal basis. The BFOR framework provides the representation of mDWI in the\nq-space and thus reduces memory requirement. In addition, since the BFOR signal\nbasis is orthonormal, the L2 norm that quantifies the differences in the\nq-space signals of any two mDWI datasets can be easily computed as the sum of\nthe squared differences in the BFOR expansion coefficients. In this work, we\nshow that the reorientation of the $q$-space signal due to spatial\ntransformation can be easily defined on the BFOR signal basis. We incorporate\nthe BFOR signal basis into the LDDMM framework and derive the gradient descent\nalgorithm for LDDMM-HYDI with explicit orientation optimization. Additionally,\nwe extend the previous Bayesian atlas estimation framework for scalar-valued\nimages to HYDIs and derive the expectation-maximization algorithm for solving\nthe HYDI atlas estimation problem. Using real HYDI datasets, we show the\nBayesian model generates the white matter atlas with anatomical details.\nMoreover, we show that it is important to consider the variation of mDWI\nreorientation due to a small change in diffeomorphic transformation in the\nLDDMM-HYDI optimization and to incorporate the full information of HYDI for\naligning mDWI.\n", "rewritten_text": "We propose a large deformation diffeomorphic metric mapping (LDDMM) algorithm to align multiple b-value diffusion-weighted imaging (mDWI) data acquired via hybrid diffusion imaging (HYDI), termed LDDMM-HYDI.  We further propose a Bayesian model for estimating a white matter atlas from HYDI data.  Following Hosseinbor et al. (2012), we represent the q-space diffusion signal using the Bessel Fourier orientation reconstruction (BFOR) signal basis.  This BFOR framework provides a compact q-space representation of mDWI, reducing memory requirements.  Furthermore, the orthonormality of the BFOR basis simplifies computation of the L2 norm quantifying differences between q-space signals; it reduces to the sum of squared differences in BFOR expansion coefficients.  We demonstrate that reorientation of the q-space signal due to spatial transformation is readily defined within the BFOR basis.  We integrate the BFOR basis into the LDDMM framework, deriving a gradient descent algorithm for LDDMM-HYDI with explicit orientation optimization.  We extend a previous Bayesian atlas estimation framework for scalar images to HYDI data, deriving an expectation-maximization algorithm for HYDI atlas estimation.  Results using real HYDI datasets demonstrate that our Bayesian model generates a white matter atlas with fine anatomical detail.  Crucially, our results highlight the importance of considering mDWI reorientation variations due to small changes in diffeomorphic transformations during LDDMM-HYDI optimization, and of incorporating the full information content of HYDI for accurate mDWI alignment.\n"}, "2101.03848": {"original_text": "  Convolutional neural networks (CNNs) have been widely used in various vision\ntasks, e.g. image classification, semantic segmentation, etc. Unfortunately,\nstandard 2D CNNs are not well suited for spherical signals such as panorama\nimages or spherical projections, as the sphere is an unstructured grid. In this\npaper, we present Spherical Transformer which can transform spherical signals\ninto vectors that can be directly processed by standard CNNs such that many\nwell-designed CNNs architectures can be reused across tasks and datasets by\npretraining. To this end, the proposed method first uses local structured\nsampling methods such as HEALPix to construct a transformer grid by using the\ninformation of spherical points and its adjacent points, and then transforms\nthe spherical signals to the vectors through the grid. By building the\nSpherical Transformer module, we can use multiple CNN architectures directly.\nWe evaluate our approach on the tasks of spherical MNIST recognition, 3D object\nclassification and omnidirectional image semantic segmentation. For 3D object\nclassification, we further propose a rendering-based projection method to\nimprove the performance and a rotational-equivariant model to improve the\nanti-rotation ability. Experimental results on three tasks show that our\napproach achieves superior performance over state-of-the-art methods.\n", "rewritten_text": "Convolutional neural networks (CNNs) are widely used in various vision tasks, including image classification and semantic segmentation.  However, standard 2D CNNs are unsuitable for spherical signals, such as panoramic images or spherical projections, due to the sphere's unstructured nature. This paper introduces the Spherical Transformer, which converts spherical signals into vectors directly processable by standard CNNs. This allows for the reuse of numerous well-designed CNN architectures across tasks and datasets through pre-training.  The method employs local structured sampling techniques, such as HEALPix, to construct a transformer grid using information from spherical points and their neighbors.  This grid then transforms the spherical signals into vectors.  The Spherical Transformer module enables the direct application of multiple CNN architectures.  We evaluate our approach on spherical MNIST recognition, 3D object classification, and omnidirectional image semantic segmentation.  For 3D object classification, we introduce a rendering-based projection method to enhance performance and a rotation-equivariant model to improve rotational invariance.  Experimental results across these three tasks demonstrate superior performance compared to state-of-the-art methods.\n"}, "2209.02686": {"original_text": "  Image-to-image translation has played an important role in enabling synthetic\ndata for computer vision. However, if the source and target domains have a\nlarge semantic mismatch, existing techniques often suffer from source content\ncorruption aka semantic flipping. To address this problem, we propose a new\nparadigm for image-to-image translation using Vector Symbolic Architectures\n(VSA), a theoretical framework which defines algebraic operations in a\nhigh-dimensional vector (hypervector) space. We introduce VSA-based constraints\non adversarial learning for source-to-target translations by learning a\nhypervector mapping that inverts the translation to ensure consistency with\nsource content. We show both qualitatively and quantitatively that our method\nimproves over other state-of-the-art techniques.\n", "rewritten_text": "Image-to-image translation has been crucial for generating synthetic data in computer vision.  However, significant semantic mismatches between source and target domains often lead to source content corruption, also known as semantic flipping, in existing methods.  To mitigate this, we propose a novel image-to-image translation paradigm using Vector Symbolic Architectures (VSAs).  VSAs provide a theoretical framework for defining algebraic operations within a high-dimensional vector (hypervector) space.  We incorporate VSA-based constraints into adversarial learning for source-to-target translation by learning a hypervector mapping that inverts the translation, thereby ensuring source content consistency.  Qualitative and quantitative results demonstrate that our method outperforms state-of-the-art techniques.\n"}, "2305.14576": {"original_text": "  Pre-trained language models (PLMs) have ignited a surge in demand for\neffective fine-tuning techniques, particularly in low-resource domains and\nlanguages. Active learning (AL), a set of algorithms designed to decrease\nlabeling costs by minimizing label complexity, has shown promise in confronting\nthe labeling bottleneck. In parallel, adapter modules designed for\nparameter-efficient fine-tuning (PEFT) have demonstrated notable potential in\nlow-resource settings. However, the interplay between AL and adapter-based PEFT\nremains unexplored. We present an empirical study of PEFT behavior with AL in\nlow-resource settings for text classification tasks. Our findings affirm the\nsuperiority of PEFT over full-fine tuning (FFT) in low-resource settings and\ndemonstrate that this advantage persists in AL setups. We further examine the\nproperties of PEFT and FFT through the lens of forgetting dynamics and\ninstance-level representations, where we find that PEFT yields more stable\nrepresentations of early and middle layers compared to FFT. Our research\nunderscores the synergistic potential of AL and PEFT in low-resource settings,\npaving the way for advancements in efficient and effective fine-tuning.\n", "rewritten_text": "Pre-trained language models (PLMs) have driven increased demand for effective fine-tuning techniques, especially in low-resource domains and languages.  Active learning (AL), which minimizes labeling costs by strategically selecting data for annotation, offers a promising solution to the labeling bottleneck.  Similarly, parameter-efficient fine-tuning (PEFT) methods, such as adapter modules, have shown significant potential in low-resource settings.  However, the interaction between AL and adapter-based PEFT remains uninvestigated.\n\nThis empirical study examines the performance of PEFT with AL in low-resource text classification.  Our results confirm the superiority of PEFT over full fine-tuning (FFT) in low-resource scenarios, a benefit that persists when using AL.  Furthermore, we analyze the forgetting dynamics and instance-level representations of PEFT and FFT, revealing that PEFT maintains more stable representations in early and middle layers.  Our findings highlight the synergistic potential of combining AL and PEFT for efficient and effective fine-tuning in low-resource settings.\n"}, "2308.15448": {"original_text": "  The negative effects of online bullying and harassment are increasing with\nInternet popularity, especially in social media. One solution is using natural\nlanguage processing (NLP) and machine learning (ML) methods for the automatic\ndetection of harmful remarks, but these methods are limited in low-resource\nlanguages like the Chittagonian dialect of Bangla.This study focuses on\ndetecting vulgar remarks in social media using supervised ML and deep learning\nalgorithms.Logistic Regression achieved promising accuracy (0.91) while simple\nRNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the\nissue that NN algorithms require more data.\n", "rewritten_text": "The negative effects of online bullying and harassment are escalating with the increasing popularity of the internet, particularly on social media.  One proposed solution involves using natural language processing (NLP) and machine learning (ML) methods for the automatic detection of harmful remarks. However, these methods are often limited in low-resource languages such as the Chittagonian dialect of Bangla. This study focuses on detecting vulgar remarks on social media using supervised machine learning and deep learning algorithms.  Logistic Regression achieved a promising accuracy of 0.91, while simpler recurrent neural networks (RNNs) using Word2Vec and FastText achieved lower accuracy (0.84\u20130.90), highlighting the need for significantly more data to train effective neural network algorithms.\n"}, "2305.06052": {"original_text": "  Quantization is a widely adopted technique for deep neural networks to reduce\nthe memory and computational resources required. However, when quantized, most\nmodels would need a suitable calibration process to keep their performance\nintact, which requires data from the target domain, such as a fraction of the\ndataset used in model training and model validation (i.e. calibration dataset).\n  In this study, we investigate the use of synthetic data as a substitute for\nthe calibration with real data for the quantization method. We propose a data\ngeneration method based on Generative Adversarial Networks that are trained\nprior to the model quantization step. We compare the performance of models\nquantized using data generated by StyleGAN2-ADA and our pre-trained DiStyleGAN,\nwith quantization using real data and an alternative data generation method\nbased on fractal images. Overall, the results of our experiments demonstrate\nthe potential of leveraging synthetic data for calibration during the\nquantization process. In our experiments, the percentage of accuracy\ndegradation of the selected models was less than 0.6%, with our best\nperformance achieved on MobileNetV2 (0.05%). The code is available at:\nhttps://github.com/ThanosM97/gsoc2022-openvino\n", "rewritten_text": "Quantization is a widely adopted technique for reducing the memory and computational requirements of deep neural networks.  However, most quantized models require calibration to maintain performance. This typically involves a calibration dataset derived from the target domain, such as a subset of the training or validation data.  This study investigates the use of synthetic data to replace real-data calibration for quantization. We propose a generative adversarial network (GAN)-based method for generating this synthetic data, trained *before* model quantization.  We compare the performance of models quantized using data generated by StyleGAN2-ADA and our pre-trained DiStyleGAN against quantization with real data and an alternative method using fractal images.  Our experiments demonstrate the potential of synthetic data for calibration, showing an accuracy degradation of less than 0.6% for the selected models; MobileNetV2 achieved the best performance with only a 0.05% degradation. The code is available at: https://github.com/ThanosM97/gsoc2022-openvino\n"}, "2406.15719": {"original_text": "  Convolutional Neural Networks (CNNs) and vision transformers (ViTs) have\nshown excellent capability in complex hyperspectral image (HSI) classification.\nHowever, these models require a significant number of training data and are\ncomputational resources. On the other hand, modern Multi-Layer Perceptrons\n(MLPs) have demonstrated great classification capability. These modern\nMLP-based models require significantly less training data compared to CNNs and\nViTs, achieving the state-of-the-art classification accuracy. Recently,\nKolmogorov-Arnold Networks (KANs) were proposed as viable alternatives for\nMLPs. Because of their internal similarity to splines and their external\nsimilarity to MLPs, KANs are able to optimize learned features with remarkable\naccuracy in addition to being able to learn new features. Thus, in this study,\nwe assess the effectiveness of KANs for complex HSI data classification.\nMoreover, to enhance the HSI classification accuracy obtained by the KANs, we\ndevelop and propose a Hybrid architecture utilizing 1D, 2D, and 3D KANs. To\ndemonstrate the effectiveness of the proposed KAN architecture, we conducted\nextensive experiments on three newly created HSI benchmark datasets:\nQUH-Pingan, QUH-Tangdaowan, and QUH-Qingyun. The results underscored the\ncompetitive or better capability of the developed hybrid KAN-based model across\nthese benchmark datasets over several other CNN- and ViT-based algorithms,\nincluding 1D-CNN, 2DCNN, 3D CNN, VGG-16, ResNet-50, EfficientNet, RNN, and ViT.\nThe code are publicly available at (https://github.com/aj1365/HSIConvKAN)\n", "rewritten_text": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) have demonstrated excellent capabilities in complex hyperspectral image (HSI) classification.  However, these models require substantial training data and computational resources.  In contrast, modern Multi-Layer Perceptrons (MLPs) have shown strong classification performance with significantly less training data than CNNs and ViTs, achieving state-of-the-art accuracy.  Recently proposed Kolmogorov-Arnold Networks (KANs), owing to their internal similarity to splines and external resemblance to MLPs, offer a viable alternative.  KANs can optimize learned features with remarkable accuracy while also learning new features.  This study assesses the effectiveness of KANs for complex HSI data classification.  Furthermore, to improve HSI classification accuracy, we propose a hybrid architecture utilizing 1D, 2D, and 3D KANs.  We evaluated this architecture through extensive experiments on three newly created HSI benchmark datasets: QUH-Pingan, QUH-Tangdaowan, and QUH-Qingyun.  Results demonstrate that the hybrid KAN-based model achieves competitive or superior performance compared to several CNN- and ViT-based algorithms, including 1D-CNN, 2D-CNN, 3D-CNN, VGG-16, ResNet-50, EfficientNet, RNN, and ViT, across these benchmark datasets. The code is publicly available at [https://github.com/aj1365/HSIConvKAN](https://github.com/aj1365/HSIConvKAN).\n"}, "1905.00413": {"original_text": "  We explore the problem of view synthesis from a narrow baseline pair of\nimages, and focus on generating high-quality view extrapolations with plausible\ndisocclusions. Our method builds upon prior work in predicting a multiplane\nimage (MPI), which represents scene content as a set of RGB$\\alpha$ planes\nwithin a reference view frustum and renders novel views by projecting this\ncontent into the target viewpoints. We present a theoretical analysis showing\nhow the range of views that can be rendered from an MPI increases linearly with\nthe MPI disparity sampling frequency, as well as a novel MPI prediction\nprocedure that theoretically enables view extrapolations of up to $4\\times$ the\nlateral viewpoint movement allowed by prior work. Our method ameliorates two\nspecific issues that limit the range of views renderable by prior methods: 1)\nWe expand the range of novel views that can be rendered without depth\ndiscretization artifacts by using a 3D convolutional network architecture along\nwith a randomized-resolution training procedure to allow our model to predict\nMPIs with increased disparity sampling frequency. 2) We reduce the repeated\ntexture artifacts seen in disocclusions by enforcing a constraint that the\nappearance of hidden content at any depth must be drawn from visible content at\nor behind that depth. Please see our results video at:\nhttps://www.youtube.com/watch?v=aJqAaMNL2m4.\n", "rewritten_text": "This paper addresses the challenge of view synthesis from a narrow baseline image pair, focusing on generating high-quality view extrapolations with realistic disocclusions.  Our method extends previous work on multiplane image (MPI) prediction.  MPIs represent scene content as a set of RGB\u03b1 planes within a reference view frustum, enabling novel view rendering by projecting this content to target viewpoints.  We provide a theoretical analysis demonstrating a linear relationship between the range of renderable views and the MPI disparity sampling frequency.  Furthermore, we introduce a novel MPI prediction procedure that theoretically allows view extrapolations up to four times the lateral movement permitted by prior methods.  Our approach mitigates two key limitations of existing techniques: 1) We expand the range of renderable views without depth discretization artifacts by employing a 3D convolutional neural network architecture and a randomized-resolution training procedure, enabling higher disparity sampling frequencies in our predicted MPIs. 2) We reduce repeated texture artifacts in disocclusions by enforcing a constraint that the appearance of hidden content at any depth must be consistent with visible content at or behind that depth.  For a visual demonstration of our results, please see our video: https://www.youtube.com/watch?v=aJqAaMNL2m4.\n"}, "1909.10579": {"original_text": "  Neural language models (LMs) perform well on tasks that require sensitivity\nto syntactic structure. Drawing on the syntactic priming paradigm from\npsycholinguistics, we propose a novel technique to analyze the representations\nthat enable such success. By establishing a gradient similarity metric between\nstructures, this technique allows us to reconstruct the organization of the\nLMs' syntactic representational space. We use this technique to demonstrate\nthat LSTM LMs' representations of different types of sentences with relative\nclauses are organized hierarchically in a linguistically interpretable manner,\nsuggesting that the LMs track abstract properties of the sentence.\n", "rewritten_text": "Neural language models (LMs) excel at tasks requiring syntactic sensitivity.  Drawing on the psycholinguistic paradigm of syntactic priming, we introduce a novel technique for analyzing the underlying representations responsible for this success.  This technique employs a gradient similarity metric to reconstruct the organization of the LMs' syntactic representational space.  We demonstrate, using this technique, that the representations of different relative clause sentence types in LSTM LMs are organized hierarchically in a linguistically interpretable way, suggesting that these models track abstract syntactic properties.\n"}, "1804.04213": {"original_text": "  We study how to synthesize novel views of human body from a single image.\nThough recent deep learning based methods work well for rigid objects, they\noften fail on objects with large articulation, like human bodies. The core step\nof existing methods is to fit a map from the observable views to novel views by\nCNNs; however, the rich articulation modes of human body make it rather\nchallenging for CNNs to memorize and interpolate the data well. To address the\nproblem, we propose a novel deep learning based pipeline that explicitly\nestimates and leverages the geometry of the underlying human body. Our new\npipeline is a composition of a shape estimation network and an image generation\nnetwork, and at the interface a perspective transformation is applied to\ngenerate a forward flow for pixel value transportation. Our design is able to\nfactor out the space of data variation and makes learning at each step much\neasier. Empirically, we show that the performance for pose-varying objects can\nbe improved dramatically. Our method can also be applied on real data captured\nby 3D sensors, and the flow generated by our methods can be used for generating\nhigh quality results in higher resolution.\n", "rewritten_text": "We present a novel deep learning approach for synthesizing novel views of the human body from a single image. While recent deep learning methods effectively handle rigid objects, they often fail with highly articulated objects like human bodies.  Existing methods typically rely on convolutional neural networks (CNNs) to map observable views to novel views; however, the complex articulation of the human body makes effective memorization and interpolation challenging for CNNs.  To overcome this limitation, we propose a pipeline that explicitly estimates and utilizes the underlying human body geometry.  Our pipeline comprises a shape estimation network and an image generation network, connected by a perspective transformation that generates a forward flow for pixel value transfer. This design decouples sources of data variation, simplifying the learning process at each stage.  Empirical results demonstrate a significant improvement in performance for pose-varying objects. Furthermore, our method is applicable to real-world data acquired from 3D sensors, and the generated flow enables the creation of high-quality, high-resolution results.\n"}, "2401.04354": {"original_text": "  With the explosive growth of video data in real-world applications, a\ncomprehensive representation of videos becomes increasingly important. In this\npaper, we address the problem of video scene recognition, whose goal is to\nlearn a high-level video representation to classify scenes in videos. Due to\nthe diversity and complexity of video contents in realistic scenarios, this\ntask remains a challenge. Most existing works identify scenes for videos only\nfrom visual or textual information in a temporal perspective, ignoring the\nvaluable information hidden in single frames, while several earlier studies\nonly recognize scenes for separate images in a non-temporal perspective. We\nargue that these two perspectives are both meaningful for this task and\ncomplementary to each other, meanwhile, externally introduced knowledge can\nalso promote the comprehension of videos. We propose a novel two-stream\nframework to model video representations from multiple perspectives, i.e.\ntemporal and non-temporal perspectives, and integrate the two perspectives in\nan end-to-end manner by self-distillation. Besides, we design a\nknowledge-enhanced feature fusion and label prediction method that contributes\nto naturally introducing knowledge into the task of video scene recognition.\nExperiments conducted on a real-world dataset demonstrate the effectiveness of\nour proposed method.\n", "rewritten_text": "The explosive growth of video data in real-world applications necessitates comprehensive video representation. This paper addresses video scene recognition\u2014the task of learning a high-level video representation for scene classification.  The diversity and complexity of real-world video content make this a challenging problem.  Existing methods typically rely solely on temporal visual or textual information, neglecting valuable intra-frame information, or conversely, focus on individual image recognition without temporal context.  We argue that both temporal and non-temporal perspectives, complemented by external knowledge, are crucial.  Therefore, we propose a novel two-stream framework that models video representations from both perspectives, integrating them end-to-end via self-distillation.  Furthermore, we introduce a knowledge-enhanced feature fusion and label prediction method for natural knowledge integration.  Experiments on a real-world dataset demonstrate the effectiveness of our approach.\n"}, "2211.08112": {"original_text": "  Active Learning (AL) is a powerful tool for learning with less labeled data,\nin particular, for specialized domains, like legal documents, where unlabeled\ndata is abundant, but the annotation requires domain expertise and is thus\nexpensive. Recent works have shown the effectiveness of AL strategies for\npre-trained language models. However, most AL strategies require a set of\nlabeled samples to start with, which is expensive to acquire. In addition,\npre-trained language models have been shown unstable during fine-tuning with\nsmall datasets, and their embeddings are not semantically meaningful. In this\nwork, we propose a pipeline for effectively using active learning with\npre-trained language models in the legal domain. To this end, we leverage the\navailable unlabeled data in three phases. First, we continue pre-training the\nmodel to adapt it to the downstream task. Second, we use knowledge distillation\nto guide the model's embeddings to a semantically meaningful space. Finally, we\npropose a simple, yet effective, strategy to find the initial set of labeled\nsamples with fewer actions compared to existing methods. Our experiments on\nContract-NLI, adapted to the classification task, and LEDGAR benchmarks show\nthat our approach outperforms standard AL strategies, and is more efficient.\nFurthermore, our pipeline reaches comparable results to the fully-supervised\napproach with a small performance gap, and dramatically reduced annotation\ncost. Code and the adapted data will be made available.\n", "rewritten_text": "Active learning (AL) is a powerful tool for learning with limited labeled data, particularly in specialized domains like legal document processing where unlabeled data is abundant but annotation is expensive due to the required domain expertise.  While recent work demonstrates the effectiveness of AL strategies for pre-trained language models (PLMs), most require an initial set of labeled samples\u2014themselves costly to acquire.  Furthermore, PLMs can be unstable during fine-tuning with small datasets, and their embeddings may lack semantic meaning.\n\nThis work proposes a three-phase pipeline for effectively employing AL with PLMs in the legal domain, leveraging available unlabeled data.  First, we continue pre-training the PLM to adapt it to the downstream task. Second, we employ knowledge distillation to guide the model's embeddings toward a semantically meaningful space. Finally, we introduce a novel, efficient strategy for selecting an initial set of labeled samples, requiring fewer labeling actions than existing methods.\n\nExperiments on the Contract-NLI (adapted for classification) and LEDGAR benchmarks demonstrate that our approach outperforms standard AL strategies and is more efficient.  Moreover, it achieves results comparable to a fully supervised approach, with a small performance gap and significantly reduced annotation costs.  The code and adapted data will be publicly available.\n"}, "2102.03115": {"original_text": "  Object detection in natural scenes can be a challenging task. In many\nreal-life situations, the visible spectrum is not suitable for traditional\ncomputer vision tasks. Moving outside the visible spectrum range, such as the\nthermal spectrum or the near-infrared (NIR) images, is much more beneficial in\nlow visibility conditions, NIR images are very helpful for understanding the\nobject's material quality. In this work, we have taken images with both the\nThermal and NIR spectrum for the object detection task. As multi-spectral data\nwith both Thermal and NIR is not available for the detection task, we needed to\ncollect data ourselves. Data collection is a time-consuming process, and we\nfaced many obstacles that we had to overcome. We train the YOLO v3 network from\nscratch to detect an object from multi-spectral images. Also, to avoid\noverfitting, we have done data augmentation and tune hyperparameters.\n", "rewritten_text": "Object detection in natural scenes presents significant challenges.  Traditional computer vision techniques often struggle in real-world scenarios where the visible spectrum is inadequate, particularly in low-visibility conditions.  Utilizing alternative spectral ranges, such as thermal or near-infrared (NIR) imaging, offers substantial advantages.  NIR imaging, in particular, provides valuable insights into material properties.  This work employs both thermal and NIR imagery for object detection.  Due to the unavailability of existing multi-spectral datasets (combining thermal and NIR data) for this task, we undertook the time-consuming and challenging process of data collection.  We then trained the YOLOv3 network from scratch on this dataset to perform object detection.  Data augmentation and hyperparameter tuning were implemented to mitigate overfitting.\n"}, "2207.09157": {"original_text": "  Supervised deep learning-based approaches have been applied to task-oriented\ndialog and have proven to be effective for limited domain and language\napplications when a sufficient number of training examples are available. In\npractice, these approaches suffer from the drawbacks of domain-driven design\nand under-resourced languages. Domain and language models are supposed to grow\nand change as the problem space evolves. On one hand, research on transfer\nlearning has demonstrated the cross-lingual ability of multilingual\nTransformers-based models to learn semantically rich representations. On the\nother, in addition to the above approaches, meta-learning have enabled the\ndevelopment of task and language learning algorithms capable of far\ngeneralization. Through this context, this article proposes to investigate the\ncross-lingual transferability of using synergistically few-shot learning with\nprototypical neural networks and multilingual Transformers-based models.\nExperiments in natural language understanding tasks on MultiATIS++ corpus shows\nthat our approach substantially improves the observed transfer learning\nperformances between the low and the high resource languages. More generally\nour approach confirms that the meaningful latent space learned in a given\nlanguage can be can be generalized to unseen and under-resourced ones using\nmeta-learning.\n", "rewritten_text": "Supervised deep learning approaches have proven effective for task-oriented dialogue in limited domains and languages, provided sufficient training data.  However, these approaches suffer from domain-specific designs and limitations with under-resourced languages.  Furthermore, domain and language models must adapt as the problem space evolves.  While transfer learning, particularly with multilingual Transformer-based models, has demonstrated cross-lingual capabilities and semantically rich representation learning, meta-learning offers further generalization for task and language acquisition.\n\nThis article investigates the cross-lingual transferability of synergistically combining few-shot learning with prototypical neural networks and multilingual Transformer-based models.  Experiments on the MultiATIS++ corpus for natural language understanding tasks demonstrate that our approach significantly improves transfer learning performance between low- and high-resource languages.  These results confirm that meaningful latent spaces learned in high-resource languages can be generalized to unseen, under-resourced languages using meta-learning.\n"}, "2203.1521": {"original_text": "  To learn camera-view invariant features for person Re-IDentification (Re-ID),\nthe cross-camera image pairs of each person play an important role. However,\nsuch cross-view training samples could be unavailable under the ISolated Camera\nSupervised (ISCS) setting, e.g., a surveillance system deployed across distant\nscenes. To handle this challenging problem, a new pipeline is introduced by\nsynthesizing the cross-camera samples in the feature space for model training.\nSpecifically, the feature encoder and generator are end-to-end optimized under\na novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint\nlearning procedure raises concern on the stability of generative model\ntraining. Therefore, a new feature generator, $\\sigma$-Regularized Conditional\nVariational Autoencoder ($\\sigma$-Reg.~CVAE), is proposed with theoretical and\nexperimental analysis on its robustness. Extensive experiments on two ISCS\nperson Re-ID datasets demonstrate the superiority of our CCSFG to the\ncompetitors.\n", "rewritten_text": "Camera-view invariant features for person re-identification (Re-ID) critically depend on cross-camera image pairs of the same individual.  However, such pairs are often unavailable in Isolated Camera Supervised (ISCS) settings, such as geographically dispersed surveillance systems.  To address this challenge, we introduce a novel pipeline that synthesizes cross-camera samples in feature space for model training.  This pipeline jointly optimizes a feature encoder and generator using Camera-Conditioned Stable Feature Generation (CCSFG).  However, the joint learning process raises concerns about the stability of generative model training.  Therefore, we propose a new feature generator, the \u03c3-Regularized Conditional Variational Autoencoder (\u03c3-Reg. CVAE), and provide theoretical and empirical analyses of its robustness.  Extensive experiments on two ISCS person Re-ID datasets demonstrate the superiority of our CCSFG approach over competing methods.\n"}, "1812.0957": {"original_text": "  In recent years, we have seen the performance of video-based person\nRe-Identification (ReID) methods have improved considerably. However, most of\nthe work in this area has dealt with videos acquired by fixed cameras with\nwider field of view. Recently, widespread use of wearable cameras and recording\ndevices such as cellphones have opened the door to interesting research in\nfirst-person Point-of-view (POV) videos (egocentric videos). Nonetheless,\nanalysis of such videos is challenging due to factors such as poor video\nquality due to ego-motion, blurriness, severe changes in lighting conditions\nand perspective distortions. To facilitate the research towards conquering\nthese challenges, this paper contributes a new dataset called EgoReID. The\ndataset is captured using 3 mobile cellphones with non-overlapping\nfield-of-view. It contains 900 IDs and around 10,200 tracks with a total of\n176,000 detections. The dataset also contains 12-sensor meta data e.g. camera\norientation pitch and rotation for each video.\n  In addition, we propose a new framework which takes advantage of both visual\nand sensor meta data to successfully perform Person ReID. We extend image-based\nre-ID method employing human body parsing trained on ten datasets to\nvideo-based re-ID. In our method, first frame level local features are\nextracted for each semantic region, then 3D convolutions are applied to encode\nthe temporal information in each sequence of semantic regions. Additionally, we\nemploy sensor meta data to predict targets' next camera and their estimated\ntime of arrival, which considerably improves our ReID performance as it\nsignificantly reduces our search space.\n", "rewritten_text": "Recent years have witnessed significant improvements in video-based person re-identification (ReID) methods.  However, most research has focused on videos from fixed, wide-field-of-view cameras. The proliferation of wearable cameras and mobile recording devices has spurred interest in first-person (POV or egocentric) video ReID.  Analyzing such videos presents challenges, including poor video quality due to ego-motion, blur, significant lighting variations, and perspective distortions.  To address these challenges, this paper introduces EgoReID, a new dataset captured using three mobile phones with non-overlapping fields of view.  EgoReID contains 900 identities, approximately 10,200 tracks, and a total of 176,000 detections.  It also includes 12-sensor metadata, such as camera orientation (pitch and roll), for each video.\n\nFurthermore, we propose a novel framework that leverages both visual and sensor metadata for improved person ReID.  We extend an image-based ReID method, employing human body parsing trained on ten datasets, to video-based ReID.  Our method first extracts frame-level local features for each semantic region.  Then, 3D convolutions encode the temporal information within each sequence of semantic regions.  Finally, we utilize sensor metadata to predict target trajectories (next camera and estimated time of arrival), significantly reducing the search space and improving ReID performance.\n"}, "2112.07928": {"original_text": "  Real-world data often follows a long-tailed distribution, which makes the\nperformance of existing classification algorithms degrade heavily. A key issue\nis that samples in tail categories fail to depict their intra-class diversity.\nHumans can imagine a sample in new poses, scenes, and view angles with their\nprior knowledge even if it is the first time to see this category. Inspired by\nthis, we propose a novel reasoning-based implicit semantic data augmentation\nmethod to borrow transformation directions from other classes. Since the\ncovariance matrix of each category represents the feature transformation\ndirections, we can sample new directions from similar categories to generate\ndefinitely different instances. Specifically, the long-tailed distributed data\nis first adopted to train a backbone and a classifier. Then, a covariance\nmatrix for each category is estimated, and a knowledge graph is constructed to\nstore the relations of any two categories. Finally, tail samples are adaptively\nenhanced via propagating information from all the similar categories in the\nknowledge graph. Experimental results on CIFAR-100-LT, ImageNet-LT, and\niNaturalist 2018 have demonstrated the effectiveness of our proposed method\ncompared with the state-of-the-art methods.\n", "rewritten_text": "Real-world data frequently exhibits long-tailed distributions, significantly degrading the performance of existing classification algorithms.  A critical limitation is the insufficient representation of intra-class diversity in tail categories.  Unlike algorithms, humans leverage prior knowledge to mentally visualize samples in novel poses, scenes, and viewpoints, even for unseen instances.  Inspired by this human capability, we propose a novel reasoning-based implicit semantic data augmentation method that leverages transformation directions from related classes.  We represent these directions using the covariance matrix of each category, sampling new directions from similar categories to generate diverse synthetic instances.  Specifically, a backbone network and classifier are first trained on the long-tailed dataset.  Then, per-category covariance matrices are estimated, and a knowledge graph is constructed to encode relationships between categories.  Finally, tail samples are augmented by propagating information from similar categories within the knowledge graph.  Experiments on CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018 demonstrate the effectiveness of our method compared to state-of-the-art approaches.\n"}, "2208.07291": {"original_text": "  One of the possible dangers that older people face in their daily lives is\nfalling. Occlusion is one of the biggest challenges of vision-based fall\ndetection systems and degrades their detection performance considerably. To\ntackle this problem, we synthesize specifically-designed occluded videos for\ntraining fall detection systems using existing datasets. Then, by defining a\nnew cost function, we introduce a framework for weighted training of fall\ndetection models using occluded and un-occluded videos, which can be applied to\nany learnable fall detection system. Finally, we use both a non-deep and deep\nmodel to evaluate the effect of the proposed weighted training method.\nExperiments show that the proposed method can improve the classification\naccuracy by 36% for a non-deep model and 55% for a deep model in occlusion\nconditions. Moreover, it is shown that the proposed training framework can also\nsignificantly improve the detection performance of a deep network on normal\nun-occluded samples.\n", "rewritten_text": "Falls pose a significant risk to older adults.  Vision-based fall detection systems are often hampered by occlusion, which substantially degrades their performance. To address this, we synthesize occluded videos from existing datasets to augment training data for fall detection models.  We then introduce a novel weighted training framework, defined by a new cost function, that leverages both occluded and unoccluded videos. This framework is applicable to any trainable fall detection system.  We evaluate its effectiveness using both shallow and deep learning models.  Experiments demonstrate a 36% accuracy improvement for the shallow model and a 55% improvement for the deep model under occlusion.  Furthermore, the framework significantly enhances the deep network's performance on unoccluded samples.\n"}, "2101.0219": {"original_text": "  Object detection and semantic segmentation are two of the most widely adopted\ndeep learning algorithms in agricultural applications. One of the major sources\nof variability in image quality acquired in the outdoors for such tasks is\nchanging lighting condition that can alter the appearance of the objects or the\ncontents of the entire image. While transfer learning and data augmentation to\nsome extent reduce the need for large amount of data to train deep neural\nnetworks, the large variety of cultivars and the lack of shared datasets in\nagriculture makes wide-scale field deployments difficult. In this paper, we\npresent a high throughput robust active lighting-based camera system that\ngenerates consistent images in all lighting conditions. We detail experiments\nthat show the consistency in images quality leading to relatively fewer images\nto train deep neural networks for the task of object detection. We further\npresent results from field experiment under extreme lighting conditions where\nimages without active lighting significantly lack to provide consistent\nresults. The experimental results show that on average, deep nets for object\ndetection trained on consistent data required nearly four times less data to\nachieve similar level of accuracy. This proposed work could potentially provide\npragmatic solutions to computer vision needs in agriculture.\n", "rewritten_text": "Object detection and semantic segmentation are two widely adopted deep learning algorithms in agricultural applications.  However, outdoor image acquisition suffers from significant variability in lighting conditions, which alters object appearance and overall image quality. While transfer learning and data augmentation mitigate the need for massive datasets to train deep neural networks, the diversity of cultivars and scarcity of shared agricultural datasets hinder widespread field deployment.  This paper presents a high-throughput, robust, active lighting-based camera system that generates consistent images under varying lighting conditions.  Experiments demonstrate this consistency, resulting in a significant reduction in the number of images required to train deep neural networks for object detection.  Field experiments under extreme lighting conditions further reveal that images acquired without active lighting lack consistency.  Results show that deep networks trained on consistently lit data required approximately four times less training data to achieve comparable accuracy. This work offers a pragmatic solution to computer vision challenges in agriculture.\n"}, "1911.03828": {"original_text": "  Wasserstein autoencoders are effective for text generation. They do not\nhowever provide any control over the style and topic of the generated sentences\nif the dataset has multiple classes and includes different topics. In this\nwork, we present a semi-supervised approach for generating stylized sentences.\nOur model is trained on a multi-class dataset and learns the latent\nrepresentation of the sentences using a mixture of Gaussian prior without any\nadversarial losses. This allows us to generate sentences in the style of a\nspecified class or multiple classes by sampling from their corresponding prior\ndistributions. Moreover, we can train our model on relatively small datasets\nand learn the latent representation of a specified class by adding external\ndata with other styles/classes to our dataset. While a simple WAE or VAE cannot\ngenerate diverse sentences in this case, generated sentences with our approach\nare diverse, fluent, and preserve the style and the content of the desired\nclasses.\n", "rewritten_text": "Wasserstein autoencoders (WAEs) are effective for text generation, but they lack control over the style and topic of generated sentences when trained on datasets with multiple classes and diverse topics.  This work presents a semi-supervised approach for generating stylized sentences.  Our model, trained on a multi-class dataset, learns a latent sentence representation using a Gaussian mixture prior, eschewing adversarial losses. This enables generation of sentences in the style of a specified class (or multiple classes) by sampling from their corresponding prior distributions.  Furthermore, our model can be trained on relatively small datasets, leveraging the latent representation of a specific class by augmenting the dataset with externally sourced data representing other styles or classes.  Unlike standard WAEs or variational autoencoders (VAEs), which struggle to generate diverse sentences under these conditions, our approach yields diverse, fluent sentences that faithfully preserve the style and content of the desired classes.\n"}, "2106.01656": {"original_text": "  Many variants of unsupervised domain adaptation (UDA) problems have been\nproposed and solved individually. Its side effect is that a method that works\nfor one variant is often ineffective for or not even applicable to another,\nwhich has prevented practical applications. In this paper, we give a general\nrepresentation of UDA problems, named Generalized Domain Adaptation (GDA). GDA\ncovers the major variants as special cases, which allows us to organize them in\na comprehensive framework. Moreover, this generalization leads to a new\nchallenging setting where existing methods fail, such as when domain labels are\nunknown, and class labels are only partially given to each domain. We propose a\nnovel approach to the new setting. The key to our approach is self-supervised\nclass-destructive learning, which enables the learning of class-invariant\nrepresentations and domain-adversarial classifiers without using any domain\nlabels. Extensive experiments using three benchmark datasets demonstrate that\nour method outperforms the state-of-the-art UDA methods in the new setting and\nthat it is competitive in existing UDA variations as well.\n", "rewritten_text": "Numerous unsupervised domain adaptation (UDA) variants have been proposed and addressed individually.  This has resulted in methods effective for one variant often proving ineffective or inapplicable to others, hindering practical deployment.  This paper introduces a generalized representation of UDA problems, termed Generalized Domain Adaptation (GDA). GDA subsumes major UDA variants as special cases, providing a unifying framework.  This generalization reveals a challenging new setting where existing methods fail, specifically when domain labels are unknown and class labels are only partially available per domain. We propose a novel approach for this setting, leveraging self-supervised class-destructive learning. This technique enables the learning of class-invariant representations and domain-adversarial classifiers without relying on domain labels.  Extensive experiments on three benchmark datasets demonstrate that our method surpasses state-of-the-art UDA methods in this novel setting and achieves competitive performance on existing UDA variants.\n"}, "2305.07152": {"original_text": "  The ability to automatically detect and track surgical instruments in\nendoscopic videos can enable transformational interventions. Assessing surgical\nperformance and efficiency, identifying skilled tool use and choreography, and\nplanning operational and logistical aspects of OR resources are just a few of\nthe applications that could benefit. Unfortunately, obtaining the annotations\nneeded to train machine learning models to identify and localize surgical tools\nis a difficult task. Annotating bounding boxes frame-by-frame is tedious and\ntime-consuming, yet large amounts of data with a wide variety of surgical tools\nand surgeries must be captured for robust training. Moreover, ongoing annotator\ntraining is needed to stay up to date with surgical instrument innovation. In\nrobotic-assisted surgery, however, potentially informative data like timestamps\nof instrument installation and removal can be programmatically harvested. The\nability to rely on tool installation data alone would significantly reduce the\nworkload to train robust tool-tracking models. With this motivation in mind we\ninvited the surgical data science community to participate in the challenge,\nSurgToolLoc 2022. The goal was to leverage tool presence data as weak labels\nfor machine learning models trained to detect tools and localize them in video\nframes with bounding boxes. We present the results of this challenge along with\nmany of the team's efforts. We conclude by discussing these results in the\nbroader context of machine learning and surgical data science. The training\ndata used for this challenge consisting of 24,695 video clips with tool\npresence labels is also being released publicly and can be accessed at\nhttps://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022.\n", "rewritten_text": "Automatic detection and tracking of surgical instruments in endoscopic videos offers transformative potential in surgical performance assessment, efficiency analysis, skilled tool-use identification, and optimized operating room resource planning.  However, training machine learning models for this task is hampered by the laborious and time-consuming process of manually annotating bounding boxes frame-by-frame across large datasets encompassing diverse surgical tools and procedures.  Furthermore, annotator training must continuously adapt to evolving surgical instrument technology.\n\nRobotic-assisted surgery offers a potential solution by providing programmatically accessible timestamps indicating instrument installation and removal.  Leveraging this data as weak labels could significantly reduce the annotation burden for robust tool-tracking model training.  To address this challenge, we organized SurgToolLoc 2022, inviting the surgical data science community to develop models that utilize tool presence data to detect and localize instruments within video frames using bounding boxes.  This paper presents the challenge results and participant efforts, concluding with a discussion of their implications for machine learning and surgical data science.\n\nThe training data used in SurgToolLoc 2022, comprising 24,695 video clips with tool presence labels, is publicly available at [https://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022](https://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022).\n"}, "2403.01858": {"original_text": "  We present TMMLU+, a new benchmark designed for Traditional Chinese language\nunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66\nsubjects from elementary to professional level. It is six times larger and\nboasts a more balanced subject distribution than its predecessor, Taiwan\nMassive Multitask Language Understanding (TMMLU). We also benchmark\nclosed-source models and 26 open-weight Chinese large language models (LLMs) of\nparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings reveal\nthat (1.) Traditional Chinese models still trail behind their Simplified\nChinese counterparts, highlighting a need for more focused advancements in LLMs\ncatering to Traditional Chinese. (2.) Current LLMs still fall short of human\nperformance in average scores, indicating a potential need for future research\nto delve deeper into social science and humanities subjects. (3.) Among all the\ntokenization compression metrics examined, we identify that only the fertility\nscore uniquely demonstrates strong correlations with our benchmark results. We\nforesee that TMMLU+ will pinpoint areas for future model improvement, thereby\nnarrowing the gap between machine and human linguistic capabilities and\nsupporting researchers in developing Traditional Chinese LLMs. Our dataset,\nalong with the benchmark source code, is accessible at\nhuggingface.co/datasets/ikala/tmmluplus.\n", "rewritten_text": "We introduce TMMLU+, a new benchmark dataset for Traditional Chinese language understanding.  TMMLU+ is a multiple-choice question-answering dataset encompassing 66 subjects, ranging from elementary to professional levels.  Six times larger and featuring a more balanced subject distribution than its predecessor, the Taiwan Massive Multitask Language Understanding (TMMLU) benchmark, TMMLU+ allows for a more comprehensive evaluation. We benchmark 26 open-source Chinese large language models (LLMs), with parameter counts ranging from 1.8B to 72B, as well as several closed-source models, on TMMLU+.  Our findings indicate that: (1) Traditional Chinese LLMs still lag behind their Simplified Chinese counterparts, underscoring the need for further research and development specifically targeting Traditional Chinese; (2) current LLMs underperform humans on average, suggesting a need for further investigation into social science and humanities domains; and (3) among various tokenization compression metrics, only the fertility score exhibits a strong correlation with our benchmark results.  We anticipate that TMMLU+ will facilitate the identification of key areas for model improvement, ultimately bridging the gap between machine and human linguistic capabilities in Traditional Chinese.  The dataset and benchmark source code are publicly available at huggingface.co/datasets/ikala/tmmluplus.\n"}, "2005.04621": {"original_text": "  Deep convolutional neural networks generally perform well in underwater\nobject recognition tasks on both optical and sonar images. Many such methods\nrequire hundreds, if not thousands, of images per class to generalize well to\nunseen examples. However, obtaining and labeling sufficiently large volumes of\ndata can be relatively costly and time-consuming, especially when observing\nrare objects or performing real-time operations. Few-Shot Learning (FSL)\nefforts have produced many promising methods to deal with low data\navailability. However, little attention has been given in the underwater\ndomain, where the style of images poses additional challenges for object\nrecognition algorithms. To the best of our knowledge, this is the first paper\nto evaluate and compare several supervised and semi-supervised Few-Shot\nLearning (FSL) methods using underwater optical and side-scan sonar imagery.\nOur results show that FSL methods offer a significant advantage over the\ntraditional transfer learning methods that fine-tune pre-trained models. We\nhope that our work will help apply FSL to autonomous underwater systems and\nexpand their learning capabilities.\n", "rewritten_text": "Deep convolutional neural networks (DCNNs) generally achieve high performance in underwater object recognition using both optical and sonar imagery.  However, many DCNN-based methods require hundreds or thousands of images per class for effective generalization to unseen examples.  Acquiring and labeling such large datasets is expensive and time-consuming, particularly for rare objects or real-time applications.  While few-shot learning (FSL) offers promising solutions for data scarcity, its application in the challenging underwater domain, with its unique image characteristics, remains largely unexplored.  To our knowledge, this paper presents the first comprehensive evaluation and comparison of several supervised and semi-supervised FSL methods applied to underwater optical and side-scan sonar imagery. Our results demonstrate that FSL methods significantly outperform traditional transfer learning approaches that fine-tune pre-trained models.  This work contributes to the advancement of FSL in autonomous underwater systems, expanding their learning capabilities.\n"}, "1712.01821": {"original_text": "  Factored neural machine translation (FNMT) is founded on the idea of using\nthe morphological and grammatical decomposition of the words (factors) at the\noutput side of the neural network. This architecture addresses two well-known\nproblems occurring in MT, namely the size of target language vocabulary and the\nnumber of unknown tokens produced in the translation. FNMT system is designed\nto manage larger vocabulary and reduce the training time (for systems with\nequivalent target language vocabulary size). Moreover, we can produce\ngrammatically correct words that are not part of the vocabulary. FNMT model is\nevaluated on IWSLT'15 English to French task and compared to the baseline\nword-based and BPE-based NMT systems. Promising qualitative and quantitative\nresults (in terms of BLEU and METEOR) are reported.\n", "rewritten_text": "Factored neural machine translation (FNMT) leverages morphological and grammatical decomposition of output words (factors) to address two common machine translation (MT) challenges: large target-language vocabularies and the generation of unknown tokens.  This architecture enables the handling of larger vocabularies and reduces training time compared to systems with equivalent vocabulary sizes.  Furthermore, FNMT can generate grammatically correct words not present in its vocabulary.  We evaluated the FNMT model on the IWSLT'15 English-to-French task, comparing it to word-based and byte-pair encoding (BPE)-based NMT baselines.  The results demonstrate promising qualitative and quantitative improvements, as measured by BLEU and METEOR scores.\n"}, "2206.01988": {"original_text": "  Automatic generation of ophthalmic reports using data-driven neural networks\nhas great potential in clinical practice. When writing a report,\nophthalmologists make inferences with prior clinical knowledge. This knowledge\nhas been neglected in prior medical report generation methods. To endow models\nwith the capability of incorporating expert knowledge, we propose a Cross-modal\nclinical Graph Transformer (CGT) for ophthalmic report generation (ORG), in\nwhich clinical relation triples are injected into the visual features as prior\nknowledge to drive the decoding procedure. However, two major common Knowledge\nNoise (KN) issues may affect models' effectiveness. 1) Existing general\nbiomedical knowledge bases such as the UMLS may not align meaningfully to the\nspecific context and language of the report, limiting their utility for\nknowledge injection. 2) Incorporating too much knowledge may divert the visual\nfeatures from their correct meaning. To overcome these limitations, we design\nan automatic information extraction scheme based on natural language processing\nto obtain clinical entities and relations directly from in-domain training\nreports. Given a set of ophthalmic images, our CGT first restores a sub-graph\nfrom the clinical graph and injects the restored triples into visual features.\nThen visible matrix is employed during the encoding procedure to limit the\nimpact of knowledge. Finally, reports are predicted by the encoded cross-modal\nfeatures via a Transformer decoder. Extensive experiments on the large-scale\nFFA-IR benchmark demonstrate that the proposed CGT is able to outperform\nprevious benchmark methods and achieve state-of-the-art performances.\n", "rewritten_text": "Automatic ophthalmic report generation using data-driven neural networks holds significant potential for clinical practice.  Existing methods neglect the crucial role of ophthalmologists' clinical expertise in report writing, which involves inferential reasoning based on prior knowledge.  To address this, we propose a Cross-modal Clinical Graph Transformer (CGT) for ophthalmic report generation (ORG).  CGT incorporates expert knowledge by injecting clinical relation triples\u2014extracted as prior knowledge\u2014into visual features, guiding the decoding process.\n\nHowever, two key knowledge noise (KN) challenges arise: 1) General biomedical knowledge bases (e.g., UMLS) may lack the specific context and language needed for effective knowledge injection into ophthalmic reports; and 2) excessive knowledge injection can distort the meaning of visual features.\n\nTo mitigate these issues, we developed an automatic information extraction scheme using natural language processing (NLP) to derive clinical entities and relations directly from in-domain training reports.  Given ophthalmic images, CGT first constructs a relevant subgraph from the clinical knowledge graph and injects the extracted triples into the visual features.  A visibility matrix is then employed during encoding to control the influence of this injected knowledge. Finally, a Transformer decoder predicts the report using the encoded cross-modal features.\n\nExtensive experiments on the large-scale FFA-IR benchmark demonstrate that CGT outperforms existing state-of-the-art methods.\n"}, "1602.01125": {"original_text": "  We propose a fully automatic method for fitting a 3D morphable model to\nsingle face images in arbitrary pose and lighting. Our approach relies on\ngeometric features (edges and landmarks) and, inspired by the iterated closest\npoint algorithm, is based on computing hard correspondences between model\nvertices and edge pixels. We demonstrate that this is superior to previous work\nthat uses soft correspondences to form an edge-derived cost surface that is\nminimised by nonlinear optimisation.\n", "rewritten_text": "We propose a fully automatic method for fitting a 3D morphable model to single face images, accommodating arbitrary pose and lighting conditions.  Our approach leverages geometric features (edges and landmarks) and, inspired by the iterative closest point algorithm, computes hard correspondences between model vertices and edge pixels.  This method outperforms previous approaches that utilize soft correspondences to create an edge-derived cost surface minimized through nonlinear optimization.\n"}, "2405.11862": {"original_text": "  Table structure recognition (TSR) aims to parse the inherent structure of a\ntable from its input image. The `\"split-and-merge\" paradigm is a pivotal\napproach to parse table structure, where the table separation line detection is\ncrucial. However, challenges such as wireless and deformed tables make it\ndemanding. In this paper, we adhere to the \"split-and-merge\" paradigm and\npropose SEMv3 (SEM: Split, Embed and Merge), a method that is both fast and\nrobust for detecting table separation lines. During the split stage, we\nintroduce a Keypoint Offset Regression (KOR) module, which effectively detects\ntable separation lines by directly regressing the offset of each line relative\nto its keypoint proposals. Moreover, in the merge stage, we define a series of\nmerge actions to efficiently describe the table structure based on table grids.\nExtensive ablation studies demonstrate that our proposed KOR module can detect\ntable separation lines quickly and accurately. Furthermore, on public datasets\n(e.g. WTW, ICDAR-2019 cTDaR Historical and iFLYTAB), SEMv3 achieves\nstate-of-the-art (SOTA) performance. The code is available at\nhttps://github.com/Chunchunwumu/SEMv3.\n", "rewritten_text": "Table structure recognition (TSR) aims to parse the inherent structure of a table from its input image.  The split-and-merge paradigm is a pivotal approach for this task, where accurate table separation line detection is crucial. However, challenges such as skewed and deformed tables present significant difficulties.  This paper proposes SEMv3 (SEM: Split, Embed, and Merge), a fast and robust method for detecting table separation lines within the split-and-merge paradigm.  The split stage introduces a Keypoint Offset Regression (KOR) module that efficiently detects these lines by directly regressing the offset of each line relative to its keypoint proposals.  The merge stage employs a series of merge actions to efficiently represent the table structure based on its grid.  Extensive ablation studies demonstrate the speed and accuracy of our proposed KOR module in detecting table separation lines.  Furthermore, SEMv3 achieves state-of-the-art (SOTA) performance on public datasets, including WTW, ICDAR-2019 cTDaR Historical, and iFLYTAB. The code is available at https://github.com/Chunchunwumu/SEMv3.\n"}, "1505.04424": {"original_text": "  In this work, we propose a novel microaneurysm (MA) detection for early\ndiabetic retinopathy screening using color fundus images. Since MA usually the\nfirst lesions to appear as an indicator of diabetic retinopathy, accurate\ndetection of MA is necessary for treatment. Each pixel of the image is\nclassified as either MA or non-MA using a deep neural network with dropout\ntraining procedure using maxout activation function. No preprocessing step or\nmanual feature extraction is required. Substantial improvements over standard\nMA detection method based on the pipeline of preprocessing, feature extraction,\nclassification followed by post processing is achieved. The presented method is\nevaluated in publicly available Retinopathy Online Challenge (ROC) and\nDiaretdb1v2 database and achieved state-of-the-art accuracy.\n", "rewritten_text": "This work proposes a novel method for microaneurysm (MA) detection in color fundus images for early diabetic retinopathy screening.  Because MAs are typically the first lesions to appear, their accurate detection is crucial for timely treatment.  A deep neural network, employing dropout training and a maxout activation function, classifies each pixel as either MA or non-MA.  No preprocessing or manual feature extraction is required.  This method significantly outperforms standard MA detection methods, which rely on pipelines involving preprocessing, feature extraction, classification, and post-processing.  Evaluated on the publicly available Retinopathy Online Challenge (ROC) and Diaretdb1v2 databases, our method achieves state-of-the-art accuracy.\n"}, "2206.13156": {"original_text": "  Transformer has been widely used in histopathology whole slide image (WSI)\nclassification for the purpose of tumor grading, prognosis analysis, etc.\nHowever, the design of token-wise self-attention and positional embedding\nstrategy in the common Transformer limits the effectiveness and efficiency in\nthe application to gigapixel histopathology images. In this paper, we propose a\nkernel attention Transformer (KAT) for histopathology WSI classification. The\ninformation transmission of the tokens is achieved by cross-attention between\nthe tokens and a set of kernels related to a set of positional anchors on the\nWSI. Compared to the common Transformer structure, the proposed KAT can better\ndescribe the hierarchical context information of the local regions of the WSI\nand meanwhile maintains a lower computational complexity. The proposed method\nwas evaluated on a gastric dataset with 2040 WSIs and an endometrial dataset\nwith 2560 WSIs, and was compared with 6 state-of-the-art methods. The\nexperimental results have demonstrated the proposed KAT is effective and\nefficient in the task of histopathology WSI classification and is superior to\nthe state-of-the-art methods. The code is available at\nhttps://github.com/zhengyushan/kat.\n", "rewritten_text": "Transformers have been widely used for histopathology whole slide image (WSI) classification tasks such as tumor grading and prognosis analysis.  However, the token-wise self-attention and positional embedding mechanisms in standard Transformer architectures limit their effectiveness and efficiency when applied to gigapixel histopathology images.  This paper introduces Kernel Attention Transformer (KAT), a novel architecture for histopathology WSI classification.  KAT achieves information transmission between tokens via cross-attention with a set of kernels associated with positional anchors on the WSI.  Compared to standard Transformers, KAT better captures the hierarchical contextual information of local WSI regions while maintaining lower computational complexity.  We evaluated KAT on two datasets: a gastric dataset (2040 WSIs) and an endometrial dataset (2560 WSIs), comparing its performance against six state-of-the-art methods.  Experimental results demonstrate that KAT is both effective and efficient for histopathology WSI classification, outperforming existing state-of-the-art approaches.  The code is available at https://github.com/zhengyushan/kat.\n"}, "1610.00291": {"original_text": "  We present a novel method for constructing Variational Autoencoder (VAE).\nInstead of using pixel-by-pixel loss, we enforce deep feature consistency\nbetween the input and the output of a VAE, which ensures the VAE's output to\npreserve the spatial correlation characteristics of the input, thus leading the\noutput to have a more natural visual appearance and better perceptual quality.\nBased on recent deep learning works such as style transfer, we employ a\npre-trained deep convolutional neural network (CNN) and use its hidden features\nto define a feature perceptual loss for VAE training. Evaluated on the CelebA\nface dataset, we show that our model produces better results than other methods\nin the literature. We also show that our method can produce latent vectors that\ncan capture the semantic information of face expressions and can be used to\nachieve state-of-the-art performance in facial attribute prediction.\n", "rewritten_text": "We propose a novel variational autoencoder (VAE) architecture that leverages deep feature consistency instead of pixel-by-pixel loss.  This approach ensures the VAE output preserves the spatial correlations of the input, resulting in more natural and perceptually appealing images.  Inspired by recent advances in style transfer, we utilize a pre-trained deep convolutional neural network (CNN) to define a feature perceptual loss function for training.  Evaluated on the CelebA face dataset, our model outperforms existing methods.  Furthermore, our method generates latent vectors that capture semantic information about facial expressions, enabling state-of-the-art performance in facial attribute prediction.\n"}, "2206.00227": {"original_text": "  A data augmentation module is utilized in contrastive learning to transform\nthe given data example into two views, which is considered essential and\nirreplaceable. However, the predetermined composition of multiple data\naugmentations brings two drawbacks. First, the artificial choice of\naugmentation types brings specific representational invariances to the model,\nwhich have different degrees of positive and negative effects on different\ndownstream tasks. Treating each type of augmentation equally during training\nmakes the model learn non-optimal representations for various downstream tasks\nand limits the flexibility to choose augmentation types beforehand. Second, the\nstrong data augmentations used in classic contrastive learning methods may\nbring too much invariance in some cases, and fine-grained information that is\nessential to some downstream tasks may be lost. This paper proposes a general\nmethod to alleviate these two problems by considering where and what to\ncontrast in a general contrastive learning framework. We first propose to learn\ndifferent augmentation invariances at different depths of the model according\nto the importance of each data augmentation instead of learning\nrepresentational invariances evenly in the backbone. We then propose to expand\nthe contrast content with augmentation embeddings to reduce the misleading\neffects of strong data augmentations. Experiments based on several baseline\nmethods demonstrate that we learn better representations for various benchmarks\non classification, detection, and segmentation downstream tasks.\n", "rewritten_text": "Contrastive learning relies on a data augmentation module to generate two views of each input example\u2014a crucial and irreplaceable step.  However, the fixed composition of augmentations presents two key limitations. First, the arbitrary selection of augmentation types biases the learned representations toward specific invariances, impacting downstream tasks differently.  Uniformly weighting all augmentations during training leads to suboptimal representations and restricts the flexibility of augmentation selection. Second, the strong augmentations commonly used can introduce excessive invariance, potentially losing fine-grained information vital for certain downstream tasks.\n\nThis paper addresses these limitations by proposing a novel method that dynamically determines *where* and *what* to contrast within a general contrastive learning framework.  We introduce a mechanism to learn different augmentation invariances at varying depths within the model, weighting them according to their importance, rather than imposing uniform invariance learning across the entire backbone.  Furthermore, we enrich the contrast content by incorporating augmentation embeddings, mitigating the negative effects of overly strong augmentations.  Experiments on several benchmark datasets demonstrate the effectiveness of our approach, yielding improved representations for diverse downstream tasks including classification, object detection, and semantic segmentation.\n"}, "1711.0719": {"original_text": "  We present a stochastic first-order optimization algorithm, named BCSC, that\nadds a cyclic constraint to stochastic block-coordinate descent. It uses\ndifferent subsets of the data to update different subsets of the parameters,\nthus limiting the detrimental effect of outliers in the training set. Empirical\ntests in benchmark datasets show that our algorithm outperforms\nstate-of-the-art optimization methods in both accuracy as well as convergence\nspeed. The improvements are consistent across different architectures, and can\nbe combined with other training techniques and regularization methods.\n", "rewritten_text": "We introduce BCSC, a novel stochastic first-order optimization algorithm that incorporates a cyclic constraint into stochastic block-coordinate descent.  By updating disjoint subsets of parameters using different data subsets, BCSC mitigates the adverse effects of outliers in the training data.  Benchmark datasets demonstrate that BCSC surpasses state-of-the-art optimization methods in both accuracy and convergence speed.  These improvements are consistent across various architectures and readily combine with other training techniques and regularization methods.\n"}, "2008.11872": {"original_text": "  In Viticulture, visual inspection of the plant is a necessary task for\nmeasuring relevant variables. In many cases, these visual inspections are\nsusceptible to automation through computer vision methods. Bud detection is one\nsuch visual task, central for the measurement of important variables such as:\nmeasurement of bud sunlight exposure, autonomous pruning, bud counting,\ntype-of-bud classification, bud geometric characterization, internode length,\nbud area, and bud development stage, among others. This paper presents a\ncomputer method for grapevine bud detection based on a Fully Convolutional\nNetworks MobileNet architecture (FCN-MN). To validate its performance, this\narchitecture was compared in the detection task with a strong method for bud\ndetection, Scanning Windows (SW) based on a patch classifier, showing\nimprovements over three aspects of detection: segmentation, correspondence\nidentification and localization. The best version of FCN-MN showed a detection\nF1-measure of $88.6\\%$ (for true positives defined as detected components whose\nintersection-over-union with the true bud is above $0.5$), and false positives\nthat are small and near the true bud. Splits -- false positives overlapping the\ntrue bud -- showed a mean segmentation precision of $89.3\\% (21.7)$, while\nfalse alarms -- false positives not overlapping the true bud -- showed a mean\npixel area of only $8\\%$ the area of a true bud, and a distance (between mass\ncenters) of $1.1$ true bud diameters. The paper concludes by discussing how\nthese results for FCN-MN would produce sufficiently accurate measurements of\nbud variables such as bud number, bud area, and internode length, suggesting a\ngood performance in a practical setup.\n", "rewritten_text": "Visual inspection is crucial for measuring key variables in viticulture.  Computer vision offers a promising means of automating these inspections, particularly bud detection, which is essential for assessing bud sunlight exposure, enabling autonomous pruning, and quantifying bud count, type, geometry, internode length, area, and developmental stage. This paper presents a grapevine bud detection method using a Fully Convolutional Network based on MobileNet architecture (FCN-MN).  We evaluated its performance against a robust Scanning Windows (SW) method, a patch-based classifier, demonstrating improvements in segmentation, correspondence identification, and localization.  The optimal FCN-MN achieved an F1-score of 88.6% (considering true positives as detected components with an intersection-over-union (IoU) > 0.5 with the ground truth bud). False positives were small and proximate to true buds.  Specifically, false positives overlapping true buds (splits) exhibited a mean segmentation precision of 89.3% (with a standard deviation of 21.7), while those not overlapping (false alarms) had a mean pixel area of only 8% that of a true bud and were located at a mean distance (between centroids) of 1.1 true bud diameters.  These results suggest that FCN-MN provides sufficiently accurate bud measurements (e.g., number, area, and internode length) for practical viticultural applications.\n"}, "1703.10798": {"original_text": "  We present a system for converting a fully panoramic ($360^\\circ$) video into\na normal field-of-view (NFOV) hyperlapse for an optimal viewing experience. Our\nsystem exploits visual saliency and semantics to non-uniformly sample in space\nand time for generating hyperlapses. In addition, users can optionally choose\nobjects of interest for customizing the hyperlapses. We first stabilize an\ninput $360^\\circ$ video by smoothing the rotation between adjacent frames and\nthen compute regions of interest and saliency scores. An initial hyperlapse is\ngenerated by optimizing the saliency and motion smoothness followed by the\nsaliency-aware frame selection. We further smooth the result using an efficient\n2D video stabilization approach that adaptively selects the motion model to\ngenerate the final hyperlapse. We validate the design of our system by showing\nresults for a variety of scenes and comparing against the state-of-the-art\nmethod through a user study.\n", "rewritten_text": "This paper presents a system for converting 360\u00b0 panoramic video into normal field-of-view (NFOV) hyperlapses, optimizing the viewing experience.  Our system leverages visual saliency and semantic information to perform spatially and temporally non-uniform sampling during hyperlapse generation.  Users can optionally specify objects of interest for customized hyperlapses.  The process begins with stabilizing the input 360\u00b0 video by smoothing rotational discrepancies between adjacent frames, followed by identifying regions of interest and calculating saliency scores. An initial hyperlapse is generated by optimizing saliency and motion smoothness, incorporating saliency-aware frame selection.  Finally, an efficient 2D video stabilization technique, adaptively selecting the motion model, refines the hyperlapse.  We validate our system's design through a user study comparing results across diverse scenes with a state-of-the-art method.\n"}, "2001.03182": {"original_text": "  Unsupervised domain adaptation algorithms aim to transfer the knowledge\nlearned from one domain to another (e.g., synthetic to real images). The\nadapted representations often do not capture pixel-level domain shifts that are\ncrucial for dense prediction tasks (e.g., semantic segmentation). In this\npaper, we present a novel pixel-wise adversarial domain adaptation algorithm.\nBy leveraging image-to-image translation methods for data augmentation, our key\ninsight is that while the translated images between domains may differ in\nstyles, their predictions for the task should be consistent. We exploit this\nproperty and introduce a cross-domain consistency loss that enforces our\nadapted model to produce consistent predictions. Through extensive experimental\nresults, we show that our method compares favorably against the\nstate-of-the-art on a wide variety of unsupervised domain adaptation tasks.\n", "rewritten_text": "Unsupervised domain adaptation algorithms aim to transfer knowledge learned from a source domain to a target domain (e.g., from synthetic to real images).  However, adapted representations often fail to capture crucial pixel-level domain shifts necessary for dense prediction tasks such as semantic segmentation.  This paper presents a novel pixel-wise adversarial domain adaptation algorithm.  Leveraging image-to-image translation for data augmentation, our approach exploits the insight that while translated images may exhibit stylistic differences across domains, their task predictions should remain consistent.  We introduce a cross-domain consistency loss to enforce this consistency in our adapted model.  Extensive experimental results demonstrate that our method achieves state-of-the-art performance across a wide range of unsupervised domain adaptation tasks.\n"}, "1802.08636": {"original_text": "  Single document summarization is the task of producing a shorter version of a\ndocument while preserving its principal information content. In this paper we\nconceptualize extractive summarization as a sentence ranking task and propose a\nnovel training algorithm which globally optimizes the ROUGE evaluation metric\nthrough a reinforcement learning objective. We use our algorithm to train a\nneural summarization model on the CNN and DailyMail datasets and demonstrate\nexperimentally that it outperforms state-of-the-art extractive and abstractive\nsystems when evaluated automatically and by humans.\n", "rewritten_text": "Single-document summarization aims to create a concise version of a document while retaining its essential information.  This paper frames extractive summarization as a sentence ranking problem and introduces a novel training algorithm that globally optimizes the ROUGE evaluation metric using a reinforcement learning objective.  We applied this algorithm to train a neural summarization model on the CNN/Daily Mail datasets, demonstrating superior performance to state-of-the-art extractive and abstractive systems through both automated and human evaluation.\n"}, "2406.11192": {"original_text": "  Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets faces issues due to inconsistent entity definitions and\nredundant data, limiting LLMs to dataset-specific learning and hindering\nout-of-domain generalization. To address this, we present B2NERD, a cohesive\nand efficient dataset for Open NER, normalized from 54 existing English or\nChinese datasets using a two-step approach. First, we detect inconsistent\nentity definitions across datasets and clarify them by distinguishable label\nnames to construct a universal taxonomy of 400+ entity types. Second, we\naddress redundancy using a data pruning strategy that selects fewer samples\nwith greater category and semantic diversity. Comprehensive evaluation shows\nthat B2NERD significantly improves LLMs' generalization on Open NER. Our B2NER\nmodels, trained on B2NERD, outperform GPT-4 by 6.8-12.0 F1 points and surpass\nprevious methods in 3 out-of-domain benchmarks across 15 datasets and 6\nlanguages.\n", "rewritten_text": "Open Named Entity Recognition (NER), encompassing the identification of arbitrary entities from diverse domains, poses a significant challenge for Large Language Models (LLMs). While fine-tuning LLMs on extensive NER data has shown promise, existing datasets suffer from inconsistent entity definitions and redundancy, limiting model generalization beyond the training data.  To address this, we introduce B2NERD, a comprehensive and efficient dataset for open NER, constructed by normalizing 54 English and Chinese datasets using a two-step process.  First, we harmonize inconsistent entity definitions across datasets, creating a unified taxonomy of over 400 entity types using distinct label names. Second, a data pruning strategy selects a smaller, more diverse subset of samples, maximizing category and semantic representation.  Extensive evaluation demonstrates that B2NERD substantially improves LLM generalization in open NER.  Our B2NER models, trained on B2NERD, achieve a 6.8\u201312.0 F1-score improvement over GPT-4 and surpass previous state-of-the-art methods on three out-of-domain benchmarks encompassing 15 datasets and 6 languages.\n"}, "2411.01465": {"original_text": "  Despite the outstanding performance in many individual tasks, deep neural\nnetworks suffer from catastrophic forgetting when learning from continuous data\nstreams in real-world scenarios. Current Non-Exemplar Class-Incremental\nLearning (NECIL) methods mitigate forgetting by storing a single prototype per\nclass, which serves to inject previous information when sequentially learning\nnew classes. However, these stored prototypes or their augmented variants often\nfail to simultaneously capture spatial distribution diversity and precision\nneeded for representing old classes. Moreover, as the model acquires new\nknowledge, these prototypes gradually become outdated, making them less\neffective. To overcome these limitations, we propose a more efficient NECIL\nmethod that replaces prototypes with synthesized retrospective features for old\nclasses. Specifically, we model each old class's feature space using a\nmultivariate Gaussian distribution and generate deep representations by\nsampling from high-likelihood regions. Additionally, we introduce a\nsimilarity-based feature compensation mechanism that integrates generated old\nclass features with similar new class features to synthesize robust\nretrospective representations. These retrospective features are then\nincorporated into our incremental learning framework to preserve the decision\nboundaries of previous classes while learning new ones. Extensive experiments\non CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate that our method\nsignificantly improves the efficiency of non-exemplar class-incremental\nlearning and achieves state-of-the-art performance.\n", "rewritten_text": "Deep neural networks, while exhibiting strong performance on individual tasks, suffer from catastrophic forgetting when trained on continuous real-world data streams.  Existing Non-Exemplar Class-Incremental Learning (NECIL) methods address this by storing a single prototype per class, leveraging these to recall prior knowledge during sequential learning. However, these prototypes, even augmented versions, often fail to adequately capture both the spatial diversity and precision necessary for accurate representation of previously learned classes.  Furthermore, their effectiveness diminishes as the model learns new information, leading to outdated representations.\n\nTo mitigate these limitations, we propose a novel NECIL method that replaces prototypes with synthesized retrospective features.  Specifically, we model each old class's feature space using a multivariate Gaussian distribution, generating deep representations by sampling from high-probability regions.  A similarity-based feature compensation mechanism further enhances robustness by integrating generated old-class features with similar new-class features.  These synthesized retrospective features are then integrated into our incremental learning framework, preserving the decision boundaries of previously learned classes while accommodating new ones.\n\nExtensive experiments on CIFAR-100, TinyImageNet, and an ImageNet subset demonstrate that our method significantly improves NECIL efficiency and achieves state-of-the-art performance.\n"}, "2404.03654": {"original_text": "  NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel\nview synthesis and 3D reconstruction, but its performance is sensitive to input\nimage quality, which struggles to achieve high-fidelity rendering when provided\nwith low-quality sparse input viewpoints. Previous methods for NeRF restoration\nare tailored for specific degradation type, ignoring the generality of\nrestoration. To overcome this limitation, we propose a generic radiance fields\nrestoration pipeline, named RaFE, which applies to various types of\ndegradations, such as low resolution, blurriness, noise, compression artifacts,\nor their combinations. Our approach leverages the success of off-the-shelf 2D\nrestoration methods to recover the multi-view images individually. Instead of\nreconstructing a blurred NeRF by averaging inconsistencies, we introduce a\nnovel approach using Generative Adversarial Networks (GANs) for NeRF generation\nto better accommodate the geometric and appearance inconsistencies present in\nthe multi-view images. Specifically, we adopt a two-level tri-plane\narchitecture, where the coarse level remains fixed to represent the low-quality\nNeRF, and a fine-level residual tri-plane to be added to the coarse level is\nmodeled as a distribution with GAN to capture potential variations in\nrestoration. We validate RaFE on both synthetic and real cases for various\nrestoration tasks, demonstrating superior performance in both quantitative and\nqualitative evaluations, surpassing other 3D restoration methods specific to\nsingle task. Please see our project website\nhttps://zkaiwu.github.io/RaFE-Project/.\n", "rewritten_text": "Neural Radiance Fields (NeRFs) show immense potential for novel view synthesis and 3D reconstruction. However, their performance is highly sensitive to input image quality, often resulting in low-fidelity rendering from low-quality or sparse input viewpoints.  Existing NeRF restoration methods typically address specific degradation types, lacking generality.  To address this, we propose RaFE, a generic radiance field restoration pipeline applicable to various degradations, including low resolution, blur, noise, compression artifacts, and their combinations.  Our approach leverages readily available 2D restoration methods to individually enhance multi-view images.  Instead of averaging inconsistencies to reconstruct a blurred NeRF, we introduce a novel Generative Adversarial Network (GAN)-based NeRF generation approach to better handle geometric and appearance inconsistencies in the multi-view images.  Specifically, we employ a two-level tri-plane architecture: a fixed coarse level representing the low-quality NeRF, and a fine-level residual tri-plane, modeled as a GAN-generated distribution to capture variations during restoration.  We validate RaFE on synthetic and real datasets across various restoration tasks, demonstrating superior quantitative and qualitative performance compared to single-task-specific 3D restoration methods.  See our project website for details: https://zkaiwu.github.io/RaFE-Project/.\n"}, "2107.04217": {"original_text": "  This paper studies joint models for selecting correct answer sentences among\nthe top $k$ provided by answer sentence selection (AS2) modules, which are core\ncomponents of retrieval-based Question Answering (QA) systems. Our work shows\nthat a critical step to effectively exploit an answer set regards modeling the\ninterrelated information between pair of answers. For this purpose, we build a\nthree-way multi-classifier, which decides if an answer supports, refutes, or is\nneutral with respect to another one. More specifically, our neural architecture\nintegrates a state-of-the-art AS2 model with the multi-classifier, and a joint\nlayer connecting all components. We tested our models on WikiQA, TREC-QA, and a\nreal-world dataset. The results show that our models obtain the new state of\nthe art in AS2.\n", "rewritten_text": "This paper presents joint models for selecting the correct answer sentence from the top k candidates provided by answer sentence selection (AS2) modules, core components of retrieval-based question answering (QA) systems.  We demonstrate that effectively leveraging an answer set critically depends on modeling the interrelationships between answer pairs.  To this end, we developed a three-way multi-classifier that determines whether each answer supports, refutes, or is neutral with respect to another.  Our neural architecture integrates a state-of-the-art AS2 model with this multi-classifier, connected via a joint layer.  We evaluated our models on WikiQA, TREC-QA, and a real-world dataset, achieving state-of-the-art performance in AS2.\n"}, "2407.05271": {"original_text": "  Name-based gender prediction has traditionally categorized individuals as\neither female or male based on their names, using a binary classification\nsystem. That binary approach can be problematic in the cases of gender-neutral\nnames that do not align with any one gender, among other reasons. Relying\nsolely on binary gender categories without recognizing gender-neutral names can\nreduce the inclusiveness of gender prediction tasks. We introduce an additional\ngender category, i.e., \"neutral\", to study and address potential gender biases\nin Large Language Models (LLMs). We evaluate the performance of several\nfoundational and large language models in predicting gender based on first\nnames only. Additionally, we investigate the impact of adding birth years to\nenhance the accuracy of gender prediction, accounting for shifting associations\nbetween names and genders over time. Our findings indicate that most LLMs\nidentify male and female names with high accuracy (over 80%) but struggle with\ngender-neutral names (under 40%), and the accuracy of gender prediction is\nhigher for English-based first names than non-English names. The experimental\nresults show that incorporating the birth year does not improve the overall\naccuracy of gender prediction, especially for names with evolving gender\nassociations. We recommend using caution when applying LLMs for gender\nidentification in downstream tasks, particularly when dealing with non-binary\ngender labels.\n", "rewritten_text": "Traditional name-based gender prediction employs a binary classification system, categorizing individuals as either female or male.  This approach is problematic, however, as it fails to account for gender-neutral names and other non-binary identities.  The reliance on binary gender categories reduces the inclusivity of gender prediction tasks.  To address potential gender biases in Large Language Models (LLMs), we introduce a \"neutral\" gender category.  We evaluate several foundational and large language models' performance in predicting gender based solely on first names, further investigating the impact of incorporating birth years to account for temporal shifts in name-gender associations.  Our results show that most LLMs achieve high accuracy (over 80%) in identifying male and female names but significantly lower accuracy (under 40%) for gender-neutral names, with higher accuracy observed for English names than non-English names.  Incorporating birth years did not improve overall accuracy, particularly for names with evolving gender associations.  We caution against using LLMs for gender identification in downstream applications, especially when non-binary genders are involved.\n"}, "2306.09379": {"original_text": "  In this technical report, we briefly introduce the solution of our team\nVIELab-HUST for coded target restoration through atmospheric turbulence in CVPR\n2023 UG$^2$+ Track 2.2. In this task, we propose an efficient multi-stage\nframework to restore a high quality image from distorted frames. Specifically,\neach distorted frame is initially aligned using image registration to suppress\ngeometric distortion. We subsequently select the sharpest set of registered\nframes by employing a frame selection approach based on image sharpness, and\naverage them to produce an image that is largely free of geometric distortion,\nalbeit with blurriness. A learning-based deblurring method is then applied to\nremove the residual blur in the averaged image. Finally, post-processing\ntechniques are utilized to further enhance the quality of the output image. Our\nframework is capable of handling different kinds of coded target dataset\nprovided in the final testing phase, and ranked 1st on the final leaderboard.\nOur code will be available at https://github.com/xsqhust/Turbulence_Removal.\n", "rewritten_text": "This technical report presents the VIELab-HUST solution for coded target restoration through atmospheric turbulence, as submitted to the CVPR 2023 UG\u00b2\u207a Track 2.2.  Our approach employs an efficient multi-stage framework to recover high-quality images from distorted frames.  First, image registration aligns each distorted frame to mitigate geometric distortion.  Next, a sharpness-based frame selection method identifies and averages the sharpest registered frames, producing a geometrically corrected but blurry image.  A learning-based deblurring technique then removes residual blur. Finally, post-processing enhances the output image quality.  This framework successfully handled the diverse coded target datasets in the final testing phase, achieving first place on the leaderboard.  The code is publicly available at https://github.com/xsqhust/Turbulence_Removal.\n"}, "2108.03502": {"original_text": "  Automatic summarization techniques aim to shorten and generalize information\ngiven in the text while preserving its core message and the most relevant\nideas. This task can be approached and treated with a variety of methods,\nhowever, not many attempts have been made to produce solutions specifically for\nthe Russian language despite existing localizations of the state-of-the-art\nmodels. In this paper, we aim to showcase ruGPT3 ability to summarize texts,\nfine-tuning it on the corpora of Russian news with their corresponding\nhuman-generated summaries. Additionally, we employ hyperparameter tuning so\nthat the model's output becomes less random and more tied to the original text.\nWe evaluate the resulting texts with a set of metrics, showing that our\nsolution can surpass the state-of-the-art model's performance without\nadditional changes in architecture or loss function. Despite being able to\nproduce sensible summaries, our model still suffers from a number of flaws,\nnamely, it is prone to altering Named Entities present in the original text\n(such as surnames, places, dates), deviating from facts stated in the given\ndocument, and repeating the information in the summary.\n", "rewritten_text": "Automatic summarization techniques aim to condense and generalize textual information while preserving its core message and most relevant ideas.  While various methods exist, relatively few have addressed this task specifically for the Russian language, despite the availability of localized state-of-the-art models.  This paper demonstrates ruGPT3's text summarization capabilities, fine-tuning it on a corpus of Russian news articles and their corresponding human-generated summaries.  Hyperparameter tuning was employed to reduce randomness and improve adherence to the source text.  Evaluation using a suite of metrics reveals that our approach surpasses the performance of existing state-of-the-art models without architectural or loss function modifications.  However, despite generating coherent summaries, our model exhibits several shortcomings:  it is prone to altering named entities (e.g., surnames, places, dates), deviating from factual information in the source document, and repeating information within the summary.\n"}, "1901.01574": {"original_text": "  This work systematically analyzes the smoothing effect of vocabulary\nreduction for phrase translation models. We extensively compare various\nword-level vocabularies to show that the performance of smoothing is not\nsignificantly affected by the choice of vocabulary. This result provides\nempirical evidence that the standard phrase translation model is extremely\nsparse. Our experiments also reveal that vocabulary reduction is more effective\nfor smoothing large-scale phrase tables.\n", "rewritten_text": "This work systematically analyzes the smoothing effect of vocabulary reduction on phrase translation models.  Extensive comparison of various word-level vocabularies demonstrates that model performance is largely insensitive to vocabulary selection. This empirically supports the hypothesis that standard phrase translation models are highly sparse.  Furthermore, our experiments show that vocabulary reduction is particularly effective for smoothing large-scale phrase tables.\n"}, "2210.16579": {"original_text": "  Generating videos is a complex task that is accomplished by generating a set\nof temporally coherent images frame-by-frame. This limits the expressivity of\nvideos to only image-based operations on the individual video frames needing\nnetwork designs to obtain temporally coherent trajectories in the underlying\nimage space. We propose INR-V, a video representation network that learns a\ncontinuous space for video-based generative tasks. INR-V parameterizes videos\nusing implicit neural representations (INRs), a multi-layered perceptron that\npredicts an RGB value for each input pixel location of the video. The INR is\npredicted using a meta-network which is a hypernetwork trained on neural\nrepresentations of multiple video instances. Later, the meta-network can be\nsampled to generate diverse novel videos enabling many downstream video-based\ngenerative tasks. Interestingly, we find that conditional regularization and\nprogressive weight initialization play a crucial role in obtaining INR-V. The\nrepresentation space learned by INR-V is more expressive than an image space\nshowcasing many interesting properties not possible with the existing works.\nFor instance, INR-V can smoothly interpolate intermediate videos between known\nvideo instances (such as intermediate identities, expressions, and poses in\nface videos). It can also in-paint missing portions in videos to recover\ntemporally coherent full videos. In this work, we evaluate the space learned by\nINR-V on diverse generative tasks such as video interpolation, novel video\ngeneration, video inversion, and video inpainting against the existing\nbaselines. INR-V significantly outperforms the baselines on several of these\ndemonstrated tasks, clearly showcasing the potential of the proposed\nrepresentation space.\n", "rewritten_text": "Generating videos is a complex process, typically involving the frame-by-frame generation of temporally coherent images. This approach limits expressivity to image-based operations on individual frames, requiring sophisticated network architectures to ensure temporal coherence.  We propose INR-V, a video representation network that learns a continuous latent space for video generation. INR-V parameterizes videos using implicit neural representations (INRs), multi-layered perceptrons that predict RGB values for each pixel location.  A meta-network\u2014a hypernetwork trained on neural representations of multiple video instances\u2014predicts the INR parameters.  Sampling this meta-network generates diverse novel videos, enabling various downstream generative tasks.  Crucially, we find that conditional regularization and progressive weight initialization are essential for INR-V's success.\n\nThe latent space learned by INR-V is significantly more expressive than traditional image-based approaches, exhibiting several novel properties. For example, INR-V smoothly interpolates between known video instances (e.g., interpolating identities, expressions, and poses in face videos) and effectively inpaints missing portions to reconstruct temporally coherent videos.  We evaluate INR-V on diverse generative tasks\u2014video interpolation, novel video generation, video inversion, and video inpainting\u2014demonstrating significant performance improvements over existing baselines.  These results highlight the potential of our proposed representation space for advanced video generation.\n"}, "2101.01843": {"original_text": "  Autonomous driving applications use two types of sensor systems to identify\nvehicles - depth sensing LiDAR and radiance sensing cameras. We compare the\nperformance (average precision) of a ResNet for vehicle detection in complex,\ndaytime, driving scenes when the input is a depth map (D = d(x,y)), a radiance\nimage (L = r(x,y)), or both [D,L]. (1) When the spatial sampling resolution of\nthe depth map and radiance image are equal to typical camera resolutions, a\nResNet detects vehicles at higher average precision from depth than radiance.\n(2) As the spatial sampling of the depth map declines to the range of current\nLiDAR devices, the ResNet average precision is higher for radiance than depth.\n(3) For a hybrid system that combines a depth map and radiance image, the\naverage precision is higher than using depth or radiance alone. We established\nthese observations in simulation and then confirmed them using realworld data.\nThe advantage of combining depth and radiance can be explained by noting that\nthe two type of information have complementary weaknesses. The radiance data\nare limited by dynamic range and motion blur. The LiDAR data have relatively\nlow spatial resolution. The ResNet combines the two data sources effectively to\nimprove overall vehicle detection.\n", "rewritten_text": "Autonomous driving applications utilize two sensor systems for vehicle detection: depth-sensing LiDAR and radiance-sensing cameras.  We compared the average precision of a ResNet for vehicle detection in complex daytime driving scenes using three input modalities: depth maps (D = d(x,y)), radiance images (L = r(x,y)), and a combination of both [D,L].  Our findings are as follows: (1) At camera-resolution spatial sampling, the ResNet achieved higher average precision using depth maps than radiance images. (2)  Reducing the depth map's spatial sampling resolution to that of current LiDAR devices resulted in higher average precision using radiance images. (3)  A hybrid system combining depth and radiance maps consistently outperformed systems using either modality alone.  These observations were initially validated through simulation and subsequently confirmed using real-world data.  The superior performance of the hybrid approach stems from the complementary nature of the two data types: radiance data suffer from limitations in dynamic range and motion blur, while LiDAR data exhibit relatively low spatial resolution. The ResNet effectively fuses these data sources, improving overall vehicle detection accuracy.\n"}, "1511.03328": {"original_text": "  Deep convolutional neural networks (CNNs) are the backbone of state-of-art\nsemantic image segmentation systems. Recent work has shown that complementing\nCNNs with fully-connected conditional random fields (CRFs) can significantly\nenhance their object localization accuracy, yet dense CRF inference is\ncomputationally expensive. We propose replacing the fully-connected CRF with\ndomain transform (DT), a modern edge-preserving filtering method in which the\namount of smoothing is controlled by a reference edge map. Domain transform\nfiltering is several times faster than dense CRF inference and we show that it\nyields comparable semantic segmentation results, accurately capturing object\nboundaries. Importantly, our formulation allows learning the reference edge map\nfrom intermediate CNN features instead of using the image gradient magnitude as\nin standard DT filtering. This produces task-specific edges in an end-to-end\ntrainable system optimizing the target semantic segmentation quality.\n", "rewritten_text": "State-of-the-art semantic image segmentation systems rely heavily on deep convolutional neural networks (CNNs).  While complementing CNNs with fully connected conditional random fields (CRFs) has been shown to significantly improve object localization accuracy, dense CRF inference is computationally expensive.  We propose replacing the fully connected CRF with a domain transform (DT), a fast, edge-preserving filtering method whose smoothing is controlled by a reference edge map.  Domain transform filtering is significantly faster than dense CRF inference while yielding comparable semantic segmentation results, accurately capturing object boundaries.  Crucially, our approach learns the reference edge map from intermediate CNN features, rather than relying on image gradient magnitude as in standard DT filtering. This end-to-end trainable system generates task-specific edges, optimizing for target semantic segmentation quality.\n"}, "1610.03155": {"original_text": "  Convolutional Neural Networks (CNN) have demon- strated its successful\napplications in computer vision, speech recognition, and natural language\nprocessing. For object recog- nition, CNNs might be limited by its strict label\nrequirement and an implicit assumption that images are supposed to be target-\nobject-dominated for optimal solutions. However, the labeling procedure,\nnecessitating laying out the locations of target ob- jects, is very tedious,\nmaking high-quality large-scale dataset prohibitively expensive. Data\naugmentation schemes are widely used when deep networks suffer the insufficient\ntraining data problem. All the images produced through data augmentation share\nthe same label, which may be problematic since not all data augmentation\nmethods are label-preserving. In this paper, we propose a weakly supervised CNN\nframework named Multiple Instance Learning Convolutional Neural Networks\n(MILCNN) to solve this problem. We apply MILCNN framework to object recognition\nand report state-of-the-art performance on three benchmark datasets: CIFAR10,\nCIFAR100 and ILSVRC2015 classification dataset.\n", "rewritten_text": "Convolutional Neural Networks (CNNs) have demonstrated successful applications in computer vision, speech recognition, and natural language processing.  However, for object recognition, CNNs are limited by their strict label requirements and the implicit assumption that images are target-object-dominated for optimal performance.  The laborious process of labeling object locations makes creating high-quality, large-scale datasets prohibitively expensive. While data augmentation techniques are commonly used to address insufficient training data in deep networks, this approach can be problematic because not all augmentation methods preserve labels; all augmented images inherit the original label.  This paper proposes a weakly supervised CNN framework, Multiple Instance Learning Convolutional Neural Networks (MILCNN), to address this limitation.  We apply the MILCNN framework to object recognition and report state-of-the-art performance on three benchmark datasets: CIFAR-10, CIFAR-100, and ILSVRC2015.\n"}, "1711.06606": {"original_text": "  To realize the full potential of deep learning for medical imaging, large\nannotated datasets are required for training. Such datasets are difficult to\nacquire because labeled medical images are not usually available due to privacy\nissues, lack of experts available for annotation, underrepresentation of rare\nconditions and poor standardization. Lack of annotated data has been addressed\nin conventional vision applications using synthetic images refined via\nunsupervised adversarial training to look like real images. However, this\napproach is difficult to extend to general medical imaging because of the\ncomplex and diverse set of features found in real human tissues. We propose an\nalternative framework that uses a reverse flow, where adversarial training is\nused to make real medical images more like synthetic images, and hypothesize\nthat clinically-relevant features can be preserved via self-regularization.\nThese domain-adapted images can then be accurately interpreted by networks\ntrained on large datasets of synthetic medical images. We test this approach\nfor the notoriously difficult task of depth-estimation from endoscopy. We train\na depth estimator on a large dataset of synthetic images generated using an\naccurate forward model of an endoscope and an anatomically-realistic colon.\nThis network predicts significantly better depths when using synthetic-like\ndomain-adapted images compared to the real images, confirming that the\nclinically-relevant features of depth are preserved.\n", "rewritten_text": "Large annotated datasets are crucial for realizing the full potential of deep learning in medical imaging.  However, acquiring such datasets is challenging due to privacy concerns, limited expert annotators, underrepresentation of rare conditions, and poor standardization of medical images.  While synthetic images refined through unsupervised adversarial training have addressed data scarcity in conventional computer vision, this approach is difficult to generalize to medical imaging because of the complex and diverse features of human tissues.\n\nWe propose a novel framework employing a reverse adversarial training strategy:  making real medical images resemble synthetic ones. We hypothesize that clinically relevant features are preserved through self-regularization.  These domain-adapted images can then be effectively interpreted by networks trained on large synthetic datasets.\n\nWe evaluated this approach on the challenging task of depth estimation from endoscopic images.  A depth estimator was trained on a large dataset of synthetic images generated using an accurate forward model of an endoscope and an anatomically realistic colon.  Results demonstrate significantly improved depth prediction using synthetic-like, domain-adapted images compared to using the original real images, confirming the preservation of clinically relevant depth features.\n"}, "2402.17074": {"original_text": "  Digital Image Correlation (DIC) is an optical technique that measures\ndisplacement and strain by tracking pattern movement in a sequence of captured\nimages during testing. DIC has gained recognition in asphalt pavement\nengineering since the early 2000s. However, users often perceive the DIC\ntechnique as an out-of-box tool and lack a thorough understanding of its\noperational and measurement principles. This article presents a state-of-art\nreview of DIC as a crucial tool for laboratory testing of asphalt concrete\n(AC), primarily focusing on the widely utilized 2D-DIC and 3D-DIC techniques.\nTo address frequently asked questions from users, the review thoroughly\nexamines the optimal methods for preparing speckle patterns, configuring\nsingle-camera or dual-camera imaging systems, conducting DIC analyses, and\nexploring various applications. Furthermore, emerging DIC methodologies such as\nDigital Volume Correlation and deep-learning-based DIC are introduced,\nhighlighting their potential for future applications in pavement engineering.\nThe article also provides a comprehensive and reliable flowchart for\nimplementing DIC in AC characterization. Finally, critical directions for\nfuture research are presented.\n", "rewritten_text": "Digital image correlation (DIC) is an optical technique that measures displacement and strain by tracking pattern movement in a sequence of images acquired during testing.  Since the early 2000s, DIC has become increasingly recognized in asphalt pavement engineering. However, users often view DIC as a \"black box\" tool, lacking a comprehensive understanding of its operational principles and measurement capabilities.  This article provides a state-of-the-art review of DIC as a crucial tool for laboratory testing of asphalt concrete (AC), focusing primarily on the widely used 2D- and 3D-DIC techniques.  To address common user questions, the review thoroughly examines optimal speckle pattern preparation, single- and dual-camera imaging system configuration, DIC analysis procedures, and diverse applications.  Additionally, emerging methodologies such as digital volume correlation (DVC) and deep-learning-based DIC are introduced, highlighting their potential future applications in pavement engineering.  A comprehensive and reliable flowchart for implementing DIC in AC characterization is also provided.  Finally, the article concludes by outlining critical directions for future research.\n"}, "1805.06173": {"original_text": "  Existing deep convolutional neural networks have found major success in image\nderaining, but at the expense of an enormous number of parameters. This limits\ntheir potential application, for example in mobile devices. In this paper, we\npropose a lightweight pyramid of networks (LPNet) for single image deraining.\nInstead of designing a complex network structures, we use domain-specific\nknowledge to simplify the learning process. Specifically, we find that by\nintroducing the mature Gaussian-Laplacian image pyramid decomposition\ntechnology to the neural network, the learning problem at each pyramid level is\ngreatly simplified and can be handled by a relatively shallow network with few\nparameters. We adopt recursive and residual network structures to build the\nproposed LPNet, which has less than 8K parameters while still achieving\nstate-of-the-art performance on rain removal. We also discuss the potential\nvalue of LPNet for other low- and high-level vision tasks.\n", "rewritten_text": "Deep convolutional neural networks (CNNs) have achieved significant success in image deraining, but their substantial parameter counts hinder deployment on resource-constrained devices such as mobile phones.  This paper introduces LPNet, a lightweight pyramid network for single image deraining.  Instead of relying on complex network architectures, we leverage domain-specific knowledge to simplify the learning process.  Specifically, integrating Gaussian-Laplacian pyramid decomposition simplifies the learning problem at each pyramid level, enabling the use of relatively shallow networks with few parameters.  LPNet employs recursive and residual network structures, resulting in a model with fewer than 8,000 parameters while achieving state-of-the-art rain removal performance.  We further discuss the potential applicability of LPNet to other low- and high-level vision tasks.\n"}, "2208.00919": {"original_text": "  Visual-inertial localization is a key problem in computer vision and robotics\napplications such as virtual reality, self-driving cars, and aerial vehicles.\nThe goal is to estimate an accurate pose of an object when either the\nenvironment or the dynamics are known. Absolute pose regression (APR)\ntechniques directly regress the absolute pose from an image input in a known\nscene using convolutional and spatio-temporal networks. Odometry methods\nperform relative pose regression (RPR) that predicts the relative pose from a\nknown object dynamic (visual or inertial inputs). The localization task can be\nimproved by retrieving information from both data sources for a cross-modal\nsetup, which is a challenging problem due to contradictory tasks. In this work,\nwe conduct a benchmark to evaluate deep multimodal fusion based on pose graph\noptimization and attention networks. Auxiliary and Bayesian learning are\nutilized for the APR task. We show accuracy improvements for the APR-RPR task\nand for the RPR-RPR task for aerial vehicles and hand-held devices. We conduct\nexperiments on the EuRoC MAV and PennCOSYVIO datasets and record and evaluate a\nnovel industry dataset.\n", "rewritten_text": "Visual-inertial localization is crucial for computer vision and robotics applications, including virtual reality, autonomous vehicles, and unmanned aerial vehicles (UAVs).  The objective is to accurately estimate an object's pose given knowledge of either the environment or its dynamics. Absolute pose regression (APR) techniques directly regress the absolute pose from an image within a known scene, leveraging convolutional and spatio-temporal networks.  Conversely, odometry methods employ relative pose regression (RPR), predicting relative pose changes based on known visual or inertial dynamics.  Improving localization by fusing information from both modalities presents a significant challenge due to potentially conflicting tasks.  This work benchmarks deep multimodal fusion using pose graph optimization and attention networks, incorporating auxiliary and Bayesian learning for APR.  We demonstrate improved accuracy for APR-RPR and RPR-RPR tasks on both UAVs and handheld devices, using the EuRoC MAV and PennCOSYVIO datasets, and a novel, industry-standard dataset collected and evaluated herein.\n"}, "2107.0984": {"original_text": "  Multilingual pre-trained contextual embedding models (Devlin et al., 2019)\nhave achieved impressive performance on zero-shot cross-lingual transfer tasks.\nFinding the most effective fine-tuning strategy to fine-tune these models on\nhigh-resource languages so that it transfers well to the zero-shot languages is\na non-trivial task. In this paper, we propose a novel meta-optimizer to\nsoft-select which layers of the pre-trained model to freeze during fine-tuning.\nWe train the meta-optimizer by simulating the zero-shot transfer scenario.\nResults on cross-lingual natural language inference show that our approach\nimproves over the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al.,\n2020).\n", "rewritten_text": "Multilingual pre-trained contextual embedding models (Devlin et al., 2019) have demonstrated impressive performance on zero-shot cross-lingual transfer tasks.  However, finding an effective fine-tuning strategy for high-resource languages that facilitates strong zero-shot transfer to low-resource languages remains a challenge.  This paper proposes a novel meta-optimizer that selectively freezes layers of the pre-trained model during fine-tuning.  We train this meta-optimizer by simulating the zero-shot transfer scenario.  Experiments on cross-lingual natural language inference demonstrate that our approach outperforms both a simple fine-tuning baseline and X-MAML (Nooralahzadeh et al., 2020).\n"}, "2010.09298": {"original_text": "  Though deep learning has achieved advanced performance recently, it remains a\nchallenging task in the field of medical imaging, as obtaining reliable labeled\ntraining data is time-consuming and expensive. In this paper, we propose a\ndouble-uncertainty weighted method for semi-supervised segmentation based on\nthe teacher-student model. The teacher model provides guidance for the student\nmodel by penalizing their inconsistent prediction on both labeled and unlabeled\ndata. We train the teacher model using Bayesian deep learning to obtain\ndouble-uncertainty, i.e. segmentation uncertainty and feature uncertainty. It\nis the first to extend segmentation uncertainty estimation to feature\nuncertainty, which reveals the capability to capture information among\nchannels. A learnable uncertainty consistency loss is designed for the\nunsupervised learning process in an interactive manner between prediction and\nuncertainty. With no ground-truth for supervision, it can still incentivize\nmore accurate teacher's predictions and facilitate the model to reduce\nuncertain estimations. Furthermore, our proposed double-uncertainty serves as a\nweight on each inconsistency penalty to balance and harmonize supervised and\nunsupervised training processes. We validate the proposed feature uncertainty\nand loss function through qualitative and quantitative analyses. Experimental\nresults show that our method outperforms the state-of-the-art uncertainty-based\nsemi-supervised methods on two public medical datasets.\n", "rewritten_text": "While deep learning has achieved remarkable performance in recent years, its application to medical image segmentation remains challenging due to the time and cost associated with acquiring reliable labeled training data.  This paper proposes a novel double-uncertainty weighted semi-supervised segmentation method based on a teacher-student model.  The teacher model guides the student model by penalizing inconsistencies in their predictions on both labeled and unlabeled data.  We train the teacher model using Bayesian deep learning to estimate both segmentation uncertainty and feature uncertainty\u2014a novel extension that leverages inter-channel information.  A learnable uncertainty consistency loss, interactively incorporating prediction and uncertainty, is designed for unsupervised learning.  Even without ground truth supervision, this loss encourages more accurate teacher predictions and reduces uncertainty in the estimations.  Furthermore, the proposed double-uncertainty weighting balances and harmonizes supervised and unsupervised training by weighting each inconsistency penalty.  We validate the proposed feature uncertainty and loss function through qualitative and quantitative analyses.  Experimental results on two public medical image datasets demonstrate that our method outperforms state-of-the-art uncertainty-based semi-supervised segmentation techniques.\n"}, "2108.11575": {"original_text": "  Spatio-temporal representational learning has been widely adopted in various\nfields such as action recognition, video object segmentation, and action\nanticipation. Previous spatio-temporal representational learning approaches\nprimarily employ ConvNets or sequential models,e.g., LSTM, to learn the\nintra-frame and inter-frame features. Recently, Transformer models have\nsuccessfully dominated the study of natural language processing (NLP), image\nclassification, etc. However, the pure-Transformer based spatio-temporal\nlearning can be prohibitively costly on memory and computation to extract\nfine-grained features from a tiny patch. To tackle the training difficulty and\nenhance the spatio-temporal learning, we construct a shifted chunk Transformer\nwith pure self-attention blocks. Leveraging the recent efficient Transformer\ndesign in NLP, this shifted chunk Transformer can learn hierarchical\nspatio-temporal features from a local tiny patch to a global video clip. Our\nshifted self-attention can also effectively model complicated inter-frame\nvariances. Furthermore, we build a clip encoder based on Transformer to model\nlong-term temporal dependencies. We conduct thorough ablation studies to\nvalidate each component and hyper-parameters in our shifted chunk Transformer,\nand it outperforms previous state-of-the-art approaches on Kinetics-400,\nKinetics-600, UCF101, and HMDB51.\n", "rewritten_text": "Spatio-temporal representation learning has been widely adopted in action recognition, video object segmentation, and action anticipation.  Previous approaches primarily relied on convolutional neural networks (ConvNets) or sequential models like LSTMs to learn intra- and inter-frame features.  While Transformer models have achieved significant success in natural language processing (NLP) and image classification, their direct application to spatio-temporal learning can be computationally expensive and memory-intensive, particularly when extracting fine-grained features from small patches.\n\nTo address this, we propose a shifted chunk Transformer with pure self-attention blocks.  Inspired by recent efficient Transformer designs in NLP, our model learns hierarchical spatio-temporal features, progressing from local patches to global video clips.  The shifted self-attention mechanism effectively models complex inter-frame variations.  Furthermore, a Transformer-based clip encoder captures long-term temporal dependencies.\n\nWe conducted thorough ablation studies to validate the individual components and hyperparameters of our shifted chunk Transformer.  Our approach outperforms state-of-the-art methods on the Kinetics-400, Kinetics-600, UCF101, and HMDB51 datasets.\n"}, "2102.0208": {"original_text": "  We introduce a top-down approach to discourse parsing that is conceptually\nsimpler than its predecessors (Kobayashi et al., 2020; Zhang et al., 2020). By\nframing the task as a sequence labelling problem where the goal is to\niteratively segment a document into individual discourse units, we are able to\neliminate the decoder and reduce the search space for splitting points. We\nexplore both traditional recurrent models and modern pre-trained transformer\nmodels for the task, and additionally introduce a novel dynamic oracle for\ntop-down parsing. Based on the Full metric, our proposed LSTM model sets a new\nstate-of-the-art for RST parsing.\n", "rewritten_text": "This paper presents a conceptually simpler top-down approach to discourse parsing than previous methods (Kobayashi et al., 2020; Zhang et al., 2020).  Framing the task as iterative sequence labeling to segment a document into discourse units eliminates the decoder and reduces the search space for segmentation points. We evaluate both recurrent and pre-trained transformer models, and introduce a novel dynamic oracle for top-down parsing.  Our LSTM model achieves a new state-of-the-art in RST parsing, as measured by the Full metric.\n"}, "1112.2386": {"original_text": "  This paper proposes a new procedure in order to improve the performance of\nblock matching and 3-D filtering (BM3D) image denoising algorithm. It is\ndemonstrated that it is possible to achieve a better performance than that of\nBM3D algorithm in a variety of noise levels. This method changes BM3D algorithm\nparameter values according to noise level, removes prefiltering, which is used\nin high noise level; therefore Peak Signal-to-Noise Ratio (PSNR) and visual\nquality get improved, and BM3D complexities and processing time are reduced.\nThis improved BM3D algorithm is extended and used to denoise satellite and\ncolor filter array (CFA) images. Output results show that the performance has\nupgraded in comparison with current methods of denoising satellite and CFA\nimages. In this regard this algorithm is compared with Adaptive PCA algorithm,\nthat has led to superior performance for denoising CFA images, on the subject\nof PSNR and visual quality. Also the processing time has decreased\nsignificantly.\n", "rewritten_text": "This paper presents a novel procedure to enhance the performance of the block-matching and 3-D filtering (BM3D) image denoising algorithm.  We demonstrate superior performance to the standard BM3D algorithm across a range of noise levels.  Our method adapts BM3D parameter values to the noise level and eliminates the prefiltering step used in high-noise scenarios. This results in improved peak signal-to-noise ratio (PSNR), enhanced visual quality, and reduced computational complexity and processing time.  This improved BM3D algorithm is then applied to denoise satellite and color filter array (CFA) images.  Results show performance improvements over existing methods for both image types.  Specifically, comparison with the Adaptive PCA algorithm reveals superior performance in CFA image denoising, exhibiting higher PSNR, improved visual quality, and significantly reduced processing time.\n"}, "2103.06678": {"original_text": "  In this paper, we explore the effects of language variants, data sizes, and\nfine-tuning task types in Arabic pre-trained language models. To do so, we\nbuild three pre-trained language models across three variants of Arabic: Modern\nStandard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a\nfourth language model which is pre-trained on a mix of the three. We also\nexamine the importance of pre-training data size by building additional models\nthat are pre-trained on a scaled-down set of the MSA variant. We compare our\ndifferent models to each other, as well as to eight publicly available models\nby fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest\nthat the variant proximity of pre-training data to fine-tuning data is more\nimportant than the pre-training data size. We exploit this insight in defining\nan optimized system selection model for the studied tasks.\n", "rewritten_text": "This paper investigates the impact of language variant, data size, and fine-tuning task type on the performance of Arabic pre-trained language models (PLMs).  We trained four PLMs: three using distinct Arabic variants\u2014Modern Standard Arabic (MSA), dialectal Arabic, and Classical Arabic\u2014and a fourth trained on a mixed corpus of all three.  To assess the effect of data size, we also trained additional models on reduced MSA datasets.  These models, along with eight publicly available PLMs, were fine-tuned on five NLP tasks across twelve datasets.  Our results indicate that the similarity between the pre-training and fine-tuning data variants is a more significant factor than pre-training data size.  This finding informed the development of an optimized system selection model for the tasks studied.\n"}, "2408.00331": {"original_text": "  Reliably detecting when a deployed machine learning model is likely to fail\non a given input is crucial for ensuring safe operation. In this work, we\npropose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel\napproach that leverages priors from large language models (LLMs) and\nvision-language models (VLMs) to detect failures in image classification\nmodels. DECIDER utilizes LLMs to specify task-relevant core attributes and\nconstructs a ``debiased'' version of the classifier by aligning its visual\nfeatures to these core attributes using a VLM, and detects potential failure by\nmeasuring disagreement between the original and debiased models. In addition to\nproactively identifying samples on which the model would fail, DECIDER also\nprovides human-interpretable explanations for failure through a novel\nattribute-ablation strategy. Through extensive experiments across diverse\nbenchmarks spanning subpopulation shifts (spurious correlations, class\nimbalance) and covariate shifts (synthetic corruptions, domain shifts), DECIDER\nconsistently achieves state-of-the-art failure detection performance,\nsignificantly outperforming baselines in terms of the overall Matthews\ncorrelation coefficient as well as failure and success recall. Our codes can be\naccessed at~\\url{https://github.com/kowshikthopalli/DECIDER/}\n", "rewritten_text": "Reliable detection of impending failure in deployed machine learning models is crucial for safe operation.  This work introduces DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel approach leveraging large language models (LLMs) and vision-language models (VLMs) to detect failures in image classification.  DECIDER uses LLMs to identify task-relevant core attributes.  It then constructs a \"debiased\" classifier by aligning the original classifier's visual features to these attributes using a VLM.  Failure is detected by measuring the disagreement between the original and debiased models.  Beyond proactive failure identification, DECIDER provides human-interpretable explanations via a novel attribute-ablation strategy.  Extensive experiments across diverse benchmarks\u2014including subpopulation shifts (spurious correlations, class imbalance) and covariate shifts (synthetic corruptions, domain shifts)\u2014demonstrate that DECIDER consistently achieves state-of-the-art failure detection performance, significantly outperforming baselines in terms of Matthews correlation coefficient, failure recall, and success recall.  The code is available at [https://github.com/kowshikthopalli/DECIDER/](https://github.com/kowshikthopalli/DECIDER/).\n"}, "1708.06126": {"original_text": "  This paper presents a novel application to detect counterfeit identity\ndocuments forged by a scan-printing operation. Texture analysis approaches are\nproposed to extract validation features from security background that is\nusually printed in documents as IDs or banknotes. The main contribution of this\nwork is the end-to-end mobile-server architecture, which provides a service for\nnon-expert users and therefore can be used in several scenarios. The system\nalso provides a crowdsourcing mode so labeled images can be gathered,\ngenerating databases for incremental training of the algorithms.\n", "rewritten_text": "This paper presents a novel application for detecting counterfeit identity documents produced via scan-and-print forgery.  We propose a texture analysis approach to extract validation features from the security backgrounds commonly printed on identity documents and banknotes.  The primary contribution is an end-to-end mobile-server architecture that provides a user-friendly service for non-experts, enabling deployment in diverse scenarios.  Furthermore, the system incorporates a crowdsourcing mechanism to collect labeled images, thereby generating databases for the incremental training of our algorithms.\n"}, "2409.12421": {"original_text": "  Camouflaged object detection (COD) aims to segment camouflaged objects which\nexhibit very similar patterns with the surrounding environment. Recent research\nworks have shown that enhancing the feature representation via the frequency\ninformation can greatly alleviate the ambiguity problem between the foreground\nobjects and the background.With the emergence of vision foundation models, like\nInternImage, Segment Anything Model etc, adapting the pretrained model on COD\ntasks with a lightweight adapter module shows a novel and promising research\ndirection. Existing adapter modules mainly care about the feature adaptation in\nthe spatial domain. In this paper, we propose a novel frequency-guided spatial\nadaptation method for COD task. Specifically, we transform the input features\nof the adapter into frequency domain. By grouping and interacting with\nfrequency components located within non overlapping circles in the spectrogram,\ndifferent frequency components are dynamically enhanced or weakened, making the\nintensity of image details and contour features adaptively adjusted. At the\nsame time, the features that are conducive to distinguishing object and\nbackground are highlighted, indirectly implying the position and shape of\ncamouflaged object. We conduct extensive experiments on four widely adopted\nbenchmark datasets and the proposed method outperforms 26 state-of-the-art\nmethods with large margins. Code will be released.\n", "rewritten_text": "Camouflaged object detection (COD) aims to segment objects that visually blend with their surroundings.  Recent research demonstrates that leveraging frequency information to enhance feature representation significantly mitigates the ambiguity between foreground and background.  The advent of vision foundation models, such as InternImage and the Segment Anything Model, suggests a promising research direction: adapting pretrained models to COD tasks using lightweight adapter modules.  Existing adapter modules primarily focus on spatial feature adaptation.  This paper introduces a novel frequency-guided spatial adaptation method for COD.  Specifically, we transform the adapter's input features into the frequency domain.  By grouping and interacting with frequency components within non-overlapping circular regions of the spectrogram, we dynamically enhance or suppress different frequencies, adaptively adjusting the intensity of image details and contour features. This process highlights features crucial for distinguishing objects from the background, indirectly indicating the camouflaged object's position and shape.  Extensive experiments on four widely used benchmark datasets demonstrate that our method surpasses 26 state-of-the-art methods by a significant margin.  The code will be publicly released.\n"}, "1712.03687": {"original_text": "  Because of affected by weather conditions, camera pose and range, etc.\nObjects are usually small, blur, occluded and diverse pose in the images\ngathered from outdoor surveillance cameras or access control system. It is\nchallenging and important to detect faces precisely for face recognition system\nin the field of public security. In this paper, we design a based on context\nmodeling structure named Feature Hierarchy Encoder-Decoder Network for face\ndetection(FHEDN), which can detect small, blur and occluded face with hierarchy\nby hierarchy from the end to the beginning likes encoder-decoder in a single\nnetwork. The proposed network is consist of multiple context modeling and\nprediction modules, which are in order to detect small, blur, occluded and\ndiverse pose faces. In addition, we analyse the influence of distribution of\ntraining set, scale of default box and receipt field size to detection\nperformance in implement stage. Demonstrated by experiments, Our network\nachieves promising performance on WIDER FACE and FDDB benchmarks.\n", "rewritten_text": "Outdoor surveillance and access control cameras often capture images of small, blurry, occluded, and variably posed faces, due to factors such as weather conditions, camera pose, and range.  Precise face detection in these images is crucial yet challenging for public security applications.  This paper presents a novel context-modeling architecture for face detection, the Feature Hierarchy Encoder-Decoder Network (FHEDN).  FHEDN detects small, blurry, and occluded faces hierarchically, employing an encoder-decoder structure within a single network to process features from coarse to fine detail.  The network comprises multiple context-modeling and prediction modules designed to handle faces with diverse poses and conditions.  Furthermore, we analyze the impact of training data distribution, default box scale, and receptive field size on detection performance.  Experimental results on the WIDER FACE and FDDB benchmarks demonstrate the promising performance of our network.\n"}, "2010.01305": {"original_text": "  Street view images classification aiming at urban land use analysis is\ndifficult because the class labels (e.g., commercial area), are concepts with\nhigher abstract level compared to the ones of general visual tasks (e.g.,\npersons and cars). Therefore, classification models using only visual features\noften fail to achieve satisfactory performance. In this paper, a novel approach\nbased on a \"Detector-Encoder-Classifier\" framework is proposed. Instead of\nusing visual features of the whole image directly as common image-level models\nbased on convolutional neural networks (CNNs) do, the proposed framework\nfirstly obtains the bounding boxes of buildings in street view images from a\ndetector. Their contextual information such as the co-occurrence patterns of\nbuilding classes and their layout are then encoded into metadata by the\nproposed algorithm \"CODING\" (Context encOding of Detected buildINGs). Finally,\nthese bounding box metadata are classified by a recurrent neural network (RNN).\nIn addition, we made a dual-labeled dataset named \"BEAUTY\" (Building dEtection\nAnd Urban funcTional-zone portraYing) of 19,070 street view images and 38,857\nbuildings based on the existing BIC GSV [1]. The dataset can be used not only\nfor street view image classification, but also for multi-class building\ndetection. Experiments on \"BEAUTY\" show that the proposed approach achieves a\n12.65% performance improvement on macro-precision and 12% on macro-recall over\nimage-level CNN based models. Our code and dataset are available at\nhttps://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/\n", "rewritten_text": "Classifying street view images for urban land-use analysis is challenging because class labels (e.g., \"commercial area\") represent higher-level abstractions than those in typical visual tasks (e.g., \"person,\" \"car\").  Consequently, models relying solely on visual features often underperform. This paper proposes a novel \"Detector-Encoder-Classifier\" framework. Unlike conventional convolutional neural network (CNN)-based image-level models, our framework first detects building bounding boxes in street view images.  A novel algorithm, CODING (Context encOding of Detected buildINGs), then encodes contextual information, such as building class co-occurrence patterns and layout, into metadata. Finally, a recurrent neural network (RNN) classifies these bounding box metadata.  We also introduce BEAUTY (Building dEtection And Urban funcTional-zone portraYing), a dual-labeled dataset of 19,070 street view images and 38,857 buildings, derived from the BIC GSV dataset [1].  BEAUTY supports both street view image classification and multi-class building detection. Experiments on BEAUTY demonstrate a 12.65% improvement in macro-precision and a 12% improvement in macro-recall over image-level CNN models.  Our code and dataset are available at https://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/.\n"}, "2403.10737": {"original_text": "  Facial action unit (AU) detection is a fundamental block for objective facial\nexpression analysis. Supervised learning approaches require a large amount of\nmanual labeling which is costly. The limited labeled data are also not diverse\nin terms of gender which can affect model fairness. In this paper, we propose\nto use synthetically generated data and multi-source domain adaptation (MSDA)\nto address the problems of the scarcity of labeled data and the diversity of\nsubjects. Specifically, we propose to generate a diverse dataset through\nsynthetic facial expression re-targeting by transferring the expressions from\nreal faces to synthetic avatars. Then, we use MSDA to transfer the AU detection\nknowledge from a real dataset and the synthetic dataset to a target dataset.\nInstead of aligning the overall distributions of different domains, we propose\nPaired Moment Matching (PM2) to align the features of the paired real and\nsynthetic data with the same facial expression. To further improve gender\nfairness, PM2 matches the features of the real data with a female and a male\nsynthetic image. Our results indicate that synthetic data and the proposed\nmodel improve both AU detection performance and fairness across genders,\ndemonstrating its potential to solve AU detection in-the-wild.\n", "rewritten_text": "Facial action unit (AU) detection is fundamental to objective facial expression analysis.  However, supervised learning approaches require extensive, costly manual labeling, resulting in limited and often gender-biased datasets that hinder model fairness.  This paper proposes a novel approach leveraging synthetic data generation and multi-source domain adaptation (MSDA) to address these data scarcity and bias issues.  We generate a diverse synthetic dataset by re-targeting facial expressions from real faces onto synthetic avatars.  Subsequently, we employ MSDA to transfer AU detection knowledge from both real and synthetic datasets to a target dataset.  Instead of aligning the overall domain distributions, we introduce Paired Moment Matching (PM2) to align features of paired real and synthetic images depicting the same expression.  To enhance gender fairness, PM2 further aligns features of real images with both female and male synthetic counterparts.  Our results demonstrate that this synthetic data augmentation and the proposed PM2 model significantly improve both AU detection performance and gender fairness, showcasing its potential for robust in-the-wild AU detection.\n"}, "2401.03129": {"original_text": "  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n", "rewritten_text": "Recent advances in large language models (LLMs) have demonstrated remarkable proficiency across diverse tasks, fueling a surge in their development.  A common practice in LLM development involves continual pre-training on previously fine-tuned models; however, this can lead to catastrophic forgetting.  This work investigates catastrophic forgetting during the continual pre-training of fine-tuned LLMs. We evaluate the impact of this continuous pre-training across various dimensions, including output format, knowledge retention, and reliability.  Experimental results highlight the significant challenge of mitigating catastrophic forgetting during continual pre-training, particularly the problem of repeated training data.\n"}, "2105.07197": {"original_text": "  Modern machine learning models for computer vision exceed humans in accuracy\non specific visual recognition tasks, notably on datasets like ImageNet.\nHowever, high accuracy can be achieved in many ways. The particular decision\nfunction found by a machine learning system is determined not only by the data\nto which the system is exposed, but also the inductive biases of the model,\nwhich are typically harder to characterize. In this work, we follow a recent\ntrend of in-depth behavioral analyses of neural network models that go beyond\naccuracy as an evaluation metric by looking at patterns of errors. Our focus is\non comparing a suite of standard Convolutional Neural Networks (CNNs) and a\nrecently-proposed attention-based network, the Vision Transformer (ViT), which\nrelaxes the translation-invariance constraint of CNNs and therefore represents\na model with a weaker set of inductive biases. Attention-based networks have\npreviously been shown to achieve higher accuracy than CNNs on vision tasks, and\nwe demonstrate, using new metrics for examining error consistency with more\ngranularity, that their errors are also more consistent with those of humans.\nThese results have implications both for building more human-like vision\nmodels, as well as for understanding visual object recognition in humans.\n", "rewritten_text": "Modern computer vision models surpass human accuracy on specific visual recognition tasks, particularly those using datasets like ImageNet.  However, high accuracy can be achieved through various means.  A machine learning system's decision function is shaped not only by its training data but also by its inherent inductive biases, which are often difficult to define precisely.  This work contributes to the growing trend of in-depth behavioral analyses of neural networks, moving beyond accuracy as the sole evaluation metric by examining error patterns.  We compare a suite of standard Convolutional Neural Networks (CNNs) with a recently proposed attention-based network, the Vision Transformer (ViT).  Unlike CNNs, ViTs relax the translation-invariance constraint, resulting in weaker inductive biases.  While attention-based networks have previously demonstrated superior accuracy on vision tasks, we show, using novel, more granular error consistency metrics, that their errors also exhibit greater consistency with human errors.  These findings have implications for both developing more human-like vision models and furthering our understanding of human visual object recognition.\n"}, "1902.01955": {"original_text": "  In conventional speech recognition, phoneme-based models outperform\ngrapheme-based models for non-phonetic languages such as English. The\nperformance gap between the two typically reduces as the amount of training\ndata is increased. In this work, we examine the impact of the choice of\nmodeling unit for attention-based encoder-decoder models. We conduct\nexperiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, using various\ntarget units (phoneme, grapheme, and word-piece); across all tasks, we find\nthat grapheme or word-piece models consistently outperform phoneme-based\nmodels, even though they are evaluated without a lexicon or an external\nlanguage model. We also investigate model complementarity: we find that we can\nimprove WERs by up to 9% relative by rescoring N-best lists generated from a\nstrong word-piece based baseline with either the phoneme or the grapheme model.\nRescoring an N-best list generated by the phonemic system, however, provides\nlimited improvements. Further analysis shows that the word-piece-based models\nproduce more diverse N-best hypotheses, and thus lower oracle WERs, than\nphonemic models.\n", "rewritten_text": "Conventional speech recognition typically employs phoneme-based models, which outperform grapheme-based models for non-phonetic languages like English.  This performance gap diminishes with increased training data.  This work investigates the impact of modeling unit choice in attention-based encoder-decoder models.  Experiments on the LibriSpeech 100hr, 460hr, and 960hr corpora, using phonemes, graphemes, and word-pieces as target units, consistently demonstrate that grapheme and word-piece models surpass phoneme-based models, even without lexicons or external language models.  Furthermore, we explored model complementarity.  Rescoring N-best lists from a strong word-piece baseline with either phoneme or grapheme models yielded up to a 9% relative Word Error Rate (WER) reduction.  However, rescoring N-best lists from a phoneme-based system offered limited improvement.  Analysis revealed that word-piece models generate more diverse N-best hypotheses, resulting in lower oracle WERs than phoneme models.\n"}, "2012.12482": {"original_text": "  We address the problem of crowd localization, i.e., the prediction of dots\ncorresponding to people in a crowded scene. Due to various challenges, a\nlocalization method is prone to spatial semantic errors, i.e., predicting\nmultiple dots within a same person or collapsing multiple dots in a cluttered\nregion. We propose a topological approach targeting these semantic errors. We\nintroduce a topological constraint that teaches the model to reason about the\nspatial arrangement of dots. To enforce this constraint, we define a\npersistence loss based on the theory of persistent homology. The loss compares\nthe topographic landscape of the likelihood map and the topology of the ground\ntruth. Topological reasoning improves the quality of the localization algorithm\nespecially near cluttered regions. On multiple public benchmarks, our method\noutperforms previous localization methods. Additionally, we demonstrate the\npotential of our method in improving the performance in the crowd counting\ntask.\n", "rewritten_text": "This paper addresses the problem of crowd localization\u2014predicting the locations of individuals (represented as dots) in crowded scenes.  Existing localization methods are susceptible to spatial semantic errors, such as assigning multiple dots to a single person or merging multiple individuals into a single dot in cluttered regions.  We propose a novel topological approach to mitigate these errors.  This approach introduces a topological constraint that encourages the model to reason about the spatial arrangement of detected dots.  We enforce this constraint using a persistence loss function derived from persistent homology theory. This loss compares the topological landscape of the predicted likelihood map with the topology of the ground truth.  Our topological reasoning significantly improves localization accuracy, particularly in cluttered areas.  Experiments on multiple public benchmarks demonstrate that our method outperforms state-of-the-art localization techniques.  Furthermore, we show that our approach improves performance on the related crowd counting task.\n"}, "2303.02982": {"original_text": "  Learning from large-scale contrastive language-image pre-training like CLIP\nhas shown remarkable success in a wide range of downstream tasks recently, but\nit is still under-explored on the challenging few-shot action recognition\n(FSAR) task. In this work, we aim to transfer the powerful multimodal knowledge\nof CLIP to alleviate the inaccurate prototype estimation issue due to data\nscarcity, which is a critical problem in low-shot regimes. To this end, we\npresent a CLIP-guided prototype modulating framework called CLIP-FSAR, which\nconsists of two key components: a video-text contrastive objective and a\nprototype modulation. Specifically, the former bridges the task discrepancy\nbetween CLIP and the few-shot video task by contrasting videos and\ncorresponding class text descriptions. The latter leverages the transferable\ntextual concepts from CLIP to adaptively refine visual prototypes with a\ntemporal Transformer. By this means, CLIP-FSAR can take full advantage of the\nrich semantic priors in CLIP to obtain reliable prototypes and achieve accurate\nfew-shot classification. Extensive experiments on five commonly used benchmarks\ndemonstrate the effectiveness of our proposed method, and CLIP-FSAR\nsignificantly outperforms existing state-of-the-art methods under various\nsettings. The source code and models will be publicly available at\nhttps://github.com/alibaba-mmai-research/CLIP-FSAR.\n", "rewritten_text": "Recent advancements in large-scale contrastive language-image pre-training, exemplified by CLIP, have yielded remarkable success across numerous downstream tasks.  However, its application to the challenging few-shot action recognition (FSAR) problem remains under-explored.  This work addresses this gap by leveraging CLIP's powerful multimodal knowledge to mitigate the inaccurate prototype estimation inherent in data-scarce, low-shot regimes.  We introduce CLIP-FSAR, a CLIP-guided prototype modulation framework comprising two key components: a video-text contrastive objective and a prototype modulation module.  The contrastive objective bridges the task discrepancy between CLIP and FSAR by contrasting videos with their corresponding class text descriptions.  The prototype modulation module leverages transferable textual concepts from CLIP to adaptively refine visual prototypes using a temporal Transformer.  This approach enables CLIP-FSAR to effectively utilize CLIP's rich semantic priors, resulting in reliable prototypes and improved few-shot classification accuracy.  Extensive experiments across five widely used benchmarks demonstrate the significant superiority of our method over existing state-of-the-art approaches under diverse settings.  The source code and pre-trained models are publicly available at https://github.com/alibaba-mmai-research/CLIP-FSAR.\n"}, "1905.07826": {"original_text": "  Multi-instance video object segmentation is to segment specific instances\nthroughout a video sequence in pixel level, given only an annotated first\nframe. In this paper, we implement an effective fully convolutional networks\nwith U-Net similar structure built on top of OSVOS fine-tuned layer. We use\ninstance isolation to transform this multi-instance segmentation problem into\nbinary labeling problem, and use weighted cross entropy loss and dice\ncoefficient loss as our loss function. Our best model achieves F mean of 0.467\nand J mean of 0.424 on DAVIS dataset, which is a comparable performance with\nthe State-of-the-Art approach. But case analysis shows this model can achieve a\nsmoother contour and better instance coverage, meaning it better for recall\nfocused segmentation scenario. We also did experiments on other convolutional\nneural networks, including Seg-Net, Mask R-CNN, and provide insightful\ncomparison and discussion.\n", "rewritten_text": "Multi-instance video object segmentation aims to segment specific instances at the pixel level throughout a video sequence, given only an annotated first frame.  This paper presents an effective fully convolutional network, architecturally similar to U-Net, built upon fine-tuned OSVOS layers.  We address the multi-instance segmentation problem by employing instance isolation to transform it into a binary labeling problem.  Our loss function combines weighted cross-entropy and Dice coefficient losses.  On the DAVIS dataset, our best model achieves a mean F-measure of 0.467 and a mean J-measure of 0.424, demonstrating comparable performance to state-of-the-art approaches.  However, case studies reveal that our model produces smoother contours and better instance coverage, making it particularly suitable for recall-focused segmentation tasks.  We further present comparative experiments and analysis using other convolutional neural networks, including SegNet and Mask R-CNN.\n"}, "2003.14282": {"original_text": "  A wide variety of transition-based algorithms are currently used for\ndependency parsers. Empirical studies have shown that performance varies across\ndifferent treebanks in such a way that one algorithm outperforms another on one\ntreebank and the reverse is true for a different treebank. There is often no\ndiscernible reason for what causes one algorithm to be more suitable for a\ncertain treebank and less so for another. In this paper we shed some light on\nthis by introducing the concept of an algorithm's inherent dependency\ndisplacement distribution. This characterises the bias of the algorithm in\nterms of dependency displacement, which quantify both distance and direction of\nsyntactic relations. We show that the similarity of an algorithm's inherent\ndistribution to a treebank's displacement distribution is clearly correlated to\nthe algorithm's parsing performance on that treebank, specifically with highly\nsignificant and substantial correlations for the predominant sentence lengths\nin Universal Dependency treebanks. We also obtain results which show a more\ndiscrete analysis of dependency displacement does not result in any meaningful\ncorrelations.\n", "rewritten_text": "Many dependency parsers utilize transition-based algorithms.  Empirical studies reveal that their performance varies significantly across different treebanks; an algorithm superior on one may be inferior on another, with no readily apparent reason. This paper addresses this variability by introducing the concept of an algorithm's inherent dependency displacement distribution. This distribution characterizes an algorithm's bias in terms of dependency displacement, quantifying both the distance and direction of syntactic relations.  We demonstrate a strong correlation between the similarity of an algorithm's inherent distribution and a treebank's displacement distribution, and the algorithm's parsing performance on that treebank. This correlation is highly significant and substantial, particularly for the prevalent sentence lengths in Universal Dependency treebanks. Conversely, a more granular analysis of dependency displacement yielded no meaningful correlations.\n"}, "2404.11682": {"original_text": "  Automated methods are becoming increasingly integrated into studies of\nformative feedback on students' science explanation writing. Most of this work,\nhowever, addresses students' responses to short answer questions. We\ninvestigate automated feedback on students' science explanation essays, where\nstudents must articulate multiple ideas. Feedback is based on a rubric that\nidentifies the main ideas students are prompted to include in explanatory\nessays about the physics of energy and mass, given their experiments with a\nsimulated roller coaster. We have found that students generally improve on\nrevised versions of their essays. Here, however, we focus on two factors that\naffect the accuracy of the automated feedback. First, we find that the main\nideas in the rubric differ with respect to how much freedom they afford in\nexplanations of the idea, thus explanation of a natural law is relatively\nconstrained. Students have more freedom in how they explain complex relations\nthey observe in their roller coasters, such as transfer of different forms of\nenergy. Second, by tracing the automated decision process, we can diagnose when\na student's statement lacks sufficient clarity for the automated tool to\nassociate it more strongly with one of the main ideas above all others. This in\nturn provides an opportunity for teachers and peers to help students reflect on\nhow to state their ideas more clearly.\n", "rewritten_text": "Automated methods are increasingly used to study formative feedback on students' science explanation writing.  However, most research focuses on short-answer responses.  This study investigates automated feedback on students' science explanation essays, requiring articulation of multiple ideas.  Feedback is rubric-based, identifying key concepts students should address in essays about the physics of energy and mass, informed by their simulated roller coaster experiments.  While students generally improved their essays after receiving feedback, this study focuses on two factors affecting automated feedback accuracy.  First, rubric criteria vary in the explanatory freedom they allow; for example, explaining a natural law is more constrained than explaining complex observed relationships, such as energy transfer, within the roller coaster. Second, analyzing the automated decision process reveals instances where a student's statement lacks clarity, hindering the system's ability to reliably associate it with a specific key concept. This allows teachers and peers to support students in clarifying their ideas.\n"}, "2211.08462": {"original_text": "  Recent years have seen an increasing trend in the volume of personal media\ncaptured by users, thanks to the advent of smartphones and smart glasses,\nresulting in large media collections. Despite conversation being an intuitive\nhuman-computer interface, current efforts focus mostly on single-shot natural\nlanguage based media retrieval to aid users query their media and re-live their\nmemories. This severely limits the search functionality as users can neither\nask follow-up queries nor obtain information without first formulating a\nsingle-turn query.\n  In this work, we propose dialogs for connected memories as a powerful tool to\nempower users to search their media collection through a multi-turn,\ninteractive conversation. Towards this, we collect a new task-oriented dialog\ndataset COMET, which contains $11.5k$ user<->assistant dialogs (totaling $103k$\nutterances), grounded in simulated personal memory graphs. We employ a\nresource-efficient, two-phase data collection pipeline that uses: (1) a novel\nmultimodal dialog simulator that generates synthetic dialog flows grounded in\nmemory graphs, and, (2) manual paraphrasing to obtain natural language\nutterances. We analyze COMET, formulate four main tasks to benchmark meaningful\nprogress, and adopt state-of-the-art language models as strong baselines, in\norder to highlight the multimodal challenges captured by our dataset.\n", "rewritten_text": "The proliferation of smartphones and smart glasses has led to a dramatic increase in the volume of personal media captured by users, resulting in massive media collections. While conversational interfaces are intuitive, current media retrieval methods primarily rely on single-turn natural language queries, severely limiting search functionality.  Users cannot ask follow-up questions or explore information without formulating a complete query in a single interaction.\n\nThis work proposes dialog-based access to connected memories as a powerful tool for navigating personal media collections through multi-turn, interactive conversations.  To this end, we introduce COMET, a new task-oriented dialog dataset comprising 11,500 user-assistant dialogs (103,000 utterances) grounded in simulated personal memory graphs.  Our resource-efficient, two-phase data collection pipeline utilizes: (1) a novel multimodal dialog simulator generating synthetic dialog flows grounded in memory graphs, and (2) manual paraphrasing to produce natural language utterances.  We analyze COMET, define four benchmark tasks to measure progress, and establish strong baselines using state-of-the-art language models to highlight the dataset's multimodal challenges.\n"}, "1911.03668": {"original_text": "  Natural Language Inference (NLI) aims to determine the logic relationships\n(i.e., entailment, neutral and contradiction) between a pair of premise and\nhypothesis. Recently, the alignment mechanism effectively helps NLI by\ncapturing the aligned parts (i.e., the similar segments) in the sentence pairs,\nwhich imply the perspective of entailment and contradiction. However, these\naligned parts will sometimes mislead the judgment of neutral relations.\nIntuitively, NLI should rely more on multiple perspectives to form a holistic\nview to eliminate bias. In this paper, we propose the Multi-Perspective\nInferrer (MPI), a novel NLI model that reasons relationships from multiple\nperspectives associated with the three relationships. The MPI determines the\nperspectives of different parts of the sentences via a routing-by-agreement\npolicy and makes the final decision from a holistic view. Additionally, we\nintroduce an auxiliary supervised signal to ensure the MPI to learn the\nexpected perspectives. Experiments on SNLI and MultiNLI show that 1) the MPI\nachieves substantial improvements on the base model, which verifies the\nmotivation of multi-perspective inference; 2) visualized evidence verifies that\nthe MPI learns highly interpretable perspectives as expected; 3) more\nimportantly, the MPI is architecture-free and compatible with the powerful\nBERT.\n", "rewritten_text": "Natural Language Inference (NLI) aims to determine the logical relationship (entailment, neutral, or contradiction) between a premise and a hypothesis.  Recent alignment mechanisms have improved NLI performance by identifying similar segments in sentence pairs, providing insights into entailment and contradiction. However, these aligned segments can sometimes misrepresent neutral relationships.  Intuitively, a more holistic approach, incorporating multiple perspectives, should mitigate this bias.\n\nThis paper introduces the Multi-Perspective Inferrer (MPI), a novel NLI model that reasons about relationships from multiple perspectives associated with each of the three relationship types.  MPI determines the perspective of different sentence parts using a routing-by-agreement policy and integrates these perspectives for a holistic judgment.  Furthermore, we introduce an auxiliary supervised signal to guide the learning of these perspectives.\n\nExperiments on SNLI and MultiNLI datasets demonstrate that: 1) MPI significantly improves upon a baseline model, validating the multi-perspective inference approach; 2) visualizations confirm that MPI learns highly interpretable perspectives; and 3) importantly, MPI is architecture-agnostic and compatible with powerful models like BERT.\n"}, "2403.04212": {"original_text": "  Providing emotional support through dialogue systems is becoming increasingly\nimportant in today's world, as it can support both mental health and social\ninteractions in many conversation scenarios. Previous works have shown that\nusing persona is effective for generating empathetic and supportive responses.\nThey have often relied on pre-provided persona rather than inferring them\nduring conversations. However, it is not always possible to obtain a user\npersona before the conversation begins. To address this challenge, we propose\nPESS (Persona Extraction through Semantic Similarity), a novel framework that\ncan automatically infer informative and consistent persona from dialogues. We\ndevise completeness loss and consistency loss based on semantic similarity\nscores. The completeness loss encourages the model to generate missing persona\ninformation, and the consistency loss guides the model to distinguish between\nconsistent and inconsistent persona. Our experimental results demonstrate that\nhigh-quality persona information inferred by PESS is effective in generating\nemotionally supportive responses.\n", "rewritten_text": "The provision of emotional support through dialogue systems is increasingly crucial in today's world, benefiting both mental health and social interactions across diverse conversational contexts.  Prior research has demonstrated the effectiveness of personas in generating empathetic and supportive responses, often relying on pre-defined personas rather than dynamic inference during conversation.  However, obtaining user personas beforehand is not always feasible.  To address this limitation, we propose PESS (Persona Extraction through Semantic Similarity), a novel framework for automatically inferring informative and consistent personas from dialogues.  We introduce completeness and consistency losses, based on semantic similarity scores, to guide the model.  The completeness loss encourages the generation of missing persona information, while the consistency loss ensures persona coherence.  Our experimental results demonstrate that PESS effectively infers high-quality persona information, leading to improved generation of emotionally supportive responses.\n"}, "1810.13391": {"original_text": "  During natural disasters and conflicts, information about what happened is\noften confusing, messy, and distributed across many sources. We would like to\nbe able to automatically identify relevant information and assemble it into\ncoherent narratives of what happened. To make this task accessible to neural\nmodels, we introduce Story Salads, mixtures of multiple documents that can be\ngenerated at scale. By exploiting the Wikipedia hierarchy, we can generate\nsalads that exhibit challenging inference problems. Story salads give rise to a\nnovel, challenging clustering task, where the objective is to group sentences\nfrom the same narratives. We demonstrate that simple bag-of-words similarity\nclustering falls short on this task and that it is necessary to take into\naccount global context and coherence.\n", "rewritten_text": "In the aftermath of natural disasters and conflicts, information is often fragmented, inconsistent, and scattered across numerous sources.  Our goal is to automatically identify and synthesize this information into coherent narratives of events. To facilitate this task for neural models, we introduce Story Salads: large-scale mixtures of multiple documents. Leveraging the Wikipedia hierarchy, we generate salads presenting challenging inference problems.  This yields a novel clustering task: grouping sentences from the same narrative. We demonstrate that simple bag-of-words similarity clustering is inadequate for this task, highlighting the necessity of considering global context and coherence.\n"}, "2407.02047": {"original_text": "  Multi-view counting (MVC) methods have shown their superiority over\nsingle-view counterparts, particularly in situations characterized by heavy\nocclusion and severe perspective distortions. However, hand-crafted heuristic\nfeatures and identical camera layout requirements in conventional MVC methods\nlimit their applicability and scalability in real-world scenarios.In this work,\nwe propose a concise 3D MVC framework called \\textbf{CountFormer}to elevate\nmulti-view image-level features to a scene-level volume representation and\nestimate the 3D density map based on the volume features. By incorporating a\ncamera encoding strategy, CountFormer successfully embeds camera parameters\ninto the volume query and image-level features, enabling it to handle various\ncamera layouts with significant differences.Furthermore, we introduce a feature\nlifting module capitalized on the attention mechanism to transform image-level\nfeatures into a 3D volume representation for each camera view. Subsequently,\nthe multi-view volume aggregation module attentively aggregates various\nmulti-view volumes to create a comprehensive scene-level volume representation,\nallowing CountFormer to handle images captured by arbitrary dynamic camera\nlayouts. The proposed method performs favorably against the state-of-the-art\napproaches across various widely used datasets, demonstrating its greater\nsuitability for real-world deployment compared to conventional MVC frameworks.\n", "rewritten_text": "Multi-view counting (MVC) methods surpass single-view methods, especially in heavily occluded scenes with significant perspective distortions.  However, conventional MVC methods rely on hand-crafted features and require identical camera layouts, limiting their real-world applicability and scalability.  This work introduces CountFormer, a concise 3D MVC framework that elevates multi-view image-level features to a scene-level volumetric representation to estimate a 3D density map.  CountFormer incorporates a camera encoding strategy, embedding camera parameters into both the volumetric query and image-level features, thus accommodating diverse camera layouts.  A novel feature lifting module, leveraging an attention mechanism, transforms image-level features into a 3D volumetric representation for each camera view.  A subsequent multi-view volume aggregation module attentively fuses these volumes into a comprehensive scene-level representation, enabling CountFormer to handle images from arbitrary, dynamic camera configurations.  Experiments on widely used datasets demonstrate that CountFormer outperforms state-of-the-art approaches, showcasing its superior suitability for real-world deployment compared to conventional MVC frameworks.\n"}, "2312.07530": {"original_text": "  Weakly supervised 3D object detection aims to learn a 3D detector with lower\nannotation cost, e.g., 2D labels. Unlike prior work which still relies on few\naccurate 3D annotations, we propose a framework to study how to leverage\nconstraints between 2D and 3D domains without requiring any 3D labels.\nSpecifically, we employ visual data from three perspectives to establish\nconnections between 2D and 3D domains. First, we design a feature-level\nconstraint to align LiDAR and image features based on object-aware regions.\nSecond, the output-level constraint is developed to enforce the overlap between\n2D and projected 3D box estimations. Finally, the training-level constraint is\nutilized by producing accurate and consistent 3D pseudo-labels that align with\nthe visual data. We conduct extensive experiments on the KITTI dataset to\nvalidate the effectiveness of the proposed three constraints. Without using any\n3D labels, our method achieves favorable performance against state-of-the-art\napproaches and is competitive with the method that uses 500-frame 3D\nannotations. Code will be made publicly available at\nhttps://github.com/kuanchihhuang/VG-W3D.\n", "rewritten_text": "Weakly supervised 3D object detection aims to reduce annotation costs by learning 3D detectors from less expensive 2D labels.  Unlike previous methods that still require a few accurate 3D annotations, we propose a novel framework that leverages constraints between 2D and 3D domains without any 3D labels.  Our approach utilizes visual data from three perspectives to establish these connections.  First, a feature-level constraint aligns LiDAR and image features based on object-aware regions. Second, an output-level constraint enforces overlap between 2D and projected 3D bounding box estimations. Finally, a training-level constraint generates accurate and consistent 3D pseudo-labels aligned with the visual data.  Extensive experiments on the KITTI dataset demonstrate the effectiveness of our proposed three-constraint framework.  Even without 3D labels, our method achieves performance comparable to state-of-the-art approaches and is competitive with methods using 500 frames of 3D annotations.  Code is publicly available at https://github.com/kuanchihhuang/VG-W3D.\n"}, "2404.09976": {"original_text": "  Recently, diffusion transformers have gained wide attention with its\nexcellent performance in text-to-image and text-to-vidoe models, emphasizing\nthe need for transformers as backbone for diffusion models. Transformer-based\nmodels have shown better generalization capability compared to CNN-based models\nfor general vision tasks. However, much less has been explored in the existing\nliterature regarding the capabilities of transformer-based diffusion backbones\nand expanding their generative prowess to other datasets. This paper focuses on\nenabling a single pre-trained diffusion transformer model to scale across\nmultiple datasets swiftly, allowing for the completion of diverse generative\ntasks using just one model. To this end, we propose DiffScaler, an efficient\nscaling strategy for diffusion models where we train a minimal amount of\nparameters to adapt to different tasks. In particular, we learn task-specific\ntransformations at each layer by incorporating the ability to utilize the\nlearned subspaces of the pre-trained model, as well as the ability to learn\nadditional task-specific subspaces, which may be absent in the pre-training\ndataset. As these parameters are independent, a single diffusion model with\nthese task-specific parameters can be used to perform multiple tasks\nsimultaneously. Moreover, we find that transformer-based diffusion models\nsignificantly outperform CNN-based diffusion models methods while performing\nfine-tuning over smaller datasets. We perform experiments on four unconditional\nimage generation datasets. We show that using our proposed method, a single\npre-trained model can scale up to perform these conditional and unconditional\ntasks, respectively, with minimal parameter tuning while performing as close as\nfine-tuning an entire diffusion model for that particular task.\n", "rewritten_text": "Diffusion transformers have recently garnered significant attention due to their excellent performance in text-to-image and text-to-video generation, highlighting the potential of transformers as the backbone for diffusion models.  While transformer-based models have demonstrated superior generalization capabilities compared to CNN-based models in general vision tasks, the exploration of transformer-based diffusion backbones and their application to diverse datasets remains limited.  This paper introduces DiffScaler, an efficient scaling strategy for diffusion models that enables a single pre-trained diffusion transformer to adapt swiftly to multiple datasets and perform diverse generative tasks.  DiffScaler minimizes the number of trainable parameters by learning task-specific transformations at each layer.  This involves leveraging the pre-trained model's learned subspaces and learning additional task-specific subspaces not present in the pre-training data.  The independence of these parameters allows a single model to perform multiple tasks concurrently.  Our experiments on four unconditional image generation datasets demonstrate that transformer-based diffusion models significantly outperform CNN-based counterparts during fine-tuning on smaller datasets.  Furthermore, DiffScaler enables a single pre-trained model to effectively handle both conditional and unconditional tasks with minimal parameter tuning, achieving performance comparable to fine-tuning an entire model for each specific task.\n"}, "2406.06843": {"original_text": "  We introduce a data capture system and a new dataset named HO-Cap that can be\nused to study 3D reconstruction and pose tracking of hands and objects in\nvideos. The capture system uses multiple RGB-D cameras and a HoloLens headset\nfor data collection, avoiding the use of expensive 3D scanners or mocap\nsystems. We propose a semi-automatic method to obtain annotations of shape and\npose of hands and objects in the collected videos, which significantly reduces\nthe required annotation time compared to manual labeling. With this system, we\ncaptured a video dataset of humans using objects to perform different tasks, as\nwell as simple pick-and-place and handover of an object from one hand to the\nother, which can be used as human demonstrations for embodied AI and robot\nmanipulation research. Our data capture setup and annotation framework can be\nused by the community to reconstruct 3D shapes of objects and human hands and\ntrack their poses in videos.\n", "rewritten_text": "We present HO-Cap, a novel data capture system and dataset for studying 3D hand and object reconstruction and pose tracking in video.  The system leverages multiple RGB-D cameras and a HoloLens headset, eliminating the need for costly 3D scanners or motion capture systems.  A semi-automatic annotation method significantly reduces labeling time compared to manual annotation, providing accurate shape and pose information for hands and objects.  HO-Cap includes videos of humans performing various tasks, including pick-and-place and handovers, offering valuable human demonstrations for embodied AI and robotic manipulation research.  This readily accessible data capture setup and annotation framework enables researchers to reconstruct 3D shapes and track poses of hands and objects in video.\n"}, "2407.05769": {"original_text": "  In autonomous driving, LiDAR sensors are vital for acquiring 3D point clouds,\nproviding reliable geometric information. However, traditional sampling methods\nof preprocessing often ignore semantic features, leading to detail loss and\nground point interference in 3D object detection. To address this, we propose a\nmulti-branch two-stage 3D object detection framework using a Semantic-aware\nMulti-branch Sampling (SMS) module and multi-view consistency constraints. The\nSMS module includes random sampling, Density Equalization Sampling (DES) for\nenhancing distant objects, and Ground Abandonment Sampling (GAS) to focus on\nnon-ground points. The sampled multi-view points are processed through a\nConsistent KeyPoint Selection (CKPS) module to generate consistent keypoint\nmasks for efficient proposal sampling. The first-stage detector uses\nmulti-branch parallel learning with multi-view consistency loss for feature\naggregation, while the second-stage detector fuses multi-view data through a\nMulti-View Fusion Pooling (MVFP) module to precisely predict 3D objects. The\nexperimental results on the KITTI dataset and Waymo Open Dataset show that our\nmethod achieves excellent detection performance improvement for a variety of\nbackbones, especially for low-performance backbones with the simple network\nstructures.\n", "rewritten_text": "LiDAR sensors are crucial for autonomous driving, providing reliable geometric information via 3D point cloud acquisition.  However, traditional preprocessing methods often neglect semantic features, resulting in detail loss and ground point interference that hinder 3D object detection.  To mitigate this, we propose a two-stage, multi-branch 3D object detection framework incorporating a Semantic-aware Multi-branch Sampling (SMS) module and multi-view consistency constraints.  The SMS module employs three sampling strategies: random sampling, Density Equalization Sampling (DES) to enhance distant object representation, and Ground Abandonment Sampling (GAS) to focus on non-ground points.  A Consistent KeyPoint Selection (CKPS) module then processes the sampled multi-view points, generating consistent keypoint masks for efficient proposal generation.  The first stage utilizes multi-branch parallel learning with a multi-view consistency loss for feature aggregation.  The second stage employs a Multi-View Fusion Pooling (MVFP) module to fuse multi-view data for precise 3D object prediction.  Experiments on the KITTI and Waymo Open Datasets demonstrate significant performance improvements across various backbones, particularly those with simpler architectures and lower performance.\n"}, "2211.0089": {"original_text": "  Few-shot learning problem focuses on recognizing unseen classes given a few\nlabeled images. In recent effort, more attention is paid to fine-grained\nfeature embedding, ignoring the relationship among different distance metrics.\nIn this paper, for the first time, we investigate the contributions of\ndifferent distance metrics, and propose an adaptive fusion scheme, bringing\nsignificant improvements in few-shot classification. We start from a naive\nbaseline of confidence summation and demonstrate the necessity of exploiting\nthe complementary property of different distance metrics. By finding the\ncompetition problem among them, built upon the baseline, we propose an Adaptive\nMetrics Module (AMM) to decouple metrics fusion into metric-prediction fusion\nand metric-losses fusion. The former encourages mutual complementary, while the\nlatter alleviates metric competition via multi-task collaborative learning.\nBased on AMM, we design a few-shot classification framework AMTNet, including\nthe AMM and the Global Adaptive Loss (GAL), to jointly optimize the few-shot\ntask and auxiliary self-supervised task, making the embedding features more\nrobust. In the experiment, the proposed AMM achieves 2% higher performance than\nthe naive metrics fusion module, and our AMTNet outperforms the\nstate-of-the-arts on multiple benchmark datasets.\n", "rewritten_text": "Few-shot learning addresses the challenge of recognizing unseen classes from limited labeled examples.  Recent work has emphasized fine-grained feature embedding, often neglecting the relationships between different distance metrics. This paper, for the first time, investigates the contributions of various distance metrics and proposes an adaptive fusion scheme, significantly improving few-shot classification performance.  We begin with a naive confidence summation baseline, demonstrating the need to leverage the complementary properties of different distance metrics.  Addressing the competitive nature of these metrics, we introduce an Adaptive Metrics Module (AMM) that decouples metric fusion into metric-prediction fusion and metric-loss fusion.  The former promotes mutual complementarity, while the latter mitigates metric competition through multi-task collaborative learning.  Building upon the AMM, we design a few-shot classification framework, AMTNet, incorporating the AMM and a Global Adaptive Loss (GAL) to jointly optimize the few-shot task and an auxiliary self-supervised task, resulting in more robust feature embeddings.  Experiments show that the proposed AMM achieves a 2% performance improvement over a naive metric fusion module, and our AMTNet surpasses state-of-the-art results on multiple benchmark datasets.\n"}, "2407.13520": {"original_text": "  3D deblurring reconstruction techniques have recently seen significant\nadvancements with the development of Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). Although these techniques can recover relatively\nclear 3D reconstructions from blurry image inputs, they still face limitations\nin handling severe blurring and complex camera motion. To address these issues,\nwe propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting\n(EaDeblur-GS), which integrates event camera data to enhance the robustness of\n3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE)\nnetwork to estimate Gaussian center deviations and using novel loss functions,\nEaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating\nperformance comparable to state-of-the-art methods.\n", "rewritten_text": "Recent advancements in 3D deblurring reconstruction techniques, particularly Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have yielded significant improvements.  However, these methods remain limited in their ability to handle severely blurred images and complex camera motion.  To address these limitations, we propose Event-assisted 3D Deblurring Reconstruction with Gaussian Splatting (EaDeblur-GS).  This method integrates event camera data to enhance the robustness of 3DGS against motion blur.  By incorporating an Adaptive Deviation Estimator (ADE) network to estimate Gaussian center deviations and employing novel loss functions, EaDeblur-GS achieves real-time, high-fidelity 3D reconstructions comparable to state-of-the-art methods.\n"}, "2103.13678": {"original_text": "  Domain Adaptation is widely used in practical applications of neural machine\ntranslation, which aims to achieve good performance on both the general-domain\nand in-domain. However, the existing methods for domain adaptation usually\nsuffer from catastrophic forgetting, domain divergence, and model explosion. To\naddress these three problems, we propose a method of \"divide and conquer\" which\nis based on the importance of neurons or parameters in the translation model.\nIn our method, we first prune the model and only keep the important neurons or\nparameters, making them responsible for both general-domain and in-domain\ntranslation. Then we further train the pruned model supervised by the original\nunpruned model with the knowledge distillation method. Last we expand the model\nto the original size and fine-tune the added parameters for the in-domain\ntranslation. We conduct experiments on different languages and domains and the\nresults show that our method can achieve significant improvements compared with\nseveral strong baselines.\n", "rewritten_text": "Neural machine translation (NMT) frequently employs domain adaptation to achieve strong performance across both general and specific domains.  However, existing domain adaptation methods often suffer from catastrophic forgetting, domain divergence, and model size explosion.  To address these challenges, we propose a \"divide and conquer\" approach leveraging the importance of neurons and parameters within the translation model.  Our method first prunes the model, retaining only the most important neurons or parameters to handle both general and specific domain translation.  This pruned model is then further trained using knowledge distillation, supervised by the original, unpruned model. Finally, the model is expanded to its original size, and the newly added parameters are fine-tuned specifically for in-domain translation.  Experiments across multiple languages and domains demonstrate that our method achieves significant improvements over several strong baselines.\n"}, "2003.11536": {"original_text": "  Estimating the actual head orientation from 2D images, with regard to its\nthree degrees of freedom, is a well known problem that is highly significant\nfor a large number of applications involving head pose knowledge. Consequently,\nthis topic has been tackled by a plethora of methods and algorithms the most\npart of which exploits neural networks. Machine learning methods, indeed,\nachieve accurate head rotation values yet require an adequate training stage\nand, to that aim, a relevant number of positive and negative examples. In this\npaper we take a different approach to this topic by using fractal coding theory\nand particularly Partitioned Iterated Function Systems to extract the fractal\ncode from the input head image and to compare this representation to the\nfractal code of a reference model through Hamming distance. According to\nexperiments conducted on both the BIWI and the AFLW2000 databases, the proposed\nPIFS based head pose estimation method provides accurate yaw/pitch/roll angular\nvalues, with a performance approaching that of state of the art of\nmachine-learning based algorithms and exceeding most of non-training based\napproaches.\n", "rewritten_text": "Estimating head orientation from 2D images, considering its three degrees of freedom (yaw, pitch, and roll), is a challenging problem crucial for numerous applications requiring head pose information.  Numerous methods and algorithms have addressed this, with many leveraging neural networks. While machine learning approaches achieve accurate head rotation estimates, they necessitate extensive training data.  This paper proposes a novel approach using fractal coding theory, specifically Partitioned Iterated Function Systems (PIFS).  We extract a fractal code from the input head image and compare it to a reference model's fractal code using Hamming distance.  Experiments on the BIWI and AFLW2000 databases demonstrate that our PIFS-based head pose estimation method yields accurate yaw, pitch, and roll angles, achieving performance comparable to state-of-the-art machine learning algorithms and surpassing most non-training-based methods.\n"}, "1706.02241": {"original_text": "  Analogy completion has been a popular task in recent years for evaluating the\nsemantic properties of word embeddings, but the standard methodology makes a\nnumber of assumptions about analogies that do not always hold, either in recent\nbenchmark datasets or when expanding into other domains. Through an analysis of\nanalogies in the biomedical domain, we identify three assumptions: that of a\nSingle Answer for any given analogy, that the pairs involved describe the Same\nRelationship, and that each pair is Informative with respect to the other. We\npropose modifying the standard methodology to relax these assumptions by\nallowing for multiple correct answers, reporting MAP and MRR in addition to\naccuracy, and using multiple example pairs. We further present BMASS, a novel\ndataset for evaluating linguistic regularities in biomedical embeddings, and\ndemonstrate that the relationships described in the dataset pose significant\nsemantic challenges to current word embedding methods.\n", "rewritten_text": "Recent years have seen analogy completion emerge as a popular task for evaluating the semantic properties of word embeddings. However, the standard methodology rests on several assumptions about analogies that often fail to hold, both in current benchmark datasets and when applied to other domains.  Analyzing biomedical analogies reveals three key problematic assumptions:  the existence of a single correct answer, the assumption that analogy pairs share an identical relationship, and the assumption that each pair is informative relative to the other.  We propose a modified methodology that relaxes these assumptions by permitting multiple correct answers, reporting mean average precision (MAP) and mean reciprocal rank (MRR) alongside accuracy, and utilizing multiple example pairs.  Furthermore, we introduce BMASS, a novel dataset for evaluating linguistic regularities in biomedical embeddings, and demonstrate that the relationships within this dataset present significant semantic challenges for existing word embedding methods.\n"}, "2312.16894": {"original_text": "  The widespread usage of cars and other large, heavy vehicles necessitates the\ndevelopment of an effective parking infrastructure. Additionally, algorithms\nfor detection and recognition of number plates are often used to identify\nautomobiles all around the world where standardized plate sizes and fonts are\nenforced, making recognition an effortless task. As a result, both kinds of\ndata can be combined to develop an intelligent parking system focuses on the\ntechnology of Automatic Number Plate Recognition (ANPR). Retrieving characters\nfrom an inputted number plate image is the sole purpose of ANPR which is a\ncostly procedure. In this article, we propose Chaurah, a minimal cost ANPR\nsystem that relies on a Raspberry Pi 3 that was specifically created for\nparking facilities. The system employs a dual-stage methodology, with the first\nstage being an ANPR system which makes use of two convolutional neural networks\n(CNNs). The primary locates and recognises license plates from a vehicle image,\nwhile the secondary performs Optical Character Recognition (OCR) to identify\nindividualized numbers from the number plate. An application built with Flutter\nand Firebase for database administration and license plate record comparison\nmakes up the second component of the overall solution. The application also\nacts as an user-interface for the billing mechanism based on parking time\nduration resulting in an all-encompassing software deployment of the study.\n", "rewritten_text": "The proliferation of cars and other large vehicles necessitates robust parking infrastructure.  Efficient parking management is further aided by Automatic Number Plate Recognition (ANPR) systems, widely used globally where standardized plate formats simplify character recognition.  However, ANPR can be costly.  This article presents Chaurah, a low-cost ANPR system for parking facilities, built using a Raspberry Pi 3.\n\nChaurah employs a two-stage approach.  The first stage utilizes a dual convolutional neural network (CNN) system: one CNN locates and recognizes license plates within an image, and a second performs Optical Character Recognition (OCR) to identify individual characters.  The second stage comprises a Flutter and Firebase-based application for database management, license plate record comparison, and user interface. This application also manages billing based on parking duration, providing a complete software solution.\n"}, "2110.13032": {"original_text": "  Though there has been a large body of recent works in language modeling (LM)\nfor high resource languages such as English and Chinese, the area is still\nunexplored for low resource languages like Bengali and Hindi. We propose an end\nto end trainable memory efficient CNN architecture named CoCNN to handle\nspecific characteristics such as high inflection, morphological richness,\nflexible word order and phonetical spelling errors of Bengali and Hindi. In\nparticular, we introduce two learnable convolutional sub-models at word and at\nsentence level that are end to end trainable. We show that state-of-the-art\n(SOTA) Transformer models including pretrained BERT do not necessarily yield\nthe best performance for Bengali and Hindi. CoCNN outperforms pretrained BERT\nwith 16X less parameters, and it achieves much better performance than SOTA\nLSTM models on multiple real-world datasets. This is the first study on the\neffectiveness of different architectures drawn from three deep learning\nparadigms - Convolution, Recurrent, and Transformer neural nets for modeling\ntwo widely used languages, Bengali and Hindi.\n", "rewritten_text": "While substantial recent work in language modeling (LM) has focused on high-resource languages like English and Chinese, low-resource languages such as Bengali and Hindi remain relatively unexplored.  This paper proposes CoCNN, a memory-efficient, end-to-end trainable convolutional neural network (CNN) architecture designed to address the specific linguistic characteristics of Bengali and Hindi, including high inflection, morphological richness, flexible word order, and phonetic spelling errors.  CoCNN incorporates two learnable convolutional sub-models operating at the word and sentence levels.  Our results demonstrate that state-of-the-art (SOTA) Transformer models, including pre-trained BERT, do not consistently achieve optimal performance for Bengali and Hindi.  CoCNN outperforms pre-trained BERT with 16 times fewer parameters and significantly surpasses SOTA LSTM models on multiple real-world datasets. This study represents the first comprehensive comparison of convolutional, recurrent, and transformer neural network architectures for language modeling in Bengali and Hindi.\n"}, "1305.4537": {"original_text": "  We describe a method for visual object detection based on an ensemble of\noptimized decision trees organized in a cascade of rejectors. The trees use\npixel intensity comparisons in their internal nodes and this makes them able to\nprocess image regions very fast. Experimental analysis is provided through a\nface detection problem. The obtained results are encouraging and demonstrate\nthat the method has practical value. Additionally, we analyse its sensitivity\nto noise and show how to perform fast rotation invariant object detection.\nComplete source code is provided at https://github.com/nenadmarkus/pico.\n", "rewritten_text": "This paper presents a novel visual object detection method employing a cascade of optimized decision tree ensembles.  Each tree utilizes pixel intensity comparisons at its nodes, enabling rapid processing of image regions.  We evaluate this method on a face detection task, achieving encouraging results that demonstrate its practical applicability.  Furthermore, we analyze its noise sensitivity and detail a technique for fast, rotation-invariant object detection.  The complete source code is available at https://github.com/nenadmarkus/pico.\n"}, "2311.18835": {"original_text": "  Empowering models to dynamically accomplish tasks specified through natural\nlanguage instructions represents a promising path toward more capable and\ngeneral artificial intelligence. In this work, we introduce InstructSeq, an\ninstruction-conditioned multi-modal modeling framework that unifies diverse\nvision tasks through flexible natural language control and handling of both\nvisual and textual data. InstructSeq employs a multimodal transformer\narchitecture encompassing visual, language, and sequential modeling. We utilize\na visual encoder to extract image features and a text encoder to encode\ninstructions. An autoregressive transformer fuses the representations and\ngenerates sequential task outputs. By training with LLM-generated natural\nlanguage instructions, InstructSeq acquires a strong comprehension of free-form\ninstructions for specifying visual tasks. This provides an intuitive interface\nfor directing capabilities using flexible natural instructions. Without any\ntask-specific tuning, InstructSeq achieves compelling performance on semantic\nsegmentation, referring expression segmentation/comprehension, and image\ncaptioning. The flexible control and multi-task unification empower the model\nwith more human-like versatility and generalizability for computer vision. The\ncode will be released soon at https://github.com/rongyaofang/InstructSeq.\n", "rewritten_text": "Instruction-following models represent a promising path toward more capable and general artificial intelligence.  This work introduces InstructSeq, a novel instruction-conditioned, multimodal modeling framework that unifies diverse vision tasks through flexible natural language control and the integrated processing of visual and textual data.  InstructSeq employs a multimodal transformer architecture incorporating visual, language, and sequential modeling components.  Image features are extracted using a visual encoder, while natural language instructions are encoded using a text encoder. An autoregressive transformer then fuses these representations to generate sequential task outputs.  Training with large language model (LLM)-generated instructions enables InstructSeq to robustly understand free-form specifications for visual tasks, providing an intuitive interface for directing its capabilities.  Without task-specific fine-tuning, InstructSeq achieves strong performance on semantic segmentation, referring expression segmentation and comprehension, and image captioning. This flexible control and multi-task unification endow the model with human-like versatility and generalizability in computer vision. The code will be released soon at [https://github.com/rongyaofang/InstructSeq](https://github.com/rongyaofang/InstructSeq).\n"}, "2008.07018": {"original_text": "  We present AutoPose, a novel neural architecture search(NAS) framework that\nis capable of automatically discovering multiple parallel branches of\ncross-scale connections towards accurate and high-resolution 2D human pose\nestimation. Recently, high-performance hand-crafted convolutional networks for\npose estimation show growing demands on multi-scale fusion and high-resolution\nrepresentations. However, current NAS works exhibit limited flexibility on\nscale searching, they dominantly adopt simplified search spaces of\nsingle-branch architectures. Such simplification limits the fusion of\ninformation at different scales and fails to maintain high-resolution\nrepresentations. The presentedAutoPose framework is able to search for\nmulti-branch scales and network depth, in addition to the cell-level\nmicrostructure. Motivated by the search space, a novel bi-level optimization\nmethod is presented, where the network-level architecture is searched via\nreinforcement learning, and the cell-level search is conducted by the\ngradient-based method. Within 2.5 GPU days, AutoPose is able to find very\ncompetitive architectures on the MS COCO dataset, that are also transferable to\nthe MPII dataset. Our code is available at\nhttps://github.com/VITA-Group/AutoPose.\n", "rewritten_text": "We introduce AutoPose, a novel neural architecture search (NAS) framework that automatically discovers multiple parallel branches of cross-scale connections for accurate, high-resolution 2D human pose estimation.  High-performance, hand-crafted convolutional networks for pose estimation increasingly demand multi-scale fusion and high-resolution representations.  However, current NAS methods exhibit limited flexibility in scale searching, predominantly employing simplified search spaces of single-branch architectures. This simplification restricts information fusion across scales and hinders the maintenance of high-resolution representations.  AutoPose searches for multi-branch scales and network depth, in addition to cell-level microstructure.  To facilitate this search space, we present a novel bi-level optimization method: network-level architecture is searched via reinforcement learning, while cell-level search is conducted using a gradient-based method.  Within 2.5 GPU days, AutoPose discovers highly competitive architectures on the MS COCO dataset, which also demonstrate transferability to the MPII dataset.  Our code is available at https://github.com/VITA-Group/AutoPose.\n"}, "1902.053": {"original_text": "  Deep learning, due to its unprecedented success in tasks such as image\nclassification, has emerged as a new tool in image reconstruction with\npotential to change the field. In this paper we demonstrate a crucial\nphenomenon: deep learning typically yields unstablemethods for image\nreconstruction. The instabilities usually occur in several forms: (1) tiny,\nalmost undetectable perturbations, both in the image and sampling domain, may\nresult in severe artefacts in the reconstruction, (2) a small structural\nchange, for example a tumour, may not be captured in the reconstructed image\nand (3) (a counterintuitive type of instability) more samples may yield poorer\nperformance. Our new stability test with algorithms and easy to use software\ndetects the instability phenomena. The test is aimed at researchers to test\ntheir networks for instabilities and for government agencies, such as the Food\nand Drug Administration (FDA), to secure safe use of deep learning methods.\n", "rewritten_text": "Deep learning's unprecedented success in tasks like image classification has established it as a transformative tool in image reconstruction.  However, this paper demonstrates a critical phenomenon: deep learning methods for image reconstruction are often unstable.  These instabilities manifest in several ways: (1) minute, nearly imperceptible perturbations in either the image or sampling domain can produce significant artifacts in the reconstruction; (2) small structural changes, such as a tumor, may be missed; and (3) counterintuitively, increasing the number of samples can sometimes degrade performance.  Our novel stability test, implemented in user-friendly software, detects these instabilities.  This test is intended for researchers to evaluate the robustness of their networks and for regulatory agencies, such as the Food and Drug Administration (FDA), to ensure the safe application of deep learning methods.\n"}, "2203.08543": {"original_text": "  Deep Metric Learning (DML) proposes to learn metric spaces which encode\nsemantic similarities as embedding space distances. These spaces should be\ntransferable to classes beyond those seen during training. Commonly, DML\nmethods task networks to solve contrastive ranking tasks defined over binary\nclass assignments. However, such approaches ignore higher-level semantic\nrelations between the actual classes. This causes learned embedding spaces to\nencode incomplete semantic context and misrepresent the semantic relation\nbetween classes, impacting the generalizability of the learned metric space. To\ntackle this issue, we propose a language guidance objective for visual\nsimilarity learning. Leveraging language embeddings of expert- and\npseudo-classnames, we contextualize and realign visual representation spaces\ncorresponding to meaningful language semantics for better semantic consistency.\nExtensive experiments and ablations provide a strong motivation for our\nproposed approach and show language guidance offering significant,\nmodel-agnostic improvements for DML, achieving competitive and state-of-the-art\nresults on all benchmarks. Code available at\nhttps://github.com/ExplainableML/LanguageGuidance_for_DML.\n", "rewritten_text": "Deep metric learning (DML) aims to learn metric spaces that encode semantic similarities as distances in an embedding space.  These spaces should generalize to classes unseen during training.  Common DML methods train networks to solve contrastive ranking tasks based on binary class assignments.  However, these approaches neglect higher-order semantic relationships between classes, resulting in incomplete semantic context within the learned embedding space and misrepresentation of inter-class relationships. This ultimately limits the generalizability of the learned metric.  To address this, we propose a language guidance objective for visual similarity learning.  By leveraging language embeddings of expert- and pseudo-class names, we contextualize and realign visual representation spaces to align with meaningful linguistic semantics, thereby improving semantic consistency.  Extensive experiments and ablation studies strongly support our approach, demonstrating significant, model-agnostic improvements to DML and achieving competitive, state-of-the-art results across all benchmarks. Code is available at https://github.com/ExplainableML/LanguageGuidance_for_DML.\n"}, "1411.5057": {"original_text": "  In this paper, we propose a novel algorithm for analysis-based sparsity\nreconstruction. It can solve the generalized problem by structured sparsity\nregularization with an orthogonal basis and total variation regularization. The\nproposed algorithm is based on the iterative reweighted least squares (IRLS)\nmodel, which is further accelerated by the preconditioned conjugate gradient\nmethod. The convergence rate of the proposed algorithm is almost the same as\nthat of the traditional IRLS algorithms, that is, exponentially fast. Moreover,\nwith the specifically devised preconditioner, the computational cost for each\niteration is significantly less than that of traditional IRLS algorithms, which\nenables our approach to handle large scale problems. In addition to the fast\nconvergence, it is straightforward to apply our method to standard sparsity,\ngroup sparsity, overlapping group sparsity and TV based problems. Experiments\nare conducted on a practical application: compressive sensing magnetic\nresonance imaging. Extensive results demonstrate that the proposed algorithm\nachieves superior performance over 14 state-of-the-art algorithms in terms of\nboth accuracy and computational cost.\n", "rewritten_text": "This paper presents a novel algorithm for analysis-based sparse reconstruction.  The algorithm solves the generalized problem using structured sparsity regularization with an orthogonal basis and total variation regularization.  Based on an iterative reweighted least squares (IRLS) model accelerated by the preconditioned conjugate gradient method, it exhibits exponentially fast convergence, comparable to traditional IRLS algorithms.  A specifically designed preconditioner significantly reduces the computational cost per iteration, enabling efficient handling of large-scale problems.  Furthermore, the algorithm readily extends to standard sparsity, group sparsity, overlapping group sparsity, and total variation (TV)-based problems.  Experiments on compressive sensing magnetic resonance imaging (CS-MRI) demonstrate superior performance over 14 state-of-the-art algorithms in terms of both accuracy and computational efficiency.\n"}, "2211.09809": {"original_text": "  Animating portraits using speech has received growing attention in recent\nyears, with various creative and practical use cases. An ideal generated video\nshould have good lip sync with the audio, natural facial expressions and head\nmotions, and high frame quality. In this work, we present SPACE, which uses\nspeech and a single image to generate high-resolution, and expressive videos\nwith realistic head pose, without requiring a driving video. It uses a\nmulti-stage approach, combining the controllability of facial landmarks with\nthe high-quality synthesis power of a pretrained face generator. SPACE also\nallows for the control of emotions and their intensities. Our method\noutperforms prior methods in objective metrics for image quality and facial\nmotions and is strongly preferred by users in pair-wise comparisons. The\nproject website is available at https://deepimagination.cc/SPACE/\n", "rewritten_text": "Recent years have witnessed a surge in interest in animating portrait images using speech, driven by diverse creative and practical applications.  An ideal generated video should exhibit accurate lip synchronization, natural facial expressions and head movements, and high-resolution visuals.  This work introduces SPACE, a novel method that generates high-resolution, expressive videos from a single image and speech audio, realistically rendering head pose without requiring a driving video.  Employing a multi-stage approach, SPACE combines the precise control afforded by facial landmarks with the high-quality synthesis capabilities of a pre-trained face generator.  Furthermore, SPACE enables control over emotions and their intensity.  Our method surpasses existing approaches in objective metrics for image quality and facial motion, and user preference testing demonstrates a strong bias towards SPACE in pairwise comparisons.  The project website is available at https://deepimagination.cc/SPACE/.\n"}, "2307.03602": {"original_text": "  Stereo vision systems have become popular in computer vision applications,\nsuch as 3D reconstruction, object tracking, and autonomous navigation. However,\ntraditional stereo vision systems that use rectilinear lenses may not be\nsuitable for certain scenarios due to their limited field of view. This has led\nto the popularity of vision systems based on one or multiple fisheye cameras in\ndifferent orientations, which can provide a field of view of 180x180 degrees or\nmore. However, fisheye cameras introduce significant distortion at the edges\nthat affects the accuracy of stereo matching and depth estimation. To overcome\nthese limitations, this paper proposes a method for distortion-removal and\ndepth estimation analysis for stereovision system using orthogonally divergent\nfisheye cameras (ODFC). The proposed method uses two virtual pinhole cameras\n(VPC), each VPC captures a small portion of the original view and presents it\nwithout any lens distortions, emulating the behavior of a pinhole camera. By\ncarefully selecting the captured regions, it is possible to create a stereo\npair using two VPCs. The performance of the proposed method is evaluated in\nboth simulation using virtual environment and experiments using real cameras\nand their results compared to stereo cameras with parallel optical axes. The\nresults demonstrate the effectiveness of the proposed method in terms of\ndistortion removal and depth estimation accuracy.\n", "rewritten_text": "Stereo vision systems, widely used in computer vision applications such as 3D reconstruction, object tracking, and autonomous navigation, often employ rectilinear lenses.  However, the limited field of view of these systems restricts their applicability in certain scenarios.  Consequently, vision systems utilizing one or more fisheye cameras, offering fields of view exceeding 180\u00b0 \u00d7 180\u00b0, have gained popularity.  Nevertheless, the significant peripheral distortion inherent in fisheye lenses compromises the accuracy of stereo matching and depth estimation.  This paper presents a novel method for distortion removal and depth estimation in stereo vision systems employing orthogonally divergent fisheye cameras (ODFCs).  The proposed approach utilizes two virtual pinhole cameras (VPCs), each capturing a small, distortion-free portion of the original fisheye image, effectively emulating pinhole camera behavior.  By strategically selecting these captured regions, a stereo pair is generated.  The method's performance is evaluated through both simulated and real-world experiments, comparing results to those obtained using stereo cameras with parallel optical axes.  The results demonstrate the effectiveness of the proposed method in achieving accurate distortion removal and depth estimation.\n"}, "2410.11816": {"original_text": "  The automatic assembly problem has attracted increasing interest due to its\ncomplex challenges that involve 3D representation. This paper introduces\nJigsaw++, a novel generative method designed to tackle the multifaceted\nchallenges of reconstruction for the reassembly problem. Existing approach\nfocusing primarily on piecewise information for both part and fracture\nassembly, often overlooking the integration of complete object prior. Jigsaw++\ndistinguishes itself by learning a category-agnostic shape prior of complete\nobjects. It employs the proposed \"retargeting\" strategy that effectively\nleverages the output of any existing assembly method to generate complete shape\nreconstructions. This capability allows it to function orthogonally to the\ncurrent methods. Through extensive evaluations on Breaking Bad dataset and\nPartNet, Jigsaw++ has demonstrated its effectiveness, reducing reconstruction\nerrors and enhancing the precision of shape reconstruction, which sets a new\ndirection for future reassembly model developments.\n", "rewritten_text": "The automatic assembly problem, with its complex 3D representation challenges, has attracted increasing interest. This paper introduces Jigsaw++, a novel generative method designed to address these multifaceted reconstruction challenges.  Existing approaches, which primarily focus on piecewise part and fracture information, often neglect the integration of complete object priors.  Jigsaw++ distinguishes itself by learning a category-agnostic shape prior for complete objects.  It employs a novel \"retargeting\" strategy that effectively leverages the output of any existing assembly method to generate complete shape reconstructions, enabling orthogonal functionality with current methods.  Extensive evaluations on the Breaking Bad and PartNet datasets demonstrate Jigsaw++'s effectiveness in reducing reconstruction errors and improving shape reconstruction precision, establishing a new direction for future reassembly model development.\n"}, "1806.05658": {"original_text": "  Seq2seq learning has produced promising results on summarization. However, in\nmany cases, system summaries still struggle to keep the meaning of the original\nintact. They may miss out important words or relations that play critical roles\nin the syntactic structure of source sentences. In this paper, we present\nstructure-infused copy mechanisms to facilitate copying important words and\nrelations from the source sentence to summary sentence. The approach naturally\ncombines source dependency structure with the copy mechanism of an abstractive\nsentence summarizer. Experimental results demonstrate the effectiveness of\nincorporating source-side syntactic information in the system, and our proposed\napproach compares favorably to state-of-the-art methods.\n", "rewritten_text": "Sequence-to-sequence (Seq2seq) learning has yielded promising results in text summarization.  However, generated summaries often fail to fully preserve the meaning of the source text, omitting crucial words or relationships vital to the source sentences' syntactic structure.  This paper introduces structure-infused copy mechanisms to improve the copying of these important words and relationships from source to summary.  Our approach seamlessly integrates source-sentence dependency structures with the copy mechanism of an abstractive summarizer.  Experimental results demonstrate the effectiveness of incorporating source-side syntactic information, showing our method favorably compares to state-of-the-art techniques.\n"}, "2108.12545": {"original_text": "  Training deep networks for semantic segmentation requires large amounts of\nlabeled training data, which presents a major challenge in practice, as\nlabeling segmentation masks is a highly labor-intensive process. To address\nthis issue, we present a framework for semi-supervised and domain-adaptive\nsemantic segmentation, which is enhanced by self-supervised monocular depth\nestimation (SDE) trained only on unlabeled image sequences.\n  In particular, we utilize SDE as an auxiliary task comprehensively across the\nentire learning framework: First, we automatically select the most useful\nsamples to be annotated for semantic segmentation based on the correlation of\nsample diversity and difficulty between SDE and semantic segmentation. Second,\nwe implement a strong data augmentation by mixing images and labels using the\ngeometry of the scene. Third, we transfer knowledge from features learned\nduring SDE to semantic segmentation by means of transfer and multi-task\nlearning. And fourth, we exploit additional labeled synthetic data with\nCross-Domain DepthMix and Matching Geometry Sampling to align synthetic and\nreal data.\n  We validate the proposed model on the Cityscapes dataset, where all four\ncontributions demonstrate significant performance gains, and achieve\nstate-of-the-art results for semi-supervised semantic segmentation as well as\nfor semi-supervised domain adaptation. In particular, with only 1/30 of the\nCityscapes labels, our method achieves 92% of the fully-supervised baseline\nperformance and even 97% when exploiting additional data from GTA. The source\ncode is available at\nhttps://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth.\n", "rewritten_text": "Training deep networks for semantic segmentation requires substantial labeled data, a significant practical challenge due to the labor-intensive nature of creating segmentation masks.  To address this, we present a semi-supervised, domain-adaptive semantic segmentation framework enhanced by self-supervised monocular depth estimation (SDE) trained solely on unlabeled image sequences.\n\nOur framework leverages SDE as an auxiliary task throughout:  First, we automatically select samples for annotation based on the correlation between SDE and semantic segmentation difficulty and diversity. Second, we employ robust data augmentation by mixing images and labels using scene geometry. Third, we transfer knowledge from SDE features to semantic segmentation via transfer and multi-task learning. Finally, we leverage synthetic data, employing Cross-Domain DepthMix and Matching Geometry Sampling to align synthetic and real data.\n\nEvaluated on the Cityscapes dataset, our four contributions yield significant performance improvements, achieving state-of-the-art results for both semi-supervised semantic segmentation and semi-supervised domain adaptation.  Using only 1/30th of the Cityscapes labels, our method achieves 92% of the fully supervised baseline performance, rising to 97% with additional data from GTA.  Source code is available at https://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth.\n"}, "2405.12833": {"original_text": "  Automatic radiology report generation can alleviate the workload for\nphysicians and minimize regional disparities in medical resources, therefore\nbecoming an important topic in the medical image analysis field. It is a\nchallenging task, as the computational model needs to mimic physicians to\nobtain information from multi-modal input data (i.e., medical images, clinical\ninformation, medical knowledge, etc.), and produce comprehensive and accurate\nreports. Recently, numerous works emerged to address this issue using deep\nlearning-based methods, such as transformers, contrastive learning, and\nknowledge-base construction. This survey summarizes the key techniques\ndeveloped in the most recent works and proposes a general workflow for deep\nlearning-based report generation with five main components, including\nmulti-modality data acquisition, data preparation, feature learning, feature\nfusion/interaction, and report generation. The state-of-the-art methods for\neach of these components are highlighted. Additionally, training strategies,\npublic datasets, evaluation methods, current challenges, and future directions\nin this field are summarized. We have also conducted a quantitative comparison\nbetween different methods under the same experimental setting. This is the most\nup-to-date survey that focuses on multi-modality inputs and data fusion for\nradiology report generation. The aim is to provide comprehensive and rich\ninformation for researchers interested in automatic clinical report generation\nand medical image analysis, especially when using multimodal inputs, and assist\nthem in developing new algorithms to advance the field.\n", "rewritten_text": "Automatic radiology report generation is a crucial area of medical image analysis, offering the potential to reduce physician workload and mitigate regional healthcare disparities.  This challenging task requires computational models to emulate physicians' abilities, extracting information from multimodal inputs (medical images, clinical data, medical knowledge, etc.) to produce comprehensive and accurate reports.  Recent advancements leverage deep learning, employing techniques such as transformers, contrastive learning, and knowledge-base construction.\n\nThis survey comprehensively reviews state-of-the-art methods in this rapidly evolving field.  We propose a general workflow for deep learning-based report generation encompassing five key components: multimodal data acquisition, data preparation, feature learning, feature fusion/interaction, and report generation.  We highlight leading techniques for each component, along with relevant training strategies, publicly available datasets, evaluation methodologies, current challenges, and promising future directions.  Furthermore, we present a quantitative comparison of different methods under standardized experimental conditions.\n\nThis survey uniquely focuses on multimodal inputs and data fusion for radiology report generation, providing researchers in automatic clinical report generation and medical image analysis\u2014particularly those working with multimodal data\u2014with a rich resource to inform the development of novel algorithms and advance the field.\n"}, "1809.10417": {"original_text": "  The tracking-by-detection framework receives growing attentions through the\nintegration with the Convolutional Neural Networks (CNNs). Existing\ntracking-by-detection based methods, however, fail to track objects with severe\nappearance variations. This is because the traditional convolutional operation\nis performed on fixed grids, and thus may not be able to find the correct\nresponse while the object is changing pose or under varying environmental\nconditions. In this paper, we propose a deformable convolution layer to enrich\nthe target appearance representations in the tracking-by-detection framework.\nWe aim to capture the target appearance variations via deformable convolution,\nwhich adaptively enhances its original features. In addition, we also propose a\ngated fusion scheme to control how the variations captured by the deformable\nconvolution affect the original appearance. The enriched feature representation\nthrough deformable convolution facilitates the discrimination of the CNN\nclassifier on the target object and background. Extensive experiments on the\nstandard benchmarks show that the proposed tracker performs favorably against\nstate-of-the-art methods.\n", "rewritten_text": "Tracking-by-detection frameworks have gained significant attention through their integration with Convolutional Neural Networks (CNNs).  However, existing tracking-by-detection methods struggle to track objects exhibiting severe appearance variations. This limitation stems from the traditional convolutional operation's reliance on fixed grids, hindering its ability to accurately locate objects undergoing pose changes or appearing under varying environmental conditions.  This paper introduces a deformable convolution layer to enhance target appearance representations within the tracking-by-detection framework.  We leverage deformable convolution to adaptively capture target appearance variations, enriching the original features.  Furthermore, a gated fusion scheme is proposed to modulate the influence of these variations on the original appearance. This enriched feature representation, facilitated by deformable convolution, improves the CNN classifier's ability to discriminate between target objects and background.  Extensive experiments on standard benchmarks demonstrate that the proposed tracker outperforms state-of-the-art methods.\n"}, "2111.15603": {"original_text": "  Modern neural networks are able to perform at least as well as humans in\nnumerous tasks involving object classification and image generation. However,\nsmall perturbations which are imperceptible to humans may significantly degrade\nthe performance of well-trained deep neural networks. We provide a\nDistributionally Robust Optimization (DRO) framework which integrates\nhuman-based image quality assessment methods to design optimal attacks that are\nimperceptible to humans but significantly damaging to deep neural networks.\nThrough extensive experiments, we show that our attack algorithm generates\nbetter-quality (less perceptible to humans) attacks than other state-of-the-art\nhuman imperceptible attack methods. Moreover, we demonstrate that DRO training\nusing our optimally designed human imperceptible attacks can improve group\nfairness in image classification. Towards the end, we provide an algorithmic\nimplementation to speed up DRO training significantly, which could be of\nindependent interest.\n", "rewritten_text": "Modern neural networks achieve human-level performance in numerous object classification and image generation tasks.  However, even well-trained deep neural networks are vulnerable to small, human-imperceptible perturbations that significantly degrade their performance.  We present a Distributionally Robust Optimization (DRO) framework that integrates human-based image quality assessment to design optimal attacks: imperceptible to humans yet highly damaging to neural networks.  Extensive experiments demonstrate that our attack algorithm generates higher-quality (less perceptually noticeable) attacks than state-of-the-art human-imperceptible methods.  Furthermore, we show that DRO training using these optimally designed attacks improves group fairness in image classification.  Finally, we provide an algorithmic implementation to significantly accelerate DRO training, a contribution of independent interest.\n"}, "2404.00829": {"original_text": "  Human writers often bookend their writing with ending sentences that relate\nback to the beginning sentences in order to compose a satisfying narrative that\n\"closes the loop.\" Motivated by this observation, we propose RENarGen, a\ncontrollable story-generation paradigm that generates narratives by ensuring\nthe first and last sentences are related and then infilling the middle\nsentences. Our contributions include an initial exploration of how various\nmethods of bookending from Narratology affect language modeling for stories.\nAutomatic and human evaluations indicate RENarGen produces better stories with\nmore narrative closure than current autoregressive models.\n", "rewritten_text": "Inspired by the common literary technique of bookending\u2014where the opening and closing sentences of a narrative are thematically linked\u2014we propose RENarGen, a controllable story generation paradigm.  RENarGen generates narratives by first establishing a relationship between the initial and final sentences, then infilling the intervening text.  Our contributions include a preliminary investigation into how various narrative bookending methods impact language modeling in story generation.  Automatic and human evaluations demonstrate that RENarGen produces stories with significantly greater narrative closure than existing autoregressive models.\n"}, "1711.01062": {"original_text": "  With the development of depth cameras such as Kinect and Intel Realsense,\nRGB-D based human detection receives continuous research attention due to its\nusage in a variety of applications. In this paper, we propose a new\nMulti-Glimpse LSTM (MG-LSTM) network, in which multi-scale contextual\ninformation is sequentially integrated to promote the human detection\nperformance. Furthermore, we propose a feature fusion strategy based on our\nMG-LSTM network to better incorporate the RGB and depth information. To the\nbest of our knowledge, this is the first attempt to utilize LSTM structure for\nRGB-D based human detection. Our method achieves superior performance on two\npublicly available datasets.\n", "rewritten_text": "RGB-D human detection has garnered significant research interest due to its widespread applicability, fueled by advancements in depth cameras like Kinect and Intel RealSense.  This paper introduces a novel Multi-Glimpse LSTM (MG-LSTM) network that sequentially integrates multi-scale contextual information to improve human detection performance.  We further propose a feature fusion strategy based on MG-LSTM to effectively combine RGB and depth data.  To our knowledge, this is the first application of LSTM architecture to RGB-D human detection.  Our method demonstrates superior performance on two publicly available datasets.\n"}, "1810.07599": {"original_text": "  As facial appearance is subject to significant intra-class variations caused\nby the aging process over time, age-invariant face recognition (AIFR) remains a\nmajor challenge in face recognition community. To reduce the intra-class\ndiscrepancy caused by the aging, in this paper we propose a novel approach\n(namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep\nface features. Specifically, we decompose deep face features into two\northogonal components to represent age-related and identity-related features.\nAs a result, identity-related features that are robust to aging are then used\nfor AIFR. Besides, for complementing the existing cross-age datasets and\nadvancing the research in this field, we construct a brand-new large-scale\nCross-Age Face dataset (CAF). Extensive experiments conducted on the three\npublic domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have\nshown the effectiveness of the proposed approach and the value of the\nconstructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most\npopular general face recognition (GFR) dataset LFW additionally demonstrates\nthe comparable generalization performance on GFR.\n", "rewritten_text": "Age-invariant face recognition (AIFR) remains a significant challenge due to substantial intra-class variations caused by aging.  This paper proposes a novel approach, Orthogonal Embedding Convolutional Neural Networks (OE-CNNs), to learn age-invariant deep face features.  OE-CNNs decompose deep face features into two orthogonal components representing age-related and identity-related information.  AIFR is then performed using the identity-related features, which are robust to aging effects.  Furthermore, to advance research in this area, we introduce a new large-scale Cross-Age Face (CAF) dataset.  Extensive experiments on three public datasets (MORPH Album 2, CACD-VS, and FG-NET) demonstrate the effectiveness of our approach and the value of the CAF dataset for AIFR.  Benchmarking against the widely used LFW dataset also shows comparable generalization performance on general face recognition (GFR) tasks.\n"}, "2107.005": {"original_text": "  Driven by recent advances in object detection with deep neural networks, the\ntracking-by-detection paradigm has gained increasing prevalence in the research\ncommunity of multi-object tracking (MOT). It has long been known that\nappearance information plays an essential role in the detection-to-track\nassociation, which lies at the core of the tracking-by-detection paradigm.\nWhile most existing works consider the appearance distances between the\ndetections and the tracks, they ignore the statistical information implied by\nthe historical appearance distance records in the tracks, which can be\nparticularly useful when a detection has similar distances with two or more\ntracks. In this work, we propose a hybrid track association (HTA) algorithm\nthat models the historical appearance distances of a track with an incremental\nGaussian mixture model (IGMM) and incorporates the derived statistical\ninformation into the calculation of the detection-to-track association cost.\nExperimental results on three MOT benchmarks confirm that HTA effectively\nimproves the target identification performance with a small compromise to the\ntracking speed. Additionally, compared to many state-of-the-art trackers, the\nDeepSORT tracker equipped with HTA achieves better or comparable performance in\nterms of the balance of tracking quality and speed.\n", "rewritten_text": "Recent advances in deep neural network-based object detection have significantly popularized the tracking-by-detection paradigm within the multi-object tracking (MOT) research community.  Appearance information is crucial for associating detections with tracks, a core component of this paradigm. While most existing methods consider only the current appearance distance between detections and tracks, they neglect the valuable statistical information inherent in the historical appearance distance records of each track. This is particularly relevant when a detection exhibits similar distances to multiple tracks.  This work proposes a novel hybrid track association (HTA) algorithm.  HTA models the historical appearance distances of each track using an incremental Gaussian mixture model (IGMM) and integrates the resulting statistical information into the detection-to-track association cost calculation.  Experiments on three MOT benchmarks demonstrate that HTA effectively improves target identification performance with minimal impact on tracking speed.  Furthermore, when integrated into the DeepSORT tracker, HTA achieves comparable or superior performance to many state-of-the-art trackers in terms of the balance between tracking quality and speed.\n"}, "1711.08879": {"original_text": "  Objects for detection usually have distinct characteristics in different\nsub-regions and different aspect ratios. However, in prevalent two-stage object\ndetection methods, Region-of-Interest (RoI) features are extracted by RoI\npooling with little emphasis on these translation-variant feature components.\nWe present feature selective networks to reform the feature representations of\nRoIs by exploiting their disparities among sub-regions and aspect ratios. Our\nnetwork produces the sub-region attention bank and aspect ratio attention bank\nfor the whole image. The RoI-based sub-region attention map and aspect ratio\nattention map are selectively pooled from the banks, and then used to refine\nthe original RoI features for RoI classification. Equipped with a light-weight\ndetection subnetwork, our network gets a consistent boost in detection\nperformance based on general ConvNet backbones (ResNet-101, GoogLeNet and\nVGG-16). Without bells and whistles, our detectors equipped with ResNet-101\nachieve more than 3% mAP improvement compared to counterparts on PASCAL VOC\n2007, PASCAL VOC 2012 and MS COCO datasets.\n", "rewritten_text": "Object detection often relies on features that vary significantly across sub-regions and aspect ratios.  However, prevalent two-stage detectors, which typically employ Region-of-Interest (RoI) pooling, often neglect these translation-variant features.  We propose feature-selective networks that leverage these variations to improve RoI feature representations.  Our network generates image-wide sub-region and aspect-ratio attention banks.  RoI-specific attention maps are then selectively pooled from these banks and used to refine the original RoI features before classification.  Integrated with a lightweight detection subnetwork, our approach consistently improves detection performance across various convolutional neural network (CNN) backbones (ResNet-101, GoogLeNet, and VGG-16).  Without extensive modifications, our ResNet-101-based detector achieves over a 3% mean Average Precision (mAP) improvement compared to baseline methods on the PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO datasets.\n"}, "2402.02145": {"original_text": "  In today's media landscape, where news outlets play a pivotal role in shaping\npublic opinion, it is imperative to address the issue of sentiment manipulation\nwithin news text. News writers often inject their own biases and emotional\nlanguage, which can distort the objectivity of reporting. This paper introduces\na novel approach to tackle this problem by reducing the polarity of latent\nsentiments in news content. Drawing inspiration from adversarial attack-based\nsentence perturbation techniques and a prompt based method using ChatGPT, we\nemploy transformation constraints to modify sentences while preserving their\ncore semantics. Using three perturbation methods: replacement, insertion, and\ndeletion coupled with a context-aware masked language model, we aim to maximize\nthe desired sentiment score for targeted news aspects through a beam search\nalgorithm. Our experiments and human evaluations demonstrate the effectiveness\nof these two models in achieving reduced sentiment polarity with minimal\nmodifications while maintaining textual similarity, fluency, and grammatical\ncorrectness. Comparative analysis confirms the competitive performance of the\nadversarial attack based perturbation methods and prompt-based methods,\noffering a promising solution to foster more objective news reporting and\ncombat emotional language bias in the media.\n", "rewritten_text": "In today's media landscape, where news outlets significantly shape public opinion, mitigating sentiment manipulation in news text is crucial.  News writers often inject bias and emotionally charged language, compromising objectivity. This paper presents a novel approach to reduce the polarity of latent sentiment in news content.  Inspired by adversarial attack-based sentence perturbation techniques and a ChatGPT-based prompt method, we utilize transformation constraints to modify sentences while preserving their core meaning.  Employing three perturbation methods\u2014replacement, insertion, and deletion\u2014in conjunction with a context-aware masked language model and a beam search algorithm, we aim to optimize targeted news aspects for a desired sentiment score.  Our experiments and human evaluations demonstrate the effectiveness of both models in reducing sentiment polarity with minimal alterations, while maintaining textual similarity, fluency, and grammatical correctness.  Comparative analysis reveals the competitive performance of both adversarial attack-based and prompt-based methods, offering a promising solution for fostering more objective news reporting and countering biased emotional language in the media.\n"}, "2312.10437": {"original_text": "  Tender notices are usually sought by most of the companies at regular\nintervals as a means for obtaining the contracts of various projects. These\nnotices consist of all the required information like description of the work,\nperiod of construction, estimated amount of project, etc. In the context of\nNepal, tender notices are usually published in national as well as local\nnewspapers. The interested bidders should search all the related tender notices\nin newspapers. However, it is very tedious for these companies to manually\nsearch tender notices in every newspaper and figure out which bid is best\nsuited for them. This project is built with the purpose of solving this tedious\ntask of manually searching the tender notices. Initially, the newspapers are\ndownloaded in PDF format using the selenium library of python. After\ndownloading the newspapers, the e-papers are scanned and tender notices are\nautomatically extracted using a neural network. For extraction purposes,\ndifferent architectures of CNN namely ResNet, GoogleNet and Xception are used\nand a model with highest performance has been implemented. Finally, these\nextracted notices are then published on the website and are accessible to the\nusers. This project is helpful for construction companies as well as\ncontractors assuring quality and efficiency. This project has great application\nin the field of competitive bidding as well as managing them in a systematic\nmanner.\n", "rewritten_text": "Many companies regularly seek tender notices to secure project contracts.  These notices contain essential information, including project descriptions, construction timelines, and estimated costs. In Nepal, tender notices are typically published in national and local newspapers.  Interested bidders must search these newspapers for relevant notices, a time-consuming manual process.  This project automates this search.  Initially, newspapers are downloaded as PDFs using Python's Selenium library.  A neural network then extracts tender notices from the downloaded e-papers.  We evaluated various Convolutional Neural Network (CNN) architectures\u2014ResNet, GoogleNet, and Xception\u2014selecting the highest-performing model for implementation.  Finally, the extracted notices are published on a user-accessible website. This project improves efficiency and quality for construction companies and contractors, offering a systematic approach to competitive bidding.\n"}, "2405.18132": {"original_text": "  In recent years, the increasing demand for dynamic 3D assets in design and\ngaming applications has given rise to powerful generative pipelines capable of\nsynthesizing high-quality 4D objects. Previous methods generally rely on score\ndistillation sampling (SDS) algorithm to infer the unseen views and motion of\n4D objects, thus leading to unsatisfactory results with defects like\nover-saturation and Janus problem. Therefore, inspired by recent progress of\nvideo diffusion models, we propose to optimize a 4D representation by\nexplicitly generating multi-view videos from one input image. However, it is\nfar from trivial to handle practical challenges faced by such a pipeline,\nincluding dramatic temporal inconsistency, inter-frame geometry and texture\ndiversity, and semantic defects brought by video generation results. To address\nthese issues, we propose DG4D, a novel multi-stage framework that generates\nhigh-quality and consistent 4D assets without score distillation. Specifically,\ncollaborative techniques and solutions are developed, including an attention\ninjection strategy to synthesize temporal-consistent multi-view videos, a\nrobust and efficient dynamic reconstruction method based on Gaussian Splatting,\nand a refinement stage with diffusion prior for semantic restoration. The\nqualitative results and user preference study demonstrate that our framework\noutperforms the baselines in generation quality by a considerable margin. Code\nwill be released at \\url{https://github.com/jasongzy/EG4D}.\n", "rewritten_text": "Recent years have witnessed a surge in demand for dynamic 3D assets in design and gaming, driving the development of powerful generative pipelines for synthesizing high-quality 4D objects.  Previous methods, relying on score distillation sampling (SDS), often yielded unsatisfactory results, exhibiting artifacts such as over-saturation and the Janus problem when inferring unseen views and motion.  Inspired by advancements in video diffusion models, we propose a novel approach that optimizes a 4D representation by explicitly generating multi-view videos from a single input image.  This, however, presents significant challenges, including temporal inconsistency, inter-frame variations in geometry and texture, and semantic defects inherent in video generation.\n\nTo address these challenges, we introduce DG4D, a multi-stage framework that generates high-quality, temporally consistent 4D assets without score distillation.  Our framework incorporates several key innovations: an attention injection strategy for temporally consistent multi-view video synthesis; a robust and efficient dynamic reconstruction method based on Gaussian splatting; and a refinement stage leveraging diffusion priors for semantic restoration.  Qualitative results and a user preference study demonstrate that DG4D significantly outperforms existing baselines in generation quality.  Code is available at [https://github.com/jasongzy/EG4D](https://github.com/jasongzy/EG4D).\n"}, "2101.04929": {"original_text": "  Motivated by applications from computer vision to bioinformatics, the field\nof shape analysis deals with problems where one wants to analyze geometric\nobjects, such as curves, while ignoring actions that preserve their shape, such\nas translations, rotations, or reparametrizations. Mathematical tools have been\ndeveloped to define notions of distances, averages, and optimal deformations\nfor geometric objects. One such framework, which has proven to be successful in\nmany applications, is based on the square root velocity (SRV) transform, which\nallows one to define a computable distance between spatial curves regardless of\nhow they are parametrized. This paper introduces a supervised deep learning\nframework for the direct computation of SRV distances between curves, which\nusually requires an optimization over the group of reparametrizations that act\non the curves. The benefits of our approach in terms of computational speed and\naccuracy are illustrated via several numerical experiments.\n", "rewritten_text": "Shape analysis, motivated by applications ranging from computer vision to bioinformatics, addresses the analysis of geometric objects like curves while disregarding shape-preserving transformations such as translations, rotations, and reparametrizations.  Mathematical tools have been developed to define distances, averages, and optimal deformations for these objects.  One particularly successful framework utilizes the square root velocity (SRV) transform, enabling the computation of distances between spatial curves irrespective of their parametrization. This paper presents a supervised deep learning framework for the direct computation of SRV distances between curves, eliminating the computationally expensive optimization over the reparametrization group typically required.  Numerical experiments demonstrate the superior computational speed and accuracy of our approach.\n"}, "1811.07461": {"original_text": "  Humans naturally perceive a 3D scene in front of them through accumulation of\ninformation obtained from multiple interconnected projections of the scene and\nby interpreting their correspondence. This phenomenon has inspired artificial\nintelligence models to extract the depth and view angle of the observed scene\nby modeling the correspondence between different views of that scene. Our paper\nis built upon previous works in the field of unsupervised depth and relative\ncamera pose estimation from temporal consecutive video frames using deep\nlearning (DL) models. Our approach uses a hybrid learning framework introduced\nin a recent work called GeoNet, which leverages geometric constraints in the 3D\nscenes to synthesize a novel view from intermediate DL-based predicted depth\nand relative pose. However, the state-of-the-art unsupervised depth and pose\nestimation DL models are exclusively trained/tested on a few available outdoor\nscene datasets and we have shown they are hardly transferable to new scenes,\nespecially from indoor environments, in which estimation requires higher\nprecision and dealing with probable occlusions. This paper introduces \"Indoor\nGeoNet\", a weakly supervised depth and camera pose estimation model targeted\nfor indoor scenes. In Indoor GeoNet, we take advantage of the availability of\nindoor RGBD datasets collected by human or robot navigators, and added partial\n(i.e. weak) supervision in depth training into the model. Experimental results\nshowed that our model effectively generalizes to new scenes from different\nbuildings. Indoor GeoNet demonstrated significant depth and pose estimation\nerror reduction when compared to the original GeoNet, while showing 3 times\nmore reconstruction accuracy in synthesizing novel views in indoor\nenvironments.\n", "rewritten_text": "Humans perceive three-dimensional scenes by integrating information from multiple, interconnected projections and interpreting their correspondence.  This process has inspired artificial intelligence models that infer depth and viewing angle by modeling the correspondence between different views.  Our work builds upon previous research in unsupervised depth and relative camera pose estimation from consecutive video frames using deep learning (DL) models.  We employ a hybrid learning framework, inspired by GeoNet, which leverages geometric constraints to synthesize novel views from intermediate DL-predicted depth and relative pose. However, state-of-the-art unsupervised DL models for depth and pose estimation are primarily trained and tested on limited outdoor datasets, exhibiting poor transferability to new, particularly indoor, scenes requiring higher precision and handling of occlusions.  This paper introduces Indoor GeoNet, a weakly supervised model for depth and camera pose estimation specifically designed for indoor environments.  Indoor GeoNet utilizes the availability of indoor RGB-D datasets collected by human or robotic navigators, incorporating weak supervision into depth training.  Experimental results demonstrate effective generalization to unseen scenes from diverse buildings.  Compared to GeoNet, Indoor GeoNet achieved significant reductions in depth and pose estimation errors, and exhibited a threefold increase in novel view synthesis accuracy in indoor environments.\n"}, "2208.04254": {"original_text": "  Image captioning models are usually trained according to human annotated\nground-truth captions, which could generate accurate but generic captions. In\nthis paper, we focus on generating distinctive captions that can distinguish\nthe target image from other similar images. To evaluate the distinctiveness of\ncaptions, we introduce a series of metrics that use large-scale vision-language\npre-training model CLIP to quantify the distinctiveness. To further improve the\ndistinctiveness of captioning models, we propose a simple and effective\ntraining strategy that trains the model by comparing target image with similar\nimage group and optimizing the group embedding gap. Extensive experiments are\nconducted on various baseline models to demonstrate the wide applicability of\nour strategy and the consistency of metric results with human evaluation. By\ncomparing the performance of our best model with existing state-of-the-art\nmodels, we claim that our model achieves new state-of-the-art towards\ndistinctiveness objective.\n", "rewritten_text": "Image captioning models, typically trained on human-annotated ground truth captions, often produce accurate but generic descriptions.  This paper addresses this limitation by focusing on generating distinctive captions that differentiate a target image from visually similar ones.  We introduce novel metrics, leveraging the large-scale vision-language pre-trained model CLIP, to quantify caption distinctiveness.  Furthermore, we propose a simple yet effective training strategy that improves distinctiveness by comparing the target image's embedding to those of similar images and optimizing the inter-group embedding distance.  Extensive experiments across various baseline models demonstrate the strategy's broad applicability and the metrics' strong correlation with human judgments.  Compared to state-of-the-art models, our approach achieves a new state-of-the-art in generating distinctive image captions.\n"}, "2212.07181": {"original_text": "  Neuromorphic vision or event vision is an advanced vision technology, where\nin contrast to the visible camera that outputs pixels, the event vision\ngenerates neuromorphic events every time there is a brightness change which\nexceeds a specific threshold in the field of view (FOV). This study focuses on\nleveraging neuromorphic event data for roadside object detection. This is a\nproof of concept towards building artificial intelligence (AI) based pipelines\nwhich can be used for forward perception systems for advanced vehicular\napplications. The focus is on building efficient state-of-the-art object\ndetection networks with better inference results for fast-moving forward\nperception using an event camera. In this article, the event-simulated A2D2\ndataset is manually annotated and trained on two different YOLOv5 networks\n(small and large variants). To further assess its robustness, single model\ntesting and ensemble model testing are carried out.\n", "rewritten_text": "Neuromorphic vision, or event-based vision, is an advanced technology that differs from conventional cameras.  Instead of outputting a stream of pixels, event cameras generate neuromorphic events only when a significant brightness change exceeds a predefined threshold within their field of view (FOV). This study explores the application of neuromorphic event data to roadside object detection.  It serves as a proof of concept for developing artificial intelligence (AI)-based perception systems for advanced vehicular applications.  The goal is to create efficient, state-of-the-art object detection networks capable of achieving superior inference results for fast-moving objects in forward-facing perception using an event camera.  This article details the manual annotation of the event-simulated A2D2 dataset and its use to train two YOLOv5 networks (small and large variants).  Single-model and ensemble-model testing were conducted to evaluate robustness.\n"}, "2402.01217": {"original_text": "  Implicit neural representations, represented by Neural Radiance Fields\n(NeRF), have dominated research in 3D computer vision by virtue of high-quality\nvisual results and data-driven benefits. However, their realistic applications\nare hindered by the need for dense inputs and per-scene optimization. To solve\nthis problem, previous methods implement generalizable NeRFs by extracting\nlocal features from sparse inputs as conditions for the NeRF decoder. However,\nalthough this way can allow feed-forward reconstruction, they suffer from the\ninherent drawback of yielding sub-optimal results caused by erroneous\nreprojected features. In this paper, we focus on this problem and aim to\naddress it by introducing pre-trained generative priors to enable high-quality\ngeneralizable novel view synthesis. Specifically, we propose a novel Indirect\nDiffusion-guided NeRF framework, termed ID-NeRF, which leverages pre-trained\ndiffusion priors as a guide for the reprojected features created by the\nprevious paradigm. Notably, to enable 3D-consistent predictions, the proposed\nID-NeRF discards the way of direct supervision commonly used in prior 3D\ngenerative models and instead adopts a novel indirect prior injection strategy.\nThis strategy is implemented by distilling pre-trained knowledge into an\nimaginative latent space via score-based distillation, and an attention-based\nrefinement module is then proposed to leverage the embedded priors to improve\nreprojected features extracted from sparse inputs. We conduct extensive\nexperiments on multiple datasets to evaluate our method, and the results\ndemonstrate the effectiveness of our method in synthesizing novel views in a\ngeneralizable manner, especially in sparse settings.\n", "rewritten_text": "Implicit neural representations, such as Neural Radiance Fields (NeRFs), have achieved significant success in 3D computer vision due to their high-quality visual results and data-driven advantages.  However, their practical applicability is limited by the requirement for dense input data and per-scene optimization.  Existing methods address this limitation by training generalizable NeRFs, conditioning the NeRF decoder on local features extracted from sparse inputs.  While enabling feed-forward reconstruction, these methods suffer from suboptimal results due to errors in feature reprojection.\n\nThis paper addresses this issue by incorporating pre-trained generative priors to facilitate high-quality, generalizable novel view synthesis.  We propose a novel Indirect Diffusion-guided NeRF framework, ID-NeRF, which leverages pre-trained diffusion models as a guide for refining reprojected features.  Unlike previous 3D generative models that rely on direct supervision, ID-NeRF employs an indirect prior injection strategy.  This involves distilling pre-trained knowledge into a latent space using score-based distillation, followed by an attention-based refinement module that leverages these embedded priors to improve the quality of reprojected features from sparse inputs.\n\nExtensive experiments on multiple datasets demonstrate the effectiveness of our method in generating high-quality novel views, particularly in sparse data scenarios.  The results showcase the superior generalizability of ID-NeRF compared to existing approaches.\n"}, "2207.02803": {"original_text": "  Recent advances in face forgery techniques produce nearly visually\nuntraceable deepfake videos, which could be leveraged with malicious\nintentions. As a result, researchers have been devoted to deepfake detection.\nPrevious studies have identified the importance of local low-level cues and\ntemporal information in pursuit to generalize well across deepfake methods,\nhowever, they still suffer from robustness problem against post-processings. In\nthis work, we propose the Local- & Temporal-aware Transformer-based Deepfake\nDetection (LTTD) framework, which adopts a local-to-global learning protocol\nwith a particular focus on the valuable temporal information within local\nsequences. Specifically, we propose a Local Sequence Transformer (LST), which\nmodels the temporal consistency on sequences of restricted spatial regions,\nwhere low-level information is hierarchically enhanced with shallow layers of\nlearned 3D filters. Based on the local temporal embeddings, we then achieve the\nfinal classification in a global contrastive way. Extensive experiments on\npopular datasets validate that our approach effectively spots local forgery\ncues and achieves state-of-the-art performance.\n", "rewritten_text": "Recent advances in face forgery techniques have produced nearly visually untraceable deepfake videos, readily exploitable for malicious purposes.  Consequently, researchers have focused intensely on deepfake detection. While previous studies have highlighted the importance of local low-level cues and temporal information for robust generalization across deepfake methods, they remain vulnerable to post-processing artifacts.  This work proposes the Local- and Temporal-Aware Transformer-based Deepfake Detection (LTTD) framework, employing a local-to-global learning strategy that emphasizes valuable temporal information within local sequences.  Specifically, we introduce a Local Sequence Transformer (LST) that models temporal consistency within sequences of spatially restricted regions.  Low-level information is hierarchically enhanced using shallow layers of learned 3D filters.  Finally, global contrastive learning is applied to these local temporal embeddings for classification.  Extensive experiments on popular datasets demonstrate the effectiveness of our approach in identifying local forgery cues and achieving state-of-the-art performance.\n"}, "2210.16621": {"original_text": "  Transformer-based architectures like BERT have achieved great success in a\nwide range of Natural Language tasks. Despite their decent performance, the\nmodels still have numerous parameters and high computational complexity,\nimpeding their deployment in resource-constrained environments. Post-Training\nQuantization (PTQ), which enables low-bit computations without extra training,\ncould be a promising tool. In this work, we conduct an empirical evaluation of\nthree PTQ methods on BERT-Base and BERT-Large: Linear Quantization (LQ),\nAnalytical Clipping for Integer Quantization (ACIQ), and Outlier Channel\nSplitting (OCS). OCS theoretically surpasses the others in minimizing the Mean\nSquare quantization Error and avoiding distorting the weights' outliers. That\nis consistent with the evaluation results of most language tasks of GLUE\nbenchmark and a reading comprehension task, SQuAD. Moreover, low-bit quantized\nBERT models could outperform the corresponding 32-bit baselines on several\nsmall language tasks, which we attribute to the alleviation of\nover-parameterization. We further explore the limit of quantization bit and\nshow that OCS could quantize BERT-Base and BERT-Large to 3-bits and retain 98%\nand 96% of the performance on the GLUE benchmark accordingly. Moreover, we\nconduct quantization on the whole BERT family, i.e., BERT models in different\nconfigurations, and comprehensively evaluate their performance on the GLUE\nbenchmark and SQuAD, hoping to provide valuable guidelines for their deployment\nin various computation environments.\n", "rewritten_text": "Transformer-based architectures such as BERT have achieved significant success in numerous natural language processing tasks.  However, their substantial parameter count and high computational complexity hinder deployment in resource-constrained environments. Post-training quantization (PTQ), which allows for low-bit computation without retraining, offers a promising solution.  This work empirically evaluates three PTQ methods\u2014linear quantization (LQ), analytical clipping for integer quantization (ACIQ), and outlier channel splitting (OCS)\u2014on BERT-Base and BERT-Large.  Theoretically, OCS minimizes mean squared quantization error and avoids distorting outlier weights more effectively than LQ and ACI.  This is supported by evaluation results on the GLUE benchmark and the SQuAD reading comprehension task.  Interestingly, low-bit quantized BERT models even outperformed their 32-bit counterparts on several smaller language tasks, potentially due to mitigation of over-parameterization.  We further investigated the limits of quantization, demonstrating that OCS can quantize BERT-Base and BERT-Large to 3 bits while retaining 98% and 96% of GLUE benchmark performance, respectively.  Finally, we conducted a comprehensive evaluation of various BERT model configurations across the GLUE benchmark and SQuAD, providing valuable guidelines for deployment in diverse computational environments.\n"}, "2401.10768": {"original_text": "  While large language models (LLMs) have demonstrated exceptional performance\nacross various tasks following human alignment, they may still generate\nresponses that sound plausible but contradict factual knowledge, a phenomenon\nknown as hallucination. In this paper, we demonstrate the feasibility of\nmitigating hallucinations by verifying and minimizing the inconsistency between\nexternal knowledge present in the alignment data and the intrinsic knowledge\nembedded within foundation LLMs. Specifically, we propose a novel approach\ncalled Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM\nto automatically formulate assessments based on external knowledge to evaluate\nthe knowledge boundaries of foundation LLMs. To address knowledge\ninconsistencies in the alignment data, KCA implements several specific\nstrategies to deal with these data instances. We demonstrate the superior\nefficacy of KCA in reducing hallucinations across six benchmarks, utilizing\nfoundation LLMs of varying backbones and scales. This confirms the\neffectiveness of mitigating hallucinations by reducing knowledge inconsistency.\nOur code, model weights, and data are openly accessible at\n\\url{https://github.com/fanqiwan/KCA}.\n", "rewritten_text": "Large language models (LLMs), despite their impressive performance on various tasks, can generate factually inaccurate yet plausible responses\u2014a phenomenon known as hallucination.  This paper demonstrates a novel approach, Knowledge Consistent Alignment (KCA), to mitigate this issue. KCA reduces hallucinations by minimizing inconsistencies between external knowledge in the alignment data and the intrinsic knowledge of foundation LLMs.  Specifically, a well-aligned LLM automatically assesses the knowledge boundaries of foundation LLMs using external knowledge.  KCA further incorporates strategies to address knowledge inconsistencies within the alignment data itself.  We demonstrate KCA's superior efficacy in reducing hallucinations across six benchmarks using foundation LLMs with diverse architectures and scales, confirming the effectiveness of our approach. Our code, model weights, and data are publicly available at [https://github.com/fanqiwan/KCA](https://github.com/fanqiwan/KCA).\n"}, "2201.01016": {"original_text": "  Recovering detailed facial geometry from a set of calibrated multi-view\nimages is valuable for its wide range of applications. Traditional multi-view\nstereo (MVS) methods adopt an optimization-based scheme to regularize the\nmatching cost. Recently, learning-based methods integrate all these into an\nend-to-end neural network and show superiority of efficiency. In this paper, we\npropose a novel architecture to recover extremely detailed 3D faces within\ndozens of seconds. Unlike previous learning-based methods that regularize the\ncost volume via 3D CNN, we propose to learn an implicit function for regressing\nthe matching cost. By fitting a 3D morphable model from multi-view images, the\nfeatures of multiple images are extracted and aggregated in the mesh-attached\nUV space, which makes the implicit function more effective in recovering\ndetailed facial shape. Our method outperforms SOTA learning-based MVS in\naccuracy by a large margin on the FaceScape dataset. The code and data are\nreleased in https://github.com/zhuhao-nju/mvfr.\n", "rewritten_text": "Recovering detailed facial geometry from calibrated multi-view images is crucial for numerous applications.  Traditional multi-view stereo (MVS) methods rely on optimization-based schemes to regularize matching costs.  Recently, learning-based methods have demonstrated superior efficiency by integrating these processes into end-to-end neural networks.  This paper presents a novel architecture for reconstructing highly detailed 3D faces in a matter of seconds.  Unlike previous learning-based methods that regularize the cost volume using 3D convolutional neural networks (CNNs), we propose learning an implicit function to regress the matching cost.  This function leverages a 3D morphable model fitted to the multi-view images; features are extracted from multiple images and aggregated in the mesh-attached UV space, enhancing the implicit function's effectiveness in recovering fine facial details.  Our method significantly outperforms state-of-the-art (SOTA) learning-based MVS methods on the FaceScape dataset. The code and data are available at https://github.com/zhuhao-nju/mvfr.\n"}, "1809.05752": {"original_text": "  Readmission after discharge from a hospital is disruptive and costly,\nregardless of the reason. However, it can be particularly problematic for\npsychiatric patients, so predicting which patients may be readmitted is\ncritically important but also very difficult. Clinical narratives in\npsychiatric electronic health records (EHRs) span a wide range of topics and\nvocabulary; therefore, a psychiatric readmission prediction model must begin\nwith a robust and interpretable topic extraction component. We created a data\npipeline for using document vector similarity metrics to perform topic\nextraction on psychiatric EHR data in service of our long-term goal of creating\na readmission risk classifier. We show initial results for our topic extraction\nmodel and identify additional features we will be incorporating in the future.\n", "rewritten_text": "Hospital readmission is disruptive and costly, regardless of the reason.  This is particularly problematic for psychiatric patients, making accurate readmission prediction critically important yet challenging.  The diverse topics and vocabulary within psychiatric electronic health record (EHR) clinical narratives necessitate a robust and interpretable topic extraction component for any effective readmission prediction model.  To this end, we developed a data pipeline employing document vector similarity metrics for topic extraction from psychiatric EHR data. This pipeline serves as a foundation for our larger goal of creating a readmission risk classifier.  We present initial results from our topic extraction model and outline planned future enhancements.\n"}, "2302.1443": {"original_text": "  3D hand tracking methods based on monocular RGB videos are easily affected by\nmotion blur, while event camera, a sensor with high temporal resolution and\ndynamic range, is naturally suitable for this task with sparse output and low\npower consumption. However, obtaining 3D annotations of fast-moving hands is\ndifficult for constructing event-based hand-tracking datasets. In this paper,\nwe provided an event-based speed adaptive hand tracker (ESAHT) to solve the\nhand tracking problem based on event camera. We enabled a CNN model trained on\na hand tracking dataset with slow motion, which enabled the model to leverage\nthe knowledge of RGB-based hand tracking solutions, to work on fast hand\ntracking tasks. To realize our solution, we constructed the first 3D hand\ntracking dataset captured by an event camera in a real-world environment,\nfigured out two data augment methods to narrow the domain gap between slow and\nfast motion data, developed a speed adaptive event stream segmentation method\nto handle hand movements in different moving speeds, and introduced a new\nevent-to-frame representation method adaptive to event streams with different\nlengths. Experiments showed that our solution outperformed RGB-based as well as\nprevious event-based solutions in fast hand tracking tasks, and our codes and\ndataset will be publicly available.\n", "rewritten_text": "Monocular RGB video-based 3D hand tracking methods are susceptible to motion blur.  Event cameras, with their high temporal resolution and dynamic range, offer a compelling alternative, despite producing sparse output and consuming low power.  However, the difficulty of annotating fast-moving hands hinders the creation of event-based hand-tracking datasets.  This paper presents ESAHT (Event-based Speed Adaptive Hand Tracker), a novel approach to event-camera-based hand tracking.  We leverage a CNN model pretrained on a slow-motion hand tracking dataset, transferring knowledge from RGB-based hand tracking solutions to the fast-motion domain.  To achieve this, we introduce the first real-world 3D hand tracking dataset captured by an event camera.  Furthermore, we developed two data augmentation methods to mitigate the domain gap between slow and fast motion data, a speed-adaptive event stream segmentation method to handle varying hand speeds, and a novel event-to-frame representation method adaptable to varying stream lengths.  Experimental results demonstrate that ESAHT outperforms both RGB-based and existing event-based methods in fast hand tracking tasks.  Our code and dataset will be publicly available.\n"}, "2407.15153": {"original_text": "  Video generation has drawn significant interest recently, pushing the\ndevelopment of large-scale models capable of producing realistic videos with\ncoherent motion. Due to memory constraints, these models typically generate\nshort video segments that are then combined into long videos. The merging\nprocess poses a significant challenge, as it requires ensuring smooth\ntransitions and overall consistency. In this paper, we introduce Anchored\nDiffusion, a novel method for synthesizing relatively long and seamless videos.\nWe extend Diffusion Transformers (DiTs) to incorporate temporal information,\ncreating our sequence-DiT (sDiT) model for generating short video segments.\nUnlike previous works, we train our model on video sequences with random\nnon-uniform temporal spacing and incorporate temporal information via external\nguidance, increasing flexibility and allowing it to capture both short and\nlong-term relationships. Furthermore, during inference, we leverage the\ntransformer architecture to modify the diffusion process, generating a batch of\nnon-uniform sequences anchored to a common frame, ensuring consistency\nregardless of temporal distance. To demonstrate our method, we focus on face\nreenactment, the task of creating a video from a source image that replicates\nthe facial expressions and movements from a driving video. Through\ncomprehensive experiments, we show our approach outperforms current techniques\nin producing longer consistent high-quality videos while offering editing\ncapabilities.\n", "rewritten_text": "Recent interest in video generation has fueled the development of large-scale models capable of producing realistic videos with coherent motion.  However, memory limitations typically restrict these models to generating short video segments, which must then be combined.  This merging process presents a significant challenge, requiring seamless transitions and overall consistency.  This paper introduces Anchored Diffusion, a novel method for synthesizing relatively long, seamless videos.  We extend Diffusion Transformers (DiTs) by incorporating temporal information, creating a sequence-DiT (sDiT) model for generating short video segments.  Unlike prior work, our model is trained on video sequences with randomly non-uniform temporal spacing, and temporal information is incorporated via external guidance. This enhances flexibility and allows the model to capture both short- and long-term temporal relationships.  Furthermore, during inference, we leverage the transformer architecture to modify the diffusion process, generating a batch of non-uniform sequences anchored to a common frame. This ensures consistency regardless of the temporal distance between segments.  We demonstrate our method through face reenactment\u2014the task of generating a video from a source image that replicates the facial expressions and movements of a driving video.  Comprehensive experiments show that our approach outperforms existing techniques in producing longer, consistent, high-quality videos, while also offering editing capabilities.\n"}, "2009.03116": {"original_text": "  This paper presents the first Swedish evaluation benchmark for textual\nsemantic similarity. The benchmark is compiled by simply running the English\nSTS-B dataset through the Google machine translation API. This paper discusses\npotential problems with using such a simple approach to compile a Swedish\nevaluation benchmark, including translation errors, vocabulary variation, and\nproductive compounding. Despite some obvious problems with the resulting\ndataset, we use the benchmark to compare the majority of the currently existing\nSwedish text representations, demonstrating that native models outperform\nmultilingual ones, and that simple bag of words performs remarkably well.\n", "rewritten_text": "This paper introduces the first Swedish benchmark for evaluating textual semantic similarity.  The benchmark was created by translating the English STS-B dataset using the Google Translate API.  We discuss potential limitations of this approach, including translation inaccuracies, vocabulary discrepancies, and the challenges posed by productive compounding in Swedish.  Despite these limitations, we utilize the benchmark to compare most existing Swedish text representation models.  Our results demonstrate that monolingual models outperform multilingual ones, and surprisingly, that a simple bag-of-words model achieves remarkably strong performance.\n"}, "2106.16138": {"original_text": "  In this paper, we introduce ELECTRA-style tasks to cross-lingual language\nmodel pre-training. Specifically, we present two pre-training tasks, namely\nmultilingual replaced token detection, and translation replaced token\ndetection. Besides, we pretrain the model, named as XLM-E, on both multilingual\nand parallel corpora. Our model outperforms the baseline models on various\ncross-lingual understanding tasks with much less computation cost. Moreover,\nanalysis shows that XLM-E tends to obtain better cross-lingual transferability.\n", "rewritten_text": "This paper introduces ELECTRA-style tasks to cross-lingual language model pretraining.  Specifically, we present two novel pretraining tasks: multilingual replaced token detection and translation replaced token detection.  We pretrained the resulting model, XLM-E, on both multilingual and parallel corpora.  XLM-E outperforms baseline models on various cross-lingual understanding tasks, achieving significant computational efficiency.  Furthermore, analysis indicates that XLM-E exhibits improved cross-lingual transferability.\n"}, "2101.03929": {"original_text": "  Learning to capture dependencies between spatial positions is essential to\nmany visual tasks, especially the dense labeling problems like scene parsing.\nExisting methods can effectively capture long-range dependencies with\nself-attention mechanism while short ones by local convolution. However, there\nis still much gap between long-range and short-range dependencies, which\nlargely reduces the models' flexibility in application to diverse spatial\nscales and relationships in complicated natural scene images. To fill such a\ngap, we develop a Middle-Range (MR) branch to capture middle-range dependencies\nby restricting self-attention into local patches. Also, we observe that the\nspatial regions which have large correlations with others can be emphasized to\nexploit long-range dependencies more accurately, and thus propose a Reweighed\nLong-Range (RLR) branch. Based on the proposed MR and RLR branches, we build an\nOmni-Range Dependencies Network (ORDNet) which can effectively capture short-,\nmiddle- and long-range dependencies. Our ORDNet is able to extract more\ncomprehensive context information and well adapt to complex spatial variance in\nscene images. Extensive experiments show that our proposed ORDNet outperforms\nprevious state-of-the-art methods on three scene parsing benchmarks including\nPASCAL Context, COCO Stuff and ADE20K, demonstrating the superiority of\ncapturing omni-range dependencies in deep models for scene parsing task.\n", "rewritten_text": "Capturing dependencies between spatial positions is crucial for many visual tasks, particularly dense labeling problems such as scene parsing.  Existing methods effectively capture long-range dependencies using self-attention mechanisms and short-range dependencies using local convolutions. However, a significant gap remains between these ranges, limiting model flexibility when applied to diverse spatial scales and relationships in complex natural scenes.  To bridge this gap, we introduce a Middle-Range (MR) branch that captures middle-range dependencies by restricting self-attention to local patches.  Furthermore, we propose a Reweighted Long-Range (RLR) branch that emphasizes spatially correlated regions to more accurately exploit long-range dependencies.  These MR and RLR branches form the basis of our Omni-Range Dependencies Network (ORDNet), which effectively captures short-, middle-, and long-range dependencies.  ORDNet extracts more comprehensive contextual information and adapts well to complex spatial variations in scene images.  Extensive experiments on three scene parsing benchmarks\u2014PASCAL Context, COCO Stuff, and ADE20K\u2014demonstrate that ORDNet outperforms state-of-the-art methods, highlighting the superiority of capturing omni-range dependencies in deep models for scene parsing.\n"}, "2012.08514": {"original_text": "  In this paper, we propose an end-end model for producing furniture layout for\ninterior scene synthesis from the random vector. This proposed model is aimed\nto support professional interior designers to produce the interior decoration\nsolutions more quickly. The proposed model combines a conditional floor-plan\nmodule of the room, a conditional graphical floor-plan module of the room and a\nconditional layout module. As compared with the prior work on scene synthesis,\nour proposed three modules enhance the ability of auto-layout generation given\nthe dimensional category of the room. We conduct our experiments on the\nproposed real-world interior layout dataset that contains $191208$ designs from\nthe professional designers. Our numerical results demonstrate that the proposed\nmodel yields higher-quality layouts in comparison with the state-of-the-art\nmodel. The dataset and code are released\n\\href{https://github.com/CODE-SUBMIT/dataset3}{Dataset,Code}\n", "rewritten_text": "This paper proposes an end-to-end model for generating furniture layouts for interior scene synthesis from a random vector.  This model aims to accelerate the interior design process for professionals.  It combines three modules: a conditional floor plan module, a conditional graphical floor plan module, and a conditional furniture layout module. Compared to prior scene synthesis work, our three-module approach significantly improves automatic layout generation, particularly given the room's dimensional category.  We evaluated our model on a new dataset of 191,208 real-world interior designs from professional designers.  Our results demonstrate that the proposed model generates higher-quality layouts than state-of-the-art methods. The dataset and code are available at [https://github.com/CODE-SUBMIT/dataset3](https://github.com/CODE-SUBMIT/dataset3).\n"}, "2004.14525": {"original_text": "  Inverted bottleneck layers, which are built upon depthwise convolutions, have\nbeen the predominant building blocks in state-of-the-art object detection\nmodels on mobile devices. In this work, we investigate the optimality of this\ndesign pattern over a broad range of mobile accelerators by revisiting the\nusefulness of regular convolutions. We discover that regular convolutions are a\npotent component to boost the latency-accuracy trade-off for object detection\non accelerators, provided that they are placed strategically in the network via\nneural architecture search. By incorporating regular convolutions in the search\nspace and directly optimizing the network architectures for object detection,\nwe obtain a family of object detection models, MobileDets, that achieve\nstate-of-the-art results across mobile accelerators. On the COCO object\ndetection task, MobileDets outperform MobileNetV3+SSDLite by 1.7 mAP at\ncomparable mobile CPU inference latencies. MobileDets also outperform\nMobileNetV2+SSDLite by 1.9 mAP on mobile CPUs, 3.7 mAP on Google EdgeTPU, 3.4\nmAP on Qualcomm Hexagon DSP and 2.7 mAP on Nvidia Jetson GPU without increasing\nlatency. Moreover, MobileDets are comparable with the state-of-the-art MnasFPN\non mobile CPUs even without using the feature pyramid, and achieve better mAP\nscores on both EdgeTPUs and DSPs with up to 2x speedup. Code and models are\navailable in the TensorFlow Object Detection API:\nhttps://github.com/tensorflow/models/tree/master/research/object_detection.\n", "rewritten_text": "Inverted bottleneck layers, based on depthwise convolutions, have been the standard building block in state-of-the-art mobile object detection models.  This work challenges the optimality of this design by exploring the potential of regular convolutions.  We find that strategically placed regular convolutions significantly improve the latency-accuracy trade-off for object detection on various mobile accelerators, as determined through neural architecture search.  This search, incorporating regular convolutions, yielded a family of object detection models, MobileDets, achieving state-of-the-art performance across multiple platforms.  On the COCO object detection benchmark, MobileDets surpasses MobileNetV3+SSDLite by 1.7 mAP at comparable mobile CPU inference latency and MobileNetV2+SSDLite by 1.9 mAP on mobile CPUs, 3.7 mAP on Google EdgeTPU, 3.4 mAP on Qualcomm Hexagon DSP, and 2.7 mAP on Nvidia Jetson GPU, all without increased latency.  Furthermore, MobileDets achieve comparable performance to MnasFPN on mobile CPUs (even without a feature pyramid) and superior mAP scores on EdgeTPUs and DSPs, with up to a 2x speedup.  Code and models are available via the TensorFlow Object Detection API: [https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection).\n"}, "2310.00031": {"original_text": "  Diffusion models are generative models with impressive text-to-image\nsynthesis capabilities and have spurred a new wave of creative methods for\nclassical machine learning tasks. However, the best way to harness the\nperceptual knowledge of these generative models for visual tasks is still an\nopen question. Specifically, it is unclear how to use the prompting interface\nwhen applying diffusion backbones to vision tasks. We find that automatically\ngenerated captions can improve text-image alignment and significantly enhance a\nmodel's cross-attention maps, leading to better perceptual performance. Our\napproach improves upon the current state-of-the-art (SOTA) in diffusion-based\nsemantic segmentation on ADE20K and the current overall SOTA for depth\nestimation on NYUv2. Furthermore, our method generalizes to the cross-domain\nsetting. We use model personalization and caption modifications to align our\nmodel to the target domain and find improvements over unaligned baselines. Our\ncross-domain object detection model, trained on Pascal VOC, achieves SOTA\nresults on Watercolor2K. Our cross-domain segmentation method, trained on\nCityscapes, achieves SOTA results on Dark Zurich-val and Nighttime Driving.\nProject page: https://www.vision.caltech.edu/tadp/. Code:\nhttps://github.com/damaggu/TADP.\n", "rewritten_text": "Diffusion models, with their impressive text-to-image synthesis capabilities, have revolutionized creative methods in classical machine learning.  However, optimally leveraging their perceptual knowledge for visual tasks remains an open challenge, particularly regarding effective prompt engineering when integrating diffusion backbones.  We demonstrate that automatically generated captions improve text-image alignment and significantly enhance cross-attention maps, leading to superior perceptual performance.  Our approach achieves state-of-the-art (SOTA) results in diffusion-based semantic segmentation on ADE20K and overall SOTA for depth estimation on NYUv2.  Furthermore, it generalizes effectively to cross-domain settings.  Through model personalization and caption modifications, we achieve improved performance compared to unaligned baselines.  Specifically, our cross-domain object detection model, trained on Pascal VOC, achieves SOTA results on Watercolor2K, and our cross-domain segmentation model, trained on Cityscapes, achieves SOTA results on Dark Zurich-val and Nighttime Driving.\n\nProject page: https://www.vision.caltech.edu/tadp/\nCode: https://github.com/damaggu/TADP\n"}, "2402.15930": {"original_text": "  The writing examples of English language learners may be different from those\nof native speakers. Given that there is a significant differences in second\nlanguage (L2) learners' error types by their proficiency levels, this paper\nattempts to reduce overcorrection by examining the interaction between LLM's\nperformance and L2 language proficiency. Our method focuses on zero-shot and\nfew-shot prompting and fine-tuning models for GEC for learners of English as a\nforeign language based on the different proficiency. We investigate GEC results\nand find that overcorrection happens primarily in advanced language learners'\nwriting (proficiency C) rather than proficiency A (a beginner level) and\nproficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot\nprompting with writing examples of English learners, actually tend to exhibit\ndecreased recall measures. To make our claim concrete, we conduct a\ncomprehensive examination of GEC outcomes and their evaluation results based on\nlanguage proficiency.\n", "rewritten_text": "English language learners (ELLs) produce writing that differs from that of native speakers.  Because second language (L2) learners exhibit error types that vary significantly with proficiency level, this paper aims to mitigate overcorrection in Grammatical Error Correction (GEC) by analyzing the interaction between large language model (LLM) performance and L2 proficiency.  Our methodology employs zero-shot and few-shot prompting, as well as fine-tuning LLMs for GEC on ELL writing samples categorized by proficiency level (A: beginner, B: intermediate, C: advanced).  Our findings indicate that overcorrection primarily affects the writing of advanced learners (proficiency C), not beginners (A) or intermediate learners (B).  Furthermore, fine-tuned LLMs, and even few-shot prompting with ELL writing examples, often demonstrate reduced recall.  To substantiate our claims, we present a comprehensive analysis of GEC outcomes and their evaluation metrics across proficiency levels.\n"}, "2306.10963": {"original_text": "  Adversarial patches are still a simple yet powerful white box attack that can\nbe used to fool object detectors by suppressing possible detections. The\npatches of these so-called evasion attacks are computational expensive to\nproduce and require full access to the attacked detector. This paper addresses\nthe problem of computational expensiveness by analyzing 375 generated patches,\ncalculating the principal components of these and show, that linear\ncombinations of the resulting \"eigenpatches\" can be used to fool object\ndetections successfully.\n", "rewritten_text": "Adversarial patches remain a simple yet powerful white-box attack capable of fooling object detectors by suppressing detections.  While effective, these evasion attacks are computationally expensive to generate and require complete access to the target detector. This paper addresses the computational cost by analyzing 375 generated patches.  We calculate their principal components and demonstrate that linear combinations of the resulting \"eigenpatches\" can successfully fool object detection.\n"}, "1806.03028": {"original_text": "  Vehicle Make and Model Recognition (MMR) systems provide a fully automatic\nframework to recognize and classify different vehicle models. Several\napproaches have been proposed to address this challenge, however they can\nperform in restricted conditions. Here, we formulate the vehicle make and model\nrecognition as a fine-grained classification problem and propose a new\nconfigurable on-road vehicle make and model recognition framework. We benefit\nfrom the unsupervised feature learning methods and in more details we employ\nLocality constraint Linear Coding (LLC) method as a fast feature encoder for\nencoding the input SIFT features. The proposed method can perform in real\nenvironments of different conditions. This framework can recognize fifty models\nof vehicles and has an advantage to classify every other vehicle not belonging\nto one of the specified fifty classes as an unknown vehicle. The proposed MMR\nframework can be configured to become faster or more accurate based on the\napplication domain. The proposed approach is examined on two datasets including\nIranian on-road vehicle dataset and CompuCar dataset. The Iranian on-road\nvehicle dataset contains images of 50 models of vehicles captured in real\nsituations by traffic cameras in different weather and lighting conditions.\nExperimental results show superiority of the proposed framework over the\nstate-of-the-art methods on Iranian on-road vehicle datatset and comparable\nresults on CompuCar dataset with 97.5% and 98.4% accuracies, respectively.\n", "rewritten_text": "Make and Model Recognition (MMR) systems offer fully automated frameworks for classifying vehicles.  While existing approaches often perform poorly under challenging conditions, this paper presents a novel, configurable MMR framework formulated as a fine-grained classification problem.  Leveraging unsupervised feature learning, specifically Locality-constrained Linear Coding (LLC), we efficiently encode input Scale-Invariant Feature Transform (SIFT) features.  This framework operates effectively in diverse real-world environments, identifying 50 specific vehicle models and classifying any remaining vehicles as \"unknown.\"  Its configurability allows for a trade-off between speed and accuracy depending on the application.\n\nThe framework was evaluated on two datasets: an Iranian on-road vehicle dataset and the CompuCar dataset. The Iranian dataset comprises images of 50 vehicle models captured under varying weather and lighting conditions by traffic cameras.  Experimental results demonstrate superior performance compared to state-of-the-art methods on the Iranian dataset (97.5% accuracy) and comparable results on the CompuCar dataset (98.4% accuracy).\n"}, "2110.10575": {"original_text": "  Recent research in opinion mining proposed word embedding-based topic\nmodeling methods that provide superior coherence compared to traditional topic\nmodeling. In this paper, we demonstrate how these methods can be used to\ndisplay correlated topic models on social media texts using SocialVisTUM, our\nproposed interactive visualization toolkit. It displays a graph with topics as\nnodes and their correlations as edges. Further details are displayed\ninteractively to support the exploration of large text collections, e.g.,\nrepresentative words and sentences of topics, topic and sentiment\ndistributions, hierarchical topic clustering, and customizable, predefined\ntopic labels. The toolkit optimizes automatically on custom data for optimal\ncoherence. We show a working instance of the toolkit on data crawled from\nEnglish social media discussions about organic food consumption. The\nvisualization confirms findings of a qualitative consumer research study.\nSocialVisTUM and its training procedures are accessible online.\n", "rewritten_text": "Recent research in opinion mining has shown that word embedding-based topic modeling methods offer superior coherence compared to traditional approaches.  This paper presents SocialVisTUM, an interactive visualization toolkit that leverages these methods to display correlated topic models from social media text.  SocialVisTUM visualizes topics as nodes in a graph, with edges representing correlations.  Interactive features facilitate exploration of large text corpora, including representative words and sentences for each topic, topic and sentiment distributions, hierarchical topic clustering, and customizable topic labels.  The toolkit automatically optimizes for optimal coherence on custom datasets. We demonstrate SocialVisTUM's functionality using data crawled from English-language social media discussions on organic food consumption.  The resulting visualization corroborates findings from a qualitative consumer research study.  SocialVisTUM and its training procedures are publicly available online.\n"}, "2404.00851": {"original_text": "  Pre-trained vision-language models have shown impressive success on various\ncomputer vision tasks with their zero-shot generalizability. Recently, prompt\nlearning approaches have been explored to efficiently and effectively adapt the\nvision-language models to a variety of downstream tasks. However, most existing\nprompt learning methods suffer from task overfitting since the general\nknowledge of the pre-trained vision language models is forgotten while the\nprompts are finetuned on a small data set from a specific target task. To\naddress this issue, we propose a Prompt Meta-Regularization (ProMetaR) to\nimprove the generalizability of prompt learning for vision-language models.\nSpecifically, ProMetaR meta-learns both the regularizer and the soft prompts to\nharness the task-specific knowledge from the downstream tasks and task-agnostic\ngeneral knowledge from the vision-language models. Further, ProMetaR augments\nthe task to generate multiple virtual tasks to alleviate the meta-overfitting.\nIn addition, we provide the analysis to comprehend how ProMetaR improves the\ngeneralizability of prompt tuning in the perspective of the gradient alignment.\nOur extensive experiments demonstrate that our ProMetaR improves the\ngeneralizability of conventional prompt learning methods under\nbase-to-base/base-to-new and domain generalization settings. The code of\nProMetaR is available at https://github.com/mlvlab/ProMetaR.\n", "rewritten_text": "Pre-trained vision-language models (VLMs) demonstrate impressive zero-shot generalizability across various computer vision tasks.  Recent work has explored prompt learning to efficiently adapt VLMs to diverse downstream tasks. However, most existing prompt learning methods suffer from overfitting to the specific target task, neglecting the general knowledge embedded within the pre-trained VLM during finetuning on limited data.  To mitigate this, we propose Prompt Meta-Regularization (ProMetaR), a novel approach to enhance the generalizability of prompt learning for VLMs.  ProMetaR meta-learns both a regularizer and soft prompts, leveraging both task-specific knowledge from downstream tasks and task-agnostic knowledge from the pre-trained VLM.  Furthermore, ProMetaR generates multiple virtual tasks to alleviate meta-overfitting.  We analyze ProMetaR's improved generalizability through the lens of gradient alignment.  Extensive experiments demonstrate ProMetaR's superior generalizability compared to conventional prompt learning methods under base-to-base, base-to-new, and domain generalization settings. The code is available at https://github.com/mlvlab/ProMetaR.\n"}, "2409.12539": {"original_text": "  In radiation therapy (RT), the reliance on pre-treatment computed tomography\n(CT) images encounter challenges due to anatomical changes, necessitating\nadaptive planning. Daily cone-beam CT (CBCT) imaging, pivotal for therapy\nadjustment, falls short in tissue density accuracy. To address this, our\ninnovative approach integrates diffusion models for CT image generation,\noffering precise control over data synthesis. Leveraging a self-training method\nwith knowledge distillation, we maximize CBCT data during therapy, complemented\nby sparse paired fan-beam CTs. This strategy, incorporated into\nstate-of-the-art diffusion-based models, surpasses conventional methods like\nPix2pix and CycleGAN. A meticulously curated dataset of 2800 paired CBCT and CT\nscans, supplemented by 4200 CBCT scans, undergoes preprocessing and teacher\nmodel training, including the Brownian Bridge Diffusion Model (BBDM).\nPseudo-label CT images are generated, resulting in a dataset combining 5600 CT\nimages with corresponding CBCT images. Thorough evaluation using MSE, SSIM,\nPSNR and LPIPS demonstrates superior performance against Pix2pix and CycleGAN.\nOur approach shows promise in generating high-quality CT images from CBCT scans\nin RT.\n", "rewritten_text": "Adaptive radiation therapy (RT) faces challenges due to anatomical changes between pre-treatment computed tomography (CT) scans and daily cone-beam CT (CBCT) images used for treatment adjustments.  CBCT's lower tissue density accuracy limits its effectiveness.  To address this, we propose a novel approach integrating diffusion models for generating high-fidelity CT images from CBCT scans.  This method leverages a self-training strategy with knowledge distillation, maximizing the use of CBCT data acquired throughout the treatment course, supplemented by sparsely acquired paired fan-beam CT scans.  Our approach, implemented using a state-of-the-art diffusion model (specifically, the Brownian Bridge Diffusion Model, or BBDM), outperforms conventional image-to-image translation methods such as Pix2Pix and CycleGAN.\n\nWe trained our model on a meticulously curated dataset comprising 2800 paired CBCT and CT scans, augmented with 4200 additional CBCT scans.  Preprocessing and teacher model training were performed, generating pseudo-labeled CT images.  This resulted in a final dataset of 5600 CT images paired with corresponding CBCT images.  Rigorous evaluation using mean squared error (MSE), structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), and learned perceptual image patch similarity (LPIPS) metrics demonstrated superior performance compared to Pix2Pix and CycleGAN.  Our results suggest that this approach holds significant promise for generating high-quality CT images from CBCT scans in adaptive RT.\n"}, "1904.0169": {"original_text": "  We present MonoPSR, a monocular 3D object detection method that leverages\nproposals and shape reconstruction. First, using the fundamental relations of a\npinhole camera model, detections from a mature 2D object detector are used to\ngenerate a 3D proposal per object in a scene. The 3D location of these\nproposals prove to be quite accurate, which greatly reduces the difficulty of\nregressing the final 3D bounding box detection. Simultaneously, a point cloud\nis predicted in an object centered coordinate system to learn local scale and\nshape information. However, the key challenge is how to exploit shape\ninformation to guide 3D localization. As such, we devise aggregate losses,\nincluding a novel projection alignment loss, to jointly optimize these tasks in\nthe neural network to improve 3D localization accuracy. We validate our method\non the KITTI benchmark where we set new state-of-the-art results among\npublished monocular methods, including the harder pedestrian and cyclist\nclasses, while maintaining efficient run-time.\n", "rewritten_text": "MonoPSR is a novel monocular 3D object detection method that leverages proposals and shape reconstruction.  It first uses a mature 2D object detector and the pinhole camera model to generate accurate 3D proposals for each object in a scene.  These proposals significantly simplify the regression of the final 3D bounding boxes.  Concurrently, a point cloud is predicted in an object-centric coordinate system to capture local scale and shape information.  To effectively utilize this shape information for improved 3D localization, we introduce aggregate losses, including a novel projection alignment loss, to jointly optimize the proposal generation and shape prediction within the neural network.  We demonstrate state-of-the-art performance on the KITTI benchmark, surpassing existing monocular methods, particularly on the challenging pedestrian and cyclist classes, while maintaining computational efficiency.\n"}, "2408.02291": {"original_text": "  Unsupervised 3D keypoints estimation from Point Cloud Data (PCD) is a complex\ntask, even more challenging when an object shape is deforming. As keypoints\nshould be semantically and geometrically consistent across all the 3D frames -\neach keypoint should be anchored to a specific part of the deforming shape\nirrespective of intrinsic and extrinsic motion. This paper presents, \"SelfGeo\",\na self-supervised method that computes persistent 3D keypoints of non-rigid\nobjects from arbitrary PCDs without the need of human annotations. The gist of\nSelfGeo is to estimate keypoints between frames that respect invariant\nproperties of deforming bodies. Our main contribution is to enforce that\nkeypoints deform along with the shape while keeping constant geodesic distances\namong them. This principle is then propagated to the design of a set of losses\nwhich minimization let emerge repeatable keypoints in specific semantic\nlocations of the non-rigid shape. We show experimentally that the use of\ngeodesic has a clear advantage in challenging dynamic scenes and with different\nclasses of deforming shapes (humans and animals). Code and data are available\nat: https://github.com/IIT-PAVIS/SelfGeo\n", "rewritten_text": "Unsupervised 3D keypoint estimation from point cloud data (PCD) is a challenging task, particularly for deforming objects.  Keypoints must maintain semantic and geometric consistency across all 3D frames, remaining anchored to specific object parts regardless of intrinsic or extrinsic motion. This paper introduces SelfGeo, a self-supervised method for computing persistent 3D keypoints on non-rigid objects from arbitrary PCD sequences without human annotation.  SelfGeo estimates keypoints between frames by enforcing invariant properties of deforming bodies.  Our key contribution is a constraint that keypoints deform with the shape while maintaining constant geodesic distances between them. This principle underpins a novel loss function, whose minimization yields repeatable keypoints at semantically meaningful locations on the non-rigid shape.  Experimental results demonstrate the advantages of using geodesic distances in challenging dynamic scenes with diverse deforming shapes (e.g., humans and animals). Code and data are available at: https://github.com/IIT-PAVIS/SelfGeo\n"}, "2304.0723": {"original_text": "  Pedestrian attribute recognition (PAR) has received increasing attention\nbecause of its wide application in video surveillance and pedestrian analysis.\nExtracting robust feature representation is one of the key challenges in this\ntask. The existing methods mainly use the convolutional neural network (CNN) as\nthe backbone network to extract features. However, these methods mainly focus\non small discriminative regions while ignoring the global perspective. To\novercome these limitations, we propose a pure transformer-based multi-task PAR\nnetwork named PARFormer, which includes four modules. In the feature extraction\nmodule, we build a transformer-based strong baseline for feature extraction,\nwhich achieves competitive results on several PAR benchmarks compared with the\nexisting CNN-based baseline methods. In the feature processing module, we\npropose an effective data augmentation strategy named batch random mask (BRM)\nblock to reinforce the attentive feature learning of random patches.\nFurthermore, we propose a multi-attribute center loss (MACL) to enhance the\ninter-attribute discriminability in the feature representations. In the\nviewpoint perception module, we explore the impact of viewpoints on pedestrian\nattributes, and propose a multi-view contrastive loss (MCVL) that enables the\nnetwork to exploit the viewpoint information. In the attribute recognition\nmodule, we alleviate the negative-positive imbalance problem to generate the\nattribute predictions. The above modules interact and jointly learn a highly\ndiscriminative feature space, and supervise the generation of the final\nfeatures. Extensive experimental results show that the proposed PARFormer\nnetwork performs well compared to the state-of-the-art methods on several\npublic datasets, including PETA, RAP, and PA100K. Code will be released at\nhttps://github.com/xwf199/PARFormer.\n", "rewritten_text": "Pedestrian attribute recognition (PAR) has gained significant attention due to its widespread applications in video surveillance and pedestrian analysis.  A key challenge in PAR is extracting robust feature representations. While existing methods predominantly utilize convolutional neural networks (CNNs) as backbone networks, they often focus on small, discriminative regions, neglecting global context.  To address this limitation, we propose PARFormer, a novel, purely transformer-based multi-task PAR network comprising four modules.  The feature extraction module establishes a strong transformer-based baseline, achieving competitive results on several PAR benchmarks against existing CNN-based methods.  The feature processing module incorporates a novel data augmentation strategy, the batch random mask (BRM) block, to enhance attentive feature learning from randomly masked patches.  Furthermore, a multi-attribute center loss (MACL) improves inter-attribute discriminability in the feature representations.  The viewpoint perception module investigates the impact of viewpoint on pedestrian attributes, introducing a multi-view contrastive loss (MCVL) to leverage viewpoint information. Finally, the attribute recognition module mitigates class imbalance in generating attribute predictions.  These modules interact, jointly learning a highly discriminative feature space and guiding the generation of final features.  Extensive experiments on public datasets, including PETA, RAP, and PA100K, demonstrate PARFormer's state-of-the-art performance.  The code is available at https://github.com/xwf199/PARFormer.\n"}, "2402.09967": {"original_text": "  Large Language Models (LLMs) excel in generating personalized content and\nfacilitating interactive dialogues, showcasing their remarkable aptitude for a\nmyriad of applications. However, their capabilities in reasoning and providing\nexplainable outputs, especially within the context of reasoning abilities,\nremain areas for improvement. In this study, we delve into the reasoning\nabilities of LLMs, highlighting the current challenges and limitations that\nhinder their effectiveness in complex reasoning scenarios.\n", "rewritten_text": "Large language models (LLMs) excel at generating personalized content and facilitating interactive dialogues, demonstrating their remarkable versatility across numerous applications.  However, their reasoning capabilities and ability to provide explainable outputs remain significant areas for improvement. This study investigates the reasoning abilities of LLMs, highlighting current challenges and limitations that hinder their effectiveness in complex reasoning tasks.\n"}, "2212.05911": {"original_text": "  Deep learning has emerged as an effective solution for solving the task of\nobject detection in images but at the cost of requiring large labeled datasets.\nTo mitigate this cost, semi-supervised object detection methods, which consist\nin leveraging abundant unlabeled data, have been proposed and have already\nshown impressive results. However, most of these methods require linking a\npseudo-label to a ground-truth object by thresholding. In previous works, this\nthreshold value is usually determined empirically, which is time consuming, and\nonly done for a single data distribution. When the domain, and thus the data\ndistribution, changes, a new and costly parameter search is necessary. In this\nwork, we introduce our method Adaptive Self-Training for Object Detection\n(ASTOD), which is a simple yet effective teacher-student method. ASTOD\ndetermines without cost a threshold value based directly on the ground value of\nthe score histogram. To improve the quality of the teacher predictions, we also\npropose a novel pseudo-labeling procedure. We use different views of the\nunlabeled images during the pseudo-labeling step to reduce the number of missed\npredictions and thus obtain better candidate labels. Our teacher and our\nstudent are trained separately, and our method can be used in an iterative\nfashion by replacing the teacher by the student. On the MS-COCO dataset, our\nmethod consistently performs favorably against state-of-the-art methods that do\nnot require a threshold parameter, and shows competitive results with methods\nthat require a parameter sweep search. Additional experiments with respect to a\nsupervised baseline on the DIOR dataset containing satellite images lead to\nsimilar conclusions, and prove that it is possible to adapt the score threshold\nautomatically in self-training, regardless of the data distribution. The code\nis available at https:// github.com/rvandeghen/ASTOD\n", "rewritten_text": "Deep learning effectively solves image object detection, but requires large labeled datasets.  To mitigate this, semi-supervised methods leveraging abundant unlabeled data have shown promising results. However, most rely on thresholding to link pseudo-labels to ground truth, a process typically involving time-consuming empirical threshold selection for a single data distribution.  Domain shifts necessitate costly parameter re-tuning.\n\nThis work introduces Adaptive Self-Training for Object Detection (ASTOD), a simple yet effective teacher-student method.  ASTOD cost-effectively determines the threshold directly from the ground truth score histogram.  Furthermore, a novel pseudo-labeling procedure improves teacher predictions by using multiple views of unlabeled images, reducing missed predictions and yielding higher-quality candidate labels.  The teacher and student are trained separately, and the method iteratively replaces the teacher with the student.\n\nOn the MS-COCO dataset, ASTOD consistently outperforms state-of-the-art methods that do not require threshold parameter tuning and achieves competitive results with those requiring parameter sweeps.  Experiments on the DIOR satellite imagery dataset, compared to a supervised baseline, yield similar conclusions, demonstrating the adaptability of the automatic score thresholding in self-training across diverse data distributions.  The code is available at [https://github.com/rvandeghen/ASTOD](https://github.com/rvandeghen/ASTOD).\n"}, "1507.02772": {"original_text": "  Data encoded as symmetric positive definite (SPD) matrices frequently arise\nin many areas of computer vision and machine learning. While these matrices\nform an open subset of the Euclidean space of symmetric matrices, viewing them\nthrough the lens of non-Euclidean Riemannian geometry often turns out to be\nbetter suited in capturing several desirable data properties. However,\nformulating classical machine learning algorithms within such a geometry is\noften non-trivial and computationally expensive. Inspired by the great success\nof dictionary learning and sparse coding for vector-valued data, our goal in\nthis paper is to represent data in the form of SPD matrices as sparse conic\ncombinations of SPD atoms from a learned dictionary via a Riemannian geometric\napproach. To that end, we formulate a novel Riemannian optimization objective\nfor dictionary learning and sparse coding in which the representation loss is\ncharacterized via the affine invariant Riemannian metric. We also present a\ncomputationally simple algorithm for optimizing our model. Experiments on\nseveral computer vision datasets demonstrate superior classification and\nretrieval performance using our approach when compared to sparse coding via\nalternative non-Riemannian formulations.\n", "rewritten_text": "Symmetric positive definite (SPD) matrices are prevalent in computer vision and machine learning.  Although they form an open subset of the Euclidean space of symmetric matrices, a Riemannian geometric approach often better captures their inherent properties.  However, adapting classical machine learning algorithms to this non-Euclidean geometry presents significant computational challenges.  Motivated by the success of dictionary learning and sparse coding for vector data, this paper proposes a novel method to represent SPD matrix data as sparse conic combinations of SPD atoms from a learned dictionary.  This is achieved using a Riemannian geometric approach and a new Riemannian optimization objective function, where the representation loss is measured using the affine-invariant Riemannian metric.  We also present a computationally efficient algorithm for optimizing this objective.  Experiments on several computer vision datasets demonstrate that our approach achieves superior classification and retrieval performance compared to alternative non-Riemannian sparse coding methods.\n"}, "1603.09742": {"original_text": "  Semantic segmentation is critical to image content understanding and object\nlocalization. Recent development in fully-convolutional neural network (FCN)\nhas enabled accurate pixel-level labeling. One issue in previous works is that\nthe FCN based method does not exploit the object boundary information to\ndelineate segmentation details since the object boundary label is ignored in\nthe network training. To tackle this problem, we introduce a double branch\nfully convolutional neural network, which separates the learning of the\ndesirable semantic class labeling with mask-level object proposals guided by\nrelabeled boundaries. This network, called object boundary guided FCN\n(OBG-FCN), is able to integrate the distinct properties of object shape and\nclass features elegantly in a fully convolutional way with a designed masking\narchitecture. We conduct experiments on the PASCAL VOC segmentation benchmark,\nand show that the end-to-end trainable OBG-FCN system offers great improvement\nin optimizing the target semantic segmentation quality.\n", "rewritten_text": "Semantic segmentation is crucial for image understanding and object localization.  Recent advances in fully convolutional neural networks (FCNs) have enabled accurate pixel-level labeling. However, previous FCN-based methods have neglected object boundary information, hindering detailed segmentation.  To address this, we propose a novel double-branch fully convolutional network that separates the learning of semantic class labels from mask-level object proposals guided by re-labeled boundaries. This network, termed Object Boundary Guided FCN (OBG-FCN), elegantly integrates object shape and class features within a fully convolutional architecture using a designed masking mechanism.  Experiments on the PASCAL VOC segmentation benchmark demonstrate that the end-to-end trainable OBG-FCN significantly improves semantic segmentation performance.\n"}, "2004.03677": {"original_text": "  Image manipulation can be considered a special case of image generation where\nthe image to be produced is a modification of an existing image. Image\ngeneration and manipulation have been, for the most part, tasks that operate on\nraw pixels. However, the remarkable progress in learning rich image and object\nrepresentations has opened the way for tasks such as text-to-image or\nlayout-to-image generation that are mainly driven by semantics. In our work, we\naddress the novel problem of image manipulation from scene graphs, in which a\nuser can edit images by merely applying changes in the nodes or edges of a\nsemantic graph that is generated from the image. Our goal is to encode image\ninformation in a given constellation and from there on generate new\nconstellations, such as replacing objects or even changing relationships\nbetween objects, while respecting the semantics and style from the original\nimage. We introduce a spatio-semantic scene graph network that does not require\ndirect supervision for constellation changes or image edits. This makes it\npossible to train the system from existing real-world datasets with no\nadditional annotation effort.\n", "rewritten_text": "Image manipulation can be viewed as a specialized form of image generation where the output image is a modification of an existing one.  Traditionally, both image generation and manipulation operated primarily on raw pixel data. However, recent advancements in learning rich image and object representations have enabled semantically driven tasks, such as text-to-image or layout-to-image generation.  This work addresses the novel problem of image manipulation from scene graphs, allowing users to edit images by modifying the nodes or edges of a semantically generated graph representing the image.  Our goal is to encode image information within a scene graph representation and then generate new representations\u2014for example, replacing objects or altering object relationships\u2014while preserving the original image's semantics and style. We introduce a spatio-semantic scene graph network that requires no direct supervision for constellation changes or image edits, enabling training on existing real-world datasets without additional annotation.\n"}, "2208.09669": {"original_text": "  Contextualized word embeddings in language models have given much advance to\nNLP. Intuitively, sentential information is integrated into the representation\nof words, which can help model polysemy. However, context sensitivity also\nleads to the variance of representations, which may break the semantic\nconsistency for synonyms. We quantify how much the contextualized embeddings of\neach word sense vary across contexts in typical pre-trained models. Results\nshow that contextualized embeddings can be highly consistent across contexts.\nIn addition, part-of-speech, number of word senses, and sentence length have an\ninfluence on the variance of sense representations. Interestingly, we find that\nword representations are position-biased, where the first words in different\ncontexts tend to be more similar. We analyze such a phenomenon and also propose\na simple way to alleviate such bias in distance-based word sense disambiguation\nsettings.\n", "rewritten_text": "Contextualized word embeddings in language models have significantly advanced natural language processing (NLP).  These embeddings integrate sentential information into word representations, thereby improving the modeling of polysemy. However, this context sensitivity can also lead to representational variance, potentially compromising the semantic consistency of synonyms.  We quantified the contextual variation in word sense embeddings across contexts in several typical pre-trained models.  Our results demonstrate that contextualized embeddings can exhibit high consistency across contexts.  Furthermore, we found that part-of-speech, the number of word senses, and sentence length influence the variance of sense representations.  Interestingly, we observed a position bias in word representations, with the first words in different contexts tending to be more similar. We analyze this phenomenon and propose a simple method to mitigate this bias in distance-based word sense disambiguation.\n"}, "1706.00842": {"original_text": "  We present a fully automatic method employing convolutional neural networks\nbased on the 2D U-net architecture and random forest classifier to solve the\nautomatic liver lesion segmentation problem of the ISBI 2017 Liver Tumor\nSegmentation Challenge (LiTS). In order to constrain the ROI in which the\ntumors could be located, a liver segmentation is performed first. For the organ\nsegmentation, an ensemble of convolutional networks is trained to segment a\nliver using a set of 179 liver CT datasets from liver surgery planning. Inside\nof the liver ROI a neural network, trained using 127 challenge training\ndatasets, identifies tumor candidates, which are subsequently filtered with a\nrandom forest classifier yielding the final tumor segmentation. The evaluation\non the 70 challenge test cases resulted in a mean Dice coefficient of 0.65,\nranking our method in the second place.\n", "rewritten_text": "We present a fully automated method for liver lesion segmentation in the ISBI 2017 Liver Tumor Segmentation Challenge (LiTS).  This method uses a 2D U-Net convolutional neural network and a random forest classifier.  First, an ensemble of convolutional neural networks, trained on 179 liver CT datasets from liver surgery planning, segments the liver to define a region of interest (ROI).  Within this ROI, a second convolutional neural network, trained on 127 LiTS training datasets, identifies tumor candidates.  A random forest classifier then filters these candidates, producing the final tumor segmentation.  Evaluation on the 70 LiTS test cases yielded a mean Dice coefficient of 0.65, achieving second place in the challenge.\n"}, "2108.07848": {"original_text": "  Identifying players in sports videos by recognizing their jersey numbers is a\nchallenging task in computer vision. We have designed and implemented a\nmulti-task learning network for jersey number recognition. In order to train a\nnetwork to recognize jersey numbers, two output label representations are used\n(1) Holistic - considers the entire jersey number as one class, and (2)\nDigit-wise - considers the two digits in a jersey number as two separate\nclasses. The proposed network learns both holistic and digit-wise\nrepresentations through a multi-task loss function. We determine the optimal\nweights to be assigned to holistic and digit-wise losses through an ablation\nstudy. Experimental results demonstrate that the proposed multi-task learning\nnetwork performs better than the constituent holistic and digit-wise\nsingle-task learning networks.\n", "rewritten_text": "Identifying players in sports videos via jersey number recognition presents a significant challenge in computer vision.  We address this challenge with a novel multi-task learning network.  This network employs two output label representations for training: (1) holistic, treating the entire jersey number as a single class, and (2) digit-wise, treating each digit as an independent class.  The network learns both representations simultaneously using a multi-task loss function, with optimal loss weights determined via ablation study.  Experimental results demonstrate that our multi-task approach outperforms both holistic and digit-wise single-task baselines.\n"}, "2104.01136": {"original_text": "  We design a family of image classification architectures that optimize the\ntrade-off between accuracy and efficiency in a high-speed regime. Our work\nexploits recent findings in attention-based architectures, which are\ncompetitive on highly parallel processing hardware. We revisit principles from\nthe extensive literature on convolutional neural networks to apply them to\ntransformers, in particular activation maps with decreasing resolutions. We\nalso introduce the attention bias, a new way to integrate positional\ninformation in vision transformers. As a result, we propose LeVIT: a hybrid\nneural network for fast inference image classification. We consider different\nmeasures of efficiency on different hardware platforms, so as to best reflect a\nwide range of application scenarios. Our extensive experiments empirically\nvalidate our technical choices and show they are suitable to most\narchitectures. Overall, LeViT significantly outperforms existing convnets and\nvision transformers with respect to the speed/accuracy tradeoff. For example,\nat 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on\nCPU. We release the code at https://github.com/facebookresearch/LeViT\n", "rewritten_text": "We present LeViT, a novel hybrid neural network architecture for fast image classification, designed to optimize the accuracy-efficiency trade-off in high-speed settings.  Leveraging recent advances in attention-based architectures and drawing inspiration from convolutional neural networks (CNNs), particularly the use of progressively decreasing resolution activation maps, we adapt these principles to transformers.  We further introduce attention bias, a new mechanism for integrating positional information within vision transformers.  Our approach considers efficiency across diverse hardware platforms to ensure broad applicability.  Extensive experiments demonstrate the effectiveness of our design choices, showing LeViT significantly outperforms existing CNNs and vision transformers in terms of speed-accuracy trade-off.  For instance, at 80% ImageNet top-1 accuracy, LeViT achieves 5x faster inference than EfficientNet on a CPU.  The code is publicly available at https://github.com/facebookresearch/LeViT.\n"}, "2103.02984": {"original_text": "  Abrupt motion of camera or objects in a scene result in a blurry video, and\ntherefore recovering high quality video requires two types of enhancements:\nvisual enhancement and temporal upsampling. A broad range of research attempted\nto recover clean frames from blurred image sequences or temporally upsample\nframes by interpolation, yet there are very limited studies handling both\nproblems jointly. In this work, we present a novel framework for deblurring,\ninterpolating and extrapolating sharp frames from a motion-blurred video in an\nend-to-end manner. We design our framework by first learning the pixel-level\nmotion that caused the blur from the given inputs via optical flow estimation\nand then predict multiple clean frames by warping the decoded features with the\nestimated flows. To ensure temporal coherence across predicted frames and\naddress potential temporal ambiguity, we propose a simple, yet effective\nflow-based rule. The effectiveness and favorability of our approach are\nhighlighted through extensive qualitative and quantitative evaluations on\nmotion-blurred datasets from high speed videos.\n", "rewritten_text": "Abrupt camera or object motion results in blurry video.  Recovering high-quality video therefore requires two enhancements: visual enhancement and temporal upsampling. While extensive research has addressed recovering clean frames from blurred sequences or upsampling frames via interpolation, few studies jointly tackle both problems.  This work presents a novel end-to-end framework for deblurring, interpolating, and extrapolating sharp frames from motion-blurred video.  Our framework first learns the pixel-level motion causing the blur via optical flow estimation.  It then predicts multiple clean frames by warping decoded features with the estimated flows.  To ensure temporal coherence and address potential ambiguity, we propose a simple yet effective flow-based rule.  Extensive qualitative and quantitative evaluations on motion-blurred high-speed video datasets demonstrate the effectiveness of our approach.\n"}, "1611.09932": {"original_text": "  Compared to earlier multistage frameworks using CNN features, recent\nend-to-end deep approaches for fine-grained recognition essentially enhance the\nmid-level learning capability of CNNs. Previous approaches achieve this by\nintroducing an auxiliary network to infuse localization information into the\nmain classification network, or a sophisticated feature encoding method to\ncapture higher order feature statistics. We show that mid-level representation\nlearning can be enhanced within the CNN framework, by learning a bank of\nconvolutional filters that capture class-specific discriminative patches\nwithout extra part or bounding box annotations. Such a filter bank is well\nstructured, properly initialized and discriminatively learned through a novel\nasymmetric multi-stream architecture with convolutional filter supervision and\na non-random layer initialization. Experimental results show that our approach\nachieves state-of-the-art on three publicly available fine-grained recognition\ndatasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and\nvisualizations are provided to understand our approach.\n", "rewritten_text": "Recent end-to-end deep learning approaches for fine-grained recognition significantly improve upon earlier multi-stage frameworks that utilize convolutional neural network (CNN) features by enhancing the CNN's mid-level learning capabilities.  Previous methods achieved this through auxiliary networks incorporating localization information into the main classification network, or via sophisticated feature encoding methods to capture higher-order statistics.  In contrast, our approach enhances mid-level representation learning within the CNN framework itself by learning a bank of convolutional filters that identify class-specific discriminative patches without requiring additional part or bounding box annotations. This filter bank is well-structured, effectively initialized, and discriminatively trained using a novel asymmetric multi-stream architecture with convolutional filter supervision and non-random layer initialization.  Experimental results demonstrate state-of-the-art performance on three publicly available fine-grained recognition datasets (CUB-200-2011, Stanford Cars, and FGVC-Aircraft).  Ablation studies and visualizations further elucidate the approach.\n"}, "1504.01013": {"original_text": "  Recent advances in semantic image segmentation have mostly been achieved by\ntraining deep convolutional neural networks (CNNs). We show how to improve\nsemantic segmentation through the use of contextual information; specifically,\nwe explore `patch-patch' context between image regions, and `patch-background'\ncontext. For learning from the patch-patch context, we formulate Conditional\nRandom Fields (CRFs) with CNN-based pairwise potential functions to capture\nsemantic correlations between neighboring patches. Efficient piecewise training\nof the proposed deep structured model is then applied to avoid repeated\nexpensive CRF inference for back propagation. For capturing the\npatch-background context, we show that a network design with traditional\nmulti-scale image input and sliding pyramid pooling is effective for improving\nperformance. Our experimental results set new state-of-the-art performance on a\nnumber of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC\n2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an\nintersection-over-union score of 78.0 on the challenging PASCAL VOC 2012\ndataset.\n", "rewritten_text": "Recent advances in semantic image segmentation have largely relied on deep convolutional neural networks (CNNs).  This work improves semantic segmentation by leveraging contextual information, specifically exploring \"patch-patch\" and \"patch-background\" context.  We address \"patch-patch\" context by formulating Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring image patches.  Efficient piecewise training of this deep structured model avoids computationally expensive repeated CRF inference during backpropagation.  For \"patch-background\" context, we demonstrate the effectiveness of a network architecture incorporating multi-scale image input and sliding pyramid pooling.  Our experimental results establish new state-of-the-art performance on several benchmark semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. Notably, we achieve a 78.0% Intersection over Union (IoU) score on the challenging PASCAL VOC 2012 dataset.\n"}, "2106.1262": {"original_text": "  The self-attention-based model, transformer, is recently becoming the leading\nbackbone in the field of computer vision. In spite of the impressive success\nmade by transformers in a variety of vision tasks, it still suffers from heavy\ncomputation and intensive memory costs. To address this limitation, this paper\npresents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).\nWe start by observing a large amount of redundant computation, mainly spent on\nuncorrelated input patches, and then introduce an interpretable module to\ndynamically and gracefully drop these redundant patches. This novel framework\nis then extended to a hierarchical structure, where uncorrelated tokens at\ndifferent stages are gradually removed, resulting in a considerable shrinkage\nof computational cost. We include extensive experiments on both image and video\ntasks, where our method could deliver up to 1.4x speed-up for state-of-the-art\nmodels like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy.\nMore importantly, contrary to other acceleration approaches, our method is\ninherently interpretable with substantial visual evidence, making vision\ntransformer closer to a more human-understandable architecture while being\nlighter. We demonstrate that the interpretability that naturally emerged in our\nframework can outperform the raw attention learned by the original visual\ntransformer, as well as those generated by off-the-shelf interpretation\nmethods, with both qualitative and quantitative results. Project Page:\nhttp://people.csail.mit.edu/bpan/ia-red/.\n", "rewritten_text": "Transformer-based models have recently become the dominant architecture in computer vision.  Despite their impressive performance across various vision tasks, these models suffer from high computational costs and significant memory demands.  To mitigate this, we introduce Interpretability-Aware Redundancy Reduction (IA-RED\u00b2), a framework that identifies and removes redundant computations.  We observe that a substantial portion of computation is wasted on uncorrelated input patches.  IA-RED\u00b2 addresses this by incorporating an interpretable module that dynamically and efficiently discards these patches.  This framework is further extended hierarchically, progressively removing uncorrelated tokens at different processing stages, leading to considerable computational savings.  Extensive experiments on image and video tasks demonstrate speedups of up to 1.4x for state-of-the-art models such as DeiT and TimeSformer, with an accuracy loss of less than 0.7%.  Unlike other acceleration techniques, IA-RED\u00b2 is inherently interpretable, providing strong visual evidence and enhancing the transparency of vision transformers.  Qualitative and quantitative results show that the interpretability inherent in our framework surpasses both the raw attention mechanisms of original vision transformers and the outputs of existing interpretation methods.  Project Page: http://people.csail.mit.edu/bpan/ia-red/\n"}, "1803.03852": {"original_text": "  Tracking the pose of instruments is a central problem in image-guided\nsurgery. For microscopic scenarios, optical coherence tomography (OCT) is\nincreasingly used as an imaging modality. OCT is suitable for accurate pose\nestimation due to its micrometer range resolution and volumetric field of view.\nHowever, OCT image processing is challenging due to speckle noise and\nreflection artifacts in addition to the images' 3D nature. We address pose\nestimation from OCT volume data with a new deep learning-based tracking\nframework. For this purpose, we design a new 3D convolutional neural network\n(CNN) architecture to directly predict the 6D pose of a small marker geometry\nfrom OCT volumes. We use a hexapod robot to automatically acquire labeled data\npoints which we use to train 3D CNN architectures for multi-output regression.\nWe use this setup to provide an in-depth analysis on deep learning-based pose\nestimation from volumes. Specifically, we demonstrate that exploiting volume\ninformation for pose estimation yields higher accuracy than relying on 2D\nrepresentations with depth information. Supporting this observation, we provide\nquantitative and qualitative results that 3D CNNs effectively exploit the depth\nstructure of marker objects. Regarding the deep learning aspect, we present\nefficient design principles for 3D CNNs, making use of insights from the 2D\ndeep learning community. In particular, we present Inception3D as a new\narchitecture which performs best for our application. We show that our deep\nlearning approach reaches errors at our ground-truth label's resolution. We\nachieve a mean average error of $\\SI{14.89 \\pm 9.3}{\\micro\\metre}$ and\n$\\SI{0.096 \\pm 0.072}{\\degree}$ for position and orientation learning,\nrespectively.\n", "rewritten_text": "Accurate instrument pose tracking is crucial in image-guided surgery.  Optical coherence tomography (OCT), with its micrometer-scale resolution and volumetric field of view, is increasingly employed for microscopic procedures. However, processing OCT data is challenging due to speckle noise, reflection artifacts, and the inherent three-dimensionality of the images.  This work presents a novel deep learning framework for 6D pose estimation from OCT volume data.  We designed a new 3D convolutional neural network (CNN) architecture to directly predict the pose of a small marker from OCT volumes.  A hexapod robot was used to automatically acquire labeled data for training the 3D CNN via multi-output regression.  This setup enabled a comprehensive analysis of deep learning-based volumetric pose estimation, demonstrating superior accuracy compared to methods relying on 2D representations with depth information.  Quantitative and qualitative results confirm the effectiveness of 3D CNNs in exploiting the depth structure of the marker.  Furthermore, we introduce efficient design principles for 3D CNNs, drawing inspiration from the 2D deep learning literature.  Our proposed Inception3D architecture achieved the best performance.  The resulting deep learning approach achieved accuracy at the resolution of the ground truth labels, with a mean average error of 14.89 \u00b1 9.3 \u00b5m for position and 0.096 \u00b1 0.072\u00b0 for orientation.\n"}, "2104.04986": {"original_text": "  Aspect-based Sentiment Analysis (ABSA), aiming at predicting the polarities\nfor aspects, is a fine-grained task in the field of sentiment analysis.\nPrevious work showed syntactic information, e.g. dependency trees, can\neffectively improve the ABSA performance. Recently, pre-trained models (PTMs)\nalso have shown their effectiveness on ABSA. Therefore, the question naturally\narises whether PTMs contain sufficient syntactic information for ABSA so that\nwe can obtain a good ABSA model only based on PTMs. In this paper, we firstly\ncompare the induced trees from PTMs and the dependency parsing trees on several\npopular models for the ABSA task, showing that the induced tree from fine-tuned\nRoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis\nexperiments reveal that the FT-RoBERTa Induced Tree is more\nsentiment-word-oriented and could benefit the ABSA task. The experiments also\nshow that the pure RoBERTa-based model can outperform or approximate to the\nprevious SOTA performances on six datasets across four languages since it\nimplicitly incorporates the task-oriented syntactic information.\n", "rewritten_text": "Aspect-based sentiment analysis (ABSA), which predicts the polarity of aspects, is a fine-grained sentiment analysis task.  Previous work demonstrated that syntactic information, such as dependency trees, effectively improves ABSA performance.  Recently, pre-trained models (PTMs) have also proven effective.  This raises the question of whether PTMs contain sufficient syntactic information for robust ABSA performance, eliminating the need for explicit syntactic features.  This paper compares trees induced from PTMs with dependency parsing trees across several popular ABSA models.  We find that the tree induced from a fine-tuned RoBERTa (FT-RoBERTa) model outperforms parser-generated trees.  Further analysis reveals that the FT-RoBERTa induced tree is more sentiment-word-oriented, benefiting ABSA.  Our experiments also show that a purely RoBERTa-based model can match or exceed previous state-of-the-art (SOTA) performance on six datasets spanning four languages, suggesting that it implicitly incorporates task-relevant syntactic information.\n"}, "2012.11581": {"original_text": "  Humans live within a 3D space and constantly interact with it to perform\ntasks. Such interactions involve physical contact between surfaces that is\nsemantically meaningful. Our goal is to learn how humans interact with scenes\nand leverage this to enable virtual characters to do the same. To that end, we\nintroduce a novel Human-Scene Interaction (HSI) model that encodes proximal\nrelationships, called POSA for \"Pose with prOximitieS and contActs\". The\nrepresentation of interaction is body-centric, which enables it to generalize\nto new scenes. Specifically, POSA augments the SMPL-X parametric human body\nmodel such that, for every mesh vertex, it encodes (a) the contact probability\nwith the scene surface and (b) the corresponding semantic scene label. We learn\nPOSA with a VAE conditioned on the SMPL-X vertices, and train on the PROX\ndataset, which contains SMPL-X meshes of people interacting with 3D scenes, and\nthe corresponding scene semantics from the PROX-E dataset. We demonstrate the\nvalue of POSA with two applications. First, we automatically place 3D scans of\npeople in scenes. We use a SMPL-X model fit to the scan as a proxy and then\nfind its most likely placement in 3D. POSA provides an effective representation\nto search for \"affordances\" in the scene that match the likely contact\nrelationships for that pose. We perform a perceptual study that shows\nsignificant improvement over the state of the art on this task. Second, we show\nthat POSA's learned representation of body-scene interaction supports monocular\nhuman pose estimation that is consistent with a 3D scene, improving on the\nstate of the art. Our model and code are available for research purposes at\nhttps://posa.is.tue.mpg.de.\n", "rewritten_text": "Humans inhabit and interact with three-dimensional space to perform tasks.  These interactions involve semantically meaningful physical contact between body surfaces and the environment.  Our goal is to understand human-scene interaction (HSI) and leverage this understanding to enable virtual characters to interact similarly.  To this end, we introduce POSA (Pose with prOximitieS and contActs), a novel HSI model encoding proximal relationships.  This body-centric representation generalizes to new scenes.  Specifically, POSA augments the SMPL-X parametric human body model by encoding, for each mesh vertex, (a) the probability of contact with the scene surface and (b) the corresponding semantic scene label.  We train POSA, a variational autoencoder (VAE) conditioned on SMPL-X vertices, using the PROX dataset of SMPL-X meshes depicting human-scene interaction and corresponding scene semantics from the PROX-E dataset.\n\nWe demonstrate POSA's value through two applications. First, we achieve improved automatic placement of 3D human scans within scenes.  Using an SMPL-X model fitted to a scan as a proxy, we identify the most likely 3D placement by leveraging POSA's representation to efficiently search for scene affordances matching the pose's likely contact relationships. A perceptual study demonstrates significant improvement over the state of the art. Second, we show that POSA's learned representation of body-scene interaction enhances monocular human pose estimation, ensuring consistency with the 3D scene and surpassing existing methods.\n\nOur model and code are available for research purposes at https://posa.is.tue.mpg.de.\n"}, "1609.04079": {"original_text": "  We present a single-shot system to recover surface geometry of objects with\nspatially-varying albedos, from images captured under a calibrated RGB\nphotometric stereo setup---with three light directions multiplexed across\ndifferent color channels in the observed RGB image. Since the problem is\nill-posed point-wise, we assume that the albedo map can be modeled as\npiece-wise constant with a restricted number of distinct albedo values. We show\nthat under ideal conditions, the shape of a non-degenerate local constant\nalbedo surface patch can theoretically be recovered exactly. Moreover, we\npresent a practical and efficient algorithm that uses this model to robustly\nrecover shape from real images. Our method first reasons about shape locally in\na dense set of patches in the observed image, producing shape distributions for\nevery patch. These local distributions are then combined to produce a single\nconsistent surface normal map. We demonstrate the efficacy of the approach\nthrough experiments on both synthetic renderings as well as real captured\nimages.\n", "rewritten_text": "We present a single-shot system for recovering the surface geometry of objects with spatially varying albedos from images acquired under a calibrated RGB photometric stereo setup.  Three light directions are multiplexed across the RGB channels of a single image.  Because the pointwise problem is ill-posed, we model the albedo map as piecewise constant with a limited number of distinct albedo values.  Under ideal conditions, we show that the shape of a non-degenerate, locally constant albedo surface patch can be recovered exactly.  We further present a practical and efficient algorithm that robustly recovers shape from real images using this model.  Our method first performs local shape estimation in a dense set of image patches, generating a shape distribution for each.  These local distributions are then globally integrated to produce a consistent surface normal map.  We demonstrate the efficacy of our approach through experiments on both synthetic and real images.\n"}, "2305.15699": {"original_text": "  Understanding action recognition in egocentric videos has emerged as a vital\nresearch topic with numerous practical applications. With the limitation in the\nscale of egocentric data collection, learning robust deep learning-based action\nrecognition models remains difficult. Transferring knowledge learned from the\nlarge-scale exocentric data to the egocentric data is challenging due to the\ndifference in videos across views. Our work introduces a novel cross-view\nlearning approach to action recognition (CVAR) that effectively transfers\nknowledge from the exocentric to the selfish view. First, we present a novel\ngeometric-based constraint into the self-attention mechanism in Transformer\nbased on analyzing the camera positions between two views. Then, we propose a\nnew cross-view self-attention loss learned on unpaired cross-view data to\nenforce the self-attention mechanism learning to transfer knowledge across\nviews. Finally, to further improve the performance of our cross-view learning\napproach, we present the metrics to measure the correlations in videos and\nattention maps effectively. Experimental results on standard egocentric action\nrecognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, and\nEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-art\nperformance.\n", "rewritten_text": "Action recognition in egocentric video is a crucial research area with significant practical applications.  However, the limited scale of available egocentric data hinders the development of robust deep learning models.  Transferring knowledge from large-scale exocentric datasets is challenging due to the inherent differences in viewpoint.  This work introduces a novel cross-view action recognition (CVAR) approach that effectively transfers knowledge from exocentric to egocentric viewpoints.  We achieve this by: (1) incorporating a novel geometric constraint into the Transformer's self-attention mechanism, based on an analysis of camera positions; (2) proposing a novel cross-view self-attention loss, trained on unpaired cross-view data, to explicitly encourage cross-view knowledge transfer; and (3) introducing metrics to effectively measure correlations between videos and attention maps, further improving performance.  Experiments on standard egocentric action recognition benchmarks (Charades-Ego, EPIC-Kitchens-55, and EPIC-Kitchens-100) demonstrate the effectiveness of our approach and its state-of-the-art performance.\n"}, "2407.10753": {"original_text": "  Accurate depth information is crucial for enhancing the performance of\nmulti-view 3D object detection. Despite the success of some existing multi-view\n3D detectors utilizing pixel-wise depth supervision, they overlook two\nsignificant phenomena: 1) the depth supervision obtained from LiDAR points is\nusually distributed on the surface of the object, which is not so friendly to\nexisting DETR-based 3D detectors due to the lack of the depth of 3D object\ncenter; 2) for distant objects, fine-grained depth estimation of the whole\nobject is more challenging. Therefore, we argue that the object-wise depth (or\n3D center of the object) is essential for accurate detection. In this paper, we\npropose a new multi-view 3D object detector named OPEN, whose main idea is to\neffectively inject object-wise depth information into the network through our\nproposed object-wise position embedding. Specifically, we first employ an\nobject-wise depth encoder, which takes the pixel-wise depth map as a prior, to\naccurately estimate the object-wise depth. Then, we utilize the proposed\nobject-wise position embedding to encode the object-wise depth information into\nthe transformer decoder, thereby producing 3D object-aware features for final\ndetection. Extensive experiments verify the effectiveness of our proposed\nmethod. Furthermore, OPEN achieves a new state-of-the-art performance with\n64.4% NDS and 56.7% mAP on the nuScenes test benchmark.\n", "rewritten_text": "Accurate depth information is crucial for improving multi-view 3D object detection performance. While some existing multi-view 3D detectors successfully utilize pixel-wise depth supervision, they neglect two key aspects: 1) LiDAR-derived depth supervision is typically surface-distributed, hindering DETR-based 3D detectors that lack 3D object center depth information; and 2) accurate, fine-grained depth estimation for distant objects is challenging.  Therefore, we propose that object-wise depth (or the 3D object center depth) is essential for accurate detection.\n\nThis paper introduces OPEN, a novel multi-view 3D object detector that effectively incorporates object-wise depth information via a proposed object-wise position embedding.  Specifically, we employ an object-wise depth encoder, using the pixel-wise depth map as input, to accurately estimate object-wise depth. This information is then encoded into the transformer decoder using our object-wise position embedding, generating 3D object-aware features for improved detection.  Extensive experiments demonstrate the effectiveness of our approach.  OPEN achieves state-of-the-art performance on the nuScenes test benchmark, with a 64.4% NDS and 56.7% mAP.\n"}, "2308.10445": {"original_text": "  Incremental learning aims to overcome catastrophic forgetting when learning\ndeep networks from sequential tasks. With impressive learning efficiency and\nperformance, prompt-based methods adopt a fixed backbone to sequential tasks by\nlearning task-specific prompts. However, existing prompt-based methods heavily\nrely on strong pretraining (typically trained on ImageNet-21k), and we find\nthat their models could be trapped if the potential gap between the pretraining\ntask and unknown future tasks is large. In this work, we develop a learnable\nAdaptive Prompt Generator (APG). The key is to unify the prompt retrieval and\nprompt learning processes into a learnable prompt generator. Hence, the whole\nprompting process can be optimized to reduce the negative effects of the gap\nbetween tasks effectively. To make our APG avoid learning ineffective\nknowledge, we maintain a knowledge pool to regularize APG with the feature\ndistribution of each class. Extensive experiments show that our method\nsignificantly outperforms advanced methods in exemplar-free incremental\nlearning without (strong) pretraining. Besides, under strong retraining, our\nmethod also has comparable performance to existing prompt-based models, showing\nthat our method can still benefit from pretraining. Codes can be found at\nhttps://github.com/TOM-tym/APG\n", "rewritten_text": "Incremental learning addresses catastrophic forgetting in deep networks trained on sequential tasks.  Prompt-based methods, while demonstrating impressive efficiency and performance by adapting a fixed backbone via task-specific prompts, heavily rely on strong pretraining (e.g., ImageNet-21k).  We observe that these methods struggle when a significant gap exists between the pretraining and subsequent, unseen tasks.  This work introduces a learnable Adaptive Prompt Generator (APG) that unifies prompt retrieval and learning into a single, optimizable process. This unified approach effectively mitigates the negative impact of inter-task discrepancies.  To prevent the APG from acquiring ineffective knowledge, we incorporate a knowledge pool that regularizes the generator based on the feature distribution of each class.  Extensive experiments demonstrate that our method significantly surpasses state-of-the-art exemplar-free incremental learning methods *without* strong pretraining. Furthermore, with strong pretraining, our method achieves performance comparable to existing prompt-based models, highlighting its ability to leverage pretraining benefits.  Code is available at https://github.com/TOM-tym/APG.\n"}, "2409.18355": {"original_text": "  Cone Beam Computed Tomography (CBCT) finds diverse applications in medicine.\nEnsuring high image quality in CBCT scans is essential for accurate diagnosis\nand treatment delivery. Yet, the susceptibility of CBCT images to noise and\nartifacts undermines both their usefulness and reliability. Existing methods\ntypically address CBCT artifacts through image-to-image translation approaches.\nThese methods, however, are limited by the artifact types present in the\ntraining data, which may not cover the complete spectrum of CBCT degradations\nstemming from variations in imaging protocols. Gathering additional data to\nencompass all possible scenarios can often pose a challenge. To address this,\nwe present SinoSynth, a physics-based degradation model that simulates various\nCBCT-specific artifacts to generate a diverse set of synthetic CBCT images from\nhigh-quality CT images without requiring pre-aligned data. Through extensive\nexperiments, we demonstrate that several different generative networks trained\non our synthesized data achieve remarkable results on heterogeneous\nmulti-institutional datasets, outperforming even the same networks trained on\nactual data. We further show that our degradation model conveniently provides\nan avenue to enforce anatomical constraints in conditional generative models,\nyielding high-quality and structure-preserving synthetic CT images.\n", "rewritten_text": "Cone beam computed tomography (CBCT) has diverse medical applications.  High-quality CBCT images are crucial for accurate diagnosis and treatment; however, noise and artifacts significantly compromise their reliability and utility.  Existing methods often rely on image-to-image translation, but these are limited by the types of artifacts present in the training data, which may not represent the full range of CBCT degradations arising from variations in imaging protocols.  Acquiring comprehensive training data encompassing all possible scenarios is challenging.  To address this, we introduce SinoSynth, a physics-based degradation model that simulates various CBCT-specific artifacts.  SinoSynth generates a diverse set of synthetic CBCT images from high-quality computed tomography (CT) images without requiring pre-aligned data.  Extensive experiments demonstrate that several generative networks trained on our synthetic data achieve superior performance on heterogeneous, multi-institutional datasets, surpassing even the same networks trained on real data.  Furthermore, our degradation model facilitates the enforcement of anatomical constraints in conditional generative models, resulting in high-quality, structure-preserving synthetic CT images.\n"}, "1904.07904": {"original_text": "  Spoken question answering (SQA) is challenging due to complex reasoning on\ntop of the spoken documents. The recent studies have also shown the\ncatastrophic impact of automatic speech recognition (ASR) errors on SQA.\nTherefore, this work proposes to mitigate the ASR errors by aligning the\nmismatch between ASR hypotheses and their corresponding reference\ntranscriptions. An adversarial model is applied to this domain adaptation task,\nwhich forces the model to learn domain-invariant features the QA model can\neffectively utilize in order to improve the SQA results. The experiments\nsuccessfully demonstrate the effectiveness of our proposed model, and the\nresults are better than the previous best model by 2% EM score.\n", "rewritten_text": "Spoken question answering (SQA) is challenging due to the complex reasoning required with spoken documents.  Recent studies have highlighted the significant negative impact of automatic speech recognition (ASR) errors on SQA performance.  This work addresses this challenge by mitigating ASR errors through alignment of mismatched ASR hypotheses and their reference transcriptions.  We apply an adversarial model to this domain adaptation task, forcing the model to learn domain-invariant features that improve SQA performance.  Experiments demonstrate the effectiveness of our proposed model, achieving a 2% improvement in exact match (EM) score over the previous state-of-the-art.\n"}, "2310.00936": {"original_text": "  Recent studies on StyleGAN variants show promising performances for various\ngeneration tasks. In these models, latent codes have traditionally been\nmanipulated and searched for the desired images. However, this approach\nsometimes suffers from a lack of photorealism in generated images due to a lack\nof knowledge about the geometry of the trained latent space. In this paper, we\nshow a simple unsupervised method that provides well-trained local latent\nsubspace, enabling latent code navigation while preserving the photorealism of\nthe generated images. Specifically, the method identifies densely mapped latent\nspaces and restricts latent manipulations within the local latent subspace.\nExperimental results demonstrate that images generated within the local latent\nsubspace maintain photorealism even when the latent codes are significantly and\nrepeatedly manipulated. Moreover, experiments show that the method can be\napplied to latent code optimization for various types of style-based models.\nOur empirical evidence of the method will benefit applications in style-based\nmodels.\n", "rewritten_text": "Recent studies demonstrate the promising performance of StyleGAN variants across various image generation tasks.  Traditional approaches manipulate and search latent codes to achieve desired image outputs. However, this often results in a lack of photorealism due to insufficient understanding of the trained latent space's geometry.  This paper presents a simple, unsupervised method for generating well-trained local latent subspaces, enabling latent code navigation while preserving photorealism.  Specifically, the method identifies densely mapped regions of the latent space and constrains latent manipulations to these local subspaces.  Experimental results show that images generated within these local subspaces retain photorealism even under significant and repeated latent code manipulation.  Furthermore, the method is applicable to latent code optimization in various style-based generative models.  Our empirical findings demonstrate the method's utility for applications involving style-based models.\n"}, "2310.00796": {"original_text": "  Strong inductive biases enable learning from little data and help\ngeneralization outside of the training distribution. Popular neural\narchitectures such as Transformers lack strong structural inductive biases for\nseq2seq NLP tasks on their own. Consequently, they struggle with systematic\ngeneralization beyond the training distribution, e.g. with extrapolating to\nlonger inputs, even when pre-trained on large amounts of text. We show how a\nstructural inductive bias can be efficiently injected into a seq2seq model by\npre-training it to simulate structural transformations on synthetic data.\nSpecifically, we inject an inductive bias towards Finite State Transducers\n(FSTs) into a Transformer by pre-training it to simulate FSTs given their\ndescriptions. Our experiments show that our method imparts the desired\ninductive bias, resulting in improved systematic generalization and better\nfew-shot learning for FST-like tasks. Our analysis shows that fine-tuned models\naccurately capture the state dynamics of the unseen underlying FSTs, suggesting\nthat the simulation process is internalized by the fine-tuned model.\n", "rewritten_text": "Strong inductive biases facilitate learning from limited data and enhance generalization beyond the training distribution.  Popular neural architectures like Transformers inherently lack strong structural inductive biases for sequence-to-sequence (seq2seq) natural language processing (NLP) tasks.  Consequently, they often struggle with systematic generalization, such as extrapolating to longer inputs, even with extensive pre-training on large text corpora.  We introduce a method for efficiently injecting a structural inductive bias into seq2seq models by pre-training them to simulate structural transformations on synthetic data.  Specifically, we instill a bias towards Finite State Transducers (FSTs) by pre-training a Transformer to simulate FSTs based on their descriptions.  Our experiments demonstrate that this method successfully imparts the desired inductive bias, leading to improved systematic generalization and enhanced few-shot learning capabilities for FST-like tasks.  Analysis reveals that fine-tuned models accurately capture the state dynamics of unseen underlying FSTs, indicating successful internalization of the simulation process.\n"}, "2309.11566": {"original_text": "  We introduce SignBank+, a clean version of the SignBank dataset, optimized\nfor machine translation between spoken language text and SignWriting, a\nphonetic sign language writing system. In addition to previous work that\nemploys complex factorization techniques to enable translation between text and\nSignWriting, we show that a traditional text-to-text translation approach\nperforms equally effectively on the cleaned SignBank+ dataset. Our evaluation\nresults indicate that models trained on SignBank+ surpass those on the original\ndataset, establishing a new benchmark for SignWriting-based sign language\ntranslation and providing an open resource for future research.\n", "rewritten_text": "SignBank+, a cleaned and optimized version of the SignBank dataset, facilitates machine translation between spoken language text and SignWriting, a phonetic sign language notation system.  While previous work relied on complex factorization techniques for text-to-SignWriting translation, we demonstrate that a standard text-to-text translation approach achieves comparable performance on SignBank+.  Our evaluation shows that models trained on SignBank+ significantly outperform those trained on the original dataset, establishing a new benchmark for SignWriting-based sign language translation.  This improved dataset is publicly available for future research.\n"}, "2212.00244": {"original_text": "  Domain adaptation for Cross-LiDAR 3D detection is challenging due to the\nlarge gap on the raw data representation with disparate point densities and\npoint arrangements. By exploring domain-invariant 3D geometric characteristics\nand motion patterns, we present an unsupervised domain adaptation method that\novercomes above difficulties. First, we propose the Spatial Geometry Alignment\nmodule to extract similar 3D shape geometric features of the same object class\nto align two domains, while eliminating the effect of distinct point\ndistributions. Second, we present Temporal Motion Alignment module to utilize\nmotion features in sequential frames of data to match two domains. Prototypes\ngenerated from two modules are incorporated into the pseudo-label reweighting\nprocedure and contribute to our effective self-training framework for the\ntarget domain. Extensive experiments show that our method achieves\nstate-of-the-art performance on cross-device datasets, especially for the\ndatasets with large gaps captured by mechanical scanning LiDARs and solid-state\nLiDARs in various scenes. Project homepage is at\nhttps://github.com/4DVLab/CL3D.git\n", "rewritten_text": "Cross-LiDAR 3D object detection is challenging due to significant differences in raw data representation, including disparate point densities and arrangements.  To address this, we propose an unsupervised domain adaptation method that leverages domain-invariant 3D geometric and motion characteristics.  This method comprises two modules:  a Spatial Geometry Alignment module, which extracts similar 3D shape features for the same object class to align the source and target domains while mitigating the effects of differing point distributions; and a Temporal Motion Alignment module, which utilizes motion features from sequential frames to further enhance domain alignment.  Prototypes generated by these modules are integrated into a pseudo-label reweighting procedure within a self-training framework for the target domain.  Extensive experiments demonstrate state-of-the-art performance on cross-device datasets, particularly those exhibiting large discrepancies, such as those captured by mechanical and solid-state LiDARs in diverse scenes.  Project code is available at https://github.com/4DVLab/CL3D.git.\n"}, "2210.10239": {"original_text": "  This paper aims to investigate representation learning for large scale visual\nplace recognition, which consists of determining the location depicted in a\nquery image by referring to a database of reference images. This is a\nchallenging task due to the large-scale environmental changes that can occur\nover time (i.e., weather, illumination, season, traffic, occlusion). Progress\nis currently challenged by the lack of large databases with accurate ground\ntruth. To address this challenge, we introduce GSV-Cities, a new image dataset\nproviding the widest geographic coverage to date with highly accurate ground\ntruth, covering more than 40 cities across all continents over a 14-year\nperiod. We subsequently explore the full potential of recent advances in deep\nmetric learning to train networks specifically for place recognition, and\nevaluate how different loss functions influence performance. In addition, we\nshow that performance of existing methods substantially improves when trained\non GSV-Cities. Finally, we introduce a new fully convolutional aggregation\nlayer that outperforms existing techniques, including GeM, NetVLAD and\nCosPlace, and establish a new state-of-the-art on large-scale benchmarks, such\nas Pittsburgh, Mapillary-SLS, SPED and Nordland. The dataset and code are\navailable for research purposes at https://github.com/amaralibey/gsv-cities.\n", "rewritten_text": "This paper investigates representation learning for large-scale visual place recognition\u2014the task of identifying the location depicted in a query image using a database of reference images. This task is challenging due to significant environmental changes over time, including variations in weather, illumination, season, traffic, and occlusion.  Progress has been hampered by the scarcity of large, accurately labeled datasets.  To address this, we introduce GSV-Cities, a novel image dataset offering unprecedented geographic coverage and highly accurate ground truth.  GSV-Cities encompasses over 40 cities across all continents, spanning a 14-year period.  We then leverage recent advances in deep metric learning to train place recognition networks, analyzing the impact of different loss functions on performance.  Our results demonstrate substantial performance improvements for existing methods when trained on GSV-Cities.  Furthermore, we introduce a novel fully convolutional aggregation layer that surpasses existing techniques, including GeM, NetVLAD, and CosPlace, establishing a new state-of-the-art on large-scale benchmarks such as Pittsburgh, Mapillary-SLS, SPED, and Nordland. The dataset and code are publicly available for research purposes at https://github.com/amaralibey/gsv-cities.\n"}, "2202.04101": {"original_text": "  Photoplethysmography (PPG) signals have become a key technology in many\nfields, such as medicine, well-being, or sports. Our work proposes a set of\npipelines to extract remote PPG signals (rPPG) from the face robustly,\nreliably, and configurable. We identify and evaluate the possible choices in\nthe critical steps of unsupervised rPPG methodologies. We assess a\nstate-of-the-art processing pipeline in six different datasets, incorporating\nimportant corrections in the methodology that ensure reproducible and fair\ncomparisons. In addition, we extend the pipeline by proposing three novel\nideas; 1) a new method to stabilize the detected face based on a rigid mesh\nnormalization; 2) a new method to dynamically select the different regions in\nthe face that provide the best raw signals, and 3) a new RGB to rPPG\ntransformation method, called Orthogonal Matrix Image Transformation (OMIT)\nbased on QR decomposition, that increases robustness against compression\nartifacts. We show that all three changes introduce noticeable improvements in\nretrieving rPPG signals from faces, obtaining state-of-the-art results compared\nwith unsupervised, non-learning-based methodologies and, in some databases,\nvery close to supervised, learning-based methods. We perform a comparative\nstudy to quantify the contribution of each proposed idea. In addition, we\ndepict a series of observations that could help in future implementations.\n", "rewritten_text": "Photoplethysmography (PPG) signals are a key technology in diverse fields, including medicine, wellness, and sports.  This work presents a set of robust, reliable, and configurable pipelines for extracting remote PPG (rPPG) signals from facial video. We identify and evaluate options at each critical step of unsupervised rPPG methodologies, assessing a state-of-the-art pipeline across six diverse datasets.  Our evaluation incorporates methodological improvements to ensure reproducible and fair comparisons.  Furthermore, we extend this pipeline with three novel contributions: 1) a rigid mesh normalization method for improved face stabilization; 2) a dynamic facial region selection method to optimize raw signal quality; and 3) a novel RGB-to-rPPG transformation, Orthogonal Matrix Image Transformation (OMIT), based on QR decomposition, which enhances robustness against compression artifacts.  These improvements demonstrably enhance rPPG signal extraction, achieving state-of-the-art results among unsupervised, non-learning-based methods and, in some cases, performance comparable to supervised, learning-based approaches.  A comparative study quantifies the contribution of each proposed innovation, and we conclude with observations to guide future research.\n"}, "1803.05449": {"original_text": "  We introduce SentEval, a toolkit for evaluating the quality of universal\nsentence representations. SentEval encompasses a variety of tasks, including\nbinary and multi-class classification, natural language inference and sentence\nsimilarity. The set of tasks was selected based on what appears to be the\ncommunity consensus regarding the appropriate evaluations for universal\nsentence representations. The toolkit comes with scripts to download and\npreprocess datasets, and an easy interface to evaluate sentence encoders. The\naim is to provide a fairer, less cumbersome and more centralized way for\nevaluating sentence representations.\n", "rewritten_text": "SentEval is a toolkit for evaluating the quality of universal sentence representations.  It offers a diverse range of tasks, including binary and multi-class classification, natural language inference, and sentence similarity, selected to reflect community consensus on appropriate evaluation metrics.  The toolkit provides scripts for dataset download and preprocessing, along with a user-friendly interface for evaluating sentence encoders.  Its goal is to offer a more standardized, efficient, and accessible method for evaluating sentence representations.\n"}, "1506.05929": {"original_text": "  Previous work has shown that the artist of an artwork can be identified by\nuse of computational methods that analyse digital images. However, the\ndigitised artworks are often investigated at a coarse scale discarding many of\nthe important details that may define an artist's style. In recent years high\nresolution images of artworks have become available, which, combined with\nincreased processing power and new computational techniques, allow us to\nanalyse digital images of artworks at a very fine scale. In this work we train\nand evaluate a Convolutional Neural Network (CNN) on the task of artist\nattribution using artwork images of varying resolutions. To this end, we\ncombine two existing methods to enable the application of high resolution\nimages to CNNs. By comparing the attribution performances obtained at different\nscales, we find that in most cases finer scales are beneficial to the\nattribution performance, whereas for a minority of the artists, coarser scales\nappear to be preferable. We conclude that artist attribution would benefit from\na multi-scale CNN approach which vastly expands the possibilities for\ncomputational art forensics.\n", "rewritten_text": "Prior research has demonstrated the feasibility of identifying an artwork's creator using computational methods applied to digital images.  However, these studies often analyzed digitized artworks at a coarse resolution, neglecting crucial stylistic details.  Recent advancements, including the availability of high-resolution images and increased computational power, enable fine-scale analysis.  This work trains and evaluates a Convolutional Neural Network (CNN) for artist attribution using artwork images at varying resolutions.  We combine two existing methods to facilitate the application of high-resolution images to CNNs.  By comparing attribution performance across scales, we find that finer scales generally improve accuracy, although coarser scales proved superior for a small subset of artists.  We conclude that a multi-scale CNN approach would significantly enhance artist attribution, expanding the potential of computational art forensics.\n"}, "2112.03803": {"original_text": "  Despite the great progress in video understanding made by deep convolutional\nneural networks, feature representation learned by existing methods may be\nbiased to static visual cues. To address this issue, we propose a novel method\nto suppress static visual cues (SSVC) based on probabilistic analysis for\nself-supervised video representation learning. In our method, video frames are\nfirst encoded to obtain latent variables under standard normal distribution via\nnormalizing flows. By modelling static factors in a video as a random variable,\nthe conditional distribution of each latent variable becomes shifted and scaled\nnormal. Then, the less-varying latent variables along time are selected as\nstatic cues and suppressed to generate motion-preserved videos. Finally,\npositive pairs are constructed by motion-preserved videos for contrastive\nlearning to alleviate the problem of representation bias to static cues. The\nless-biased video representation can be better generalized to various\ndownstream tasks. Extensive experiments on publicly available benchmarks\ndemonstrate that the proposed method outperforms the state of the art when only\nsingle RGB modality is used for pre-training.\n", "rewritten_text": "While deep convolutional neural networks have significantly advanced video understanding, existing methods often learn feature representations biased toward static visual cues.  To mitigate this, we propose a novel self-supervised video representation learning method, Static Visual Cue Suppression (SSVC), based on probabilistic analysis.  Our method first encodes video frames into latent variables following a standard normal distribution using normalizing flows.  We model static factors as a random variable, resulting in a shifted and scaled normal distribution for each latent variable.  Latent variables exhibiting low temporal variance are identified as static cues and suppressed to generate motion-preserved videos.  These videos then form positive pairs for contrastive learning, thereby reducing the bias toward static cues.  This less-biased representation generalizes better to various downstream tasks.  Extensive experiments on publicly available benchmarks demonstrate that our method surpasses the state-of-the-art using only a single RGB modality for pre-training.\n"}, "2103.16889": {"original_text": "  Current state-of-the-art visual recognition systems usually rely on the\nfollowing pipeline: (a) pretraining a neural network on a large-scale dataset\n(e.g., ImageNet) and (b) finetuning the network weights on a smaller,\ntask-specific dataset. Such a pipeline assumes the sole weight adaptation is\nable to transfer the network capability from one domain to another domain,\nbased on a strong assumption that a fixed architecture is appropriate for all\ndomains. However, each domain with a distinct recognition target may need\ndifferent levels/paths of feature hierarchy, where some neurons may become\nredundant, and some others are re-activated to form new network structures. In\nthis work, we prove that dynamically adapting network architectures tailored\nfor each domain task along with weight finetuning benefits in both efficiency\nand effectiveness, compared to the existing image recognition pipeline that\nonly tunes the weights regardless of the architecture. Our method can be easily\ngeneralized to an unsupervised paradigm by replacing supernet training with\nself-supervised learning in the source domain tasks and performing linear\nevaluation in the downstream tasks. This further improves the search efficiency\nof our method. Moreover, we also provide principled and empirical analysis to\nexplain why our approach works by investigating the ineffectiveness of existing\nneural architecture search. We find that preserving the joint distribution of\nthe network architecture and weights is of importance. This analysis not only\nbenefits image recognition but also provides insights for crafting neural\nnetworks. Experiments on five representative image recognition tasks such as\nperson re-identification, age estimation, gender recognition, image\nclassification, and unsupervised domain adaptation demonstrate the\neffectiveness of our method.\n", "rewritten_text": "State-of-the-art visual recognition systems typically employ a two-stage pipeline: (a) pre-training a neural network on a large-scale dataset (e.g., ImageNet), followed by (b) fine-tuning the network weights on a smaller, task-specific dataset. This approach assumes that weight adaptation alone suffices to transfer capabilities across domains, based on the premise that a fixed architecture is universally suitable.  However, different recognition tasks may require distinct feature hierarchies, with some neurons becoming redundant while others are activated to form new structures.\n\nThis work demonstrates that dynamically adapting the network architecture for each domain, in conjunction with weight fine-tuning, improves both efficiency and effectiveness compared to solely tuning weights.  Our method generalizes readily to an unsupervised paradigm by substituting supernet training with self-supervised learning in the source domain and employing linear evaluation in downstream tasks, further enhancing search efficiency.  Furthermore, we provide theoretical and empirical analyses to explain our approach's success by examining the limitations of existing neural architecture search methods.  We find that preserving the joint distribution of network architecture and weights is crucial. This analysis offers insights not only for image recognition but also for broader neural network design.\n\nExperiments across five representative image recognition tasks\u2014person re-identification, age estimation, gender recognition, image classification, and unsupervised domain adaptation\u2014demonstrate the effectiveness of our method.\n"}, "2003.13239": {"original_text": "  Cross view feature fusion is the key to address the occlusion problem in\nhuman pose estimation. The current fusion methods need to train a separate\nmodel for every pair of cameras making them difficult to scale. In this work,\nwe introduce MetaFuse, a pre-trained fusion model learned from a large number\nof cameras in the Panoptic dataset. The model can be efficiently adapted or\nfinetuned for a new pair of cameras using a small number of labeled images. The\nstrong adaptation power of MetaFuse is due in large part to the proposed\nfactorization of the original fusion model into two parts (1) a generic fusion\nmodel shared by all cameras, and (2) lightweight camera-dependent\ntransformations. Furthermore, the generic model is learned from many cameras by\na meta-learning style algorithm to maximize its adaptation capability to\nvarious camera poses. We observe in experiments that MetaFuse finetuned on the\npublic datasets outperforms the state-of-the-arts by a large margin which\nvalidates its value in practice.\n", "rewritten_text": "Cross-view feature fusion is crucial for addressing occlusion in human pose estimation.  Existing fusion methods require training a separate model for each camera pair, hindering scalability.  This work introduces MetaFuse, a pre-trained fusion model learned from numerous cameras within the Panoptic dataset.  This model efficiently adapts or fine-tunes to new camera pairs using only a small number of labeled images.  MetaFuse's strong adaptation capabilities stem from a novel factorization of the fusion model into: (1) a generic fusion model shared across all cameras, and (2) lightweight, camera-specific transformations.  Moreover, the generic model is trained using a meta-learning approach on data from many cameras, maximizing its adaptability to diverse camera poses.  Experiments demonstrate that MetaFuse, fine-tuned on public datasets, significantly outperforms state-of-the-art methods, validating its practical value.\n"}, "2011.12528": {"original_text": "  We propose a novel reference-based video colorization framework with\nspatiotemporal correspondence. Reference-based methods colorize grayscale\nframes referencing a user input color frame. Existing methods suffer from the\ncolor leakage between objects and the emergence of average colors, derived from\nnon-local semantic correspondence in space. To address this issue, we warp\ncolors only from the regions on the reference frame restricted by\ncorrespondence in time. We propagate masks as temporal correspondences, using\ntwo complementary tracking approaches: off-the-shelf instance tracking for high\nperformance segmentation, and newly proposed dense tracking to track various\ntypes of objects. By restricting temporally-related regions for referencing\ncolors, our approach propagates faithful colors throughout the video.\nExperiments demonstrate that our method outperforms state-of-the-art methods\nquantitatively and qualitatively.\n", "rewritten_text": "This paper introduces a novel reference-based video colorization framework leveraging spatiotemporal correspondence.  Reference-based methods colorize grayscale video frames using a user-provided color reference frame.  Existing methods suffer from color bleeding between objects and the prevalence of averaged colors due to inaccurate non-local semantic correspondences.  To mitigate these issues, we restrict color warping to regions in the reference frame defined by temporal correspondences.  These correspondences are propagated using two complementary tracking approaches:  an off-the-shelf instance tracker for robust segmentation and a novel dense tracker capable of handling diverse object types.  By limiting color referencing to temporally consistent regions, our method ensures faithful color propagation throughout the video.  Quantitative and qualitative experimental results demonstrate superior performance compared to state-of-the-art techniques.\n"}}