{"1812.10788": {"original_text": "  Hyperspectral remote sensing is a prominent research topic in data\nprocessing. Most of the spectral unmixing algorithms are developed by adopting\nthe linear mixing models. Nonnegative matrix factorization (NMF) and its\ndevelopments are used widely for estimation of signatures and fractional\nabundances in the SU problem. Sparsity constraints was added to NMF, and was\nregularized by $ L_ {q} $ norm. In this paper, at first hyperspectral images\nare clustered by fuzzy c- means method, and then a new algorithm based on\nsparsity constrained distributed optimization is used for spectral unmixing. In\nthe proposed algorithm, a network including clusters is employed. Each pixel in\nthe hyperspectral images considered as a node in this network. The proposed\nalgorithm is optimized with diffusion LMS strategy, and then the update\nequations for fractional abundance and signature matrices are obtained.\nSimulation results based on defined performance metrics illustrate advantage of\nthe proposed algorithm in spectral unmixing of hyperspectral data compared with\nother methods.\n", "rewritten_text": "\\boxed{\nHyperspectral remote sensing is a significant area of research in data processing, with most spectral unmixing algorithms relying on linear mixing models. Nonnegative matrix factorization (NMF) and its variants are widely used for estimating signatures and fractional abundances in spectral unmixing problems. To improve NMF, sparsity constraints have been added and regularized using the Lq norm. This paper proposes a new algorithm that combines fuzzy c-means clustering with sparsity-constrained distributed optimization for spectral unmixing. The algorithm represents hyperspectral images as a network of clusters, where each pixel is a node. The proposed algorithm is optimized using the diffusion LMS strategy, yielding update equations for fractional abundance and signature matrices. Simulation results demonstrate the superiority of the proposed algorithm in spectral unmixing of hyperspectral data compared to other methods.\n}"}, "2305.04628": {"original_text": "  In this paper, we introduce a novel framework for the challenging problem of\nOne-Shot Unsupervised Domain Adaptation (OSUDA), which aims to adapt to a\ntarget domain with only a single unlabeled target sample. Unlike existing\napproaches that rely on large labeled source and unlabeled target data, our\nTarget-driven One-Shot UDA (TOS-UDA) approach employs a learnable augmentation\nstrategy guided by the target sample's style to align the source distribution\nwith the target distribution. Our method consists of three modules: an\naugmentation module, a style alignment module, and a classifier. Unlike\nexisting methods, our augmentation module allows for strong transformations of\nthe source samples, and the style of the single target sample available is\nexploited to guide the augmentation by ensuring perceptual similarity.\nFurthermore, our approach integrates augmentation with style alignment,\neliminating the need for separate pre-training on additional datasets. Our\nmethod outperforms or performs comparably to existing OS-UDA methods on the\nDigits and DomainNet benchmarks.\n", "rewritten_text": "\\boxed{\nThis paper presents a novel framework for One-Shot Unsupervised Domain Adaptation (OSUDA), which involves adapting to a target domain with only a single unlabeled target sample. Unlike existing approaches that require large amounts of labeled source and unlabeled target data, our Target-driven One-Shot UDA (TOS-UDA) approach uses a learnable augmentation strategy guided by the target sample's style to align the source and target distributions. Our method consists of three modules: an augmentation module that allows for strong transformations of source samples, a style alignment module that ensures perceptual similarity with the target sample, and a classifier. By integrating augmentation with style alignment, our approach eliminates the need for separate pre-training on additional datasets. Experimental results show that our method outperforms or performs comparably to existing OS-UDA methods on the Digits and DomainNet benchmarks."}, "2011.14589": {"original_text": "  Monocular 3D object detection, with the aim of predicting the geometric\nproperties of on-road objects, is a promising research topic for the\nintelligent perception systems of autonomous driving. Most state-of-the-art\nmethods follow a keypoint-based paradigm, where the keypoints of objects are\npredicted and employed as the basis for regressing the other geometric\nproperties. In this work, a unified network named as FADNet is presented to\naddress the task of monocular 3D object detection. In contrast to previous\nkeypoint-based methods, we propose to divide the output modalities into\ndifferent groups according to the estimation difficulty of object properties.\nDifferent groups are treated differently and sequentially associated by a\nconvolutional Gated Recurrent Unit. Another contribution of this work is the\nstrategy of depth hint augmentation. To provide characterized depth patterns as\nhints for depth estimation, a dedicated depth hint module is designed to\ngenerate row-wise features named as depth hints, which are explicitly\nsupervised in a bin-wise manner. The contributions of this work are validated\nby conducting experiments and ablation study on the KITTI benchmark. Without\nutilizing depth priors, post optimization, or other refinement modules, our\nnetwork performs competitively against state-of-the-art methods while\nmaintaining a decent running speed.\n", "rewritten_text": "\\boxed{\nMonocular 3D object detection is a crucial aspect of autonomous driving, and most current approaches rely on predicting keypoints of objects to estimate their geometric properties. This paper proposes a unified network called FADNet, which diverges from traditional keypoint-based methods by categorizing output modalities based on estimation difficulty and processing them sequentially using a convolutional Gated Recurrent Unit. Additionally, the paper introduces a depth hint augmentation strategy, which generates row-wise features (depth hints) to aid depth estimation. The effectiveness of FADNet is demonstrated through experiments and ablation studies on the KITTI benchmark, showcasing competitive performance without relying on depth priors, post-optimization, or refinement modules, while maintaining a reasonable running speed.\n}"}, "2307.13958": {"original_text": "  Recently, vision transformer based multimodal learning methods have been\nproposed to improve the robustness of face anti-spoofing (FAS) systems.\nHowever, multimodal face data collected from the real world is often imperfect\ndue to missing modalities from various imaging sensors. Recently,\nflexible-modal FAS~\\cite{yu2023flexible} has attracted more attention, which\naims to develop a unified multimodal FAS model using complete multimodal face\ndata but is insensitive to test-time missing modalities. In this paper, we\ntackle one main challenge in flexible-modal FAS, i.e., when missing modality\noccurs either during training or testing in real-world situations. Inspired by\nthe recent success of the prompt learning in language models, we propose\n\\textbf{V}isual \\textbf{P}rompt flexible-modal \\textbf{FAS} (VP-FAS), which\nlearns the modal-relevant prompts to adapt the frozen pre-trained foundation\nmodel to downstream flexible-modal FAS task. Specifically, both vanilla visual\nprompts and residual contextual prompts are plugged into multimodal\ntransformers to handle general missing-modality cases, while only requiring\nless than 4\\% learnable parameters compared to training the entire model.\nFurthermore, missing-modality regularization is proposed to force models to\nlearn consistent multimodal feature embeddings when missing partial modalities.\nExtensive experiments conducted on two multimodal FAS benchmark datasets\ndemonstrate the effectiveness of our VP-FAS framework that improves the\nperformance under various missing-modality cases while alleviating the\nrequirement of heavy model re-training.\n", "rewritten_text": "\\boxed{\nRecently, vision transformer-based methods have been proposed to enhance the robustness of face anti-spoofing (FAS) systems. However, real-world multimodal face data often lacks certain modalities due to imperfect imaging sensors. To address this, flexible-modal FAS has gained attention, aiming to develop a unified multimodal FAS model that can handle missing modalities during testing. This paper tackles the challenge of missing modalities during training or testing by proposing Visual Prompt flexible-modal FAS (VP-FAS). Inspired by prompt learning in language models, VP-FAS learns modal-relevant prompts to adapt a pre-trained foundation model to the flexible-modal FAS task, requiring less than 4% of the model's parameters to be learned. Additionally, a missing-modality regularization technique is introduced to ensure consistent multimodal feature embeddings when partial modalities are missing. Experimental results on two benchmark datasets demonstrate the effectiveness of VP-FAS in improving performance under various missing-modality scenarios while reducing the need for extensive model re-training."}, "1803.0634": {"original_text": "  We present a method for estimating detailed scene illumination using human\nfaces in a single image. In contrast to previous works that estimate lighting\nin terms of low-order basis functions or distant point lights, our technique\nestimates illumination at a higher precision in the form of a non-parametric\nenvironment map. Based on the observation that faces can exhibit strong\nhighlight reflections from a broad range of lighting directions, we propose a\ndeep neural network for extracting highlights from faces, and then trace these\nreflections back to the scene to acquire the environment map. Since real\ntraining data for highlight extraction is very limited, we introduce an\nunsupervised scheme for finetuning the network on real images, based on the\nconsistent diffuse chromaticity of a given face seen in multiple real images.\nIn tracing the estimated highlights to the environment, we reduce the blurring\neffect of skin reflectance on reflected light through a deconvolution\ndetermined by prior knowledge on face material properties. Comparisons to\nprevious techniques for highlight extraction and illumination estimation show\nthe state-of-the-art performance of this approach on a variety of indoor and\noutdoor scenes.\n", "rewritten_text": "\\boxed{We introduce a novel method for estimating detailed scene illumination from a single image using human faces. Unlike previous approaches that rely on low-order basis functions or distant point lights, our technique estimates illumination with higher precision using a non-parametric environment map. By leveraging the fact that faces can exhibit strong highlight reflections from various lighting directions, we develop a deep neural network to extract highlights from faces and then trace them back to the scene to obtain the environment map. To overcome the limited availability of real training data, we propose an unsupervised finetuning scheme based on the consistent diffuse chromaticity of a face across multiple images. Additionally, we reduce the blurring effect of skin reflectance on reflected light through a deconvolution process informed by prior knowledge of face material properties. Our approach outperforms existing techniques for highlight extraction and illumination estimation in various indoor and outdoor scenes.}"}, "1902.00595": {"original_text": "  Intuitively, human readers cope easily with errors in text; typos,\nmisspelling, word substitutions, etc. do not unduly disrupt natural reading.\nPrevious work indicates that letter transpositions result in increased reading\ntimes, but it is unclear if this effect generalizes to more natural errors. In\nthis paper, we report an eye-tracking study that compares two error types\n(letter transpositions and naturally occurring misspelling) and two error rates\n(10% or 50% of all words contain errors). We find that human readers show\nunimpaired comprehension in spite of these errors, but error words cause more\nreading difficulty than correct words. Also, transpositions are more difficult\nthan misspellings, and a high error rate increases difficulty for all words,\nincluding correct ones. We then present a computational model that uses\ncharacter-based (rather than traditional word-based) surprisal to account for\nthese results. The model explains that transpositions are harder than\nmisspellings because they contain unexpected letter combinations. It also\nexplains the error rate effect: upcoming words are more difficultto predict\nwhen the context is degraded, leading to increased surprisal.\n", "rewritten_text": "\\boxed{Humans can easily read texts with errors, such as typos, misspellings, and word substitutions, without significant disruption. However, research has shown that letter transpositions can increase reading times, but it's unclear if this applies to more natural errors. Our eye-tracking study compared letter transpositions and natural misspellings at two error rates (10% and 50% of all words) and found that readers can still comprehend the text despite errors, but error words cause more difficulty than correct words. Specifically, transpositions are more challenging than misspellings, and a high error rate increases difficulty for all words, including correct ones. We developed a computational model that uses character-based surprisal to explain these findings, showing that transpositions are harder due to unexpected letter combinations and that a high error rate makes upcoming words harder to predict, leading to increased surprisal.}"}, "cs/0406031": {"original_text": "  This paper describes a standalone, publicly-available implementation of the\nResolution of Anaphora Procedure (RAP) given by Lappin and Leass (1994). The\nRAP algorithm resolves third person pronouns, lexical anaphors, and identifies\npleonastic pronouns. Our implementation, JavaRAP, fills a current need in\nanaphora resolution research by providing a reference implementation that can\nbe benchmarked against current algorithms. The implementation uses the\nstandard, publicly available Charniak (2000) parser as input, and generates a\nlist of anaphora-antecedent pairs as output. Alternately, an in-place\nannotation or substitution of the anaphors with their antecedents can be\nproduced. Evaluation on the MUC-6 co-reference task shows that JavaRAP has an\naccuracy of 57.9%, similar to the performance given previously in the\nliterature (e.g., Preiss 2002).\n", "rewritten_text": "\\boxed{This paper presents JavaRAP, a publicly available implementation of the Resolution of Anaphora Procedure (RAP) algorithm, which resolves third person pronouns, lexical anaphors, and identifies pleonastic pronouns. JavaRAP fills a gap in anaphora resolution research by providing a benchmarkable reference implementation that takes input from the Charniak parser and outputs anaphora-antecedent pairs or in-place annotations/substitutions. Evaluation on the MUC-6 co-reference task shows that JavaRAP achieves an accuracy of 57.9%, comparable to previous results in the literature.}"}, "2010.13508": {"original_text": "  The SHApe Recovery from Partial textured 3D scans challenge, SHARP 2020, is\nthe first edition of a challenge fostering and benchmarking methods for\nrecovering complete textured 3D scans from raw incomplete data. SHARP 2020 is\norganised as a workshop in conjunction with ECCV 2020. There are two\ncomplementary challenges, the first one on 3D human scans, and the second one\non generic objects. Challenge 1 is further split into two tracks, focusing,\nfirst, on large body and clothing regions, and, second, on fine body details. A\nnovel evaluation metric is proposed to quantify jointly the shape\nreconstruction, the texture reconstruction and the amount of completed data.\nAdditionally, two unique datasets of 3D scans are proposed, to provide raw\nground-truth data for the benchmarks. The datasets are released to the\nscientific community. Moreover, an accompanying custom library of software\nroutines is also released to the scientific community. It allows for processing\n3D scans, generating partial data and performing the evaluation. Results of the\ncompetition, analysed in comparison to baselines, show the validity of the\nproposed evaluation metrics, and highlight the challenging aspects of the task\nand of the datasets. Details on the SHARP 2020 challenge can be found at\nhttps://cvi2.uni.lu/sharp2020/.\n", "rewritten_text": "\\boxed{\nThe SHARP 2020 challenge is a novel competition that aims to advance and evaluate methods for recovering complete textured 3D scans from incomplete data. Held in conjunction with ECCV 2020, the challenge consists of two complementary tracks: one focused on 3D human scans and another on generic objects. The human scan track is further divided into two sub-tracks, concentrating on large body and clothing regions, and fine body details. A new evaluation metric is introduced to jointly assess shape reconstruction, texture reconstruction, and completed data. To support the challenge, two unique datasets of 3D scans are released, along with a custom software library for processing 3D scans, generating partial data, and performing evaluations. The challenge results, compared to baselines, demonstrate the effectiveness of the proposed evaluation metrics and highlight the difficulties of the task and datasets. For more information, visit https://cvi2.uni.lu/sharp2020/.\n}"}, "2305.08844": {"original_text": "  Despite their unprecedented success, even the largest language models make\nmistakes. Similar to how humans learn and improve using feedback, previous work\nproposed providing language models with natural language feedback to guide them\nin repairing their outputs. Because human-generated critiques are expensive to\nobtain, researchers have devised learned critique generators in lieu of human\ncritics while assuming one can train downstream models to utilize generated\nfeedback. However, this approach does not apply to black-box or limited access\nmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\nlarge general-purpose language agents, fine-tuning is neither computationally\nnor spatially efficient as it results in multiple copies of the network. In\nthis work, we introduce RL4F (Reinforcement Learning for Feedback), a\nmulti-agent collaborative framework where the critique generator is trained to\nmaximize end-task performance of GPT-3, a fixed model more than 200 times its\nsize. RL4F produces critiques that help GPT-3 revise its outputs. We study\nthree datasets for action planning, summarization and alphabetization and show\nrelative improvements up to 10% in multiple text similarity metrics over other\nlearned, retrieval-augmented or prompting-based critique generators.\n", "rewritten_text": "\\boxed{Despite their achievements, large language models still make mistakes. To improve them, researchers have suggested using natural language feedback, similar to how humans learn from feedback. However, obtaining human feedback is costly, so learned critique generators have been developed as an alternative. But this approach doesn't work for models like ChatGPT, which can't be fine-tuned. Fine-tuning is also inefficient for large language agents. To address this, we propose RL4F, a collaborative framework that trains a critique generator to improve the performance of a fixed model, GPT-3, without fine-tuning. Our approach shows significant improvements in text similarity metrics across three datasets, outperforming other learned and retrieval-based critique generators.}"}, "2312.14697": {"original_text": "  Polarization information of the light can provide rich cues for computer\nvision and scene understanding tasks, such as the type of material, pose, and\nshape of the objects. With the advent of new and cheap polarimetric sensors,\nthis imaging modality is becoming accessible to a wider public for solving\nproblems such as pose estimation, 3D reconstruction, underwater navigation, and\ndepth estimation. However, we observe several limitations regarding the usage\nof this sensorial modality, as well as a lack of standards and publicly\navailable tools to analyze polarization images. Furthermore, although\npolarization camera manufacturers usually provide acquisition tools to\ninterface with their cameras, they rarely include processing algorithms that\nmake use of the polarization information. In this paper, we review recent\nadvances in applications that involve polarization imaging, including a\ncomprehensive survey of recent advances on polarization for vision and robotics\nperception tasks. We also introduce a complete software toolkit that provides\ncommon standards to communicate with and process information from most of the\nexisting micro-grid polarization cameras on the market. The toolkit also\nimplements several image processing algorithms for this modality, and it is\npublicly available on GitHub: https://github.com/vibot-lab/Pola4all_JEI_2023.\n", "rewritten_text": "\\boxed{Polarization imaging has the potential to provide valuable insights for computer vision and scene understanding tasks, such as object material, pose, and shape recognition. With the increasing availability of affordable polarimetric sensors, this technology is becoming more accessible for solving problems like pose estimation, 3D reconstruction, underwater navigation, and depth estimation. However, there are limitations and a lack of standards and publicly available tools for analyzing polarization images. While camera manufacturers provide acquisition tools, they often do not include processing algorithms that utilize polarization information. This paper reviews recent advances in polarization imaging applications, including a comprehensive survey of recent advances in polarization for vision and robotics perception tasks. Additionally, it introduces a publicly available software toolkit on GitHub, which provides common standards for communicating with and processing information from most micro-grid polarization cameras, as well as implementing several image processing algorithms for this modality.}"}, "1809.04191": {"original_text": "  To realize the promise of ubiquitous embedded deep network inference, it is\nessential to seek limits of energy and area efficiency. To this end,\nlow-precision networks offer tremendous promise because both energy and area\nscale down quadratically with the reduction in precision. Here we demonstrate\nResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on\nthe ImageNet classification benchmark that, at 8-bit precision exceed the\naccuracy of the full-precision baseline networks after one epoch of finetuning,\nthereby leveraging the availability of pretrained models. We also demonstrate\nResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn 4-bit models that match\nthe accuracy of the full-precision baseline networks -- the highest scores to\ndate. Surprisingly, the weights of the low-precision networks are very close\n(in cosine similarity) to the weights of the corresponding baseline networks,\nmaking training from scratch unnecessary.\n  We find that gradient noise due to quantization during training increases\nwith reduced precision, and seek ways to overcome this noise. The number of\niterations required by SGD to achieve a given training error is related to the\nsquare of (a) the distance of the initial solution from the final plus (b) the\nmaximum variance of the gradient estimates. Therefore, we (a) reduce solution\ndistance by starting with pretrained fp32 precision baseline networks and\nfine-tuning, and (b) combat gradient noise introduced by quantization by\ntraining longer and reducing learning rates. Sensitivity analysis indicates\nthat these simple techniques, coupled with proper activation function range\ncalibration to take full advantage of the limited precision, are sufficient to\ndiscover low-precision networks, if they exist, close to fp32 precision\nbaseline networks. The results herein provide evidence that 4-bits suffice for\nclassification.\n", "rewritten_text": "Here is the rephrased text:\n\n\\boxed{\nTo achieve efficient deep network inference, we need to minimize energy and area usage. One promising approach is to use low-precision networks, which can reduce energy and area requirements quadratically. We demonstrate that several popular neural networks (ResNet, Inception, Densenet, and VGG) can achieve high accuracy on the ImageNet classification task using 8-bit precision, and even match the accuracy of full-precision models using 4-bit precision. Surprisingly, the weights of these low-precision networks are very similar to those of the full-precision models, making training from scratch unnecessary. We also find that reducing precision increases gradient noise during training, but this can be overcome by fine-tuning pre-trained models, training for longer periods, and reducing learning rates. Our results show that 4-bit precision is sufficient for image classification tasks."}, "2103.14799": {"original_text": "  Image representation is an important topic in computer vision and pattern\nrecognition. It plays a fundamental role in a range of applications towards\nunderstanding visual contents. Moment-based image representation has been\nreported to be effective in satisfying the core conditions of semantic\ndescription due to its beneficial mathematical properties, especially geometric\ninvariance and independence. This paper presents a comprehensive survey of the\northogonal moments for image representation, covering recent advances in\nfast/accurate calculation, robustness/invariance optimization, definition\nextension, and application. We also create a software package for a variety of\nwidely-used orthogonal moments and evaluate such methods in a same base. The\npresented theory analysis, software implementation, and evaluation results can\nsupport the community, particularly in developing novel techniques and\npromoting real-world applications.\n", "rewritten_text": "\\boxed{Image representation is a crucial aspect of computer vision and pattern recognition, playing a vital role in understanding visual content. One effective approach is moment-based image representation, which satisfies key conditions for semantic description due to its geometric invariance and independence properties. This paper provides a thorough review of orthogonal moments for image representation, covering recent developments in efficient calculation, robustness optimization, definition extension, and application. Additionally, a software package is created to implement and evaluate various widely-used orthogonal moments, providing a valuable resource for the community to develop new techniques and promote real-world applications.}"}, "2406.19297": {"original_text": "  Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we\ndemonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)\napproach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.\n", "rewritten_text": "\\boxed{\nContinual learning in visual question answering (VQA) involves training a model on a sequence of tasks while maintaining performance on previous tasks. However, existing approaches overlook the impact of multimodal inputs on learning dynamics. We show that different modalities (e.g., vision and language) learn at varying rates across tasks, even in state-of-the-art models. To address this, we propose a modality-aware feature distillation approach (MAFED) that outperforms baselines in three multimodal continual learning settings. Our results highlight the importance of considering modality-specific dynamics to prevent forgetting in multimodal continual learning.\n}"}, "2410.15865": {"original_text": "  Grammatical features such as number and gender serve two central functions in\nhuman languages. While they encode salient semantic attributes like numerosity\nand animacy, they also offload sentence processing cost by predictably linking\nwords together via grammatical agreement. Grammars exhibit consistent\norganizational patterns across diverse languages, invariably rooted in a\nsemantic foundation, a widely confirmed but still theoretically unexplained\nphenomenon. To explain the basis of universal grammatical patterns, we unify\ntwo fundamental properties of grammar, semantic encoding and agreement-based\npredictability, into a single information-theoretic objective under cognitive\nconstraints. Our analyses reveal that grammatical organization provably\ninherits from perceptual attributes, but that grammars empirically prioritize\nfunctional goals, promoting efficient language processing over semantic\nencoding.\n", "rewritten_text": "\\boxed{ \nIn human languages, grammatical features like number and gender have two main purposes. They convey important semantic information, such as quantity and animacy, and they also simplify sentence processing by establishing predictable connections between words through grammatical agreement. Despite their differences, languages exhibit consistent patterns in their grammatical structures, which are rooted in semantics but lack a theoretical explanation. To understand the basis of these universal patterns, we combine the two fundamental aspects of grammar - semantic encoding and predictability based on agreement - into a single objective that is constrained by cognitive factors. Our analysis shows that grammatical organization is derived from perceptual attributes, but languages prioritize functional goals, favoring efficient language processing over semantic encoding. }"}, "2306.05119": {"original_text": "  Factuality is important to dialogue summarization. Factual error correction\n(FEC) of model-generated summaries is one way to improve factuality. Current\nFEC evaluation that relies on factuality metrics is not reliable and detailed\nenough. To address this problem, we are the first to manually annotate a FEC\ndataset for dialogue summarization containing 4000 items and propose FERRANTI,\na fine-grained evaluation framework based on reference correction that\nautomatically evaluates the performance of FEC models on different error\ncategories. Using this evaluation framework, we conduct sufficient experiments\nwith FEC approaches under a variety of settings and find the best training\nmodes and significant differences in the performance of the existing approaches\non different factual error categories.\n", "rewritten_text": "\\boxed{Ensuring the accuracy of facts is crucial in dialogue summarization. To achieve this, factual error correction (FEC) of model-generated summaries is essential. However, current evaluation methods for FEC are inadequate and unreliable. To overcome this limitation, we have created a dataset of 4000 annotated items for FEC in dialogue summarization and developed FERRANTI, a comprehensive evaluation framework that assesses the performance of FEC models based on reference correction and categorizes errors. Using this framework, we have conducted extensive experiments with various FEC approaches and identified the most effective training modes and significant performance differences among existing approaches in correcting different types of factual errors.}"}, "2410.16190": {"original_text": "  This work explores how human judgement about salient regions of an image can\nbe introduced into deep convolutional neural network (DCNN) training.\nTraditionally, training of DCNNs is purely data-driven. This often results in\nlearning features of the data that are only coincidentally correlated with\nclass labels. Human saliency can guide network training using our proposed new\ncomponent of the loss function that ConveYs Brain Oversight to Raise\nGeneralization (CYBORG) and penalizes the model for using non-salient regions.\nThis mechanism produces DCNNs achieving higher accuracy and generalization\ncompared to using the same training data without human salience. Experimental\nresults demonstrate that CYBORG applies across multiple network architectures\nand problem domains (detection of synthetic faces, iris presentation attacks\nand anomalies in chest X-rays), while requiring significantly less data than\ntraining without human saliency guidance. Visualizations show that\nCYBORG-trained models' saliency is more consistent across independent training\nruns than traditionally-trained models, and also in better agreement with\nhumans. To lower the cost of collecting human annotations, we also explore\nusing deep learning to provide automated annotations. CYBORG training of CNNs\naddresses important issues such as reducing the appetite for large training\nsets, increasing interpretability, and reducing fragility by generalizing\nbetter to new types of data.\n", "rewritten_text": "\\boxed{\nThis research introduces a novel approach to incorporate human judgment into the training of deep convolutional neural networks (DCNNs). Unlike traditional data-driven training, which often focuses on coincidental correlations, our method utilizes human saliency to guide network training through a new loss function component called ConveYs Brain Oversight to Raise Generalization (CYBORG). This approach penalizes the model for relying on non-salient regions, resulting in DCNNs with higher accuracy and generalization capabilities. Experimental results demonstrate the effectiveness of CYBORG across various network architectures and problem domains, requiring significantly less training data. Visualizations show that CYBORG-trained models exhibit more consistent saliency across independent training runs and better align with human annotations. To reduce the cost of human annotations, we also explore using deep learning for automated annotations. The CYBORG training method addresses key issues, including reducing the need for large training sets, increasing interpretability, and improving generalization to new data types.\n}"}, "2311.07611": {"original_text": "  In this study we intentionally introduce biases into large language model\nresponses in an attempt to create specific personas for interactive media\npurposes. We explore the differences between open source models such as\nFalcon-7b and the GPT-4 model from Open AI, and we quantify some differences in\nresponses afforded by the two systems. We find that the guardrails in the GPT-4\nmixture of experts models with a supervisor, while useful in assuring AI\nalignment in general, are detrimental in trying to construct personas with a\nvariety of uncommon viewpoints. This study aims to set the groundwork for\nfuture exploration in intentional biases of large language models such that\nthese practices can be applied in the creative field, and new forms of media.\n", "rewritten_text": "\\boxed{This study deliberately injects biases into large language model responses to craft distinct personas for interactive media. We compare open-source models like Falcon-7b with Open AI's GPT-4 model, quantifying their response differences. Our findings suggest that while GPT-4's expert models with supervisor guardrails ensure AI alignment, they hinder the creation of personas with diverse, unconventional views. This research lays the foundation for future exploration of intentional biases in large language models, enabling their application in creative fields and innovative media forms.}"}, "2409.01522": {"original_text": "  Long-term motion generation is a challenging task that requires producing\ncoherent and realistic sequences over extended durations. Current methods\nprimarily rely on framewise motion representations, which capture only static\nspatial details and overlook temporal dynamics. This approach leads to\nsignificant redundancy across the temporal dimension, complicating the\ngeneration of effective long-term motion. To overcome these limitations, we\nintroduce the novel concept of Lagrangian Motion Fields, specifically designed\nfor long-term motion generation. By treating each joint as a Lagrangian\nparticle with uniform velocity over short intervals, our approach condenses\nmotion representations into a series of \"supermotions\" (analogous to\nsuperpixels). This method seamlessly integrates static spatial information with\ninterpretable temporal dynamics, transcending the limitations of existing\nnetwork architectures and motion sequence content types. Our solution is\nversatile and lightweight, eliminating the need for neural network\npreprocessing. Our approach excels in tasks such as long-term music-to-dance\ngeneration and text-to-motion generation, offering enhanced efficiency,\nsuperior generation quality, and greater diversity compared to existing\nmethods. Additionally, the adaptability of Lagrangian Motion Fields extends to\napplications like infinite motion looping and fine-grained controlled motion\ngeneration, highlighting its broad utility. Video demonstrations are available\nat \\url{https://plyfager.github.io/LaMoG}.\n", "rewritten_text": "\\boxed{\nGenerating motion over long periods of time is a difficult task that requires creating coherent and realistic sequences. Current methods focus on individual frames, which only capture static details and ignore how things change over time. This approach leads to repetitive information across time, making it hard to generate effective long-term motion. To overcome this, we introduce Lagrangian Motion Fields, a new concept designed for long-term motion generation. By treating each joint as a particle moving at a constant speed over short intervals, we can condense motion into a series of \"supermotions\" that combine static and dynamic information. Our approach is flexible, efficient, and doesn't require preprocessing with neural networks. It excels in tasks like generating dance moves from music or motion from text, producing higher-quality and more diverse results than existing methods. Additionally, Lagrangian Motion Fields can be used for infinite motion looping and fine-grained controlled motion generation, demonstrating its broad applicability. Video demonstrations are available at https://plyfager.github.io/LaMoG.\n}"}, "1704.06756": {"original_text": "  We have developed convolutional neural networks (CNN) for a facial expression\nrecognition task. The goal is to classify each facial image into one of the\nseven facial emotion categories considered in this study. We trained CNN models\nwith different depth using gray-scale images. We developed our models in Torch\nand exploited Graphics Processing Unit (GPU) computation in order to expedite\nthe training process. In addition to the networks performing based on raw pixel\ndata, we employed a hybrid feature strategy by which we trained a novel CNN\nmodel with the combination of raw pixel data and Histogram of Oriented\nGradients (HOG) features. To reduce the overfitting of the models, we utilized\ndifferent techniques including dropout and batch normalization in addition to\nL2 regularization. We applied cross validation to determine the optimal\nhyper-parameters and evaluated the performance of the developed models by\nlooking at their training histories. We also present the visualization of\ndifferent layers of a network to show what features of a face can be learned by\nCNN models.\n", "rewritten_text": "\\boxed{We designed and trained convolutional neural networks (CNNs) to recognize facial expressions, categorizing images into seven emotional categories. Our approach involved developing models of varying depths using gray-scale images, leveraging Torch and GPU computation to accelerate training. We explored two strategies: one using raw pixel data and another combining raw pixels with Histogram of Oriented Gradients (HOG) features. To prevent overfitting, we employed techniques like dropout, batch normalization, and L2 regularization. Through cross-validation, we identified optimal hyper-parameters and evaluated model performance by analyzing training histories. Additionally, we visualized network layers to illustrate the facial features learned by the CNN models.}"}, "2312.17292": {"original_text": "  Word embedding methods (WEMs) are extensively used for representing text\ndata. The dimensionality of these embeddings varies across various tasks and\nimplementations. The effect of dimensionality change on the accuracy of the\ndownstream task is a well-explored question. However, how the dimensionality\nchange affects the bias of word embeddings needs to be investigated. Using the\nEnglish Wikipedia corpus, we study this effect for two static (Word2Vec and\nfastText) and two context-sensitive (ElMo and BERT) WEMs. We have two\nobservations. First, there is a significant variation in the bias of word\nembeddings with the dimensionality change. Second, there is no uniformity in\nhow the dimensionality change affects the bias of word embeddings. These\nfactors should be considered while selecting the dimensionality of word\nembeddings.\n", "rewritten_text": "\\boxed{The impact of dimensionality on the accuracy of word embeddings has been well-studied, but its effect on bias has not been thoroughly examined. This study investigates how dimensionality changes affect the bias of word embeddings using four different methods (Word2Vec, fastText, ElMo, and BERT) on the English Wikipedia corpus. The results show that the bias of word embeddings varies significantly with dimensionality changes, and that there is no consistent pattern in how this bias is affected. These findings highlight the importance of considering dimensionality when selecting word embeddings to minimize bias.}"}, "2401.07745": {"original_text": "  Open-vocabulary 3D instance segmentation is cutting-edge for its ability to\nsegment 3D instances without predefined categories. However, progress in 3D\nlags behind its 2D counterpart due to limited annotated 3D data. To address\nthis, recent works first generate 2D open-vocabulary masks through 2D models\nand then merge them into 3D instances based on metrics calculated between two\nneighboring frames. In contrast to these local metrics, we propose a novel\nmetric, view consensus rate, to enhance the utilization of multi-view\nobservations. The key insight is that two 2D masks should be deemed part of the\nsame 3D instance if a significant number of other 2D masks from different views\ncontain both these two masks. Using this metric as edge weight, we construct a\nglobal mask graph where each mask is a node. Through iterative clustering of\nmasks showing high view consensus, we generate a series of clusters, each\nrepresenting a distinct 3D instance. Notably, our model is training-free.\nThrough extensive experiments on publicly available datasets, including\nScanNet++, ScanNet200 and MatterPort3D, we demonstrate that our method achieves\nstate-of-the-art performance in open-vocabulary 3D instance segmentation. Our\nproject page is at https://pku-epic.github.io/MaskClustering.\n", "rewritten_text": "\\boxed{\nOpen-vocabulary 3D instance segmentation is a cutting-edge technology that can segment 3D instances without prior knowledge of categories. However, its development lags behind its 2D counterpart due to the limited availability of annotated 3D data. To overcome this limitation, recent studies have proposed generating 2D open-vocabulary masks using 2D models and then merging them into 3D instances based on local metrics calculated between adjacent frames. In contrast, we introduce a novel metric called view consensus rate, which leverages multi-view observations to improve the merging process. Our approach is based on the idea that two 2D masks are likely to belong to the same 3D instance if they are consistently observed in multiple views. We construct a global mask graph, where each mask is a node, and use the view consensus rate as edge weights. Through iterative clustering of masks with high view consensus, we generate distinct 3D instances. Notably, our method does not require training. Our experiments on publicly available datasets, including ScanNet++, ScanNet200, and MatterPort3D, demonstrate that our approach achieves state-of-the-art performance in open-vocabulary 3D instance segmentation. Our project page is available at https://pku-epic.github.io/MaskClustering.\n}"}, "2404.05916": {"original_text": "  Echocardiography segmentation for cardiac analysis is time-consuming and\nresource-intensive due to the variability in image quality and the necessity to\nprocess scans from various standard views. While current automated segmentation\nmethods in echocardiography show promising performance, they are trained on\nspecific scan views to analyze corresponding data. However, this solution has a\nlimitation as the number of required models increases with the number of\nstandard views. To address this, in this paper, we present a prompt-driven\nuniversal method for view-agnostic echocardiography analysis. Considering the\ndomain shift between standard views, we first introduce a method called prompt\nmatching, aimed at learning prompts specific to different views by matching\nprompts and querying input embeddings using a pre-trained vision model. Then,\nwe utilized a pre-trained medical language model to align textual information\nwith pixel data for accurate segmentation. Extensive experiments on three\nstandard views showed that our approach significantly outperforms the\nstate-of-the-art universal methods and achieves comparable or even better\nperformances over the segmentation model trained and tested on same views.\n", "rewritten_text": "\\boxed{Automated echocardiography segmentation is a time-consuming and resource-intensive process due to image quality variability and the need to process scans from multiple standard views. While current methods show promise, they are limited by their view-specific training, requiring multiple models for different views. This paper proposes a universal, prompt-driven approach for view-agnostic echocardiography analysis, addressing the domain shift between views through prompt matching and alignment of textual information with pixel data using pre-trained models. Experimental results on three standard views demonstrate significant outperformance of state-of-the-art universal methods and comparable or better performance to view-specific models.}"}, "1908.09884": {"original_text": "  We consider the problem of discovering novel object categories in an image\ncollection. While these images are unlabelled, we also assume prior knowledge\nof related but different image classes. We use such prior knowledge to reduce\nthe ambiguity of clustering, and improve the quality of the newly discovered\nclasses. Our contributions are twofold. The first contribution is to extend\nDeep Embedded Clustering to a transfer learning setting; we also improve the\nalgorithm by introducing a representation bottleneck, temporal ensembling, and\nconsistency. The second contribution is a method to estimate the number of\nclasses in the unlabelled data. This also transfers knowledge from the known\nclasses, using them as probes to diagnose different choices for the number of\nclasses in the unlabelled subset. We thoroughly evaluate our method,\nsubstantially outperforming state-of-the-art techniques in a large number of\nbenchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN.\n", "rewritten_text": "\\boxed{This paper tackles the problem of identifying new object categories in an unlabelled image collection, leveraging prior knowledge of related image classes to improve clustering quality. The proposed approach extends Deep Embedded Clustering to a transfer learning setting, incorporating a representation bottleneck, temporal ensembling, and consistency. Additionally, a method is introduced to estimate the number of classes in the unlabelled data by utilizing known classes as probes. The approach is extensively evaluated and demonstrates significant performance improvements over state-of-the-art techniques on various benchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN.}"}, "2212.08334": {"original_text": "  Scene understanding has made tremendous progress over the past few years, as\ndata acquisition systems are now providing an increasing amount of data of\nvarious modalities (point cloud, depth, RGB...). However, this improvement\ncomes at a large cost on computation resources and data annotation\nrequirements. To analyze geometric information and images jointly, many\napproaches rely on both a 2D loss and 3D loss, requiring not only 2D per\npixel-labels but also 3D per-point labels. However, obtaining a 3D groundtruth\nis challenging, time-consuming and error-prone. In this paper, we show that\nimage segmentation can benefit from 3D geometric information without requiring\na 3D groundtruth, by training the geometric feature extraction and the 2D\nsegmentation network jointly, in an end-to-end fashion, using only the 2D\nsegmentation loss. Our method starts by extracting a map of 3D features\ndirectly from a provided point cloud by using a lightweight 3D neural network.\nThe 3D feature map, merged with the RGB image, is then used as an input to a\nclassical image segmentation network. Our method can be applied to many 2D\nsegmentation networks, improving significantly their performance with only a\nmarginal network weight increase and light input dataset requirements, since no\n3D groundtruth is required.\n", "rewritten_text": "\\boxed{Recent advancements in scene understanding have been driven by the availability of large amounts of multimodal data. However, this progress comes at the cost of increased computational resources and data annotation requirements. Traditional approaches rely on both 2D and 3D losses, necessitating time-consuming and error-prone 3D ground truth labels. This paper proposes a novel method that leverages 3D geometric information to improve image segmentation without requiring 3D ground truth labels. By jointly training a 3D feature extraction network and a 2D segmentation network using only 2D segmentation loss, our approach can significantly enhance the performance of various 2D segmentation networks with minimal network weight increase and reduced input dataset requirements.}"}, "2309.09379": {"original_text": "  Nowadays, photogrammetrically derived point clouds are widely used in many\ncivilian applications due to their low cost and flexibility in acquisition.\nTypically, photogrammetric point clouds are assessed through reference data\nsuch as LiDAR point clouds. However, when reference data are not available, the\nassessment of photogrammetric point clouds may be challenging. Since these\npoint clouds are algorithmically derived, their accuracies and precisions are\nhighly varying with the camera networks, scene complexity, and dense image\nmatching (DIM) algorithms, and there is no standard error metric to determine\nper-point errors. The theory of internal reliability of camera networks has\nbeen well studied through first-order error estimation of Bundle Adjustment\n(BA), which is used to understand the errors of 3D points assuming known\nmeasurement errors. However, the measurement errors of the DIM algorithms are\nintricate to an extent that every single point may have its error function\ndetermined by factors such as pixel intensity, texture entropy, and surface\nsmoothness. Despite the complexity, there exist a few common metrics that may\naid the process of estimating the posterior reliability of the derived points,\nespecially in a multi-view stereo (MVS) setup when redundancies are present. In\nthis paper, by using an aerial oblique photogrammetric block with LiDAR\nreference data, we analyze several internal matching metrics within a common\nMVS framework, including statistics in ray convergence, intersection angles,\nDIM energy, etc.\n", "rewritten_text": "\\boxed{\nPhotogrammetric point clouds are increasingly used in various civilian applications due to their affordability and flexibility. However, evaluating their accuracy can be challenging when reference data is unavailable. The accuracy of these point clouds depends on factors such as camera networks, scene complexity, and dense image matching algorithms, making it difficult to determine per-point errors. While the internal reliability of camera networks has been studied through Bundle Adjustment, the measurement errors of dense image matching algorithms are complex and influenced by various factors. This paper explores the use of internal matching metrics, including ray convergence, intersection angles, and DIM energy, within a multi-view stereo framework to estimate the reliability of derived points, using an aerial oblique photogrammetric block with LiDAR reference data.\n}"}, "2306.05061": {"original_text": "  Multi-task visual perception has a wide range of applications in scene\nunderstanding such as autonomous driving. In this work, we devise an efficient\nunified framework to solve multiple common perception tasks, including instance\nsegmentation, semantic segmentation, monocular 3D detection, and depth\nestimation. Simply sharing the same visual feature representations for these\ntasks impairs the performance of tasks, while independent task-specific feature\nextractors lead to parameter redundancy and latency. Thus, we design two\nfeature-merge branches to learn feature basis, which can be useful to, and thus\nshared by, multiple perception tasks. Then, each task takes the corresponding\nfeature basis as the input of the prediction task head to fulfill a specific\ntask. In particular, one feature merge branch is designed for instance-level\nrecognition the other for dense predictions. To enhance inter-branch\ncommunication, the instance branch passes pixel-wise spatial information of\neach instance to the dense branch using efficient dynamic convolution\nweighting. Moreover, a simple but effective dynamic routing mechanism is\nproposed to isolate task-specific features and leverage common properties among\ntasks. Our proposed framework, termed D2BNet, demonstrates a unique approach to\nparameter-efficient predictions for multi-task perception. In addition, as\ntasks benefit from co-training with each other, our solution achieves on par\nresults on partially labeled settings on nuScenes and outperforms previous\nworks for 3D detection and depth estimation on the Cityscapes dataset with full\nsupervision.\n", "rewritten_text": "\\boxed{\nThis paper presents a unified framework, called D2BNet, for multi-task visual perception, which is essential for scene understanding in applications like autonomous driving. The framework efficiently solves four common perception tasks: instance segmentation, semantic segmentation, monocular 3D detection, and depth estimation. To avoid performance impairment and parameter redundancy, the framework uses two feature-merge branches to learn shared feature bases for multiple tasks. Each task then uses its corresponding feature basis as input to its prediction task head. The branches are designed for instance-level recognition and dense predictions, and they communicate through efficient dynamic convolution weighting and a dynamic routing mechanism. The proposed framework achieves parameter-efficient predictions and outperforms previous works on Cityscapes and nuScenes datasets."}, "2107.00977": {"original_text": "  Cardiac ultrasound imaging is used to diagnose various heart diseases. Common\nanalysis pipelines involve manual processing of the video frames by expert\nclinicians. This suffers from intra- and inter-observer variability. We propose\na novel approach to ultrasound video analysis using a transformer architecture\nbased on a Residual Auto-Encoder Network and a BERT model adapted for token\nclassification. This enables videos of any length to be processed. We apply our\nmodel to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection\nand the automated computation of the left ventricular ejection fraction. We\nachieve an average frame distance of 3.36 frames for the ES and 7.17 frames for\nthe ED on videos of arbitrary length. Our end-to-end learnable approach can\nestimate the ejection fraction with a MAE of 5.95 and $R^2$ of 0.52 in 0.15s\nper video, showing that segmentation is not the only way to predict ejection\nfraction. Code and models are available at https://github.com/HReynaud/UVT.\n", "rewritten_text": "\\boxed{A novel approach to cardiac ultrasound video analysis is proposed, utilizing a transformer architecture based on a Residual Auto-Encoder Network and a BERT model adapted for token classification. This method enables the processing of videos of any length and achieves accurate detection of End-Systolic and End-Diastolic frames, as well as automated computation of left ventricular ejection fraction, outperforming traditional manual analysis pipelines that suffer from variability.}"}, "1710.05994": {"original_text": "  Recent advancements in neutron and X-ray sources, instrumentation and data\ncollection modes have significantly increased the experimental data size (which\ncould easily contain 10$^{8}$ -- 10$^{10}$ data points), so that conventional\nvolumetric visualization approaches become inefficient for both still imaging\nand interactive OpenGL rendition in a 3D setting. We introduce a new approach\nbased on the unsupervised machine learning algorithm, Density-Based Spatial\nClustering of Applications with Noise (DBSCAN), to efficiently analyze and\nvisualize large volumetric datasets. Here we present two examples of analyzing\nand visualizing datasets from the diffuse scattering experiment of a single\ncrystal sample and the tomographic reconstruction of a neutron scanning of a\nturbine blade. We found that by using the intensity as the weighting factor in\nthe clustering process, DBSCAN becomes very effective in de-noising and\nfeature/boundary detection, and thus enables better visualization of the\nhierarchical internal structures of the neutron scattering data.\n", "rewritten_text": "\\boxed{\nRecent breakthroughs in neutron and X-ray sources, instrumentation, and data collection modes have led to a significant increase in experimental data size, making conventional volumetric visualization approaches inefficient. To address this, we propose a novel approach utilizing the unsupervised machine learning algorithm DBSCAN to efficiently analyze and visualize large volumetric datasets. We demonstrate the effectiveness of this approach through two examples: analyzing diffuse scattering data from a single crystal sample and tomographic reconstruction of a neutron-scanned turbine blade. By using intensity as a weighting factor in the clustering process, DBSCAN proves highly effective in denoising and feature/boundary detection, enabling better visualization of the hierarchical internal structures in neutron scattering data.\n}"}, "1812.0081": {"original_text": "  It is well known that the generative adversarial nets (GANs) are remarkably\ndifficult to train. The recently proposed Wasserstein GAN (WGAN) creates\nprincipled research directions towards addressing these issues. But we found in\npractice that gradient penalty WGANs (GP-WGANs) still suffer from training\ninstability. In this paper, we combine a Total Variational (TV) regularizing\nterm into the WGAN formulation instead of weight clipping or gradient penalty,\nwhich implies that the Lipschitz constraint is enforced on the critic network.\nOur proposed method is more stable at training than GP-WGANs and works well\nacross varied GAN architectures. We also present a method to control the\ntrade-off between image diversity and visual quality. It does not bring any\ncomputation burden.\n", "rewritten_text": "\\boxed{Training generative adversarial networks (GANs) is notoriously challenging. While the Wasserstein GAN (WGAN) has made progress in addressing these issues, we found that gradient penalty WGANs (GP-WGANs) still struggle with training instability. To overcome this, we introduce a Total Variational (TV) regularizing term into the WGAN formulation, which enforces the Lipschitz constraint on the critic network. Our approach demonstrates improved training stability compared to GP-WGANs, is adaptable to various GAN architectures, and allows for controlling the trade-off between image diversity and visual quality without adding computational overhead.}"}, "1709.06247": {"original_text": "  Most of convolutional neural networks share the same characteristic: each\nconvolutional layer is followed by a nonlinear activation layer where Rectified\nLinear Unit (ReLU) is the most widely used. In this paper, we argue that the\ndesigned structure with the equal ratio between these two layers may not be the\nbest choice since it could result in the poor generalization ability. Thus, we\ntry to investigate a more suitable method on using ReLU to explore the better\nnetwork architectures. Specifically, we propose a proportional module to keep\nthe ratio between convolution and ReLU amount to be N:M (N>M). The proportional\nmodule can be applied in almost all networks with no extra computational cost\nto improve the performance. Comprehensive experimental results indicate that\nthe proposed method achieves better performance on different benchmarks with\ndifferent network architectures, thus verify the superiority of our work.\n", "rewritten_text": "\\boxed{Most convolutional neural networks follow a standard structure, where each convolutional layer is paired with a nonlinear activation layer, typically using Rectified Linear Unit (ReLU). However, this uniform design may not be optimal, potentially leading to poor generalization. This paper explores alternative approaches to using ReLU, proposing a proportional module that maintains a ratio of N:M (N>M) between convolutional and ReLU layers. This module can be applied to various networks without additional computational cost, and experimental results demonstrate its effectiveness in improving performance across different benchmarks and architectures.}"}, "2201.06674": {"original_text": "  Providing feedback on the argumentation of the learner is essential for\ndeveloping critical thinking skills, however, it requires a lot of time and\neffort. To mitigate the overload on teachers, we aim to automate a process of\nproviding feedback, especially giving diagnostic comments which point out the\nweaknesses inherent in the argumentation. It is recommended to give specific\ndiagnostic comments so that learners can recognize the diagnosis without\nmisinterpretation. However, it is not obvious how the task of providing\nspecific diagnostic comments should be formulated. We present a formulation of\nthe task as template selection and slot filling to make an automatic evaluation\neasier and the behavior of the model more tractable. The key to the formulation\nis the possibility of creating a template set that is sufficient for practical\nuse. In this paper, we define three criteria that a template set should\nsatisfy: expressiveness, informativeness, and uniqueness, and verify the\nfeasibility of creating a template set that satisfies these criteria as a first\ntrial. We will show that it is feasible through an annotation study that\nconverts diagnostic comments given in a text to a template format. The corpus\nused in the annotation study is publicly available.\n", "rewritten_text": "\\boxed{Providing feedback on learners' argumentation is crucial for developing critical thinking skills, but it's a time-consuming task for teachers. To alleviate this burden, we aim to automate the feedback process, focusing on diagnostic comments that identify weaknesses in argumentation. To ensure learners understand the feedback correctly, specific diagnostic comments are necessary. However, formulating this task is challenging. We propose a solution by framing the task as template selection and slot filling, making automatic evaluation easier and model behavior more predictable. The key is to create a practical template set that meets three criteria: expressiveness, informativeness, and uniqueness. We verify the feasibility of creating such a template set through an annotation study, where we convert diagnostic comments into a template format. The study's corpus is publicly available.}"}, "2008.01167": {"original_text": "  Current anchor-free object detectors label all the features that spatially\nfall inside a predefined central region of a ground-truth box as positive. This\napproach causes label noise during training, since some of these positively\nlabeled features may be on the background or an occluder object, or they are\nsimply not discriminative features. In this paper, we propose a new labeling\nstrategy aimed to reduce the label noise in anchor-free detectors. We sum-pool\npredictions stemming from individual features into a single prediction. This\nallows the model to reduce the contributions of non-discriminatory features\nduring training. We develop a new one-stage, anchor-free object detector,\nPPDet, to employ this labeling strategy during training and a similar\nprediction pooling method during inference. On the COCO dataset, PPDet achieves\nthe best performance among anchor-free top-down detectors and performs on-par\nwith the other state-of-the-art methods. It also outperforms all major\none-stage and two-stage methods in small object detection (${AP}_{S}$ $31.4$).\nCode is available at https://github.com/nerminsamet/ppdet\n", "rewritten_text": "\\boxed{\nCurrent object detectors that don't use anchors label all features within a ground-truth box's central region as positive, which can lead to noisy labels during training. This is because some of these features may be part of the background, an occluding object, or simply not distinctive. To address this issue, we propose a new labeling strategy that reduces label noise in anchor-free detectors. Our approach involves sum-pooling predictions from individual features into a single prediction, allowing the model to downplay non-discriminative features during training. We develop a new one-stage, anchor-free object detector called PPDet, which employs this labeling strategy during training and a similar prediction pooling method during inference. Our results on the COCO dataset show that PPDet outperforms other anchor-free top-down detectors and is on par with state-of-the-art methods. Additionally, it excels in small object detection, achieving an AP_S of 31.4. The code is available at https://github.com/nerminsamet/ppdet.\n}"}, "1807.10657": {"original_text": "  Saliency map estimation in computer vision aims to estimate the locations\nwhere people gaze in images. Since people tend to look at objects in images,\nthe parameters of the model pretrained on ImageNet for image classification are\nuseful for the saliency map estimation. However, there is no research on the\nrelationship between the image classification accuracy and the performance of\nthe saliency map estimation. In this paper, it is shown that there is a strong\ncorrelation between image classification accuracy and saliency map estimation\naccuracy. We also investigated the effective architecture based on multi scale\nimages and the upsampling layers to refine the saliency-map resolution. Our\nmodel achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003\ndatasets. In the MIT Saliency Benchmark, our model achieved the best\nperformance in some metrics and competitive results in the other metrics.\n", "rewritten_text": "\\boxed{Estimating saliency maps in computer vision involves identifying where people focus their attention in images. Interestingly, models pre-trained on ImageNet for image classification can be useful for this task, as people tend to gaze at objects in images. This paper explores the connection between image classification accuracy and saliency map estimation performance, revealing a strong correlation between the two. The authors also investigate the effectiveness of multi-scale images and upsampling layers in refining saliency map resolution. Their model achieves state-of-the-art accuracy on several datasets, including PASCAL-S, OSIE, and MIT1003, and demonstrates top performance in some metrics and competitive results in others in the MIT Saliency Benchmark.}"}, "1812.05642": {"original_text": "  Unsupervised learning for geometric perception (depth, optical flow, etc.) is\nof great interest to autonomous systems. Recent works on unsupervised learning\nhave made considerable progress on perceiving geometry; however, they usually\nignore the coherence of objects and perform poorly under scenarios with dark\nand noisy environments. In contrast, supervised learning algorithms, which are\nrobust, require large labeled geometric dataset. This paper introduces SIGNet,\na novel framework that provides robust geometry perception without requiring\ngeometrically informative labels. Specifically, SIGNet integrates semantic\ninformation to make depth and flow predictions consistent with objects and\nrobust to low lighting conditions. SIGNet is shown to improve upon the\nstate-of-the-art unsupervised learning for depth prediction by 30% (in squared\nrelative error). In particular, SIGNet improves the dynamic object class\nperformance by 39% in depth prediction and 29% in flow prediction. Our code\nwill be made available at https://github.com/mengyuest/SIGNet\n", "rewritten_text": "\\boxed{\nAutonomous systems can benefit greatly from unsupervised learning for geometric perception, such as depth and optical flow. While recent advances have been made, they often neglect object coherence and struggle in dark or noisy environments. In contrast, supervised learning methods are robust but require large labeled datasets. This paper proposes SIGNet, a novel framework that achieves robust geometry perception without relying on geometric labels. By incorporating semantic information, SIGNet ensures consistent depth and flow predictions that are resilient to low lighting conditions. The results show a 30% improvement in depth prediction and significant enhancements in dynamic object class performance, with 39% and 29% improvements in depth and flow prediction, respectively. The code is available at https://github.com/mengyuest/SIGNet.\n}"}, "2212.09098": {"original_text": "  Fine-grained semantic segmentation of a person's face and head, including\nfacial parts and head components, has progressed a great deal in recent years.\nHowever, it remains a challenging task, whereby considering ambiguous\nocclusions and large pose variations are particularly difficult. To overcome\nthese difficulties, we propose a novel framework termed Mask-FPAN. It uses a\nde-occlusion module that learns to parse occluded faces in a semi-supervised\nway. In particular, face landmark localization, face occlusionstimations, and\ndetected head poses are taken into account. A 3D morphable face model combined\nwith the UV GAN improves the robustness of 2D face parsing. In addition, we\nintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for face\nparing work. The proposed Mask-FPAN framework addresses the face parsing\nproblem in the wild and shows significant performance improvements with MIOU\nfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging face\ndatasets.\n", "rewritten_text": "\\boxed{\nRecent advancements in fine-grained semantic segmentation of facial features and head components have been significant, but the task remains challenging due to ambiguous occlusions and large pose variations. To tackle these issues, we introduce Mask-FPAN, a novel framework that utilizes a semi-supervised de-occlusion module to parse occluded faces. This module considers face landmark localization, face occlusion estimation, and detected head poses. Additionally, we incorporate a 3D morphable face model with the UV GAN to enhance the robustness of 2D face parsing. We also introduce two new high-quality datasets, FaceOccMask-HQ and CelebAMaskOcc-HQ, for face parsing tasks. Our proposed Mask-FPAN framework demonstrates significant performance improvements, achieving a mean intersection over union (MIOU) of 0.9013, outperforming the state-of-the-art on challenging face datasets.\n}"}, "2206.08347": {"original_text": "  By leveraging contrastive learning, clustering, and other pretext tasks,\nunsupervised methods for learning image representations have reached impressive\nresults on standard benchmarks. The result has been a crowded field - many\nmethods with substantially different implementations yield results that seem\nnearly identical on popular benchmarks, such as linear evaluation on ImageNet.\nHowever, a single result does not tell the whole story. In this paper, we\ncompare methods using performance-based benchmarks such as linear evaluation,\nnearest neighbor classification, and clustering for several different datasets,\ndemonstrating the lack of a clear front-runner within the current\nstate-of-the-art. In contrast to prior work that performs only supervised vs.\nunsupervised comparison, we compare several different unsupervised methods\nagainst each other. To enrich this comparison, we analyze embeddings with\nmeasurements such as uniformity, tolerance, and centered kernel alignment\n(CKA), and propose two new metrics of our own: nearest neighbor graph\nsimilarity and linear prediction overlap. We reveal through our analysis that\nin isolation, single popular methods should not be treated as though they\nrepresent the field as a whole, and that future work ought to consider how to\nleverage the complimentary nature of these methods. We also leverage CKA to\nprovide a framework to robustly quantify augmentation invariance, and provide a\nreminder that certain types of invariance will be undesirable for downstream\ntasks.\n", "rewritten_text": "\\boxed{\nRecent advances in unsupervised image representation learning have achieved impressive results on standard benchmarks, making it a crowded field with many methods yielding similar results. However, a single benchmark result does not provide a complete picture. This paper conducts a comprehensive comparison of various unsupervised methods across multiple datasets and evaluation metrics, including linear evaluation, nearest neighbor classification, and clustering. The analysis reveals that no single method stands out as the best, and that each method has its strengths and weaknesses. Additionally, new metrics are proposed to evaluate the quality of image representations, including nearest neighbor graph similarity and linear prediction overlap. The study highlights the importance of considering the complementary nature of different methods and the need to quantify augmentation invariance to ensure desirable outcomes in downstream tasks.\n}"}, "2108.08432": {"original_text": "  Deep learning has achieved remarkable success in medicalimage segmentation,\nbut it usually requires a large numberof images labeled with fine-grained\nsegmentation masks, andthe annotation of these masks can be very expensive\nandtime-consuming. Therefore, recent methods try to use un-supervised domain\nadaptation (UDA) methods to borrow in-formation from labeled data from other\ndatasets (source do-mains) to a new dataset (target domain). However, due tothe\nabsence of labels in the target domain, the performance ofUDA methods is much\nworse than that of the fully supervisedmethod. In this paper, we propose a\nweakly supervised do-main adaptation setting, in which we can partially label\nnewdatasets with bounding boxes, which are easier and cheaperto obtain than\nsegmentation masks. Accordingly, we proposea new weakly-supervised domain\nadaptation method calledBox-Adapt, which fully explores the fine-grained\nsegmenta-tion mask in the source domain and the weak bounding boxin the target\ndomain. Our Box-Adapt is a two-stage methodthat first performs joint training\non the source and target do-mains, and then conducts self-training with the\npseudo-labelsof the target domain. We demonstrate the effectiveness of\nourmethod in the liver segmentation task. Weakly supervised do-main adaptation\n", "rewritten_text": "\\boxed{Deep learning has excelled in medical image segmentation, but it typically requires a large dataset of finely labeled images, which can be costly and time-consuming to annotate. To address this, recent methods have employed unsupervised domain adaptation (UDA) to leverage labeled data from other datasets. However, UDA's performance is limited by the lack of target domain labels. This paper proposes a weakly supervised domain adaptation approach, where new datasets are partially labeled with bounding boxes, which are easier and cheaper to obtain than segmentation masks. The proposed Box-Adapt method fully utilizes the fine-grained segmentation masks in the source domain and weak bounding boxes in the target domain through a two-stage process involving joint training and self-training with pseudo-labels. The effectiveness of Box-Adapt is demonstrated in liver segmentation tasks.}"}, "2203.02231": {"original_text": "  Light field disparity estimation is an essential task in computer vision with\nvarious applications. Although supervised learning-based methods have achieved\nboth higher accuracy and efficiency than traditional optimization-based\nmethods, the dependency on ground-truth disparity for training limits the\noverall generalization performance not to say for real-world scenarios where\nthe ground-truth disparity is hard to capture. In this paper, we argue that\nunsupervised methods can achieve comparable accuracy, but, more importantly,\nmuch higher generalization capacity and efficiency than supervised methods.\nSpecifically, we present the Occlusion Pattern Aware Loss, named OPAL, which\nsuccessfully extracts and encodes the general occlusion patterns inherent in\nthe light field for loss calculation. OPAL enables: i) accurate and robust\nestimation by effectively handling occlusions without using any ground-truth\ninformation for training and ii) much efficient performance by significantly\nreducing the network parameters required for accurate inference. Besides, a\ntransformer-based network and a refinement module are proposed for achieving\neven more accurate results. Extensive experiments demonstrate our method not\nonly significantly improves the accuracy compared with the SOTA unsupervised\nmethods, but also possesses strong generalization capacity, even for real-world\ndata, compared with supervised methods. Our code will be made publicly\navailable.\n", "rewritten_text": "\\boxed{\nEstimating light field disparities is a crucial task in computer vision with numerous applications. While supervised learning methods have achieved higher accuracy and efficiency, they rely on ground-truth disparity data for training, which limits their generalization performance, especially in real-world scenarios where ground-truth data is difficult to obtain. In contrast, unsupervised methods can achieve comparable accuracy and offer higher generalization capacity and efficiency. This paper proposes the Occlusion Pattern Aware Loss (OPAL) method, which extracts and encodes general occlusion patterns in light fields for loss calculation, enabling accurate and robust estimation without ground-truth information. OPAL also reduces network parameters, leading to more efficient performance. Additionally, a transformer-based network and refinement module are introduced to further improve accuracy. Experimental results show that our method outperforms state-of-the-art unsupervised methods in terms of accuracy and demonstrates strong generalization capacity, even with real-world data, compared to supervised methods. The code will be made publicly available.\n}"}, "2006.1038": {"original_text": "  Video semantic segmentation is active in recent years benefited from the\ngreat progress of image semantic segmentation. For such a task, the per-frame\nimage segmentation is generally unacceptable in practice due to high\ncomputation cost. To tackle this issue, many works use the flow-based feature\npropagation to reuse the features of previous frames. However, the optical flow\nestimation inevitably suffers inaccuracy and then causes the propagated\nfeatures distorted. In this paper, we propose distortion-aware feature\ncorrection to alleviate the issue, which improves video segmentation\nperformance by correcting distorted propagated features. To be specific, we\nfirstly propose to transfer distortion patterns from feature into image space\nand conduct effective distortion map prediction. Benefited from the guidance of\ndistortion maps, we proposed Feature Correction Module (FCM) to rectify\npropagated features in the distorted areas. Our proposed method can\nsignificantly boost the accuracy of video semantic segmentation at a low price.\nThe extensive experimental results on Cityscapes and CamVid show that our\nmethod outperforms the recent state-of-the-art methods.\n", "rewritten_text": "\\boxed{\nVideo semantic segmentation has made significant progress in recent years, building on the advancements in image semantic segmentation. However, performing per-frame image segmentation is often impractical due to the high computational cost. To address this, many approaches utilize flow-based feature propagation to reuse features from previous frames. Nevertheless, optical flow estimation can be inaccurate, leading to distorted propagated features. This paper proposes a distortion-aware feature correction method to mitigate this issue, enhancing video segmentation performance by correcting distorted features. Specifically, we introduce a technique to transfer distortion patterns from feature space to image space and predict effective distortion maps. Guided by these maps, our Feature Correction Module (FCM) rectifies propagated features in distorted areas. Our method significantly improves video semantic segmentation accuracy at a low computational cost. Extensive experiments on Cityscapes and CamVid datasets demonstrate that our approach outperforms recent state-of-the-art methods.\n}"}, "2101.11251": {"original_text": "  Junctions reflect the important geometrical structure information of the\nimage, and are of primary significance to applications such as image matching\nand motion analysis. Previous event-based feature extraction methods are mainly\nfocused on corners, which mainly find their locations, however, ignoring the\ngeometrical structure information like orientations and scales of edges. This\npaper adapts the frame-based a-contrario junction detector(ACJ) to event data,\nproposing the event-based a-contrario junction detector(e-ACJ), which yields\njunctions' locations while giving the scales and orientations of their\nbranches. The proposed method relies on an a-contrario model and can operate on\nasynchronous events directly without generating synthesized event frames. We\nevaluate the performance on public event datasets. The result shows our method\nsuccessfully finds the orientations and scales of branches, while maintaining\nhigh accuracy in junction's location.\n", "rewritten_text": "\\boxed{Junctions, which capture essential geometric structure information in images, are crucial for applications like image matching and motion analysis. While previous event-based feature extraction methods primarily focus on detecting corner locations, they neglect important details like edge orientations and scales. This paper presents an event-based adaptation of the frame-based a-contrario junction detector (ACJ), called e-ACJ, which not only identifies junction locations but also provides the scales and orientations of their branches. The proposed method leverages an a-contrario model to process asynchronous events directly, without requiring synthesized event frames. Evaluation on public event datasets demonstrates the method's success in accurately detecting branch orientations and scales while maintaining high location accuracy.}"}, "2210.16239": {"original_text": "  The Hyperspectral image (HSI) contains several hundred bands of the same\nregion called the Ground Truth (GT). The bands are taken in juxtaposed\nfrequencies, but some of them are noisily measured or contain no information.\nFor the classification, the selection of bands, affects significantly the\nresults of classification, in fact, using a subset of relevant bands, these\nresults can be better than those obtained using all bands, from which the need\nto reduce the dimensionality of the HSI. In this paper, a categorization of\ndimensionality reduction methods, according to the generation process, is\npresented. Furthermore, we reproduce an algorithm based on mutual information\n(MI) to reduce dimensionality by features selection and we introduce an\nalgorithm using mutual information and homogeneity. The two schemas are a\nfilter strategy. Finally, to validate this, we consider the case study AVIRIS\nHSI 92AV3C.\n  Keywords: Hyperspectrale images; classification; features selection; mutual\ninformation; homogeneity\n", "rewritten_text": "\\boxed{\nHyperspectral images (HSIs) consist of hundreds of bands of the same region, known as the Ground Truth (GT), captured at adjacent frequencies. However, some bands may be noisy or lack information, making band selection crucial for accurate classification. In fact, using a subset of relevant bands can lead to better results than using all bands, highlighting the need for dimensionality reduction. This paper categorizes dimensionality reduction methods based on their generation process and presents two filter strategies: one based on mutual information (MI) for feature selection and another that combines MI with homogeneity. The effectiveness of these approaches is demonstrated using the AVIRIS HSI 92AV3C case study.\nKeywords: Hyperspectral images; classification; feature selection; mutual information; homogeneity\n}"}, "2108.07127": {"original_text": "  We translate a closed text that is known in advance and available in many\nlanguages into a new and severely low resource language. Most human translation\nefforts adopt a portion-based approach to translate consecutive pages/chapters\nin order, which may not suit machine translation. We compare the portion-based\napproach that optimizes coherence of the text locally with the random sampling\napproach that increases coverage of the text globally. Our results show that\nthe random sampling approach performs better. When training on a seed corpus of\n~1,000 lines from the Bible and testing on the rest of the Bible (~30,000\nlines), random sampling gives a performance gain of +11.0 BLEU using English as\na simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a\nMayan language. Furthermore, we compare three ways of updating machine\ntranslation models with increasing amount of human post-edited data through\niterations. We find that adding newly post-edited data to training after\nvocabulary update without self-supervision performs the best. We propose an\nalgorithm for human and machine to work together seamlessly to translate a\nclosed text into a severely low resource language.\n", "rewritten_text": "\\boxed{We explore the translation of a well-known text into a new, severely low-resource language. Unlike traditional human translation methods that focus on consecutive sections, we compare two machine translation approaches: a portion-based method that prioritizes local coherence and a random sampling method that aims for global coverage. Our results show that random sampling outperforms the portion-based approach, with significant improvements in BLEU scores (+11.0 using English and +4.9 using Eastern Pokomchi). We also investigate the best way to update machine translation models with increasing amounts of human-post-edited data, finding that adding new data after vocabulary updates without self-supervision yields the best results. Our proposed algorithm enables seamless collaboration between humans and machines to translate closed texts into severely low-resource languages.}"}, "2103.10978": {"original_text": "  This paper addresses the problem of 3D human body shape and pose estimation\nfrom RGB images. Recent progress in this field has focused on single images,\nvideo or multi-view images as inputs. In contrast, we propose a new task: shape\nand pose estimation from a group of multiple images of a human subject, without\nconstraints on subject pose, camera viewpoint or background conditions between\nimages in the group. Our solution to this task predicts distributions over SMPL\nbody shape and pose parameters conditioned on the input images in the group. We\nprobabilistically combine predicted body shape distributions from each image to\nobtain a final multi-image shape prediction. We show that the additional body\nshape information present in multi-image input groups improves 3D human shape\nestimation metrics compared to single-image inputs on the SSP-3D dataset and a\nprivate dataset of tape-measured humans. In addition, predicting distributions\nover 3D bodies allows us to quantify pose prediction uncertainty, which is\nuseful when faced with challenging input images with significant occlusion. Our\nmethod demonstrates meaningful pose uncertainty on the 3DPW dataset and is\ncompetitive with the state-of-the-art in terms of pose estimation metrics.\n", "rewritten_text": "\\boxed{This paper tackles the challenge of estimating 3D human body shape and pose from a collection of RGB images, without any restrictions on the subject's pose, camera angle, or background across images. Our approach predicts the distribution of possible body shapes and poses based on the input images, and then combines these predictions to produce a final, more accurate estimate. We demonstrate that using multiple images leads to better shape estimation results compared to using a single image, and also allows us to quantify the uncertainty of pose predictions, which is particularly useful when dealing with images that have significant occlusion. Our method shows promising results on multiple datasets, including SSP-3D, a private dataset, and 3DPW.}"}, "2307.01447": {"original_text": "  Accurately matching local features between a pair of images is a challenging\ncomputer vision task. Previous studies typically use attention based graph\nneural networks (GNNs) with fully-connected graphs over keypoints within/across\nimages for visual and geometric information reasoning. However, in the context\nof feature matching, considerable keypoints are non-repeatable due to occlusion\nand failure of the detector, and thus irrelevant for message passing. The\nconnectivity with non-repeatable keypoints not only introduces redundancy,\nresulting in limited efficiency, but also interferes with the representation\naggregation process, leading to limited accuracy. Targeting towards high\naccuracy and efficiency, we propose MaKeGNN, a sparse attention-based GNN\narchitecture which bypasses non-repeatable keypoints and leverages matchable\nones to guide compact and meaningful message passing. More specifically, our\nBilateral Context-Aware Sampling Module first dynamically samples two small\nsets of well-distributed keypoints with high matchability scores from the image\npair. Then, our Matchable Keypoint-Assisted Context Aggregation Module regards\nsampled informative keypoints as message bottlenecks and thus constrains each\nkeypoint only to retrieve favorable contextual information from intra- and\ninter- matchable keypoints, evading the interference of irrelevant and\nredundant connectivity with non-repeatable ones. Furthermore, considering the\npotential noise in initial keypoints and sampled matchable ones, the MKACA\nmodule adopts a matchability-guided attentional aggregation operation for purer\ndata-dependent context propagation. By these means, we achieve the\nstate-of-the-art performance on relative camera estimation, fundamental matrix\nestimation, and visual localization, while significantly reducing computational\nand memory complexity compared to typical attentional GNNs.\n", "rewritten_text": "\\boxed{\nMatching local features between two images is a difficult task in computer vision. Previous methods used attention-based graph neural networks (GNNs) with fully-connected graphs to reason about visual and geometric information. However, many keypoints are not repeatable due to occlusion and detector failure, making them irrelevant for message passing. These non-repeatable keypoints introduce redundancy, reduce efficiency, and interfere with representation aggregation, leading to limited accuracy. To achieve high accuracy and efficiency, we propose MaKeGNN, a sparse attention-based GNN architecture that bypasses non-repeatable keypoints and leverages matchable ones for compact and meaningful message passing. Our approach dynamically samples well-distributed keypoints with high matchability scores, regards them as message bottlenecks, and constrains each keypoint to retrieve favorable contextual information from intra- and inter-matchable keypoints. Additionally, our method adopts a matchability-guided attentional aggregation operation to reduce noise and achieve purer data-dependent context propagation. As a result, we achieve state-of-the-art performance on relative camera estimation, fundamental matrix estimation, and visual localization while significantly reducing computational and memory complexity compared to typical attentional GNNs.\n}"}, "1703.00087": {"original_text": "  Lesion segmentation is the first step in most automatic melanoma recognition\nsystems. Deficiencies and difficulties in dermoscopic images such as color\ninconstancy, hair occlusion, dark corners and color charts make lesion\nsegmentation an intricate task. In order to detect the lesion in the presence\nof these problems, we propose a supervised saliency detection method tailored\nfor dermoscopic images based on the discriminative regional feature integration\n(DRFI). DRFI method incorporates multi-level segmentation, regional contrast,\nproperty, background descriptors, and a random forest regressor to create\nsaliency scores for each region in the image. In our improved saliency\ndetection method, mDRFI, we have added some new features to regional property\ndescriptors. Also, in order to achieve more robust regional background\ndescriptors, a thresholding algorithm is proposed to obtain a new\npseudo-background region. Findings reveal that mDRFI is superior to DRFI in\ndetecting the lesion as the salient object in dermoscopic images. The proposed\noverall lesion segmentation framework uses detected saliency map to construct\nan initial mask of the lesion through thresholding and post-processing\noperations. The initial mask is then evolving in a level set framework to fit\nbetter on the lesion's boundaries. The results of evaluation tests on three\npublic datasets show that our proposed segmentation method outperforms the\nother conventional state-of-the-art segmentation algorithms and its performance\nis comparable with most recent approaches that are based on deep convolutional\nneural networks.\n", "rewritten_text": "\\boxed{\nLesion segmentation is a crucial initial step in automated melanoma recognition systems, but it's a challenging task due to issues like color inconsistencies, hair occlusion, and dark corners in dermoscopic images. To overcome these difficulties, we propose a supervised saliency detection method, mDRFI, which builds upon the discriminative regional feature integration (DRFI) approach. mDRFI incorporates additional features to regional property descriptors and a thresholding algorithm to obtain a more robust pseudo-background region. Our results show that mDRFI outperforms DRFI in detecting lesions as salient objects in dermoscopic images. The proposed lesion segmentation framework uses the detected saliency map to create an initial lesion mask, which is then refined using a level set framework to better fit the lesion's boundaries. Evaluation tests on three public datasets demonstrate that our proposed segmentation method outperforms conventional state-of-the-art algorithms and is comparable to recent deep convolutional neural network-based approaches."}, "2405.12209": {"original_text": "  Recent advancements in large language models (LLMs) have showcased\nsignificant improvements in mathematics. However, traditional math benchmarks\nlike GSM8k offer a unidimensional perspective, falling short in providing a\nholistic assessment of the LLMs' math capabilities. To address this gap, we\nintroduce MathBench, a new benchmark that rigorously assesses the mathematical\ncapabilities of large language models. MathBench spans a wide range of\nmathematical disciplines, offering a detailed evaluation of both theoretical\nunderstanding and practical problem-solving skills. The benchmark progresses\nthrough five distinct stages, from basic arithmetic to college mathematics, and\nis structured to evaluate models at various depths of knowledge. Each stage\nincludes theoretical questions and application problems, allowing us to measure\na model's mathematical proficiency and its ability to apply concepts in\npractical scenarios. MathBench aims to enhance the evaluation of LLMs'\nmathematical abilities, providing a nuanced view of their knowledge\nunderstanding levels and problem solving skills in a bilingual context. The\nproject is released at https://github.com/open-compass/MathBench .\n", "rewritten_text": "\\boxed{\nRecent breakthroughs in large language models (LLMs) have demonstrated substantial progress in mathematics. Nevertheless, traditional math benchmarks, such as GSM8k, provide a limited perspective, failing to offer a comprehensive evaluation of LLMs' mathematical capabilities. To bridge this gap, we propose MathBench, a novel benchmark that thoroughly assesses the mathematical abilities of large language models. MathBench encompasses a broad range of mathematical disciplines, providing a detailed evaluation of both theoretical understanding and practical problem-solving skills. The benchmark consists of five distinct stages, progressing from basic arithmetic to college-level mathematics, and is designed to evaluate models at varying levels of knowledge. Each stage includes theoretical questions and application problems, enabling the measurement of a model's mathematical proficiency and its ability to apply concepts in real-world scenarios. MathBench aims to enhance the evaluation of LLMs' mathematical abilities, providing a nuanced understanding of their knowledge levels and problem-solving skills in a bilingual context. The project is available at https://github.com/open-compass/MathBench.\n}"}, "2407.03993": {"original_text": "  Natural language counterfactual generation aims to minimally modify a given\ntext such that the modified text will be classified into a different class. The\ngenerated counterfactuals provide insight into the reasoning behind a model's\npredictions by highlighting which words significantly influence the outcomes.\nAdditionally, they can be used to detect model fairness issues and augment the\ntraining data to enhance the model's robustness. A substantial amount of\nresearch has been conducted to generate counterfactuals for various NLP tasks,\nemploying different models and methodologies. With the rapid growth of studies\nin this field, a systematic review is crucial to guide future researchers and\ndevelopers. To bridge this gap, this survey provides a comprehensive overview\nof textual counterfactual generation methods, particularly those based on Large\nLanguage Models. We propose a new taxonomy that systematically categorizes the\ngeneration methods into four groups and summarizes the metrics for evaluating\nthe generation quality. Finally, we discuss ongoing research challenges and\noutline promising directions for future work.\n", "rewritten_text": "\\boxed{\nCounterfactual text generation involves making minimal changes to a given text to alter its classification outcome, providing insights into a model's decision-making process and identifying influential words. This technique can also detect bias in models and enhance their robustness by augmenting training data. Despite extensive research in this area, a comprehensive review is necessary to guide future research and development. This survey aims to fill this gap by providing an in-depth overview of textual counterfactual generation methods, particularly those leveraging Large Language Models. A new taxonomy is proposed to categorize these methods into four groups, and evaluation metrics are summarized. Finally, ongoing challenges and promising future research directions are discussed."}, "1909.13411": {"original_text": "  Mesoscale eddies play a significant role in marine energy transport, marine\nbiological environment and marine climate. Due to their huge impact on the\nocean, mesoscale eddy detection has become a hot research area in recent years.\nTherefore, more and more people are entering the field of mesoscale eddy\ndetection. However, the existing detection methods mainly based on traditional\ndetection methods typically only use Sea Surface Height (SSH) as a variable to\ndetect, resulting in inaccurate performance. In this paper, we propose a\nmesoscale eddy detection method based on multivariate fusion data to solve this\nproblem. We not only use the SSH variable, but also add the two variables: Sea\nSurface Temperature (SST) and velocity of flow, achieving a multivariate\ninformation fusion input. We design a novel symmetric network, which merges\nlow-level feature maps from the downsampling pathway and high-level feature\nmaps from the upsampling pathway by lateral connection. In addition, we apply\ndilated convolutions to network structure to increase the receptive field and\nobtain more contextual information in the case of constant parameter. In the\nend, we demonstrate the effectiveness of our method on dataset provided by us,\nachieving the test set performance of 97.06% , greatly improved the performance\nof previous methods of mesoscale eddy detection.\n", "rewritten_text": "\\boxed{\nMesoscale eddies have a significant impact on marine energy transport, biological environments, and climate. As a result, detecting these eddies has become a popular research area. However, traditional methods that rely solely on Sea Surface Height (SSH) data often yield inaccurate results. To address this issue, we propose a novel method that combines SSH data with Sea Surface Temperature (SST) and velocity of flow data to create a multivariate fusion input. Our approach utilizes a symmetric network that merges low-level and high-level feature maps, along with dilated convolutions to increase the receptive field and capture more contextual information. Our method achieves a test set performance of 97.06%, significantly outperforming previous methods for mesoscale eddy detection.\n}"}, "1808.07733": {"original_text": "  We analyze the performance of different sentiment classification models on\nsyntactically complex inputs like A-but-B sentences. The first contribution of\nthis analysis addresses reproducible research: to meaningfully compare\ndifferent models, their accuracies must be averaged over far more random seeds\nthan what has traditionally been reported. With proper averaging in place, we\nnotice that the distillation model described in arXiv:1603.06318v4 [cs.LG],\nwhich incorporates explicit logic rules for sentiment classification, is\nineffective. In contrast, using contextualized ELMo embeddings\n(arXiv:1802.05365v2 [cs.CL]) instead of logic rules yields significantly better\nperformance. Additionally, we provide analysis and visualizations that\ndemonstrate ELMo's ability to implicitly learn logic rules. Finally, a\ncrowdsourced analysis reveals how ELMo outperforms baseline models even on\nsentences with ambiguous sentiment labels.\n", "rewritten_text": "\\boxed{This study examines the effectiveness of various sentiment classification models on complex sentences, such as A-but-B sentences. To ensure reliable comparisons, we averaged model accuracies over a large number of random seeds, revealing that a distillation model incorporating explicit logic rules performs poorly. In contrast, using contextualized ELMo embeddings yields significantly better results. Furthermore, our analysis and visualizations show that ELMo can implicitly learn logic rules, and a crowdsourced evaluation demonstrates its superiority over baseline models, even on sentences with ambiguous sentiment labels.}"}, "2011.09634": {"original_text": "  In this paper, we teach machines to understand visuals and natural language\nby learning the mapping between sentences and noisy video snippets without\nexplicit annotations. Firstly, we define a self-supervised learning framework\nthat captures the cross-modal information. A novel adversarial learning module\nis then introduced to explicitly handle the noises in the natural videos, where\nthe subtitle sentences are not guaranteed to be strongly corresponded to the\nvideo snippets. For training and evaluation, we contribute a new dataset\n`ApartmenTour' that contains a large number of online videos and subtitles. We\ncarry out experiments on the bidirectional retrieval tasks between sentences\nand videos, and the results demonstrate that our proposed model achieves the\nstate-of-the-art performance on both retrieval tasks and exceeds several strong\nbaselines. The dataset can be downloaded at https://github.com/zyj-13/WAL.\n", "rewritten_text": "\\boxed{This paper presents a self-supervised learning framework that enables machines to understand visuals and natural language by learning the correlation between sentences and noisy video snippets without explicit annotations. A novel adversarial learning module is introduced to handle noisy videos where subtitles may not strongly correspond to video snippets. A new dataset, `ApartmenTour', is contributed, containing online videos and subtitles. Experimental results on bidirectional retrieval tasks demonstrate the proposed model's state-of-the-art performance, outperforming strong baselines. The dataset is available at https://github.com/zyj-13/WAL.}"}, "1708.01654": {"original_text": "  We demonstrate the use of shape-from-shading (SfS) to improve both the\nquality and the robustness of 3D reconstruction of dynamic objects captured by\na single camera. Unlike previous approaches that made use of SfS as a\npost-processing step, we offer a principled integrated approach that solves\ndynamic object tracking and reconstruction and SfS as a single unified cost\nfunction. Moving beyond Lambertian S f S , we propose a general approach that\nmodels both specularities and shading while simultaneously tracking and\nreconstructing general dynamic objects. Solving these problems jointly prevents\nthe kinds of tracking failures which can not be recovered from by pipeline\napproaches. We show state-of-the-art results both qualitatively and\nquantitatively.\n", "rewritten_text": "\\boxed{We present a novel approach that combines shape-from-shading (SfS) with dynamic object tracking and reconstruction, solving both problems simultaneously as a single cost function. This integrated method improves the quality and robustness of 3D reconstruction from single-camera captures, outperforming previous pipeline approaches that used SfS as a post-processing step. Our approach can handle general dynamic objects, modeling both specularities and shading, and achieves state-of-the-art results in both qualitative and quantitative evaluations.}"}, "1711.07721": {"original_text": "  Post capture refocusing effect in smartphone cameras is achievable by using\nfocal stacks. However, the accuracy of this effect is totally dependent on the\ncombination of the depth layers in the stack. The accuracy of the extended\ndepth of field effect in this application can be improved significantly by\ncomputing an accurate depth map which has been an open issue for decades. To\ntackle this issue, in this paper, a framework is proposed based on\nPreconditioned Alternating Direction Method of Multipliers (PADMM) for depth\nfrom the focal stack and synthetic defocus application. In addition to its\nability to provide high structural accuracy and occlusion handling, the\noptimization function of the proposed method can, in fact, converge faster and\nbetter than state of the art methods. The evaluation has been done on 21 sets\nof focal stacks and the optimization function has been compared against 5 other\nmethods. Preliminary results indicate that the proposed method has a better\nperformance in terms of structural accuracy and optimization in comparison to\nthe current state of the art methods.\n", "rewritten_text": "\\boxed{\nRefocusing effects in smartphone cameras can be achieved using focal stacks, but the accuracy of this effect relies heavily on the combination of depth layers in the stack. To improve the accuracy of extended depth of field effects, a precise depth map is essential, which has been a long-standing challenge. This paper proposes a framework based on Preconditioned Alternating Direction Method of Multipliers (PADMM) to compute depth from focal stacks and synthetic defocus applications. The proposed method excels in providing high structural accuracy, handling occlusions, and converging faster and better than existing state-of-the-art methods. Evaluation on 21 focal stack sets and comparison with 5 other methods demonstrate the superior performance of the proposed method in terms of structural accuracy and optimization.\n}"}, "2004.06376": {"original_text": "  Understanding the shape of a scene from a single color image is a formidable\ncomputer vision task. However, most methods aim to predict the geometry of\nsurfaces that are visible to the camera, which is of limited use when planning\npaths for robots or augmented reality agents. Such agents can only move when\ngrounded on a traversable surface, which we define as the set of classes which\nhumans can also walk over, such as grass, footpaths and pavement. Models which\npredict beyond the line of sight often parameterize the scene with voxels or\nmeshes, which can be expensive to use in machine learning frameworks.\n  We introduce a model to predict the geometry of both visible and occluded\ntraversable surfaces, given a single RGB image as input. We learn from stereo\nvideo sequences, using camera poses, per-frame depth and semantic segmentation\nto form training data, which is used to supervise an image-to-image network. We\ntrain models from the KITTI driving dataset, the indoor Matterport dataset, and\nfrom our own casually captured stereo footage. We find that a surprisingly low\nbar for spatial coverage of training scenes is required. We validate our\nalgorithm against a range of strong baselines, and include an assessment of our\npredictions for a path-planning task.\n", "rewritten_text": "\\boxed{\nPredicting the shape of a scene from a single color image is a challenging task in computer vision. While most methods focus on predicting the geometry of visible surfaces, this is limited for applications like robot path planning and augmented reality, which require identifying traversable surfaces that humans can walk on, such as grass, footpaths, and pavement. Existing models that predict beyond the line of sight often use voxels or meshes, which can be computationally expensive. We propose a model that predicts the geometry of both visible and occluded traversable surfaces from a single RGB image. Our model is trained on stereo video sequences with camera poses, depth, and semantic segmentation, and we evaluate its performance on the KITTI driving dataset, the Matterport dataset, and our own stereo footage. We find that a relatively low spatial coverage of training scenes is sufficient, and our algorithm outperforms strong baselines in a path-planning task."}, "2107.08173": {"original_text": "  This ability to learn consecutive tasks without forgetting how to perform\npreviously trained problems is essential for developing an online dialogue\nsystem. This paper proposes an effective continual learning for the\ntask-oriented dialogue system with iterative network pruning, expanding and\nmasking (TPEM), which preserves performance on previously encountered tasks\nwhile accelerating learning progress on subsequent tasks. Specifically, TPEM\n(i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts\nnetwork expanding to create free weights for new tasks, and (iii) introduces\ntask-specific network masking to alleviate the negative impact of fixed weights\nof old tasks on new tasks. We conduct extensive experiments on seven different\ntasks from three benchmark datasets and show empirically that TPEM leads to\nsignificantly improved results over the strong competitors. For\nreproducibility, we submit the code and data at:\nhttps://github.com/siat-nlp/TPEM\n", "rewritten_text": "\\boxed{This paper presents a novel approach to continual learning for task-oriented dialogue systems, called TPEM, which enables the system to learn consecutive tasks without forgetting previous ones. TPEM achieves this by combining network pruning to retain knowledge from old tasks, network expanding to create new weights for new tasks, and task-specific masking to mitigate the negative impact of old tasks on new ones. Experimental results on seven tasks from three benchmark datasets demonstrate that TPEM significantly outperforms strong competitors. The code and data are available at https://github.com/siat-nlp/TPEM for reproducibility.}"}, "1811.09789": {"original_text": "  There has been much recent work on image captioning models that describe the\nfactual aspects of an image. Recently, some models have incorporated\nnon-factual aspects into the captions, such as sentiment or style. However,\nsuch models typically have difficulty in balancing the semantic aspects of the\nimage and the non-factual dimensions of the caption; in addition, it can be\nobserved that humans may focus on different aspects of an image depending on\nthe chosen sentiment or style of the caption. To address this, we design an\nattention-based model to better add sentiment to image captions. The model\nembeds and learns sentiment with respect to image-caption data, and uses both\nhigh-level and word-level sentiment information during the learning process.\nThe model outperforms the state-of-the-art work in image captioning with\nsentiment using standard evaluation metrics. An analysis of generated captions\nalso shows that our model does this by a better selection of the\nsentiment-bearing adjectives and adjective-noun pairs.\n", "rewritten_text": "\\boxed{Recent image captioning models have focused on describing factual aspects of images, but some have attempted to incorporate non-factual elements like sentiment or style. However, these models often struggle to balance semantic accuracy with emotional tone. To overcome this, we propose an attention-based model that learns to integrate sentiment into image captions by leveraging both high-level and word-level sentiment information. Our model outperforms existing state-of-the-art approaches in sentiment-based image captioning, and analysis reveals that it achieves this through more effective selection of sentiment-bearing adjectives and adjective-noun pairs.}"}, "2310.15724": {"original_text": "  Pre-trained language models (PLMs) have achieved remarkable results on NLP\ntasks but at the expense of huge parameter sizes and the consequent\ncomputational costs. In this paper, we propose Variator, a parameter-efficient\nacceleration method that enhances computational efficiency through\nplug-and-play compression plugins. Compression plugins are designed to reduce\nthe sequence length via compressing multiple hidden vectors into one and\ntrained with original PLMs frozen. Different from traditional model\nacceleration methods, which compress PLMs to smaller sizes, Variator offers two\ndistinct advantages: (1) In real-world applications, the plug-and-play nature\nof our compression plugins enables dynamic selection of different compression\nplugins with varying acceleration ratios based on the current workload. (2) The\ncompression plugin comprises a few compact neural network layers with minimal\nparameters, significantly saving storage and memory overhead, particularly in\nscenarios with a growing number of tasks. We validate the effectiveness of\nVariator on seven datasets. Experimental results show that Variator can save\n53% computational costs using only 0.9% additional parameters with a\nperformance drop of less than 2%. Moreover, when the model scales to billions\nof parameters, Variator matches the strong performance of uncompressed PLMs.\n", "rewritten_text": "\\boxed{Pre-trained language models have achieved impressive results in NLP tasks, but at the cost of large parameter sizes and high computational costs. This paper introduces Variator, a method that improves computational efficiency by using plug-and-play compression plugins. These plugins reduce sequence length by compressing multiple hidden vectors into one, and are trained with the original model frozen. Unlike traditional acceleration methods that compress entire models, Variator offers two key advantages: it allows for dynamic selection of compression plugins based on workload, and it requires minimal storage and memory due to its compact neural network layers. Experiments on seven datasets show that Variator can reduce computational costs by 53% with only a 0.9% increase in parameters and a performance drop of less than 2%. Moreover, Variator matches the performance of uncompressed models even when they have billions of parameters.}"}, "2110.02929": {"original_text": "  Event-based dynamic vision sensors provide very sparse output in the form of\nspikes, which makes them suitable for low-power applications. Convolutional\nspiking neural networks model such event-based data and develop their full\nenergy-saving potential when deployed on asynchronous neuromorphic hardware.\nEvent-based vision being a nascent field, the sensitivity of spiking neural\nnetworks to potentially malicious adversarial attacks has received little\nattention so far. We show how white-box adversarial attack algorithms can be\nadapted to the discrete and sparse nature of event-based visual data, and\ndemonstrate smaller perturbation magnitudes at higher success rates than the\ncurrent state-of-the-art algorithms. For the first time, we also verify the\neffectiveness of these perturbations directly on neuromorphic hardware.\nFinally, we discuss the properties of the resulting perturbations, the effect\nof adversarial training as a defense strategy, and future directions.\n", "rewritten_text": "\\boxed{\nEvent-based dynamic vision sensors offer a low-power solution by producing sparse output in the form of spikes. Convolutional spiking neural networks can effectively process this data when deployed on asynchronous neuromorphic hardware, unlocking their full energy-saving potential. However, the vulnerability of these networks to adversarial attacks has been largely overlooked. This study adapts white-box adversarial attack algorithms to accommodate the discrete and sparse nature of event-based visual data, achieving higher success rates with smaller perturbation magnitudes than current state-of-the-art algorithms. Furthermore, the effectiveness of these perturbations is verified on neuromorphic hardware for the first time. The properties of the resulting perturbations, the impact of adversarial training as a defense strategy, and future research directions are also explored.\n}"}, "2405.20259": {"original_text": "  The proliferation of deep learning solutions and the scarcity of large\nannotated datasets pose significant challenges in real-world applications.\nVarious strategies have been explored to overcome this challenge, with data\naugmentation (DA) approaches emerging as prominent solutions. DA approaches\ninvolve generating additional examples by transforming existing labeled data,\nthereby enriching the dataset and helping deep learning models achieve improved\ngeneralization without succumbing to overfitting. In real applications, where\nsolutions based on deep learning are widely used, there is facial expression\nrecognition (FER), which plays an essential role in human communication,\nimproving a range of knowledge areas (e.g., medicine, security, and marketing).\nIn this paper, we propose a simple and comprehensive face data augmentation\napproach based on mixed face component regularization that outperforms the\nclassical DA approaches from the literature, including the MixAugment which is\na specific approach for the target task in two well-known FER datasets existing\nin the literature.\n", "rewritten_text": "\\boxed{The scarcity of large annotated datasets and the proliferation of deep learning solutions pose significant challenges in real-world applications. To overcome this, data augmentation approaches have been explored, which involve generating additional examples by transforming existing labeled data to improve deep learning model generalization and prevent overfitting. One such application is facial expression recognition, which is crucial in human communication and has implications for various fields. This paper proposes a novel face data augmentation approach based on mixed face component regularization, which outperforms existing classical approaches, including MixAugment, in two well-known facial expression recognition datasets.}"}, "2308.09983": {"original_text": "  Early detection of dysplasia of the cervix is critical for cervical cancer\ntreatment. However, automatic cervical dysplasia diagnosis via visual\ninspection, which is more appropriate in low-resource settings, remains a\nchallenging problem. Though promising results have been obtained by recent deep\nlearning models, their performance is significantly hindered by the limited\nscale of the available cervix datasets. Distinct from previous methods that\nlearn from a single dataset, we propose to leverage cross-domain cervical\nimages that were collected in different but related clinical studies to improve\nthe model's performance on the targeted cervix dataset. To robustly learn the\ntransferable information across datasets, we propose a novel prototype-based\nknowledge filtering method to estimate the transferability of cross-domain\nsamples. We further optimize the shared feature space by aligning the\ncross-domain image representations simultaneously on domain level with early\nalignment and class level with supervised contrastive learning, which endows\nmodel training and knowledge transfer with stronger robustness. The empirical\nresults on three real-world benchmark cervical image datasets show that our\nproposed method outperforms the state-of-the-art cervical dysplasia visual\ninspection by an absolute improvement of 4.7% in top-1 accuracy, 7.0% in\nprecision, 1.4% in recall, 4.6% in F1 score, and 0.05 in ROC-AUC.\n", "rewritten_text": "\\boxed{Detecting cervical dysplasia early is crucial for effective cervical cancer treatment. However, automating this process through visual inspection, particularly in low-resource settings, remains a significant challenge. While recent deep learning models have shown promise, their performance is limited by the small size of available cervix datasets. To address this, we propose a novel approach that leverages cervical images from different clinical studies to improve model performance on the target dataset. Our method uses a prototype-based knowledge filtering technique to identify transferable information across datasets and optimizes the shared feature space by aligning image representations across domains and classes. Our approach outperforms state-of-the-art methods in cervical dysplasia visual inspection, achieving absolute improvements of 4.7% in top-1 accuracy, 7.0% in precision, 1.4% in recall, 4.6% in F1 score, and 0.05 in ROC-AUC on three real-world benchmark datasets.}"}, "1704.02268": {"original_text": "  Unsupervised learning of visual similarities is of paramount importance to\ncomputer vision, particularly due to lacking training data for fine-grained\nsimilarities. Deep learning of similarities is often based on relationships\nbetween pairs or triplets of samples. Many of these relations are unreliable\nand mutually contradicting, implying inconsistencies when trained without\nsupervision information that relates different tuples or triplets to each\nother. To overcome this problem, we use local estimates of reliable\n(dis-)similarities to initially group samples into compact surrogate classes\nand use local partial orders of samples to classes to link classes to each\nother. Similarity learning is then formulated as a partial ordering task with\nsoft correspondences of all samples to classes. Adopting a strategy of\nself-supervision, a CNN is trained to optimally represent samples in a mutually\nconsistent manner while updating the classes. The similarity learning and\ngrouping procedure are integrated in a single model and optimized jointly. The\nproposed unsupervised approach shows competitive performance on detailed pose\nestimation and object classification.\n", "rewritten_text": "\\boxed{Unsupervised learning of visual similarities is crucial in computer vision, especially when fine-grained training data is limited. Traditional deep learning methods rely on pairwise or triplet relationships, but these can be unreliable and contradictory, leading to inconsistencies without supervision. To address this, we propose a novel approach that initially groups similar samples into compact classes using local estimates of reliable similarities and partial orders. We then formulate similarity learning as a partial ordering task with soft correspondences between samples and classes. By adopting self-supervision, a CNN is trained to represent samples consistently while updating the classes. Our integrated model, optimized jointly, demonstrates competitive performance in pose estimation and object classification.}"}, "1909.07598": {"original_text": "  Multi-hop question answering (QA) requires an information retrieval (IR)\nsystem that can find \\emph{multiple} supporting evidence needed to answer the\nquestion, making the retrieval process very challenging. This paper introduces\nan IR technique that uses information of entities present in the initially\nretrieved evidence to learn to `\\emph{hop}' to other relevant evidence. In a\nsetting, with more than \\textbf{5 million} Wikipedia paragraphs, our approach\nleads to significant boost in retrieval performance. The retrieved evidence\nalso increased the performance of an existing QA model (without any training)\non the \\hotpot benchmark by \\textbf{10.59} F1.\n", "rewritten_text": "\\boxed{Answering multi-hop questions requires a robust information retrieval system that can identify multiple pieces of evidence to support the answer. This paper proposes a novel technique that leverages entity information from initial evidence to \"hop\" to additional relevant evidence. Tested on a massive dataset of over 5 million Wikipedia paragraphs, our approach yields a significant improvement in retrieval performance, resulting in a 10.59 F1 score boost for an existing QA model on the HotPot benchmark without requiring additional training.}"}, "2211.06726": {"original_text": "  Vision Transformer (ViT) is a pioneering deep learning framework that can\naddress real-world computer vision issues, such as image classification and\nobject recognition. Importantly, ViTs are proven to outperform traditional deep\nlearning models, such as convolutional neural networks (CNNs). Relatively\nrecently, a number of ViT mutations have been transplanted into the field of\nmedical imaging, thereby resolving a variety of critical classification and\nsegmentation challenges, especially in terms of brain imaging data. In this\nwork, we provide a novel multimodal deep learning pipeline, MultiCrossViT,\nwhich is capable of analyzing both structural MRI (sMRI) and static functional\nnetwork connectivity (sFNC) data for the prediction of schizophrenia disease.\nOn a dataset with minimal training subjects, our novel model can achieve an AUC\nof 0.832. Finally, we visualize multiple brain regions and covariance patterns\nmost relevant to schizophrenia based on the resulting ViT attention maps by\nextracting features from transformer encoders.\n", "rewritten_text": "\\boxed{The Vision Transformer (ViT) framework has revolutionized computer vision tasks such as image classification and object recognition, outperforming traditional convolutional neural networks (CNNs). Recently, ViT variants have been applied to medical imaging, successfully tackling classification and segmentation challenges, particularly in brain imaging. This study introduces a novel multimodal deep learning pipeline, MultiCrossViT, which integrates structural MRI and static functional network connectivity data to predict schizophrenia. Despite limited training data, our model achieves an AUC of 0.832. Furthermore, we utilize ViT attention maps to identify and visualize key brain regions and covariance patterns associated with schizophrenia.}"}, "1607.02504": {"original_text": "  We present a method to predict image deformations based on patch-wise image\nappearance. Specifically, we design a patch-based deep encoder-decoder network\nwhich learns the pixel/voxel-wise mapping between image appearance and\nregistration parameters. Our approach can predict general deformation\nparameterizations, however, we focus on the large deformation diffeomorphic\nmetric mapping (LDDMM) registration model. By predicting the LDDMM\nmomentum-parameterization we retain the desirable theoretical properties of\nLDDMM, while reducing computation time by orders of magnitude: combined with\npatch pruning, we achieve a 1500x/66x speed up compared to GPU-based\noptimization for 2D/3D image registration. Our approach has better prediction\naccuracy than predicting deformation or velocity fields and results in\ndiffeomorphic transformations. Additionally, we create a Bayesian probabilistic\nversion of our network, which allows evaluation of deformation field\nuncertainty through Monte Carlo sampling using dropout at test time. We show\nthat deformation uncertainty highlights areas of ambiguous deformations. We\ntest our method on the OASIS brain image dataset in 2D and 3D.\n", "rewritten_text": "\\boxed{We propose a patch-based deep learning approach to predict image deformations by learning the relationship between image appearance and registration parameters. Our method can handle various deformation parameterizations, but we focus on the large deformation diffeomorphic metric mapping (LDDMM) model. By predicting the LDDMM momentum-parameterization, we maintain the desirable theoretical properties of LDDMM while significantly reducing computation time (by 1500x/66x for 2D/3D image registration). Our approach outperforms predicting deformation or velocity fields, resulting in diffeomorphic transformations. We also develop a Bayesian probabilistic version of our network, which enables the evaluation of deformation field uncertainty through Monte Carlo sampling. We demonstrate our method's effectiveness on the OASIS brain image dataset in 2D and 3D, and show that deformation uncertainty highlights areas of ambiguous deformations.}"}, "1911.08007": {"original_text": "  The classification of streets on road networks has been focused on the\nvehicular transportational features of streets such as arterials, major roads,\nminor roads and so forth based on their transportational use. City authorities\non the other hand have been shifting to more urban inclusive planning of\nstreets, encompassing the side use of a street combined with the\ntransportational features of a street. In such classification schemes, streets\nare labeled for example as commercial throughway, residential neighborhood,\npark etc. This modern approach to urban planning has been adopted by major\ncities such as the city of San Francisco, the states of Florida and\nPennsylvania among many others. Currently, the process of labeling streets\naccording to their contexts is manual and hence is tedious and time consuming.\nIn this paper, we propose an approach to collect and label imagery data then\ndeploy advancements in computer vision towards modern urban planning. We\ncollect and label street imagery then train deep convolutional neural networks\n(CNN) to perform the classification of street context. We show that CNN models\ncan perform well achieving accuracies in the 81% to 87%, we then visualize\nsamples from the embedding space of streets using the t-SNE method and apply\nclass activation mapping methods to interpret the features in street imagery\ncontributing to output classification from a model.\n", "rewritten_text": "\\boxed{\nTraditional street classification has focused on vehicular transportation features, categorizing streets as arterials, major roads, and minor roads based on their transportation use. However, city authorities are shifting towards a more inclusive approach, considering the multiple uses of a street, such as commercial, residential, or recreational areas. This modern approach has been adopted by cities like San Francisco and states like Florida and Pennsylvania. Currently, labeling streets according to their context is a manual and time-consuming process. This paper proposes a new approach, using computer vision to collect and label street imagery data, and training deep convolutional neural networks (CNN) to classify street context. The results show that CNN models can achieve high accuracies, ranging from 81% to 87%. Additionally, the paper uses visualization techniques to interpret the features in street imagery that contribute to the classification output.\n}"}, "1811.11387": {"original_text": "  The success of deep neural networks generally requires a vast amount of\ntraining data to be labeled, which is expensive and unfeasible in scale,\nespecially for video collections. To alleviate this problem, in this paper, we\npropose 3DRotNet: a fully self-supervised approach to learn spatiotemporal\nfeatures from unlabeled videos. A set of rotations are applied to all videos,\nand a pretext task is defined as prediction of these rotations. When\naccomplishing this task, 3DRotNet is actually trained to understand the\nsemantic concepts and motions in videos. In other words, it learns a\nspatiotemporal video representation, which can be transferred to improve video\nunderstanding tasks in small datasets. Our extensive experiments successfully\ndemonstrate the effectiveness of the proposed framework on action recognition,\nleading to significant improvements over the state-of-the-art self-supervised\nmethods. With the self-supervised pre-trained 3DRotNet from large datasets, the\nrecognition accuracy is boosted up by 20.4% on UCF101 and 16.7% on HMDB51\nrespectively, compared to the models trained from scratch.\n", "rewritten_text": "\\boxed{Deep neural networks typically require a large amount of labeled training data, which can be costly and impractical, especially for video collections. To address this issue, we introduce 3DRotNet, a self-supervised approach that learns spatiotemporal features from unlabeled videos by predicting rotations applied to the videos. Through this process, 3DRotNet develops an understanding of semantic concepts and motions in videos, resulting in a transferable video representation that improves performance on video understanding tasks with limited data. Our experiments show that 3DRotNet significantly outperforms state-of-the-art self-supervised methods on action recognition tasks, boosting accuracy by 20.4% on UCF101 and 16.7% on HMDB51 compared to models trained from scratch.}"}, "2402.01619": {"original_text": "  Program induction (PI) has become a promising paradigm for using knowledge\nbases (KBs) to help large language models (LLMs) answer complex\nknowledge-intensive questions. Nonetheless, PI typically relies on a large\nnumber of parallel question-program pairs to make the LLM aware of the schema\nof the given KB, and is thus challenging for many low-resourced KBs that lack\nannotated data. To this end, we propose KB-Plugin, a plug-and-play framework\nthat enables LLMs to induce programs over any low-resourced KB. Firstly,\nKB-Plugin adopts self-supervised learning to encode the detailed schema\ninformation of a given KB into a pluggable module, namely schema plugin.\nSecondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB\nto train another pluggable module, namely PI plugin, which can help the LLM\nextract question-relevant schema information from the schema plugin of any KB\nand utilize this information to induce programs over this KB. Experiments on\nfive heterogeneous KBQA datasets show that KB-Plugin achieves better or\ncomparable performance with 25$\\times$ smaller backbone LLM compared to SoTA PI\nmethods for low-resourced KBs, and even approaches the performance of\nsupervised methods. Our code and data are available at\nhttps://github.com/THU-KEG/KB-Plugin.\n", "rewritten_text": "\\boxed{\nProgram induction (PI) is a promising approach for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions. However, PI typically requires a large number of parallel question-program pairs to make the LLM aware of the KB's schema, which can be challenging for low-resourced KBs with limited annotated data. To address this, we propose KB-Plugin, a flexible framework that enables LLMs to induce programs over any low-resourced KB. KB-Plugin uses self-supervised learning to encode the KB's schema information into a pluggable module, and then utilizes annotated data from a rich-resourced KB to train another module that helps the LLM extract relevant schema information and induce programs. Our experiments on five heterogeneous KBQA datasets show that KB-Plugin achieves better or comparable performance with a smaller backbone LLM compared to state-of-the-art PI methods for low-resourced KBs, and even approaches the performance of supervised methods.\n}"}, "1812.01458": {"original_text": "  Recent advances in deep learning have shown exciting promise in filling large\nholes and lead to another orientation for image inpainting. However, existing\nlearning-based methods often create artifacts and fallacious textures because\nof insufficient cognition understanding. Previous generative networks are\nlimited with single receptive type and give up pooling in consideration of\ndetail sharpness. Human cognition is constant regardless of the target\nattribute. As multiple receptive fields improve the ability of abstract image\ncharacterization and pooling can keep feature invariant, specifically, deep\ninception learning is adopted to promote high-level feature representation and\nenhance model learning capacity for local patches. Moreover, approaches for\ngenerating diverse mask images are introduced and a random mask dataset is\ncreated. We benchmark our methods on ImageNet, Places2 dataset, and CelebA-HQ.\nExperiments for regular, irregular, and custom regions completion are all\nperformed and free-style image inpainting is also presented. Quantitative\ncomparisons with previous state-of-the-art methods show that ours obtain much\nmore natural image completions.\n", "rewritten_text": "\\boxed{\nRecent breakthroughs in deep learning have opened up new possibilities for image inpainting, but existing methods often produce unrealistic textures and artifacts due to limited understanding of human cognition. Traditional generative networks are restricted by a single receptive field type and avoid pooling to preserve detail sharpness. In contrast, human cognition is consistent regardless of the target attribute. To address this, we adopt deep inception learning to enhance feature representation and model learning capacity for local patches, leveraging multiple receptive fields to improve image characterization and pooling to maintain feature invariance. We also introduce approaches for generating diverse mask images and create a random mask dataset. Our methods are evaluated on ImageNet, Places2, and CelebA-HQ datasets, with experiments on regular, irregular, and custom region completion, as well as free-style image inpainting. Quantitative comparisons with state-of-the-art methods demonstrate that our approach produces more natural and realistic image completions.\n}"}, "cs/0207005": {"original_text": "  We present a broad coverage Japanese grammar written in the HPSG formalism\nwith MRS semantics. The grammar is created for use in real world applications,\nsuch that robustness and performance issues play an important role. It is\nconnected to a POS tagging and word segmentation tool. This grammar is being\ndeveloped in a multilingual context, requiring MRS structures that are easily\ncomparable across languages.\n", "rewritten_text": "\\boxed{We introduce a comprehensive Japanese grammar formalized in HPSG with MRS semantics, designed for practical applications where robustness and performance are crucial. Integrated with a POS tagging and word segmentation tool, this grammar is being developed in a multilingual setting, enabling easy comparison of MRS structures across languages.}"}, "2307.10696": {"original_text": "  Improving the feature representation ability is the foundation of many whole\nslide pathological image (WSIs) tasks. Recent works have achieved great success\nin pathological-specific self-supervised learning (SSL). However, most of them\nonly focus on learning patch-level representations, thus there is still a gap\nbetween pretext and slide-level downstream tasks, e.g., subtyping, grading and\nstaging. Aiming towards slide-level representations, we propose Slide-Level\nPrototypical Distillation (SLPD) to explore intra- and inter-slide semantic\nstructures for context modeling on WSIs. Specifically, we iteratively perform\nintra-slide clustering for the regions (4096x4096 patches) within each WSI to\nyield the prototypes and encourage the region representations to be closer to\nthe assigned prototypes. By representing each slide with its prototypes, we\nfurther select similar slides by the set distance of prototypes and assign the\nregions by cross-slide prototypes for distillation. SLPD achieves\nstate-of-the-art results on multiple slide-level benchmarks and demonstrates\nthat representation learning of semantic structures of slides can make a\nsuitable proxy task for WSI analysis. Code will be available at\nhttps://github.com/Carboxy/SLPD.\n", "rewritten_text": "\\boxed{\nImproving feature representation is crucial for whole slide pathological image (WSI) tasks. While recent self-supervised learning (SSL) methods have shown success in learning patch-level representations, they often neglect slide-level representations, which are essential for tasks like subtyping, grading, and staging. To bridge this gap, we propose Slide-Level Prototypical Distillation (SLPD), a method that explores intra- and inter-slide semantic structures for context modeling on WSIs. SLPD involves iterative intra-slide clustering to generate prototypes, which are then used to represent each slide. Similar slides are selected based on prototype distances, and regions are assigned to cross-slide prototypes for distillation. Our approach achieves state-of-the-art results on multiple slide-level benchmarks, demonstrating that learning semantic structures of slides can be a suitable proxy task for WSI analysis. The code is available at https://github.com/Carboxy/SLPD.\n}"}, "2205.05869": {"original_text": "  We address the task of view synthesis, generating novel views of a scene\ngiven a set of images as input. In many recent works such as NeRF (Mildenhall\net al., 2020), the scene geometry is parameterized using neural implicit\nrepresentations (i.e., MLPs). Implicit neural representations have achieved\nimpressive visual quality but have drawbacks in computational efficiency. In\nthis work, we propose a new approach that performs view synthesis using point\nclouds. It is the first point-based method that achieves better visual quality\nthan NeRF while being 100x faster in rendering speed. Our approach builds on\nexisting works on differentiable point-based rendering but introduces a novel\ntechnique we call \"Sculpted Neural Points (SNP)\", which significantly improves\nthe robustness to errors and holes in the reconstructed point cloud. We further\npropose to use view-dependent point features based on spherical harmonics to\ncapture non-Lambertian surfaces, and new designs in the point-based rendering\npipeline that further boost the performance. Finally, we show that our system\nsupports fine-grained scene editing. Code is available at\nhttps://github.com/princeton-vl/SNP.\n", "rewritten_text": "\\boxed{\nThis paper tackles the task of view synthesis, which involves generating new views of a scene from a set of input images. While neural implicit representations (e.g., NeRF) have achieved high-quality results, they are computationally inefficient. We propose a novel approach that uses point clouds for view synthesis, achieving better visual quality than NeRF while being 100 times faster. Our method, called Sculpted Neural Points (SNP), improves the robustness of point-based rendering to errors and holes in the reconstructed point cloud. We also introduce view-dependent point features based on spherical harmonics to capture non-Lambertian surfaces and optimize the point-based rendering pipeline. Additionally, our system allows for fine-grained scene editing. The code is available at https://github.com/princeton-vl/SNP.\n}"}, "1910.03484": {"original_text": "  In Natural Language Generation (NLG), End-to-End (E2E) systems trained\nthrough deep learning have recently gained a strong interest. Such deep models\nneed a large amount of carefully annotated data to reach satisfactory\nperformance. However, acquiring such datasets for every new NLG application is\na tedious and time-consuming task. In this paper, we propose a semi-supervised\ndeep learning scheme that can learn from non-annotated data and annotated data\nwhen available. It uses an NLG and a Natural Language Understanding (NLU)\nsequence-to-sequence models which are learned jointly to compensate for the\nlack of annotation. Experiments on two benchmark datasets show that, with\nlimited amount of annotated data, the method can achieve very competitive\nresults while not using any pre-processing or re-scoring tricks. These findings\nopen the way to the exploitation of non-annotated datasets which is the current\nbottleneck for the E2E NLG system development to new applications.\n", "rewritten_text": "\\boxed{Recently, there has been growing interest in End-to-End (E2E) systems for Natural Language Generation (NLG) that utilize deep learning. However, these models require large amounts of carefully annotated data to achieve satisfactory performance, which can be a time-consuming and tedious task to acquire for each new NLG application. To address this challenge, we propose a semi-supervised deep learning approach that can learn from both annotated and non-annotated data. Our method jointly trains NLG and Natural Language Understanding (NLU) sequence-to-sequence models to compensate for the lack of annotation. Experimental results on two benchmark datasets demonstrate that our approach can achieve competitive results with limited annotated data, without relying on pre-processing or re-scoring techniques. This breakthrough opens up opportunities for exploiting non-annotated datasets, which is currently a major bottleneck in developing E2E NLG systems for new applications.}"}, "2109.09883": {"original_text": "  Few-shot classification aims at classifying categories of a novel task by\nlearning from just a few (typically, 1 to 5) labelled examples. An effective\napproach to few-shot classification involves a prior model trained on a\nlarge-sample base domain, which is then finetuned over the novel few-shot task\nto yield generalizable representations. However, task-specific finetuning is\nprone to overfitting due to the lack of enough training examples. To alleviate\nthis issue, we propose a new finetuning approach based on contrastive learning\nthat reuses unlabelled examples from the base domain in the form of\ndistractors. Unlike the nature of unlabelled data used in prior works,\ndistractors belong to classes that do not overlap with the novel categories. We\ndemonstrate for the first time that inclusion of such distractors can\nsignificantly boost few-shot generalization. Our technical novelty includes a\nstochastic pairing of examples sharing the same category in the few-shot task\nand a weighting term that controls the relative influence of task-specific\nnegatives and distractors. An important aspect of our finetuning objective is\nthat it is agnostic to distractor labels and hence applicable to various base\ndomain settings. Compared to state-of-the-art approaches, our method shows\naccuracy gains of up to $12\\%$ in cross-domain and up to $5\\%$ in unsupervised\nprior-learning settings.\n", "rewritten_text": "\\boxed{\nFew-shot classification, which involves categorizing new tasks with only a few labeled examples, can be improved by fine-tuning a pre-trained model on a large base domain. However, this approach often leads to overfitting due to the limited number of training examples. To address this, we propose a novel fine-tuning method that utilizes contrastive learning and incorporates unlabeled examples from the base domain as \"distractors\" that belong to non-overlapping classes. Our approach, which includes stochastic pairing of examples and a weighting term to balance task-specific negatives and distractors, significantly enhances few-shot generalization without requiring distractor labels. Compared to existing state-of-the-art methods, our technique achieves accuracy improvements of up to 12% in cross-domain and 5% in unsupervised prior-learning settings.\n}"}, "2003.12137": {"original_text": "  We explore novel approaches to the task of image generation from their\nrespective captions, building on state-of-the-art GAN architectures.\nParticularly, we baseline our models with the Attention-based GANs that learn\nattention mappings from words to image features. To better capture the features\nof the descriptions, we then built a novel cyclic design that learns an inverse\nfunction to maps the image back to original caption. Additionally, we\nincorporated recently developed BERT pretrained word embeddings as our initial\ntext featurizer and observe a noticeable improvement in qualitative and\nquantitative performance compared to the Attention GAN baseline.\n", "rewritten_text": "\\boxed{We develop innovative methods for generating images from captions, leveraging cutting-edge GAN architectures. Specifically, we improve upon Attention-based GANs, which learn to map words to image features, by introducing a novel cyclic design that reverses this process, mapping images back to their original captions. Furthermore, we utilize BERT's pre-trained word embeddings as our initial text featurizer, resulting in significant enhancements to both qualitative and quantitative performance compared to the Attention GAN baseline.}"}, "2003.02683": {"original_text": "  We introduce the first method for automatic image generation from scene-level\nfreehand sketches. Our model allows for controllable image generation by\nspecifying the synthesis goal via freehand sketches. The key contribution is an\nattribute vector bridged Generative Adversarial Network called EdgeGAN, which\nsupports high visual-quality object-level image content generation without\nusing freehand sketches as training data. We have built a large-scale composite\ndataset called SketchyCOCO to support and evaluate the solution. We validate\nour approach on the tasks of both object-level and scene-level image generation\non SketchyCOCO. Through quantitative, qualitative results, human evaluation and\nablation studies, we demonstrate the method's capacity to generate realistic\ncomplex scene-level images from various freehand sketches.\n", "rewritten_text": "\\boxed{We propose a novel method for generating images from scene-level freehand sketches, enabling controllable image synthesis through sketch-based goal specification. Our EdgeGAN model, which leverages an attribute vector bridge, can produce high-quality object-level image content without relying on freehand sketches as training data. We created a large-scale dataset, SketchyCOCO, to support and evaluate our approach, and demonstrate its effectiveness in generating realistic complex scene-level images from diverse freehand sketches through quantitative, qualitative, and human evaluation results, as well as ablation studies.}"}, "2409.00942": {"original_text": "  Normalizing flows, a category of probabilistic models famed for their\ncapabilities in modeling complex data distributions, have exhibited remarkable\nefficacy in unsupervised anomaly detection. This paper explores the potential\nof normalizing flows in multi-class anomaly detection, wherein the normal data\nis compounded with multiple classes without providing class labels. Through the\nintegration of vector quantization (VQ), we empower the flow models to\ndistinguish different concepts of multi-class normal data in an unsupervised\nmanner, resulting in a novel flow-based unified method, named VQ-Flow.\nSpecifically, our VQ-Flow leverages hierarchical vector quantization to\nestimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for\nconcept distinction and its concomitant Concept-Specific Pattern Codebook\n(CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow\nare conditioned on the concept-specific patterns captured in CSPC, capable of\nmodeling specific normal patterns associated with different concepts. Moreover,\nCPC further enables our VQ-Flow for concept-aware distribution modeling,\nfaithfully mimicking the intricate multi-class normal distribution through a\nmixed Gaussian distribution reparametrized on the conceptual prototypes.\nThrough the introduction of vector quantization, the proposed VQ-Flow advances\nthe state-of-the-art in multi-class anomaly detection within a unified training\nscheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase\nis publicly available at https://github.com/cool-xuan/vqflow.\n", "rewritten_text": "\\boxed{\nNormalizing flows, a type of probabilistic model, have shown great success in detecting anomalies in unsupervised settings. This paper explores their potential in detecting anomalies in multi-class data, where normal data consists of multiple classes without labels. By combining normalizing flows with vector quantization (VQ), we propose a novel method called VQ-Flow, which can distinguish different concepts in multi-class normal data without supervision. VQ-Flow uses hierarchical vector quantization to create two codebooks: one for concept distinction and another for capturing concept-specific normal patterns. The flow models are conditioned on these patterns, allowing them to model specific normal patterns associated with different concepts. Additionally, VQ-Flow enables concept-aware distribution modeling, accurately mimicking the complex multi-class normal distribution. Our results show that VQ-Flow advances the state-of-the-art in multi-class anomaly detection, achieving a detection/localization AUROC of 99.5%/98.3% on the MVTec AD dataset. The code is publicly available at https://github.com/cool-xuan/vqflow.\n}"}, "1708.00801": {"original_text": "  We study the impact of big models (in terms of the degree of lexicalization)\nand big data (in terms of the training corpus size) on dependency grammar\ninduction. We experimented with L-DMV, a lexicalized version of Dependency\nModel with Valence and L-NDMV, our lexicalized extension of the Neural\nDependency Model with Valence. We find that L-DMV only benefits from very small\ndegrees of lexicalization and moderate sizes of training corpora. L-NDMV can\nbenefit from big training data and lexicalization of greater degrees,\nespecially when enhanced with good model initialization, and it achieves a\nresult that is competitive with the current state-of-the-art.\n", "rewritten_text": "\\boxed{This study examines the effects of large models with extensive lexicalization and big data with large training corpora on dependency grammar induction. The results show that the lexicalized Dependency Model with Valence (L-DMV) only improves with minimal lexicalization and moderate training data, whereas the lexicalized Neural Dependency Model with Valence (L-NDMV) benefits from large training data and higher degrees of lexicalization, especially when combined with good model initialization, achieving competitive results with the current state-of-the-art.}"}, "2303.1309": {"original_text": "  Recent trends in semi-supervised learning have significantly boosted the\nperformance of 3D semi-supervised medical image segmentation. Compared with 2D\nimages, 3D medical volumes involve information from different directions, e.g.,\ntransverse, sagittal, and coronal planes, so as to naturally provide\ncomplementary views. These complementary views and the intrinsic similarity\namong adjacent 3D slices inspire us to develop a novel annotation way and its\ncorresponding semi-supervised model for effective segmentation. Specifically,\nwe firstly propose the orthogonal annotation by only labeling two orthogonal\nslices in a labeled volume, which significantly relieves the burden of\nannotation. Then, we perform registration to obtain the initial pseudo labels\nfor sparsely labeled volumes. Subsequently, by introducing unlabeled volumes,\nwe propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that\nexploits dense pseudo labels in early stage and sparse labels in later stage\nand meanwhile forces consistent output of two networks. Experimental results on\nthree benchmark datasets validated our effectiveness in performance and\nefficiency in annotation. For example, with only 10 annotated slices, our\nmethod reaches a Dice up to 86.93% on KiTS19 dataset.\n", "rewritten_text": "\\boxed{Recent advances in semi-supervised learning have significantly improved the accuracy of 3D medical image segmentation. The unique characteristics of 3D medical volumes, which provide complementary views from different directions, inspired us to develop a novel annotation approach and corresponding semi-supervised model. Our approach involves annotating only two orthogonal slices in a labeled volume, reducing the annotation burden, and then registering to obtain initial pseudo labels. We then propose a dual-network paradigm, Dense-Sparse Co-training (DeSCO), which leverages dense pseudo labels early on and sparse labels later, while ensuring consistent output from two networks. Our method has been validated on three benchmark datasets, achieving high performance with minimal annotation, such as 86.93% Dice on KiTS19 with only 10 annotated slices.}"}, "2401.08123": {"original_text": "  Guided depth super-resolution (GDSR) involves restoring missing depth details\nusing the high-resolution RGB image of the same scene. Previous approaches have\nstruggled with the heterogeneity and complementarity of the multi-modal inputs,\nand neglected the issues of modal misalignment, geometrical misalignment, and\nfeature selection. In this study, we rethink some essential components in GDSR\nnetworks and propose a simple yet effective Dynamic Dual Alignment and\nAggregation network (D2A2). D2A2 mainly consists of 1) a dynamic dual alignment\nmodule that adapts to alleviate the modal misalignment via a learnable domain\nalignment block and geometrically align cross-modal features by learning the\noffset; and 2) a mask-to-pixel feature aggregate module that uses the gated\nmechanism and pixel attention to filter out irrelevant texture noise from RGB\nfeatures and combine the useful features with depth features. By combining the\nstrengths of RGB and depth features while minimizing disturbance introduced by\nthe RGB image, our method with simple reuse and redesign of basic components\nachieves state-of-the-art performance on multiple benchmark datasets. The code\nis available at https://github.com/JiangXinni/D2A2.\n", "rewritten_text": "\\boxed{\nGuided depth super-resolution (GDSR) is a technique that restores missing depth details using a high-resolution RGB image of the same scene. However, previous approaches have struggled with the differences and complementarities of the multi-modal inputs, and have neglected issues such as modal misalignment, geometrical misalignment, and feature selection. To address these challenges, we propose a novel Dynamic Dual Alignment and Aggregation network (D2A2) that consists of two main components: 1) a dynamic dual alignment module that adapts to alleviate modal misalignment and geometrically aligns cross-modal features, and 2) a mask-to-pixel feature aggregate module that filters out irrelevant texture noise from RGB features and combines useful features with depth features. Our method achieves state-of-the-art performance on multiple benchmark datasets by effectively combining the strengths of RGB and depth features while minimizing disturbance introduced by the RGB image. The code is available at https://github.com/JiangXinni/D2A2.\n}"}, "2112.09414": {"original_text": "  By highlighting the regions of the input image that contribute the most to\nthe decision, saliency maps have become a popular method to make neural\nnetworks interpretable. In medical imaging, they are particularly well-suited\nto explain neural networks in the context of abnormality localization. However,\nfrom our experiments, they are less suited to classification problems where the\nfeatures that allow to distinguish between the different classes are spatially\ncorrelated, scattered and definitely non-trivial. In this paper we thus propose\na new paradigm for better interpretability. To this end we provide the user\nwith relevant and easily interpretable information so that he can form his own\nopinion. We use Disentangled Variational Auto-Encoders which latent\nrepresentation is divided into two components: the non-interpretable part and\nthe disentangled part. The latter accounts for the categorical variables\nexplicitly representing the different classes of interest. In addition to\nproviding the class of a given input sample, such a model offers the\npossibility to transform the sample from a given class to a sample of another\nclass, by modifying the value of the categorical variables in the latent\nrepresentation. This paves the way to easier interpretation of class\ndifferences. We illustrate the relevance of this approach in the context of\nautomatic sex determination from hip bones in forensic medicine. The features\nencoded by the model, that distinguish the different classes were found to be\nconsistent with expert knowledge.\n", "rewritten_text": "\\boxed{\nSaliency maps are a popular method for making neural networks interpretable by highlighting the most important regions of an input image. However, they are less effective for classification problems where features are spatially correlated and scattered. To address this, we propose a new approach using Disentangled Variational Auto-Encoders, which separate the latent representation into interpretable and non-interpretable components. This allows for explicit representation of categorical variables and enables transformation of a sample from one class to another by modifying these variables. This approach facilitates easier interpretation of class differences and is illustrated in the context of automatic sex determination from hip bones in forensic medicine, where the encoded features are consistent with expert knowledge.\n}"}, "1910.06431": {"original_text": "  There has been great success recently in tackling challenging NLP tasks by\nneural networks which have been pre-trained and fine-tuned on large amounts of\ntask data. In this paper, we investigate one such model, BERT for\nquestion-answering, with the aim to analyze why it is able to achieve\nsignificantly better results than other models. We run DeepLIFT on the model\npredictions and test the outcomes to monitor shift in the attention values for\ninput. We also cluster the results to analyze any possible patterns similar to\nhuman reasoning depending on the kind of input paragraph and question the model\nis trying to answer.\n", "rewritten_text": "\\boxed{Recently, neural networks pre-trained and fine-tuned on large datasets have achieved remarkable success in tackling challenging NLP tasks. This paper delves into the performance of BERT, a model that excels in question-answering, to understand the reasons behind its superior results. We employ DeepLIFT to analyze the model's predictions, tracking changes in attention values for input data. Furthermore, we cluster the results to identify patterns similar to human reasoning, depending on the input paragraph and question being addressed.}"}, "2410.15277": {"original_text": "  Retrieval-augmented generation (RAG) can supplement large language models\n(LLMs) by integrating external knowledge. However, as the number of retrieved\ndocuments increases, the input length to LLMs grows linearly, causing a\ndramatic increase in latency and a degradation in long-context understanding.\nThis is particularly serious for multi-hop questions that require a chain of\nreasoning across documents. To accelerate inference, reduce costs, and minimize\ndistractions, this paper presents BRIEF (Bridging Retrieval and Inference\nthrough Evidence Fusion), a lightweight approach that performs query-aware\nmulti-hop reasoning by compressing retrieved documents into highly dense\ntextual summaries to integrate into in-context learning. To enable learning\ncompression for multi-hop reasoning, we curate synthetic data by extracting\natomic proposition expressions that encapsulate distinct factoids from the\nsource documents to compose synthetic summaries. Based on our synthetic data\nbuilt entirely by open-source models, BRIEF generates more concise summaries\nand enables a range of LLMs to achieve exceptional open-domain question\nanswering (QA) performance. For example, on HotpotQA, BRIEF improves the\ncompression rate by 2 times compared to the state-of-the-art baseline, while\noutperforming it by 3.00% EM and 4.16% F1 with Flan-UL2 as the reader LM. It\nalso generates more concise summaries than proprietary GPT-3.5, while\ndemonstrating nearly identical QA performance.\n", "rewritten_text": "\\boxed{\nRetrieval-augmented generation (RAG) can be enhanced by incorporating external knowledge into large language models (LLMs). However, as more documents are retrieved, the input length to LLMs increases, leading to slower processing times and decreased understanding of long contexts. This is particularly problematic for multi-hop questions that require reasoning across multiple documents. To address this, we propose BRIEF, a lightweight approach that accelerates inference, reduces costs, and minimizes distractions by compressing retrieved documents into concise summaries. These summaries are integrated into in-context learning, enabling query-aware multi-hop reasoning. We create synthetic data by extracting key factoids from source documents to train BRIEF. Our approach generates more concise summaries and achieves exceptional open-domain question answering performance with various LLMs. For instance, on HotpotQA, BRIEF outperforms the state-of-the-art baseline by 3.00% EM and 4.16% F1, while generating more concise summaries than proprietary GPT-3.5 with similar QA performance.\n}"}, "2403.17512": {"original_text": "  Improving the efficiency of current neural networks and modeling them in\nbiological neural systems have become popular research directions in recent\nyears. Pulse-coupled neural network (PCNN) is a well applicated model for\nimitating the computation characteristics of the human brain in computer vision\nand neural network fields. However, differences between the PCNN and biological\nneural systems remain: limited neural connection, high computational cost, and\nlack of stochastic property. In this study, random-coupled neural network\n(RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic\ncomputing via a random inactivation process. This process randomly closes some\nneural connections in the RCNN model, realized by the random inactivation\nweight matrix of link input. This releases the computational burden of PCNN,\nmaking it affordable to achieve vast neural connections. Furthermore, the image\nand video processing mechanisms of RCNN are researched. It encodes constant\nstimuli as periodic spike trains and periodic stimuli as chaotic spike trains,\nthe same as biological neural information encoding characteristics. Finally,\nthe RCNN is applicated to image segmentation, fusion, and pulse shape\ndiscrimination subtasks. It is demonstrated to be robust, efficient, and highly\nanti-noised, with outstanding performance in all applications mentioned above.\n", "rewritten_text": "\\boxed{\nRecent research has focused on improving the efficiency of neural networks and modeling them after biological neural systems. The Pulse-Coupled Neural Network (PCNN) is a well-established model that mimics the human brain's computation characteristics in computer vision and neural networks. However, it has limitations, including limited neural connections, high computational costs, and a lack of stochastic properties. To address these issues, this study proposes the Random-Coupled Neural Network (RCNN), which incorporates a random inactivation process to overcome the difficulties of PCNN's neuromorphic computing. This process randomly closes some neural connections, reducing computational costs and enabling vast neural connections. The RCNN model is also shown to encode stimuli similarly to biological neural systems, and its applications in image segmentation, fusion, and pulse shape discrimination demonstrate its robustness, efficiency, and anti-noise capabilities.\n}"}, "1907.0916": {"original_text": "  Facial Micro-Expressions (MEs) are spontaneous, involuntary facial movements\nwhen a person experiences an emotion but deliberately or unconsciously attempts\nto conceal his or her genuine emotions. Recently, ME recognition has attracted\nincreasing attention due to its potential applications such as clinical\ndiagnosis, business negotiation, interrogations, and security. However, it is\nexpensive to build large scale ME datasets, mainly due to the difficulty of\ninducing spontaneous MEs. This limits the application of deep learning\ntechniques which require lots of training data. In this paper, we propose a\nsimple, efficient yet robust descriptor called Extended Local Binary Patterns\non Three Orthogonal Planes (ELBPTOP) for ME recognition. ELBPTOP consists of\nthree complementary binary descriptors: LBPTOP and two novel ones Radial\nDifference LBPTOP (RDLBPTOP) and Angular Difference LBPTOP (ADLBPTOP), which\nexplore the local second order information along the radial and angular\ndirections contained in ME video sequences. ELBPTOP is a novel ME descriptor\ninspired by unique and subtle facial movements. It is computationally efficient\nand only marginally increases the cost of computing LBPTOP, yet is extremely\neffective for ME recognition. In addition, by firstly introducing Whitened\nPrincipal Component Analysis (WPCA) to ME recognition, we can further obtain\nmore compact and discriminative feature representations, then achieve\nsignificantly computational savings. Extensive experimental evaluation on three\npopular spontaneous ME datasets SMIC, CASME II and SAMM show that our proposed\nELBPTOP approach significantly outperforms the previous state-of-the-art on all\nthree single evaluated datasets and achieves promising results on\ncross-database recognition.Our code will be made available.\n", "rewritten_text": "\\boxed{\nFacial micro-expressions (MEs) are brief, involuntary facial movements that occur when a person tries to hide their true emotions. Recognizing MEs has many potential applications, including clinical diagnosis, business negotiations, and security. However, creating large ME datasets is expensive and time-consuming, which limits the use of deep learning techniques that require extensive training data. To address this, we propose a new descriptor called Extended Local Binary Patterns on Three Orthogonal Planes (ELBPTOP) for ME recognition. ELBPTOP combines three binary descriptors that capture local second-order information in ME video sequences, making it computationally efficient and effective for ME recognition. Additionally, we introduce Whitened Principal Component Analysis (WPCA) to ME recognition, which reduces computational costs and improves feature representations. Our experiments on three popular ME datasets show that ELBPTOP outperforms previous state-of-the-art methods and achieves promising results in cross-database recognition. Our code will be made publicly available.\n}"}, "2402.11431": {"original_text": "  To address the issue of increased triangulation uncertainty caused by\nselecting views with small camera baselines in Structure from Motion (SFM) view\nselection, this paper proposes a robust error-resistant view selection method.\nThe method utilizes a triangulation-based computation to obtain an\nerror-resistant model, which is then used to construct an error-resistant\nmatrix. The sorting results of each row in the error-resistant matrix determine\nthe candidate view set for each view. By traversing the candidate view sets of\nall views and completing the missing views based on the error-resistant matrix,\nthe integrity of 3D reconstruction is ensured. Experimental comparisons between\nthis method and the exhaustive method with the highest accuracy in the COLMAP\nprogram are conducted in terms of average reprojection error and absolute\ntrajectory error in the reconstruction results. The proposed method\ndemonstrates an average reduction of 29.40% in reprojection error accuracy and\n5.07% in absolute trajectory error on the TUM dataset and DTU dataset.\n", "rewritten_text": "\\boxed{This paper presents a robust view selection method to mitigate the issue of triangulation uncertainty in Structure from Motion (SFM) caused by selecting views with small camera baselines. The proposed method uses a triangulation-based computation to create an error-resistant model and matrix, which are then used to determine the optimal view set for each view. Experimental results show that the proposed method outperforms the exhaustive method in COLMAP, achieving an average reduction of 29.40% in reprojection error and 5.07% in absolute trajectory error on the TUM and DTU datasets, ensuring more accurate 3D reconstruction.}"}, "2306.12693": {"original_text": "  This paper gives an Indic-to-Indic (IL-IL) MNMT baseline model for 11 ILs\nimplemented on the Samanantar corpus and analyzed on the Flores-200 corpus. All\nthe models are evaluated using the BLEU score. In addition, the languages are\nclassified under three groups namely East Indo- Aryan (EI), Dravidian (DR), and\nWest Indo-Aryan (WI). The effect of language relatedness on MNMT model\nefficiency is studied. Owing to the presence of large corpora from English (EN)\nto ILs, MNMT IL-IL models using EN as a pivot are also built and examined. To\nachieve this, English- Indic (EN-IL) models are also developed, with and\nwithout the usage of related languages. Results reveal that using related\nlanguages is beneficial for the WI group only, while it is detrimental for the\nEI group and shows an inconclusive effect on the DR group, but it is useful for\nEN-IL models. Thus, related language groups are used to develop pivot MNMT\nmodels. Furthermore, the IL corpora are transliterated from the corresponding\nscripts to a modified ITRANS script, and the best MNMT models from the previous\napproaches are built on the transliterated corpus. It is observed that the\nusage of pivot models greatly improves MNMT baselines with AS-TA achieving the\nminimum BLEU score and PA-HI achieving the maximum score. Among languages, AS,\nML, and TA achieve the lowest BLEU score, whereas HI, PA, and GU perform the\nbest. Transliteration also helps the models with few exceptions. The best\nincrement of scores is observed in ML, TA, and BN and the worst average\nincrement is observed in KN, HI, and PA, across all languages. The best model\nobtained is the PA-HI language pair trained on PAWI transliterated corpus which\ngives 24.29 BLEU.\n", "rewritten_text": "\\boxed{\nThis paper presents a baseline model for Indic-to-Indic machine translation (IL-IL MNMT) for 11 languages, using the Samanantar corpus and evaluated on the Flores-200 corpus. The languages are categorized into three groups: East Indo-Aryan, Dravidian, and West Indo-Aryan. The impact of language relatedness on MNMT model efficiency is investigated. Additionally, MNMT models using English as a pivot language are developed and analyzed. The results show that using related languages improves performance for the West Indo-Aryan group, but hinders it for the East Indo-Aryan group, and has an inconclusive effect on the Dravidian group. However, it is beneficial for English-Indic models. The paper also explores the effect of transliterating the Indic scripts to a modified ITRANS script on MNMT performance. The results indicate that using pivot models significantly improves MNMT baselines, with the PA-HI language pair achieving the highest BLEU score of 24.29. Among languages, AS, ML, and TA perform poorly, while HI, PA, and GU perform well. Transliteration generally improves model performance, with the most significant improvements observed in ML, TA, and BN, and the least in KN, HI, and PA."}, "2403.08487": {"original_text": "  Diffusion models pose risks of privacy breaches and copyright disputes,\nprimarily stemming from the potential utilization of unauthorized data during\nthe training phase. The Training Membership Inference (TMI) task aims to\ndetermine whether a specific sample has been used in the training process of a\ntarget model, representing a critical tool for privacy violation verification.\nHowever, the increased stochasticity inherent in diffusion renders traditional\nshadow-model-based or metric-based methods ineffective when applied to\ndiffusion models. Moreover, existing methods only yield binary classification\nlabels which lack necessary comprehensibility in practical applications. In\nthis paper, we explore a novel perspective for the TMI task by leveraging the\nintrinsic generative priors within the diffusion model. Compared with unseen\nsamples, training samples exhibit stronger generative priors within the\ndiffusion model, enabling the successful reconstruction of substantially\ndegraded training images. Consequently, we propose the Degrade Restore Compare\n(DRC) framework. In this framework, an image undergoes sequential degradation\nand restoration, and its membership is determined by comparing it with the\nrestored counterpart. Experimental results verify that our approach not only\nsignificantly outperforms existing methods in terms of accuracy but also\nprovides comprehensible decision criteria, offering evidence for potential\nprivacy violations.\n", "rewritten_text": "\\boxed{\nDiffusion models raise concerns about privacy breaches and copyright issues due to the potential use of unauthorized data during training. To address this, the Training Membership Inference (TMI) task aims to identify whether a specific sample was used to train a target model. However, traditional methods are ineffective for diffusion models due to their inherent stochasticity. Existing methods also only provide binary labels, lacking practical applicability. This paper proposes a novel approach to TMI by leveraging the generative priors within diffusion models. By degrading and restoring images, our Degrade Restore Compare (DRC) framework determines membership by comparing the original and restored images. Our approach outperforms existing methods in accuracy and provides comprehensible decision criteria, offering evidence for potential privacy violations.\n}"}, "2309.03390": {"original_text": "  In this paper, we seek a new method in designing an iris recognition system.\nIn this method, first the Haar wavelet features are extracted from iris images.\nThe advantage of using these features is the high-speed extraction, as well as\nbeing unique to each iris. Then the back propagation neural network (BPNN) is\nused as a classifier. In this system, the BPNN parallel algorithms and their\nimplementation on GPUs have been used by the aid of CUDA in order to speed up\nthe learning process. Finally, the system performance and the speeding outcomes\nin a way that this algorithm is done in series are presented.\n", "rewritten_text": "\\boxed{This paper proposes a novel approach to designing an iris recognition system, which involves extracting Haar wavelet features from iris images, leveraging their high-speed extraction and uniqueness to each iris. The system utilizes a back propagation neural network (BPNN) as a classifier, and to accelerate the learning process, parallel algorithms are implemented on Graphics Processing Units (GPUs) using CUDA. The performance of the system and the resulting speedup are evaluated and compared to a serial implementation.}"}, "2311.10436": {"original_text": "  Since their inception, embeddings have become a primary ingredient in many\nflavours of Natural Language Processing (NLP) tasks supplanting earlier types\nof representation. Even though multilingual embeddings have been used for the\nincreasing number of multilingual tasks, due to the scarcity of parallel\ntraining data, low-resource languages such as Sinhala, tend to focus more on\nmonolingual embeddings. Then when it comes to the aforementioned multi-lingual\ntasks, it is challenging to utilize these monolingual embeddings given that\neven if the embedding spaces have a similar geometric arrangement due to an\nidentical training process, the embeddings of the languages considered are not\naligned. This is solved by the embedding alignment task. Even in this,\nhigh-resource language pairs are in the limelight while low-resource languages\nsuch as Sinhala which is in dire need of help seem to have fallen by the\nwayside. In this paper, we try to align Sinhala and English word embedding\nspaces based on available alignment techniques and introduce a benchmark for\nSinhala language embedding alignment. In addition to that, to facilitate the\nsupervised alignment, as an intermediate task, we also introduce\nSinhala-English alignment datasets. These datasets serve as our anchor datasets\nfor supervised word embedding alignment. Even though we do not obtain results\ncomparable to the high-resource languages such as French, German, or Chinese,\nwe believe our work lays the groundwork for more specialized alignment between\nEnglish and Sinhala embeddings.\n", "rewritten_text": "\\boxed{\nEmbeddings have become a crucial component in various Natural Language Processing (NLP) tasks, replacing traditional representation methods. While multilingual embeddings are widely used, low-resource languages like Sinhala often rely on monolingual embeddings due to limited parallel training data. However, these monolingual embeddings are challenging to utilize in multilingual tasks because they are not aligned, even if they share a similar geometric structure. To address this, embedding alignment techniques are employed. Unfortunately, high-resource language pairs have received more attention, leaving low-resource languages like Sinhala behind. This paper aims to bridge this gap by aligning Sinhala and English word embeddings using available techniques and establishing a benchmark for Sinhala language embedding alignment. Additionally, we introduce Sinhala-English alignment datasets to facilitate supervised alignment. Although our results may not be comparable to those of high-resource languages like French, German, or Chinese, our work lays the foundation for more specialized alignment between English and Sinhala embeddings.\n}"}, "2301.08245": {"original_text": "  Estimating depth from images nowadays yields outstanding results, both in\nterms of in-domain accuracy and generalization. However, we identify two main\nchallenges that remain open in this field: dealing with non-Lambertian\nmaterials and effectively processing high-resolution images. Purposely, we\npropose a novel dataset that includes accurate and dense ground-truth labels at\nhigh resolution, featuring scenes containing several specular and transparent\nsurfaces. Our acquisition pipeline leverages a novel deep space-time stereo\nframework, enabling easy and accurate labeling with sub-pixel precision. The\ndataset is composed of 606 samples collected in 85 different scenes, each\nsample includes both a high-resolution pair (12 Mpx) as well as an unbalanced\nstereo pair (Left: 12 Mpx, Right: 1.1 Mpx), typical of modern mobile devices\nthat mount sensors with different resolutions. Additionally, we provide\nmanually annotated material segmentation masks and 15K unlabeled samples. The\ndataset is composed of a train set and two test sets, the latter devoted to the\nevaluation of stereo and monocular depth estimation networks. Our experiments\nhighlight the open challenges and future research directions in this field.\n", "rewritten_text": "\\boxed{\nEstimating depth from images has made significant progress, achieving high accuracy and generalization. However, two major challenges remain: handling non-Lambertian materials and processing high-resolution images efficiently. To address these challenges, we introduce a novel dataset featuring high-resolution images with accurate labels, including scenes with specular and transparent surfaces. Our dataset consists of 606 samples from 85 scenes, with each sample including a high-resolution stereo pair and an unbalanced stereo pair, similar to those found in modern mobile devices. Additionally, we provide material segmentation masks and 15,000 unlabeled samples. The dataset is divided into a training set and two test sets, one for evaluating stereo depth estimation and the other for monocular depth estimation. Our experiments reveal the open challenges and future research directions in this field.\n}"}, "1709.00813": {"original_text": "  Various text analysis techniques exist, which attempt to uncover unstructured\ninformation from text. In this work, we explore using statistical dependence\nmeasures for textual classification, representing text as word vectors. Student\nsatisfaction scores on a 3-point scale and their free text comments written\nabout university subjects are used as the dataset. We have compared two textual\nrepresentations: a frequency word representation and term frequency\nrelationship to word vectors, and found that word vectors provide a greater\naccuracy. However, these word vectors have a large number of features which\naggravates the burden of computational complexity. Thus, we explored using a\nnon-linear dependency measure for feature selection by maximizing the\ndependence between the text reviews and corresponding scores. Our quantitative\nand qualitative analysis on a student satisfaction dataset shows that our\napproach achieves comparable accuracy to the full feature vector, while being\nan order of magnitude faster in testing. These text analysis and feature\nreduction techniques can be used for other textual data applications such as\nsentiment analysis.\n", "rewritten_text": "\\boxed{This study examines the use of statistical dependence measures for text classification, where text is represented as word vectors. Using a dataset of student satisfaction scores and corresponding free text comments, we compare two textual representations and find that word vectors provide higher accuracy. However, the large number of features in word vectors increases computational complexity. To address this, we employ a non-linear dependency measure for feature selection, which maximizes the dependence between text reviews and scores. Our analysis shows that this approach achieves comparable accuracy to using the full feature vector, but with a significantly faster testing time. These techniques can be applied to other textual data applications, such as sentiment analysis.}"}, "1704.0803": {"original_text": "  Some lung diseases are related to bronchial airway structures and morphology.\nAlthough airway segmentation from chest CT volumes is an important task in the\ncomputer-aided diagnosis and surgery assistance systems for the chest, complete\n3-D airway structure segmentation is a quite challenging task due to its\ncomplex tree-like structure. In this paper, we propose a new airway\nsegmentation method from 3D chest CT volumes based on volume of interests (VOI)\nusing gradient vector flow (GVF). This method segments the bronchial regions by\napplying the cavity enhancement filter (CEF) to trace the bronchial tree\nstructure from the trachea. It uses the CEF in the VOI to segment each branch.\nAnd a tube-likeness function based on GVF and the GVF magnitude map in each VOI\nare utilized to assist predicting the positions and directions of child\nbranches. By calculating the tube-likeness function based on GVF and the GVF\nmagnitude map, the airway-like candidate structures are identified and their\ncentrelines are extracted. Based on the extracted centrelines, we can detect\nthe branch points of the bifurcations and directions of the airway branches in\nthe next level. At the same time, a leakage detection is performed to avoid the\nleakage by analysing the pixel information and the shape information of airway\ncandidate regions extracted in the VOI. Finally, we unify all of the extracted\nbronchial regions to form an integrated airway tree. Preliminary experiments\nusing four cases of chest CT volumes demonstrated that the proposed method can\nextract more bronchial branches in comparison with other methods.\n", "rewritten_text": "\\boxed{\nSegmenting 3D airway structures from chest CT scans is crucial for computer-aided diagnosis and surgery assistance, but it's a challenging task due to the complex tree-like structure of the airways. This paper proposes a new method for airway segmentation using volume of interests (VOI) and gradient vector flow (GVF). The method enhances cavity regions to trace the bronchial tree, and uses a tube-likeness function to predict branch positions and directions. By analyzing pixel and shape information, the method detects branch points and avoids leakage. The extracted bronchial regions are then unified to form a complete airway tree. Preliminary experiments show that this method can extract more bronchial branches than other methods.\n}"}, "1906.03249": {"original_text": "  Word embeddings are traditionally trained on a large corpus in an\nunsupervised setting, with no specific design for incorporating domain\nknowledge. This can lead to unsatisfactory performances when training data\noriginate from heterogeneous domains. In this paper, we propose two novel\nmechanisms for domain-aware word embedding training, namely domain indicator\nand domain attention, which integrate domain-specific knowledge into the widely\nused SG and CBOW models, respectively. The two methods are based on a joint\nlearning paradigm and ensure that words in a target domain are intensively\nfocused when trained on a source domain corpus. Qualitative and quantitative\nevaluation confirm the validity and effectiveness of our models. Compared to\nbaseline methods, our method is particularly effective in near-cold-start\nscenarios.\n", "rewritten_text": "\\boxed{Traditional word embeddings, trained on large corpora without domain-specific design, can perform poorly when dealing with heterogeneous domains. To address this, we introduce two novel mechanisms, domain indicator and domain attention, which incorporate domain knowledge into popular models like SG and CBOW. Our joint learning approach ensures that target domain words receive focused attention when training on a source domain corpus. Our methods are validated through qualitative and quantitative evaluations, and demonstrate particular effectiveness in near-cold-start scenarios, outperforming baseline methods.}"}, "2305.17975": {"original_text": "  Automated assembly of 3D fractures is essential in orthopedics, archaeology,\nand our daily life. This paper presents Jigsaw, a novel framework for\nassembling physically broken 3D objects from multiple pieces. Our approach\nleverages hierarchical features of global and local geometry to match and align\nthe fracture surfaces. Our framework consists of four components: (1) front-end\npoint feature extractor with attention layers, (2) surface segmentation to\nseparate fracture and original parts, (3) multi-parts matching to find\ncorrespondences among fracture surface points, and (4) robust global alignment\nto recover the global poses of the pieces. We show how to jointly learn\nsegmentation and matching and seamlessly integrate feature matching and\nrigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and\nachieve superior performance compared to state-of-the-art methods. Our method\nalso generalizes well to diverse fracture modes, objects, and unseen instances.\nTo the best of our knowledge, this is the first learning-based method designed\nspecifically for 3D fracture assembly over multiple pieces. Our code is\navailable at https://jiaxin-lu.github.io/Jigsaw/.\n", "rewritten_text": "\\boxed{\nThe assembly of 3D fractures is crucial in various fields, including orthopedics, archaeology, and everyday life. This paper introduces Jigsaw, a novel framework for reassembling physically broken 3D objects from multiple fragments. Jigsaw leverages hierarchical features of global and local geometry to match and align fracture surfaces. The framework consists of four components: a front-end point feature extractor with attention layers, surface segmentation to separate fracture and original parts, multi-part matching to find correspondences among fracture surface points, and robust global alignment to recover the global poses of the pieces. The paper demonstrates how to jointly learn segmentation and matching, and seamlessly integrate feature matching and rigidity constraints. Jigsaw outperforms state-of-the-art methods on the Breaking Bad dataset and generalizes well to diverse fracture modes, objects, and unseen instances. This is the first learning-based method specifically designed for 3D fracture assembly over multiple pieces, and the code is available at https://jiaxin-lu.github.io/Jigsaw/.\n}"}, "2312.08367": {"original_text": "  In this work, we propose an efficient Video-Language Alignment (ViLA)\nnetwork. Our ViLA model addresses both efficient frame sampling and effective\ncross-modal alignment in a unified way. In our ViLA network, we design a new\nlearnable text-guided Frame-Prompter together with a new cross-modal\ndistillation (QFormer-Distiller) module. Pre-trained large image-language\nmodels have shown promising results on problems such as visual question\nanswering (VQA). However, how to efficiently and effectively sample video\nframes when adapting pre-trained large image-language model to video-language\nalignment is still the major challenge. Compared with prior work, our ViLA\nmodel demonstrates the capability of selecting key frames with critical\ncontents, thus improving the video-language alignment accuracy while reducing\nthe inference latency +3.3% on NExT-QA Temporal with 3.0X speed up). Overall,\nour ViLA network outperforms the state-of-the-art methods on the video\nquestion-answering benchmarks: +4.6% on STAR Interaction, +2.2% on STAR average\nwith 3.0X speed up, ours 2-frames out-perform SeViLA 4-frames on the VLEP\ndataset with 4.2X speed-up. The code will be available at\nhttps://github.com/xijun-cs/ViLA.\n", "rewritten_text": "\\boxed{\nThis paper introduces an efficient Video-Language Alignment (ViLA) network that tackles both frame sampling and cross-modal alignment in a unified manner. The proposed ViLA model features a learnable text-guided Frame-Prompter and a cross-modal distillation module called QFormer-Distiller. While pre-trained image-language models have shown promise in tasks like visual question answering, adapting them to video-language alignment remains a significant challenge, particularly in terms of efficient frame sampling. Our ViLA model excels at selecting key frames with critical content, thereby improving alignment accuracy while reducing inference latency. Experimental results demonstrate the superiority of our ViLA network, outperforming state-of-the-art methods on video question-answering benchmarks, including NExT-QA Temporal, STAR Interaction, and STAR average, with significant speed-ups. The code for our ViLA model is available at https://github.com/xijun-cs/ViLA."}, "1809.10692": {"original_text": "  In this paper, we target the problem of fracture classification from clinical\nX-Ray images towards an automated Computer Aided Diagnosis (CAD) system.\nAlthough primarily dealing with an image classification problem, we argue that\nlocalizing the fracture in the image is crucial to make good class predictions.\nTherefore, we propose and thoroughly analyze several schemes for simultaneous\nfracture localization and classification. We show that using an auxiliary\nlocalization task, in general, improves the classification performance.\nMoreover, it is possible to avoid the need for additional localization\nannotations thanks to recent advancements in weakly-supervised deep learning\napproaches. Among such approaches, we investigate and adapt Spatial\nTransformers (ST), Self-Transfer Learning (STL), and localization from global\npooling layers. We provide a detailed quantitative and qualitative validation\non a dataset of 1347 femur fractures images and report high accuracy with\nregard to inter-expert correlation values reported in the literature. Our\ninvestigations show that i) lesion localization improves the classification\noutcome, ii) weakly-supervised methods improve baseline classification without\nany additional cost, iii) STL guides feature activations and boost performance.\nWe plan to make both the dataset and code available.\n", "rewritten_text": "\\boxed{\nThis paper focuses on developing an automated Computer Aided Diagnosis (CAD) system for fracture classification from clinical X-Ray images. We emphasize the importance of localizing the fracture in the image to achieve accurate classification. To address this, we propose and evaluate various schemes for simultaneous fracture localization and classification. Our results show that incorporating an auxiliary localization task generally enhances classification performance. Furthermore, we leverage recent advancements in weakly-supervised deep learning approaches to eliminate the need for additional localization annotations. We investigate and adapt Spatial Transformers, Self-Transfer Learning, and localization from global pooling layers, and validate our approach on a dataset of 1347 femur fracture images. Our findings indicate that lesion localization improves classification outcomes, weakly-supervised methods enhance baseline classification without additional cost, and Self-Transfer Learning guides feature activations and boosts performance. We plan to make the dataset and code publicly available.\n}"}, "2309.16039": {"original_text": "  We present a series of long-context LLMs that support effective context\nwindows of up to 32,768 tokens. Our model series are built through continual\npretraining from Llama 2 with longer training sequences and on a dataset where\nlong texts are upsampled. We perform extensive evaluation on language modeling,\nsynthetic context probing tasks, and a wide range of research benchmarks. On\nresearch benchmarks, our models achieve consistent improvements on most regular\ntasks and significant improvements on long-context tasks over Llama 2. Notably,\nwith a cost-effective instruction tuning procedure that does not require\nhuman-annotated long instruction data, the 70B variant can already surpass\ngpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.\nAlongside these results, we provide an in-depth analysis on the individual\ncomponents of our method. We delve into Llama's position encodings and discuss\nits limitation in modeling long dependencies. We also examine the impact of\nvarious design choices in the pretraining process, including the data mix and\nthe training curriculum of sequence lengths -- our ablation experiments suggest\nthat having abundant long texts in the pretrain dataset is not the key to\nachieving strong performance, and we empirically verify that long context\ncontinual pretraining is more efficient and similarly effective compared to\npretraining from scratch with long sequences.\n", "rewritten_text": "\\boxed{We introduce a series of large language models (LLMs) that can handle context windows of up to 32,768 tokens, achieving state-of-the-art results on various benchmarks. Our models are trained through continual pretraining from Llama 2, using longer sequences and a dataset with upsampled long texts. We evaluate our models on language modeling, synthetic context probing tasks, and a range of research benchmarks, showing consistent improvements over Llama 2. Notably, our 70B model outperforms gpt-3.5-turbo-16k on long-context tasks with a cost-effective instruction tuning procedure. We also provide an in-depth analysis of our method, discussing the limitations of Llama's position encodings and the impact of pretraining design choices, and demonstrating that continual pretraining is more efficient and effective than pretraining from scratch.}"}, "2403.02211": {"original_text": "  Popular methods usually use a degradation model in a supervised way to learn\na watermark removal model. However, it is true that reference images are\ndifficult to obtain in the real world, as well as collected images by cameras\nsuffer from noise. To overcome these drawbacks, we propose a perceptive\nself-supervised learning network for noisy image watermark removal (PSLNet) in\nthis paper. PSLNet depends on a parallel network to remove noise and\nwatermarks. The upper network uses task decomposition ideas to remove noise and\nwatermarks in sequence. The lower network utilizes the degradation model idea\nto simultaneously remove noise and watermarks. Specifically, mentioned paired\nwatermark images are obtained in a self supervised way, and paired noisy images\n(i.e., noisy and reference images) are obtained in a supervised way. To enhance\nthe clarity of obtained images, interacting two sub-networks and fusing\nobtained clean images are used to improve the effects of image watermark\nremoval in terms of structural information and pixel enhancement. Taking into\ntexture information account, a mixed loss uses obtained images and features to\nachieve a robust model of noisy image watermark removal. Comprehensive\nexperiments show that our proposed method is very effective in comparison with\npopular convolutional neural networks (CNNs) for noisy image watermark removal.\nCodes can be obtained at https://github.com/hellloxiaotian/PSLNet.\n", "rewritten_text": "\\boxed{\nTraditional methods for watermark removal rely on supervised learning with degradation models, but these approaches are limited by the difficulty of obtaining reference images and the presence of noise in real-world camera-captured images. To address these challenges, we propose a novel self-supervised learning network, PSLNet, which utilizes a parallel network architecture to remove noise and watermarks simultaneously. The upper network employs task decomposition to remove noise and watermarks sequentially, while the lower network leverages degradation models to remove both simultaneously. By combining the strengths of both sub-networks and incorporating texture information, our approach achieves state-of-the-art performance in noisy image watermark removal, outperforming popular CNN-based methods. The implementation of PSLNet is available at https://github.com/hellloxiaotian/PSLNet.\n}"}, "2104.09386": {"original_text": "  Mapping a single exposure low dynamic range (LDR) image into a high dynamic\nrange (HDR) is considered among the most strenuous image to image translation\ntasks due to exposure-related missing information. This study tackles the\nchallenges of single-shot LDR to HDR mapping by proposing a novel two-stage\ndeep network. Notably, our proposed method aims to reconstruct an HDR image\nwithout knowing hardware information, including camera response function (CRF)\nand exposure settings. Therefore, we aim to perform image enhancement task like\ndenoising, exposure correction, etc., in the first stage. Additionally, the\nsecond stage of our deep network learns tone mapping and bit-expansion from a\nconvex set of data samples. The qualitative and quantitative comparisons\ndemonstrate that the proposed method can outperform the existing LDR to HDR\nworks with a marginal difference. Apart from that, we collected an LDR image\ndataset incorporating different camera systems. The evaluation with our\ncollected real-world LDR images illustrates that the proposed method can\nreconstruct plausible HDR images without presenting any visual artefacts. Code\navailable: https://github. com/sharif-apu/twostageHDR_NTIRE21.\n", "rewritten_text": "\\boxed{\nConverting a single low dynamic range (LDR) image to a high dynamic range (HDR) image is a challenging task due to the lack of exposure-related information. To address this, we propose a novel two-stage deep network that can reconstruct an HDR image without requiring hardware information such as camera response function and exposure settings. The first stage of the network performs image enhancement tasks like denoising and exposure correction, while the second stage learns tone mapping and bit-expansion from a set of data samples. Our method outperforms existing LDR to HDR methods with minimal difference, and can reconstruct plausible HDR images from real-world LDR images without visual artifacts. We have also collected an LDR image dataset from different camera systems and made our code available at https://github.com/sharif-apu/twostageHDR_NTIRE21.\n}"}, "2409.09170": {"original_text": "  The diagnosis and treatment of individuals with communication disorders\noffers many opportunities for the application of speech technology, but\nresearch so far has not adequately considered: the diversity of conditions, the\nrole of pragmatic deficits, and the challenges of limited data. This paper\nexplores how a general-purpose model of perceived pragmatic similarity may\novercome these limitations. It explains how it might support several use cases\nfor clinicians and clients, and presents evidence that a simple model can\nprovide value, and in particular can capture utterance aspects that are\nrelevant to diagnoses of autism and specific language impairment.\n", "rewritten_text": "\\boxed{The use of speech technology in diagnosing and treating communication disorders has vast potential, but existing research has neglected three crucial aspects: the varied nature of these conditions, the impact of pragmatic deficits, and the scarcity of data. This paper proposes a general-purpose model of perceived pragmatic similarity to address these limitations, demonstrating its potential to support various clinical applications and provide valuable insights, particularly in diagnosing autism and specific language impairment.}"}, "2309.04462": {"original_text": "  Real-world application of chest X-ray abnormality classification requires\ndealing with several challenges: (i) limited training data; (ii) training and\nevaluation sets that are derived from different domains; and (iii) classes that\nappear during training may have partial overlap with classes of interest during\nevaluation. To address these challenges, we present an integrated framework\ncalled Generalized Cross-Domain Multi-Label Few-Shot Learning (GenCDML-FSL).\nThe framework supports overlap in classes during training and evaluation,\ncross-domain transfer, adopts meta-learning to learn using few training\nsamples, and assumes each chest X-ray image is either normal or associated with\none or more abnormalities. Furthermore, we propose Generalized Episodic\nTraining (GenET), a training strategy that equips models to operate with\nmultiple challenges observed in the GenCDML-FSL scenario. Comparisons with\nwell-established methods such as transfer learning, hybrid transfer learning,\nand multi-label meta-learning on multiple datasets show the superiority of our\napproach.\n", "rewritten_text": "\\boxed{\nClassifying chest X-ray abnormalities in real-world scenarios is hindered by several challenges, including limited training data, domain differences between training and evaluation sets, and partial overlap between classes. To overcome these challenges, we introduce a novel framework called Generalized Cross-Domain Multi-Label Few-Shot Learning (GenCDML-FSL), which enables class overlap, cross-domain transfer, and few-shot learning. Additionally, we propose a training strategy called Generalized Episodic Training (GenET) to equip models to handle multiple challenges. Our approach outperforms established methods, including transfer learning, hybrid transfer learning, and multi-label meta-learning, on multiple datasets."}, "1811.071": {"original_text": "  Few-shot deep learning is a topical challenge area for scaling visual\nrecognition to open ended growth of unseen new classes with limited labeled\nexamples. A promising approach is based on metric learning, which trains a deep\nembedding to support image similarity matching. Our insight is that effective\ngeneral purpose matching requires non-linear comparison of features at multiple\nabstraction levels. We thus propose a new deep comparison network comprised of\nembedding and relation modules that learn multiple non-linear distance metrics\nbased on different levels of features simultaneously. Furthermore, to reduce\nover-fitting and enable the use of deeper embeddings, we represent images as\ndistributions rather than vectors via learning parameterized Gaussian noise\nregularization. The resulting network achieves excellent performance on both\nminiImageNet and tieredImageNet.\n", "rewritten_text": "\\boxed{Deep learning faces a significant challenge in scaling visual recognition to accommodate an open-ended growth of new, unseen classes with limited labeled examples. One promising solution involves metric learning, which trains a deep embedding to enable image similarity matching. We propose a novel deep comparison network that learns multiple non-linear distance metrics simultaneously, based on features at different abstraction levels. Additionally, we introduce a regularization technique that represents images as distributions rather than vectors, reducing over-fitting and enabling the use of deeper embeddings. Our approach achieves outstanding performance on both miniImageNet and tieredImageNet benchmarks.}"}, "2306.07279": {"original_text": "  We introduce Cap3D, an automatic approach for generating descriptive text for\n3D objects. This approach utilizes pretrained models from image captioning,\nimage-text alignment, and LLM to consolidate captions from multiple views of a\n3D asset, completely side-stepping the time-consuming and costly process of\nmanual annotation. We apply Cap3D to the recently introduced large-scale 3D\ndataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted\nusing 41k human annotations from the same dataset, demonstrates that Cap3D\nsurpasses human-authored descriptions in terms of quality, cost, and speed.\nThrough effective prompt engineering, Cap3D rivals human performance in\ngenerating geometric descriptions on 17k collected annotations from the ABO\ndataset. Finally, we finetune Text-to-3D models on Cap3D and human captions,\nand show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E,\nand DreamFusion.\n", "rewritten_text": "\\boxed{We present Cap3D, a novel approach that automatically generates descriptive text for 3D objects by leveraging pre-trained models from image captioning, image-text alignment, and large language models. This method eliminates the need for manual annotation, which is time-consuming and costly. When applied to the large-scale 3D dataset Objaverse, Cap3D produces 660,000 3D-text pairs. Our evaluation, based on 41,000 human annotations, shows that Cap3D outperforms human-authored descriptions in terms of quality, cost, and speed. Furthermore, with effective prompt engineering, Cap3D rivals human performance in generating geometric descriptions on 17,000 collected annotations from the ABO dataset. Additionally, we fine-tune Text-to-3D models on Cap3D and human captions, demonstrating that Cap3D outperforms state-of-the-art models, including Point-E, Shape-E, and DreamFusion.}"}, "2311.08469": {"original_text": "  Language technologies that accurately model the dynamics of events must\nperform commonsense reasoning. Existing work evaluating commonsense reasoning\nfocuses on making inferences about common, everyday situations. To instead\ninvestigate the ability to model unusual, unexpected, and unlikely situations,\nwe explore the task of uncommonsense abductive reasoning. Given a piece of\ncontext with an unexpected outcome, this task requires reasoning abductively to\ngenerate an explanation that makes the unexpected outcome more likely in the\ncontext. To this end, we curate and release a new English language corpus\ncalled UNcommonsense. We characterize the performance differences between human\nexplainers and the best-performing large language models, finding that\nmodel-enhanced human-written explanations achieve the highest quality by\ntrading off between specificity and diversity. Finally, we experiment with\nseveral imitation learning algorithms to train open and accessible language\nmodels on this task. When compared with the vanilla supervised fine-tuning\napproach, these methods consistently reduce lose rates on both common and\nuncommonsense abductive reasoning judged by human evaluators.\n", "rewritten_text": "\\boxed{\nAccurate language technologies require commonsense reasoning, but existing research focuses on everyday situations. To address this limitation, we introduce \"uncommonsense abductive reasoning,\" which involves generating explanations for unexpected outcomes in unusual situations. We create a new corpus, UNcommonsense, and compare human and AI performance on this task. Our results show that combining human and AI explanations achieves the highest quality by balancing specificity and diversity. We also experiment with imitation learning algorithms to train accessible language models, which outperform traditional fine-tuning approaches in reducing error rates on both common and uncommonsense abductive reasoning tasks.\n}"}, "2305.1374": {"original_text": "  Tense inconsistency frequently occurs in machine translation. However, there\nare few criteria to assess the model's mastery of tense prediction from a\nlinguistic perspective. In this paper, we present a parallel tense test set,\ncontaining French-English 552 utterances. We also introduce a corresponding\nbenchmark, tense prediction accuracy. With the tense test set and the\nbenchmark, researchers are able to measure the tense consistency performance of\nmachine translation systems for the first time.\n", "rewritten_text": "\\boxed{Machine translation often struggles with inconsistent verb tenses, but there is a lack of standards to evaluate a model's ability to accurately predict tenses from a linguistic standpoint. To address this, we have developed a parallel test set of 552 French-English sentences and a corresponding benchmark to measure tense prediction accuracy, enabling researchers to assess the tense consistency of machine translation systems for the first time.}"}, "2410.19294": {"original_text": "  Vision-language models, such as CLIP, have shown impressive generalization\ncapacities when using appropriate text descriptions. While optimizing prompts\non downstream labeled data has proven effective in improving performance, these\nmethods entail labor costs for annotations and are limited by their quality.\nAdditionally, since CLIP is pre-trained on highly imbalanced Web-scale data, it\nsuffers from inherent label bias that leads to suboptimal performance. To\ntackle the above challenges, we propose a label-Free prompt distribution\nlearning and bias correction framework, dubbed as **Frolic**, which boosts\nzero-shot performance without the need for labeled data. Specifically, our\nFrolic learns distributions over prompt prototypes to capture diverse visual\nrepresentations and adaptively fuses these with the original CLIP through\nconfidence matching. This fused model is further enhanced by correcting label\nbias via a label-free logit adjustment. Notably, our method is not only\ntraining-free but also circumvents the necessity for hyper-parameter tuning.\nExtensive experimental results across 16 datasets demonstrate the efficacy of\nour approach, particularly outperforming the state-of-the-art by an average of\n$2.6\\%$ on 10 datasets with CLIP ViT-B/16 and achieving an average margin of\n$1.5\\%$ on ImageNet and its five distribution shifts with CLIP ViT-B/16. Codes\nare available in https://github.com/zhuhsingyuu/Frolic.\n", "rewritten_text": "\\boxed{\nVision-language models like CLIP have shown impressive generalization abilities when using suitable text descriptions. However, optimizing prompts using labeled data can be costly and limited by data quality. Moreover, CLIP's pre-training on imbalanced Web-scale data leads to label bias and suboptimal performance. To address these challenges, we introduce **Frolic**, a label-free framework that enhances zero-shot performance without requiring labeled data. Frolic learns prompt distributions to capture diverse visual representations and adaptively combines them with the original CLIP model through confidence matching. Additionally, it corrects label bias using a label-free logit adjustment. Our approach is training-free, eliminates hyper-parameter tuning, and outperforms state-of-the-art methods by an average of 2.6% on 10 datasets and 1.5% on ImageNet and its distribution shifts. The code is available at https://github.com/zhuhsingyuu/Frolic.\n}"}, "1809.01219": {"original_text": "  A novel graph-to-tree conversion mechanism called the deep-tree generation\n(DTG) algorithm is first proposed to predict text data represented by graphs.\nThe DTG method can generate a richer and more accurate representation for nodes\n(or vertices) in graphs. It adds flexibility in exploring the vertex\nneighborhood information to better reflect the second order proximity and\nhomophily equivalence in a graph. Then, a Deep-Tree Recursive Neural Network\n(DTRNN) method is presented and used to classify vertices that contains text\ndata in graphs. To demonstrate the effectiveness of the DTRNN method, we apply\nit to three real-world graph datasets and show that the DTRNN method\noutperforms several state-of-the-art benchmarking methods.\n", "rewritten_text": "\\boxed{A novel approach, called the Deep-Tree Generation (DTG) algorithm, is introduced to convert graph data into a tree structure, enabling more accurate representation of node features. This method allows for flexible exploration of node neighborhoods, capturing second-order proximity and homophily equivalence in graphs. A Deep-Tree Recursive Neural Network (DTRNN) is then proposed to classify text data in graphs, and its effectiveness is demonstrated through experiments on three real-world datasets, outperforming existing state-of-the-art methods.}"}, "2403.09636": {"original_text": "  Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.\n", "rewritten_text": "\\boxed{Transformers, the foundation of large language models, are hindered by inefficient generation due to the need to store a large cache of key-value representations, which grows linearly with input sequence length and batch size. To address this, we introduce Dynamic Memory Compression (DMC), a method that compresses the key-value cache in real-time during inference. Notably, the model learns to apply varying compression ratios across different heads and layers. By retrofitting pre-trained language models like Llama 2 with DMC, we achieve up to 7x faster throughput on an NVIDIA H100 GPU while maintaining original performance with up to 4x cache compression. DMC outperforms existing methods, such as grouped-query attention and key-value eviction policies, and can be combined with them for even greater gains. As a result, DMC can be seamlessly integrated into existing language models to accommodate longer contexts and larger batches within any given memory constraint.}"}, "2108.06536": {"original_text": "  We address the problem of generalized zero-shot semantic segmentation (GZS3)\npredicting pixel-wise semantic labels for seen and unseen classes. Most GZS3\nmethods adopt a generative approach that synthesizes visual features of unseen\nclasses from corresponding semantic ones (e.g., word2vec) to train novel\nclassifiers for both seen and unseen classes. Although generative methods show\ndecent performance, they have two limitations: (1) the visual features are\nbiased towards seen classes; (2) the classifier should be retrained whenever\nnovel unseen classes appear. We propose a discriminative approach to address\nthese limitations in a unified framework. To this end, we leverage visual and\nsemantic encoders to learn a joint embedding space, where the semantic encoder\ntransforms semantic features to semantic prototypes that act as centers for\nvisual features of corresponding classes. Specifically, we introduce\nboundary-aware regression (BAR) and semantic consistency (SC) losses to learn\ndiscriminative features. Our approach to exploiting the joint embedding space,\ntogether with BAR and SC terms, alleviates the seen bias problem. At test time,\nwe avoid the retraining process by exploiting semantic prototypes as a\nnearest-neighbor (NN) classifier. To further alleviate the bias problem, we\nalso propose an inference technique, dubbed Apollonius calibration (AC), that\nmodulates the decision boundary of the NN classifier to the Apollonius circle\nadaptively. Experimental results demonstrate the effectiveness of our\nframework, achieving a new state of the art on standard benchmarks.\n", "rewritten_text": "\\boxed{\nWe tackle the problem of generalized zero-shot semantic segmentation (GZS3), which involves predicting pixel-wise semantic labels for both seen and unseen classes. While most existing GZS3 methods use a generative approach to synthesize visual features of unseen classes from their semantic features, these methods have two major limitations: they are biased towards seen classes and require retraining whenever new unseen classes are introduced. To overcome these limitations, we propose a discriminative approach that leverages visual and semantic encoders to learn a joint embedding space. In this space, semantic features are transformed into prototypes that serve as centers for visual features of corresponding classes. We introduce two novel losses, boundary-aware regression (BAR) and semantic consistency (SC), to learn discriminative features. Our approach, combined with BAR and SC losses, mitigates the seen bias problem. At test time, we use semantic prototypes as a nearest-neighbor (NN) classifier, eliminating the need for retraining. Furthermore, we propose an Apollonius calibration (AC) technique that adaptively adjusts the decision boundary of the NN classifier to the Apollonius circle, thereby reducing bias. Our experimental results demonstrate the effectiveness of our framework, achieving state-of-the-art performance on standard benchmarks.\n}"}, "1907.10815": {"original_text": "  Improvements in data-capture and face modeling techniques have enabled us to\ncreate high-fidelity realistic face models. However, driving these realistic\nface models requires special input data, e.g. 3D meshes and unwrapped textures.\nAlso, these face models expect clean input data taken under controlled lab\nenvironments, which is very different from data collected in the wild. All\nthese constraints make it challenging to use the high-fidelity models in\ntracking for commodity cameras. In this paper, we propose a self-supervised\ndomain adaptation approach to enable the animation of high-fidelity face models\nfrom a commodity camera. Our approach first circumvents the requirement for\nspecial input data by training a new network that can directly drive a face\nmodel just from a single 2D image. Then, we overcome the domain mismatch\nbetween lab and uncontrolled environments by performing self-supervised domain\nadaptation based on \"consecutive frame texture consistency\" based on the\nassumption that the appearance of the face is consistent over consecutive\nframes, avoiding the necessity of modeling the new environment such as lighting\nor background. Experiments show that we are able to drive a high-fidelity face\nmodel to perform complex facial motion from a cellphone camera without\nrequiring any labeled data from the new domain.\n", "rewritten_text": "\\boxed{\nRecent advances in data capture and face modeling have led to the creation of highly realistic face models. However, these models require specific input data, such as 3D meshes and unwrapped textures, and are typically trained on clean data collected in controlled laboratory environments. This makes it challenging to use these models for facial tracking with commodity cameras, which capture data in uncontrolled environments. To address this, we propose a self-supervised domain adaptation approach that enables the animation of high-fidelity face models using a single 2D image from a commodity camera. Our approach involves training a new network to drive the face model and adapting to the new environment through consecutive frame texture consistency, which assumes that the face appearance remains consistent across consecutive frames. Our experiments demonstrate that we can successfully drive a high-fidelity face model to perform complex facial motions using a cellphone camera without requiring any labeled data from the new domain.\n}"}, "2311.10651": {"original_text": "  Analysis of the 3D Texture is indispensable for various tasks, such as\nretrieval, segmentation, classification, and inspection of sculptures, knitted\nfabrics, and biological tissues. A 3D texture is a locally repeated surface\nvariation independent of the surface's overall shape and can be determined\nusing the local neighborhood and its characteristics. Existing techniques\ntypically employ computer vision techniques that analyze a 3D mesh globally,\nderive features, and then utilize the obtained features for retrieval or\nclassification. Several traditional and learning-based methods exist in the\nliterature, however, only a few are on 3D texture, and nothing yet, to the best\nof our knowledge, on the unsupervised schemes. This paper presents an original\nframework for the unsupervised segmentation of the 3D texture on the mesh\nmanifold. We approach this problem as binary surface segmentation, partitioning\nthe mesh surface into textured and non-textured regions without prior\nannotation. We devise a mutual transformer-based system comprising a label\ngenerator and a cleaner. The two models take geometric image representations of\nthe surface mesh facets and label them as texture or non-texture across an\niterative mutual learning scheme. Extensive experiments on three publicly\navailable datasets with diverse texture patterns demonstrate that the proposed\nframework outperforms standard and SOTA unsupervised techniques and competes\nreasonably with supervised methods.\n", "rewritten_text": "\\boxed{\nAnalyzing 3D textures is crucial for various applications, including retrieval, segmentation, classification, and inspection of sculptures, fabrics, and biological tissues. A 3D texture refers to a repeated surface variation that is independent of the overall shape and can be determined by examining the local neighborhood and its characteristics. While existing techniques typically analyze 3D meshes globally and extract features for retrieval or classification, few methods focus specifically on 3D textures, and none have explored unsupervised schemes. This paper proposes a novel framework for unsupervised 3D texture segmentation on mesh manifolds, which involves partitioning the mesh surface into textured and non-textured regions without prior annotation. Our approach uses a mutual transformer-based system consisting of a label generator and a cleaner, which iteratively learn to label surface mesh facets as texture or non-texture. Experimental results on three publicly available datasets with diverse texture patterns demonstrate the superiority of our framework over standard and state-of-the-art unsupervised techniques, and its competitiveness with supervised methods.\n}"}, "1906.05416": {"original_text": "  We introduce a novel method of generating synthetic question answering\ncorpora by combining models of question generation and answer extraction, and\nby filtering the results to ensure roundtrip consistency. By pretraining on the\nresulting corpora we obtain significant improvements on SQuAD2 and NQ,\nestablishing a new state-of-the-art on the latter. Our synthetic data\ngeneration models, for both question generation and answer extraction, can be\nfully reproduced by finetuning a publicly available BERT model on the\nextractive subsets of SQuAD2 and NQ. We also describe a more powerful variant\nthat does full sequence-to-sequence pretraining for question generation,\nobtaining exact match and F1 at less than 0.1% and 0.4% from human performance\non SQuAD2.\n", "rewritten_text": "\\boxed{We propose a new approach to create synthetic question answering datasets by combining question generation and answer extraction models, and then filtering the results to ensure consistency. By pre-training on these datasets, we achieve significant improvements on SQuAD2 and NQ benchmarks, setting a new state-of-the-art on NQ. Our models can be easily replicated by fine-tuning a publicly available BERT model on the extractive subsets of SQuAD2 and NQ. Furthermore, we develop a more advanced variant that uses full sequence-to-sequence pre-training for question generation, achieving near-human performance on SQuAD2 with exact match and F1 scores within 0.1% and 0.4% of human performance.}"}, "2404.02393": {"original_text": "  While multilingual machine translation (MNMT) systems hold substantial\npromise, they also have security vulnerabilities. Our research highlights that\nMNMT systems can be susceptible to a particularly devious style of backdoor\nattack, whereby an attacker injects poisoned data into a low-resource language\npair to cause malicious translations in other languages, including\nhigh-resource languages. Our experimental results reveal that injecting less\nthan 0.01% poisoned data into a low-resource language pair can achieve an\naverage 20% attack success rate in attacking high-resource language pairs. This\ntype of attack is of particular concern, given the larger attack surface of\nlanguages inherent to low-resource settings. Our aim is to bring attention to\nthese vulnerabilities within MNMT systems with the hope of encouraging the\ncommunity to address security concerns in machine translation, especially in\nthe context of low-resource languages.\n", "rewritten_text": "\\boxed{Multilingual machine translation systems, despite their potential, have a significant weakness: they can be exploited through a specific type of backdoor attack. An attacker can inject a small amount of malicious data into a lesser-used language pair, causing the system to produce harmful translations in other languages, including widely spoken ones. Our experiments show that even a tiny amount of poisoned data (less than 0.01%) can lead to a 20% success rate in attacking high-resource language pairs. This vulnerability is particularly concerning in low-resource language settings, where the attack surface is larger. We aim to raise awareness about these security risks in machine translation, especially for low-resource languages, and encourage the community to address them.}"}, "2104.00556": {"original_text": "  Two-view structure-from-motion (SfM) is the cornerstone of 3D reconstruction\nand visual SLAM. Existing deep learning-based approaches formulate the problem\nby either recovering absolute pose scales from two consecutive frames or\npredicting a depth map from a single image, both of which are ill-posed\nproblems. In contrast, we propose to revisit the problem of deep two-view SfM\nby leveraging the well-posedness of the classic pipeline. Our method consists\nof 1) an optical flow estimation network that predicts dense correspondences\nbetween two frames; 2) a normalized pose estimation module that computes\nrelative camera poses from the 2D optical flow correspondences, and 3) a\nscale-invariant depth estimation network that leverages epipolar geometry to\nreduce the search space, refine the dense correspondences, and estimate\nrelative depth maps. Extensive experiments show that our method outperforms all\nstate-of-the-art two-view SfM methods by a clear margin on KITTI depth, KITTI\nVO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth\nestimation.\n", "rewritten_text": "\\boxed{Two-view structure-from-motion (SfM) is a fundamental component of 3D reconstruction and visual SLAM. While existing deep learning-based approaches have limitations, our proposed method revisits the problem by building upon the well-established classic pipeline. Our approach consists of three key components: an optical flow estimation network to predict dense correspondences between two frames, a normalized pose estimation module to compute relative camera poses, and a scale-invariant depth estimation network that leverages epipolar geometry to refine correspondences and estimate relative depth maps. Our method achieves state-of-the-art performance on multiple datasets, outperforming existing two-view SfM methods in both relative pose and depth estimation.}"}, "2309.03903": {"original_text": "  Training data for video segmentation are expensive to annotate. This impedes\nextensions of end-to-end algorithms to new video segmentation tasks, especially\nin large-vocabulary settings. To 'track anything' without training on video\ndata for every individual task, we develop a decoupled video segmentation\napproach (DEVA), composed of task-specific image-level segmentation and\nclass/task-agnostic bi-directional temporal propagation. Due to this design, we\nonly need an image-level model for the target task (which is cheaper to train)\nand a universal temporal propagation model which is trained once and\ngeneralizes across tasks. To effectively combine these two modules, we use\nbi-directional propagation for (semi-)online fusion of segmentation hypotheses\nfrom different frames to generate a coherent segmentation. We show that this\ndecoupled formulation compares favorably to end-to-end approaches in several\ndata-scarce tasks including large-vocabulary video panoptic segmentation,\nopen-world video segmentation, referring video segmentation, and unsupervised\nvideo object segmentation. Code is available at:\nhttps://hkchengrex.github.io/Tracking-Anything-with-DEVA\n", "rewritten_text": "\\boxed{Annotating training data for video segmentation is a costly process, which hinders the expansion of end-to-end algorithms to new tasks, especially when dealing with a large number of categories. To overcome this limitation, we propose a decoupled video segmentation approach called DEVA, which consists of two modules: task-specific image-level segmentation and class-agnostic bi-directional temporal propagation. This design allows us to train an image-level model for the target task and a universal temporal propagation model that can be applied to various tasks, eliminating the need for task-specific video data. By combining these modules using bi-directional propagation, we can generate coherent segmentations by fusing hypotheses from different frames. Our approach outperforms end-to-end methods in several data-scarce tasks, including video panoptic segmentation, open-world video segmentation, referring video segmentation, and unsupervised video object segmentation. The code for DEVA is available at https://hkchengrex.github.io/Tracking-Anything-with-DEVA.}"}, "2312.08869": {"original_text": "  We are living in a world surrounded by diverse and \"smart\" devices with rich\nmodalities of sensing ability. Conveniently capturing the interactions between\nus humans and these objects remains far-reaching. In this paper, we present\nI'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the\nhuman and object in a novel setting: using a minimal amount of RGB camera and\nobject-mounted Inertial Measurement Unit (IMU). It combines general motion\ninference and category-aware refinement. For the former, we introduce a\nholistic human-object tracking method to fuse the IMU signals and the RGB\nstream and progressively recover the human motions and subsequently the\ncompanion object motions. For the latter, we tailor a category-aware motion\ndiffusion model, which is conditioned on both the raw IMU observations and the\nresults from the previous stage under over-parameterization representation. It\nsignificantly refines the initial results and generates vivid body, hand, and\nobject motions. Moreover, we contribute a large dataset with ground truth human\nand object motions, dense RGB inputs, and rich object-mounted IMU measurements.\nExtensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid\ncapture setting. Our dataset and code will be released to the community.\n", "rewritten_text": "\\boxed{We live in a world where we interact with various \"smart\" devices that can sense our actions in different ways. However, capturing these interactions between humans and objects remains a significant challenge. This paper introduces I'm-HOI, a novel approach that uses a single RGB camera and an object-mounted sensor to accurately capture the 3D motions of both humans and objects. Our method combines two stages: first, it tracks the human and object motions by fusing the sensor data and camera stream, and then refines the results using a category-aware motion model. We also provide a large dataset with ground truth motions, dense RGB inputs, and sensor measurements. Our experiments show that I'm-HOI is effective in capturing human-object interactions, and we will make our dataset and code publicly available.}"}, "1810.04864": {"original_text": "  We present a comparison of word-based and character-based\nsequence-to-sequence models for data-to-text natural language generation, which\ngenerate natural language descriptions for structured inputs. On the datasets\nof two recent generation challenges, our models achieve comparable or better\nautomatic evaluation results than the best challenge submissions. Subsequent\ndetailed statistical and human analyses shed light on the differences between\nthe two input representations and the diversity of the generated texts. In a\ncontrolled experiment with synthetic training data generated from templates, we\ndemonstrate the ability of neural models to learn novel combinations of the\ntemplates and thereby generalize beyond the linguistic structures they were\ntrained on.\n", "rewritten_text": "\\boxed{This paper compares word-based and character-based sequence-to-sequence models for natural language generation, which produce descriptions from structured data. Our models outperform or match the top submissions in two recent challenges, as measured by automatic evaluation metrics. Further analysis, both statistical and human, reveals differences between the two input representations and the diversity of generated texts. Additionally, we show that neural models can generalize beyond their training data by learning to combine templates in novel ways, as demonstrated through a controlled experiment with synthetic training data.}"}, "2405.16116": {"original_text": "  Scene Graph Generation (SGG) can extract abstract semantic relations between\nentities in images as graph representations. This task holds strong promises\nfor other downstream tasks such as the embodied cognition of an autonomous\nagent. However, to power such applications, SGG needs to solve the gap of\nreal-time latency. In this work, we propose to investigate the bottlenecks of\ncurrent approaches for real-time constraint applications. Then, we propose a\nsimple yet effective implementation of a real-time SGG approach using YOLOV8 as\nan object detection backbone. Our implementation is the first to obtain more\nthan 48 FPS for the task with no loss of accuracy, successfully outperforming\nany other lightweight approaches. Our code is freely available at\nhttps://github.com/Maelic/SGG-Benchmark.\n", "rewritten_text": "\\boxed{Scene Graph Generation (SGG) has the potential to extract abstract semantic relationships between entities in images as graph representations, which can be beneficial for applications such as embodied cognition in autonomous agents. However, to achieve real-time performance, SGG needs to overcome the latency issue. This work identifies the bottlenecks in current approaches and proposes a simple yet effective real-time SGG method using YOLOV8 as the object detection backbone, achieving over 48 FPS without sacrificing accuracy and outperforming other lightweight approaches. The implementation is available at https://github.com/Maelic/SGG-Benchmark.}"}, "2012.08549": {"original_text": "  Voice Assistants such as Alexa, Siri, and Google Assistant typically use a\ntwo-stage Spoken Language Understanding pipeline; first, an Automatic Speech\nRecognition (ASR) component to process customer speech and generate text\ntranscriptions, followed by a Natural Language Understanding (NLU) component to\nmap transcriptions to an actionable hypothesis. An end-to-end (E2E) system that\ngoes directly from speech to a hypothesis is a more attractive option. These\nsystems were shown to be smaller, faster, and better optimized. However, they\nrequire massive amounts of end-to-end training data and in addition, don't take\nadvantage of the already available ASR and NLU training data.\n  In this work, we propose an E2E system that is designed to jointly train on\nmultiple speech-to-text tasks, such as ASR (speech-transcription) and SLU\n(speech-hypothesis), and text-to-text tasks, such as NLU (text-hypothesis). We\ncall this the Audio-Text All-Task (AT-AT) Model and we show that it beats the\nperformance of E2E models trained on individual tasks, especially ones trained\non limited data. We show this result on an internal music dataset and two\npublic datasets, FluentSpeech and SNIPS Audio, where we achieve\nstate-of-the-art results. Since our model can process both speech and text\ninput sequences and learn to predict a target sequence, it also allows us to do\nzero-shot E2E SLU by training on only text-hypothesis data (without any speech)\nfrom a new domain. We evaluate this ability of our model on the Facebook TOP\ndataset and set a new benchmark for zeroshot E2E performance. We will soon\nrelease the audio data collected for the TOP dataset for future research.\n", "rewritten_text": "\\boxed{\nTraditional voice assistants like Alexa, Siri, and Google Assistant use a two-step process to understand spoken language: first, they recognize speech and transcribe it into text, and then they analyze the text to determine the user's intent. However, end-to-end systems that can directly process speech and generate a response are more desirable because they are smaller, faster, and more efficient. \n\nThe challenge with end-to-end systems is that they require massive amounts of training data and don't leverage existing speech recognition and language understanding data. To address this, we propose a new approach called the Audio-Text All-Task (AT-AT) Model, which can be trained on multiple tasks simultaneously, including speech-to-text and text-to-text tasks. Our model outperforms traditional end-to-end models, especially when trained on limited data, and achieves state-of-the-art results on several datasets. Additionally, our model can process both speech and text inputs and learn to predict a target sequence, enabling zero-shot end-to-end spoken language understanding without requiring any speech data from a new domain. We demonstrate this capability on the Facebook TOP dataset and set a new benchmark for zero-shot end-to-end performance.\n}"}, "2207.09059": {"original_text": "  Few-shot open-set recognition aims to classify both seen and novel images\ngiven only limited training data of seen classes. The challenge of this task is\nthat the model is required not only to learn a discriminative classifier to\nclassify the pre-defined classes with few training data but also to reject\ninputs from unseen classes that never appear at training time. In this paper,\nwe propose to solve the problem from two novel aspects. First, instead of\nlearning the decision boundaries between seen classes, as is done in standard\nclose-set classification, we reserve space for unseen classes, such that images\nlocated in these areas are recognized as the unseen classes. Second, to\neffectively learn such decision boundaries, we propose to utilize the\nbackground features from seen classes. As these background regions do not\nsignificantly contribute to the decision of close-set classification, it is\nnatural to use them as the pseudo unseen classes for classifier learning. Our\nextensive experiments show that our proposed method not only outperforms\nmultiple baselines but also sets new state-of-the-art results on three popular\nbenchmarks, namely tieredImageNet, miniImageNet, and Caltech-USCD\nBirds-200-2011 (CUB).\n", "rewritten_text": "\\boxed{Few-shot open-set recognition is a challenging task that requires a model to classify both known and unknown images with limited training data. To tackle this, we propose a novel approach that focuses on reserving space for unknown classes and utilizing background features from known classes to learn decision boundaries. Our method outperforms multiple baselines and achieves state-of-the-art results on three popular benchmarks: tieredImageNet, miniImageNet, and Caltech-USCD Birds-200-2011 (CUB).}"}, "1710.04943": {"original_text": "  In this paper, we report on our efforts for using Deep Learning for\nclassifying artifacts and their features in digital visuals as a part of the\nNeoclassica framework. It was conceived to provide scholars with new methods\nfor analyzing and classifying artifacts and aesthetic forms from the era of\nClassicism. The framework accommodates both traditional knowledge\nrepresentation as a formal ontology and data-driven knowledge discovery, where\ncultural patterns will be identified by means of algorithms in statistical\nanalysis and machine learning. We created a Deep Learning approach trained on\nphotographs to classify the objects inside these photographs. In a next step,\nwe will apply a different Deep Learning approach. It is capable of locating\nmultiple objects inside an image and classifying them with a high accuracy.\n", "rewritten_text": "\\boxed{This paper presents our research on utilizing Deep Learning to classify artifacts and their features in digital images within the Neoclassica framework. This framework aims to provide scholars with innovative methods for analyzing and categorizing artifacts and aesthetic forms from the Classical era, combining traditional knowledge representation with data-driven discovery. We developed a Deep Learning approach to classify objects in photographs, and our next step is to apply a more advanced approach that can accurately locate and classify multiple objects within an image.}"}, "2111.07239": {"original_text": "  Object detection has achieved promising performance on clean datasets, but\nhow to achieve better tradeoff between the adversarial robustness and clean\nprecision is still under-explored. Adversarial training is the mainstream\nmethod to improve robustness, but most of the works will sacrifice clean\nprecision to gain robustness than standard training. In this paper, we propose\nUnified Decoupled Feature Alignment (UDFA), a novel fine-tuning paradigm which\nachieves better performance than existing methods, by fully exploring the\ncombination between self-knowledge distillation and adversarial training for\nobject detection. We first use decoupled fore/back-ground features to construct\nself-knowledge distillation branch between clean feature representation from\npretrained detector (served as teacher) and adversarial feature representation\nfrom student detector. Then we explore the self-knowledge distillation from a\nnew angle by decoupling original branch into a self-supervised learning branch\nand a new self-knowledge distillation branch. With extensive experiments on the\nPASCAL-VOC and MS-COCO benchmarks, the evaluation results show that UDFA can\nsurpass the standard training and state-of-the-art adversarial training methods\nfor object detection. For example, compared with teacher detector, our approach\non GFLV2 with ResNet-50 improves clean precision by 2.2 AP on PASCAL-VOC;\ncompared with SOTA adversarial training methods, our approach improves clean\nprecision by 1.6 AP, while improving adversarial robustness by 0.5 AP. Our code\nwill be available at https://github.com/grispeut/udfa.\n", "rewritten_text": "\\boxed{\nObject detection has made significant progress on clean datasets, but finding a balance between adversarial robustness and clean precision remains a challenge. While adversarial training is a popular method to improve robustness, it often comes at the cost of clean precision. This paper proposes a novel fine-tuning approach called Unified Decoupled Feature Alignment (UDFA), which combines self-knowledge distillation and adversarial training to achieve better performance in object detection. By decoupling features and using a self-supervised learning branch, UDFA surpasses standard training and state-of-the-art adversarial training methods. Experimental results on PASCAL-VOC and MS-COCO benchmarks show that UDFA improves clean precision by up to 2.2 AP and adversarial robustness by 0.5 AP. The code is available at https://github.com/grispeut/udfa."}, "2304.05694": {"original_text": "  Self-attention modules have demonstrated remarkable capabilities in capturing\nlong-range relationships and improving the performance of point cloud tasks.\nHowever, point cloud objects are typically characterized by complex,\ndisordered, and non-Euclidean spatial structures with multiple scales, and\ntheir behavior is often dynamic and unpredictable. The current self-attention\nmodules mostly rely on dot product multiplication and dimension alignment among\nquery-key-value features, which cannot adequately capture the multi-scale\nnon-Euclidean structures of point cloud objects. To address these problems,\nthis paper proposes a self-attention plug-in module with its variants,\nMulti-scale Geometry-aware Transformer (MGT). MGT processes point cloud data\nwith multi-scale local and global geometric information in the following three\naspects. At first, the MGT divides point cloud data into patches with multiple\nscales. Secondly, a local feature extractor based on sphere mapping is proposed\nto explore the geometry inner each patch and generate a fixed-length\nrepresentation for each patch. Thirdly, the fixed-length representations are\nfed into a novel geodesic-based self-attention to capture the global\nnon-Euclidean geometry between patches. Finally, all the modules are integrated\ninto the framework of MGT with an end-to-end training scheme. Experimental\nresults demonstrate that the MGT vastly increases the capability of capturing\nmulti-scale geometry using the self-attention mechanism and achieves strong\ncompetitive performance on mainstream point cloud benchmarks.\n", "rewritten_text": "\\boxed{\nSelf-attention modules have shown impressive capabilities in modeling long-range relationships, leading to improved performance in point cloud tasks. However, point cloud objects often exhibit complex, disordered, and non-Euclidean spatial structures with multiple scales, and their behavior can be dynamic and unpredictable. Current self-attention modules, which rely on dot product multiplication and dimension alignment, struggle to capture these multi-scale non-Euclidean structures. To address this limitation, we propose the Multi-scale Geometry-aware Transformer (MGT), a self-attention plug-in module that incorporates multi-scale local and global geometric information. MGT processes point cloud data in three stages: dividing data into patches with multiple scales, extracting local features using sphere mapping, and capturing global non-Euclidean geometry between patches using a novel geodesic-based self-attention. Our experimental results demonstrate that MGT significantly enhances the capability of capturing multi-scale geometry and achieves competitive performance on mainstream point cloud benchmarks.\n}"}, "2108.07708": {"original_text": "  Can language models learn grounded representations from text distribution\nalone? This question is both central and recurrent in natural language\nprocessing; authors generally agree that grounding requires more than textual\ndistribution. We propose to experimentally test this claim: if any two words\nhave different meanings and yet cannot be distinguished from distribution\nalone, then grounding is out of the reach of text-based models. To that end, we\npresent early work on an online game for the collection of human judgments on\nthe distributional similarity of word pairs in five languages. We further\nreport early results of our data collection campaign.\n", "rewritten_text": "\\boxed{Can language models learn meaningful representations solely from text distribution? While many experts believe that additional information is necessary for grounding, we aim to test this claim experimentally. Specifically, if two words with different meanings cannot be distinguished based on their distribution alone, then text-based models are insufficient for grounding. To investigate this, we have developed an online game to collect human judgments on the similarity of word pairs in five languages and present our preliminary findings from this data collection effort.}"}, "2302.03194": {"original_text": "  We propose two methods to make unsupervised domain adaptation (UDA) more\nparameter efficient using adapters, small bottleneck layers interspersed with\nevery layer of the large-scale pre-trained language model (PLM). The first\nmethod deconstructs UDA into a two-step process: first by adding a domain\nadapter to learn domain-invariant information and then by adding a task adapter\nthat uses domain-invariant information to learn task representations in the\nsource domain. The second method jointly learns a supervised classifier while\nreducing the divergence measure. Compared to strong baselines, our simple\nmethods perform well in natural language inference (MNLI) and the cross-domain\nsentiment classification task. We even outperform unsupervised domain\nadaptation methods such as DANN and DSN in sentiment classification, and we are\nwithin 0.85% F1 for natural language inference task, by fine-tuning only a\nfraction of the full model parameters. We release our code at\nhttps://github.com/declare-lab/domadapter\n", "rewritten_text": "\\boxed{We introduce two novel approaches to improve the parameter efficiency of unsupervised domain adaptation (UDA) by leveraging adapters, which are small bottleneck layers inserted between every layer of a large-scale pre-trained language model (PLM). Our first approach involves a two-step process, where a domain adapter is added to learn domain-invariant information, followed by a task adapter that utilizes this information to learn task representations in the source domain. The second approach jointly learns a supervised classifier while minimizing the divergence measure. Our simple methods demonstrate strong performance in natural language inference (MNLI) and cross-domain sentiment classification tasks, outperforming existing UDA methods such as DANN and DSN in sentiment classification and achieving competitive results in natural language inference, all while fine-tuning only a fraction of the full model parameters. Our code is available at https://github.com/declare-lab/domadapter.}"}, "2103.04059": {"original_text": "  Few-shot class incremental learning (FSCIL) portrays the problem of learning\nnew concepts gradually, where only a few examples per concept are available to\nthe learner. Due to the limited number of examples for training, the techniques\ndeveloped for standard incremental learning cannot be applied verbatim to\nFSCIL. In this work, we introduce a distillation algorithm to address the\nproblem of FSCIL and propose to make use of semantic information during\ntraining. To this end, we make use of word embeddings as semantic information\nwhich is cheap to obtain and which facilitate the distillation process.\nFurthermore, we propose a method based on an attention mechanism on multiple\nparallel embeddings of visual data to align visual and semantic vectors, which\nreduces issues related to catastrophic forgetting. Via experiments on\nMiniImageNet, CUB200, and CIFAR100 dataset, we establish new state-of-the-art\nresults by outperforming existing approaches.\n", "rewritten_text": "\\boxed{Few-shot class incremental learning (FSCIL) involves learning new concepts with limited training examples. Traditional incremental learning methods are not directly applicable to FSCIL due to the scarcity of training data. To tackle this challenge, we propose a novel distillation algorithm that leverages semantic information, specifically word embeddings, to facilitate learning. Additionally, we introduce an attention-based method that aligns visual and semantic vectors using multiple parallel embeddings, mitigating the issue of catastrophic forgetting. Our approach achieves state-of-the-art results on MiniImageNet, CUB200, and CIFAR100 datasets, outperforming existing methods.}"}, "1710.07395": {"original_text": "  In the wake of a polarizing election, the cyber world is laden with hate\nspeech. Context accompanying a hate speech text is useful for identifying hate\nspeech, which however has been largely overlooked in existing datasets and hate\nspeech detection models. In this paper, we provide an annotated corpus of hate\nspeech with context information well kept. Then we propose two types of hate\nspeech detection models that incorporate context information, a logistic\nregression model with context features and a neural network model with learning\ncomponents for context. Our evaluation shows that both models outperform a\nstrong baseline by around 3% to 4% in F1 score and combining these two models\nfurther improve the performance by another 7% in F1 score.\n", "rewritten_text": "\\boxed{Following a divisive election, the online sphere is plagued by hate speech. While context is crucial in identifying hate speech, it has been largely neglected in existing datasets and detection models. This paper addresses this gap by providing a annotated corpus of hate speech with preserved context information. We also propose two hate speech detection models that leverage context information: a logistic regression model incorporating context features and a neural network model with context-learning components. Our evaluation reveals that both models surpass a strong baseline by 3-4% in F1 score, and combining them yields a further 7% improvement in F1 score.}"}, "2401.17597": {"original_text": "  Multi-turn dialogues are characterized by their extended length and the\npresence of turn-taking conversations. Traditional language models often\noverlook the distinct features of these dialogues by treating them as regular\ntext. In this paper, we propose a speaker-enhanced pre-training method for long\ndialogue summarization, which leverages the inherent structure of multiple-turn\ndialogues. To support our study, we curate a diverse dataset that includes\ntranscripts from real-world scenarios, movie or TV show transcripts, and\ndialogues generated by a Large Language Model. We then perform a pre-training,\nwhich encompasses the detection of speaker changes, and masked utterance\ngeneration. Experimental results of fine-tuned models demonstrate that our\nmodel achieves state-of-the-art performance on downstream benchmarks with long\ncontext, surpassing baseline models and highlighting the effectiveness of our\napproach. Our findings highlight the importance of curating pre-training\ndatasets that exhibit diversity and variations in length distribution to ensure\neffective alignment with downstream datasets.\n", "rewritten_text": "\\boxed{Traditional language models often fail to capture the unique characteristics of multi-turn dialogues, which are marked by their length and turn-taking conversations. To address this, we propose a novel pre-training method that takes into account the inherent structure of these dialogues, leveraging speaker information and masked utterance generation. Our approach is supported by a diverse dataset comprising real-world transcripts, movie/TV show scripts, and dialogues generated by a Large Language Model. Experimental results show that our fine-tuned model outperforms baseline models on downstream benchmarks with long context, demonstrating the importance of curating diverse pre-training datasets that align with downstream datasets.}"}, "2309.01036": {"original_text": "  Spatial transcriptomics is an emerging technology that aligns histopathology\nimages with spatially resolved gene expression profiling. It holds the\npotential for understanding many diseases but faces significant bottlenecks\nsuch as specialized equipment and domain expertise. In this work, we present\nSEPAL, a new model for predicting genetic profiles from visual tissue\nappearance. Our method exploits the biological biases of the problem by\ndirectly supervising relative differences with respect to mean expression, and\nleverages local visual context at every coordinate to make predictions using a\ngraph neural network. This approach closes the gap between complete locality\nand complete globality in current methods. In addition, we propose a novel\nbenchmark that aims to better define the task by following current best\npractices in transcriptomics and restricting the prediction variables to only\nthose with clear spatial patterns. Our extensive evaluation in two different\nhuman breast cancer datasets indicates that SEPAL outperforms previous\nstate-of-the-art methods and other mechanisms of including spatial context.\n", "rewritten_text": "\\boxed{Spatial transcriptomics, a cutting-edge technology that combines histopathology images with spatially resolved gene expression profiling, has the potential to uncover the mysteries of various diseases. However, it is hindered by the need for specialized equipment and expertise. To address this, we introduce SEPAL, a novel model that predicts genetic profiles based on visual tissue appearance. By leveraging local visual context and biological biases, SEPAL outperforms existing methods and bridges the gap between local and global approaches. We also propose a new benchmark that adheres to best practices in transcriptomics and focuses on predicting variables with clear spatial patterns. Our evaluation on two human breast cancer datasets demonstrates SEPAL's superiority over previous state-of-the-art methods.}"}, "2011.0212": {"original_text": "  Diabetic retinopathy (DR) is one of the leading causes of blindness. However,\nno specific symptoms of early DR lead to a delayed diagnosis, which results in\ndisease progression in patients. To determine the disease severity levels,\nophthalmologists need to focus on the discriminative parts of the fundus\nimages. In recent years, deep learning has achieved great success in medical\nimage analysis. However, most works directly employ algorithms based on\nconvolutional neural networks (CNNs), which ignore the fact that the difference\namong classes is subtle and gradual. Hence, we consider automatic image grading\nof DR as a fine-grained classification task, and construct a bilinear model to\nidentify the pathologically discriminative areas. In order to leverage the\nordinal information among classes, we use an ordinal regression method to\nobtain the soft labels. In addition, other than only using a categorical loss\nto train our network, we also introduce the metric loss to learn a more\ndiscriminative feature space. Experimental results demonstrate the superior\nperformance of the proposed method on two public IDRiD and DeepDR datasets.\n", "rewritten_text": "\\boxed{Diabetic retinopathy, a leading cause of blindness, often goes undiagnosed in its early stages due to a lack of distinct symptoms, leading to disease progression. To accurately diagnose and determine disease severity, ophthalmologists must focus on specific areas of fundus images. Recent advancements in deep learning have shown promise in medical image analysis, but most approaches rely on convolutional neural networks (CNNs) that overlook the subtle differences between classes. To address this, we propose a bilinear model that identifies pathologically discriminative areas and treats diabetic retinopathy image grading as a fine-grained classification task. By incorporating ordinal regression and metric loss, our method outperforms others on two public datasets, IDRiD and DeepDR.}"}, "1905.02462": {"original_text": "  Recently, image super-resolution has been widely studied and achieved\nsignificant progress by leveraging the power of deep convolutional neural\nnetworks. However, there has been limited advancement in video super-resolution\n(VSR) due to the complex temporal patterns in videos. In this paper, we\ninvestigate how to adapt state-of-the-art methods of image super-resolution for\nvideo super-resolution. The proposed adapting method is straightforward. The\ninformation among successive frames is well exploited, while the overhead on\nthe original image super-resolution method is negligible. Furthermore, we\npropose a learning-based method to ensemble the outputs from multiple\nsuper-resolution models. Our methods show superior performance and rank second\nin the NTIRE2019 Video Super-Resolution Challenge Track 1.\n", "rewritten_text": "\\boxed{Deep learning-based image super-resolution has seen significant advancements, but video super-resolution (VSR) has lagged behind due to the complexity of temporal patterns in videos. This paper explores adapting state-of-the-art image super-resolution methods for VSR, proposing a straightforward approach that effectively utilizes information between consecutive frames without significantly adding to the original method's computational overhead. Additionally, a learning-based method is introduced to combine outputs from multiple super-resolution models, resulting in superior performance and a second-place ranking in the NTIRE2019 Video Super-Resolution Challenge Track 1.}"}, "2312.14157": {"original_text": "  3D hand tracking from a monocular video is a very challenging problem due to\nhand interactions, occlusions, left-right hand ambiguity, and fast motion. Most\nexisting methods rely on RGB inputs, which have severe limitations under\nlow-light conditions and suffer from motion blur. In contrast, event cameras\ncapture local brightness changes instead of full image frames and do not suffer\nfrom the described effects. Unfortunately, existing image-based techniques\ncannot be directly applied to events due to significant differences in the data\nmodalities. In response to these challenges, this paper introduces the first\nframework for 3D tracking of two fast-moving and interacting hands from a\nsingle monocular event camera. Our approach tackles the left-right hand\nambiguity with a novel semi-supervised feature-wise attention mechanism and\nintegrates an intersection loss to fix hand collisions. To facilitate advances\nin this research domain, we release a new synthetic large-scale dataset of two\ninteracting hands, Ev2Hands-S, and a new real benchmark with real event streams\nand ground-truth 3D annotations, Ev2Hands-R. Our approach outperforms existing\nmethods in terms of the 3D reconstruction accuracy and generalises to real data\nunder severe light conditions.\n", "rewritten_text": "\\boxed{Tracking 3D hand movements from a single monocular video is a difficult task due to hand interactions, occlusions, and fast motion. While most methods use RGB inputs, they have limitations in low-light conditions and suffer from motion blur. Event cameras, which capture brightness changes, can overcome these issues. However, existing image-based techniques cannot be applied to event cameras due to differences in data modalities. This paper proposes a novel framework for 3D tracking of two fast-moving and interacting hands from a single monocular event camera. The approach uses a semi-supervised feature-wise attention mechanism to resolve left-right hand ambiguity and an intersection loss to handle hand collisions. A new synthetic dataset, Ev2Hands-S, and a real benchmark, Ev2Hands-R, are introduced to facilitate research in this area. The proposed approach outperforms existing methods in 3D reconstruction accuracy and generalizes well to real data under severe light conditions.}"}, "2010.08243": {"original_text": "  3D object detectors based only on LiDAR point clouds hold the\nstate-of-the-art on modern street-view benchmarks. However, LiDAR-based\ndetectors poorly generalize across domains due to domain shift. In the case of\nLiDAR, in fact, domain shift is not only due to changes in the environment and\nin the object appearances, as for visual data from RGB cameras, but is also\nrelated to the geometry of the point clouds (e.g., point density variations).\nThis paper proposes SF-UDA$^{3D}$, the first Source-Free Unsupervised Domain\nAdaptation (SF-UDA) framework to domain-adapt the state-of-the-art PointRCNN 3D\ndetector to target domains for which we have no annotations (unsupervised),\nneither we hold images nor annotations of the source domain (source-free).\nSF-UDA$^{3D}$ is novel on both aspects. Our approach is based on\npseudo-annotations, reversible scale-transformations and motion coherency.\nSF-UDA$^{3D}$ outperforms both previous domain adaptation techniques based on\nfeatures alignment and state-of-the-art 3D object detection methods which\nadditionally use few-shot target annotations or target annotation statistics.\nThis is demonstrated by extensive experiments on two large-scale datasets,\ni.e., KITTI and nuScenes.\n", "rewritten_text": "\\boxed{State-of-the-art 3D object detectors that rely solely on LiDAR point clouds excel in modern street-view benchmarks, but they struggle to generalize across different domains due to domain shift. This issue is exacerbated in LiDAR-based detectors, where domain shift is not only caused by environmental and object appearance changes, but also by variations in point cloud geometry. To address this, we propose SF-UDA$^{3D}$, a novel Source-Free Unsupervised Domain Adaptation framework that adapts the state-of-the-art PointRCNN 3D detector to target domains without requiring annotations or source domain data. Our approach leverages pseudo-annotations, reversible scale-transformations, and motion coherency, and outperforms existing domain adaptation techniques and 3D object detection methods that utilize few-shot target annotations or target annotation statistics, as demonstrated by extensive experiments on the KITTI and nuScenes datasets.}"}, "2305.09407": {"original_text": "  Visual quality inspection in high performance manufacturing can benefit from\nautomation, due to cost savings and improved rigor. Deep learning techniques\nare the current state of the art for generic computer vision tasks like\nclassification and object detection. Manufacturing data can pose a challenge\nfor deep learning because data is highly repetitive and there are few images of\ndefects or deviations to learn from. Deep learning models trained with such\ndata can be fragile and sensitive to context, and can under-detect new defects\nnot found in the training data. In this work, we explore training defect\ndetection models to learn specific defects out of context, so that they are\nmore likely to be detected in new situations. We demonstrate how models trained\non diverse images containing a common defect type can pick defects out in new\ncircumstances. Such generic models could be more robust to new defects not\nfound data collected for training, and can reduce data collection impediments\nto implementing visual inspection on production lines. Additionally, we\ndemonstrate that object detection models trained to predict a label and\nbounding box outperform classifiers that predict a label only on held out test\ndata typical of manufacturing inspection tasks. Finally, we studied the factors\nthat affect generalization in order to train models that work under a wider\nrange of conditions.\n", "rewritten_text": "\\boxed{\nAutomating visual quality inspection in high-performance manufacturing can lead to cost savings and improved rigor. However, deep learning techniques, which are currently the state of the art for computer vision tasks, can be limited by the repetitive and scarce nature of manufacturing data. To address this, we investigate training defect detection models to recognize specific defects in various contexts, enabling them to detect new defects in unseen situations. Our results show that models trained on diverse images of a common defect type can identify defects in new circumstances, making them more robust to unknown defects and reducing the need for extensive data collection. Furthermore, we find that object detection models that predict both a label and bounding box outperform classifiers that predict only a label in manufacturing inspection tasks. Finally, we examine the factors that influence generalization to develop models that can operate effectively under a broader range of conditions."}, "2406.09858": {"original_text": "  The visual quality of an image is confounded by a number of intertwined\nfactors including its semantic content, distortion characteristics and\nappearance properties such as brightness, contrast, sharpness, and\ncolourfulness. Distilling high level knowledge about all these quality bearing\nattributes is crucial for developing objective Image Quality Assessment\n(IQA).While existing solutions have modeled some of these aspects, a\ncomprehensive solution that involves all these important quality related\nattributes has not yet been developed. In this paper, we present a new blind\nIQA (BIQA) model termed Self-supervision and Vision-Language supervision Image\nQUality Evaluator (SLIQUE) that features a joint vision-language and visual\ncontrastive representation learning framework for acquiring high level\nknowledge about the images semantic contents, distortion characteristics and\nappearance properties for IQA. For training SLIQUE, we have developed a\nsystematic approach to constructing a first of its kind large image database\nannotated with all three categories of quality relevant texts. The Text\nAnnotated Distortion, Appearance and Content (TADAC) database has over 1.6\nmillion images annotated with textual descriptions of their semantic contents,\ndistortion characteristics and appearance properties. The method for\nconstructing TADAC and the database itself will be particularly useful for\nexploiting vision-language modeling for advanced IQA applications. Extensive\nexperimental results show that SLIQUE has superior performances over state of\nthe art, demonstrating the soundness of its design principle and the\neffectiveness of its implementation.\n", "rewritten_text": "\\boxed{\nThe quality of an image is influenced by multiple interconnected factors, including its meaning, distortion, and visual properties such as brightness and color. To develop an effective Image Quality Assessment (IQA) system, it's essential to understand these factors. While previous solutions have addressed some of these aspects, a comprehensive approach that considers all of them has not been developed. This paper introduces a new blind IQA model called SLIQUE, which uses a joint vision-language and visual contrastive representation learning framework to acquire high-level knowledge about an image's meaning, distortion, and visual properties. To train SLIQUE, we created a large image database called TADAC, which contains over 1.6 million images annotated with textual descriptions of their meaning, distortion, and visual properties. Our experimental results show that SLIQUE outperforms existing state-of-the-art models, demonstrating the effectiveness of its design and implementation.\n}"}, "2406.16860": {"original_text": "  We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, addressing the difficulties involved in consolidating and\ninterpreting results from various tasks, and introduce a new vision-centric\nbenchmark, CV-Bench. To further improve visual grounding, we propose the\nSpatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that\nintegrates high-resolution vision features with LLMs while reducing the number\nof tokens. Additionally, we discuss the curation of high-quality visual\ninstruction-tuning data from publicly available sources, emphasizing the\nimportance of data source balancing and distribution ratio. Collectively,\nCambrian-1 not only achieves state-of-the-art performance but also serves as a\ncomprehensive, open cookbook for instruction-tuned MLLMs. We provide model\nweights, code, supporting tools, datasets, and detailed instruction-tuning and\nevaluation recipes. We hope our release will inspire and accelerate\nadvancements in multimodal systems and visual representation learning.\n", "rewritten_text": "\\boxed{\nWe present Cambrian-1, a novel family of multimodal language models (MLLMs) that prioritize a vision-centric approach. Despite the importance of strong language models for multimodal capabilities, the design of vision components is often overlooked and disconnected from visual representation learning research, hindering accurate sensory grounding in real-world scenarios. Our study bridges this gap by using LLMs and visual instruction tuning to evaluate various visual representations, providing new insights into different models and architectures through experiments with over 20 vision encoders. We critically assess existing MLLM benchmarks, introduce a new vision-centric benchmark called CV-Bench, and propose the Spatial Vision Aggregator (SVA), a dynamic connector that integrates high-resolution vision features with LLMs while reducing token numbers. Furthermore, we discuss the importance of curating high-quality visual instruction-tuning data from public sources, emphasizing data source balancing and distribution ratio. Cambrian-1 achieves state-of-the-art performance and serves as a comprehensive, open resource for instruction-tuned MLLMs, providing model weights, code, tools, datasets, and detailed recipes for instruction-tuning and evaluation. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning."}, "2403.09593": {"original_text": "  Names are essential to both human cognition and vision-language models.\nOpen-vocabulary models utilize class names as text prompts to generalize to\ncategories unseen during training. However, the precision of these names is\noften overlooked in existing datasets. In this paper, we address this\nunderexplored problem by presenting a framework for \"renovating\" names in\nopen-vocabulary segmentation benchmarks (RENOVATE). Our framework features a\nrenaming model that enhances the quality of names for each visual segment.\nThrough experiments, we demonstrate that our renovated names help train\nstronger open-vocabulary models with up to 15% relative improvement and\nsignificantly enhance training efficiency with improved data quality. We also\nshow that our renovated names improve evaluation by better measuring\nmisclassification and enabling fine-grained model analysis. We will provide our\ncode and relabelings for several popular segmentation datasets (MS COCO,\nADE20K, Cityscapes) to the research community.\n", "rewritten_text": "\\boxed{The importance of accurate names in open-vocabulary models for human cognition and vision-language tasks is often underestimated. This paper introduces RENOVATE, a framework that refines names in segmentation benchmarks, leading to stronger models with up to 15% relative improvement and enhanced training efficiency. The renovated names also improve evaluation by better capturing misclassification and enabling detailed model analysis. The code and relabelings for popular datasets (MS COCO, ADE20K, Cityscapes) will be shared with the research community.}"}, "2310.13201": {"original_text": "  Early identification of abnormalities in plants is an important task for\nensuring proper growth and achieving high yields from crops. Precision\nagriculture can significantly benefit from modern computer vision tools to make\nfarming strategies addressing these issues efficient and effective. As farming\nlands are typically quite large, farmers have to manually check vast areas to\ndetermine the status of the plants and apply proper treatments. In this work,\nwe consider the problem of automatically identifying abnormal regions in maize\nplants from images captured by a UAV. Using deep learning techniques, we have\ndeveloped a methodology which can detect different levels of abnormality (i.e.,\nlow, medium, high or no abnormality) in maize plants independently of their\ngrowth stage. The primary goal is to identify anomalies at the earliest\npossible stage in order to maximize the effectiveness of potential treatments.\nAt the same time, the proposed system can provide valuable information to human\nannotators for ground truth data collection by helping them to focus their\nattention on a much smaller set of images only. We have experimented with two\ndifferent but complimentary approaches, the first considering abnormality\ndetection as a classification problem and the second considering it as a\nregression problem. Both approaches can be generalized to different types of\nabnormalities and do not make any assumption about the abnormality occurring at\nan early plant growth stage which might be easier to detect due to the plants\nbeing smaller and easier to separate. As a case study, we have considered a\npublicly available data set which exhibits mostly Nitrogen deficiency in maize\nplants of various growth stages. We are reporting promising preliminary results\nwith an 88.89\\% detection accuracy of low abnormality and 100\\% detection\naccuracy of no abnormality.\n", "rewritten_text": "\\boxed{\nIdentifying abnormalities in plants early on is crucial for optimal growth and high yields. Modern computer vision techniques can enhance precision agriculture by making farming strategies more efficient and effective. Currently, farmers must manually inspect large areas to determine plant health and apply treatments. This study proposes a methodology using deep learning to automatically detect abnormal regions in maize plants from UAV-captured images, regardless of growth stage. The goal is to identify anomalies early to maximize treatment effectiveness. The system can also assist human annotators by focusing their attention on a smaller set of images. Two approaches were experimented with: abnormality detection as a classification problem and as a regression problem. Both approaches can be generalized to different types of abnormalities and do not assume early detection is easier. A case study using a publicly available dataset exhibiting Nitrogen deficiency in maize plants showed promising results, with 88.89% accuracy in detecting low abnormality and 100% accuracy in detecting no abnormality."}, "1711.0664": {"original_text": "  We investigate the problem of producing structured graph representations of\nvisual scenes. Our work analyzes the role of motifs: regularly appearing\nsubstructures in scene graphs. We present new quantitative insights on such\nrepeated structures in the Visual Genome dataset. Our analysis shows that\nobject labels are highly predictive of relation labels but not vice-versa. We\nalso find that there are recurring patterns even in larger subgraphs: more than\n50% of graphs contain motifs involving at least two relations. Our analysis\nmotivates a new baseline: given object detections, predict the most frequent\nrelation between object pairs with the given labels, as seen in the training\nset. This baseline improves on the previous state-of-the-art by an average of\n3.6% relative improvement across evaluation settings. We then introduce Stacked\nMotif Networks, a new architecture designed to capture higher order motifs in\nscene graphs that further improves over our strong baseline by an average 7.1%\nrelative gain. Our code is available at github.com/rowanz/neural-motifs.\n", "rewritten_text": "\\boxed{\nThis study explores the creation of structured graph representations of visual scenes, with a focus on motifs, which are regularly occurring substructures in scene graphs. An analysis of the Visual Genome dataset reveals that object labels are strong predictors of relation labels, but not the reverse. Furthermore, recurring patterns are found in larger subgraphs, with over 50% of graphs containing motifs involving at least two relations. These insights lead to the development of a new baseline that predicts the most frequent relation between object pairs with given labels, resulting in a 3.6% relative improvement over previous state-of-the-art methods. Additionally, a novel architecture called Stacked Motif Networks is introduced, which captures higher-order motifs in scene graphs and achieves a 7.1% relative gain over the baseline. The code for this work is available at github.com/rowanz/neural-motifs.\n}"}, "2207.01932": {"original_text": "  Image Coding for Machines (ICM) aims to compress images for AI tasks analysis\nrather than meeting human perception. Learning a kind of feature that is both\ngeneral (for AI tasks) and compact (for compression) is pivotal for its\nsuccess. In this paper, we attempt to develop an ICM framework by learning\nuniversal features while also considering compression. We name such features as\nomnipotent features and the corresponding framework as Omni-ICM. Considering\nself-supervised learning (SSL) improves feature generalization, we integrate it\nwith the compression task into the Omni-ICM framework to learn omnipotent\nfeatures. However, it is non-trivial to coordinate semantics modeling in SSL\nand redundancy removing in compression, so we design a novel information\nfiltering (IF) module between them by co-optimization of instance\ndistinguishment and entropy minimization to adaptively drop information that is\nweakly related to AI tasks (e.g., some texture redundancy). Different from\nprevious task-specific solutions, Omni-ICM could directly support AI tasks\nanalysis based on the learned omnipotent features without joint training or\nextra transformation. Albeit simple and intuitive, Omni-ICM significantly\noutperforms existing traditional and learning-based codecs on multiple\nfundamental vision tasks.\n", "rewritten_text": "\\boxed{\nImage Coding for Machines (ICM) focuses on compressing images for artificial intelligence (AI) task analysis, rather than for human visual perception. To achieve this, it is crucial to learn features that are both general for AI tasks and compact for compression. This paper proposes an ICM framework, called Omni-ICM, which learns universal features that balance generality and compactness. By integrating self-supervised learning (SSL) with compression, Omni-ICM learns features that are adaptable to various AI tasks. A novel information filtering module is designed to coordinate semantics modeling in SSL and redundancy removal in compression, allowing the framework to selectively discard information irrelevant to AI tasks. Unlike previous task-specific solutions, Omni-ICM can directly support AI task analysis using the learned features, without requiring joint training or additional transformations. The proposed framework demonstrates significant performance improvements over traditional and learning-based codecs on multiple fundamental vision tasks."}, "1711.07183": {"original_text": "  Generating adversarial examples is an intriguing problem and an important way\nof understanding the working mechanism of deep neural networks. Most existing\napproaches generated perturbations in the image space, i.e., each pixel can be\nmodified independently. However, in this paper we pay special attention to the\nsubset of adversarial examples that correspond to meaningful changes in 3D\nphysical properties (like rotation and translation, illumination condition,\netc.). These adversaries arguably pose a more serious concern, as they\ndemonstrate the possibility of causing neural network failure by easy\nperturbations of real-world 3D objects and scenes.\n  In the contexts of object classification and visual question answering, we\naugment state-of-the-art deep neural networks that receive 2D input images with\na rendering module (either differentiable or not) in front, so that a 3D scene\n(in the physical space) is rendered into a 2D image (in the image space), and\nthen mapped to a prediction (in the output space). The adversarial\nperturbations can now go beyond the image space, and have clear meanings in the\n3D physical world. Though image-space adversaries can be interpreted as\nper-pixel albedo change, we verify that they cannot be well explained along\nthese physically meaningful dimensions, which often have a non-local effect.\nBut it is still possible to successfully attack beyond the image space on the\nphysical space, though this is more difficult than image-space attacks,\nreflected in lower success rates and heavier perturbations required.\n", "rewritten_text": "\\boxed{\nGenerating adversarial examples is a crucial problem in understanding deep neural networks. While most approaches focus on perturbing individual pixels in the image space, this paper explores a more critical subset of adversarial examples that involve meaningful changes to 3D physical properties, such as rotation, translation, and illumination. These adversaries are more concerning as they demonstrate the potential to cause neural network failure through simple modifications to real-world 3D objects and scenes.\n\nTo address this, we propose augmenting state-of-the-art deep neural networks with a rendering module that transforms 3D scenes into 2D images, allowing adversarial perturbations to have clear meanings in the physical world. While image-space adversaries can be interpreted as per-pixel changes, we find that they cannot be explained by physically meaningful dimensions, which often have a non-local effect. However, it is still possible to successfully attack beyond the image space, although this is more challenging and requires heavier perturbations.\n}"}, "2105.02039": {"original_text": "  In this paper, we fill the research gap by adopting state-of-the-art computer\nvision techniques for the data extraction stage in a data mining system. As\nshown in Fig.1, this stage contains two subtasks, namely, plot element\ndetection and data conversion. For building a robust box detector, we\ncomprehensively compare different deep learning-based methods and find a\nsuitable method to detect box with high precision. For building a robust point\ndetector, a fully convolutional network with feature fusion module is adopted,\nwhich can distinguish close points compared to traditional methods. The\nproposed system can effectively handle various chart data without making\nheuristic assumptions. For data conversion, we translate the detected element\ninto data with semantic value. A network is proposed to measure feature\nsimilarities between legends and detected elements in the legend matching\nphase. Furthermore, we provide a baseline on the competition of Harvesting raw\ntables from Infographics. Some key factors have been found to improve the\nperformance of each stage. Experimental results demonstrate the effectiveness\nof the proposed system.\n", "rewritten_text": "\\boxed{This paper addresses a research gap by utilizing cutting-edge computer vision techniques in the data extraction stage of a data mining system. This stage involves two subtasks: plot element detection and data conversion. We develop a robust box detector by comparing various deep learning-based methods and selecting the most accurate one. Additionally, we employ a fully convolutional network with a feature fusion module to create a robust point detector, which outperforms traditional methods in distinguishing close points. Our system can effectively process diverse chart data without relying on heuristic assumptions. In the data conversion stage, we translate detected elements into semantically valuable data. We also propose a network to measure feature similarities between legends and detected elements during the legend matching phase. Furthermore, we establish a baseline for the competition of harvesting raw tables from infographics and identify key factors that enhance the performance of each stage. Experimental results confirm the effectiveness of our proposed system.}"}, "2401.08508": {"original_text": "  Sentiment analysis and emotion detection are important research topics in\nnatural language processing (NLP) and benefit many downstream tasks. With the\nwidespread application of LLMs, researchers have started exploring the\napplication of LLMs based on instruction-tuning in the field of sentiment\nanalysis. However, these models only focus on single aspects of affective\nclassification tasks (e.g. sentimental polarity or categorical emotions), and\noverlook the regression tasks (e.g. sentiment strength or emotion intensity),\nwhich leads to poor performance in downstream tasks. The main reason is the\nlack of comprehensive affective instruction tuning datasets and evaluation\nbenchmarks, which cover various affective classification and regression tasks.\nMoreover, although emotional information is useful for downstream tasks,\nexisting downstream datasets lack high-quality and comprehensive affective\nannotations. In this paper, we propose EmoLLMs, the first series of\nopen-sourced instruction-following LLMs for comprehensive affective analysis\nbased on fine-tuning various LLMs with instruction data, the first multi-task\naffective analysis instruction dataset (AAID) with 234K data samples based on\nvarious classification and regression tasks to support LLM instruction tuning,\nand a comprehensive affective evaluation benchmark (AEB) with 14 tasks from\nvarious sources and domains to test the generalization ability of LLMs. We\npropose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various\naffective instruction tasks. We compare our model with a variety of LLMs on\nAEB, where our models outperform all other open-sourced LLMs, and surpass\nChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve\nthe ChatGPT-level and GPT-4-level generalization capabilities on affective\nanalysis tasks, and demonstrates our models can be used as affective annotation\ntools.\n", "rewritten_text": "\\boxed{\nSentiment analysis and emotion detection are crucial areas of research in natural language processing (NLP), with numerous applications in downstream tasks. Recently, researchers have explored the use of large language models (LLMs) with instruction-tuning for sentiment analysis. However, these models focus solely on specific aspects of affective classification tasks, neglecting regression tasks, which leads to subpar performance in downstream tasks. The primary reason for this limitation is the lack of comprehensive datasets and evaluation benchmarks that cover various affective classification and regression tasks. Furthermore, existing downstream datasets lack high-quality and comprehensive emotional annotations. To address this, we introduce EmoLLMs, a series of open-sourced instruction-following LLMs for comprehensive affective analysis, fine-tuned with instruction data. We also propose the first multi-task affective analysis instruction dataset (AAID) with 234K data samples and a comprehensive affective evaluation benchmark (AEB) with 14 tasks from various sources and domains. Our EmoLLMs outperform other open-sourced LLMs and rival ChatGPT and GPT-4 in most tasks, demonstrating their ability to generalize to affective analysis tasks and their potential as affective annotation tools.\n}"}, "2210.05564": {"original_text": "  Semantic segmentation is a fundamental topic in computer vision. Several deep\nlearning methods have been proposed for semantic segmentation with outstanding\nresults. However, these models require a lot of densely annotated images. To\naddress this problem, we propose a new algorithm that uses HyperGraph\nConvolutional Networks for Weakly-supervised Semantic Segmentation\n(HyperGCN-WSS). Our algorithm constructs spatial and k-Nearest Neighbor (k-NN)\ngraphs from the images in the dataset to generate the hypergraphs. Then, we\ntrain a specialized HyperGraph Convolutional Network (HyperGCN) architecture\nusing some weak signals. The outputs of the HyperGCN are denominated\npseudo-labels, which are later used to train a DeepLab model for semantic\nsegmentation. HyperGCN-WSS is evaluated on the PASCAL VOC 2012 dataset for\nsemantic segmentation, using scribbles or clicks as weak signals. Our algorithm\nshows competitive performance against previous methods.\n", "rewritten_text": "\\boxed{Semantic segmentation, a crucial computer vision task, has seen significant advancements through deep learning methods, but these models require a large amount of densely annotated images. To overcome this limitation, we introduce HyperGCN-WSS, a novel algorithm that leverages HyperGraph Convolutional Networks for weakly-supervised semantic segmentation. By constructing spatial and k-NN graphs from the dataset, we train a specialized HyperGCN architecture using weak signals, generating pseudo-labels that are then used to train a DeepLab model for semantic segmentation. Our approach demonstrates competitive performance on the PASCAL VOC 2012 dataset, using scribbles or clicks as weak signals.}"}, "2112.0618": {"original_text": "  We present 360-DFPE, a sequential floor plan estimation method that directly\ntakes 360-images as input without relying on active sensors or 3D information.\nOur approach leverages a loosely coupled integration between a monocular visual\nSLAM solution and a monocular 360-room layout approach, which estimate camera\nposes and layout geometries, respectively. Since our task is to sequentially\ncapture the floor plan using monocular images, the entire scene structure, room\ninstances, and room shapes are unknown. To tackle these challenges, we first\nhandle the scale difference between visual odometry and layout geometry via\nformulating an entropy minimization process, which enables us to directly align\n360-layouts without knowing the entire scene in advance. Second, to\nsequentially identify individual rooms, we propose a novel room identification\nalgorithm that tracks every room along the camera exploration using geometry\ninformation. Lastly, to estimate the final shape of the room, we propose a\nshortest path algorithm with an iterative coarse-to-fine strategy, which\nimproves prior formulations with higher accuracy and faster run-time. Moreover,\nwe collect a new floor plan dataset with challenging large-scale scenes,\nproviding both point clouds and sequential 360-image information. Experimental\nresults show that our monocular solution achieves favorable performance against\nthe current state-of-the-art algorithms that rely on active sensors and require\nthe entire scene reconstruction data in advance.\n", "rewritten_text": "\\boxed{We introduce 360-DFPE, a novel method for estimating floor plans from 360-degree images without relying on active sensors or 3D information. Our approach combines a monocular visual SLAM solution with a monocular 360-room layout approach to estimate camera poses and layout geometries. To overcome the challenges of unknown scene structure, room instances, and room shapes, we develop an entropy minimization process to align 360-layouts, a novel room identification algorithm to track individual rooms, and a shortest path algorithm to estimate room shapes. We also create a new floor plan dataset with large-scale scenes and demonstrate that our monocular solution outperforms state-of-the-art algorithms that require active sensors and entire scene reconstruction data.}"}, "2205.08811": {"original_text": "  Object pose estimation is crucial for robotic applications and augmented\nreality. Beyond instance level 6D object pose estimation methods, estimating\ncategory-level pose and shape has become a promising trend. As such, a new\nresearch field needs to be supported by well-designed datasets. To provide a\nbenchmark with high-quality ground truth annotations to the community, we\nintroduce a multimodal dataset for category-level object pose estimation with\nphotometrically challenging objects termed PhoCaL. PhoCaL comprises 60 high\nquality 3D models of household objects over 8 categories including highly\nreflective, transparent and symmetric objects. We developed a novel\nrobot-supported multi-modal (RGB, depth, polarisation) data acquisition and\nannotation process. It ensures sub-millimeter accuracy of the pose for opaque\ntextured, shiny and transparent objects, no motion blur and perfect camera\nsynchronisation. To set a benchmark for our dataset, state-of-the-art RGB-D and\nmonocular RGB methods are evaluated on the challenging scenes of PhoCaL.\n", "rewritten_text": "\\boxed{Estimating the pose and shape of objects at the category level is a growing trend in robotic applications and augmented reality, and requires well-designed datasets to support research in this area. To address this need, we introduce PhoCaL, a multimodal dataset for category-level object pose estimation that includes 60 high-quality 3D models of household objects across 8 categories, featuring challenging objects with reflective, transparent, and symmetric properties. Our novel data acquisition and annotation process ensures highly accurate pose estimation with sub-millimeter accuracy, no motion blur, and perfect camera synchronization. We evaluate state-of-the-art RGB-D and monocular RGB methods on PhoCaL's challenging scenes to set a benchmark for the dataset.}"}, "2005.14439": {"original_text": "  Dynamic routing networks, aimed at finding the best routing paths in the\nnetworks, have achieved significant improvements to neural networks in terms of\naccuracy and efficiency. In this paper, we see dynamic routing networks in a\nfresh light, formulating a routing method as a mapping from a sample space to a\nrouting space. From the perspective of space mapping, prevalent methods of\ndynamic routing didn't consider how inference paths would be distributed in the\nrouting space. Thus, we propose a novel method, termed CoDiNet, to model the\nrelationship between a sample space and a routing space by regularizing the\ndistribution of routing paths with the properties of consistency and diversity.\nSpecifically, samples with similar semantics should be mapped into the same\narea in routing space, while those with dissimilar semantics should be mapped\ninto different areas. Moreover, we design a customizable dynamic routing\nmodule, which can strike a balance between accuracy and efficiency. When\ndeployed upon ResNet models, our method achieves higher performance and\neffectively reduces average computational cost on four widely used datasets.\n", "rewritten_text": "\\boxed{\nDynamic routing networks have significantly improved neural networks' accuracy and efficiency by finding optimal routing paths. This paper reexamines dynamic routing networks by viewing them as a mapping between a sample space and a routing space. Existing methods overlook how inference paths are distributed in the routing space. To address this, we introduce CoDiNet, a novel method that models the relationship between sample and routing spaces by ensuring consistent and diverse routing path distributions. Specifically, semantically similar samples are mapped to the same routing space area, while dissimilar ones are mapped to different areas. Our customizable dynamic routing module balances accuracy and efficiency, achieving higher performance and reduced computational cost on four datasets when applied to ResNet models.\n}"}, "2110.05594": {"original_text": "  We present a modern solution to the multi-view photometric stereo problem\n(MVPS). Our work suitably exploits the image formation model in a MVPS\nexperimental setup to recover the dense 3D reconstruction of an object from\nimages. We procure the surface orientation using a photometric stereo (PS)\nimage formation model and blend it with a multi-view neural radiance field\nrepresentation to recover the object's surface geometry. Contrary to the\nprevious multi-staged framework to MVPS, where the position, iso-depth\ncontours, or orientation measurements are estimated independently and then\nfused later, our method is simple to implement and realize. Our method performs\nneural rendering of multi-view images while utilizing surface normals estimated\nby a deep photometric stereo network. We render the MVPS images by considering\nthe object's surface normals for each 3D sample point along the viewing\ndirection rather than explicitly using the density gradient in the volume space\nvia 3D occupancy information. We optimize the proposed neural radiance field\nrepresentation for the MVPS setup efficiently using a fully connected deep\nnetwork to recover the 3D geometry of an object. Extensive evaluation on the\nDiLiGenT-MV benchmark dataset shows that our method performs better than the\napproaches that perform only PS or only multi-view stereo (MVS) and provides\ncomparable results against the state-of-the-art multi-stage fusion methods.\n", "rewritten_text": "\\boxed{We introduce a novel approach to the multi-view photometric stereo problem, which leverages the image formation model to recover the dense 3D reconstruction of an object from images. By combining photometric stereo and multi-view neural radiance field representation, our method efficiently recovers the object's surface geometry without requiring independent estimation and fusion of position, iso-depth contours, or orientation measurements. Our approach involves neural rendering of multi-view images using surface normals estimated by a deep photometric stereo network, and optimizes the neural radiance field representation using a fully connected deep network. Experimental results on the DiLiGenT-MV benchmark dataset demonstrate that our method outperforms approaches that only use photometric stereo or multi-view stereo, and achieves comparable results to state-of-the-art multi-stage fusion methods.}"}, "2206.13263": {"original_text": "  Robust maritime obstacle detection is critical for safe navigation of\nautonomous boats and timely collision avoidance. The current state-of-the-art\nis based on deep segmentation networks trained on large datasets. However,\nper-pixel ground truth labeling of such datasets is labor-intensive and\nexpensive. We propose a new scaffolding learning regime (SLR) that leverages\nweak annotations consisting of water edges, the horizon location, and obstacle\nbounding boxes to train segmentation-based obstacle detection networks, thereby\nreducing the required ground truth labeling effort by a factor of twenty. SLR\ntrains an initial model from weak annotations and then alternates between\nre-estimating the segmentation pseudo-labels and improving the network\nparameters. Experiments show that maritime obstacle segmentation networks\ntrained using SLR on weak annotations not only match but outperform the same\nnetworks trained with dense ground truth labels, which is a remarkable result.\nIn addition to the increased accuracy, SLR also increases domain generalization\nand can be used for domain adaptation with a low manual annotation load. The\nSLR code and pre-trained models are available at\nhttps://github.com/lojzezust/SLR .\n", "rewritten_text": "\\boxed{\nAccurate detection of maritime obstacles is crucial for safe autonomous boat navigation and timely collision avoidance. Currently, deep segmentation networks trained on large datasets are the state-of-the-art solution. However, creating these datasets requires labor-intensive and expensive per-pixel ground truth labeling. To address this, we introduce a novel scaffolding learning regime (SLR) that utilizes weak annotations, including water edges, horizon location, and obstacle bounding boxes, to train segmentation-based obstacle detection networks. This approach reduces the required ground truth labeling effort by a factor of twenty. The SLR method involves training an initial model with weak annotations, then iteratively refining the segmentation pseudo-labels and network parameters. Our experiments demonstrate that maritime obstacle segmentation networks trained using SLR on weak annotations not only match but surpass the performance of networks trained with dense ground truth labels, a remarkable achievement. Additionally, SLR improves domain generalization and enables domain adaptation with minimal manual annotation. The SLR code and pre-trained models are available at https://github.com/lojzezust/SLR."}, "2204.07126": {"original_text": "  Recent development of neural implicit function has shown tremendous success\non high-quality 3D shape reconstruction. However, most works divide the space\ninto inside and outside of the shape, which limits their representing power to\nsingle-layer and watertight shapes. This limitation leads to tedious data\nprocessing (converting non-watertight raw data to watertight) as well as the\nincapability of representing general object shapes in the real world. In this\nwork, we propose a novel method to represent general shapes including\nnon-watertight shapes and shapes with multi-layer surfaces. We introduce\nGeneral Implicit Function for 3D Shape (GIFS), which models the relationships\nbetween every two points instead of the relationships between points and\nsurfaces. Instead of dividing 3D space into predefined inside-outside regions,\nGIFS encodes whether two points are separated by any surface. Experiments on\nShapeNet show that GIFS outperforms previous state-of-the-art methods in terms\nof reconstruction quality, rendering efficiency, and visual fidelity. Project\npage is available at https://jianglongye.com/gifs .\n", "rewritten_text": "\\boxed{\nRecent advances in neural implicit functions have achieved great success in 3D shape reconstruction, but most approaches are limited to single-layer and watertight shapes, requiring tedious data processing and unable to represent general object shapes. To overcome this, we propose a novel method, General Implicit Function for 3D Shape (GIFS), which models relationships between points rather than between points and surfaces, encoding whether two points are separated by any surface. Our experiments on ShapeNet demonstrate that GIFS surpasses previous state-of-the-art methods in reconstruction quality, rendering efficiency, and visual fidelity. For more information, visit https://jianglongye.com/gifs.\n}"}, "2103.03166": {"original_text": "  Annotated medical images are typically rarer than labeled natural images\nsince they are limited by domain knowledge and privacy constraints. Recent\nadvances in transfer and contrastive learning have provided effective solutions\nto tackle such issues from different perspectives. The state-of-the-art\ntransfer learning (e.g., Big Transfer (BiT)) and contrastive learning (e.g.,\nSimple Siamese Contrastive Learning (SimSiam)) approaches have been\ninvestigated independently, without considering the complementary nature of\nsuch techniques. It would be appealing to accelerate contrastive learning with\ntransfer learning, given that slow convergence speed is a critical limitation\nof modern contrastive learning approaches. In this paper, we investigate the\nfeasibility of aligning BiT with SimSiam. From empirical analyses, different\nnormalization techniques (Group Norm in BiT vs. Batch Norm in SimSiam) are the\nkey hurdle of adapting BiT to SimSiam. When combining BiT with SimSiam, we\nevaluated the performance of using BiT, SimSiam, and BiT+SimSiam on CIFAR-10\nand HAM10000 datasets. The results suggest that the BiT models accelerate the\nconvergence speed of SimSiam. When used together, the model gives superior\nperformance over both of its counterparts. We hope this study will motivate\nresearchers to revisit the task of aggregating big pre-trained models with\ncontrastive learning models for image analysis.\n", "rewritten_text": "\\boxed{\nMedical images with annotations are scarce due to limited domain knowledge and privacy concerns. Recent breakthroughs in transfer and contrastive learning have addressed these issues from different angles. However, state-of-the-art transfer learning (e.g., Big Transfer) and contrastive learning (e.g., Simple Siamese Contrastive Learning) approaches have been explored separately, overlooking their complementary nature. This study investigates the potential of combining Big Transfer with Simple Siamese Contrastive Learning to accelerate contrastive learning, which is hindered by slow convergence speed. Our empirical analysis reveals that different normalization techniques (Group Norm vs. Batch Norm) are the main obstacle to adapting Big Transfer to Simple Siamese Contrastive Learning. Our experiments on CIFAR-10 and HAM10000 datasets show that integrating Big Transfer with Simple Siamese Contrastive Learning accelerates convergence speed and yields superior performance compared to using either approach alone. This study aims to inspire researchers to explore the integration of large pre-trained models with contrastive learning models for image analysis.\n}"}, "2401.00642": {"original_text": "  During times of increasing antibiotic resistance and the spread of infectious\ndiseases like COVID-19, it is important to classify genes related to antibiotic\nresistance. As natural language processing has advanced with transformer-based\nlanguage models, many language models that learn characteristics of nucleotide\nsequences have also emerged. These models show good performance in classifying\nvarious features of nucleotide sequences. When classifying nucleotide\nsequences, not only the sequence itself, but also various background knowledge\nis utilized. In this study, we use not only a nucleotide sequence-based\nlanguage model but also a text language model based on PubMed articles to\nreflect more biological background knowledge in the model. We propose a method\nto fine-tune the nucleotide sequence language model and the text language model\nbased on various databases of antibiotic resistance genes. We also propose an\nLLM-based augmentation technique to supplement the data and an ensemble method\nto effectively combine the two models. We also propose a benchmark for\nevaluating the model. Our method achieved better performance than the\nnucleotide sequence language model in the drug resistance class prediction.\n", "rewritten_text": "\\boxed{In the face of growing antibiotic resistance and the spread of infectious diseases like COVID-19, accurately identifying genes linked to antibiotic resistance is crucial. Recent advancements in natural language processing, particularly with transformer-based language models, have led to the development of models that can learn characteristics of nucleotide sequences. These models have shown promising results in classifying various features of nucleotide sequences. To improve classification, we propose a method that combines a nucleotide sequence-based language model with a text language model based on PubMed articles, incorporating more biological background knowledge. Our approach involves fine-tuning both models using databases of antibiotic resistance genes, augmenting data with an LLM-based technique, and combining the models using an ensemble method. We also establish a benchmark for evaluating the model's performance. Our method outperforms the nucleotide sequence language model in predicting drug resistance classes.}"}, "2406.15718": {"original_text": "  As large language models (LLMs) increasingly permeate daily lives, there is a\ngrowing demand for real-time interactions that mirror human conversations.\nTraditional turn-based chat systems driven by LLMs prevent users from verbally\ninteracting with the system while it is generating responses. To overcome these\nlimitations, we adapt existing LLMs to \\textit{duplex models} so that these\nLLMs can listen for users while generating output and dynamically adjust\nthemselves to provide users with instant feedback. % such as in response to\ninterruptions. Specifically, we divide the queries and responses of\nconversations into several time slices and then adopt a\ntime-division-multiplexing (TDM) encoding-decoding strategy to\npseudo-simultaneously process these slices. Furthermore, to make LLMs\nproficient enough to handle real-time conversations, we build a fine-tuning\ndataset consisting of alternating time slices of queries and responses as well\nas covering typical feedback types in instantaneous interactions. Our\nexperiments show that although the queries and responses of conversations are\nsegmented into incomplete slices for processing, LLMs can preserve their\noriginal performance on standard benchmarks with a few fine-tuning steps on our\ndataset. Automatic and human evaluation indicate that duplex models make\nuser-AI interactions more natural and human-like, and greatly improve user\nsatisfaction compared to vanilla LLMs. Our duplex model and dataset will be\nreleased.\n", "rewritten_text": "\\boxed{As large language models become more prevalent in daily life, there is a growing need for real-time conversations that mimic human interactions. Traditional chat systems, which rely on language models, don't allow users to interrupt or interact with the system while it's generating responses. To overcome this limitation, we've developed \"duplex models\" that enable language models to listen to users while generating output and adapt to provide instant feedback. We achieve this by dividing conversations into time slices and using a time-division-multiplexing strategy to process them simultaneously. We've also created a fine-tuning dataset that includes alternating time slices of queries and responses, as well as typical feedback types. Our experiments show that our duplex models can preserve their performance on standard benchmarks with minimal fine-tuning, and both automatic and human evaluations indicate that they make user-AI interactions more natural and improve user satisfaction. We will release our duplex model and dataset.}"}, "2011.01535": {"original_text": "  3D-LaneNet+ is a camera-based DNN method for anchor free 3D lane detection\nwhich is able to detect 3d lanes of any arbitrary topology such as splits,\nmerges, as well as short and perpendicular lanes. We follow recently proposed\n3D-LaneNet, and extend it to enable the detection of these previously\nunsupported lane topologies. Our output representation is an anchor free,\nsemi-local tile representation that breaks down lanes into simple lane segments\nwhose parameters can be learnt. In addition we learn, per lane instance,\nfeature embedding that reasons for the global connectivity of locally detected\nsegments to form full 3d lanes. This combination allows 3D-LaneNet+ to avoid\nusing lane anchors, non-maximum suppression, and lane model fitting as in the\noriginal 3D-LaneNet. We demonstrate the efficacy of 3D-LaneNet+ using both\nsynthetic and real world data. Results show significant improvement relative to\nthe original 3D-LaneNet that can be attributed to better generalization to\ncomplex lane topologies, curvatures and surface geometries.\n", "rewritten_text": "\\boxed{3D-LaneNet+ is a camera-based deep neural network (DNN) method that detects 3D lanes with arbitrary topologies, including splits, merges, short lanes, and perpendicular lanes. Building upon the 3D-LaneNet, we extend its capabilities to detect these complex lane topologies. Our approach uses an anchor-free, semi-local tile representation to break down lanes into simple segments, and learns feature embeddings to reason about global connectivity and form complete 3D lanes. This eliminates the need for lane anchors, non-maximum suppression, and lane model fitting. We demonstrate the effectiveness of 3D-LaneNet+ using synthetic and real-world data, showing significant improvement over the original 3D-LaneNet in generalizing to complex lane topologies, curvatures, and surface geometries.}"}, "1612.06496": {"original_text": "  Image segmentation is a popular area of research in computer vision that has\nmany applications in automated image processing. A recent technique called\npiecewise flat embeddings (PFE) has been proposed for use in image\nsegmentation; PFE transforms image pixel data into a lower dimensional\nrepresentation where similar pixels are pulled close together and dissimilar\npixels are pushed apart. This technique has shown promising results, but its\noriginal formulation is not computationally feasible for large images. We\npropose two improvements to the algorithm for computing PFE: first, we\nreformulate portions of the algorithm to enable various linear algebra\noperations to be performed in parallel; second, we propose utilizing an\niterative linear solver (preconditioned conjugate gradient) to quickly solve a\nlinear least-squares problem that occurs in the inner loop of a nested\niteration. With these two computational improvements, we show on a publicly\navailable image database that PFE can be sped up by an order of magnitude\nwithout sacrificing segmentation performance. Our results make this technique\nmore practical for use on large data sets, not only for image segmentation, but\nfor general data clustering problems.\n", "rewritten_text": "\\boxed{Image segmentation, a crucial area of computer vision research, has numerous applications in automated image processing. A recent technique, piecewise flat embeddings (PFE), has shown promise in image segmentation by transforming pixel data into a lower-dimensional representation, where similar pixels are clustered together and dissimilar ones are separated. However, the original formulation is computationally expensive for large images. To address this, we propose two algorithmic improvements: parallelizing linear algebra operations and utilizing an iterative linear solver to efficiently solve a linear least-squares problem. These enhancements enable PFE to achieve an order-of-magnitude speedup without compromising segmentation performance, making it more practical for large-scale image segmentation and data clustering applications.}"}, "1511.06783": {"original_text": "  We present a novel dataset and a novel algorithm for recognizing activities\nof daily living (ADL) from a first-person wearable camera. Handled objects are\ncrucially important for egocentric ADL recognition. For specific examination of\nobjects related to users' actions separately from other objects in an\nenvironment, many previous works have addressed the detection of handled\nobjects in images captured from head-mounted and chest-mounted cameras.\nNevertheless, detecting handled objects is not always easy because they tend to\nappear small in images. They can be occluded by a user's body. As described\nherein, we mount a camera on a user's wrist. A wrist-mounted camera can capture\nhandled objects at a large scale, and thus it enables us to skip object\ndetection process. To compare a wrist-mounted camera and a head-mounted camera,\nwe also develop a novel and publicly available dataset that includes videos and\nannotations of daily activities captured simultaneously by both cameras.\nAdditionally, we propose a discriminative video representation that retains\nspatial and temporal information after encoding frame descriptors extracted by\nConvolutional Neural Networks (CNN).\n", "rewritten_text": "\\boxed{We introduce a new dataset and algorithm for recognizing daily activities from a wearable camera on the wrist, which provides a unique perspective for capturing handled objects at a larger scale, eliminating the need for object detection. We compare this approach to a head-mounted camera and provide a publicly available dataset with annotated videos of daily activities captured by both cameras. Our proposed algorithm uses a discriminative video representation that preserves spatial and temporal information from frame descriptors extracted by Convolutional Neural Networks (CNN).}"}, "2305.11806": {"original_text": "  Neural metrics for machine translation evaluation, such as COMET, exhibit\nsignificant improvements in their correlation with human judgments, as compared\nto traditional metrics based on lexical overlap, such as BLEU. Yet, neural\nmetrics are, to a great extent, \"black boxes\" returning a single sentence-level\nscore without transparency about the decision-making process. In this work, we\ndevelop and compare several neural explainability methods and demonstrate their\neffectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our\nstudy reveals that these metrics leverage token-level information that can be\ndirectly attributed to translation errors, as assessed through comparison of\ntoken-level neural saliency maps with Multidimensional Quality Metrics (MQM)\nannotations and with synthetically-generated critical translation errors. To\nease future research, we release our code at:\nhttps://github.com/Unbabel/COMET/tree/explainable-metrics.\n", "rewritten_text": "\\boxed{Neural metrics, such as COMET, have been shown to outperform traditional metrics like BLEU in evaluating machine translation, but they lack transparency in their decision-making process. To address this, we developed and compared various neural explainability methods to interpret state-of-the-art fine-tuned neural metrics. Our study found that these metrics utilize token-level information that can be directly linked to translation errors, as confirmed by comparing token-level neural saliency maps with MQM annotations and synthetically-generated critical translation errors. We have made our code available at https://github.com/Unbabel/COMET/tree/explainable-metrics to facilitate future research.}"}, "1906.11143": {"original_text": "  Accurate segmentation of the optic disc (OD) and cup (OC)in fundus images\nfrom different datasets is critical for glaucoma disease screening. The\ncross-domain discrepancy (domain shift) hinders the generalization of deep\nneural networks to work on different domain datasets.In this work, we present\nan unsupervised domain adaptation framework,called Boundary and Entropy-driven\nAdversarial Learning (BEAL), to improve the OD and OC segmentation performance,\nespecially on the ambiguous boundary regions. In particular, our proposed BEAL\nframe-work utilizes the adversarial learning to encourage the boundary\nprediction and mask probability entropy map (uncertainty map) of the target\ndomain to be similar to the source ones, generating more accurate boundaries\nand suppressing the high uncertainty predictions of OD and OC segmentation. We\nevaluate the proposed BEAL framework on two public retinal fundus image\ndatasets (Drishti-GS and RIM-ONE-r3), and the experiment results demonstrate\nthat our method outperforms the state-of-the-art unsupervised domain adaptation\nmethods. Codes will be available at https://github.com/EmmaW8/BEAL.\n", "rewritten_text": "\\boxed{\nAccurate segmentation of the optic disc and cup in fundus images is crucial for glaucoma disease screening, but deep neural networks struggle to generalize across different datasets due to domain shift. To address this, we propose an unsupervised domain adaptation framework called Boundary and Entropy-driven Adversarial Learning (BEAL), which improves optic disc and cup segmentation performance, particularly in ambiguous boundary regions. BEAL uses adversarial learning to align boundary predictions and uncertainty maps between the target and source domains, resulting in more accurate boundaries and reduced uncertainty. Our experiments on two public retinal fundus image datasets (Drishti-GS and RIM-ONE-r3) demonstrate that BEAL outperforms state-of-the-art unsupervised domain adaptation methods. The code is available at https://github.com/EmmaW8/BEAL.\n}"}, "2108.08109": {"original_text": "  Illustrations are an essential transmission instrument. For an historian, the\nfirst step in studying their evolution in a corpus of similar manuscripts is to\nidentify which ones correspond to each other. This image collation task is\ndaunting for manuscripts separated by many lost copies, spreading over\ncenturies, which might have been completely re-organized and greatly modified\nto adapt to novel knowledge or belief and include hundreds of illustrations.\nOur contributions in this paper are threefold. First, we introduce the task of\nillustration collation and a large annotated public dataset to evaluate\nsolutions, including 6 manuscripts of 2 different texts with more than 2 000\nillustrations and 1 200 annotated correspondences. Second, we analyze state of\nthe art similarity measures for this task and show that they succeed in simple\ncases but struggle for large manuscripts when the illustrations have undergone\nvery significant changes and are discriminated only by fine details. Finally,\nwe show clear evidence that significant performance boosts can be expected by\nexploiting cycle-consistent correspondences. Our code and data are available on\nhttp://imagine.enpc.fr/~shenx/ImageCollation.\n", "rewritten_text": "\\boxed{\nIllustrations play a crucial role in transmitting information, and historians face a significant challenge when trying to identify matching illustrations across multiple manuscripts, especially when they have undergone significant changes over time. To address this issue, we introduce the task of illustration collation and provide a large, annotated public dataset for evaluation, featuring 6 manuscripts with over 2,000 illustrations and 1,200 annotated correspondences. We analyze existing similarity measures and find that while they work well for simple cases, they struggle with large manuscripts where illustrations have undergone significant changes and can only be distinguished by subtle details. Our research suggests that exploiting cycle-consistent correspondences can lead to significant performance improvements. Our code and data are available at http://imagine.enpc.fr/~shenx/ImageCollation.\n}"}, "2204.01062": {"original_text": "  Several popular computer vision (CV) datasets, specifically employed for\nObject Detection (OD) in autonomous driving tasks exhibit biases due to a range\nof factors including weather and lighting conditions. These biases may impair a\nmodel's generalizability, rendering it ineffective for OD in novel and unseen\ndatasets. Especially, in autonomous driving, it may prove extremely high risk\nand unsafe for the vehicle and its surroundings. This work focuses on\nunderstanding these datasets better by identifying such \"good-weather\" bias.\nMethods to mitigate such bias which allows the OD models to perform better and\nimprove the robustness are also demonstrated. A simple yet effective OD\nframework for studying bias mitigation is proposed. Using this framework, the\nperformance on popular datasets is analyzed and a significant difference in\nmodel performance is observed. Additionally, a knowledge transfer technique and\na synthetic image corruption technique are proposed to mitigate the identified\nbias. Finally, using the DAWN dataset, the findings are validated on the OD\ntask, demonstrating the effectiveness of our techniques in mitigating\nreal-world \"good-weather\" bias. The experiments show that the proposed\ntechniques outperform baseline methods by averaged fourfold improvement.\n", "rewritten_text": "\\boxed{\nComputer vision datasets used for object detection in autonomous driving often contain biases due to factors like weather and lighting conditions, which can limit a model's ability to generalize and perform well in new, unseen datasets. This can be particularly dangerous in autonomous driving, where it can pose a high risk to the vehicle and its surroundings. This research aims to identify and mitigate these biases, specifically the \"good-weather\" bias, to improve the robustness of object detection models. A simple yet effective framework is proposed to study bias mitigation, and experiments show that the proposed techniques, including knowledge transfer and synthetic image corruption, can significantly improve model performance, outperforming baseline methods by an average of fourfold. The effectiveness of these techniques is validated using the DAWN dataset, demonstrating their ability to mitigate real-world biases."}, "2302.01097": {"original_text": "  Tree kernels have been proposed to be used in many areas as the automatic\nlearning of natural language applications. In this paper, we propose a new\nlinear time algorithm based on the concept of weighted tree automata for\nSubTree kernel computation. First, we introduce a new class of weighted tree\nautomata, called Root-Weighted Tree Automata, and their associated formal tree\nseries. Then we define, from this class, the SubTree automata that represent\ncompact computational models for finite tree languages. This allows us to\ndesign a theoretically guaranteed linear-time algorithm for computing the\nSubTree Kernel based on weighted tree automata intersection. The key idea\nbehind the proposed algorithm is to replace DAG reduction and nodes sorting\nsteps used in previous approaches by states equivalence classes computation\nallowed in the weighted tree automata approach. Our approach has three major\nadvantages: it is output-sensitive, it is free sensitive from the tree types\n(ordered trees versus unordered trees), and it is well adapted to any\nincremental tree kernel based learning methods. Finally, we conduct a variety\nof comparative experiments on a wide range of synthetic tree languages datasets\nadapted for a deep algorithm analysis. The obtained results show that the\nproposed algorithm outperforms state-of-the-art methods.\n", "rewritten_text": "\\boxed{\nThis paper presents a novel linear-time algorithm for computing the SubTree kernel, a crucial component in natural language processing applications. The approach leverages weighted tree automata, specifically Root-Weighted Tree Automata, to create compact models for finite tree languages. By exploiting the properties of these automata, the algorithm avoids the need for DAG reduction and node sorting, instead utilizing states equivalence classes computation. This results in an output-sensitive, tree-type-agnostic, and incremental-learning-friendly method. Experimental evaluations on diverse synthetic tree languages datasets demonstrate the superiority of the proposed algorithm over existing state-of-the-art methods.\n}"}, "2404.10877": {"original_text": "  In this paper, we aim to generate text classification data given arbitrary\nclass definitions (i.e., user instruction), so one can train a small text\nclassifier without any human annotation or raw corpus. Compared with pioneer\nattempts, our proposed Incubator is the first framework that can handle\ncomplicated and even mutually dependent classes (e.g., \"TED Talk given by\nEducator\" and \"Other\"). Specifically, Incubator is an LLM firstly tuned on the\ninstruction-to-data mappings that we obtained from classification datasets and\ndescriptions on HuggingFace together with in-context augmentation by GPT-4. We\nthen refine Incubator by learning on the cluster centers of semantic textual\nembeddings to emphasize the uniformity and semantic diversity in generations.\nWe compare Incubator on various classification tasks with strong baselines such\nas direct LLM-based inference and training data generation by prompt\nengineering. Experiments show Incubator is able to (1) perform well on\ntraditional benchmarks, (2) take label dependency and user preference into\nconsideration, and (3) enable logical text mining by incubating multiple\nclassifiers.\n", "rewritten_text": "\\boxed{This paper presents a novel framework, called Incubator, which can generate text classification data based on user-defined class definitions without requiring human annotation or a raw corpus. Unlike previous attempts, Incubator can handle complex and interdependent classes, such as \"TED Talk given by Educator\" and \"Other\". The framework uses a large language model (LLM) fine-tuned on instruction-to-data mappings and augmented with GPT-4, and is further refined by learning from cluster centers of semantic textual embeddings to ensure uniformity and diversity in generated data. Experimental results show that Incubator outperforms strong baselines, including direct LLM-based inference and training data generation by prompt engineering, and enables logical text mining by supporting multiple classifiers.}"}, "2210.09782": {"original_text": "  This paper focuses on developing a more effective method of hierarchical\npropagation for semi-supervised Video Object Segmentation (VOS). Based on\nvision transformers, the recently-developed Associating Objects with\nTransformers (AOT) approach introduces hierarchical propagation into VOS and\nhas shown promising results. The hierarchical propagation can gradually\npropagate information from past frames to the current frame and transfer the\ncurrent frame feature from object-agnostic to object-specific. However, the\nincrease of object-specific information will inevitably lead to the loss of\nobject-agnostic visual information in deep propagation layers. To solve such a\nproblem and further facilitate the learning of visual embeddings, this paper\nproposes a Decoupling Features in Hierarchical Propagation (DeAOT) approach.\nFirstly, DeAOT decouples the hierarchical propagation of object-agnostic and\nobject-specific embeddings by handling them in two independent branches.\nSecondly, to compensate for the additional computation from dual-branch\npropagation, we propose an efficient module for constructing hierarchical\npropagation, i.e., Gated Propagation Module, which is carefully designed with\nsingle-head attention. Extensive experiments show that DeAOT significantly\noutperforms AOT in both accuracy and efficiency. On YouTube-VOS, DeAOT can\nachieve 86.0% at 22.4fps and 82.0% at 53.4fps. Without test-time augmentations,\nwe achieve new state-of-the-art performance on four benchmarks, i.e.,\nYouTube-VOS (86.2%), DAVIS 2017 (86.2%), DAVIS 2016 (92.9%), and VOT 2020\n(0.622). Project page: https://github.com/z-x-yang/AOT.\n", "rewritten_text": "\\boxed{\nThis paper presents a novel approach to improve hierarchical propagation for semi-supervised Video Object Segmentation (VOS) using vision transformers. Building upon the Associating Objects with Transformers (AOT) method, which introduced hierarchical propagation to VOS, this paper proposes a Decoupling Features in Hierarchical Propagation (DeAOT) approach to address the issue of losing object-agnostic visual information in deep propagation layers. DeAOT separates the hierarchical propagation of object-agnostic and object-specific embeddings into two independent branches and utilizes a Gated Propagation Module with single-head attention to reduce computational costs. Experimental results demonstrate that DeAOT outperforms AOT in terms of accuracy and efficiency, achieving state-of-the-art performance on four benchmarks: YouTube-VOS, DAVIS 2017, DAVIS 2016, and VOT 2020."}, "2406.18173": {"original_text": "  Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases.\n", "rewritten_text": "\\boxed{Large language models struggle to manage long texts due to limited context window sizes. To address this, we propose UIO-LLMs, an unbiased incremental optimization approach for memory-enhanced transformers. Our method uses a streamlined encoder-decoder framework to process long texts in segments, storing context information in memories and using these memories to predict subsequent segments. By treating our models as recurrent neural networks, we can refine the training process using the Truncated Backpropagation Through Time algorithm, which reduces time complexity and eliminates bias in gradient computation. As a result, UIO-LLMs can handle extremely long contexts, such as increasing the context window of Llama2-7b-chat from 4,000 to 100,000 tokens with minimal additional parameters, while maintaining nearly linear inference costs.}"}, "2306.07713": {"original_text": "  Segment anything model (SAM), as the name suggests, is claimed to be capable\nof cutting out any object and demonstrates impressive zero-shot transfer\nperformance with the guidance of prompts. However, there is currently a lack of\ncomprehensive evaluation regarding its robustness under various corruptions.\nUnderstanding the robustness of SAM across different corruption scenarios is\ncrucial for its real-world deployment. Prior works show that SAM is biased\ntowards texture (style) rather than shape, motivated by which we start by\ninvestigating its robustness against style transfer, which is synthetic\ncorruption. Following by interpreting the effects of synthetic corruption as\nstyle changes, we proceed to conduct a comprehensive evaluation for its\nrobustness against 15 types of common corruption. These corruptions mainly fall\ninto categories such as digital, noise, weather, and blur, and within each\ncorruption category, we explore 5 severity levels to simulate real-world\ncorruption scenarios. Beyond the corruptions, we further assess the robustness\nof SAM against local occlusion and local adversarial patch attacks. To the best\nof our knowledge, our work is the first of its kind to evaluate the robustness\nof SAM under style change, local occlusion, and local adversarial patch\nattacks. Given that patch attacks visible to human eyes are easily detectable,\nwe further assess its robustness against global adversarial attacks that are\nimperceptible to human eyes. Overall, this work provides a comprehensive\nempirical study of the robustness of SAM, evaluating its performance under\nvarious corruptions and extending the assessment to critical aspects such as\nlocal occlusion, local adversarial patch attacks, and global adversarial\nattacks. These evaluations yield valuable insights into the practical\napplicability and effectiveness of SAM in addressing real-world challenges.\n", "rewritten_text": "\\boxed{\nThe Segment Anything Model (SAM) is a powerful tool that can extract objects from images with impressive zero-shot transfer performance. However, its robustness under various corruptions has not been thoroughly evaluated. To address this, we investigate SAM's robustness against style transfer, a synthetic corruption, and then evaluate its performance under 15 types of common corruptions, including digital, noise, weather, and blur, with 5 severity levels each. Additionally, we assess SAM's robustness against local occlusion, local adversarial patch attacks, and global adversarial attacks that are imperceptible to human eyes. Our comprehensive study provides valuable insights into SAM's practical applicability and effectiveness in real-world scenarios, highlighting its strengths and weaknesses.\n}"}, "2103.1399": {"original_text": "  A fundamental challenge faced by existing Fine-Grained Sketch-Based Image\nRetrieval (FG-SBIR) models is the data scarcity -- model performances are\nlargely bottlenecked by the lack of sketch-photo pairs. Whilst the number of\nphotos can be easily scaled, each corresponding sketch still needs to be\nindividually produced. In this paper, we aim to mitigate such an upper-bound on\nsketch data, and study whether unlabelled photos alone (of which they are many)\ncan be cultivated for performances gain. In particular, we introduce a novel\nsemi-supervised framework for cross-modal retrieval that can additionally\nleverage large-scale unlabelled photos to account for data scarcity. At the\ncentre of our semi-supervision design is a sequential photo-to-sketch\ngeneration model that aims to generate paired sketches for unlabelled photos.\nImportantly, we further introduce a discriminator guided mechanism to guide\nagainst unfaithful generation, together with a distillation loss based\nregularizer to provide tolerance against noisy training samples. Last but not\nleast, we treat generation and retrieval as two conjugate problems, where a\njoint learning procedure is devised for each module to mutually benefit from\neach other. Extensive experiments show that our semi-supervised model yields\nsignificant performance boost over the state-of-the-art supervised\nalternatives, as well as existing methods that can exploit unlabelled photos\nfor FG-SBIR.\n", "rewritten_text": "\\boxed{\nExisting Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) models are limited by the scarcity of sketch-photo pairs, which hinders their performance. While it's easy to increase the number of photos, creating corresponding sketches is a time-consuming task. This paper proposes a novel semi-supervised framework to overcome this limitation by utilizing large-scale unlabelled photos to improve performance. The framework consists of a sequential photo-to-sketch generation model that generates paired sketches for unlabelled photos, guided by a discriminator to prevent unfaithful generation and a distillation loss-based regularizer to handle noisy samples. The generation and retrieval modules are jointly learned to benefit from each other. Experimental results show that the proposed semi-supervised model outperforms state-of-the-art supervised models and existing methods that exploit unlabelled photos for FG-SBIR."}, "1412.7854": {"original_text": "  Traditional object recognition approaches apply feature extraction, part\ndeformation handling, occlusion handling and classification sequentially while\nthey are independent from each other. Ouyang and Wang proposed a model for\njointly learning of all of the mentioned processes using one deep neural\nnetwork. We utilized, and manipulated their toolbox in order to apply it in car\ndetection scenarios where it had not been tested. Creating a single deep\narchitecture from these components, improves the interaction between them and\ncan enhance the performance of the whole system. We believe that the approach\ncan be used as a general purpose object detection toolbox. We tested the\nalgorithm on UIUC car dataset, and achieved an outstanding result. The accuracy\nof our method was 97 % while the previously reported results showed an accuracy\nof up to 91 %. We strongly believe that having an experiment on a larger\ndataset can show the advantage of using deep models over shallow ones.\n", "rewritten_text": "\\boxed{Conventional object recognition methods involve separate steps for feature extraction, part deformation handling, occlusion handling, and classification. However, Ouyang and Wang introduced a deep neural network that integrates all these processes into a single model. We adapted and modified their toolbox to apply it to car detection, an untested area, and achieved remarkable results. By combining these components into a unified deep architecture, we improved their interaction and enhanced the overall system's performance. We believe this approach can serve as a general-purpose object detection toolbox. Our method achieved an accuracy of 97% on the UIUC car dataset, outperforming previous results of up to 91%. We are confident that testing on a larger dataset will further demonstrate the superiority of deep models over shallow ones.}"}, "1905.03672": {"original_text": "  In this paper, we are interested in boosting the representation capability of\nconvolution neural networks which utilizing the inverted residual structure.\nBased on the success of Inverted Residual structure[Sandler et al. 2018] and\nInterleaved Low-Rank Group Convolutions[Sun et al. 2018], we rethink this two\npattern of neural network structure, rather than NAS(Neural architecture\nsearch) method[Zoph and Le 2017; Pham et al. 2018; Liu et al. 2018b], we\nintroduce uneven point-wise group convolution, which provide a novel search\nspace for designing basic blocks to obtain better trade-off between\nrepresentation capability and computational cost. Meanwhile, we propose two\nnovel information flow patterns that will enable cross-group information flow\nfor multiple group convolution layers with and without any channel\npermute/shuffle operation. Dense experiments on image classification task show\nthat our proposed model, named Seesaw-Net, achieves state-of-the-art(SOTA)\nperformance with limited computation and memory cost. Our code will be\nopen-source and available together with pre-trained models.\n", "rewritten_text": "\\boxed{This paper focuses on enhancing the representation capabilities of convolutional neural networks that employ the inverted residual structure. Building on the successes of the Inverted Residual structure and Interleaved Low-Rank Group Convolutions, we revisit these two neural network patterns and introduce uneven point-wise group convolution, providing a new search space for designing basic blocks that balance representation capability and computational cost. We also propose two novel information flow patterns that enable cross-group information flow for multiple group convolution layers, with and without channel permutation/shuffling. Our proposed model, Seesaw-Net, achieves state-of-the-art performance on image classification tasks with limited computational and memory costs, and our code and pre-trained models will be made publicly available.}"}, "1912.1163": {"original_text": "  Person re-identification has attracted many researchers' attention for its\nwide application, but it is still a very challenging task because only part of\nthe image information can be used for personnel matching. Most of current\nmethods uses CNN to learn to embeddings that can capture semantic similarity\ninformation among data points. Many of the state-of-the-arts methods use\ncomplex network structures with multiple branches that fuse multiple features\nwhile training or testing, using classification loss, Triplet loss or a\ncombination of the two as loss function. However, the method that using Triplet\nloss as loss function converges slowly, and the method in which pull features\nof the same class as close as possible in features space leads to poor feature\nstability. This paper will combine the ranking motivated structured loss,\nproposed a new metric learning loss function that make the features of the same\nclass are sparsely distributed into the range of small hyperspheres and the\nfeatures of different classes are uniformly distributed at a clearly angle. And\nadopted a new single-branch network structure that only using global feature\ncan also get great performance. The validity of our method is verified on the\nMarket1501 and DukeMTMC-ReID person re-identification datasets. Finally\nacquires 90.9% rank-1 accuracy and 80.8% mAP on DukeMTMC-reID, 95.3% rank-1\naccuracy and 88.7% mAP on Market1501. Codes and models are available in\nGithub.https://github.com/Qidian213/Ranked_Person_ReID.\n", "rewritten_text": "\\boxed{\nPerson re-identification, a challenging task due to limited image information, has garnered significant attention from researchers. Current methods employ CNNs to learn embeddings that capture semantic similarity among data points. State-of-the-art methods utilize complex network structures with multiple branches, combining features during training or testing, and leveraging classification loss, Triplet loss, or a combination of both. However, Triplet loss-based methods converge slowly, and methods that pull same-class features together suffer from poor feature stability. This paper proposes a novel metric learning loss function, combining ranking-motivated structured loss, which distributes same-class features sparsely within small hyperspheres and different-class features uniformly at a clear angle. A new single-branch network structure, relying solely on global features, achieves excellent performance. The method's validity is demonstrated on the Market1501 and DukeMTMC-ReID person re-identification datasets, yielding 90.9% rank-1 accuracy and 80.8% mAP on DukeMTMC-ReID, and 95.3% rank-1 accuracy and 88.7% mAP on Market1501. The code and models are available on Github at https://github.com/Qidian213/Ranked_Person_ReID.\n}"}, "1907.1144": {"original_text": "  Pooling is one of the main elements in convolutional neural networks. The\npooling reduces the size of the feature map, enabling training and testing with\na limited amount of computation. This paper proposes a new pooling method named\nuniversal pooling. Unlike the existing pooling methods such as average pooling,\nmax pooling, and stride pooling with fixed pooling function, universal pooling\ngenerates any pooling function, depending on a given problem and dataset.\nUniversal pooling was inspired by attention methods and can be considered as a\nchannel-wise form of local spatial attention. Universal pooling is trained\njointly with the main network and it is shown that it includes the existing\npooling methods. Finally, when applied to two benchmark problems, the proposed\nmethod outperformed the existing pooling methods and performed with the\nexpected diversity, adapting to the given problem.\n", "rewritten_text": "\\boxed{This paper introduces a novel pooling method called universal pooling, which can generate any pooling function based on the problem and dataset at hand. Unlike traditional pooling methods like average pooling, max pooling, and stride pooling, which use fixed functions, universal pooling is a flexible and adaptive approach inspired by attention methods. It can be seen as a form of local spatial attention applied channel-wise. By training universal pooling jointly with the main network, it encompasses existing pooling methods and outperforms them on two benchmark problems, demonstrating its ability to adapt to different tasks.}"}, "2406.17236": {"original_text": "  Although recent years have witnessed significant advancements in image\nediting thanks to the remarkable progress of text-to-image diffusion models,\nthe problem of non-rigid image editing still presents its complexities and\nchallenges. Existing methods often fail to achieve consistent results due to\nthe absence of unique identity characteristics. Thus, learning a personalized\nidentity prior might help with consistency in the edited results. In this\npaper, we explore a novel task: learning the personalized identity prior for\ntext-based non-rigid image editing. To address the problems in jointly learning\nprior and editing the image, we present LIPE, a two-stage framework designed to\ncustomize the generative model utilizing a limited set of images of the same\nsubject, and subsequently employ the model with learned prior for non-rigid\nimage editing. Experimental results demonstrate the advantages of our approach\nin various editing scenarios over past related leading methods in qualitative\nand quantitative ways.\n", "rewritten_text": "\\boxed{\nDespite significant progress in image editing using text-to-image diffusion models, non-rigid image editing remains a challenging task. Current methods often produce inconsistent results due to the lack of unique identity features. To address this, we propose learning a personalized identity prior to improve consistency in edited images. We introduce LIPE, a two-stage framework that customizes a generative model using a limited set of images of the same subject, and then uses the learned prior for non-rigid image editing. Our approach outperforms leading methods in both qualitative and quantitative evaluations.\n}"}, "2406.12679": {"original_text": "  Large Language Models (LLMs) are increasingly being used in educational and\nlearning applications. Research has demonstrated that controlling for style, to\nfit the needs of the learner, fosters increased understanding, promotes\ninclusion, and helps with knowledge distillation. To understand the\ncapabilities and limitations of contemporary LLMs in style control, we\nevaluated five state-of-the-art models: GPT-3.5, GPT-4, GPT-4o, Llama-3, and\nMistral-instruct- 7B across two style control tasks. We observed significant\ninconsistencies in the first task, with model performances averaging between\n5th and 8th grade reading levels for tasks intended for first-graders, and\nstandard deviations up to 27.6. For our second task, we observed a\nstatistically significant improvement in performance from 0.02 to 0.26.\nHowever, we find that even without stereotypes in reference texts, LLMs often\ngenerated culturally insensitive content during their tasks. We provide a\nthorough analysis and discussion of the results.\n", "rewritten_text": "\\boxed{Large Language Models (LLMs) are being increasingly used in educational settings, and research has shown that adapting the style of the content to suit the learner's needs can improve understanding, promote inclusivity, and facilitate knowledge retention. To assess the capabilities and limitations of current LLMs in style control, we evaluated five top-performing models across two tasks. Our results revealed significant inconsistencies in the first task, with models producing content at a 5th-8th grade reading level for tasks intended for 1st-graders, and large standard deviations. In the second task, we observed a significant improvement in performance. However, we found that even without biased reference texts, LLMs often generated culturally insensitive content. We provide a detailed analysis and discussion of our findings.}"}, "1907.06882": {"original_text": "  Majority of state-of-the-art monocular depth estimation methods are\nsupervised learning approaches. The success of such approaches heavily depends\non the high-quality depth labels which are expensive to obtain. Some recent\nmethods try to learn depth networks by leveraging unsupervised cues from\nmonocular videos which are easier to acquire but less reliable. In this paper,\nwe propose to resolve this dilemma by transferring knowledge from synthetic\nvideos with easily obtainable ground-truth depth labels. Due to the stylish\ndifference between synthetic and real images, we propose a\ntemporally-consistent domain adaptation (TCDA) approach that simultaneously\nexplores labels in the synthetic domain and temporal constraints in the videos\nto improve style transfer and depth prediction. Furthermore, we make use of the\nground-truth optical flow and pose information in the synthetic data to learn\nmoving mask and pose prediction networks. The learned moving masks can filter\nout moving regions that produces erroneous temporal constraints and the\nestimated poses provide better initializations for estimating temporal\nconstraints. Experimental results demonstrate the effectiveness of our method\nand comparable performance against state-of-the-art.\n", "rewritten_text": "\\boxed{Most advanced monocular depth estimation methods rely on supervised learning, which requires high-quality depth labels that are costly to obtain. Recent approaches have attempted to overcome this limitation by using unsupervised cues from monocular videos, which are easier to acquire but less reliable. This paper proposes a novel solution by transferring knowledge from synthetic videos with readily available ground-truth depth labels. To address the style difference between synthetic and real images, we introduce a temporally-consistent domain adaptation approach that leverages labels in the synthetic domain and temporal constraints in videos to improve style transfer and depth prediction. Additionally, we utilize ground-truth optical flow and pose information in synthetic data to learn moving mask and pose prediction networks, which help filter out moving regions and provide better initializations for estimating temporal constraints. Our experimental results show the effectiveness of our method, achieving comparable performance to state-of-the-art approaches.}"}, "2104.14839": {"original_text": "  Recently, various neural encoder-decoder models pioneered by Seq2Seq\nframework have been proposed to achieve the goal of generating more abstractive\nsummaries by learning to map input text to output text. At a high level, such\nneural models can freely generate summaries without any constraint on the words\nor phrases used. Moreover, their format is closer to human-edited summaries and\noutput is more readable and fluent. However, the neural model's abstraction\nability is a double-edged sword. A commonly observed problem with the generated\nsummaries is the distortion or fabrication of factual information in the\narticle. This inconsistency between the original text and the summary has\ncaused various concerns over its applicability, and the previous evaluation\nmethods of text summarization are not suitable for this issue. In response to\nthe above problems, the current research direction is predominantly divided\ninto two categories, one is to design fact-aware evaluation metrics to select\noutputs without factual inconsistency errors, and the other is to develop new\nsummarization systems towards factual consistency. In this survey, we focus on\npresenting a comprehensive review of these fact-specific evaluation methods and\ntext summarization models.\n", "rewritten_text": "\\boxed{Neural encoder-decoder models, such as Seq2Seq, have been developed to generate more abstract summaries by learning to map input text to output text. While these models can produce summaries that are more readable and fluent, they often struggle with factual inconsistencies, distorting or fabricating information from the original article. To address this issue, researchers are working on two main approaches: designing evaluation metrics that can detect factual errors and developing new summarization systems that prioritize factual consistency. This survey provides a comprehensive review of these fact-specific evaluation methods and text summarization models.}"}, "1701.01619": {"original_text": "  We present an approach to effectively use millions of images with noisy\nannotations in conjunction with a small subset of cleanly-annotated images to\nlearn powerful image representations. One common approach to combine clean and\nnoisy data is to first pre-train a network using the large noisy dataset and\nthen fine-tune with the clean dataset. We show this approach does not fully\nleverage the information contained in the clean set. Thus, we demonstrate how\nto use the clean annotations to reduce the noise in the large dataset before\nfine-tuning the network using both the clean set and the full set with reduced\nnoise. The approach comprises a multi-task network that jointly learns to clean\nnoisy annotations and to accurately classify images. We evaluate our approach\non the recently released Open Images dataset, containing ~9 million images,\nmultiple annotations per image and over 6000 unique classes. For the small\nclean set of annotations we use a quarter of the validation set with ~40k\nimages. Our results demonstrate that the proposed approach clearly outperforms\ndirect fine-tuning across all major categories of classes in the Open Image\ndataset. Further, our approach is particularly effective for a large number of\nclasses with wide range of noise in annotations (20-80% false positive\nannotations).\n", "rewritten_text": "\\boxed{We propose a novel approach to leverage both noisy and clean image annotations to learn robust image representations. Unlike traditional methods that pre-train on noisy data and fine-tune on clean data, our approach uses clean annotations to denoise the large dataset before fine-tuning. We achieve this through a multi-task network that simultaneously cleans noisy annotations and classifies images. Evaluating our approach on the Open Images dataset, we demonstrate significant improvements over direct fine-tuning, particularly for classes with high noise levels (20-80% false positives).}"}, "1905.04215": {"original_text": "  We study the problem of unsupervised domain adaptation which aims to adapt\nmodels trained on a labeled source domain to a completely unlabeled target\ndomain. Recently, the cluster assumption has been applied to unsupervised\ndomain adaptation and achieved strong performance. One critical factor in\nsuccessful training of the cluster assumption is to impose the\nlocally-Lipschitz constraint to the model. Existing methods only impose the\nlocally-Lipschitz constraint around the training points while miss the other\nareas, such as the points in-between training data. In this paper, we address\nthis issue by encouraging the model to behave linearly in-between training\npoints. We propose a new regularization method called Virtual Mixup Training\n(VMT), which is able to incorporate the locally-Lipschitz constraint to the\nareas in-between training data. Unlike the traditional mixup model, our method\nconstructs the combination samples without using the label information,\nallowing it to apply to unsupervised domain adaptation. The proposed method is\ngeneric and can be combined with most existing models such as the recent\nstate-of-the-art model called VADA. Extensive experiments demonstrate that VMT\nsignificantly improves the performance of VADA on six domain adaptation\nbenchmark datasets. For the challenging task of adapting MNIST to SVHN, VMT can\nimprove the accuracy of VADA by over 30\\%. Code is available at\n\\url{https://github.com/xudonmao/VMT}.\n", "rewritten_text": "\\boxed{\nThis paper tackles the problem of unsupervised domain adaptation, where models trained on a labeled source domain are adapted to an unlabeled target domain. The cluster assumption has been successfully applied to this problem, but its success relies on imposing a locally-Lipschitz constraint on the model. Existing methods only apply this constraint to areas around the training points, neglecting other areas. To address this, we propose Virtual Mixup Training (VMT), a new regularization method that encourages the model to behave linearly in-between training points, effectively incorporating the locally-Lipschitz constraint to these areas. Unlike traditional mixup models, VMT doesn't require label information, making it suitable for unsupervised domain adaptation. Our method is generic and can be combined with existing models, such as VADA. Experimental results on six benchmark datasets show that VMT significantly improves VADA's performance, with an accuracy improvement of over 30% when adapting MNIST to SVHN. The code is available at https://github.com/xudonmao/VMT.\n}"}, "2311.17590": {"original_text": "  Achieving high synchronization in the synthesis of realistic, speech-driven\ntalking head videos presents a significant challenge. Traditional Generative\nAdversarial Networks (GAN) struggle to maintain consistent facial identity,\nwhile Neural Radiance Fields (NeRF) methods, although they can address this\nissue, often produce mismatched lip movements, inadequate facial expressions,\nand unstable head poses. A lifelike talking head requires synchronized\ncoordination of subject identity, lip movements, facial expressions, and head\nposes. The absence of these synchronizations is a fundamental flaw, leading to\nunrealistic and artificial outcomes. To address the critical issue of\nsynchronization, identified as the \"devil\" in creating realistic talking heads,\nwe introduce SyncTalk. This NeRF-based method effectively maintains subject\nidentity, enhancing synchronization and realism in talking head synthesis.\nSyncTalk employs a Face-Sync Controller to align lip movements with speech and\ninnovatively uses a 3D facial blendshape model to capture accurate facial\nexpressions. Our Head-Sync Stabilizer optimizes head poses, achieving more\nnatural head movements. The Portrait-Sync Generator restores hair details and\nblends the generated head with the torso for a seamless visual experience.\nExtensive experiments and user studies demonstrate that SyncTalk outperforms\nstate-of-the-art methods in synchronization and realism. We recommend watching\nthe supplementary video: https://ziqiaopeng.github.io/synctalk\n", "rewritten_text": "\\boxed{\nCreating realistic, speech-driven talking head videos is a significant challenge due to the difficulty in achieving high synchronization. Traditional GANs struggle to maintain consistent facial identity, while NeRF methods often produce mismatched lip movements, inadequate facial expressions, and unstable head poses. A realistic talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. To address this critical issue, we propose SyncTalk, a NeRF-based method that effectively maintains subject identity and enhances synchronization and realism in talking head synthesis. SyncTalk uses a Face-Sync Controller to align lip movements with speech, a 3D facial blendshape model to capture accurate facial expressions, and a Head-Sync Stabilizer to optimize head poses. Additionally, the Portrait-Sync Generator restores hair details and blends the generated head with the torso for a seamless visual experience. Our experiments and user studies demonstrate that SyncTalk outperforms state-of-the-art methods in synchronization and realism. A supplementary video is available at https://ziqiaopeng.github.io/synctalk.\n}"}, "1901.07656": {"original_text": "  Word vector representations are well developed tools for various NLP and\nMachine Learning tasks and are known to retain significant semantic and\nsyntactic structure of languages. But they are prone to carrying and amplifying\nbias which can perpetrate discrimination in various applications. In this work,\nwe explore new simple ways to detect the most stereotypically gendered words in\nan embedding and remove the bias from them. We verify how names are masked\ncarriers of gender bias and then use that as a tool to attenuate bias in\nembeddings. Further, we extend this property of names to show how names can be\nused to detect other types of bias in the embeddings such as bias based on\nrace, ethnicity, and age.\n", "rewritten_text": "\\boxed{Word vector representations, a powerful tool in NLP and Machine Learning, capture language structures but often perpetuate biases, leading to discrimination. This study proposes novel methods to identify and remove gender bias from embeddings, leveraging the fact that names often conceal gender bias. Furthermore, it explores the potential of names in detecting other biases, such as those based on race, ethnicity, and age, in embeddings.}"}, "2210.02318": {"original_text": "  Recently, two-stage Deformable DETR introduced the query-based two-stage\nhead, a new type of two-stage head different from the region-based two-stage\nheads of classical detectors as Faster R-CNN. In query-based two-stage heads,\nthe second stage selects one feature per detection processed by a transformer,\ncalled the query, as opposed to pooling a rectangular grid of features\nprocessed by CNNs as in region-based detectors. In this work, we improve the\nquery-based head by improving the prior of the cross-attention operation with\nanchors, significantly speeding up the convergence while increasing its\nperformance. Additionally, we empirically show that by improving the\ncross-attention prior, auxiliary losses and iterative bounding box mechanisms\ntypically used by DETR-based detectors are no longer needed. By combining the\nbest of both the classical and the DETR-based detectors, our FQDet head peaks\nat 45.4 AP on the 2017 COCO validation set when using a ResNet-50+TPN backbone,\nonly after training for 12 epochs using the 1x schedule. We outperform other\nhigh-performing two-stage heads such as e.g. Cascade R-CNN, while using the\nsame backbone and while being computationally cheaper. Additionally, when using\nthe large ResNeXt-101-DCN+TPN backbone and multi-scale testing, our FQDet head\nachieves 52.9 AP on the 2017 COCO test-dev set after only 12 epochs of\ntraining. Code is released at https://github.com/CedricPicron/FQDet .\n", "rewritten_text": "\\boxed{\nRecently, a new type of two-stage head, called query-based two-stage head, was introduced in Deformable DETR. This approach differs from classical detectors like Faster R-CNN, which use region-based two-stage heads. In query-based heads, a transformer processes one feature per detection, called the query, whereas region-based detectors pool a rectangular grid of features using CNNs. We improved the query-based head by enhancing the prior of the cross-attention operation with anchors, resulting in faster convergence and better performance. Our experiments show that improving the cross-attention prior eliminates the need for auxiliary losses and iterative bounding box mechanisms commonly used in DETR-based detectors. By combining the strengths of classical and DETR-based detectors, our FQDet head achieves 45.4 AP on the 2017 COCO validation set with a ResNet-50+TPN backbone after only 12 epochs of training. We outperform other high-performing two-stage heads, such as Cascade R-CNN, while being computationally more efficient. With a larger ResNeXt-101-DCN+TPN backbone and multi-scale testing, our FQDet head reaches 52.9 AP on the 2017 COCO test-dev set after 12 epochs of training. The code is available at https://github.com/CedricPicron/FQDet.\n}"}, "1411.634": {"original_text": "  While widely acknowledged as highly effective in computer vision, multi-label\nMRFs with non-convex priors are difficult to optimize. To tackle this, we\nintroduce an algorithm that iteratively approximates the original energy with\nan appropriately weighted surrogate energy that is easier to minimize. Our\nalgorithm guarantees that the original energy decreases at each iteration. In\nparticular, we consider the scenario where the global minimizer of the weighted\nsurrogate energy can be obtained by a multi-label graph cut algorithm, and show\nthat our algorithm then lets us handle of large variety of non-convex priors.\nWe demonstrate the benefits of our method over state-of-the-art MRF energy\nminimization techniques on stereo and inpainting problems.\n", "rewritten_text": "\\boxed{We propose an algorithm to optimize multi-label Markov Random Fields (MRFs) with non-convex priors, which are challenging to optimize despite their effectiveness in computer vision. Our approach iteratively approximates the original energy function with a weighted surrogate function that is easier to minimize, ensuring a decrease in the original energy at each iteration. By leveraging multi-label graph cut algorithms, our method can handle a wide range of non-convex priors. We demonstrate the superiority of our approach over existing MRF energy minimization techniques in stereo and inpainting applications.}"}, "2009.02649": {"original_text": "  Causality visualization can help people understand temporal chains of events,\nsuch as messages sent in a distributed system, cause and effect in a historical\nconflict, or the interplay between political actors over time. However, as the\nscale and complexity of these event sequences grows, even these visualizations\ncan become overwhelming to use. In this paper, we propose the use of textual\nnarratives as a data-driven storytelling method to augment causality\nvisualization. We first propose a design space for how textual narratives can\nbe used to describe causal data. We then present results from a crowdsourced\nuser study where participants were asked to recover causality information from\ntwo causality visualizations--causal graphs and Hasse diagrams--with and\nwithout an associated textual narrative. Finally, we describe CAUSEWORKS, a\ncausality visualization system for understanding how specific interventions\ninfluence a causal model. The system incorporates an automatic textual\nnarrative mechanism based on our design space. We validate CAUSEWORKS through\ninterviews with experts who used the system for understanding complex events.\n", "rewritten_text": "\\boxed{Visualizing causal relationships between events can be challenging, especially when dealing with complex and large-scale data. To address this, we propose using textual narratives to enhance causality visualization. We define a design space for incorporating narratives into causal data and conduct a user study to evaluate the effectiveness of this approach. Our results show that adding textual narratives to visualizations, such as causal graphs and Hasse diagrams, improves users' ability to understand causal relationships. We also introduce CAUSEWORKS, a system that generates automatic textual narratives to facilitate understanding of complex events and interventions. Expert feedback validates the effectiveness of CAUSEWORKS in facilitating comprehension of intricate causal relationships.}"}, "2204.07183": {"original_text": "  We propose an interactive approach for 3D instance segmentation, where users\ncan iteratively collaborate with a deep learning model to segment objects in a\n3D point cloud directly. Current methods for 3D instance segmentation are\ngenerally trained in a fully-supervised fashion, which requires large amounts\nof costly training labels, and does not generalize well to classes unseen\nduring training. Few works have attempted to obtain 3D segmentation masks using\nhuman interactions. Existing methods rely on user feedback in the 2D image\ndomain. As a consequence, users are required to constantly switch between 2D\nimages and 3D representations, and custom architectures are employed to combine\nmultiple input modalities. Therefore, integration with existing standard 3D\nmodels is not straightforward. The core idea of this work is to enable users to\ninteract directly with 3D point clouds by clicking on desired 3D objects of\ninterest~(or their background) to interactively segment the scene in an\nopen-world setting. Specifically, our method does not require training data\nfrom any target domain, and can adapt to new environments where no appropriate\ntraining sets are available. Our system continuously adjusts the object\nsegmentation based on the user feedback and achieves accurate dense 3D\nsegmentation masks with minimal human effort (few clicks per object). Besides\nits potential for efficient labeling of large-scale and varied 3D datasets, our\napproach, where the user directly interacts with the 3D environment, enables\nnew applications in AR/VR and human-robot interaction.\n", "rewritten_text": "\\boxed{\nWe introduce an interactive 3D instance segmentation approach that allows users to collaborate with a deep learning model to directly segment objects in a 3D point cloud. Unlike traditional fully-supervised methods that require large amounts of labeled training data and struggle to generalize to unseen classes, our approach enables users to interactively segment objects in a 3D environment without requiring training data from the target domain. By clicking on objects of interest or their background, users can iteratively refine the segmentation, achieving accurate and dense 3D masks with minimal effort. This approach has the potential to efficiently label large-scale and varied 3D datasets, and enables new applications in augmented reality, virtual reality, and human-robot interaction."}, "2012.0536": {"original_text": "  Semantic aware reconstruction is more advantageous than geometric-only\nreconstruction for future robotic and AR/VR applications because it represents\nnot only where things are, but also what things are. Object-centric mapping is\na task to build an object-level reconstruction where objects are separate and\nmeaningful entities that convey both geometry and semantic information. In this\npaper, we present MOLTR, a solution to object-centric mapping using only\nmonocular image sequences and camera poses. It is able to localise, track, and\nreconstruct multiple objects in an online fashion when an RGB camera captures a\nvideo of the surrounding. Given a new RGB frame, MOLTR firstly applies a\nmonocular 3D detector to localise objects of interest and extract their shape\ncodes that represent the object shapes in a learned embedding space. Detections\nare then merged to existing objects in the map after data association. Motion\nstate (i.e. kinematics and the motion status) of each object is tracked by a\nmultiple model Bayesian filter and object shape is progressively refined by\nfusing multiple shape code. We evaluate localisation, tracking, and\nreconstruction on benchmarking datasets for indoor and outdoor scenes, and show\nsuperior performance over previous approaches.\n", "rewritten_text": "\\boxed{Semantic-aware reconstruction is superior to geometric-only reconstruction for future robotic and AR/VR applications because it provides both spatial and semantic information about objects. Object-centric mapping, which involves building an object-level reconstruction with separate and meaningful entities that convey geometry and semantics, is a crucial task. This paper introduces MOLTR, a monocular image sequence-based solution for object-centric mapping that can localize, track, and reconstruct multiple objects in real-time. MOLTR uses a monocular 3D detector to identify objects of interest, extracts their shape codes, and refines their shapes by fusing multiple codes. The system also tracks the motion state of each object using a Bayesian filter. Experimental results on benchmarking datasets for indoor and outdoor scenes demonstrate the superior performance of MOLTR compared to previous approaches.}"}, "1605.06052": {"original_text": "  Similarity scores in face recognition represent the proximity between pairs\nof images as computed by a matching algorithm. Given a large set of images and\nthe proximities between all pairs, a similarity score space is defined. Cluster\nanalysis was applied to the similarity score space to develop various\ntaxonomies. Given the number of subjects in the dataset, we used hierarchical\nmethods to aggregate images of the same subject. We also explored the hierarchy\nabove and below the subject level, including clusters that reflect gender and\nethnicity. Evidence supports the existence of clustering by race, gender,\nsubject, and illumination condition.\n", "rewritten_text": "\\boxed{In face recognition, similarity scores measure the closeness between image pairs, computed by a matching algorithm. By analyzing these scores, a similarity score space is created. Applying cluster analysis to this space, various taxonomies were developed. Using hierarchical methods, images of the same individual were grouped together, and further exploration revealed clusters based on gender, ethnicity, and lighting conditions, supporting the existence of clustering by these factors.}"}, "2401.00653": {"original_text": "  Deceptive images can be shared in seconds with social networking services,\nposing substantial risks. Tampering traces, such as boundary artifacts and\nhigh-frequency information, have been significantly emphasized by massive\nnetworks in the Image Manipulation Localization (IML) field. However, they are\nprone to image post-processing operations, which limit the generalization and\nrobustness of existing methods. We present a novel Prompt-IML framework. We\nobserve that humans tend to discern the authenticity of an image based on both\nsemantic and high-frequency information, inspired by which, the proposed\nframework leverages rich semantic knowledge from pre-trained visual foundation\nmodels to assist IML. We are the first to design a framework that utilizes\nvisual foundation models specially for the IML task. Moreover, we design a\nFeature Alignment and Fusion module to align and fuse features of semantic\nfeatures with high-frequency features, which aims at locating tampered regions\nfrom multiple perspectives. Experimental results demonstrate that our model can\nachieve better performance on eight typical fake image datasets and outstanding\nrobustness.\n", "rewritten_text": "\\boxed{\nDeceptive images can spread rapidly through social media, posing significant risks. While current methods in Image Manipulation Localization (IML) focus on tampering traces like boundary artifacts and high-frequency information, they are vulnerable to image post-processing operations, limiting their generalizability and robustness. Our novel Prompt-IML framework addresses this by leveraging rich semantic knowledge from pre-trained visual foundation models to aid IML, inspired by how humans assess image authenticity based on both semantic and high-frequency information. We introduce a Feature Alignment and Fusion module to combine semantic and high-frequency features, enabling the detection of tampered regions from multiple angles. Our approach outperforms existing methods on eight fake image datasets and demonstrates exceptional robustness.\n}"}, "1809.08495": {"original_text": "  Earlier work demonstrates the promise of deep-learning-based approaches for\npoint cloud segmentation; however, these approaches need to be improved to be\npractically useful. To this end, we introduce a new model SqueezeSegV2 that is\nmore robust to dropout noise in LiDAR point clouds. With improved model\nstructure, training loss, batch normalization and additional input channel,\nSqueezeSegV2 achieves significant accuracy improvement when trained on real\ndata. Training models for point cloud segmentation requires large amounts of\nlabeled point-cloud data, which is expensive to obtain. To sidestep the cost of\ncollection and annotation, simulators such as GTA-V can be used to create\nunlimited amounts of labeled, synthetic data. However, due to domain shift,\nmodels trained on synthetic data often do not generalize well to the real\nworld. We address this problem with a domain-adaptation training pipeline\nconsisting of three major components: 1) learned intensity rendering, 2)\ngeodesic correlation alignment, and 3) progressive domain calibration. When\ntrained on real data, our new model exhibits segmentation accuracy improvements\nof 6.0-8.6% over the original SqueezeSeg. When training our new model on\nsynthetic data using the proposed domain adaptation pipeline, we nearly double\ntest accuracy on real-world data, from 29.0% to 57.4%. Our source code and\nsynthetic dataset will be open-sourced.\n", "rewritten_text": "\\boxed{\nPrevious research has shown that deep-learning-based methods are promising for point cloud segmentation, but they need to be improved for practical use. To address this, we propose a new model called SqueezeSegV2, which is more robust to noise in LiDAR point clouds. With its improved architecture, training loss, batch normalization, and additional input channel, SqueezeSegV2 achieves significant accuracy improvements when trained on real data. However, collecting and annotating large amounts of labeled point-cloud data is expensive. To overcome this, we use simulators like GTA-V to generate unlimited synthetic data. Nevertheless, models trained on synthetic data often fail to generalize well to real-world data due to domain shift. To tackle this, we introduce a domain-adaptation training pipeline consisting of learned intensity rendering, geodesic correlation alignment, and progressive domain calibration. Our new model shows 6.0-8.6% accuracy improvements over the original SqueezeSeg when trained on real data. Moreover, when trained on synthetic data using our domain adaptation pipeline, our model nearly doubles the test accuracy on real-world data, from 29.0% to 57.4%. We will open-source our code and synthetic dataset.\n}"}, "2209.11214": {"original_text": "  Automatic tomato disease recognition from leaf images is vital to avoid crop\nlosses by applying control measures on time. Even though recent deep\nlearning-based tomato disease recognition methods with classical training\nprocedures showed promising recognition results, they demand large labelled\ndata and involve expensive training. The traditional deep learning models\nproposed for tomato disease recognition also consume high memory and storage\nbecause of a high number of parameters. While lightweight networks overcome\nsome of these issues to a certain extent, they continue to show low performance\nand struggle to handle imbalanced data. In this paper, a novel Siamese\nnetwork-based lightweight framework is proposed for automatic tomato leaf\ndisease recognition. This framework achieves the highest accuracy of 96.97% on\nthe tomato subset obtained from the PlantVillage dataset and 95.48% on the\nTaiwan tomato leaf disease dataset. Experimental results further confirm that\nthe proposed framework is effective with imbalanced and small data. The\nbackbone deep network integrated with this framework is lightweight with\napproximately 2.9629 million trainable parameters, which is way lower than\nexisting lightweight deep networks.\n", "rewritten_text": "\\boxed{Recognizing tomato diseases from leaf images is crucial to prevent crop losses, but current deep learning methods require large amounts of labeled data and expensive training. Traditional models also consume high memory and storage due to their complex architecture. To address these issues, this paper proposes a novel lightweight framework based on Siamese networks, which achieves high accuracy (96.97% and 95.48%) on two datasets and performs well with imbalanced and limited data. The framework's backbone network is also extremely lightweight, with only approximately 2.9629 million trainable parameters.}"}, "1912.01496": {"original_text": "  Stories are diverse and highly personalized, resulting in a large possible\noutput space for story generation. Existing end-to-end approaches produce\nmonotonous stories because they are limited to the vocabulary and knowledge in\na single training dataset. This paper introduces KG-Story, a three-stage\nframework that allows the story generation model to take advantage of external\nKnowledge Graphs to produce interesting stories. KG-Story distills a set of\nrepresentative words from the input prompts, enriches the word set by using\nexternal knowledge graphs, and finally generates stories based on the enriched\nword set. This distill-enrich-generate framework allows the use of external\nresources not only for the enrichment phase, but also for the distillation and\ngeneration phases. In this paper, we show the superiority of KG-Story for\nvisual storytelling, where the input prompt is a sequence of five photos and\nthe output is a short story. Per the human ranking evaluation, stories\ngenerated by KG-Story are on average ranked better than that of the\nstate-of-the-art systems. Our code and output stories are available at\nhttps://github.com/zychen423/KE-VIST.\n", "rewritten_text": "\\boxed{The generation of stories is a complex task due to the vast possibilities of output, leading to monotonous results from traditional end-to-end approaches limited by their training datasets. This paper proposes KG-Story, a novel three-stage framework that leverages external Knowledge Graphs to produce engaging stories. KG-Story extracts key words from input prompts, expands them using external knowledge, and generates stories based on the enriched words. This framework allows for the integration of external resources throughout the process. The effectiveness of KG-Story is demonstrated in visual storytelling, where it outperforms state-of-the-art systems in human ranking evaluations. The code and generated stories are available at https://github.com/zychen423/KE-VIST.}"}, "2307.16825": {"original_text": "  With sufficient paired training samples, the supervised deep learning methods\nhave attracted much attention in image denoising because of their superior\nperformance. However, it is still very challenging to widely utilize the\nsupervised methods in real cases due to the lack of paired noisy-clean images.\nMeanwhile, most self-supervised denoising methods are ineffective as well when\napplied to the real-world denoising tasks because of their strict assumptions\nin applications. For example, as a typical method for self-supervised\ndenoising, the original blind spot network (BSN) assumes that the noise is\npixel-wise independent, which is much different from the real cases. To solve\nthis problem, we propose a novel self-supervised real image denoising framework\nnamed Sampling Difference As Perturbation (SDAP) based on Random Sub-samples\nGeneration (RSG) with a cyclic sample difference loss. Specifically, we dig\ndeeper into the properties of BSN to make it more suitable for real noise.\nSurprisingly, we find that adding an appropriate perturbation to the training\nimages can effectively improve the performance of BSN. Further, we propose that\nthe sampling difference can be considered as perturbation to achieve better\nresults. Finally we propose a new BSN framework in combination with our RSG\nstrategy. The results show that it significantly outperforms other\nstate-of-the-art self-supervised denoising methods on real-world datasets. The\ncode is available at https://github.com/p1y2z3/SDAP.\n", "rewritten_text": "\\boxed{\nSupervised deep learning methods have shown excellent performance in image denoising, but their widespread adoption is hindered by the lack of paired noisy-clean images. Self-supervised methods, on the other hand, often rely on unrealistic assumptions, such as pixel-wise independent noise, which limits their effectiveness in real-world applications. To address this issue, we propose a novel self-supervised framework, Sampling Difference As Perturbation (SDAP), which leverages Random Sub-samples Generation (RSG) and a cyclic sample difference loss. By building upon the Blind Spot Network (BSN) and incorporating an appropriate perturbation, we significantly improve its performance. Our approach outperforms state-of-the-art self-supervised denoising methods on real-world datasets, and the code is available at https://github.com/p1y2z3/SDAP.\n}"}, "2409.16685": {"original_text": "  Integrating aerial imagery-based scene generation into applications like\nautonomous driving and gaming enhances realism in 3D environments, but\nchallenges remain in creating detailed content for occluded areas and ensuring\nreal-time, consistent rendering. In this paper, we introduce Skyeyes, a novel\nframework that can generate photorealistic sequences of ground view images\nusing only aerial view inputs, thereby creating a ground roaming experience.\nMore specifically, we combine a 3D representation with a view consistent\ngeneration model, which ensures coherence between generated images. This method\nallows for the creation of geometrically consistent ground view images, even\nwith large view gaps. The images maintain improved spatial-temporal coherence\nand realism, enhancing scene comprehension and visualization from aerial\nperspectives. To the best of our knowledge, there are no publicly available\ndatasets that contain pairwise geo-aligned aerial and ground view imagery.\nTherefore, we build a large, synthetic, and geo-aligned dataset using Unreal\nEngine. Both qualitative and quantitative analyses on this synthetic dataset\ndisplay superior results compared to other leading synthesis approaches. See\nthe project page for more results:\nhttps://chaoren2357.github.io/website-skyeyes/.\n", "rewritten_text": "\\boxed{\nCreating realistic 3D environments for applications like autonomous driving and gaming is enhanced by integrating aerial imagery-based scene generation. However, generating detailed content for occluded areas and ensuring real-time rendering remains a challenge. To address this, we propose Skyeyes, a novel framework that generates photorealistic ground view images from aerial view inputs, providing a realistic ground roaming experience. By combining a 3D representation with a view consistent generation model, we ensure coherence between generated images, creating geometrically consistent ground view images even with large view gaps. Our approach improves spatial-temporal coherence and realism, enhancing scene comprehension and visualization from aerial perspectives. We also create a large, synthetic, and geo-aligned dataset using Unreal Engine, which outperforms other leading synthesis approaches in both qualitative and quantitative analyses. More results can be found at https://chaoren2357.github.io/website-skyeyes/.\n}"}, "2210.02844": {"original_text": "  In this paper, we explore the following question: Are synonym substitution\nattacks really synonym substitution attacks (SSAs)? We approach this question\nby examining how SSAs replace words in the original sentence and show that\nthere are still unresolved obstacles that make current SSAs generate invalid\nadversarial samples. We reveal that four widely used word substitution methods\ngenerate a large fraction of invalid substitution words that are ungrammatical\nor do not preserve the original sentence's semantics. Next, we show that the\nsemantic and grammatical constraints used in SSAs for detecting invalid word\nreplacements are highly insufficient in detecting invalid adversarial samples.\n", "rewritten_text": "\\boxed{This paper investigates whether synonym substitution attacks (SSAs) truly qualify as such. Our analysis of how SSAs replace words in original sentences reveals that current methods still face significant challenges in generating valid adversarial samples. We find that four commonly used word substitution methods produce a substantial number of invalid substitutions that are either ungrammatical or fail to preserve the original sentence's meaning. Furthermore, we demonstrate that the semantic and grammatical constraints employed in SSAs to detect invalid word replacements are grossly inadequate in identifying invalid adversarial samples.}"}, "2307.09769": {"original_text": "  Unsupervised domain adaptation (UDA) has increasingly gained interests for\nits capacity to transfer the knowledge learned from a labeled source domain to\nan unlabeled target domain. However, typical UDA methods require concurrent\naccess to both the source and target domain data, which largely limits its\napplication in medical scenarios where source data is often unavailable due to\nprivacy concern. To tackle the source data-absent problem, we present a novel\ntwo-stage source-free domain adaptation (SFDA) framework for medical image\nsegmentation, where only a well-trained source segmentation model and unlabeled\ntarget data are available during domain adaptation. Specifically, in the\nprototype-anchored feature alignment stage, we first utilize the weights of the\npre-trained pixel-wise classifier as source prototypes, which preserve the\ninformation of source features. Then, we introduce the bi-directional transport\nto align the target features with class prototypes by minimizing its expected\ncost. On top of that, a contrastive learning stage is further devised to\nutilize those pixels with unreliable predictions for a more compact target\nfeature distribution. Extensive experiments on a cross-modality medical\nsegmentation task demonstrate the superiority of our method in large domain\ndiscrepancy settings compared with the state-of-the-art SFDA approaches and\neven some UDA methods. Code is available at\nhttps://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA.\n", "rewritten_text": "\\boxed{\nUnsupervised domain adaptation (UDA) has gained popularity for its ability to transfer knowledge from a labeled source domain to an unlabeled target domain. However, most UDA methods require access to both source and target domain data, which is often not possible in medical scenarios due to privacy concerns. To address this limitation, we propose a novel two-stage source-free domain adaptation (SFDA) framework for medical image segmentation, which only requires a pre-trained source segmentation model and unlabeled target data. Our framework consists of two stages: prototype-anchored feature alignment, which aligns target features with source prototypes using bi-directional transport, and contrastive learning, which utilizes unreliable predictions to refine the target feature distribution. Experimental results on a cross-modality medical segmentation task demonstrate the superiority of our method over state-of-the-art SFDA approaches and even some UDA methods. The code is available at https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA.\n}"}, "1811.08015": {"original_text": "  This paper introduces the problem of automatic font pairing. Font pairing is\nan important design task that is difficult for novices. Given a font selection\nfor one part of a document (e.g., header), our goal is to recommend a font to\nbe used in another part (e.g., body) such that the two fonts used together look\nvisually pleasing. There are three main challenges in font pairing. First, this\nis a fine-grained problem, in which the subtle distinctions between fonts may\nbe important. Second, rules and conventions of font pairing given by human\nexperts are difficult to formalize. Third, font pairing is an asymmetric\nproblem in that the roles played by header and body fonts are not\ninterchangeable. To address these challenges, we propose automatic font pairing\nthrough learning visual relationships from large-scale human-generated font\npairs. We introduce a new database for font pairing constructed from millions\nof PDF documents available on the Internet. We propose two font pairing\nalgorithms: dual-space k-NN and asymmetric similarity metric learning (ASML).\nThese two methods automatically learn fine-grained relationships from\nlarge-scale data. We also investigate several baseline methods based on the\nrules from professional designers. Experiments and user studies demonstrate the\neffectiveness of our proposed dataset and methods.\n", "rewritten_text": "\\boxed{\nThis paper tackles the challenge of automatically pairing fonts, a crucial design task that can be daunting for beginners. The goal is to suggest a font for one part of a document (e.g., body) that visually complements a given font for another part (e.g., header). The difficulties in font pairing lie in its subtlety, the lack of formalized expert guidelines, and the asymmetrical roles of header and body fonts. To overcome these hurdles, we propose a machine learning approach that learns from a vast collection of human-curated font pairs. We introduce a novel database of font pairs extracted from millions of online PDF documents. Our approach involves two algorithms: dual-space k-NN and asymmetric similarity metric learning (ASML), which can capture fine-grained relationships from large-scale data. We also explore baseline methods based on professional designers' rules. Experimental results and user studies demonstrate the effectiveness of our proposed dataset and methods."}, "2403.15119": {"original_text": "  Person re-identification (ReID) has made great strides thanks to the\ndata-driven deep learning techniques. However, the existing benchmark datasets\nlack diversity, and models trained on these data cannot generalize well to\ndynamic wild scenarios. To meet the goal of improving the explicit\ngeneralization of ReID models, we develop a new Open-World, Diverse,\nCross-Spatial-Temporal dataset named OWD with several distinct features. 1)\nDiverse collection scenes: multiple independent open-world and highly dynamic\ncollecting scenes, including streets, intersections, shopping malls, etc. 2)\nDiverse lighting variations: long time spans from daytime to nighttime with\nabundant illumination changes. 3) Diverse person status: multiple camera\nnetworks in all seasons with normal/adverse weather conditions and diverse\npedestrian appearances (e.g., clothes, personal belongings, poses, etc.). 4)\nProtected privacy: invisible faces for privacy critical applications. To\nimprove the implicit generalization of ReID, we further propose a Latent Domain\nExpansion (LDE) method to develop the potential of source data, which decouples\ndiscriminative identity-relevant and trustworthy domain-relevant features and\nimplicitly enforces domain-randomized identity feature space expansion with\nricher domain diversity to facilitate domain invariant representations. Our\ncomprehensive evaluations with most benchmark datasets in the community are\ncrucial for progress, although this work is far from the grand goal toward\nopen-world and dynamic wild applications.\n", "rewritten_text": "\\boxed{\nPerson re-identification (ReID) has seen significant advancements with the help of deep learning techniques driven by data. However, current benchmark datasets lack diversity, resulting in models that struggle to generalize to real-world scenarios. To address this limitation, we introduce a novel dataset called OWD, which features diverse collection scenes, lighting variations, person statuses, and protected privacy. Additionally, we propose a Latent Domain Expansion (LDE) method to improve the generalization of ReID models by decoupling identity-relevant and domain-relevant features, leading to domain-invariant representations. Our comprehensive evaluations on various benchmark datasets demonstrate the importance of this work towards achieving open-world and dynamic wild applications.\n}"}, "1906.03731": {"original_text": "  Attention mechanisms have recently boosted performance on a range of NLP\ntasks. Because attention layers explicitly weight input components'\nrepresentations, it is also often assumed that attention can be used to\nidentify information that models found important (e.g., specific contextualized\nword tokens). We test whether that assumption holds by manipulating attention\nweights in already-trained text classification models and analyzing the\nresulting differences in their predictions. While we observe some ways in which\nhigher attention weights correlate with greater impact on model predictions, we\nalso find many ways in which this does not hold, i.e., where gradient-based\nrankings of attention weights better predict their effects than their\nmagnitudes. We conclude that while attention noisily predicts input components'\noverall importance to a model, it is by no means a fail-safe indicator.\n", "rewritten_text": "\\boxed{Recent advances in NLP tasks have been attributed to attention mechanisms, which are thought to highlight crucial input components by assigning them higher weights. However, our experiments on pre-trained text classification models reveal that this assumption is not entirely accurate. While there is some correlation between attention weights and their impact on model predictions, we find that gradient-based rankings are often better predictors of a component's importance. Therefore, attention weights should not be solely relied upon to determine input components' significance.}"}, "1611.08134": {"original_text": "  Color based re-identification methods usually rely on a distance function to\nmeasure the similarity between individuals. In this paper we study the behavior\nof several histogram distance measures in different color spaces. We wonder\nwhether there is a particular histogram distance measure better than others,\nlikewise also, if there is a color space that present better discrimination\nfeatures. Several experiments are designed and evaluated in several images to\nobtain measures against various color spaces. We test in several image\ndatabases. A measure ranking is generated to calculate the area under the CMC,\nthis area is the indicator used to evaluate which distance measure and color\nspace present the best performance for the considered databases. Also, other\nparameters such as the image division in horizontal stripes and number of\nhistogram bins, have been studied.\n", "rewritten_text": "\\boxed{This paper investigates the performance of various histogram distance measures in different color spaces for color-based re-identification methods. The goal is to determine the most effective distance measure and color space for distinguishing individuals. Experiments are conducted on multiple image databases, evaluating the area under the CMC curve as a performance indicator. Additionally, the impact of image division into horizontal stripes and the number of histogram bins on the results is examined. The study aims to provide a ranking of the distance measures and color spaces based on their performance.}"}, "2003.05065": {"original_text": "  For many of the physical phenomena around us, we have developed sophisticated\nmodels explaining their behavior. Nevertheless, measuring physical properties\nfrom visual observations is challenging due to the high number of causally\nunderlying physical parameters -- including material properties and external\nforces. In this paper, we propose to measure latent physical properties for\ncloth in the wind without ever having seen a real example before. Our solution\nis an iterative refinement procedure with simulation at its core. The algorithm\ngradually updates the physical model parameters by running a simulation of the\nobserved phenomenon and comparing the current simulation to a real-world\nobservation. The correspondence is measured using an embedding function that\nmaps physically similar examples to nearby points. We consider a case study of\ncloth in the wind, with curling flags as our leading example -- a seemingly\nsimple phenomena but physically highly involved. Based on the physics of cloth\nand its visual manifestation, we propose an instantiation of the embedding\nfunction. For this mapping, modeled as a deep network, we introduce a spectral\nlayer that decomposes a video volume into its temporal spectral power and\ncorresponding frequencies. Our experiments demonstrate that the proposed method\ncompares favorably to prior work on the task of measuring cloth material\nproperties and external wind force from a real-world video.\n", "rewritten_text": "\\boxed{\nMeasuring physical properties from visual observations is a challenging task due to the complexity of underlying physical parameters. This paper proposes a novel approach to measure latent physical properties of cloth in the wind without prior examples. The method involves an iterative refinement procedure that uses simulation to update physical model parameters by comparing simulated and real-world observations. The similarity between the two is measured using an embedding function that maps physically similar examples to nearby points. The approach is demonstrated on a case study of curling flags, where a deep network with a spectral layer is used to decompose video volumes into temporal spectral power and frequencies. The results show that the proposed method outperforms prior work in measuring cloth material properties and external wind force from real-world videos.\n}"}, "2408.14860": {"original_text": "  This paper presents DiffSurf, a transformer-based denoising diffusion model\nfor generating and reconstructing 3D surfaces. Specifically, we design a\ndiffusion transformer architecture that predicts noise from noisy 3D surface\nvertices and normals. With this architecture, DiffSurf is able to generate 3D\nsurfaces in various poses and shapes, such as human bodies, hands, animals and\nman-made objects. Further, DiffSurf is versatile in that it can address various\n3D downstream tasks including morphing, body shape variation and 3D human mesh\nfitting to 2D keypoints. Experimental results on 3D human model benchmarks\ndemonstrate that DiffSurf can generate shapes with greater diversity and higher\nquality than previous generative models. Furthermore, when applied to the task\nof single-image 3D human mesh recovery, DiffSurf achieves accuracy comparable\nto prior techniques at a near real-time rate.\n", "rewritten_text": "\\boxed{This paper introduces DiffSurf, a novel transformer-based denoising diffusion model capable of generating and reconstructing 3D surfaces with high accuracy and diversity. The proposed diffusion transformer architecture predicts noise from noisy 3D surface vertices and normals, enabling the generation of 3D surfaces in various poses and shapes, including human bodies, hands, animals, and man-made objects. Additionally, DiffSurf can be applied to various 3D downstream tasks, such as morphing, body shape variation, and 3D human mesh fitting to 2D keypoints. Experimental results on 3D human model benchmarks demonstrate the superiority of DiffSurf in generating high-quality shapes with greater diversity, and its ability to achieve near real-time performance in single-image 3D human mesh recovery.}"}, "2111.1341": {"original_text": "  Manual annotation of medical images is highly subjective, leading to\ninevitable and huge annotation biases. Deep learning models may surpass human\nperformance on a variety of tasks, but they may also mimic or amplify these\nbiases. Although we can have multiple annotators and fuse their annotations to\nreduce stochastic errors, we cannot use this strategy to handle the bias caused\nby annotators' preferences. In this paper, we highlight the issue of\nannotator-related biases on medical image segmentation tasks, and propose a\nPreference-involved Annotation Distribution Learning (PADL) framework to\naddress it from the perspective of disentangling an annotator's preference from\nstochastic errors using distribution learning so as to produce not only a meta\nsegmentation but also the segmentation possibly made by each annotator. Under\nthis framework, a stochastic error modeling (SEM) module estimates the meta\nsegmentation map and average stochastic error map, and a series of human\npreference modeling (HPM) modules estimate each annotator's segmentation and\nthe corresponding stochastic error. We evaluated our PADL framework on two\nmedical image benchmarks with different imaging modalities, which have been\nannotated by multiple medical professionals, and achieved promising performance\non all five medical image segmentation tasks.\n", "rewritten_text": "\\boxed{\nManual annotation of medical images is prone to subjective biases, which can be perpetuated or exacerbated by deep learning models. While having multiple annotators can reduce random errors, it cannot eliminate biases stemming from individual preferences. This paper addresses the issue of annotator-related biases in medical image segmentation tasks by proposing a Preference-involved Annotation Distribution Learning (PADL) framework. This framework disentangles an annotator's preference from random errors using distribution learning, producing both a meta segmentation and individual annotator segmentations. The PADL framework consists of a stochastic error modeling module and multiple human preference modeling modules, which estimate the meta segmentation, average stochastic error, and individual annotator segmentations. Evaluation on two medical image benchmarks with multiple annotations demonstrates promising performance on five medical image segmentation tasks.\n}"}, "2312.06598": {"original_text": "  Early action recognition is an important and challenging problem that enables\nthe recognition of an action from a partially observed video stream where the\nactivity is potentially unfinished or even not started. In this work, we\npropose a novel model that learns a prototypical representation of the full\naction for each class and uses it to regularize the architecture and the visual\nrepresentations of the partial observations. Our model is very simple in design\nand also efficient. We decompose the video into short clips, where a visual\nencoder extracts features from each clip independently. Later, a decoder\naggregates together in an online fashion features from all the clips for the\nfinal class prediction. During training, for each partial observation, the\nmodel is jointly trained to both predict the label as well as the action\nprototypical representation which acts as a regularizer. We evaluate our method\non multiple challenging real-world datasets and outperform the current\nstate-of-the-art by a significant margin. For example, on early recognition\nobserving only the first 10% of each video, our method improves the SOTA by\n+2.23 Top-1 accuracy on Something-Something-v2, +3.55 on UCF-101, +3.68 on\nSSsub21, and +5.03 on EPIC-Kitchens-55, where prior work used either\nmulti-modal inputs (e.g. optical-flow) or batched inference. Finally, we also\npresent exhaustive ablation studies to motivate the design choices we made, as\nwell as gather insights regarding what our model is learning semantically.\n", "rewritten_text": "\\boxed{\nEarly action recognition, which involves identifying an action from a partially observed video, is a crucial and challenging task. We propose a novel model that learns a prototypical representation of each action class and uses it to regularize the architecture and visual representations of partial observations. Our model is simple, efficient, and decomposes videos into short clips, extracting features from each clip independently. During training, the model predicts the label and action prototypical representation, which acts as a regularizer. We evaluate our method on multiple real-world datasets, outperforming the current state-of-the-art by a significant margin, and present ablation studies to motivate our design choices and gain insights into what our model is learning semantically."}, "2007.1562": {"original_text": "  Named Entity Recognition (NER) is a fundamental NLP task, commonly formulated\nas classification over a sequence of tokens. Morphologically-Rich Languages\n(MRLs) pose a challenge to this basic formulation, as the boundaries of Named\nEntities do not necessarily coincide with token boundaries, rather, they\nrespect morphological boundaries. To address NER in MRLs we then need to answer\ntwo fundamental questions, namely, what are the basic units to be labeled, and\nhow can these units be detected and classified in realistic settings, i.e.,\nwhere no gold morphology is available. We empirically investigate these\nquestions on a novel NER benchmark, with parallel tokenlevel and morpheme-level\nNER annotations, which we develop for Modern Hebrew, a morphologically\nrich-and-ambiguous language. Our results show that explicitly modeling\nmorphological boundaries leads to improved NER performance, and that a novel\nhybrid architecture, in which NER precedes and prunes morphological\ndecomposition, greatly outperforms the standard pipeline, where morphological\ndecomposition strictly precedes NER, setting a new performance bar for both\nHebrew NER and Hebrew morphological decomposition tasks.\n", "rewritten_text": "\\boxed{Named Entity Recognition (NER) is a crucial task in Natural Language Processing (NLP) that involves classifying sequences of tokens. However, this task becomes challenging when dealing with Morphologically-Rich Languages (MRLs) because named entities don't always align with token boundaries, but rather with morphological boundaries. To overcome this, we need to identify the basic units to be labeled and develop a method to detect and classify them in real-world scenarios where gold morphology is not available. We investigate these questions using a new NER benchmark for Modern Hebrew, a language with rich and ambiguous morphology, and find that explicitly modeling morphological boundaries improves NER performance. Moreover, a novel hybrid approach that combines NER and morphological decomposition outperforms the traditional pipeline, setting a new standard for Hebrew NER and morphological decomposition tasks.}"}, "2102.12452": {"original_text": "  Probing classifiers have emerged as one of the prominent methodologies for\ninterpreting and analyzing deep neural network models of natural language\nprocessing. The basic idea is simple -- a classifier is trained to predict some\nlinguistic property from a model's representations -- and has been used to\nexamine a wide variety of models and properties. However, recent studies have\ndemonstrated various methodological limitations of this approach. This article\ncritically reviews the probing classifiers framework, highlighting their\npromises, shortcomings, and advances.\n", "rewritten_text": "\\boxed{The probing classifiers approach has gained popularity in interpreting and analyzing deep neural network models in natural language processing, involving training a classifier to predict linguistic properties from a model's representations. Despite its widespread use, recent studies have revealed several methodological limitations. This article provides a critical review of the probing classifiers framework, discussing its benefits, drawbacks, and recent developments.}"}, "2408.11567": {"original_text": "  Point cloud analysis has achieved significant development and is\nwell-performed in multiple downstream tasks like point cloud classification and\nsegmentation, etc. Being conscious of the simplicity of the position encoding\nstructure in Transformer-based architectures, we attach importance to the\nposition encoding as a high-dimensional part and the patch encoder to offer\nmulti-scale information. Together with the sequential Transformer, the whole\nmodule with position encoding comprehensively constructs a multi-scale feature\nabstraction module that considers both the local parts from the patch and the\nglobal parts from center points as position encoding. With only a few\nparameters, the position embedding module fits the setting of PEFT\n(Parameter-Efficient Fine-Tuning) tasks pretty well. Thus we unfreeze these\nparameters as a fine-tuning part. At the same time, we review the existing\nprompt and adapter tuning methods, proposing a fresh way of prompts and\nsynthesizing them with adapters as dynamic adjustments. Our Proposed method of\nPEFT tasks, namely PPT, with only 1.05% of parameters for training, gets\nstate-of-the-art results in several mainstream datasets, such as 95.01%\naccuracy in the ScanObjectNN OBJ_BG dataset. Codes will be released at\nhttps://github.com/zsc000722/PPT.\n", "rewritten_text": "\\boxed{\nPoint cloud analysis has made significant progress, achieving excellent results in tasks such as classification and segmentation. We focus on improving the position encoding structure in Transformer-based architectures, which is crucial for capturing multi-scale information. Our proposed module combines position encoding with a sequential Transformer to create a comprehensive multi-scale feature abstraction module. This module considers both local and global features, and its position embedding module is highly efficient, making it suitable for fine-tuning tasks. We also introduce a novel approach to prompt and adapter tuning, which achieves state-of-the-art results in several datasets, including 95.01% accuracy in the ScanObjectNN OBJ_BG dataset, with only 1.05% of parameters required for training. The code for our proposed method, PPT, is available at https://github.com/zsc000722/PPT.\n}"}, "1607.0003": {"original_text": "  Human evaluation of machine translation normally uses sentence-level measures\nsuch as relative ranking or adequacy scales. However, these provide no insight\ninto possible errors, and do not scale well with sentence length. We argue for\na semantics-based evaluation, which captures what meaning components are\nretained in the MT output, thus providing a more fine-grained analysis of\ntranslation quality, and enabling the construction and tuning of\nsemantics-based MT. We present a novel human semantic evaluation measure, Human\nUCCA-based MT Evaluation (HUME), building on the UCCA semantic representation\nscheme. HUME covers a wider range of semantic phenomena than previous methods\nand does not rely on semantic annotation of the potentially garbled MT output.\nWe experiment with four language pairs, demonstrating HUME's broad\napplicability, and report good inter-annotator agreement rates and correlation\nwith human adequacy scores.\n", "rewritten_text": "\\boxed{Evaluating machine translation typically involves sentence-level assessments, such as ranking or adequacy scales, which fail to identify errors and are limited by sentence length. Instead, we propose a semantics-based approach that examines the preservation of meaning components in the translated output, providing a more detailed analysis of translation quality and enabling the development of semantics-based machine translation. We introduce HUME, a novel human evaluation measure based on the UCCA semantic representation scheme, which covers a broader range of semantic phenomena and doesn't require annotation of potentially flawed translated output. Our experiments with four language pairs demonstrate HUME's versatility, with high inter-annotator agreement rates and correlation with human adequacy scores.}"}, "2205.0809": {"original_text": "  Event cameras are bio-inspired sensors that capture per-pixel asynchronous\nintensity change rather than the synchronous absolute intensity frames captured\nby a classical camera sensor. Such cameras are ideal for robotics applications\nsince they have high temporal resolution, high dynamic range and low latency.\nHowever, due to their high temporal resolution, event cameras are particularly\nsensitive to flicker such as from fluorescent or LED lights. During every cycle\nfrom bright to dark, pixels that image a flickering light source generate many\nevents that provide little or no useful information for a robot, swamping the\nuseful data in the scene. In this paper, we propose a novel linear filter to\npreprocess event data to remove unwanted flicker events from an event stream.\nThe proposed algorithm achieves over 4.6 times relative improvement in the\nsignal-to-noise ratio when compared to the raw event stream due to the\neffective removal of flicker from fluorescent lighting. Thus, it is ideally\nsuited to robotics applications that operate in indoor settings or scenes\nilluminated by flickering light sources.\n", "rewritten_text": "\\boxed{Event cameras, inspired by biology, capture changes in intensity rather than absolute intensity frames. They're ideal for robotics due to their high temporal resolution, dynamic range, and low latency. However, they're sensitive to flicker from fluorescent or LED lights, which can overwhelm useful data. To address this, we propose a linear filter to preprocess event data, removing unwanted flicker events. Our algorithm improves the signal-to-noise ratio by over 4.6 times, making it suitable for robotics applications in indoor settings or scenes with flickering light sources.}"}, "2205.00627": {"original_text": "  White matter fiber clustering is an important strategy for white matter\nparcellation, which enables quantitative analysis of brain connections in\nhealth and disease. In combination with expert neuroanatomical labeling,\ndata-driven white matter fiber clustering is a powerful tool for creating\natlases that can model white matter anatomy across individuals. While widely\nused fiber clustering approaches have shown good performance using classical\nunsupervised machine learning techniques, recent advances in deep learning\nreveal a promising direction toward fast and effective fiber clustering. In\nthis work, we propose a novel deep learning framework for white matter fiber\nclustering, Deep Fiber Clustering (DFC), which solves the unsupervised\nclustering problem as a self-supervised learning task with a domain-specific\npretext task to predict pairwise fiber distances. This process learns a\nhigh-dimensional embedding feature representation for each fiber, regardless of\nthe order of fiber points reconstructed during tractography. We design a novel\nnetwork architecture that represents input fibers as point clouds and allows\nthe incorporation of additional sources of input information from gray matter\nparcellation to improve anatomical coherence of clusters. In addition, DFC\nconducts outlier removal naturally by rejecting fibers with low cluster\nassignment probability. We evaluate DFC on three independently acquired\ncohorts, including data from 220 individuals across genders, ages (young and\nelderly adults), and different health conditions (healthy control and multiple\nneuropsychiatric disorders). We compare DFC to several state-of-the-art white\nmatter fiber clustering algorithms. Experimental results demonstrate superior\nperformance of DFC in terms of cluster compactness, generalization ability,\nanatomical coherence, and computational efficiency.\n", "rewritten_text": "\\boxed{\nWhite matter fiber clustering is a crucial step in understanding brain connections, and combining it with expert neuroanatomical labeling enables the creation of atlases that model white matter anatomy across individuals. While traditional machine learning techniques have shown good performance, recent advances in deep learning offer a promising approach to fast and effective fiber clustering. This study proposes a novel deep learning framework, Deep Fiber Clustering (DFC), which solves the unsupervised clustering problem by predicting pairwise fiber distances and learning a high-dimensional embedding feature representation for each fiber. The proposed network architecture represents input fibers as point clouds, incorporates additional information from gray matter parcellation, and naturally removes outliers by rejecting fibers with low cluster assignment probability. Evaluation on three independent cohorts demonstrates the superior performance of DFC in terms of cluster compactness, generalization ability, anatomical coherence, and computational efficiency compared to state-of-the-art algorithms."}, "1707.08401": {"original_text": "  In the last two decades Computer Aided Diagnostics (CAD) systems were\ndeveloped to help radiologists analyze screening mammograms. The benefits of\ncurrent CAD technologies appear to be contradictory and they should be improved\nto be ultimately considered useful. Since 2012 deep convolutional neural\nnetworks (CNN) have been a tremendous success in image recognition, reaching\nhuman performance. These methods have greatly surpassed the traditional\napproaches, which are similar to currently used CAD solutions. Deep CNN-s have\nthe potential to revolutionize medical image analysis. We propose a CAD system\nbased on one of the most successful object detection frameworks, Faster R-CNN.\nThe system detects and classifies malignant or benign lesions on a mammogram\nwithout any human intervention. The proposed method sets the state of the art\nclassification performance on the public INbreast database, AUC = 0.95 . The\napproach described here has achieved the 2nd place in the Digital Mammography\nDREAM Challenge with AUC = 0.85 . When used as a detector, the system reaches\nhigh sensitivity with very few false positive marks per image on the INbreast\ndataset. Source code, the trained model and an OsiriX plugin are availaible\nonline at https://github.com/riblidezso/frcnn_cad .\n", "rewritten_text": "\\boxed{\nOver the past twenty years, Computer Aided Diagnostics (CAD) systems have been developed to assist radiologists in analyzing screening mammograms. However, the benefits of current CAD technologies are inconsistent and require improvement to be considered truly useful. Since 2012, deep convolutional neural networks (CNN) have achieved remarkable success in image recognition, surpassing human performance. These methods have outperformed traditional approaches, similar to those used in current CAD solutions. Deep CNNs have the potential to revolutionize medical image analysis. We propose a CAD system based on the successful Faster R-CNN object detection framework, which detects and classifies malignant or benign lesions on a mammogram without human intervention. Our proposed method achieves state-of-the-art classification performance on the public INbreast database, with an AUC of 0.95, and secures 2nd place in the Digital Mammography DREAM Challenge with an AUC of 0.85. When used as a detector, the system achieves high sensitivity with minimal false positive marks per image on the INbreast dataset. The source code, trained model, and OsiriX plugin are available online at https://github.com/riblidezso/frcnn_cad.\n}"}, "1604.02975": {"original_text": "  We propose a novel Coupled Projection multi-task Metric Learning (CP-mtML)\nmethod for large scale face retrieval. In contrast to previous works which were\nlimited to low dimensional features and small datasets, the proposed method\nscales to large datasets with high dimensional face descriptors. It utilises\npairwise (dis-)similarity constraints as supervision and hence does not require\nexhaustive class annotation for every training image. While, traditionally,\nmulti-task learning methods have been validated on same dataset but different\ntasks, we work on the more challenging setting with heterogeneous datasets and\ndifferent tasks. We show empirical validation on multiple face image datasets\nof different facial traits, e.g. identity, age and expression. We use classic\nLocal Binary Pattern (LBP) descriptors along with the recent Deep Convolutional\nNeural Network (CNN) features. The experiments clearly demonstrate the\nscalability and improved performance of the proposed method on the tasks of\nidentity and age based face image retrieval compared to competitive existing\nmethods, on the standard datasets and with the presence of a million distractor\nface images.\n", "rewritten_text": "\\boxed{We introduce a novel method called Coupled Projection multi-task Metric Learning (CP-mtML) for large-scale face retrieval, which can handle high-dimensional face descriptors and large datasets without requiring exhaustive class annotation. Unlike traditional multi-task learning methods, our approach is validated on heterogeneous datasets with different tasks. We demonstrate the effectiveness of CP-mtML on multiple face image datasets with varying facial traits, using both classic Local Binary Pattern (LBP) descriptors and recent Deep Convolutional Neural Network (CNN) features. Our experiments show that CP-mtML outperforms existing methods in identity and age-based face image retrieval, even with a million distractor face images present.}"}, "1705.10872": {"original_text": "  We introduce a novel loss for learning local feature descriptors which is\ninspired by the Lowe's matching criterion for SIFT. We show that the proposed\nloss that maximizes the distance between the closest positive and closest\nnegative patch in the batch is better than complex regularization methods; it\nworks well for both shallow and deep convolution network architectures.\nApplying the novel loss to the L2Net CNN architecture results in a compact\ndescriptor -- it has the same dimensionality as SIFT (128) that shows\nstate-of-art performance in wide baseline stereo, patch verification and\ninstance retrieval benchmarks. It is fast, computing a descriptor takes about 1\nmillisecond on a low-end GPU.\n", "rewritten_text": "\\boxed{We propose a new loss function for learning local feature descriptors, inspired by Lowe's SIFT matching criterion. This loss function, which maximizes the distance between the closest positive and negative patches in a batch, outperforms complex regularization methods and works well with both shallow and deep convolutional neural networks. When applied to the L2Net CNN architecture, it produces a compact 128-dimensional descriptor that achieves state-of-the-art performance in various benchmarks, including wide baseline stereo, patch verification, and instance retrieval, while being computationally efficient, taking only about 1 millisecond to compute on a low-end GPU.}"}, "2103.17107": {"original_text": "  In this paper, the multi-task learning of lightweight convolutional neural\nnetworks is studied for face identification and classification of facial\nattributes (age, gender, ethnicity) trained on cropped faces without margins.\nThe necessity to fine-tune these networks to predict facial expressions is\nhighlighted. Several models are presented based on MobileNet, EfficientNet and\nRexNet architectures. It was experimentally demonstrated that they lead to near\nstate-of-the-art results in age, gender and race recognition on the UTKFace\ndataset and emotion classification on the AffectNet dataset. Moreover, it is\nshown that the usage of the trained models as feature extractors of facial\nregions in video frames leads to 4.5% higher accuracy than the previously known\nstate-of-the-art single models for the AFEW and the VGAF datasets from the\nEmotiW challenges. The models and source code are publicly available at\nhttps://github.com/HSE-asavchenko/face-emotion-recognition.\n", "rewritten_text": "\\boxed{This paper explores the use of lightweight convolutional neural networks for multi-task learning of face identification and facial attribute classification, including age, gender, and ethnicity, without margins. The importance of fine-tuning these networks for facial expression prediction is emphasized. The study presents several models based on MobileNet, EfficientNet, and RexNet architectures, achieving near state-of-the-art results on the UTKFace and AffectNet datasets. Additionally, using the trained models as feature extractors for facial regions in video frames yields a 4.5% increase in accuracy on the AFEW and VGAF datasets from the EmotiW challenges. The models and source code are publicly available on GitHub.}"}, "2310.06474": {"original_text": "  While large language models (LLMs) exhibit remarkable capabilities across a\nwide range of tasks, they pose potential safety concerns, such as the\n``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to\nexhibit undesirable behavior. Although several preventive measures have been\ndeveloped to mitigate the potential risks associated with LLMs, they have\nprimarily focused on English. In this study, we reveal the presence of\nmultilingual jailbreak challenges within LLMs and consider two potential risky\nscenarios: unintentional and intentional. The unintentional scenario involves\nusers querying LLMs using non-English prompts and inadvertently bypassing the\nsafety mechanisms, while the intentional scenario concerns malicious users\ncombining malicious instructions with multilingual prompts to deliberately\nattack LLMs. The experimental results reveal that in the unintentional\nscenario, the rate of unsafe content increases as the availability of languages\ndecreases. Specifically, low-resource languages exhibit about three times the\nlikelihood of encountering harmful content compared to high-resource languages,\nwith both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts\ncan exacerbate the negative impact of malicious instructions, with\nastonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for\nGPT-4. To handle such a challenge in the multilingual context, we propose a\nnovel \\textsc{Self-Defense} framework that automatically generates multilingual\ntraining data for safety fine-tuning. Experimental results show that ChatGPT\nfine-tuned with such data can achieve a substantial reduction in unsafe content\ngeneration. Data is available at\n\\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.\n", "rewritten_text": "\\boxed{\nLarge language models (LLMs) are incredibly capable, but they also pose safety risks, such as the \"jailbreak\" problem, where malicious instructions can manipulate them to behave undesirably. While measures have been taken to mitigate these risks, they have mainly focused on English. This study reveals that multilingual LLMs are also vulnerable to jailbreak challenges, with two potential scenarios: unintentional and intentional. In the unintentional scenario, users may inadvertently bypass safety mechanisms when querying LLMs with non-English prompts, while in the intentional scenario, malicious users may combine malicious instructions with multilingual prompts to attack LLMs. The study finds that low-resource languages are more likely to produce harmful content, and multilingual prompts can exacerbate the negative impact of malicious instructions. To address this, a novel \"Self-Defense\" framework is proposed, which generates multilingual training data for safety fine-tuning. Experimental results show that this approach can significantly reduce the generation of unsafe content."}, "2110.13793": {"original_text": "  With camera resolution ever increasing and the need to rapidly recalibrate\nrobotic platforms in less than ideal environments, there is a need for faster\nand more robust chessboard fiducial marker detectors. A new chessboard detector\nis proposed that is specifically designed for: high resolution images,\nfocus/motion blur, harsh lighting conditions, and background clutter. This is\naccomplished using a new x-corner detector, where for the first time blur is\nestimated and used in a novel way to enhance corner localization, edge\nvalidation, and connectivity. Performance is measured and compared against\nother libraries using a diverse set of images created by combining multiple\nthird party datasets and including new specially crafted scenarios designed to\nstress the state-of-the-art. The proposed detector has the best F1- Score of\n0.97, runs 1.9x faster than next fastest, and is a top performer for corner\naccuracy, while being the only detector to have consistent good performance in\nall scenarios.\n", "rewritten_text": "\\boxed{A novel chessboard fiducial marker detector is proposed to address the challenges of high-resolution images, motion blur, harsh lighting, and background clutter in robotic platforms. This detector utilizes a new x-corner detection method that estimates and leverages blur to enhance corner localization, edge validation, and connectivity. Experimental results show that the proposed detector outperforms other libraries, achieving an F1-score of 0.97, running 1.9 times faster, and demonstrating consistent good performance in all scenarios, including those designed to stress the state-of-the-art.}"}, "2309.13492": {"original_text": "  Today's image style transfer methods have difficulty retaining humans face\nindividual features after the whole stylizing process. This occurs because the\nfeatures like face geometry and people's expressions are not captured by the\ngeneral-purpose image classifiers like the VGG-19 pre-trained models. This\npaper proposes the use of embeddings from an auxiliary pre-trained face\nrecognition model to encourage the algorithm to propagate human face features\nfrom the content image to the final stylized result.\n", "rewritten_text": "\\boxed{Current image style transfer methods struggle to preserve the unique facial features of individuals after stylization, as general-purpose image classifiers like VGG-19 pre-trained models fail to capture face geometry and expressions. This paper suggests utilizing embeddings from a pre-trained face recognition model to ensure the algorithm retains human face features from the original content image in the final stylized result.}"}, "2206.04242": {"original_text": "  Despite advances in image classification methods, detecting the samples not\nbelonging to the training classes is still a challenging problem. There has\nbeen a burst of interest in this subject recently, which is called Open-Set\nRecognition (OSR). In OSR, the goal is to achieve both the classification and\ndetecting out-of-distribution (OOD) samples. Several ideas have been proposed\nto push the empirical result further through complicated techniques. We believe\nthat such complication is indeed not necessary. To this end, we have shown that\nMaximum Softmax Probability (MSP), as the simplest baseline for OSR, applied on\nVision Transformers (ViTs) as the base classifier that is trained with non-OOD\naugmentations can surprisingly outperform many recent methods. Non-OOD\naugmentations are the ones that do not alter the data distribution by much. Our\nresults outperform state-of-the-art in CIFAR-10 datasets, and is also better\nthan most of the current methods in SVHN and MNIST. We show that training\naugmentation has a significant effect on the performance of ViTs in the OSR\ntasks, and while they should produce significant diversity in the augmented\nsamples, the generated sample OOD-ness must remain limited.\n", "rewritten_text": "\\boxed{Despite recent advances in image classification, identifying samples that don't belong to the training classes remains a significant challenge, known as Open-Set Recognition (OSR). The goal of OSR is to achieve both classification and detection of out-of-distribution (OOD) samples. While many complex techniques have been proposed to improve OSR, we demonstrate that a simple approach using Maximum Softmax Probability (MSP) on Vision Transformers (ViTs) with non-OOD augmentations can outperform many recent methods. Our results surpass the state-of-the-art on CIFAR-10 datasets and are competitive with current methods on SVHN and MNIST. We show that training augmentation has a significant impact on ViT performance in OSR tasks, requiring a balance between diversity in augmented samples and limited OOD-ness.}"}, "2010.01041": {"original_text": "  Planar homography estimation is foundational to many computer vision\nproblems, such as Simultaneous Localization and Mapping (SLAM) and Augmented\nReality (AR). However, conditions of high variance confound even the\nstate-of-the-art algorithms. In this report, we analyze the performance of two\nrecently published methods using Convolutional Neural Networks (CNNs) that are\nmeant to replace the more traditional feature-matching based approaches to the\nestimation of homography. Our evaluation of the CNN based methods focuses\nparticularly on measuring the performance under conditions of significant\nnoise, illumination shift, and occlusion. We also measure the benefits of\ntraining CNNs to varying degrees of noise. Additionally, we compare the effect\nof using color images instead of grayscale images for inputs to CNNs. Finally,\nwe compare the results against baseline feature-matching based homography\nestimation methods using SIFT, SURF, and ORB. We find that CNNs can be trained\nto be more robust against noise, but at a small cost to accuracy in the\nnoiseless case. Additionally, CNNs perform significantly better in conditions\nof extreme variance than their feature-matching based counterparts. With regard\nto color inputs, we conclude that with no change in the CNN architecture to\ntake advantage of the additional information in the color planes, the\ndifference in performance using color inputs or grayscale inputs is negligible.\nAbout the CNNs trained with noise-corrupted inputs, we show that training a CNN\nto a specific magnitude of noise leads to a \"Goldilocks Zone\" with regard to\nthe noise levels where that CNN performs best.\n", "rewritten_text": "\\boxed{\nEstimating planar homography is a crucial task in computer vision, essential for applications like SLAM and AR. However, high-variance conditions can hinder even the most advanced algorithms. This report examines the performance of two recent CNN-based methods, designed to replace traditional feature-matching approaches, under noisy, illumination-shifted, and occluded conditions. We investigate the benefits of training CNNs with varying noise levels and compare the effects of using color versus grayscale images as inputs. Our results show that CNNs can be trained to be more noise-robust, albeit with a slight accuracy trade-off in noiseless cases. Moreover, CNNs outperform feature-matching methods in high-variance conditions. We find that using color inputs without architecture modifications yields negligible performance differences. Finally, we observe that training CNNs with noise-corrupted inputs creates a \"Goldilocks Zone\" where the CNN performs optimally within a specific noise range.\n}"}, "2308.10168": {"original_text": "  Since the recent prosperity of Large Language Models (LLMs), there have been\ninterleaved discussions regarding how to reduce hallucinations from LLM\nresponses, how to increase the factuality of LLMs, and whether Knowledge Graphs\n(KGs), which store the world knowledge in a symbolic form, will be replaced\nwith LLMs. In this paper, we try to answer these questions from a new angle:\nHow knowledgeable are LLMs?\n  To answer this question, we constructed Head-to-Tail, a benchmark that\nconsists of 18K question-answer (QA) pairs regarding head, torso, and tail\nfacts in terms of popularity. We designed an automated evaluation method and a\nset of metrics that closely approximate the knowledge an LLM confidently\ninternalizes. Through a comprehensive evaluation of 16 publicly available LLMs,\nwe show that existing LLMs are still far from being perfect in terms of their\ngrasp of factual knowledge, especially for facts of torso-to-tail entities.\n", "rewritten_text": "\\boxed{Recent advancements in Large Language Models (LLMs) have sparked debates about reducing hallucinations, increasing factuality, and the potential replacement of Knowledge Graphs. This paper tackles these issues by investigating the knowledge possessed by LLMs. To do so, we developed a benchmark called Head-to-Tail, comprising 18,000 question-answer pairs, and an evaluation method to assess the knowledge confidently internalized by LLMs. Our comprehensive analysis of 16 publicly available LLMs reveals that they still fall short in grasping factual knowledge, particularly for lesser-known entities.}"}, "1912.06683": {"original_text": "  Semantic image segmentation plays a pivotal role in many vision applications\nincluding autonomous driving and medical image analysis. Most of the former\napproaches move towards enhancing the performance in terms of accuracy with a\nlittle awareness of computational efficiency. In this paper, we introduce\nLiteSeg, a lightweight architecture for semantic image segmentation. In this\nwork, we explore a new deeper version of Atrous Spatial Pyramid Pooling module\n(ASPP) and apply short and long residual connections, and depthwise separable\nconvolution, resulting in a faster and efficient model. LiteSeg architecture is\nintroduced and tested with multiple backbone networks as Darknet19, MobileNet,\nand ShuffleNet to provide multiple trade-offs between accuracy and\ncomputational cost. The proposed model LiteSeg, with MobileNetV2 as a backbone\nnetwork, achieves an accuracy of 67.81% mean intersection over union at 161\nframes per second with $640 \\times 360$ resolution on the Cityscapes dataset.\n", "rewritten_text": "\\boxed{Semantic image segmentation is crucial in applications like autonomous driving and medical image analysis. While previous approaches focused on improving accuracy, they often neglected computational efficiency. This paper presents LiteSeg, a lightweight architecture for semantic image segmentation that balances accuracy and efficiency. By introducing a deeper Atrous Spatial Pyramid Pooling module, short and long residual connections, and depthwise separable convolution, LiteSeg achieves faster and more efficient segmentation. The architecture is tested with various backbone networks, including Darknet19, MobileNet, and ShuffleNet, offering trade-offs between accuracy and computational cost. Notably, LiteSeg with MobileNetV2 as the backbone network achieves 67.81% mean intersection over union at 161 frames per second with a resolution of $640 \\times 360$ on the Cityscapes dataset.}"}, "2402.14281": {"original_text": "  Map representation learned by expert demonstrations has shown promising\nresearch value. However, recent advancements in the visual navigation field\nface challenges due to the lack of human datasets in the real world for\nefficient supervised representation learning of the environments. We present a\nLandmark-Aware Visual Navigation (LAVN) dataset to allow for supervised\nlearning of human-centric exploration policies and map building. We collect RGB\nobservation and human point-click pairs as a human annotator explores virtual\nand real-world environments with the goal of full coverage exploration of the\nspace. The human annotators also provide distinct landmark examples along each\ntrajectory, which we intuit will simplify the task of map or graph building and\nlocalization. These human point-clicks serve as direct supervision for waypoint\nprediction when learning to explore in environments. Our dataset covers a wide\nspectrum of scenes, including rooms in indoor environments, as well as walkways\noutdoors. Dataset is available at DOI: 10.5281/zenodo.10608067.\n", "rewritten_text": "\\boxed{\nResearch on map representation using expert demonstrations has shown great potential, but it's hindered by the lack of real-world human datasets for efficient supervised learning. To address this, we introduce the Landmark-Aware Visual Navigation (LAVN) dataset, which enables supervised learning of human-centric exploration policies and map building. Our dataset consists of RGB observations and human-annotated point-click pairs from virtual and real-world environments, with the goal of fully exploring the space. The annotators also provide distinct landmark examples, which should simplify map or graph building and localization. These point-clicks serve as direct supervision for waypoint prediction when learning to explore environments. The dataset covers a range of scenes, including indoor rooms and outdoor walkways, and is available at DOI: 10.5281/zenodo.10608067.\n}"}, "2408.05184": {"original_text": "  This paper describes our solution of the first subtask from the AXOLOTL-24\nshared task on Semantic Change Modeling. The goal of this subtask is to\ndistribute a given set of usages of a polysemous word from a newer time period\nbetween senses of this word from an older time period and clusters representing\ngained senses of this word. We propose and experiment with three new methods\nsolving this task. Our methods achieve SOTA results according to both official\nmetrics of the first substask. Additionally, we develop a model that can tell\nif a given word usage is not described by any of the provided sense\ndefinitions. This model serves as a component in one of our methods, but can\npotentially be useful on its own.\n", "rewritten_text": "\\boxed{This paper presents a solution to the first subtask of the AXOLOTL-24 shared task on Semantic Change Modeling, which involves categorizing polysemous word usages from a newer time period into either existing senses from an older time period or newly gained senses. We introduce and experiment with three novel methods that achieve state-of-the-art results according to the official metrics. Furthermore, we develop a model that can identify word usages not covered by provided sense definitions, which can be a useful standalone tool or a component of our proposed methods.}"}, "2312.00335": {"original_text": "  Self-supervised learning (SSL) approaches have recently shown substantial\nsuccess in learning visual representations from unannotated images. Compared\nwith photographic images, medical images acquired with the same imaging\nprotocol exhibit high consistency in anatomy. To exploit this anatomical\nconsistency, this paper introduces a novel SSL approach, called PEAC (patch\nembedding of anatomical consistency), for medical image analysis. Specifically,\nin this paper, we propose to learn global and local consistencies via stable\ngrid-based matching, transfer pre-trained PEAC models to diverse downstream\ntasks, and extensively demonstrate that (1) PEAC achieves significantly better\nperformance than the existing state-of-the-art fully/self-supervised methods,\nand (2) PEAC captures the anatomical structure consistency across views of the\nsame patient and across patients of different genders, weights, and healthy\nstatuses, which enhances the interpretability of our method for medical image\nanalysis.\n", "rewritten_text": "\\boxed{Recently, self-supervised learning approaches have achieved significant success in learning visual representations from unannotated images. Building on this, we propose a novel approach called PEAC (patch embedding of anatomical consistency) that leverages the high anatomical consistency in medical images acquired with the same imaging protocol. Our method learns global and local consistencies through stable grid-based matching and demonstrates superior performance compared to existing state-of-the-art methods in various downstream tasks. Moreover, PEAC captures anatomical structure consistency across different views of the same patient and across patients with varying characteristics, enhancing the interpretability of our method for medical image analysis.}"}, "2408.15159": {"original_text": "  Translating written sentences from oral languages to a sequence of manual and\nnon-manual gestures plays a crucial role in building a more inclusive society\nfor deaf and hard-of-hearing people. Facial expressions (non-manual), in\nparticular, are responsible for encoding the grammar of the sentence to be\nspoken, applying punctuation, pronouns, or emphasizing signs. These non-manual\ngestures are closely related to the semantics of the sentence being spoken and\nalso to the utterance of the speaker's emotions. However, most Sign Language\nProduction (SLP) approaches are centered on synthesizing manual gestures and do\nnot focus on modeling the speakers expression. This paper introduces a new\nmethod focused in synthesizing facial expressions for sign language. Our goal\nis to improve sign language production by integrating sentiment information in\nfacial expression generation. The approach leverages a sentence sentiment and\nsemantic features to sample from a meaningful representation space, integrating\nthe bias of the non-manual components into the sign language production\nprocess. To evaluate our method, we extend the Frechet Gesture Distance (FGD)\nand propose a new metric called Frechet Expression Distance (FED) and apply an\nextensive set of metrics to assess the quality of specific regions of the face.\nThe experimental results showed that our method achieved state of the art,\nbeing superior to the competitors on How2Sign and PHOENIX14T datasets.\nMoreover, our architecture is based on a carefully designed graph pyramid that\nmakes it simpler, easier to train, and capable of leveraging emotions to\nproduce facial expressions.\n", "rewritten_text": "\\boxed{\nTranslating spoken languages into sign language is crucial for creating a more inclusive society for the deaf and hard-of-hearing. Facial expressions play a vital role in conveying grammar, punctuation, and emotions, but most sign language production approaches focus solely on manual gestures, neglecting facial expressions. This paper proposes a novel method that prioritizes synthesizing facial expressions for sign language, incorporating sentiment information to improve production quality. By leveraging sentence sentiment and semantic features, our approach generates meaningful facial expressions that integrate non-manual components into the sign language production process. We evaluate our method using an extended metric, Frechet Expression Distance (FED), and achieve state-of-the-art results on two datasets, outperforming competitors. Our architecture is based on a graph pyramid, making it simpler, easier to train, and capable of producing emotional facial expressions."}, "2011.08894": {"original_text": "  Medical image segmentation is a relevant task as it serves as the first step\nfor several diagnosis processes, thus it is indispensable in clinical usage.\nWhilst major success has been reported using supervised techniques, they assume\na large and well-representative labelled set. This is a strong assumption in\nthe medical domain where annotations are expensive, time-consuming, and\ninherent to human bias. To address this problem, unsupervised techniques have\nbeen proposed in the literature yet it is still an open problem due to the\ndifficulty of learning any transformation pattern. In this work, we present a\nnovel optimisation model framed into a new CNN-based contrastive registration\narchitecture for unsupervised medical image segmentation. The core of our\napproach is to exploit image-level registration and feature-level from a\ncontrastive learning mechanism, to perform registration-based segmentation.\nFirstly, we propose an architecture to capture the image-to-image\ntransformation pattern via registration for unsupervised medical image\nsegmentation. Secondly, we embed a contrastive learning mechanism into the\nregistration architecture to enhance the discriminating capacity of the network\nin the feature-level. We show that our proposed technique mitigates the major\ndrawbacks of existing unsupervised techniques. We demonstrate, through\nnumerical and visual experiments, that our technique substantially outperforms\nthe current state-of-the-art unsupervised segmentation methods on two major\nmedical image datasets.\n", "rewritten_text": "\\boxed{\nMedical image segmentation is a crucial step in diagnosis, but it relies heavily on annotated datasets, which are expensive, time-consuming, and prone to human bias. While supervised techniques have shown success, they are limited by these assumptions. To overcome this, unsupervised techniques have been proposed, but they struggle to learn transformation patterns. This work presents a novel optimization model using a CNN-based contrastive registration architecture for unsupervised medical image segmentation. Our approach combines image-level registration and feature-level contrastive learning to perform registration-based segmentation. We demonstrate that our technique outperforms existing unsupervised methods on two major medical image datasets, mitigating their major drawbacks.\n}"}, "2301.02307": {"original_text": "  Narrated ''how-to'' videos have emerged as a promising data source for a wide\nrange of learning problems, from learning visual representations to training\nrobot policies. However, this data is extremely noisy, as the narrations do not\nalways describe the actions demonstrated in the video. To address this problem\nwe introduce the novel task of visual narration detection, which entails\ndetermining whether a narration is visually depicted by the actions in the\nvideo. We propose What You Say is What You Show (WYS^2), a method that\nleverages multi-modal cues and pseudo-labeling to learn to detect visual\nnarrations with only weakly labeled data. Our model successfully detects visual\nnarrations in in-the-wild videos, outperforming strong baselines, and we\ndemonstrate its impact for state-of-the-art summarization and temporal\nalignment of instructional videos.\n", "rewritten_text": "\\boxed{A new task called visual narration detection has been introduced to tackle the issue of noisy narrated \"how-to\" videos, where the spoken commentary doesn't always match the actions shown. The proposed method, What You Say is What You Show (WYS^2), uses multi-modal cues and pseudo-labeling to detect visual narrations from weakly labeled data, achieving better results than strong baselines and improving video summarization and temporal alignment.}"}, "2403.05170": {"original_text": "  This paper proposes a new pipeline for long-tail (LT) recognition. Instead of\nre-weighting or re-sampling, we utilize the long-tailed dataset itself to\ngenerate a balanced proxy that can be optimized through cross-entropy (CE).\nSpecifically, a randomly initialized diffusion model, trained exclusively on\nthe long-tailed dataset, is employed to synthesize new samples for\nunderrepresented classes. Then, we utilize the inherent information in the\noriginal dataset to filter out harmful samples and keep the useful ones. Our\nstrategy, Diffusion model for Long-Tail recognition (DiffuLT), represents a\npioneering utilization of generative models in long-tail recognition. DiffuLT\nachieves state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT,\nsurpassing the best competitors with non-trivial margins. Abundant ablations\nmake our pipeline interpretable, too. The whole generation pipeline is done\nwithout any external data or pre-trained model weights, making it highly\ngeneralizable to real-world long-tailed settings.\n", "rewritten_text": "\\boxed{This paper introduces a novel pipeline, DiffuLT, for long-tail recognition, which leverages the long-tailed dataset itself to generate a balanced proxy for cross-entropy optimization. By training a diffusion model on the long-tailed dataset, new samples are synthesized for underrepresented classes, and then filtered to retain only useful ones. This approach achieves state-of-the-art results on multiple benchmarks, outperforming competitors by significant margins, and offers high interpretability and generalizability to real-world long-tailed settings without requiring external data or pre-trained models.}"}, "2404.14779": {"original_text": "  This study presents a comprehensive analysis and comparison of two\npredominant fine-tuning methodologies - full-parameter fine-tuning and\nparameter-efficient tuning - within the context of medical Large Language\nModels (LLMs). We developed and refined a series of LLMs, based on the Llama-2\narchitecture, specifically designed to enhance medical knowledge retrieval,\nreasoning, and question-answering capabilities. Our experiments systematically\nevaluate the effectiveness of these tuning strategies across various well-known\nmedical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of\n72% on the US Medical Licensing Examination (USMLE) datasets, setting a new\nstandard in performance for openly available medical LLMs. Through this\ncomparative analysis, we aim to identify the most effective and efficient\nmethod for fine-tuning LLMs in the medical domain, thereby contributing\nsignificantly to the advancement of AI-driven healthcare applications.\n", "rewritten_text": "\\boxed{This study compares two fine-tuning methods, full-parameter fine-tuning and parameter-efficient tuning, for medical Large Language Models (LLMs) based on the Llama-2 architecture. We developed and refined a series of LLMs to improve medical knowledge retrieval, reasoning, and question-answering capabilities, and evaluated their effectiveness on various medical benchmarks. Our best model, Med42, achieved an accuracy of 72% on the US Medical Licensing Examination (USMLE) datasets, setting a new standard for openly available medical LLMs. This analysis aims to identify the most effective and efficient fine-tuning method for medical LLMs, contributing to the advancement of AI-driven healthcare applications.}"}, "2411.09694": {"original_text": "  Reranking a list of candidates from a machine translation system with an\nexternal scoring model and returning the highest-scoring candidate remains a\nsimple and effective method for improving the overall output quality.\nTranslation scoring models continue to grow in size, with the best models being\ncomparable to generation models. Thus, reranking can add substantial\ncomputational cost to the translation pipeline. In this work, we pose reranking\nas a Bayesian optimization (BayesOpt) problem. By strategically selecting\ncandidates to score based on a balance of exploration and exploitation, we show\nthat it is possible to find top-scoring candidates when scoring only a fraction\nof the candidate list. For instance, our method achieves the same CometKiwi\nscore using only 70 scoring evaluations compared a baseline system using 180.\nWe present a multi-fidelity setting for BayesOpt, where the candidates are\nfirst scored with a cheaper but noisier proxy scoring model, which further\nimproves the cost-performance tradeoff when using smaller but well-trained\ndistilled proxy scorers.\n", "rewritten_text": "\\boxed{Improving machine translation output quality by reranking candidates with an external scoring model is effective, but can be computationally expensive due to the growing size of scoring models. To address this, we frame reranking as a Bayesian optimization problem, strategically selecting candidates to score to balance exploration and exploitation. Our approach achieves the same quality with significantly fewer scoring evaluations, and can be further improved by using a cheaper, noisier proxy scoring model to prescreen candidates.}"}, "2407.07080": {"original_text": "  Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.\n", "rewritten_text": "\\boxed{\nTraining large language models (LLMs) in languages with limited resources, such as Hebrew, presents distinct challenges. This paper introduces DictaLM2.0 and DictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a massive corpus of around 200 billion tokens in both Hebrew and English. To adapt a pre-trained model to a new language, specialized techniques are required, differing significantly from training a model from scratch or further training existing models on well-resourced languages like English. We describe these novel training methodologies, which enable effective learning and adaptation to Hebrew's linguistic properties. Furthermore, we fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to enhance its performance on task-specific instructions. To thoroughly evaluate our models, we introduce a new benchmark suite for Hebrew LLM evaluation, covering a range of tasks including Question Answering, Sentiment Analysis, Winograd Schema Challenge, Translation, and Summarization. Our work addresses the complexities of training LLMs in low-resource languages and proposes a framework for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP."}, "2211.08544": {"original_text": "  Quantization-aware training (QAT) receives extensive popularity as it well\nretains the performance of quantized networks. In QAT, the contemporary\nexperience is that all quantized weights are updated for an entire training\nprocess. In this paper, this experience is challenged based on an interesting\nphenomenon we observed. Specifically, a large portion of quantized weights\nreaches the optimal quantization level after a few training epochs, which we\nrefer to as the partly scratch-off lottery ticket. This\nstraightforward-yet-valuable observation naturally inspires us to zero out\ngradient calculations of these weights in the remaining training period to\navoid meaningless updating. To effectively find the ticket, we develop a\nheuristic method, dubbed lottery ticket scratcher (LTS), which freezes a weight\nonce the distance between the full-precision one and its quantization level is\nsmaller than a controllable threshold. Surprisingly, the proposed LTS typically\neliminates 50%-70% weight updating and 25%-35% FLOPs of the backward pass,\nwhile still resulting on par with or even better performance than the compared\nbaseline. For example, compared with the baseline, LTS improves 2-bit\nMobileNetV2 by 5.05%, eliminating 46% weight updating and 23% FLOPs of the\nbackward pass. Code is at url{https://github.com/zysxmu/LTS}.\n", "rewritten_text": "\\boxed{\nQuantization-aware training (QAT) is a popular method for retaining the performance of quantized neural networks. However, the conventional approach updates all quantized weights throughout the entire training process. This paper challenges this approach by observing that a significant portion of quantized weights reach their optimal level after just a few training epochs, which is referred to as the \"partly scratch-off lottery ticket\". This observation leads to the idea of freezing these weights and avoiding unnecessary updates. A heuristic method, called the \"lottery ticket scratcher\" (LTS), is developed to identify and freeze these weights based on a controllable threshold. Surprisingly, LTS reduces weight updates by 50-70% and backward pass FLOPs by 25-35% while maintaining or even improving performance compared to the baseline. For instance, LTS improves 2-bit MobileNetV2 by 5.05% while eliminating 46% of weight updates and 23% of backward pass FLOPs. The code is available at https://github.com/zysxmu/LTS.\n}"}, "2001.04642": {"original_text": "  We address the dual problems of novel view synthesis and environment\nreconstruction from hand-held RGBD sensors. Our contributions include 1)\nmodeling highly specular objects, 2) modeling inter-reflections and Fresnel\neffects, and 3) enabling surface light field reconstruction with the same input\nneeded to reconstruct shape alone. In cases where scene surface has a strong\nmirror-like material component, we generate highly detailed environment images,\nrevealing room composition, objects, people, buildings, and trees visible\nthrough windows. Our approach yields state of the art view synthesis\ntechniques, operates on low dynamic range imagery, and is robust to geometric\nand calibration errors.\n", "rewritten_text": "\\boxed{We tackle the challenges of generating novel views and reconstructing environments using handheld RGBD sensors. Our key innovations include accurately modeling highly reflective objects, accounting for inter-reflections and Fresnel effects, and enabling the reconstruction of surface light fields using the same input required for shape reconstruction. In scenes with strong mirror-like reflections, our approach produces highly detailed environment images, capturing details such as room layouts, objects, people, buildings, and trees visible through windows. Our method achieves state-of-the-art view synthesis results, works with low dynamic range images, and is resilient to geometric and calibration errors.}"}, "1601.00199": {"original_text": "  Active Appearance Models (AAMs) are one of the most popular and\nwell-established techniques for modeling deformable objects in computer vision.\nIn this paper, we study the problem of fitting AAMs using Compositional\nGradient Descent (CGD) algorithms. We present a unified and complete view of\nthese algorithms and classify them with respect to three main characteristics:\ni) cost function; ii) type of composition; and iii) optimization method.\nFurthermore, we extend the previous view by: a) proposing a novel Bayesian cost\nfunction that can be interpreted as a general probabilistic formulation of the\nwell-known project-out loss; b) introducing two new types of composition,\nasymmetric and bidirectional, that combine the gradients of both image and\nappearance model to derive better conver- gent and more robust CGD algorithms;\nand c) providing new valuable insights into existent CGD algorithms by\nreinterpreting them as direct applications of the Schur complement and the\nWiberg method. Finally, in order to encourage open research and facilitate\nfuture comparisons with our work, we make the implementa- tion of the\nalgorithms studied in this paper publicly available as part of the Menpo\nProject.\n", "rewritten_text": "\\boxed{\nThis paper explores the use of Compositional Gradient Descent (CGD) algorithms for fitting Active Appearance Models (AAMs) in computer vision. We provide a comprehensive overview of CGD algorithms, categorizing them based on three key characteristics: cost function, composition type, and optimization method. Our contributions include a novel Bayesian cost function, two new composition types (asymmetric and bidirectional), and new insights into existing CGD algorithms through the lens of the Schur complement and Wiberg method. To facilitate future research and comparisons, we have made our implementation of these algorithms publicly available as part of the Menpo Project.\n}"}, "1709.01695": {"original_text": "  3D action recognition was shown to benefit from a covariance representation\nof the input data (joint 3D positions). A kernel machine feed with such feature\nis an effective paradigm for 3D action recognition, yielding state-of-the-art\nresults. Yet, the whole framework is affected by the well-known scalability\nissue. In fact, in general, the kernel function has to be evaluated for all\npairs of instances inducing a Gram matrix whose complexity is quadratic in the\nnumber of samples. In this work we reduce such complexity to be linear by\nproposing a novel and explicit feature map to approximate the kernel function.\nThis allows to train a linear classifier with an explicit feature encoding,\nwhich implicitly implements a Log-Euclidean machine in a scalable fashion. Not\nonly we prove that the proposed approximation is unbiased, but also we work out\nan explicit strong bound for its variance, attesting a theoretical superiority\nof our approach with respect to existing ones. Experimentally, we verify that\nour representation provides a compact encoding and outperforms other\napproximation schemes on a number of publicly available benchmark datasets for\n3D action recognition.\n", "rewritten_text": "\\boxed{\n3D action recognition can be improved by using a covariance representation of joint 3D positions as input data. This approach, combined with a kernel machine, has been shown to produce state-of-the-art results. However, a major limitation is the scalability issue, which arises from the need to evaluate the kernel function for all pairs of instances, resulting in a quadratic complexity in the number of samples. To address this, we propose a novel feature map that approximates the kernel function, reducing the complexity to linear. This allows for training a linear classifier with an explicit feature encoding, effectively implementing a Log-Euclidean machine in a scalable manner. We theoretically prove that our approximation is unbiased and provide a strong bound for its variance, demonstrating its superiority over existing methods. Experimental results on public benchmark datasets for 3D action recognition confirm that our representation provides a compact encoding and outperforms other approximation schemes."}, "2212.0583": {"original_text": "  Directly training a document-to-document (Doc2Doc) neural machine translation\n(NMT) via Transformer from scratch, especially on small datasets usually fails\nto converge. Our dedicated probing tasks show that 1) both the absolute\nposition and relative position information gets gradually weakened or even\nvanished once it reaches the upper encoder layers, and 2) the vanishing of\nabsolute position information in encoder output causes the training failure of\nDoc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer\n(P-Transformer) to enhance both the absolute and relative position information\nin both self-attention and cross-attention. Specifically, we integrate absolute\npositional information, i.e., position embeddings, into the query-key pairs\nboth in self-attention and cross-attention through a simple yet effective\naddition operation. Moreover, we also integrate relative position encoding in\nself-attention. The proposed P-Transformer utilizes sinusoidal position\nencoding and does not require any task-specified position embedding, segment\nembedding, or attention mechanism. Through the above methods, we build a\nDoc2Doc NMT model with P-Transformer, which ingests the source document and\ncompletely generates the target document in a sequence-to-sequence (seq2seq)\nway. In addition, P-Transformer can be applied to seq2seq-based\ndocument-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)\ntranslation. Extensive experimental results of Doc2Doc NMT show that\nP-Transformer significantly outperforms strong baselines on widely-used 9\ndocument-level datasets in 7 language pairs, covering small-, middle-, and\nlarge-scales, and achieves a new state-of-the-art. Experimentation on discourse\nphenomena shows that our Doc2Doc NMT models improve the translation quality in\nboth BLEU and discourse coherence. We make our code available on Github.\n", "rewritten_text": "\\boxed{\nTraining a document-to-document neural machine translation (NMT) model from scratch using the Transformer architecture often fails to converge, especially with small datasets. Our analysis reveals that the model loses absolute and relative position information as it progresses through the encoder layers, leading to training failure. To address this issue, we propose a position-aware Transformer (P-Transformer) that enhances both absolute and relative position information in self-attention and cross-attention mechanisms. Our approach integrates absolute positional information through a simple addition operation and incorporates relative position encoding in self-attention. The resulting P-Transformer model can be applied to document-to-document, document-to-sentence, and sentence-to-sentence translation tasks. Experimental results on nine document-level datasets in seven language pairs demonstrate that our P-Transformer model significantly outperforms strong baselines and achieves state-of-the-art performance. Additionally, our model improves translation quality in terms of both BLEU score and discourse coherence. Our code is available on Github.\n}"}, "2308.05430": {"original_text": "  In this work, we propose an ensemble modeling approach for multimodal action\nrecognition. We independently train individual modality models using a variant\nof focal loss tailored to handle the long-tailed distribution of the MECCANO\n[21] dataset. Based on the underlying principle of focal loss, which captures\nthe relationship between tail (scarce) classes and their prediction\ndifficulties, we propose an exponentially decaying variant of focal loss for\nour current task. It initially emphasizes learning from the hard misclassified\nexamples and gradually adapts to the entire range of examples in the dataset.\nThis annealing process encourages the model to strike a balance between\nfocusing on the sparse set of hard samples, while still leveraging the\ninformation provided by the easier ones. Additionally, we opt for the late\nfusion strategy to combine the resultant probability distributions from RGB and\nDepth modalities for final action prediction. Experimental evaluations on the\nMECCANO dataset demonstrate the effectiveness of our approach.\n", "rewritten_text": "\\boxed{This paper presents an ensemble modeling approach for multimodal action recognition, which involves training individual modality models using a modified focal loss function designed to handle the long-tailed distribution of the MECCANO dataset. The proposed focal loss variant, which decays exponentially, prioritizes learning from misclassified examples and adapts to the entire dataset over time, striking a balance between focusing on hard samples and leveraging easier ones. The approach combines the probability distributions from RGB and Depth modalities using late fusion, and experimental results on the MECCANO dataset demonstrate its effectiveness.}"}, "1810.1161": {"original_text": "  Despite remarkable advances in image synthesis research, existing works often\nfail in manipulating images under the context of large geometric\ntransformations. Synthesizing person images conditioned on arbitrary poses is\none of the most representative examples where the generation quality largely\nrelies on the capability of identifying and modeling arbitrary transformations\non different body parts. Current generative models are often built on local\nconvolutions and overlook the key challenges (e.g. heavy occlusions, different\nviews or dramatic appearance changes) when distinct geometric changes happen\nfor each part, caused by arbitrary pose manipulations. This paper aims to\nresolve these challenges induced by geometric variability and spatial\ndisplacements via a new Soft-Gated Warping Generative Adversarial Network\n(Warping-GAN), which is composed of two stages: 1) it first synthesizes a\ntarget part segmentation map given a target pose, which depicts the\nregion-level spatial layouts for guiding image synthesis with higher-level\nstructure constraints; 2) the Warping-GAN equipped with a soft-gated\nwarping-block learns feature-level mapping to render textures from the original\nimage into the generated segmentation map. Warping-GAN is capable of\ncontrolling different transformation degrees given distinct target poses.\nMoreover, the proposed warping-block is light-weight and flexible enough to be\ninjected into any networks. Human perceptual studies and quantitative\nevaluations demonstrate the superiority of our Warping-GAN that significantly\noutperforms all existing methods on two large datasets.\n", "rewritten_text": "\\boxed{\nDespite significant progress in image synthesis, current methods struggle to manipulate images under large geometric transformations. One challenging example is synthesizing person images in arbitrary poses, which requires identifying and modeling transformations on different body parts. Existing generative models, based on local convolutions, fail to address key challenges such as heavy occlusions, different views, and dramatic appearance changes caused by pose manipulations. This paper proposes a novel Soft-Gated Warping Generative Adversarial Network (Warping-GAN) to overcome these challenges. The Warping-GAN consists of two stages: (1) synthesizing a target part segmentation map given a target pose, and (2) using a soft-gated warping-block to render textures from the original image into the generated segmentation map. The Warping-GAN can control different transformation degrees for distinct target poses and is lightweight and flexible enough to be integrated into any network. Human perceptual studies and quantitative evaluations demonstrate the superiority of Warping-GAN, outperforming existing methods on two large datasets.\n}"}, "2410.10407": {"original_text": "  The widespread dissemination of false information through manipulative\ntactics that combine deceptive text and images threatens the integrity of\nreliable sources of information. While there has been research on detecting\nfake news in high resource languages using multimodal approaches, methods for\nlow resource Indic languages primarily rely on textual analysis. This\ndifference highlights the need for robust methods that specifically address\nmultimodal fake news in Indic languages, where the lack of extensive datasets\nand tools presents a significant obstacle to progress. To this end, we\nintroduce the Multimodal Multilingual dataset for Indic Fake News Detection\n(MMIFND). This meticulously curated dataset consists of 28,085 instances\ndistributed across Hindi, Bengali, Marathi, Malayalam, Tamil, Gujarati and\nPunjabi. We further propose the Multimodal Multilingual Caption-aware framework\nfor Fake News Detection (MMCFND). MMCFND utilizes pre-trained unimodal encoders\nand pairwise encoders from a foundational model that aligns vision and\nlanguage, allowing for extracting deep representations from visual and textual\ncomponents of news articles. The multimodal fusion encoder in the foundational\nmodel integrates text and image representations derived from its pairwise\nencoders to generate a comprehensive cross modal representation. Furthermore,\nwe generate descriptive image captions that provide additional context to\ndetect inconsistencies and manipulations. The retrieved features are then fused\nand fed into a classifier to determine the authenticity of news articles. The\ncurated dataset can potentially accelerate research and development in low\nresource environments significantly. Thorough experimentation on MMIFND\ndemonstrates that our proposed framework outperforms established methods for\nextracting relevant fake news detection features.\n", "rewritten_text": "\\boxed{\nThe spread of false information through deceptive tactics that combine misleading text and images poses a threat to the credibility of reliable sources of information. Despite research on detecting fake news in well-resourced languages using multimodal approaches, methods for low-resourced Indic languages mainly rely on text analysis. This disparity highlights the need for robust methods that specifically address multimodal fake news in Indic languages, where the lack of extensive datasets and tools hinders progress. To address this, we introduce the Multimodal Multilingual dataset for Indic Fake News Detection (MMIFND), a meticulously curated dataset of 28,085 instances across seven Indic languages. We also propose the Multimodal Multilingual Caption-aware framework for Fake News Detection (MMCFND), which utilizes pre-trained encoders to extract deep representations from visual and textual components of news articles. The framework integrates text and image representations to generate a comprehensive cross-modal representation, and generates descriptive image captions to detect inconsistencies and manipulations. Our proposed framework outperforms established methods in extracting relevant fake news detection features, and the curated dataset has the potential to significantly accelerate research and development in low-resourced environments.\n}"}, "2401.17207": {"original_text": "  A comprehensive understanding of the organizational principles in the human\nbrain requires, among other factors, well-quantifiable descriptors of nerve\nfiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a\nmicroscopic imaging technique that enables insights into the fine-grained\norganization of myelinated nerve fibers with high resolution. Descriptors\ncharacterizing the fiber architecture observed in 3D-PLI would enable\ndownstream analysis tasks such as multimodal correlation studies, clustering,\nand mapping. However, best practices for observer-independent characterization\nof fiber architecture in 3D-PLI are not yet available. To this end, we propose\nthe application of a fully data-driven approach to characterize nerve fiber\narchitecture in 3D-PLI images using self-supervised representation learning. We\nintroduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the\nspatial neighborhood of texture examples across histological brain sections of\na 3D reconstructed volume to sample positive pairs for contrastive learning. We\ncombine this sampling strategy with specifically designed image augmentations\nto gain robustness to typical variations in 3D-PLI parameter maps. The approach\nis demonstrated for the 3D reconstructed occipital lobe of a vervet monkey\nbrain. We show that extracted features are highly sensitive to different\nconfigurations of nerve fibers, yet robust to variations between consecutive\nbrain sections arising from histological processing. We demonstrate their\npractical applicability for retrieving clusters of homogeneous fiber\narchitecture and performing data mining for interactively selected templates of\nspecific components of fiber architecture such as U-fibers.\n", "rewritten_text": "\\boxed{\nUnderstanding the human brain's organization requires quantifiable descriptions of nerve fiber architecture. Three-dimensional polarized light imaging (3D-PLI) provides high-resolution insights into myelinated nerve fibers. However, there is a lack of standardized methods for characterizing fiber architecture in 3D-PLI images. To address this, we propose a data-driven approach using self-supervised representation learning to characterize nerve fiber architecture. Our method, 3D-Context Contrastive Learning (CL-3D), uses spatial neighborhood information from 3D reconstructed brain volumes to sample positive pairs for contrastive learning. We combine this with image augmentations to ensure robustness to variations in 3D-PLI parameter maps. We demonstrate the approach on a vervet monkey brain's occipital lobe, showing that extracted features are sensitive to different nerve fiber configurations and robust to variations between brain sections. The features are applicable for clustering homogeneous fiber architecture and data mining for specific fiber components, such as U-fibers.\n}"}, "1703.04454": {"original_text": "  We address the problem of estimating human pose and body shape from 3D scans\nover time. Reliable estimation of 3D body shape is necessary for many\napplications including virtual try-on, health monitoring, and avatar creation\nfor virtual reality. Scanning bodies in minimal clothing, however, presents a\npractical barrier to these applications. We address this problem by estimating\nbody shape under clothing from a sequence of 3D scans. Previous methods that\nhave exploited body models produce smooth shapes lacking personalized details.\nWe contribute a new approach to recover a personalized shape of the person. The\nestimated shape deviates from a parametric model to fit the 3D scans. We\ndemonstrate the method using high quality 4D data as well as sequences of\nvisual hulls extracted from multi-view images. We also make available BUFF, a\nnew 4D dataset that enables quantitative evaluation\n(http://buff.is.tue.mpg.de). Our method outperforms the state of the art in\nboth pose estimation and shape estimation, qualitatively and quantitatively.\n", "rewritten_text": "\\boxed{This paper tackles the challenge of estimating human pose and body shape from 3D scans over time, which is crucial for various applications such as virtual try-on, health monitoring, and avatar creation for virtual reality. To overcome the limitation of scanning bodies in minimal clothing, we propose a novel approach to estimate body shape under clothing from a sequence of 3D scans. Unlike previous methods that produce smooth shapes lacking personalized details, our approach recovers a personalized shape that deviates from a parametric model to fit the 3D scans. We validate our method using high-quality 4D data and sequences of visual hulls extracted from multi-view images, and introduce a new 4D dataset called BUFF for quantitative evaluation. Our results show that our method outperforms the state of the art in both pose estimation and shape estimation, both qualitatively and quantitatively.}"}, "2002.01359": {"original_text": "  This paper gives an overview of the Schema-Guided Dialogue State Tracking\ntask of the 8th Dialogue System Technology Challenge. The goal of this task is\nto develop dialogue state tracking models suitable for large-scale virtual\nassistants, with a focus on data-efficient joint modeling across domains and\nzero-shot generalization to new APIs. This task provided a new dataset\nconsisting of over 16000 dialogues in the training set spanning 16 domains to\nhighlight these challenges, and a baseline model capable of zero-shot\ngeneralization to new APIs. Twenty-five teams participated, developing a range\nof neural network models, exceeding the performance of the baseline model by a\nvery high margin. The submissions incorporated a variety of pre-trained\nencoders and data augmentation techniques. This paper describes the task\ndefinition, dataset and evaluation methodology. We also summarize the approach\nand results of the submitted systems to highlight the overall trends in the\nstate-of-the-art.\n", "rewritten_text": "\\boxed{This paper provides an overview of the 8th Dialogue System Technology Challenge, specifically the Schema-Guided Dialogue State Tracking task. The task aims to develop dialogue state tracking models that can efficiently handle large-scale virtual assistants, joint modeling across multiple domains, and zero-shot generalization to new APIs. A new dataset of over 16,000 dialogues across 16 domains was provided, along with a baseline model that can generalize to new APIs. Twenty-five teams participated, submitting a range of neural network models that significantly outperformed the baseline model. The submissions utilized various pre-trained encoders and data augmentation techniques. This paper outlines the task definition, dataset, and evaluation methodology, and summarizes the approaches and results of the submitted systems to highlight current trends in the state-of-the-art.}"}, "2403.10574": {"original_text": "  The rich spatio-temporal information is crucial to capture the complicated\ntarget appearance variations in visual tracking. However, most top-performing\ntracking algorithms rely on many hand-crafted components for spatio-temporal\ninformation aggregation. Consequently, the spatio-temporal information is far\naway from being fully explored. To alleviate this issue, we propose an adaptive\ntracker with spatio-temporal transformers (named AQATrack), which adopts simple\nautoregressive queries to effectively learn spatio-temporal information without\nmany hand-designed components. Firstly, we introduce a set of learnable and\nautoregressive queries to capture the instantaneous target appearance changes\nin a sliding window fashion. Then, we design a novel attention mechanism for\nthe interaction of existing queries to generate a new query in current frame.\nFinally, based on the initial target template and learnt autoregressive\nqueries, a spatio-temporal information fusion module (STM) is designed for\nspatiotemporal formation aggregation to locate a target object. Benefiting from\nthe STM, we can effectively combine the static appearance and instantaneous\nchanges to guide robust tracking. Extensive experiments show that our method\nsignificantly improves the tracker's performance on six popular tracking\nbenchmarks: LaSOT, LaSOText, TrackingNet, GOT-10k, TNL2K, and UAV123.\n", "rewritten_text": "\\boxed{\nCapturing the complexities of target appearance variations in visual tracking requires rich spatio-temporal information. However, current top-performing tracking algorithms rely heavily on hand-crafted components to aggregate this information, leaving much of it unexplored. To address this, we propose AQATrack, an adaptive tracker that utilizes spatio-temporal transformers to learn spatio-temporal information without relying on hand-designed components. AQATrack employs learnable autoregressive queries to capture instantaneous target appearance changes in a sliding window fashion, and a novel attention mechanism to generate new queries. A spatio-temporal information fusion module combines the initial target template and learnt queries to locate the target object, effectively combining static appearance and instantaneous changes for robust tracking. Our approach significantly outperforms existing methods on six popular tracking benchmarks."}, "2206.1503": {"original_text": "  Question Answering (QA) is one of the most important natural language\nprocessing (NLP) tasks. It aims using NLP technologies to generate a\ncorresponding answer to a given question based on the massive unstructured\ncorpus. With the development of deep learning, more and more challenging QA\ndatasets are being proposed, and lots of new methods for solving them are also\nemerging. In this paper, we investigate influential QA datasets that have been\nreleased in the era of deep learning. Specifically, we begin with introducing\ntwo of the most common QA tasks - textual question answer and visual question\nanswering - separately, covering the most representative datasets, and then\ngive some current challenges of QA research.\n", "rewritten_text": "\\boxed{Question Answering (QA) is a crucial task in natural language processing (NLP) that involves generating an answer to a given question based on a large unstructured corpus using NLP technologies. With the advancement of deep learning, increasingly challenging QA datasets have been proposed, and novel methods to tackle them have emerged. This paper examines influential QA datasets released during the deep learning era, focusing on two primary QA tasks: textual question answering and visual question answering, including their most representative datasets, and discusses current challenges in QA research.}"}, "1604.04333": {"original_text": "  Deep Convolutional Neural Networks (CNN) have exhibited superior performance\nin many visual recognition tasks including image classification, object\ndetection, and scene label- ing, due to their large learning capacity and\nresistance to overfit. For the image classification task, most of the current\ndeep CNN- based approaches take the whole size-normalized image as input and\nhave achieved quite promising results. Compared with the previously dominating\napproaches based on feature extraction, pooling, and classification, the deep\nCNN-based approaches mainly rely on the learning capability of deep CNN to\nachieve superior results: the burden of minimizing intra-class variation while\nmaximizing inter-class difference is entirely dependent on the implicit feature\nlearning component of deep CNN; we rely upon the implicitly learned filters and\npooling component to select the discriminative regions, which correspond to the\nactivated neurons. However, if the irrelevant regions constitute a large\nportion of the image of interest, the classification performance of the deep\nCNN, which takes the whole image as input, can be heavily affected. To solve\nthis issue, we propose a novel latent CNN framework, which treats the most\ndiscriminate region as a latent variable. We can jointly learn the global CNN\nwith the latent CNN to avoid the aforementioned big irrelevant region issue,\nand our experimental results show the evident advantage of the proposed latent\nCNN over traditional deep CNN: latent CNN outperforms the state-of-the-art\nperformance of deep CNN on standard benchmark datasets including the CIFAR-10,\nCIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset.\n", "rewritten_text": "\\boxed{\nDeep Convolutional Neural Networks (CNNs) have achieved exceptional performance in various visual recognition tasks, including image classification, object detection, and scene labeling, due to their high learning capacity and resistance to overfitting. However, traditional deep CNN-based approaches, which take the entire normalized image as input, can be negatively impacted by irrelevant regions in the image. To address this issue, we propose a novel latent CNN framework that treats the most discriminative region as a latent variable. By jointly learning the global CNN with the latent CNN, we can avoid the problem of irrelevant regions and improve classification performance. Our experimental results demonstrate the superiority of the proposed latent CNN over traditional deep CNNs, achieving state-of-the-art performance on benchmark datasets such as CIFAR-10, CIFAR-100, MNIST, and PASCAL VOC 2007 Classification dataset.\n}"}, "2301.0734": {"original_text": "  Semi-Supervised Semantic Segmentation aims at training the segmentation model\nwith limited labeled data and a large amount of unlabeled data. To effectively\nleverage the unlabeled data, pseudo labeling, along with the teacher-student\nframework, is widely adopted in semi-supervised semantic segmentation. Though\nproved to be effective, this paradigm suffers from incorrect pseudo labels\nwhich inevitably exist and are taken as auxiliary training data. To alleviate\nthe negative impact of incorrect pseudo labels, we delve into the current\nSemi-Supervised Semantic Segmentation frameworks. We argue that the unlabeled\ndata with pseudo labels can facilitate the learning of representative features\nin the feature extractor, but it is unreliable to supervise the mask predictor.\nMotivated by this consideration, we propose a novel framework, Gentle Teaching\nAssistant (GTA-Seg) to disentangle the effects of pseudo labels on feature\nextractor and mask predictor of the student model. Specifically, in addition to\nthe original teacher-student framework, our method introduces a teaching\nassistant network which directly learns from pseudo labels generated by the\nteacher network. The gentle teaching assistant (GTA) is coined gentle since it\nonly transfers the beneficial feature representation knowledge in the feature\nextractor to the student model in an Exponential Moving Average (EMA) manner,\nprotecting the student model from the negative influences caused by unreliable\npseudo labels in the mask predictor. The student model is also supervised by\nreliable labeled data to train an accurate mask predictor, further facilitating\nfeature representation. Extensive experiment results on benchmark datasets\nvalidate that our method shows competitive performance against previous\nmethods. Code is available at https://github.com/Jin-Ying/GTA-Seg.\n", "rewritten_text": "\\boxed{\nSemi-supervised semantic segmentation aims to train a model with limited labeled data and a large amount of unlabeled data. To utilize the unlabeled data, pseudo labeling and the teacher-student framework are commonly used. However, this approach is limited by the presence of incorrect pseudo labels. To mitigate this issue, we propose a novel framework, Gentle Teaching Assistant (GTA-Seg), which separates the effects of pseudo labels on the feature extractor and mask predictor of the student model. Our method introduces a teaching assistant network that learns from pseudo labels generated by the teacher network, and gently transfers beneficial feature representation knowledge to the student model while protecting it from negative influences. The student model is also supervised by reliable labeled data to train an accurate mask predictor. Experimental results on benchmark datasets demonstrate the competitiveness of our method."}, "2108.03886": {"original_text": "  A meme is an part of media created to share an opinion or emotion across the\ninternet. Due to its popularity, memes have become the new forms of\ncommunication on social media. However, due to its nature, they are being used\nin harmful ways such as trolling and cyberbullying progressively. Various data\nmodelling methods create different possibilities in feature extraction and\nturning them into beneficial information. The variety of modalities included in\ndata plays a significant part in predicting the results. We try to explore the\nsignificance of visual features of images in classifying memes. Memes are a\nblend of both image and text, where the text is embedded into the image. We try\nto incorporate the memes as troll and non-trolling memes based on the images\nand the text on them. However, the images are to be analysed and combined with\nthe text to increase performance. Our work illustrates different textual\nanalysis methods and contrasting multimodal methods ranging from simple merging\nto cross attention to utilising both worlds' - best visual and textual\nfeatures. The fine-tuned cross-lingual language model, XLM, performed the best\nin textual analysis, and the multimodal transformer performs the best in\nmultimodal analysis.\n", "rewritten_text": "\\boxed{\nMemes, which are units of media designed to convey opinions or emotions online, have become a popular form of communication on social media. However, they are increasingly being used for harmful purposes such as trolling and cyberbullying. To address this issue, various data modeling methods can be employed to extract features from memes and turn them into useful information. The diversity of data modalities plays a crucial role in predicting outcomes. This study explores the significance of visual features in classifying memes, which typically combine images and embedded text. We aim to categorize memes as either trolling or non-trolling based on their images and text. To achieve this, we analyze and combine image and text data to improve performance. Our research compares different textual analysis methods and multimodal approaches, ranging from simple merging to cross-attention, to leverage the strengths of both visual and textual features. Our results show that the fine-tuned cross-lingual language model, XLM, performs best in textual analysis, while the multimodal transformer excels in multimodal analysis."}, "2305.03973": {"original_text": "  Implicit Discourse Relation Recognition (IDRR) is a sophisticated and\nchallenging task to recognize the discourse relations between the arguments\nwith the absence of discourse connectives. The sense labels for each discourse\nrelation follow a hierarchical classification scheme in the annotation process\n(Prasad et al., 2008), forming a hierarchy structure. Most existing works do\nnot well incorporate the hierarchy structure but focus on the syntax features\nand the prior knowledge of connectives in the manner of pure text\nclassification. We argue that it is more effective to predict the paths inside\nthe hierarchical tree (e.g., \"Comparison -> Contrast -> however\") rather than\nflat labels (e.g., Contrast) or connectives (e.g., however). We propose a\nprompt-based path prediction method to utilize the interactive information and\nintrinsic senses among the hierarchy in IDRR. This is the first work that\ninjects such structure information into pre-trained language models via prompt\ntuning, and the performance of our solution shows significant and consistent\nimprovement against competitive baselines.\n", "rewritten_text": "\\boxed{Recognizing implicit discourse relations between arguments without connectives is a complex task. Existing approaches focus on syntax features and prior knowledge of connectives, neglecting the hierarchical structure of sense labels. We propose a novel prompt-based method that predicts paths within the hierarchical tree, leveraging interactive information and intrinsic senses. By incorporating this structure into pre-trained language models via prompt tuning, our solution achieves significant and consistent improvement over competitive baselines.}"}, "1908.06665": {"original_text": "  Recently, significant progresses have been made in object detection on common\nbenchmarks (i.e., Pascal VOC). However, object detection in real world is still\nchallenging due to the serious data imbalance. Images in real world are\ndominated by easy samples like the wide range of background and some easily\nrecognizable objects, for example. Although two-stage detectors like Faster\nR-CNN achieved big successes in object detection due to the strategy of\nextracting region proposals by region proposal network, they show their poor\nadaption in real-world object detection as a result of without considering\nmining hard samples during extracting region proposals. To address this issue,\nwe propose a Cascade framework of Region Proposal Networks, referred to as\nC-RPNs. The essence of C-RPNs is adopting multiple stages to mine hard samples\nwhile extracting region proposals and learn stronger classifiers. Meanwhile, a\nfeature chain and a score chain are proposed to help learning more\ndiscriminative representations for proposals. Moreover, a loss function of\ncascade stages is designed to train cascade classifiers through\nbackpropagation. Our proposed method has been evaluated on Pascal VOC and\nseveral challenging datasets like BSBDV 2017, CityPersons, etc. Our method\nachieves competitive results compared with the current state-of-the-arts and\nall-sided improvements in error analysis, validating its efficacy for detection\nin real world.\n", "rewritten_text": "\\boxed{\nObject detection has made significant progress on standard benchmarks like Pascal VOC, but it remains a challenge in real-world scenarios due to the severe data imbalance. Real-world images are dominated by easy samples, such as diverse backgrounds and easily recognizable objects. Two-stage detectors like Faster R-CNN have achieved success in object detection by extracting region proposals, but they struggle in real-world detection because they don't consider mining hard samples during proposal extraction. To address this, we propose a Cascade framework of Region Proposal Networks (C-RPNs), which adopts multiple stages to mine hard samples and learn stronger classifiers. Our approach also includes a feature chain and score chain to learn more discriminative representations for proposals, and a cascade loss function to train classifiers through backpropagation. Our method has been evaluated on various datasets, including Pascal VOC, BSBDV 2017, and CityPersons, and has achieved competitive results and all-around improvements in error analysis, demonstrating its effectiveness in real-world detection.\n}"}, "2001.04388": {"original_text": "  The efficient fusion of depth maps is a key part of most state-of-the-art 3D\nreconstruction methods. Besides requiring high accuracy, these depth fusion\nmethods need to be scalable and real-time capable. To this end, we present a\nnovel real-time capable machine learning-based method for depth map fusion.\nSimilar to the seminal depth map fusion approach by Curless and Levoy, we only\nupdate a local group of voxels to ensure real-time capability. Instead of a\nsimple linear fusion of depth information, we propose a neural network that\npredicts non-linear updates to better account for typical fusion errors. Our\nnetwork is composed of a 2D depth routing network and a 3D depth fusion network\nwhich efficiently handle sensor-specific noise and outliers. This is especially\nuseful for surface edges and thin objects for which the original approach\nsuffers from thickening artifacts. Our method outperforms the traditional\nfusion approach and related learned approaches on both synthetic and real data.\nWe demonstrate the performance of our method in reconstructing fine geometric\ndetails from noise and outlier contaminated data on various scenes.\n", "rewritten_text": "\\boxed{We propose a novel, real-time capable machine learning-based method for fusing depth maps, which is a crucial component of state-of-the-art 3D reconstruction techniques. Our approach updates a local group of voxels to ensure scalability and uses a neural network to predict non-linear updates, outperforming traditional linear fusion methods. The network consists of a 2D depth routing network and a 3D depth fusion network, which effectively handle sensor-specific noise and outliers, particularly for surface edges and thin objects. Our method demonstrates superior performance on both synthetic and real data, accurately reconstructing fine geometric details from noisy and outlier-contaminated data in various scenes.}"}, "1810.08768": {"original_text": "  Motion estimation (ME) and motion compensation (MC) have been widely used for\nclassical video frame interpolation systems over the past decades. Recently, a\nnumber of data-driven frame interpolation methods based on convolutional neural\nnetworks have been proposed. However, existing learning based methods typically\nestimate either flow or compensation kernels, thereby limiting performance on\nboth computational efficiency and interpolation accuracy. In this work, we\npropose a motion estimation and compensation driven neural network for video\nframe interpolation. A novel adaptive warping layer is developed to integrate\nboth optical flow and interpolation kernels to synthesize target frame pixels.\nThis layer is fully differentiable such that both the flow and kernel\nestimation networks can be optimized jointly. The proposed model benefits from\nthe advantages of motion estimation and compensation methods without using\nhand-crafted features. Compared to existing methods, our approach is\ncomputationally efficient and able to generate more visually appealing results.\nFurthermore, the proposed MEMC-Net can be seamlessly adapted to several video\nenhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive\nquantitative and qualitative evaluations demonstrate that the proposed method\nperforms favorably against the state-of-the-art video frame interpolation and\nenhancement algorithms on a wide range of datasets.\n", "rewritten_text": "\\boxed{\nMotion estimation and motion compensation have been essential components of traditional video frame interpolation systems for decades. Recently, data-driven approaches using convolutional neural networks have emerged. However, these learning-based methods typically focus on either estimating optical flow or compensation kernels, which limits their performance in terms of computational efficiency and interpolation accuracy. This work proposes a novel neural network that integrates motion estimation and compensation for video frame interpolation. A key component is an adaptive warping layer that combines optical flow and interpolation kernels to synthesize target frame pixels. This layer is fully differentiable, allowing for joint optimization of flow and kernel estimation networks. The proposed MEMC-Net leverages the strengths of motion estimation and compensation methods without relying on hand-crafted features, resulting in computationally efficient and visually appealing results. Moreover, MEMC-Net can be easily adapted to various video enhancement tasks, such as super-resolution, denoising, and deblocking. Extensive evaluations demonstrate that the proposed method outperforms state-of-the-art video frame interpolation and enhancement algorithms across a range of datasets.\n}"}, "2303.12793": {"original_text": "  This work focuses on sign language retrieval-a recently proposed task for\nsign language understanding. Sign language retrieval consists of two sub-tasks:\ntext-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval.\nDifferent from traditional video-text retrieval, sign language videos, not only\ncontain visual signals but also carry abundant semantic meanings by themselves\ndue to the fact that sign languages are also natural languages. Considering\nthis character, we formulate sign language retrieval as a cross-lingual\nretrieval problem as well as a video-text retrieval task. Concretely, we take\ninto account the linguistic properties of both sign languages and natural\nlanguages, and simultaneously identify the fine-grained cross-lingual (i.e.,\nsign-to-word) mappings while contrasting the texts and the sign videos in a\njoint embedding space. This process is termed as cross-lingual contrastive\nlearning. Another challenge is raised by the data scarcity issue-sign language\ndatasets are orders of magnitude smaller in scale than that of speech\nrecognition. We alleviate this issue by adopting a domain-agnostic sign encoder\npre-trained on large-scale sign videos into the target domain via\npseudo-labeling. Our framework, termed as domain-aware sign language retrieval\nvia Cross-lingual Contrastive learning or CiCo for short, outperforms the\npioneering method by large margins on various datasets, e.g., +22.4 T2V and\n+28.0 V2T R@1 improvements on How2Sign dataset, and +13.7 T2V and +17.1 V2T R@1\nimprovements on PHOENIX-2014T dataset. Code and models are available at:\nhttps://github.com/FangyunWei/SLRT.\n", "rewritten_text": "\\boxed{\nThis research focuses on sign language retrieval, a novel task that involves understanding sign language. Sign language retrieval consists of two sub-tasks: retrieving sign videos from text (T2V) and retrieving text from sign videos (V2T). Unlike traditional video-text retrieval, sign language videos contain both visual and semantic information, making it a cross-lingual retrieval problem. To address this, we propose a cross-lingual contrastive learning approach that considers the linguistic properties of both sign languages and natural languages, and identifies fine-grained mappings between signs and words in a joint embedding space. Additionally, we tackle the issue of limited sign language datasets by using a pre-trained sign encoder and pseudo-labeling. Our framework, CiCo, significantly outperforms existing methods on various datasets, achieving improvements of +22.4 T2V and +28.0 V2T R@1 on How2Sign, and +13.7 T2V and +17.1 V2T R@1 on PHOENIX-2014T. The code and models are available at https://github.com/FangyunWei/SLRT.\n}"}, "2410.01928": {"original_text": "  In the domain of battery research, the processing of high-resolution\nmicroscopy images is a challenging task, as it involves dealing with complex\nimages and requires a prior understanding of the components involved. The\nutilization of deep learning methodologies for image analysis has attracted\nconsiderable interest in recent years, with multiple investigations employing\nsuch techniques for image segmentation and analysis within the realm of battery\nresearch. However, the automated analysis of high-resolution microscopy images\nfor detecting phases and components in composite materials is still an\nunderexplored area. This work proposes a novel workflow for detecting\ncomponents and phase segmentation from raw high resolution transmission\nelectron microscopy (TEM) images using a trained U-Net segmentation model. The\ndeveloped model can expedite the detection of components and phase\nsegmentation, diminishing the temporal and cognitive demands associated with\nscrutinizing an extensive array of TEM images, thereby mitigating the potential\nfor human errors. This approach presents a novel and efficient image analysis\napproach with broad applicability beyond the battery field and holds potential\nfor application in other related domains characterized by phase and composition\ndistribution, such as alloy production.\n", "rewritten_text": "\\boxed{\nIn the field of battery research, analyzing high-resolution microscopy images is a complex task that requires prior knowledge of the components involved. Recently, deep learning methods have gained popularity for image analysis, with many studies using them for image segmentation and analysis in battery research. However, automatically analyzing high-resolution microscopy images to detect phases and components in composite materials remains an underexplored area. This study proposes a new workflow that uses a trained U-Net segmentation model to detect components and segment phases from raw high-resolution transmission electron microscopy (TEM) images. This approach can accelerate component detection and phase segmentation, reducing the time and mental effort required to examine a large number of TEM images, and minimizing the risk of human error. This efficient image analysis approach has broad applicability beyond the battery field and can be applied to other fields involving phase and composition distribution, such as alloy production.\n}"}, "2103.15467": {"original_text": "  Unsupervised domain adaptation (UDA) becomes more and more popular in\ntackling real-world problems without ground truth of the target domain. Though\ntedious annotation work is not required, UDA unavoidably faces two problems: 1)\nhow to narrow the domain discrepancy to boost the transferring performance; 2)\nhow to improve pseudo annotation producing mechanism for self-supervised\nlearning (SSL). In this paper, we focus on UDA for semantic segmentation task.\nFirstly, we introduce adversarial learning into style gap bridging mechanism to\nkeep the style information from two domains in the similar space. Secondly, to\nkeep the balance of pseudo labels on each category, we propose a\ncategory-adaptive threshold mechanism to choose category-wise pseudo labels for\nSSL. The experiments are conducted using GTA5 as the source domain, Cityscapes\nas the target domain. The results show that our model outperforms the\nstate-of-the-arts with a noticeable gain on cross-domain adaptation tasks.\n", "rewritten_text": "\\boxed{Unsupervised domain adaptation (UDA) has gained popularity in addressing real-world problems where target domain ground truth is unavailable. However, UDA faces two significant challenges: reducing the domain discrepancy to enhance transfer performance and improving the pseudo-annotation generation mechanism for self-supervised learning (SSL). This paper focuses on UDA for semantic segmentation tasks and proposes two novel solutions. Firstly, it incorporates adversarial learning into a style gap bridging mechanism to align style information from both domains. Secondly, it introduces a category-adaptive threshold mechanism to balance pseudo labels across categories for SSL. Experimental results using GTA5 as the source domain and Cityscapes as the target domain demonstrate that the proposed model outperforms state-of-the-art methods, achieving a significant improvement in cross-domain adaptation tasks.}"}, "2203.17013": {"original_text": "  Video streams are utilised to guide minimally-invasive surgery and diagnostic\nprocedures in a wide range of procedures, and many computer assisted techniques\nhave been developed to automatically analyse them. These approaches can provide\nadditional information to the surgeon such as lesion detection, instrument\nnavigation, or anatomy 3D shape modeling. However, the necessary image features\nto recognise these patterns are not always reliably detected due to the\npresence of irregular light patterns such as specular highlight reflections. In\nthis paper, we aim at removing specular highlights from endoscopic videos using\nmachine learning. We propose using a temporal generative adversarial network\n(GAN) to inpaint the hidden anatomy under specularities, inferring its\nappearance spatially and from neighbouring frames where they are not present in\nthe same location. This is achieved using in-vivo data of gastric endoscopy\n(Hyper-Kvasir) in a fully unsupervised manner that relies on automatic\ndetection of specular highlights. System evaluations show significant\nimprovements to traditional methods through direct comparison as well as other\nmachine learning techniques through an ablation study that depicts the\nimportance of the network's temporal and transfer learning components. The\ngeneralizability of our system to different surgical setups and procedures was\nalso evaluated qualitatively on in-vivo data of gastric endoscopy and ex-vivo\nporcine data (SERV-CT, SCARED). We also assess the effect of our method in\ncomputer vision tasks that underpin 3D reconstruction and camera motion\nestimation, namely stereo disparity, optical flow, and sparse point feature\nmatching. These are evaluated quantitatively and qualitatively and results show\na positive effect of specular highlight inpainting on these tasks in a novel\ncomprehensive analysis.\n", "rewritten_text": "\\boxed{\nVideo streams play a crucial role in guiding minimally-invasive surgeries and diagnostic procedures. Computer-assisted techniques have been developed to analyze these streams, providing surgeons with valuable information such as lesion detection and anatomy modeling. However, the presence of irregular light patterns, known as specular highlights, can hinder the detection of necessary image features. This paper proposes a machine learning approach to remove specular highlights from endoscopic videos using a temporal generative adversarial network (GAN). The GAN is trained to inpaint hidden anatomy under specularities, using spatial and temporal information from neighboring frames. The approach is evaluated using in-vivo gastric endoscopy data and shows significant improvements over traditional methods. The system's generalizability is also demonstrated on different surgical setups and procedures. Furthermore, the impact of specular highlight removal on computer vision tasks, such as 3D reconstruction and camera motion estimation, is assessed and found to have a positive effect.\n}"}, "2010.02808": {"original_text": "  We propose a method to learn image representations from uncurated videos. We\ncombine a supervised loss from off-the-shelf object detectors and\nself-supervised losses which naturally arise from the video-shot-frame-object\nhierarchy present in each video. We report competitive results on 19 transfer\nlearning tasks of the Visual Task Adaptation Benchmark (VTAB), and on 8\nout-of-distribution-generalization tasks, and discuss the benefits and\nshortcomings of the proposed approach. In particular, it improves over the\nbaseline on all 18/19 few-shot learning tasks and 8/8 out-of-distribution\ngeneralization tasks. Finally, we perform several ablation studies and analyze\nthe impact of the pretrained object detector on the performance across this\nsuite of tasks.\n", "rewritten_text": "\\boxed{We introduce a novel approach to learn image representations from uncurated videos by combining supervised object detection losses with self-supervised losses derived from the video-shot-frame-object hierarchy. Our method achieves competitive results on 19 transfer learning tasks and 8 out-of-distribution generalization tasks, outperforming the baseline on all few-shot learning tasks and out-of-distribution generalization tasks. We also conduct ablation studies to examine the impact of the pre-trained object detector on performance across these tasks.}"}, "1903.12529": {"original_text": "  While deep neural networks (DNN) based single image super-resolution (SISR)\nmethods are rapidly gaining popularity, they are mainly designed for the\nwidely-used bicubic degradation, and there still remains the fundamental\nchallenge for them to super-resolve low-resolution (LR) image with arbitrary\nblur kernels. In the meanwhile, plug-and-play image restoration has been\nrecognized with high flexibility due to its modular structure for easy plug-in\nof denoiser priors. In this paper, we propose a principled formulation and\nframework by extending bicubic degradation based deep SISR with the help of\nplug-and-play framework to handle LR images with arbitrary blur kernels.\nSpecifically, we design a new SISR degradation model so as to take advantage of\nexisting blind deblurring methods for blur kernel estimation. To optimize the\nnew degradation induced energy function, we then derive a plug-and-play\nalgorithm via variable splitting technique, which allows us to plug any\nsuper-resolver prior rather than the denoiser prior as a modular part.\nQuantitative and qualitative evaluations on synthetic and real LR images\ndemonstrate that the proposed deep plug-and-play super-resolution framework is\nflexible and effective to deal with blurry LR images.\n", "rewritten_text": "\\boxed{While deep neural networks have made significant progress in single image super-resolution, they are limited to handling images degraded by bicubic kernels. To overcome this limitation, we propose a novel framework that combines the strengths of deep SISR with the flexibility of plug-and-play image restoration. Our approach allows for the estimation of arbitrary blur kernels and incorporates any super-resolver prior into the framework, making it a powerful tool for handling blurry low-resolution images. Experimental results demonstrate the effectiveness and flexibility of our proposed deep plug-and-play super-resolution framework.}"}, "2407.05092": {"original_text": "  Computational and human perception are often considered separate approaches\nfor studying sound changes over time; few works have touched on the\nintersection of both. To fill this research gap, we provide a pioneering review\ncontrasting computational with human perception from the perspectives of\nmethods and tasks. Overall, computational approaches rely on computer-driven\nmodels to perceive historical sound changes on etymological datasets, while\nhuman approaches use listener-driven models to perceive ongoing sound changes\non recording corpora. Despite their differences, both approaches complement\neach other on phonetic and acoustic levels, showing the potential to achieve a\nmore comprehensive perception of sound change. Moreover, we call for a\ncomparative study on the datasets used by both approaches to investigate the\ninfluence of historical sound changes on ongoing changes. Lastly, we discuss\nthe applications of sound change in computational linguistics, and point out\nthat perceiving sound change alone is insufficient, as many processes of\nlanguage change are complex, with entangled changes at syntactic, semantic, and\nphonetic levels.\n", "rewritten_text": "\\boxed{\nThe study of sound changes over time has traditionally been approached from two separate angles: computational perception and human perception. This review aims to bridge the gap between these two approaches by comparing and contrasting their methods and tasks. While computational models analyze etymological datasets to identify historical sound changes, human listeners examine recording corpora to detect ongoing changes. Despite their differences, both approaches can complement each other on phonetic and acoustic levels, offering a more comprehensive understanding of sound change. Furthermore, a comparative study of the datasets used by both approaches could provide valuable insights into the impact of historical sound changes on ongoing changes. The applications of sound change research in computational linguistics are also discussed, highlighting the importance of considering the complex interplay between syntactic, semantic, and phonetic changes in language evolution.\n}"}, "1812.03621": {"original_text": "  The majority of Multi-Object Tracking (MOT) algorithms based on the\ntracking-by-detection scheme do not use higher order dependencies among objects\nor tracklets, which makes them less effective in handling complex scenarios. In\nthis work, we present a new near-online MOT algorithm based on non-uniform\nhypergraph, which can model different degrees of dependencies among tracklets\nin a unified objective. The nodes in the hypergraph correspond to the tracklets\nand the hyperedges with different degrees encode various kinds of dependencies\namong them. Specifically, instead of setting the weights of hyperedges with\ndifferent degrees empirically, they are learned automatically using the\nstructural support vector machine algorithm (SSVM). Several experiments are\ncarried out on various challenging datasets (i.e., PETS09, ParkingLot sequence,\nSubwayFace, and MOT16 benchmark), to demonstrate that our method achieves\nfavorable performance against the state-of-the-art MOT methods.\n", "rewritten_text": "\\boxed{Most Multi-Object Tracking (MOT) algorithms that rely on the tracking-by-detection approach fail to consider complex relationships between objects or tracklets, leading to poor performance in challenging scenarios. This paper proposes a novel near-online MOT algorithm that utilizes a non-uniform hypergraph to model varying degrees of dependencies between tracklets in a unified objective. The hypergraph's nodes represent tracklets, while hyperedges with different degrees encode various dependencies. Notably, the weights of these hyperedges are learned automatically using the structural support vector machine algorithm, rather than being set empirically. Experimental results on several challenging datasets (PETS09, ParkingLot sequence, SubwayFace, and MOT16 benchmark) demonstrate that our method outperforms state-of-the-art MOT methods.}"}, "2211.08358": {"original_text": "  Few-shot classification has made great strides due to foundation models that,\nthrough priming and prompting, are highly effective few-shot learners. However,\nthis approach has high variance both across different sets of few shots (data\nselection) and across different finetuning runs (run variability). This is\nproblematic not only because it impedes the fair comparison of different\napproaches, but especially because it makes few-shot learning too unreliable\nfor many real-world applications. To alleviate these issues, we make two\ncontributions for more stable and effective few-shot learning: First, we\npropose novel ensembling methods and show that they substantially reduce run\nvariability. Second, we introduce a new active learning (AL) criterion for data\nselection and present the first AL-based approach specifically tailored towards\nprompt-based learning. In our experiments, we show that our combined method,\nMEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),\nimproves overall performance of prompt-based finetuning by 2.3 points on five\ndiverse tasks. We publicly share our code and data splits in\nhttps://github.com/akoksal/MEAL.\n", "rewritten_text": "\\boxed{Few-shot classification has made significant progress thanks to foundation models that can learn from limited data through priming and prompting. However, this approach suffers from high variability in performance depending on the specific few-shot examples used and the finetuning runs. This variability hinders fair comparisons between different approaches and makes few-shot learning unreliable for real-world applications. To address these issues, we propose two solutions: novel ensembling methods to reduce run variability and an active learning criterion for selecting the most informative few-shot examples. Our combined approach, MEAL, improves the performance of prompt-based finetuning by 2.3 points on five diverse tasks. Our code and data splits are publicly available at https://github.com/akoksal/MEAL.}"}, "1909.09934": {"original_text": "  We propose methods to train convolutional neural networks (CNNs) with both\nbinarized weights and activations, leading to quantized models that are\nspecifically friendly to mobile devices with limited power capacity and\ncomputation resources. Previous works on quantizing CNNs often seek to\napproximate the floating-point information using a set of discrete values,\nwhich we call value approximation, typically assuming the same architecture as\nthe full-precision networks. Here we take a novel \"structure approximation\"\nview of quantization -- it is very likely that different architectures designed\nfor low-bit networks may be better for achieving good performance. In\nparticular, we propose a \"network decomposition\" strategy, termed Group-Net, in\nwhich we divide the network into groups. Thus, each full-precision group can be\neffectively reconstructed by aggregating a set of homogeneous binary branches.\nIn addition, we learn effective connections among groups to improve the\nrepresentation capability. Moreover, the proposed Group-Net shows strong\ngeneralization to other tasks. For instance, we extend Group-Net for accurate\nsemantic segmentation by embedding rich context into the binary structure.\nFurthermore, for the first time, we apply binary neural networks to object\ndetection. Experiments on both classification, semantic segmentation and object\ndetection tasks demonstrate the superior performance of the proposed methods\nover various quantized networks in the literature. Our methods outperform the\nprevious best binary neural networks in terms of accuracy and computation\nefficiency.\n", "rewritten_text": "\\boxed{We introduce a novel approach to train convolutional neural networks (CNNs) with binarized weights and activations, making them suitable for mobile devices with limited power and computation resources. Unlike previous methods that approximate floating-point values with discrete values, we propose a \"structure approximation\" approach, which involves designing new architectures tailored for low-bit networks. Our proposed \"Group-Net\" strategy divides the network into groups, each consisting of homogeneous binary branches that can be aggregated to reconstruct the full-precision network. We also learn effective connections between groups to enhance representation capabilities. Our approach demonstrates strong generalizability to various tasks, including semantic segmentation and object detection, and outperforms existing quantized networks in terms of accuracy and computation efficiency.}"}, "2407.04024": {"original_text": "  Deep unfolding methods and transformer architecture have recently shown\npromising results in hyperspectral image (HSI) reconstruction. However, there\nstill exist two issues: (1) in the data subproblem, most methods represents the\nstepsize utilizing a learnable parameter. Nevertheless, for different spectral\nchannel, error between features and ground truth is unequal. (2) Transformer\nstruggles to balance receptive field size with pixel-wise detail information.\nTo overcome the aforementioned drawbacks, We proposed an adaptive step-size\nperception unfolding network (ASPUN), a deep unfolding network based on FISTA\nalgorithm, which uses an adaptive step-size perception module to estimate the\nupdate step-size of each spectral channel. In addition, we design a Non-local\nHybrid Attention Transformer(NHAT) module for fully leveraging the receptive\nfield advantage of transformer. By plugging the NLHA into the Non-local\nInformation Aggregation (NLIA) module, the unfolding network can achieve better\nreconstruction results. Experimental results show that our ASPUN is superior to\nthe existing SOTA algorithms and achieves the best performance.\n", "rewritten_text": "\\boxed{\nRecent advances in deep unfolding methods and transformer architecture have shown promising results in hyperspectral image reconstruction. However, two key issues remain: (1) existing methods use a learnable parameter to represent the step size, which fails to account for the varying error between features and ground truth across different spectral channels, and (2) transformers struggle to balance receptive field size with pixel-wise detail information. To address these limitations, we propose the Adaptive Step-size Perception Unfolding Network (ASPUN), a deep unfolding network based on the FISTA algorithm that incorporates an adaptive step-size perception module to estimate the update step size for each spectral channel. Additionally, we design a Non-local Hybrid Attention Transformer (NHAT) module to fully leverage the receptive field advantage of transformers. By integrating NHAT into the Non-local Information Aggregation (NLIA) module, our unfolding network achieves superior reconstruction results. Experimental results demonstrate that ASPUN outperforms existing state-of-the-art algorithms and achieves the best performance.\n}"}, "1810.10519": {"original_text": "  With the increasing use of social networks and mobile devices, the number of\nvideos posted on the Internet is growing exponentially. Among the inappropriate\ncontents published on the Internet, pornography is one of the most worrying as\nit can be accessed by teens and children. Two spatiotemporal CNNs, VGG-C3D CNN\nand ResNet R(2+1)D CNN, were assessed for pornography detection in videos in\nthe present study. Experimental results using the Pornography-800 dataset\nshowed that these spatiotemporal CNNs performed better than some\nstate-of-the-art methods based on bag of visual words and are competitive with\nother CNN-based approaches, reaching accuracy of 95.1%.\n", "rewritten_text": "\\boxed{The rapid growth of online video content, particularly on social networks and mobile devices, has led to an increase in inappropriate material, including pornography, which can be easily accessed by minors. To address this issue, this study evaluated the effectiveness of two spatiotemporal CNNs, VGG-C3D and ResNet R(2+1)D, in detecting pornography in videos using the Pornography-800 dataset. The results showed that these CNNs outperformed some state-of-the-art methods and were competitive with other CNN-based approaches, achieving an accuracy of 95.1%.}"}, "2406.10209": {"original_text": "  Large language models can memorize and repeat their training data, causing\nprivacy and copyright risks. To mitigate memorization, we introduce a subtle\nmodification to the next-token training objective that we call the goldfish\nloss. During training, randomly sampled subsets of tokens are excluded from the\nloss computation. These dropped tokens are not memorized by the model, which\nprevents verbatim reproduction of a complete chain of tokens from the training\nset. We run extensive experiments training billion-scale Llama-2 models, both\npre-trained and trained from scratch, and demonstrate significant reductions in\nextractable memorization with little to no impact on downstream benchmarks.\n", "rewritten_text": "\\boxed{To address privacy and copyright concerns, we propose a modified training objective called the \"goldfish loss\" that prevents large language models from memorizing and repeating their training data. By randomly excluding certain tokens from the loss computation during training, we can prevent the model from memorizing entire sequences of tokens, thereby reducing the risk of verbatim reproduction. Our experiments with billion-scale Llama-2 models show that this approach significantly reduces memorization while maintaining performance on downstream tasks.}"}, "2110.14398": {"original_text": "  To consider Hawrami and Zaza (Zazaki) standalone languages or dialects of a\nlanguage have been discussed and debated for a while among linguists active in\nstudying Iranian languages. The question of whether those languages/dialects\nbelong to the Kurdish language or if they are independent descendants of\nIranian languages was answered by MacKenzie (1961). However, a majority of\npeople who speak the dialects are against that answer. Their disapproval mainly\nseems to be based on the sociological, cultural, and historical relationship\namong the speakers of the dialects. While the case of Hawrami and Zaza has\nremained unexplored and under-examined, an almost unanimous agreement exists\nabout the classification of Kurmanji and Sorani as Kurdish dialects. The\nrelated studies to address the mentioned cases are primarily qualitative.\nHowever, computational linguistics could approach the question from a\nquantitative perspective. In this research, we look into three questions from a\nlinguistic distance point of view. First, how similar or dissimilar Hawrami and\nZaza are, considering no common geographical coexistence between the two.\nSecond, what about Kurmanji and Sorani that have geographical overlap. Finally,\nwhat is the distance among all these dialects, pair by pair? We base our\ncomputation on phonetic presentations of these dialects (languages), and we\ncalculate various linguistic distances among the pairs. We analyze the data and\ndiscuss the results to conclude.\n", "rewritten_text": "\\boxed{\nThe classification of Hawrami and Zaza as standalone languages or dialects of a language has been a topic of debate among linguists studying Iranian languages. While MacKenzie (1961) provided an answer to this question, many speakers of these dialects disagree, citing sociological, cultural, and historical ties. In contrast, there is widespread agreement that Kurmanji and Sorani are Kurdish dialects. This study approaches the question from a quantitative perspective, using computational linguistics to investigate three questions: (1) the similarity between Hawrami and Zaza, despite their lack of geographical overlap; (2) the relationship between Kurmanji and Sorani, which do overlap geographically; and (3) the linguistic distance between all these dialects, pair by pair. Our analysis is based on phonetic representations of these dialects and calculates linguistic distances between pairs. We discuss the results and draw conclusions.\n}"}, "2401.14718": {"original_text": "  Future Frame Synthesis (FFS) aims to enable models to generate sequences of\nfuture frames based on existing content. This survey comprehensively reviews\nhistorical and contemporary works in FFS, including widely used datasets and\nalgorithms. It scrutinizes the challenges and the evolving landscape of FFS\nwithin computer vision, with a focus on the transition from deterministic to\ngenerative synthesis methodologies. Our taxonomy highlights the significant\nadvancements and shifts in approach, underscoring the growing importance of\ngenerative models in achieving realistic and diverse future frame predictions.\n", "rewritten_text": "\\boxed{This survey provides a comprehensive review of Future Frame Synthesis (FFS), a technology that enables models to generate future frames based on existing content. It covers historical and contemporary works, including popular datasets and algorithms, and examines the challenges and evolution of FFS in computer vision, particularly the shift from deterministic to generative synthesis methods. The survey's taxonomy highlights significant advancements and changes in approach, emphasizing the increasing importance of generative models in producing realistic and diverse future frame predictions.}"}, "1808.0841": {"original_text": "  Training a good deep learning model often requires a lot of annotated data.\nAs a large amount of labeled data is typically difficult to collect and even\nmore difficult to annotate, data augmentation and data generation are widely\nused in the process of training deep neural networks. However, there is no\nclear common understanding on how much labeled data is needed to get\nsatisfactory performance. In this paper, we try to address such a question\nusing vehicle license plate character recognition as an example application. We\napply computer graphic scripts and Generative Adversarial Networks to generate\nand augment a large number of annotated, synthesized license plate images with\nrealistic colors, fonts, and character composition from a small number of real,\nmanually labeled license plate images. Generated and augmented data are mixed\nand used as training data for the license plate recognition network modified\nfrom DenseNet. The experimental results show that the model trained from the\ngenerated mixed training data has good generalization ability, and the proposed\napproach achieves a new state-of-the-art accuracy on Dataset-1 and AOLP, even\nwith a very limited number of original real license plates. In addition, the\naccuracy improvement caused by data generation becomes more significant when\nthe number of labeled images is reduced. Data augmentation also plays a more\nsignificant role when the number of labeled images is increased.\n", "rewritten_text": "\\boxed{Collecting and annotating a large amount of data is a significant challenge in training deep learning models. To address this, data augmentation and generation techniques are commonly used. However, the amount of labeled data required for satisfactory performance is unclear. This paper explores this question using vehicle license plate character recognition as an example. By generating and augmenting a large number of realistic license plate images from a small set of real images, we demonstrate that a model trained on this mixed data achieves state-of-the-art accuracy, even with limited real data. The results show that data generation and augmentation become increasingly important when the amount of labeled data is reduced or increased, respectively.}"}, "2401.03183": {"original_text": "  Defeasibility in causal reasoning implies that the causal relationship\nbetween cause and effect can be strengthened or weakened. Namely, the causal\nstrength between cause and effect should increase or decrease with the\nincorporation of strengthening arguments (supporters) or weakening arguments\n(defeaters), respectively. However, existing works ignore defeasibility in\ncausal reasoning and fail to evaluate existing causal strength metrics in\ndefeasible settings. In this work, we present $\\delta$-CAUSAL, the first\nbenchmark dataset for studying defeasibility in causal reasoning.\n$\\delta$-CAUSAL includes around 11K events spanning ten domains, featuring\ndefeasible causality pairs, i.e., cause-effect pairs accompanied by supporters\nand defeaters. We further show current causal strength metrics fail to reflect\nthe change of causal strength with the incorporation of supporters or defeaters\nin $\\delta$-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation\nwith Attention Rating), a metric that measures causal strength based on\ntoken-level causal relationships. CESAR achieves a significant 69.7% relative\nimprovement over existing metrics, increasing from 47.2% to 80.1% in capturing\nthe causal strength change brought by supporters and defeaters. We further\ndemonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and\n10.7 points behind humans in generating supporters and defeaters, emphasizing\nthe challenge posed by $\\delta$-CAUSAL.\n", "rewritten_text": "\\boxed{\nCausal relationships can be strengthened or weakened by additional arguments, but current research overlooks this aspect of defeasibility. To address this, we introduce $\\delta$-CAUSAL, a benchmark dataset of 11,000 events across 10 domains, featuring cause-effect pairs with supporting and opposing arguments. Our analysis reveals that existing metrics fail to capture changes in causal strength when additional arguments are considered. We propose CESAR, a new metric that measures causal strength based on token-level relationships and attention rating, which achieves a 69.7% relative improvement over existing metrics. Furthermore, we show that even advanced language models like GPT-3.5 struggle to generate supporting and opposing arguments, highlighting the challenges posed by $\\delta$-CAUSAL.\n}"}, "1310.5767": {"original_text": "  Salient object detection aims to locate objects that capture human attention\nwithin images. Previous approaches often pose this as a problem of image\ncontrast analysis. In this work, we model an image as a hypergraph that\nutilizes a set of hyperedges to capture the contextual properties of image\npixels or regions. As a result, the problem of salient object detection becomes\none of finding salient vertices and hyperedges in the hypergraph. The main\nadvantage of hypergraph modeling is that it takes into account each pixel's (or\nregion's) affinity with its neighborhood as well as its separation from image\nbackground. Furthermore, we propose an alternative approach based on\ncenter-versus-surround contextual contrast analysis, which performs salient\nobject detection by optimizing a cost-sensitive support vector machine (SVM)\nobjective function. Experimental results on four challenging datasets\ndemonstrate the effectiveness of the proposed approaches against the\nstate-of-the-art approaches to salient object detection.\n", "rewritten_text": "\\boxed{Salient object detection, which identifies attention-grabbing objects in images, is typically approached through image contrast analysis. This work presents a novel method that represents images as hypergraphs, comprising hyperedges that capture contextual relationships between pixels or regions. This formulation enables the detection of salient objects by identifying prominent vertices and hyperedges in the hypergraph. The hypergraph model offers a key advantage by considering both the affinity between neighboring pixels/regions and their distinction from the background. Additionally, an alternative approach is proposed, which leverages center-versus-surround contextual contrast analysis and optimizes a cost-sensitive support vector machine (SVM) objective function for salient object detection. Experimental results on four challenging datasets demonstrate the superiority of the proposed methods over existing state-of-the-art approaches.}"}, "2111.02394": {"original_text": "  We propose an accurate and efficient scene text detection framework, termed\nFAST (i.e., faster arbitrarily-shaped text detector). Different from recent\nadvanced text detectors that used complicated post-processing and hand-crafted\nnetwork architectures, resulting in low inference speed, FAST has two new\ndesigns. (1) We design a minimalist kernel representation (only has 1-channel\noutput) to model text with arbitrary shape, as well as a GPU-parallel\npost-processing to efficiently assemble text lines with a negligible time\noverhead. (2) We search the network architecture tailored for text detection,\nleading to more powerful features than most networks that are searched for\nimage classification. Benefiting from these two designs, FAST achieves an\nexcellent trade-off between accuracy and efficiency on several challenging\ndatasets, including Total Text, CTW1500, ICDAR 2015, and MSRA-TD500. For\nexample, FAST-T yields 81.6% F-measure at 152 FPS on Total-Text, outperforming\nthe previous fastest method by 1.7 points and 70 FPS in terms of accuracy and\nspeed. With TensorRT optimization, the inference speed can be further\naccelerated to over 600 FPS. Code and models will be released at\nhttps://github.com/czczup/FAST.\n", "rewritten_text": "\\boxed{We introduce FAST, a highly accurate and efficient scene text detection framework that outperforms existing methods. Unlike recent advanced text detectors that rely on complex post-processing and custom network architectures, resulting in slow inference speeds, FAST features two innovative designs. Firstly, it uses a simple kernel representation and parallel post-processing to efficiently detect text with arbitrary shapes. Secondly, it employs a tailored network architecture that yields more powerful features than those designed for image classification. As a result, FAST achieves an excellent balance between accuracy and efficiency on several challenging datasets, including Total Text, CTW1500, ICDAR 2015, and MSRA-TD500. For instance, FAST-T achieves an F-measure of 81.6% at 152 FPS on Total-Text, surpassing the previous fastest method by 1.7 points and 70 FPS in terms of accuracy and speed. With TensorRT optimization, the inference speed can be further accelerated to over 600 FPS. The code and models are available at https://github.com/czczup/FAST.}"}, "2403.17856": {"original_text": "  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.\n", "rewritten_text": "\\boxed{English morphology is characterized by its ability to convert words from one part of speech to another, known as lexical-syntactic flexibility. Despite its prevalence, little research has been done to determine how well language models can generalize this process. This study investigates the ability of five language models, including two proprietary models (GPT-3.5 and GPT-4) and three open-source models, to adapt to non-prototypical parts of speech. The results show that GPT-4 performs best, followed by GPT-3.5, but the open-source models also demonstrate this ability, with the 7B parameter Mistral model showing a surprisingly small difference in performance compared to the larger GPT-4 model.}"}, "1712.05277": {"original_text": "  Depth cameras allow to set up reliable solutions for people monitoring and\nbehavior understanding, especially when unstable or poor illumination\nconditions make unusable common RGB sensors. Therefore, we propose a complete\nframework for the estimation of the head and shoulder pose based on depth\nimages only. A head detection and localization module is also included, in\norder to develop a complete end-to-end system. The core element of the\nframework is a Convolutional Neural Network, called POSEidon+, that receives as\ninput three types of images and provides the 3D angles of the pose as output.\nMoreover, a Face-from-Depth component based on a Deterministic Conditional GAN\nmodel is able to hallucinate a face from the corresponding depth image. We\nempirically demonstrate that this positively impacts the system performances.\nWe test the proposed framework on two public datasets, namely Biwi Kinect Head\nPose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by\nthe automotive setup. Experimental results show that our method overcomes\nseveral recent state-of-art works based on both intensity and depth input data,\nrunning in real-time at more than 30 frames per second.\n", "rewritten_text": "\\boxed{We propose a comprehensive framework for estimating head and shoulder pose using only depth images, which is particularly useful in low-light conditions where traditional RGB sensors are ineffective. Our system consists of a head detection and localization module, as well as a Convolutional Neural Network called POSEidon+, which takes in three types of images and outputs 3D pose angles. Additionally, a Face-from-Depth component uses a Deterministic Conditional GAN model to generate a face from the depth image, improving system performance. We demonstrate the effectiveness of our framework by testing it on three datasets, including a new challenging dataset inspired by automotive settings, and show that it outperforms recent state-of-the-art methods while running in real-time at over 30 frames per second.}"}, "2404.19287": {"original_text": "  Pretrained vision-language models (VLMs) like CLIP exhibit exceptional\ngeneralization across diverse downstream tasks. While recent studies reveal\ntheir vulnerability to adversarial attacks, research to date has primarily\nfocused on enhancing the robustness of image encoders against image-based\nattacks, with defenses against text-based and multimodal attacks remaining\nlargely unexplored. To this end, this work presents the first comprehensive\nstudy on improving the adversarial robustness of VLMs against attacks targeting\nimage, text, and multimodal inputs. This is achieved by proposing multimodal\ncontrastive adversarial training (MMCoA). Such an approach strengthens the\nrobustness of both image and text encoders by aligning the clean text\nembeddings with adversarial image embeddings, and adversarial text embeddings\nwith clean image embeddings. The robustness of the proposed MMCoA is examined\nagainst existing defense methods over image, text, and multimodal attacks on\nthe CLIP model. Extensive experiments on 15 datasets across two tasks reveal\nthe characteristics of different adversarial defense methods under distinct\ndistribution shifts and dataset complexities across the three attack types.\nThis paves the way for a unified framework of adversarial robustness against\ndifferent modality attacks, opening up new possibilities for securing VLMs\nagainst multimodal attacks. The code is available at\nhttps://github.com/ElleZWQ/MMCoA.git.\n", "rewritten_text": "\\boxed{\nPretrained vision-language models, such as CLIP, have shown impressive performance across various tasks, but they are vulnerable to adversarial attacks. While previous research has focused on improving the robustness of image encoders against image-based attacks, defenses against text-based and multimodal attacks have been largely overlooked. This study addresses this gap by proposing a novel approach called multimodal contrastive adversarial training (MMCoA), which enhances the robustness of both image and text encoders by aligning clean and adversarial embeddings. The effectiveness of MMCoA is evaluated against existing defense methods on 15 datasets across two tasks, revealing the strengths and weaknesses of different defense methods under various distribution shifts and dataset complexities. This research paves the way for a unified framework for adversarial robustness against different modality attacks, enabling the development of more secure vision-language models. The implementation is available at https://github.com/ElleZWQ/MMCoA.git.\n}"}, "2211.09469": {"original_text": "  Generating consecutive descriptions for videos, i.e., Video Captioning,\nrequires taking full advantage of visual representation along with the\ngeneration process. Existing video captioning methods focus on making an\nexploration of spatial-temporal representations and their relationships to\nproduce inferences. However, such methods only exploit the superficial\nassociation contained in the video itself without considering the intrinsic\nvisual commonsense knowledge that existed in a video dataset, which may hinder\ntheir capabilities of knowledge cognitive to reason accurate descriptions. To\naddress this problem, we propose a simple yet effective method, called Visual\nCommonsense-aware Representation Network (VCRN), for video captioning.\nSpecifically, we construct a Video Dictionary, a plug-and-play component,\nobtained by clustering all video features from the total dataset into multiple\nclustered centers without additional annotation. Each center implicitly\nrepresents a visual commonsense concept in the video domain, which is utilized\nin our proposed Visual Concept Selection (VCS) to obtain a video-related\nconcept feature. Next, a Conceptual Integration Generation (CIG) is proposed to\nenhance the caption generation. Extensive experiments on three publicly video\ncaptioning benchmarks: MSVD, MSR-VTT, and VATEX, demonstrate that our method\nreaches state-of-the-art performance, indicating the effectiveness of our\nmethod. In addition, our approach is integrated into the existing method of\nvideo question answering and improves this performance, further showing the\ngeneralization of our method. Source code has been released at\nhttps://github.com/zchoi/VCRN.\n", "rewritten_text": "\\boxed{\nVideo captioning, which involves generating consecutive descriptions for videos, requires effectively utilizing visual representations and the generation process. While existing methods focus on exploring spatial-temporal representations and their relationships, they neglect the intrinsic visual commonsense knowledge present in video datasets, limiting their ability to reason accurately. To address this, we propose the Visual Commonsense-aware Representation Network (VCRN) for video captioning. Our approach involves constructing a Video Dictionary, a plug-and-play component, by clustering video features into multiple centers that represent visual commonsense concepts. We then utilize these concepts in our Visual Concept Selection (VCS) and Conceptual Integration Generation (CIG) to enhance caption generation. Our method achieves state-of-the-art performance on three video captioning benchmarks and improves video question answering performance when integrated into existing methods, demonstrating its effectiveness and generalization. The source code is available at https://github.com/zchoi/VCRN.\n}"}, "2203.12788": {"original_text": "  A fundamental characteristic of natural language is the high rate at which\nspeakers produce novel expressions. Because of this novelty, a heavy-tail of\nrare events accounts for a significant amount of the total probability mass of\ndistributions in language (Baayen, 2001). Standard language modeling metrics\nsuch as perplexity quantify the performance of language models (LM) in\naggregate. As a result, we have relatively little understanding of whether\nneural LMs accurately estimate the probability of sequences in this heavy-tail\nof rare events. To address this gap, we develop a controlled evaluation scheme\nwhich uses generative models trained on natural data as artificial languages\nfrom which we can exactly compute sequence probabilities. Training LMs on\ngenerations from these artificial languages, we compare the sequence-level\nprobability estimates given by LMs to the true probabilities in the target\nlanguage. Our experiments reveal that LSTM and Transformer language models (i)\nsystematically underestimate the probability of sequences drawn from the target\nlanguage, and (ii) do so more severely for less-probable sequences.\nInvestigating where this probability mass went, (iii) we find that LMs tend to\noverestimate the probability of ill formed (perturbed) sequences. In addition,\nwe find that this underestimation behaviour (iv) is weakened, but not\neliminated by greater amounts of training data, and (v) is exacerbated for\ntarget distributions with lower entropy.\n", "rewritten_text": "\\boxed{\nOne key feature of natural language is its ability to produce new and unique expressions at a high rate. This novelty leads to a significant portion of the total probability mass being attributed to rare events. However, standard language modeling metrics, such as perplexity, only provide an overall performance measure and do not reveal how well language models estimate the probability of these rare events. To address this knowledge gap, we developed a controlled evaluation method that uses generative models trained on natural data to create artificial languages, allowing us to exactly compute sequence probabilities. Our experiments show that LSTM and Transformer language models consistently underestimate the probability of sequences from the target language, especially for less probable sequences. Furthermore, we found that language models tend to overestimate the probability of ill-formed sequences. Additionally, we discovered that increasing the amount of training data only slightly improves this underestimation, and that it worsens for target distributions with lower entropy."}, "2207.09884": {"original_text": "  We present a Momentum Re-identification (MoReID) framework that can leverage\na very large number of negative samples in training for general\nre-identification task. The design of this framework is inspired by Momentum\nContrast (MoCo), which uses a dictionary to store current and past batches to\nbuild a large set of encoded samples. As we find it less effective to use past\npositive samples which may be highly inconsistent to the encoded feature\nproperty formed with the current positive samples, MoReID is designed to use\nonly a large number of negative samples stored in the dictionary. However, if\nwe train the model using the widely used Triplet loss that uses only one sample\nto represent a set of positive/negative samples, it is hard to effectively\nleverage the enlarged set of negative samples acquired by the MoReID framework.\nTo maximize the advantage of using the scaled-up negative sample set, we newly\nintroduce Hard-distance Elastic loss (HE loss), which is capable of using more\nthan one hard sample to represent a large number of samples. Our experiments\ndemonstrate that a large number of negative samples provided by MoReID\nframework can be utilized at full capacity only with the HE loss, achieving the\nstate-of-the-art accuracy on three re-ID benchmarks, VeRi-776, Market-1501, and\nVeRi-Wild.\n", "rewritten_text": "\\boxed{We propose a Momentum Re-identification (MoReID) framework that can effectively utilize a large number of negative samples for general re-identification tasks. Inspired by Momentum Contrast (MoCo), MoReID stores a large set of encoded samples in a dictionary, but only uses negative samples to avoid inconsistencies. To fully leverage the enlarged negative sample set, we introduce the Hard-distance Elastic (HE) loss, which can handle multiple hard samples. Our experiments show that combining MoReID with HE loss achieves state-of-the-art accuracy on three re-ID benchmarks: VeRi-776, Market-1501, and VeRi-Wild.}"}, "2410.12158": {"original_text": "  Foundation models have significantly enhanced 2D task performance, and recent\nworks like Bridge3D have successfully applied these models to improve 3D scene\nunderstanding through knowledge distillation, marking considerable\nadvancements. Nonetheless, challenges such as the misalignment between 2D and\n3D representations and the persistent long-tail distribution in 3D datasets\nstill restrict the effectiveness of knowledge distillation from 2D to 3D using\nfoundation models. To tackle these issues, we introduce a novel SAM-guided\ntokenization method that seamlessly aligns 3D transformer structures with\nregion-level knowledge distillation, replacing the traditional KNN-based\ntokenization techniques. Additionally, we implement a group-balanced\nre-weighting strategy to effectively address the long-tail problem in knowledge\ndistillation. Furthermore, inspired by the recent success of masked feature\nprediction, our framework incorporates a two-stage masked token prediction\nprocess in which the student model predicts both the global embeddings and the\ntoken-wise local embeddings derived from the teacher models trained in the\nfirst stage. Our methodology has been validated across multiple datasets,\nincluding SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and\nsemantic segmentation. The results demonstrate significant improvements over\ncurrent State-of-the-art self-supervised methods, establishing new benchmarks\nin this field.\n", "rewritten_text": "\\boxed{\nRecent advancements in 2D task performance using foundation models have paved the way for improving 3D scene understanding through knowledge distillation, as seen in works like Bridge3D. However, challenges persist, including the mismatch between 2D and 3D representations and the long-tail distribution in 3D datasets, which hinder the effectiveness of knowledge distillation from 2D to 3D. To overcome these limitations, we propose a novel SAM-guided tokenization method that aligns 3D transformer structures with region-level knowledge distillation, replacing traditional KNN-based tokenization techniques. Additionally, we introduce a group-balanced re-weighting strategy to address the long-tail problem. Our framework also incorporates a two-stage masked token prediction process, where the student model predicts both global and local embeddings derived from teacher models. Our approach has been validated on multiple datasets, including SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and semantic segmentation, achieving significant improvements over current state-of-the-art self-supervised methods and establishing new benchmarks.\n}"}, "2307.09892": {"original_text": "  We propose 3Deformer, a general-purpose framework for interactive 3D shape\nediting. Given a source 3D mesh with semantic materials, and a user-specified\nsemantic image, 3Deformer can accurately edit the source mesh following the\nshape guidance of the semantic image, while preserving the source topology as\nrigid as possible. Recent studies of 3D shape editing mostly focus on learning\nneural networks to predict 3D shapes, which requires high-cost 3D training\ndatasets and is limited to handling objects involved in the datasets. Unlike\nthese studies, our 3Deformer is a non-training and common framework, which only\nrequires supervision of readily-available semantic images, and is compatible\nwith editing various objects unlimited by datasets. In 3Deformer, the source\nmesh is deformed utilizing the differentiable renderer technique, according to\nthe correspondences between semantic images and mesh materials. However,\nguiding complex 3D shapes with a simple 2D image incurs extra challenges, that\nis, the deform accuracy, surface smoothness, geometric rigidity, and global\nsynchronization of the edited mesh should be guaranteed. To address these\nchallenges, we propose a hierarchical optimization architecture to balance the\nglobal and local shape features, and propose further various strategies and\nlosses to improve properties of accuracy, smoothness, rigidity, and so on.\nExtensive experiments show that our 3Deformer is able to produce impressive\nresults and reaches the state-of-the-art level.\n", "rewritten_text": "\\boxed{We introduce 3Deformer, a versatile framework for interactive 3D shape editing that can accurately modify a source 3D mesh based on a user-provided semantic image while preserving the original topology. Unlike recent studies that rely on neural networks and require large 3D training datasets, our approach is non-training and universal, only needing semantic images as supervision. It uses a differentiable renderer to deform the mesh according to the correspondences between the semantic image and mesh materials. To overcome the challenges of guiding complex 3D shapes with a 2D image, we propose a hierarchical optimization architecture and various strategies to ensure deform accuracy, surface smoothness, geometric rigidity, and global synchronization. Our experiments demonstrate that 3Deformer produces impressive results, achieving state-of-the-art performance.}"}, "2105.03571": {"original_text": "  Dialogue state tracking (DST) plays a key role in task-oriented dialogue\nsystems to monitor the user's goal. In general, there are two strategies to\ntrack a dialogue state: predicting it from scratch and updating it from\nprevious state. The scratch-based strategy obtains each slot value by inquiring\nall the dialogue history, and the previous-based strategy relies on the current\nturn dialogue to update the previous dialogue state. However, it is hard for\nthe scratch-based strategy to correctly track short-dependency dialogue state\nbecause of noise; meanwhile, the previous-based strategy is not very useful for\nlong-dependency dialogue state tracking. Obviously, it plays different roles\nfor the context information of different granularity to track different kinds\nof dialogue states. Thus, in this paper, we will study and discuss how the\ncontext information of different granularity affects dialogue state tracking.\nFirst, we explore how greatly different granularities affect dialogue state\ntracking. Then, we further discuss how to combine multiple granularities for\ndialogue state tracking. Finally, we apply the findings about context\ngranularity to few-shot learning scenario. Besides, we have publicly released\nall codes.\n", "rewritten_text": "\\boxed{\nDialogue state tracking (DST) is a crucial component of task-oriented dialogue systems, responsible for monitoring the user's goal. There are two primary approaches to DST: predicting the state from scratch and updating it from the previous state. The scratch-based approach involves analyzing the entire dialogue history to determine each slot value, while the previous-based approach relies on the current turn's dialogue to update the previous state. However, the scratch-based approach struggles with short-dependency dialogue states due to noise, and the previous-based approach is less effective for long-dependency states. The context information's granularity plays a significant role in tracking different types of dialogue states. This paper investigates how different granularities impact DST, explores the effects of combining multiple granularities, and applies the findings to few-shot learning scenarios. The code for this research is publicly available.\n}"}, "2405.10474": {"original_text": "  Over the last decade, a wide range of training and deployment strategies for\nLarge Language Models (LLMs) have emerged. Among these, the prompting paradigms\nof Auto-regressive LLMs (AR-LLMs) have catalyzed a significant surge in\nArtificial Intelligence (AI). This paper aims to emphasize the significance of\nutilizing free-form modalities (forms of input and output) and verbal free-form\ncontexts as user-directed channels (methods for transforming modalities) for\ndownstream deployment. Specifically, we analyze the structure of modalities\nwithin both two types of LLMs and six task-specific channels during deployment.\nFrom the perspective of users, our analysis introduces and applies the\nanalytical metrics of task customizability, transparency, and complexity to\ngauge their usability, highlighting the superior nature of AR-LLMs' prompting\nparadigms. Moreover, we examine the stimulation of diverse cognitive behaviors\nin LLMs through the adoption of free-form text and verbal contexts, mirroring\nhuman linguistic expressions of such behaviors. We then detail four common\ncognitive behaviors to underscore how AR-LLMs' prompting successfully imitate\nhuman-like behaviors using this free-form modality and channel. Lastly, the\npotential for improving LLM deployment, both as autonomous agents and within\nmulti-agent systems, is identified via cognitive behavior concepts and\nprinciples.\n", "rewritten_text": "\\boxed{\nOver the past decade, various training and deployment strategies for Large Language Models (LLMs) have been developed. One approach that has driven significant progress in Artificial Intelligence (AI) is the use of auto-regressive LLMs (AR-LLMs) with prompting paradigms. This paper highlights the importance of using flexible input and output formats, as well as verbal contexts, to enable user-directed transformations for downstream deployment. We examine the structure of these formats within two types of LLMs and six task-specific channels, and evaluate their usability from the user's perspective using metrics such as customizability, transparency, and complexity. Our analysis shows that AR-LLMs' prompting paradigms are superior and can stimulate diverse cognitive behaviors, mimicking human-like linguistic expressions. We identify four common cognitive behaviors that AR-LLMs can imitate using free-form text and verbal contexts, and discuss the potential for improving LLM deployment as autonomous agents or within multi-agent systems by leveraging cognitive behavior concepts and principles.\n}"}, "2212.04214": {"original_text": "  In a citation graph, adjacent paper nodes share related scientific terms and\ntopics. The graph thus conveys unique structure information of document-level\nrelatedness that can be utilized in the paper summarization task, for exploring\nbeyond the intra-document information. In this work, we focus on leveraging\ncitation graphs to improve scientific paper extractive summarization under\ndifferent settings. We first propose a Multi-granularity Unsupervised\nSummarization model (MUS) as a simple and low-cost solution to the task. MUS\nfinetunes a pre-trained encoder model on the citation graph by link prediction\ntasks. Then, the abstract sentences are extracted from the corresponding paper\nconsidering multi-granularity information. Preliminary results demonstrate that\ncitation graph is helpful even in a simple unsupervised framework. Motivated by\nthis, we next propose a Graph-based Supervised Summarization model (GSS) to\nachieve more accurate results on the task when large-scale labeled data are\navailable. Apart from employing the link prediction as an auxiliary task, GSS\nintroduces a gated sentence encoder and a graph information fusion module to\ntake advantage of the graph information to polish the sentence representation.\nExperiments on a public benchmark dataset show that MUS and GSS bring\nsubstantial improvements over the prior state-of-the-art model.\n", "rewritten_text": "\\boxed{In a citation graph, connected papers share similar scientific topics and terms, providing a unique structure for document relationships. This structure can be leveraged to improve scientific paper summarization by going beyond individual document information. This work focuses on utilizing citation graphs to enhance extractive summarization in various settings. Two models are proposed: a simple, low-cost Multi-granularity Unsupervised Summarization (MUS) model that fine-tunes a pre-trained encoder on the citation graph, and a more accurate Graph-based Supervised Summarization (GSS) model that incorporates a gated sentence encoder and graph information fusion module. Experimental results on a public dataset demonstrate that both models significantly outperform the previous state-of-the-art model.}"}, "2310.14561": {"original_text": "  Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by\nwell-designed perturbations. This could lead to disastrous results on critical\napplications such as self-driving cars, surveillance security, and medical\ndiagnosis. At present, adversarial training is one of the most effective\ndefenses against adversarial examples. However, traditional adversarial\ntraining makes it difficult to achieve a good trade-off between clean accuracy\nand robustness since spurious features are still learned by DNNs. The intrinsic\nreason is that traditional adversarial training makes it difficult to fully\nlearn core features from adversarial examples when adversarial noise and clean\nexamples cannot be disentangled. In this paper, we disentangle the adversarial\nexamples into natural and perturbed patterns by bit-plane slicing. We assume\nthe higher bit-planes represent natural patterns and the lower bit-planes\nrepresent perturbed patterns, respectively. We propose a Feature-Focusing\nAdversarial Training (F$^2$AT), which differs from previous work in that it\nenforces the model to focus on the core features from natural patterns and\nreduce the impact of spurious features from perturbed patterns. The\nexperimental results demonstrated that F$^2$AT outperforms state-of-the-art\nmethods in clean accuracy and adversarial robustness.\n", "rewritten_text": "\\boxed{\nDeep neural networks are susceptible to attacks from carefully crafted input examples, which can have devastating consequences in critical applications like self-driving cars, surveillance, and medical diagnosis. Currently, one of the most effective ways to defend against these attacks is through adversarial training. However, traditional adversarial training struggles to balance accuracy on clean data with robustness against attacks, as it can still learn irrelevant features. This is because it's difficult to separate the underlying patterns from the noise in the adversarial examples. To address this, we propose a new approach called Feature-Focusing Adversarial Training (F$^2$AT), which uses bit-plane slicing to disentangle the natural and perturbed patterns in the data. By focusing on the core features from the natural patterns and reducing the impact of the perturbed patterns, our method achieves better results than existing state-of-the-art methods in both clean accuracy and adversarial robustness.\n}"}, "2308.04782": {"original_text": "  Point cloud registration is a task to estimate the rigid transformation\nbetween two unaligned scans, which plays an important role in many computer\nvision applications. Previous learning-based works commonly focus on supervised\nregistration, which have limitations in practice. Recently, with the advance of\ninexpensive RGB-D sensors, several learning-based works utilize RGB-D data to\nachieve unsupervised registration. However, most of existing unsupervised\nmethods follow a cascaded design or fuse RGB-D data in a unidirectional manner,\nwhich do not fully exploit the complementary information in the RGB-D data. To\nleverage the complementary information more effectively, we propose a network\nimplementing multi-scale bidirectional fusion between RGB images and point\nclouds generated from depth images. By bidirectionally fusing visual and\ngeometric features in multi-scales, more distinctive deep features for\ncorrespondence estimation can be obtained, making our registration more\naccurate. Extensive experiments on ScanNet and 3DMatch demonstrate that our\nmethod achieves new state-of-the-art performance. Code will be released at\nhttps://github.com/phdymz/PointMBF\n", "rewritten_text": "\\boxed{\nEstimating the rigid transformation between two unaligned 3D scans, known as point cloud registration, is a crucial task in computer vision. While previous learning-based methods focused on supervised registration, recent approaches utilize RGB-D data for unsupervised registration. However, most existing unsupervised methods have limitations in fully exploiting the complementary information in RGB-D data. To address this, we propose a network that combines RGB images and point clouds generated from depth images through multi-scale bidirectional fusion. This approach enables the extraction of more distinctive deep features for correspondence estimation, leading to more accurate registration. Our method achieves state-of-the-art performance on ScanNet and 3DMatch datasets, and the code is available at https://github.com/phdymz/PointMBF.\n}"}, "2404.02573": {"original_text": "  Knowledge distillation (KD) is a promising yet challenging model compression\ntechnique that transfers rich learning representations from a well-performing\nbut cumbersome teacher model to a compact student model. Previous methods for\nimage super-resolution (SR) mostly compare the feature maps directly or after\nstandardizing the dimensions with basic algebraic operations (e.g. average,\ndot-product). However, the intrinsic semantic differences among feature maps\nare overlooked, which are caused by the disparate expressive capacity between\nthe networks. This work presents MiPKD, a multi-granularity mixture of prior KD\nframework, to facilitate efficient SR model through the feature mixture in a\nunified latent space and stochastic network block mixture. Extensive\nexperiments demonstrate the effectiveness of the proposed MiPKD method.\n", "rewritten_text": "\\boxed{Knowledge distillation, a model compression technique, aims to transfer rich representations from a large teacher model to a smaller student model. Existing image super-resolution methods compare feature maps directly or after standardization, but neglect the inherent semantic differences between feature maps due to varying network capacities. This work introduces MiPKD, a novel framework that combines prior knowledge distillation methods to enable efficient super-resolution models through unified latent space feature mixture and stochastic network block mixture, demonstrating its effectiveness through extensive experiments.}"}, "2307.08198": {"original_text": "  We introduce the notion of point affiliation into feature upsampling. By\nabstracting a feature map into non-overlapped semantic clusters formed by\npoints of identical semantic meaning, feature upsampling can be viewed as point\naffiliation -- designating a semantic cluster for each upsampled point. In the\nframework of kernel-based dynamic upsampling, we show that an upsampled point\ncan resort to its low-res decoder neighbors and high-res encoder point to\nreason the affiliation, conditioned on the mutual similarity between them. We\ntherefore present a generic formulation for generating similarity-aware\nupsampling kernels and prove that such kernels encourage not only semantic\nsmoothness but also boundary sharpness. This formulation constitutes a novel,\nlightweight, and universal upsampling solution, Similarity-Aware Point\nAffiliation (SAPA). We show its working mechanism via our preliminary designs\nwith window-shape kernel. After probing the limitations of the designs on\nobject detection, we reveal additional insights for upsampling, leading to SAPA\nwith the dynamic kernel shape. Extensive experiments demonstrate that SAPA\noutperforms prior upsamplers and invites consistent performance improvements on\na number of dense prediction tasks, including semantic segmentation, object\ndetection, instance segmentation, panoptic segmentation, image matting, and\ndepth estimation. Code is made available at: https://github.com/tiny-smart/sapa\n", "rewritten_text": "\\boxed{\nWe propose a novel approach to feature upsampling by introducing point affiliation, where a feature map is divided into non-overlapping clusters of points with the same semantic meaning. This allows us to view upsampling as assigning a semantic cluster to each upsampled point. We develop a kernel-based dynamic upsampling framework that enables an upsampled point to determine its affiliation by considering its low-resolution decoder neighbors and high-resolution encoder point, based on their mutual similarity. Our method, called Similarity-Aware Point Affiliation (SAPA), generates similarity-aware upsampling kernels that promote both semantic smoothness and boundary sharpness. We demonstrate the effectiveness of SAPA through extensive experiments on various dense prediction tasks, including semantic segmentation, object detection, and depth estimation, and provide the code at https://github.com/tiny-smart/sapa.\n}"}, "2410.16646": {"original_text": "  Diffusion models excel at creating visually impressive images but often\nstruggle to generate images with a specified topology. The Betti number, which\nrepresents the number of structures in an image, is a fundamental measure in\ntopology. Yet, diffusion models fail to satisfy even this basic constraint.\nThis limitation restricts their utility in applications requiring exact\ncontrol, like robotics and environmental modeling. To address this, we propose\nTopoDiffusionNet (TDN), a novel approach that enforces diffusion models to\nmaintain the desired topology. We leverage tools from topological data\nanalysis, particularly persistent homology, to extract the topological\nstructures within an image. We then design a topology-based objective function\nto guide the denoising process, preserving intended structures while\nsuppressing noisy ones. Our experiments across four datasets demonstrate\nsignificant improvements in topological accuracy. TDN is the first to integrate\ntopology with diffusion models, opening new avenues of research in this area.\n", "rewritten_text": "\\boxed{\nDiffusion models are skilled at generating visually stunning images, but they often struggle to produce images with a specific topology. The Betti number, a fundamental topological measure, is frequently not met by these models. This limitation hinders their applicability in fields like robotics and environmental modeling, where precise control is crucial. To overcome this, we introduce TopoDiffusionNet (TDN), a novel approach that ensures diffusion models maintain the desired topology. By utilizing topological data analysis tools, particularly persistent homology, we extract an image's topological structures. We then design a topology-based objective function to guide the denoising process, preserving intended structures while suppressing noisy ones. Our experiments on four datasets demonstrate significant improvements in topological accuracy. TDN is a pioneering approach that combines topology with diffusion models, paving the way for new research in this area.\n}"}, "1603.01684": {"original_text": "  In this paper, we propose an improved mechanism for saliency detection.\nFirstly,based on a neoteric background prior selecting four corners of an image\nas background,we use color and spatial contrast with each superpixel to obtain\na salinecy map(CBP). Inspired by reverse-measurement methods to improve the\naccuracy of measurement in Engineering,we employ the Objectness labels as\nforeground prior based on part of information of CBP to construct a\nmap(OFP).Further,an original energy function is applied to optimize both of\nthem respectively and a single-layer saliency map(SLP)is formed by merging the\nabove twos.Finally,to deal with the scale problem,we obtain our multi-layer\nmap(MLP) by presenting an integration algorithm to take advantage of multiple\nsaliency maps. Quantitative and qualitative experiments on three datasets\ndemonstrate that our method performs favorably against the state-of-the-art\nalgorithm.\n", "rewritten_text": "\\boxed{\nThis paper presents an enhanced mechanism for saliency detection. We begin by selecting four corners of an image as background using a novel background prior, and then utilize color and spatial contrast with each superpixel to generate a saliency map (CBP). Inspired by reverse-measurement methods in Engineering, we leverage Objectness labels as foreground prior based on partial information from CBP to construct another map (OFP). Next, we apply an original energy function to optimize both maps separately and combine them to form a single-layer saliency map (SLP). Finally, to address the scale problem, we integrate multiple saliency maps using a novel algorithm to obtain our multi-layer map (MLP). Experimental results on three datasets demonstrate that our approach outperforms state-of-the-art algorithms."}, "2403.16998": {"original_text": "  Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly.\n", "rewritten_text": "\\boxed{Recent advances in Large Language Models (LLMs) have led to exceptional performance in long-video understanding tasks. This study examines the role of extensive world knowledge and strong reasoning skills of LLMs in achieving this success. Interestingly, it is found that LLM-based approaches can still achieve high accuracy in long-video tasks even with limited or no video-specific information. Building on this, the study explores the integration of video-specific information into an LLM-based framework. By leveraging off-the-shelf vision tools to extract object-centric information from videos and fusing it with natural language, the proposed Multimodal Video Understanding (MVU) framework achieves state-of-the-art performance across multiple video understanding benchmarks, demonstrating strong generality even in robotics domain tasks. The code for this framework will be publicly released.}"}, "2006.01372": {"original_text": "  In general, the labels used in sequence labeling consist of different types\nof elements. For example, IOB-format entity labels, such as B-Person and\nI-Person, can be decomposed into span (B and I) and type information (Person).\nHowever, while most sequence labeling models do not consider such label\ncomponents, the shared components across labels, such as Person, can be\nbeneficial for label prediction. In this work, we propose to integrate label\ncomponent information as embeddings into models. Through experiments on English\nand Japanese fine-grained named entity recognition, we demonstrate that the\nproposed method improves performance, especially for instances with\nlow-frequency labels.\n", "rewritten_text": "\\boxed{Sequence labeling models can be improved by incorporating label component information, such as the type and span of entities, into the model as embeddings. This approach is particularly effective for instances with rare or low-frequency labels, as demonstrated through experiments on fine-grained named entity recognition in English and Japanese.}"}, "2410.09140": {"original_text": "  The remarkable development of text-to-image generation models has raised\nnotable security concerns, such as the infringement of portrait rights and the\ngeneration of inappropriate content. Concept erasure has been proposed to\nremove the model's knowledge about protected and inappropriate concepts.\nAlthough many methods have tried to balance the efficacy (erasing target\nconcepts) and specificity (retaining irrelevant concepts), they can still\ngenerate abundant erasure concepts under the steering of semantically related\ninputs. In this work, we propose RealEra to address this \"concept residue\"\nissue. Specifically, we first introduce the mechanism of neighbor-concept\nmining, digging out the associated concepts by adding random perturbation into\nthe embedding of erasure concept, thus expanding the erasing range and\neliminating the generations even through associated concept inputs.\nFurthermore, to mitigate the negative impact on the generation of irrelevant\nconcepts caused by the expansion of erasure scope, RealEra preserves the\nspecificity through the beyond-concept regularization. This makes irrelevant\nconcepts maintain their corresponding spatial position, thereby preserving\ntheir normal generation performance. We also employ the closed-form solution to\noptimize weights of U-Net for the cross-attention alignment, as well as the\nprediction noise alignment with the LoRA module. Extensive experiments on\nmultiple benchmarks demonstrate that RealEra outperforms previous concept\nerasing methods in terms of superior erasing efficacy, specificity, and\ngenerality. More details are available on our project page\nhttps://realerasing.github.io/RealEra/ .\n", "rewritten_text": "\\boxed{\nThe rapid advancement of text-to-image generation models has sparked significant security concerns, including the violation of portrait rights and the creation of inappropriate content. To address this issue, concept erasure has been proposed to remove the model's knowledge of protected and inappropriate concepts. However, existing methods struggle to balance erasing target concepts while preserving irrelevant ones, often generating numerous erasure concepts when prompted with semantically related inputs. Our proposed solution, RealEra, tackles this \"concept residue\" problem by introducing neighbor-concept mining, which expands the erasing range by adding random perturbations to the embedding of erasure concepts. Additionally, RealEra employs beyond-concept regularization to maintain the specificity of irrelevant concepts, ensuring they retain their normal generation performance. Our approach also utilizes a closed-form solution to optimize U-Net weights for cross-attention alignment and prediction noise alignment with the LoRA module. Extensive experiments on multiple benchmarks demonstrate RealEra's superior performance in erasing efficacy, specificity, and generality compared to previous concept erasing methods. Further details can be found on our project page.\n}"}, "2210.02843": {"original_text": "  Focusing on the issue of how to effectively capture and utilize\ncross-modality information in RGB-D salient object detection (SOD) task, we\npresent a convolutional neural network (CNN) model, named CIR-Net, based on the\nnovel cross-modality interaction and refinement. For the cross-modality\ninteraction, 1) a progressive attention guided integration unit is proposed to\nsufficiently integrate RGB-D feature representations in the encoder stage, and\n2) a convergence aggregation structure is proposed, which flows the RGB and\ndepth decoding features into the corresponding RGB-D decoding streams via an\nimportance gated fusion unit in the decoder stage. For the cross-modality\nrefinement, we insert a refinement middleware structure between the encoder and\nthe decoder, in which the RGB, depth, and RGB-D encoder features are further\nrefined by successively using a self-modality attention refinement unit and a\ncross-modality weighting refinement unit. At last, with the gradually refined\nfeatures, we predict the saliency map in the decoder stage. Extensive\nexperiments on six popular RGB-D SOD benchmarks demonstrate that our network\noutperforms the state-of-the-art saliency detectors both qualitatively and\nquantitatively.\n", "rewritten_text": "\\boxed{\nWe propose a novel convolutional neural network (CNN) model, CIR-Net, that effectively captures and utilizes cross-modality information for RGB-D salient object detection (SOD). Our model consists of three key components: a progressive attention guided integration unit that integrates RGB-D features in the encoder stage, a convergence aggregation structure that fuses RGB and depth decoding features in the decoder stage, and a refinement middleware structure that refines features using self-modality attention and cross-modality weighting. Experimental results on six popular RGB-D SOD benchmarks show that our model outperforms state-of-the-art saliency detectors in both qualitative and quantitative evaluations."}, "1703.01515": {"original_text": "  Temporal action localization is an important yet challenging problem. Given a\nlong, untrimmed video consisting of multiple action instances and complex\nbackground contents, we need not only to recognize their action categories, but\nalso to localize the start time and end time of each instance. Many\nstate-of-the-art systems use segment-level classifiers to select and rank\nproposal segments of pre-determined boundaries. However, a desirable model\nshould move beyond segment-level and make dense predictions at a fine\ngranularity in time to determine precise temporal boundaries. To this end, we\ndesign a novel Convolutional-De-Convolutional (CDC) network that places CDC\nfilters on top of 3D ConvNets, which have been shown to be effective for\nabstracting action semantics but reduce the temporal length of the input data.\nThe proposed CDC filter performs the required temporal upsampling and spatial\ndownsampling operations simultaneously to predict actions at the frame-level\ngranularity. It is unique in jointly modeling action semantics in space-time\nand fine-grained temporal dynamics. We train the CDC network in an end-to-end\nmanner efficiently. Our model not only achieves superior performance in\ndetecting actions in every frame, but also significantly boosts the precision\nof localizing temporal boundaries. Finally, the CDC network demonstrates a very\nhigh efficiency with the ability to process 500 frames per second on a single\nGPU server. We will update the camera-ready version and publish the source\ncodes online soon.\n", "rewritten_text": "\\boxed{\nTemporal action localization is a challenging task that requires recognizing action categories and pinpointing their start and end times in untrimmed videos with complex backgrounds. While current systems use segment-level classifiers to select and rank proposal segments, a more desirable approach would involve making dense predictions at a fine granularity in time to determine precise temporal boundaries. To achieve this, we propose a novel Convolutional-De-Convolutional (CDC) network that builds upon 3D ConvNets, which are effective for abstracting action semantics but reduce temporal length. Our CDC filter performs temporal upsampling and spatial downsampling simultaneously to predict actions at the frame level, jointly modeling action semantics in space-time and fine-grained temporal dynamics. We train the CDC network end-to-end efficiently, achieving superior performance in detecting actions in every frame and significantly boosting the precision of localizing temporal boundaries. Notably, our model can process 500 frames per second on a single GPU server, demonstrating high efficiency.\n}"}, "2305.0541": {"original_text": "  The medical conversational question answering (CQA) system aims at providing\na series of professional medical services to improve the efficiency of medical\ncare. Despite the success of large language models (LLMs) in complex reasoning\ntasks in various fields, such as mathematics, logic, and commonsense QA, they\nstill need to improve with the increased complexity and specialization of the\nmedical field. This is because medical CQA tasks require not only strong\nmedical reasoning, but also the ability to think broadly and deeply. In this\npaper, to address these challenges in medical CQA tasks that need to be\nconsidered and understood in many aspects, we propose the Holistically Thought\n(HoT) method, which is designed to guide the LLMs to perform the diffused and\nfocused thinking for generating high-quality medical responses. The proposed\nHoT method has been evaluated through automated and manual assessments in three\ndifferent medical CQA datasets containing the English and Chinese languages.\nThe extensive experimental results show that our method can produce more\ncorrectness, professional, and considerate answers than several\nstate-of-the-art (SOTA) methods, manifesting its effectiveness. Our code in\nhttps://github.com/WENGSYX/HoT.\n", "rewritten_text": "\\boxed{\nThe goal of a medical conversational question answering (CQA) system is to enhance the efficiency of medical care by providing a range of professional medical services. While large language models (LLMs) have achieved success in complex reasoning tasks in various fields, they still require improvement to tackle the intricacies and specialization of the medical field. Medical CQA tasks demand not only strong medical reasoning but also the ability to think comprehensively and deeply. To address these challenges, we propose the Holistically Thought (HoT) method, which guides LLMs to perform diffused and focused thinking to generate high-quality medical responses. Our method has been evaluated through automated and manual assessments on three medical CQA datasets in both English and Chinese languages, and the results demonstrate that it can produce more accurate, professional, and considerate answers than several state-of-the-art methods, showcasing its effectiveness. The code is available at https://github.com/WENGSYX/HoT.\n}"}, "2110.02204": {"original_text": "  Contextualised word embeddings generated from Neural Language Models (NLMs),\nsuch as BERT, represent a word with a vector that considers the semantics of\nthe target word as well its context. On the other hand, static word embeddings\nsuch as GloVe represent words by relatively low-dimensional, memory- and\ncompute-efficient vectors but are not sensitive to the different senses of the\nword. We propose Context Derived Embeddings of Senses (CDES), a method that\nextracts sense related information from contextualised embeddings and injects\nit into static embeddings to create sense-specific static embeddings.\nExperimental results on multiple benchmarks for word sense disambiguation and\nsense discrimination tasks show that CDES can accurately learn sense-specific\nstatic embeddings reporting comparable performance to the current\nstate-of-the-art sense embeddings.\n", "rewritten_text": "\\boxed{Neural Language Models like BERT generate contextualized word embeddings that capture a word's semantics and context, whereas static embeddings like GloVe use low-dimensional vectors that are efficient but sense-insensitive. We introduce Context Derived Embeddings of Senses (CDES), a method that extracts sense-related information from contextualized embeddings and integrates it into static embeddings to create sense-specific static embeddings. Our experiments on multiple benchmarks demonstrate that CDES can learn accurate sense-specific static embeddings, achieving performance comparable to state-of-the-art sense embeddings.}"}, "1804.00247": {"original_text": "  This article describes our experiments in neural machine translation using\nthe recent Tensor2Tensor framework and the Transformer sequence-to-sequence\nmodel (Vaswani et al., 2017). We examine some of the critical parameters that\naffect the final translation quality, memory usage, training stability and\ntraining time, concluding each experiment with a set of recommendations for\nfellow researchers. In addition to confirming the general mantra \"more data and\nlarger models\", we address scaling to multiple GPUs and provide practical tips\nfor improved training regarding batch size, learning rate, warmup steps,\nmaximum sentence length and checkpoint averaging. We hope that our observations\nwill allow others to get better results given their particular hardware and\ndata constraints.\n", "rewritten_text": "\\boxed{This article outlines our neural machine translation experiments using the Tensor2Tensor framework and the Transformer sequence-to-sequence model. We investigate key parameters impacting translation quality, memory usage, training stability, and training time, providing recommendations for fellow researchers. Our findings confirm the importance of large datasets and models, while also offering practical advice on scaling to multiple GPUs, optimizing batch size, learning rate, warmup steps, maximum sentence length, and checkpoint averaging to improve training outcomes. Our goal is to help others achieve better results within their specific hardware and data limitations.}"}, "2402.08874": {"original_text": "  While large language models (LLMs) excel at understanding and generating\nplain text, they are not tailored to handle hierarchical text structures or\ndirectly predict task-specific properties such as text rating. In fact,\nselectively and repeatedly grasping the hierarchical structure of large-scale\ntext is pivotal for deciphering its essence. To this end, we propose a novel\nframework for hierarchical text rating utilizing LLMs, which incorporates\nRecurrent Alignment with Hard Attention (RAHA). Particularly, hard attention\nmechanism prompts a frozen LLM to selectively focus on pertinent leaf texts\nassociated with the root text and generate symbolic representations of their\nrelationships. Inspired by the gradual stabilization of the Markov Chain,\nrecurrent alignment strategy involves feeding predicted ratings iteratively\nback into the prompts of another trainable LLM, aligning it to progressively\napproximate the desired target. Experimental results demonstrate that RAHA\noutperforms existing state-of-the-art methods on three hierarchical text rating\ndatasets. Theoretical and empirical analysis confirms RAHA's ability to\ngradually converge towards the underlying target through multiple inferences.\nAdditional experiments on plain text rating datasets verify the effectiveness\nof this Markov-like alignment. Our data and code can be available in\nhttps://github.com/ECNU-Text-Computing/Markov-LLM.\n", "rewritten_text": "\\boxed{\nLarge language models (LLMs) struggle to handle hierarchical text structures and predict task-specific properties like text ratings. To address this, we propose a novel framework that combines LLMs with Recurrent Alignment and Hard Attention (RAHA) to rate hierarchical text. RAHA uses a frozen LLM to focus on relevant leaf texts and generate symbolic representations of their relationships. Then, it iteratively feeds predicted ratings back into a trainable LLM to align it with the target rating. Our experiments show that RAHA outperforms existing methods on three datasets and converges towards the target rating through multiple inferences. We also demonstrate its effectiveness on plain text rating datasets. Our code and data are available at https://github.com/ECNU-Text-Computing/Markov-LLM.\n}"}, "2410.10227": {"original_text": "  Few-Shot Learning (FSL) aims to recognize new classes with limited labeled\ndata. Recent studies have attempted to address the challenge of rare samples\nwith textual prompts to modulate visual features. However, they usually\nstruggle to capture complex semantic relationships between textual and visual\nfeatures. Moreover, vanilla self-attention is heavily affected by useless\ninformation in images, severely constraining the potential of semantic priors\nin FSL due to the confusion of numerous irrelevant tokens during interaction.\nTo address these aforementioned issues, a K-NN Transformer with Pyramid Prompts\n(KTPP) is proposed to select discriminative information with K-NN Context\nAttention (KCA) and adaptively modulate visual features with Pyramid\nCross-modal Prompts (PCP). First, for each token, the KCA only selects the K\nmost relevant tokens to compute the self-attention matrix and incorporates the\nmean of all tokens as the context prompt to provide the global context in three\ncascaded stages. As a result, irrelevant tokens can be progressively\nsuppressed. Secondly, pyramid prompts are introduced in the PCP to emphasize\nvisual features via interactions between text-based class-aware prompts and\nmulti-scale visual features. This allows the ViT to dynamically adjust the\nimportance weights of visual features based on rich semantic information at\ndifferent scales, making models robust to spatial variations. Finally,\naugmented visual features and class-aware prompts are interacted via the KCA to\nextract class-specific features. Consequently, our model further enhances\nnoise-free visual representations via deep cross-modal interactions, extracting\ngeneralized visual representation in scenarios with few labeled samples.\nExtensive experiments on four benchmark datasets demonstrate the effectiveness\nof our method.\n", "rewritten_text": "\\boxed{\nFew-Shot Learning (FSL) faces the challenge of recognizing new classes with limited labeled data. To tackle this, recent studies have used textual prompts to modulate visual features, but they struggle to capture complex semantic relationships between the two. Moreover, traditional self-attention is hindered by irrelevant image information, limiting the potential of semantic priors in FSL.\n\nTo address these issues, we propose the K-NN Transformer with Pyramid Prompts (KTPP) model. KTPP uses K-NN Context Attention (KCA) to select relevant tokens and Pyramid Cross-modal Prompts (PCP) to adaptively modulate visual features. The KCA selects the top K tokens for self-attention and incorporates a global context prompt to suppress irrelevant tokens. The PCP introduces pyramid prompts to emphasize visual features through interactions between text-based class-aware prompts and multi-scale visual features, making the model robust to spatial variations.\n\nBy interacting augmented visual features and class-aware prompts via the KCA, our model extracts class-specific features and enhances noise-free visual representations through deep cross-modal interactions. This enables the extraction of generalized visual representations in scenarios with few labeled samples. Our method is validated through extensive experiments on four benchmark datasets.\n}"}, "2005.00692": {"original_text": "  Cross-lingual Entity Linking (XEL), the problem of grounding mentions of\nentities in a foreign language text into an English knowledge base such as\nWikipedia, has seen a lot of research in recent years, with a range of\npromising techniques. However, current techniques do not rise to the challenges\nintroduced by text in low-resource languages (LRL) and, surprisingly, fail to\ngeneralize to text not taken from Wikipedia, on which they are usually trained.\n  This paper provides a thorough analysis of low-resource XEL techniques,\nfocusing on the key step of identifying candidate English Wikipedia titles that\ncorrespond to a given foreign language mention. Our analysis indicates that\ncurrent methods are limited by their reliance on Wikipedia's interlanguage\nlinks and thus suffer when the foreign language's Wikipedia is small. We\nconclude that the LRL setting requires the use of outside-Wikipedia\ncross-lingual resources and present a simple yet effective zero-shot XEL\nsystem, QuEL, that utilizes search engines query logs. With experiments on 25\nlanguages, QuEL~shows an average increase of 25\\% in gold candidate recall and\nof 13\\% in end-to-end linking accuracy over state-of-the-art baselines.\n", "rewritten_text": "\\boxed{\nCross-lingual Entity Linking (XEL) has been a popular research topic in recent years, but current methods struggle with texts in low-resource languages and fail to generalize to texts outside of Wikipedia, where they are typically trained. This paper examines the limitations of existing XEL techniques, particularly in identifying English Wikipedia titles that match foreign language mentions. The analysis reveals that current methods rely too heavily on Wikipedia's interlanguage links, which are insufficient for small foreign language Wikipedias. To address this, the paper proposes a zero-shot XEL system called QuEL, which leverages search engine query logs and achieves significant improvements in candidate recall and linking accuracy across 25 languages, outperforming state-of-the-art baselines by 25% and 13%, respectively.\n}"}, "2103.12462": {"original_text": "  Person ReID methods always learn through a stationary domain that is fixed by\nthe choice of a given dataset. In many contexts (e.g., lifelong learning),\nthose methods are ineffective because the domain is continually changing in\nwhich case incremental learning over multiple domains is required potentially.\nIn this work we explore a new and challenging ReID task, namely lifelong person\nre-identification (LReID), which enables to learn continuously across multiple\ndomains and even generalise on new and unseen domains. Following the cognitive\nprocesses in the human brain, we design an Adaptive Knowledge Accumulation\n(AKA) framework that is endowed with two crucial abilities: knowledge\nrepresentation and knowledge operation. Our method alleviates catastrophic\nforgetting on seen domains and demonstrates the ability to generalize to unseen\ndomains. Correspondingly, we also provide a new and large-scale benchmark for\nLReID. Extensive experiments demonstrate our method outperforms other\ncompetitors by a margin of 5.8% mAP in generalising evaluation.\n", "rewritten_text": "\\boxed{Traditional person re-identification (ReID) methods are limited by their inability to adapt to changing domains, making them ineffective in real-world scenarios like lifelong learning. To address this, we propose a new task called lifelong person re-identification (LReID), which enables continuous learning across multiple domains and generalization to new, unseen domains. Inspired by the human brain, we develop an Adaptive Knowledge Accumulation (AKA) framework that can represent and operate on knowledge, thereby mitigating catastrophic forgetting and achieving generalization to unseen domains. We also introduce a large-scale benchmark for LReID and demonstrate that our method outperforms others by a significant margin of 5.8% in mean average precision (mAP) in generalization evaluation.}"}, "1705.07426": {"original_text": "  While the research community appears to have developed a consensus on the\nmethods of acquiring annotated data, design and training of CNNs, many\nquestions still remain to be answered. In this paper, we explore the following\nquestions that are critical to face recognition research: (i) Can we train on\nstill images and expect the systems to work on videos? (ii) Are deeper datasets\nbetter than wider datasets? (iii) Does adding label noise lead to improvement\nin performance of deep networks? (iv) Is alignment needed for face recognition?\nWe address these questions by training CNNs using CASIA-WebFace, UMDFaces, and\na new video dataset and testing on YouTube- Faces, IJB-A and a disjoint portion\nof UMDFaces datasets. Our new data set, which will be made publicly available,\nhas 22,075 videos and 3,735,476 human annotated frames extracted from them.\n", "rewritten_text": "\\boxed{Despite a general agreement on how to obtain annotated data and design/training of Convolutional Neural Networks (CNNs), several crucial questions in face recognition research remain unanswered. This paper investigates four key questions: (i) Can CNNs trained on still images effectively recognize faces in videos? (ii) Do deeper datasets outperform wider datasets? (iii) Does introducing label noise improve deep network performance? (iv) Is face alignment necessary for face recognition? To answer these questions, we train CNNs using three datasets (CASIA-WebFace, UMDFaces, and a new video dataset) and test them on three datasets (YouTube-Faces, IJB-A, and a portion of UMDFaces). Our new dataset, which will be publicly available, consists of 22,075 videos with 3,735,476 human-annotated frames.}"}, "2010.06363": {"original_text": "  Lip motion reflects behavior characteristics of speakers, and thus can be\nused as a new kind of biometrics in speaker recognition. In the literature,\nlots of works used two-dimensional (2D) lip images to recognize speaker in a\ntextdependent context. However, 2D lip easily suffers from various face\norientations. To this end, in this work, we present a novel end-to-end 3D lip\nmotion Network (3LMNet) by utilizing the sentence-level 3D lip motion (S3DLM)\nto recognize speakers in both the text-independent and text-dependent contexts.\nA new regional feedback module (RFM) is proposed to obtain attentions in\ndifferent lip regions. Besides, prior knowledge of lip motion is investigated\nto complement RFM, where landmark-level and frame-level features are merged to\nform a better feature representation. Moreover, we present two methods, i.e.,\ncoordinate transformation and face posture correction to pre-process the LSD-AV\ndataset, which contains 68 speakers and 146 sentences per speaker. The\nevaluation results on this dataset demonstrate that our proposed 3LMNet is\nsuperior to the baseline models, i.e., LSTM, VGG-16 and ResNet-34, and\noutperforms the state-of-the-art using 2D lip image as well as the 3D face. The\ncode of this work is released at\nhttps://github.com/wutong18/Three-Dimensional-Lip-\nMotion-Network-for-Text-Independent-Speaker-Recognition.\n", "rewritten_text": "\\boxed{\nLip motion can be used as a biometric for speaker recognition, but traditional 2D lip images are limited by face orientation. To overcome this, we propose a novel 3D lip motion network (3LMNet) that utilizes sentence-level 3D lip motion to recognize speakers in both text-independent and text-dependent contexts. Our approach includes a regional feedback module to focus on different lip regions, and incorporates prior knowledge of lip motion to enhance feature representation. We also introduce pre-processing methods for the LSD-AV dataset, which contains 68 speakers and 146 sentences per speaker. Our evaluation results show that 3LMNet outperforms baseline models and state-of-the-art methods using 2D lip images and 3D faces. The code for this work is available at https://github.com/wutong18/Three-Dimensional-Lip-Motion-Network-for-Text-Independent-Speaker-Recognition.\n}"}, "2210.09345": {"original_text": "  Relation Extraction (RE) has attracted increasing attention, but current RE\nevaluation is limited to in-domain evaluation setups. Little is known on how\nwell a RE system fares in challenging, but realistic out-of-distribution\nevaluation setups. To address this gap, we propose CrossRE, a new,\nfreely-available cross-domain benchmark for RE, which comprises six distinct\ntext domains and includes multi-label annotations. An additional innovation is\nthat we release meta-data collected during annotation, to include explanations\nand flags of difficult instances. We provide an empirical evaluation with a\nstate-of-the-art model for relation classification. As the meta-data enables us\nto shed new light on the state-of-the-art model, we provide a comprehensive\nanalysis on the impact of difficult cases and find correlations between model\nand human annotations. Overall, our empirical investigation highlights the\ndifficulty of cross-domain RE. We release our dataset, to spur more research in\nthis direction.\n", "rewritten_text": "\\boxed{The evaluation of Relation Extraction (RE) systems has been limited to in-domain setups, leaving a knowledge gap about their performance in realistic out-of-distribution scenarios. To bridge this gap, we introduce CrossRE, a novel, publicly available benchmark that spans six diverse text domains and includes multi-label annotations. Additionally, we provide metadata from the annotation process, including explanations and flags for challenging instances. Our empirical evaluation using a state-of-the-art relation classification model reveals the difficulties of cross-domain RE and sheds light on the impact of difficult cases, highlighting correlations between model and human annotations. We make our dataset publicly available to encourage further research in this area.}"}, "1006.2734": {"original_text": "  A difficult problem in clustering is how to handle data with a manifold\nstructure, i.e. data that is not shaped in the form of compact clouds of\npoints, forming arbitrary shapes or paths embedded in a high-dimensional space.\nIn this work we introduce the Penalized k-Nearest-Neighbor-Graph (PKNNG) based\nmetric, a new tool for evaluating distances in such cases. The new metric can\nbe used in combination with most clustering algorithms. The PKNNG metric is\nbased on a two-step procedure: first it constructs the k-Nearest-Neighbor-Graph\nof the dataset of interest using a low k-value and then it adds edges with an\nexponentially penalized weight for connecting the sub-graphs produced by the\nfirst step. We discuss several possible schemes for connecting the different\nsub-graphs. We use three artificial datasets in four different embedding\nsituations to evaluate the behavior of the new metric, including a comparison\namong different clustering methods. We also evaluate the new metric in a real\nworld application, clustering the MNIST digits dataset. In all cases the PKNNG\nmetric shows promising clustering results.\n", "rewritten_text": "\\boxed{Clustering data with complex structures, such as arbitrary shapes or paths in high-dimensional spaces, is a challenging problem. To address this, we propose the Penalized k-Nearest-Neighbor-Graph (PKNNG) metric, a novel tool for measuring distances in such cases. This metric can be used with most clustering algorithms and involves a two-step process: first, it builds a k-Nearest-Neighbor-Graph with a low k-value, and then adds edges with penalized weights to connect the resulting sub-graphs. We explore different schemes for connecting these sub-graphs and evaluate the PKNNG metric using three artificial datasets in various embedding situations, as well as a real-world application involving the MNIST digits dataset. The results show promising clustering performance using the PKNNG metric.}"}, "1804.01422": {"original_text": "  In this paper, we propose a simple but effective semantic-based aggregation\n(SBA) method. The proposed SBA utilizes the discriminative filters of deep\nconvolutional layers as semantic detectors. Moreover, we propose the effective\nunsupervised strategy to select some semantic detectors to generate the\n\"probabilistic proposals\", which highlight certain discriminative pattern of\nobjects and suppress the noise of background. The final global SBA\nrepresentation could then be acquired by aggregating the regional\nrepresentations weighted by the selected \"probabilistic proposals\"\ncorresponding to various semantic content. Our unsupervised SBA is easy to\ngeneralize and achieves excellent performance on various tasks. We conduct\ncomprehensive experiments and show that our unsupervised SBA outperforms the\nstate-of-the-art unsupervised and supervised aggregation methods on image\nretrieval, place recognition and cloud classification.\n", "rewritten_text": "\\boxed{This paper introduces a novel semantic-based aggregation (SBA) method that leverages the discriminative capabilities of deep convolutional layers as semantic detectors. An effective unsupervised strategy is proposed to select a subset of these detectors, generating \"probabilistic proposals\" that highlight distinctive object patterns while suppressing background noise. The final SBA representation is obtained by aggregating regional representations, weighted by the selected proposals corresponding to diverse semantic content. The proposed unsupervised SBA method is easily generalizable and demonstrates superior performance on various tasks, outperforming state-of-the-art unsupervised and supervised aggregation methods in image retrieval, place recognition, and cloud classification.}"}, "2109.00881": {"original_text": "  There is a growing interest in product aesthetics analytics and design.\nHowever, the lack of available large-scale data that covers various variables\nand information is one of the biggest challenges faced by analysts and\nresearchers. In this paper, we present our multidisciplinary initiative of\ndeveloping a comprehensive automotive dataset from different online sources and\nformats. Specifically, the created dataset contains 1.4 million images from 899\ncar models and their corresponding model specifications and sales information\nover more than ten years in the UK market. Our work makes significant\ncontributions to: (i) research and applications in the automotive industry;\n(ii) big data creation and sharing; (iii) database design; and (iv) data\nfusion. Apart from our motivation, technical details and data structure, we\nfurther present three simple examples to demonstrate how our data can be used\nin business research and applications.\n", "rewritten_text": "\\boxed{There is a rising interest in analyzing and designing product aesthetics, but a major obstacle is the scarcity of large-scale data that covers various variables and information. To address this, we have developed a comprehensive dataset of automotive data from diverse online sources and formats, featuring 1.4 million images of 899 car models, along with their specifications and sales data from the UK market over a decade. This dataset makes significant contributions to research and applications in the automotive industry, big data creation and sharing, database design, and data fusion. We also provide examples of how this data can be utilized in business research and applications, in addition to discussing our motivation, technical details, and data structure.}"}, "1611.09007": {"original_text": "  Hyperspectral imaging sensors are becoming increasingly popular in robotics\napplications such as agriculture and mining, and allow per-pixel thematic\nclassification of materials in a scene based on their unique spectral\nsignatures. Recently, convolutional neural networks have shown remarkable\nperformance for classification tasks, but require substantial amounts of\nlabelled training data. This data must sufficiently cover the variability\nexpected to be encountered in the environment. For hyperspectral data, one of\nthe main variations encountered outdoors is due to incident illumination, which\ncan change in spectral shape and intensity depending on the scene geometry. For\nexample, regions occluded from the sun have a lower intensity and their\nincident irradiance skewed towards shorter wavelengths.\n  In this work, a data augmentation strategy based on relighting is used during\ntraining of a hyperspectral convolutional neural network. It allows training to\noccur in the outdoor environment given only a small labelled region, which does\nnot need to sufficiently represent the geometric variability of the entire\nscene. This is important for applications where obtaining large amounts of\ntraining data is labourious, hazardous or difficult, such as labelling pixels\nwithin shadows. Radiometric normalisation approaches for pre-processing the\nhyperspectral data are analysed and it is shown that methods based on the raw\npixel data are sufficient to be used as input for the classifier. This removes\nthe need for external hardware such as calibration boards, which can restrict\nthe application of hyperspectral sensors in robotics applications. Experiments\nto evaluate the classification system are carried out on two datasets captured\nfrom a field-based platform.\n", "rewritten_text": "\\boxed{\nHyperspectral imaging sensors are gaining popularity in robotics applications like agriculture and mining, enabling per-pixel classification of materials based on their unique spectral signatures. However, convolutional neural networks (CNNs) require substantial labeled training data to achieve good performance. This data must cover the expected variability in the environment, including changes in incident illumination due to scene geometry. To address this, a data augmentation strategy based on relighting is proposed for training a hyperspectral CNN. This approach allows training with a small labeled region, without requiring it to represent the entire scene's geometric variability. This is particularly useful in applications where obtaining large amounts of training data is challenging. The effectiveness of radiometric normalization approaches for pre-processing hyperspectral data is also analyzed, showing that methods based on raw pixel data are sufficient for the classifier. This eliminates the need for external hardware like calibration boards, making hyperspectral sensors more applicable in robotics. The proposed classification system is evaluated on two datasets captured from a field-based platform.\n}"}, "1905.00996": {"original_text": "  In this work, we propose a novel framework named Region-Aware Network\n(RANet), which learns the ability of anti-confusing in case of heavy occlusion,\nnearby person and symmetric appearance, for human pose estimation.\nSpecifically, the proposed method addresses three key aspects, i.e., data\naugmentation, feature learning and prediction fusion, respectively. First, we\npropose Parsing-based Data Augmentation (PDA) to generate abundant data that\nsynthesizes confusing textures. Second, we not only propose a Feature Pyramid\nStem (FPS) to learn stronger low-level features in lower stage; but also\nincorporate an Effective Region Extraction (ERE) module to excavate better\ntarget-specific features. Third, we introduce Cascade Voting Fusion (CVF) to\nexplicitly exclude the inferior predictions and fuse the rest effective\npredictions for the final pose estimation. Extensive experimental results on\ntwo popular benchmarks, i.e. MPII and LSP, demonstrate the effectiveness of our\nmethod against the state-of-the-art competitors. Especially on\neasily-confusable joints, our method makes significant improvement.\n", "rewritten_text": "\\boxed{We introduce a novel framework called Region-Aware Network (RANet) that tackles the challenges of heavy occlusion, nearby people, and symmetric appearance in human pose estimation. Our approach focuses on three key areas: data augmentation, feature learning, and prediction fusion. We propose a parsing-based data augmentation method to generate diverse data with confusing textures, a feature pyramid stem to learn robust low-level features, and an effective region extraction module to extract target-specific features. Additionally, we introduce a cascade voting fusion method to eliminate inferior predictions and combine effective ones for accurate pose estimation. Our experimental results on the MPII and LSP benchmarks show that our method outperforms state-of-the-art competitors, particularly on easily confusable joints.}"}, "2304.00025": {"original_text": "  After the pandemic, artificial intelligence (AI) powered support for mental\nhealth care has become increasingly important. The breadth and complexity of\nsignificant challenges required to provide adequate care involve: (a)\nPersonalized patient understanding, (b) Safety-constrained and medically\nvalidated chatbot patient interactions, and (c) Support for continued\nfeedback-based refinements in design using chatbot-patient interactions. We\npropose Alleviate, a chatbot designed to assist patients suffering from mental\nhealth challenges with personalized care and assist clinicians with\nunderstanding their patients better. Alleviate draws from an array of publicly\navailable clinically valid mental-health texts and databases, allowing\nAlleviate to make medically sound and informed decisions. In addition,\nAlleviate's modular design and explainable decision-making lends itself to\nrobust and continued feedback-based refinements to its design. In this paper,\nwe explain the different modules of Alleviate and submit a short video\ndemonstrating Alleviate's capabilities to help patients and clinicians\nunderstand each other better to facilitate optimal care strategies.\n", "rewritten_text": "\\boxed{\nThe COVID-19 pandemic has highlighted the need for artificial intelligence (AI) to support mental health care. To provide adequate care, three significant challenges must be addressed: (1) understanding patients on a personal level, (2) ensuring safe and medically validated interactions between patients and chatbots, and (3) refining chatbot design through continuous feedback. To address these challenges, we propose Alleviate, a chatbot that offers personalized care to patients with mental health issues and helps clinicians better understand their patients. Alleviate draws on publicly available, clinically validated mental health texts and databases to make informed decisions. Its modular design and transparent decision-making process enable continuous refinement through feedback. This paper outlines Alleviate's modules and includes a video demonstration of its capabilities to facilitate optimal care strategies by improving patient-clinician understanding.\n}"}, "1204.2294": {"original_text": "  This paper present our new intensity chromaticity space-based feature\ndetection and matching algorithm. This approach utilizes hybridization of\nwireless local area network and camera internal sensor which to receive signal\nstrength from a access point and the same time retrieve interest point\ninformation from hallways. This information is combined by model fitting\napproach in order to find the absolute of user target position. No conventional\nsearching algorithm is required, thus it is expected reducing the computational\ncomplexity. Finally we present pre-experimental results to illustrate the\nperformance of the localization system for an indoor environment set-up.\n", "rewritten_text": "\\boxed{This paper introduces a novel feature detection and matching algorithm based on intensity chromaticity space. By combining data from a wireless local area network and a camera's internal sensor, our approach simultaneously captures signal strength from an access point and interest point information from hallways. We then use a model fitting approach to determine the user's absolute target position, eliminating the need for traditional searching algorithms and reducing computational complexity. Preliminary experimental results demonstrate the effectiveness of our localization system in an indoor setting.}"}, "2410.09595": {"original_text": "  Deformable image registration remains a fundamental task in clinical\npractice, yet solving registration problems involving complex deformations\nremains challenging. Current deep learning-based registration methods employ\ncontinuous deformation to model large deformations, which often suffer from\naccumulated registration errors and interpolation inaccuracies. Moreover,\nachieving satisfactory results with these frameworks typically requires a large\nnumber of cascade stages, demanding substantial computational resources.\nTherefore, we propose a novel approach, the field refinement framework\n(FiRework), tailored for unsupervised deformable registration, aiming to\naddress these challenges. In FiRework, we redesign the continuous deformation\nframework to mitigate the aforementioned errors. Notably, our FiRework requires\nonly one level of recursion during training and supports continuous inference,\noffering improved efficacy compared to continuous deformation frameworks. We\nconducted experiments on two brain MRI datasets, enhancing two existing\ndeformable registration networks with FiRework. The experimental results\ndemonstrate the superior performance of our proposed framework in deformable\nregistration. The code is publicly available at\nhttps://github.com/ZAX130/FiRework.\n", "rewritten_text": "\\boxed{\nDeformable image registration is a crucial task in clinical practice, but it remains a challenge to solve registration problems with complex deformations. Current deep learning-based methods use continuous deformation to model large deformations, but they often struggle with accumulated errors and interpolation inaccuracies. Moreover, these methods typically require multiple stages and significant computational resources to achieve satisfactory results. To address these limitations, we introduce a novel approach called FiRework, designed for unsupervised deformable registration. FiRework refines the continuous deformation framework to mitigate errors and requires only one level of recursion during training, supporting continuous inference. Our experiments on two brain MRI datasets show that FiRework enhances existing deformable registration networks and outperforms continuous deformation frameworks. The code is available at https://github.com/ZAX130/FiRework.\n}"}, "2401.07770": {"original_text": "  Computer vision tasks typically involve describing what is present in an\nimage (e.g. classification, detection, segmentation, and captioning). We study\na visual common sense task that requires understanding what is not present.\nSpecifically, given an image (e.g. of a living room) and name of an object\n(\"cushion\"), a vision system is asked to predict semantically-meaningful\nregions (masks or bounding boxes) in the image where that object could be\nplaced or is likely be placed by humans (e.g. on the sofa). We call this task:\nSemantic Placement (SP) and believe that such common-sense visual understanding\nis critical for assitive robots (tidying a house), and AR devices\n(automatically rendering an object in the user's space). Studying the invisible\nis hard. Datasets for image description are typically constructed by curating\nrelevant images and asking humans to annotate the contents of the image;\nneither of those two steps are straightforward for objects not present in the\nimage. We overcome this challenge by operating in the opposite direction: we\nstart with an image of an object in context from web, and then remove that\nobject from the image via inpainting. This automated pipeline converts\nunstructured web data into a dataset comprising pairs of images with/without\nthe object. Using this, we collect a novel dataset, with ${\\sim}1.3$M images\nacross $9$ object categories, and train a SP prediction model called CLIP-UNet.\nCLIP-UNet outperforms existing VLMs and baselines that combine semantic priors\nwith object detectors on real-world and simulated images. In our user studies,\nwe find that the SP masks predicted by CLIP-UNet are favored $43.7\\%$ and\n$31.3\\%$ times when comparing against the $4$ SP baselines on real and\nsimulated images. In addition, we demonstrate leveraging SP mask predictions\nfrom CLIP-UNet enables downstream applications like building tidying robots in\nindoor environments.\n", "rewritten_text": "\\boxed{\nComputer vision typically focuses on identifying what is present in an image, but we explore a task that requires understanding what is not present. We introduce \"Semantic Placement\" (SP), where a system predicts regions in an image where a specified object could be placed or is likely to be placed by humans. This task is crucial for assistive robots and AR devices. However, creating a dataset for SP is challenging since it involves annotating objects not present in the image. To overcome this, we use an automated pipeline that starts with an image of an object in context, removes the object via inpainting, and generates a dataset of image pairs with/without the object. We collect a novel dataset of 1.3M images across 9 object categories and train a model called CLIP-UNet, which outperforms existing models and baselines. Our user studies show that CLIP-UNet's predictions are preferred over baselines, and we demonstrate its application in building tidying robots in indoor environments.\n}"}, "2112.11454": {"original_text": "  Generating digital humans that move realistically has many applications and\nis widely studied, but existing methods focus on the major limbs of the body,\nignoring the hands and head. Hands have been separately studied, but the focus\nhas been on generating realistic static grasps of objects. To synthesize\nvirtual characters that interact with the world, we need to generate full-body\nmotions and realistic hand grasps simultaneously. Both sub-problems are\nchallenging on their own and, together, the state-space of poses is\nsignificantly larger, the scales of hand and body motions differ, and the\nwhole-body posture and the hand grasp must agree, satisfy physical constraints,\nand be plausible. Additionally, the head is involved because the avatar must\nlook at the object to interact with it. For the first time, we address the\nproblem of generating full-body, hand and head motions of an avatar grasping an\nunknown object. As input, our method, called GOAL, takes a 3D object, its\nposition, and a starting 3D body pose and shape. GOAL outputs a sequence of\nwhole-body poses using two novel networks. First, GNet generates a goal\nwhole-body grasp with a realistic body, head, arm, and hand pose, as well as\nhand-object contact. Second, MNet generates the motion between the starting and\ngoal pose. This is challenging, as it requires the avatar to walk towards the\nobject with foot-ground contact, orient the head towards it, reach out, and\ngrasp it with a realistic hand pose and hand-object contact. To achieve this,\nthe networks exploit a representation that combines SMPL-X body parameters and\n3D vertex offsets. We train and evaluate GOAL, both qualitatively and\nquantitatively, on the GRAB dataset. Results show that GOAL generalizes well to\nunseen objects, outperforming baselines. GOAL takes a step towards synthesizing\nrealistic full-body object grasping.\n", "rewritten_text": "\\boxed{\nGenerating realistic digital humans that interact with objects is a complex task that has been largely overlooked. While previous methods have focused on simulating the movement of major limbs, they have neglected the hands and head. To create virtual characters that can engage with their environment, it is essential to generate full-body motions and realistic hand grasps simultaneously. This is a challenging problem due to the large state-space of poses, differing scales of hand and body motions, and the need for physical constraints and plausibility.\n\nOur proposed method, GOAL, addresses this issue by generating full-body, hand, and head motions of an avatar grasping an unknown object. Given a 3D object, its position, and a starting 3D body pose and shape, GOAL outputs a sequence of whole-body poses using two novel networks: GNet, which generates a goal whole-body grasp, and MNet, which generates the motion between the starting and goal pose.\n\nWe train and evaluate GOAL on the GRAB dataset, and the results show that it generalizes well to unseen objects, outperforming baselines. GOAL takes a significant step towards synthesizing realistic full-body object grasping, enabling the creation of more realistic virtual characters.\n}"}, "2103.05861": {"original_text": "  Neural network pruning is an essential approach for reducing the\ncomputational complexity of deep models so that they can be well deployed on\nresource-limited devices. Compared with conventional methods, the recently\ndeveloped dynamic pruning methods determine redundant filters variant to each\ninput instance which achieves higher acceleration. Most of the existing methods\ndiscover effective sub-networks for each instance independently and do not\nutilize the relationship between different inputs. To maximally excavate\nredundancy in the given network architecture, this paper proposes a new\nparadigm that dynamically removes redundant filters by embedding the manifold\ninformation of all instances into the space of pruned networks (dubbed as\nManiDP). We first investigate the recognition complexity and feature similarity\nbetween images in the training set. Then, the manifold relationship between\ninstances and the pruned sub-networks will be aligned in the training\nprocedure. The effectiveness of the proposed method is verified on several\nbenchmarks, which shows better performance in terms of both accuracy and\ncomputational cost compared to the state-of-the-art methods. For example, our\nmethod can reduce 55.3% FLOPs of ResNet-34 with only 0.57% top-1 accuracy\ndegradation on ImageNet.\n", "rewritten_text": "\\boxed{\nNeural network pruning is crucial for deploying deep models on resource-constrained devices. Recent dynamic pruning methods have shown higher acceleration by identifying redundant filters specific to each input instance. However, most existing methods focus on individual instances without considering the relationships between them. This paper proposes a novel approach, ManiDP, which dynamically removes redundant filters by incorporating the manifold information of all instances into the pruned network space. By analyzing recognition complexity and feature similarity between training images, the manifold relationship between instances and pruned sub-networks is aligned during training. The proposed method outperforms state-of-the-art methods on several benchmarks, achieving better accuracy and computational efficiency. For instance, it reduces ResNet-34's FLOPs by 55.3% with only 0.57% top-1 accuracy degradation on ImageNet."}, "2002.01127": {"original_text": "  How to generate descriptions from structured data organized in tables?\nExisting approaches using neural encoder-decoder models often suffer from\nlacking diversity. We claim that an open set of templates is crucial for\nenriching the phrase constructions and realizing varied generations. Learning\nsuch templates is prohibitive since it often requires a large paired <table,\ndescription> corpus, which is seldom available. This paper explores the problem\nof automatically learning reusable \"templates\" from paired and non-paired data.\nWe propose the variational template machine (VTM), a novel method to generate\ntext descriptions from data tables. Our contributions include: a) we carefully\ndevise a specific model architecture and losses to explicitly disentangle text\ntemplate and semantic content information, in the latent spaces, and b)we\nutilize both small parallel data and large raw text without aligned tables to\nenrich the template learning. Experiments on datasets from a variety of\ndifferent domains show that VTM is able to generate more diversely while\nkeeping a good fluency and quality.\n", "rewritten_text": "\\boxed{Generating descriptions from structured data in tables is a challenging task, particularly when using neural encoder-decoder models that often lack diversity. This paper proposes a novel approach, the Variational Template Machine (VTM), which learns reusable templates from paired and non-paired data to produce diverse and fluent text descriptions. By disentangling text template and semantic content information in the latent spaces, VTM can effectively utilize small parallel data and large raw text without aligned tables, resulting in improved generation quality and diversity across various domains.}"}, "2010.03725": {"original_text": "  We present a new benchmark dataset called PARADE for paraphrase\nidentification that requires specialized domain knowledge. PARADE contains\nparaphrases that overlap very little at the lexical and syntactic level but are\nsemantically equivalent based on computer science domain knowledge, as well as\nnon-paraphrases that overlap greatly at the lexical and syntactic level but are\nnot semantically equivalent based on this domain knowledge. Experiments show\nthat both state-of-the-art neural models and non-expert human annotators have\npoor performance on PARADE. For example, BERT after fine-tuning achieves an F1\nscore of 0.709, which is much lower than its performance on other paraphrase\nidentification datasets. PARADE can serve as a resource for researchers\ninterested in testing models that incorporate domain knowledge. We make our\ndata and code freely available.\n", "rewritten_text": "\\boxed{We introduce PARADE, a novel benchmark dataset for paraphrase identification that demands specialized knowledge in computer science. This dataset comprises paraphrases with minimal lexical and syntactic overlap but equivalent semantic meaning, as well as non-paraphrases with significant overlap but differing semantic meaning. Our experiments reveal that both advanced neural models and human annotators without expertise struggle with PARADE, with fine-tuned BERT achieving a relatively low F1 score of 0.709. PARADE offers a valuable resource for researchers seeking to evaluate models that incorporate domain-specific knowledge, and we provide our data and code for public access.}"}, "2307.12067": {"original_text": "  We introduce Replay, a collection of multi-view, multi-modal videos of humans\ninteracting socially. Each scene is filmed in high production quality, from\ndifferent viewpoints with several static cameras, as well as wearable action\ncameras, and recorded with a large array of microphones at different positions\nin the room. Overall, the dataset contains over 4000 minutes of footage and\nover 7 million timestamped high-resolution frames annotated with camera poses\nand partially with foreground masks. The Replay dataset has many potential\napplications, such as novel-view synthesis, 3D reconstruction, novel-view\nacoustic synthesis, human body and face analysis, and training generative\nmodels. We provide a benchmark for training and evaluating novel-view\nsynthesis, with two scenarios of different difficulty. Finally, we evaluate\nseveral baseline state-of-the-art methods on the new benchmark.\n", "rewritten_text": "\\boxed{We present Replay, a comprehensive dataset of high-quality, multi-view, and multi-modal videos capturing human social interactions. The dataset features over 4000 minutes of footage, comprising 7 million annotated frames with camera poses and partial foreground masks, recorded from multiple static cameras and wearable action cameras, as well as a large array of microphones. Replay offers numerous applications, including novel-view synthesis, 3D reconstruction, acoustic synthesis, human body and face analysis, and generative model training. We also provide a benchmark for novel-view synthesis, with two scenarios of varying difficulty, and evaluate several state-of-the-art baseline methods on this new benchmark.}"}, "1703.10645": {"original_text": "  We propose a novel method called the Relevance Subject Machine (RSM) to solve\nthe person re-identification (re-id) problem. RSM falls under the category of\nBayesian sparse recovery algorithms and uses the sparse representation of the\ninput video under a pre-defined dictionary to identify the subject in the\nvideo. Our approach focuses on the multi-shot re-id problem, which is the\nprevalent problem in many video analytics applications. RSM captures the\nessence of the multi-shot re-id problem by constraining the support of the\nsparse codes for each input video frame to be the same. Our proposed approach\nis also robust enough to deal with time varying outliers and occlusions by\nintroducing a sparse, non-stationary noise term in the model error. We provide\na novel Variational Bayesian based inference procedure along with an intuitive\ninterpretation of the proposed update rules. We evaluate our approach over\nseveral commonly used re-id datasets and show superior performance over current\nstate-of-the-art algorithms. Specifically, for ILIDS-VID, a recent large scale\nre-id dataset, RSM shows significant improvement over all published approaches,\nachieving an 11.5% (absolute) improvement in rank 1 accuracy over the closest\ncompeting algorithm considered.\n", "rewritten_text": "\\boxed{We introduce the Relevance Subject Machine (RSM), a novel approach to person re-identification (re-id) that leverages Bayesian sparse recovery algorithms. By representing input video frames as sparse codes under a predefined dictionary, RSM effectively tackles the multi-shot re-id problem, a common challenge in video analytics. Our method constrains the support of sparse codes for each frame to be identical, ensuring robustness against time-varying outliers and occlusions. We propose a Variational Bayesian inference procedure with intuitive update rules, and demonstrate superior performance on several re-id datasets, including a significant 11.5% improvement in rank 1 accuracy on the ILIDS-VID dataset compared to the closest competing algorithm.}"}, "1908.08498": {"original_text": "  We focus on multi-modal fusion for egocentric action recognition, and propose\na novel architecture for multi-modal temporal-binding, i.e. the combination of\nmodalities within a range of temporal offsets. We train the architecture with\nthree modalities -- RGB, Flow and Audio -- and combine them with mid-level\nfusion alongside sparse temporal sampling of fused representations. In contrast\nwith previous works, modalities are fused before temporal aggregation, with\nshared modality and fusion weights over time. Our proposed architecture is\ntrained end-to-end, outperforming individual modalities as well as late-fusion\nof modalities.\n  We demonstrate the importance of audio in egocentric vision, on per-class\nbasis, for identifying actions as well as interacting objects. Our method\nachieves state of the art results on both the seen and unseen test sets of the\nlargest egocentric dataset: EPIC-Kitchens, on all metrics using the public\nleaderboard.\n", "rewritten_text": "\\boxed{\nWe introduce a novel architecture for multi-modal fusion in egocentric action recognition, which combines RGB, Flow, and Audio modalities with mid-level fusion and sparse temporal sampling. Unlike previous approaches, our method fuses modalities before temporal aggregation, using shared weights over time. Our end-to-end trained architecture outperforms individual modalities and late-fusion methods. We highlight the significance of audio in egocentric vision for identifying actions and interacting objects, and achieve state-of-the-art results on the EPIC-Kitchens dataset, surpassing existing methods on all metrics."}, "1611.09813": {"original_text": "  We propose a CNN-based approach for 3D human body pose estimation from single\nRGB images that addresses the issue of limited generalizability of models\ntrained solely on the starkly limited publicly available 3D pose data. Using\nonly the existing 3D pose data and 2D pose data, we show state-of-the-art\nperformance on established benchmarks through transfer of learned features,\nwhile also generalizing to in-the-wild scenes. We further introduce a new\ntraining set for human body pose estimation from monocular images of real\nhumans that has the ground truth captured with a multi-camera marker-less\nmotion capture system. It complements existing corpora with greater diversity\nin pose, human appearance, clothing, occlusion, and viewpoints, and enables an\nincreased scope of augmentation. We also contribute a new benchmark that covers\noutdoor and indoor scenes, and demonstrate that our 3D pose dataset shows\nbetter in-the-wild performance than existing annotated data, which is further\nimproved in conjunction with transfer learning from 2D pose data. All in all,\nwe argue that the use of transfer learning of representations in tandem with\nalgorithmic and data contributions is crucial for general 3D body pose\nestimation.\n", "rewritten_text": "\\boxed{We present a CNN-based method for estimating 3D human body poses from single RGB images, which overcomes the limitation of existing models trained on scarce public 3D pose data. By leveraging both 3D and 2D pose data, our approach achieves state-of-the-art performance on established benchmarks and generalizes well to real-world scenes. We introduce a new dataset for human body pose estimation from monocular images, featuring diverse poses, appearances, clothing, occlusions, and viewpoints. This dataset, combined with transfer learning from 2D pose data, enables better performance in outdoor and indoor scenes. Our results demonstrate that combining transfer learning with algorithmic and data contributions is essential for accurate 3D body pose estimation.}"}, "2109.12338": {"original_text": "  Model binarization is an effective method of compressing neural networks and\naccelerating their inference process. However, a significant performance gap\nstill exists between the 1-bit model and the 32-bit one. The empirical study\nshows that binarization causes a great loss of information in the forward and\nbackward propagation. We present a novel Distribution-sensitive Information\nRetention Network (DIR-Net) that retains the information in the forward and\nbackward propagation by improving internal propagation and introducing external\nrepresentations. The DIR-Net mainly relies on three technical contributions:\n(1) Information Maximized Binarization (IMB): minimizing the information loss\nand the binarization error of weights/activations simultaneously by weight\nbalance and standardization; (2) Distribution-sensitive Two-stage Estimator\n(DTE): retaining the information of gradients by distribution-sensitive soft\napproximation by jointly considering the updating capability and accurate\ngradient; (3) Representation-align Binarization-aware Distillation (RBD):\nretaining the representation information by distilling the representations\nbetween full-precision and binarized networks. The DIR-Net investigates both\nforward and backward processes of BNNs from the unified information\nperspective, thereby providing new insight into the mechanism of network\nbinarization. The three techniques in our DIR-Net are versatile and effective\nand can be applied in various structures to improve BNNs. Comprehensive\nexperiments on the image classification and objective detection tasks show that\nour DIR-Net consistently outperforms the state-of-the-art binarization\napproaches under mainstream and compact architectures, such as ResNet, VGG,\nEfficientNet, DARTS, and MobileNet. Additionally, we conduct our DIR-Net on\nreal-world resource-limited devices which achieves 11.1x storage saving and\n5.4x speedup.\n", "rewritten_text": "\\boxed{\nModel binarization is a powerful technique for compressing neural networks and speeding up their inference process, but it still lags behind 32-bit models in terms of performance. This is because binarization leads to significant information loss during forward and backward propagation. To address this, we propose a novel Distribution-sensitive Information Retention Network (DIR-Net) that preserves information by improving internal propagation and introducing external representations. DIR-Net consists of three key components: (1) Information Maximized Binarization (IMB), which minimizes information loss and binarization error; (2) Distribution-sensitive Two-stage Estimator (DTE), which retains gradient information; and (3) Representation-align Binarization-aware Distillation (RBD), which preserves representation information. By examining both forward and backward processes from an information-centric perspective, DIR-Net provides new insights into network binarization. Our approach is versatile and effective, and can be applied to various architectures to improve binary neural networks (BNNs). Experimental results on image classification and object detection tasks demonstrate that DIR-Net outperforms state-of-the-art binarization methods on mainstream and compact architectures, achieving significant storage savings and speedup on resource-limited devices.\n}"}, "2310.07343": {"original_text": "  Although large language models (LLMs) are impressive in solving various\ntasks, they can quickly be outdated after deployment. Maintaining their\nup-to-date status is a pressing concern in the current era. This paper provides\na comprehensive review of recent advances in aligning LLMs with the\never-changing world knowledge without re-training from scratch. We categorize\nresearch works systemically and provide in-depth comparisons and discussion. We\nalso discuss existing challenges and highlight future directions to facilitate\nresearch in this field. We release the paper list at\nhttps://github.com/hyintell/awesome-refreshing-llms\n", "rewritten_text": "\\boxed{This paper presents a thorough review of recent developments in updating large language models (LLMs) to keep pace with the constantly evolving knowledge landscape, without requiring a complete retraining. The review categorizes and compares existing research, discusses challenges, and identifies future research directions. A comprehensive list of papers on this topic is available at https://github.com/hyintell/awesome-refreshing-llms.}"}, "2301.05792": {"original_text": "  Class-Incremental Learning (CIL) [40] trains classifiers under a strict\nmemory budget: in each incremental phase, learning is done for new data, most\nof which is abandoned to free space for the next phase. The preserved data are\nexemplars used for replaying. However, existing methods use a static and ad hoc\nstrategy for memory allocation, which is often sub-optimal. In this work, we\npropose a dynamic memory management strategy that is optimized for the\nincremental phases and different object classes. We call our method reinforced\nmemory management (RMM), leveraging reinforcement learning. RMM training is not\nnaturally compatible with CIL as the past, and future data are strictly\nnon-accessible during the incremental phases. We solve this by training the\npolicy function of RMM on pseudo CIL tasks, e.g., the tasks built on the data\nof the 0-th phase, and then applying it to target tasks. RMM propagates two\nlevels of actions: Level-1 determines how to split the memory between old and\nnew classes, and Level-2 allocates memory for each specific class. In essence,\nit is an optimizable and general method for memory management that can be used\nin any replaying-based CIL method. For evaluation, we plug RMM into two\ntop-performing baselines (LUCIR+AANets and POD+AANets [30]) and conduct\nexperiments on three benchmarks (CIFAR-100, ImageNet-Subset, and\nImageNet-Full). Our results show clear improvements, e.g., boosting POD+AANets\nby 3.6%, 4.4%, and 1.9% in the 25-Phase settings of the above benchmarks,\nrespectively.\n", "rewritten_text": "\\boxed{\nClass-Incremental Learning (CIL) involves training classifiers with limited memory, where new data is learned and most of it is discarded to make room for the next phase. The preserved data, called exemplars, are used for replaying. However, existing methods use a fixed and arbitrary strategy for allocating memory, which is often suboptimal. This paper proposes a dynamic memory management strategy, called Reinforced Memory Management (RMM), which uses reinforcement learning to optimize memory allocation for each incremental phase and object class. RMM is trained on simulated CIL tasks and can be applied to any replaying-based CIL method. It makes two levels of decisions: how to split memory between old and new classes, and how to allocate memory for each class. Experimental results show that RMM improves the performance of two top-performing baselines on three benchmarks, with gains of up to 4.4%.\n}"}, "1711.01362": {"original_text": "  An Unreliable news is any piece of information which is false or misleading,\ndeliberately spread to promote political, ideological and financial agendas.\nRecently the problem of unreliable news has got a lot of attention as the\nnumber instances of using news and social media outlets for propaganda have\nincreased rapidly. This poses a serious threat to society, which calls for\ntechnology to automatically and reliably identify unreliable news sources. This\npaper is an effort made in this direction to build systems for detecting\nunreliable news articles. In this paper, various NLP algorithms were built and\nevaluated on Unreliable News Data 2017 dataset. Variants of hierarchical\nattention networks (HAN) are presented for encoding and classifying news\narticles which achieve the best results of 0.944 ROC-AUC. Finally, Attention\nlayer weights are visualized to understand and give insight into the decisions\nmade by HANs. The results obtained are very promising and encouraging to deploy\nand use these systems in the real world to mitigate the problem of unreliable\nnews.\n", "rewritten_text": "\\boxed{Misinformation, deliberately spread to sway public opinion and advance specific agendas, has become a pressing concern. The proliferation of false or misleading news on social media and news outlets has sparked widespread alarm. To combat this threat, there is a growing need for technology that can accurately identify unreliable news sources. This paper proposes a solution, presenting a system that utilizes natural language processing (NLP) algorithms to detect unreliable news articles. The approach, which employs variants of hierarchical attention networks (HAN), achieves a high degree of accuracy, with a ROC-AUC score of 0.944. Furthermore, the visualization of attention layer weights provides valuable insights into the decision-making process of HANs. The promising results of this study suggest that such systems could be effectively deployed in real-world settings to mitigate the problem of misinformation.}"}, "1803.10464": {"original_text": "  The deficiency of segmentation labels is one of the main obstacles to\nsemantic segmentation in the wild. To alleviate this issue, we present a novel\nframework that generates segmentation labels of images given their image-level\nclass labels. In this weakly supervised setting, trained models have been known\nto segment local discriminative parts rather than the entire object area. Our\nsolution is to propagate such local responses to nearby areas which belong to\nthe same semantic entity. To this end, we propose a Deep Neural Network (DNN)\ncalled AffinityNet that predicts semantic affinity between a pair of adjacent\nimage coordinates. The semantic propagation is then realized by random walk\nwith the affinities predicted by AffinityNet. More importantly, the supervision\nemployed to train AffinityNet is given by the initial discriminative part\nsegmentation, which is incomplete as a segmentation annotation but sufficient\nfor learning semantic affinities within small image areas. Thus the entire\nframework relies only on image-level class labels and does not require any\nextra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with\nsegmentation labels generated by our method outperforms previous models trained\nwith the same level of supervision, and is even as competitive as those relying\non stronger supervision.\n", "rewritten_text": "\\boxed{The lack of segmentation labels is a major hurdle in semantic segmentation. To overcome this, we propose a novel framework that generates segmentation labels from image-level class labels. In this weakly supervised setting, models often focus on local discriminative parts rather than the entire object. Our solution involves propagating these local responses to nearby areas of the same semantic entity using a Deep Neural Network called AffinityNet, which predicts semantic affinity between adjacent image coordinates. The framework relies solely on image-level class labels, without requiring additional data or annotations, and achieves competitive results on the PASCAL VOC 2012 dataset.}"}, "1902.07938": {"original_text": "  Named entity recognition (NER) is an important task in NLP, which is all the\nmore challenging in conversational domain with their noisy facets. Moreover,\nconversational texts are often available in limited amount, making supervised\ntasks infeasible. To learn from small data, strong inductive biases are\nrequired. Previous work relied on hand-crafted features to encode these biases\nuntil transfer learning emerges. Here, we explore a transfer learning method,\nnamely language model pretraining, on NER task in Indonesian conversational\ntexts. We utilize large unlabeled data (generic domain) to be transferred to\nconversational texts, enabling supervised training on limited in-domain data.\nWe report two transfer learning variants, namely supervised model fine-tuning\nand unsupervised pretrained LM fine-tuning. Our experiments show that both\nvariants outperform baseline neural models when trained on small data (100\nsentences), yielding an absolute improvement of 32 points of test F1 score.\nFurthermore, we find that the pretrained LM encodes part-of-speech information\nwhich is a strong predictor for NER.\n", "rewritten_text": "\\boxed{Named entity recognition (NER) is a crucial task in natural language processing (NLP) that becomes even more challenging when dealing with conversational texts, which are often noisy and limited in quantity. To overcome these limitations, strong inductive biases are necessary, and transfer learning has emerged as a promising approach. This study explores the application of language model pretraining to NER tasks in Indonesian conversational texts, leveraging large amounts of unlabeled data from generic domains to enhance supervised training on limited in-domain data. The results show that two transfer learning variants, supervised model fine-tuning and unsupervised pretrained language model fine-tuning, outperform baseline neural models when trained on small datasets (100 sentences), achieving a significant improvement of 32 points in test F1 score. Additionally, the study finds that the pretrained language model captures part-of-speech information, which is a strong predictor for NER.}"}, "1504.0234": {"original_text": "  In this paper, we focus on the two key aspects of multiple target tracking\nproblem: 1) designing an accurate affinity measure to associate detections and\n2) implementing an efficient and accurate (near) online multiple target\ntracking algorithm. As the first contribution, we introduce a novel Aggregated\nLocal Flow Descriptor (ALFD) that encodes the relative motion pattern between a\npair of temporally distant detections using long term interest point\ntrajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust\naffinity measure for estimating the likelihood of matching detections\nregardless of the application scenarios. As another contribution, we present a\nNear-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is\nformulated as a data-association between targets and detections in a temporal\nwindow, that is performed repeatedly at every frame. While being efficient,\nNOMT achieves robustness via integrating multiple cues including ALFD metric,\ntarget dynamics, appearance similarity, and long term trajectory regularization\ninto the model. Our ablative analysis verifies the superiority of the ALFD\nmetric over the other conventional affinity metrics. We run a comprehensive\nexperimental evaluation on two challenging tracking datasets, KITTI and MOT\ndatasets. The NOMT method combined with ALFD metric achieves the best accuracy\nin both datasets with significant margins (about 10% higher MOTA) over the\nstate-of-the-arts.\n", "rewritten_text": "\\boxed{This paper tackles two crucial aspects of multiple target tracking: developing an accurate method to associate detections and creating an efficient online tracking algorithm. Our contributions include introducing the Aggregated Local Flow Descriptor (ALFD), a novel affinity measure that captures the relative motion pattern between detections using long-term interest point trajectories, and the Near-Online Multi-target Tracking (NOMT) algorithm. The NOMT algorithm formulates the tracking problem as a data-association task, integrating multiple cues including ALFD, target dynamics, appearance similarity, and trajectory regularization. Our experiments on the KITTI and MOT datasets demonstrate the superiority of the ALFD metric and the NOMT algorithm, achieving the best accuracy with a significant margin of around 10% higher MOTA over existing state-of-the-art methods.}"}, "2405.07399": {"original_text": "  Weeds present a significant challenge in agriculture, causing yield loss and\nrequiring expensive control measures. Automatic weed detection using computer\nvision and deep learning offers a promising solution. However, conventional\ndeep learning methods often require large amounts of labelled training data,\nwhich can be costly and time-consuming to acquire. This paper introduces a\nnovel method for semi-supervised weed detection, comprising two main\ncomponents. Firstly, a multi-scale feature representation technique is employed\nto capture distinctive weed features across different scales. Secondly, we\npropose an adaptive pseudo-label assignment strategy, leveraging a small set of\nlabelled images during training. This strategy dynamically assigns confidence\nscores to pseudo-labels generated from unlabeled data. Additionally, our\napproach integrates epoch-corresponding and mixed pseudo-labels to further\nenhance the learning process. Experimental results on the COCO dataset and five\nprominent weed datasets -- CottonWeedDet12, CropAndWeed, Palmer amaranth,\nRadishWheat, and RoboWeedMap -- illustrate that our method achieves\nstate-of-the-art performance in weed detection, even with significantly less\nlabelled data compared to existing techniques. This approach holds the\npotential to alleviate the labelling burden and enhance the feasibility and\ndeployment speed of deep learning for weed detection in real-world agricultural\nscenarios.\n", "rewritten_text": "\\boxed{\nWeeds are a major problem in agriculture, causing crop losses and requiring costly control measures. Computer vision and deep learning can help detect weeds automatically, but traditional methods need a lot of labeled training data, which is time-consuming and expensive to obtain. This paper proposes a new semi-supervised method for weed detection that uses two key components. First, it uses a multi-scale feature representation technique to capture distinct weed features at different scales. Second, it employs an adaptive pseudo-label assignment strategy that leverages a small set of labeled images during training and dynamically assigns confidence scores to pseudo-labels generated from unlabeled data. The approach also combines epoch-corresponding and mixed pseudo-labels to enhance the learning process. Experimental results on several prominent weed datasets show that our method achieves state-of-the-art performance in weed detection with significantly less labeled data than existing techniques. This approach has the potential to reduce the labeling burden and speed up the deployment of deep learning for weed detection in real-world agricultural scenarios.\n}"}, "2204.12785": {"original_text": "  Language models (LMs) have shown great potential as implicit knowledge bases\n(KBs). And for their practical use, knowledge in LMs need to be updated\nperiodically. However, existing tasks to assess LMs' efficacy as KBs do not\nadequately consider multiple large-scale updates. To this end, we first propose\na novel task--Continuously-updated QA (CuQA)--in which multiple large-scale\nupdates are made to LMs, and the performance is measured with respect to the\nsuccess in adding and updating knowledge while retaining existing knowledge. We\nthen present LMs with plug-in modules that effectively handle the updates.\nExperiments conducted on zsRE QA and NQ datasets show that our method\noutperforms existing approaches. We find that our method is 4x more effective\nin terms of updates/forgets ratio, compared to a fine-tuning baseline.\n", "rewritten_text": "\\boxed{Language models have demonstrated great potential as implicit knowledge bases, but their practical application requires periodic updates. However, current evaluation tasks for language models as knowledge bases do not adequately account for multiple large-scale updates. To address this, we introduce a new task called Continuously-updated QA (CuQA), which assesses a language model's ability to add and update knowledge while retaining existing knowledge after multiple large-scale updates. We also propose a novel approach using plug-in modules that effectively handle updates. Our experiments on the zsRE QA and NQ datasets show that our method outperforms existing approaches, achieving a 4x improvement in the updates/forgets ratio compared to a fine-tuning baseline.}"}, "2406.08215": {"original_text": "  Extractive summarization is a task of highlighting the most important parts\nof the text. We introduce a new approach to extractive summarization task using\nhidden clustering structure of the text. Experimental results on CNN/DailyMail\ndemonstrate that our approach generates more accurate summaries than both\nextractive and abstractive methods, achieving state-of-the-art results in terms\nof ROUGE-2 metric exceeding the previous approaches by 10%. Additionally, we\nshow that hidden structure of the text could be interpreted as aspects.\n", "rewritten_text": "\\boxed{We propose a novel approach to extractive summarization that leverages the hidden clustering structure of the text, outperforming both extractive and abstractive methods with state-of-the-art results, achieving a 10% improvement in ROUGE-2 metric on the CNN/DailyMail dataset, and further demonstrating that the hidden structure can be interpreted as aspects.}"}, "1205.3183": {"original_text": "  Existing probabilistic scanners and parsers impose hard constraints on the\nway lexical and syntactic ambiguities can be resolved. Furthermore, traditional\ngrammar-based parsing tools are limited in the mechanisms they allow for taking\ncontext into account. In this paper, we propose a model-driven tool that allows\nfor statistical language models with arbitrary probability estimators. Our work\non model-driven probabilistic parsing is built on top of ModelCC, a model-based\nparser generator, and enables the probabilistic interpretation and resolution\nof anaphoric, cataphoric, and recursive references in the disambiguation of\nabstract syntax graphs. In order to prove the expression power of ModelCC, we\ndescribe the design of a general-purpose natural language parser.\n", "rewritten_text": "\\boxed{Traditional parsing tools have limitations in resolving lexical and syntactic ambiguities and considering context. This paper proposes a model-driven approach that integrates statistical language models with arbitrary probability estimators, enabling the probabilistic resolution of complex references in abstract syntax graphs. Building on ModelCC, a model-based parser generator, we demonstrate the design of a general-purpose natural language parser, showcasing the expressiveness of ModelCC.}"}, "1607.06408": {"original_text": "  Action recognition has received increasing attention from the computer vision\nand machine learning communities in the last decade. To enable the study of\nthis problem, there exist a vast number of action datasets, which are recorded\nunder controlled laboratory settings, real-world surveillance environments, or\ncrawled from the Internet. Apart from the \"in-the-wild\" datasets, the training\nand test split of conventional datasets often possess similar environments\nconditions, which leads to close to perfect performance on constrained\ndatasets. In this paper, we introduce a new dataset, namely Multi-Camera Action\nDataset (MCAD), which is designed to evaluate the open view classification\nproblem under the surveillance environment. In total, MCAD contains 14,298\naction samples from 18 action categories, which are performed by 20 subjects\nand independently recorded with 5 cameras. Inspired by the well received\nevaluation approach on the LFW dataset, we designed a standard evaluation\nprotocol and benchmarked MCAD under several scenarios. The benchmark shows that\nwhile an average of 85% accuracy is achieved under the closed-view scenario,\nthe performance suffers from a significant drop under the cross-view scenario.\nIn the worst case scenario, the performance of 10-fold cross validation drops\nfrom 87.0% to 47.4%.\n", "rewritten_text": "\\boxed{Action recognition has become a prominent area of research in computer vision and machine learning over the past decade, with numerous datasets available for study. However, most datasets are limited by their controlled environments, leading to near-perfect performance. To address this, we introduce the Multi-Camera Action Dataset (MCAD), which evaluates open-view classification in surveillance environments. MCAD consists of 14,298 action samples from 18 categories, performed by 20 subjects and recorded by 5 cameras. Our benchmarking results show that while accuracy reaches 85% in closed-view scenarios, it significantly drops in cross-view scenarios, with a worst-case 10-fold cross-validation performance decrease from 87.0% to 47.4%.}"}, "2007.06233": {"original_text": "  In the majority of object detection frameworks, the confidence of instance\nclassification is used as the quality criterion of predicted bounding boxes,\nlike the confidence-based ranking in non-maximum suppression (NMS). However,\nthe quality of bounding boxes, indicating the spatial relations, is not only\ncorrelated with the classification scores. Compared with the region proposal\nnetwork (RPN) based detectors, single-shot object detectors suffer the box\nquality as there is a lack of pre-selection of box proposals. In this paper, we\naim at single-shot object detectors and propose a location-aware anchor-based\nreasoning (LAAR) for the bounding boxes. LAAR takes both the location and\nclassification confidences into consideration for the quality evaluation of\nbounding boxes. We introduce a novel network block to learn the relative\nlocation between the anchors and the ground truths, denoted as a localization\nscore, which acts as a location reference during the inference stage. The\nproposed localization score leads to an independent regression branch and\ncalibrates the bounding box quality by scoring the predicted localization score\nso that the best-qualified bounding boxes can be picked up in NMS. Experiments\non MS COCO and PASCAL VOC benchmarks demonstrate that the proposed\nlocation-aware framework enhances the performances of current anchor-based\nsingle-shot object detection frameworks and yields consistent and robust\ndetection results.\n", "rewritten_text": "\\boxed{\nMost object detection frameworks use the confidence of instance classification to evaluate the quality of predicted bounding boxes, such as in non-maximum suppression (NMS). However, the quality of bounding boxes is not only related to classification scores, but also to spatial relations. Single-shot object detectors, unlike region proposal network (RPN) based detectors, struggle with box quality due to the lack of pre-selection of box proposals. To address this, we propose a location-aware anchor-based reasoning (LAAR) approach that considers both location and classification confidences to evaluate bounding box quality. Our approach introduces a novel network block to learn the relative location between anchors and ground truths, producing a localization score that acts as a location reference during inference. This score is used to calibrate bounding box quality and pick the best-qualified boxes in NMS. Our experiments on MS COCO and PASCAL VOC benchmarks show that our location-aware framework improves the performance of anchor-based single-shot object detection frameworks and yields consistent and robust detection results.\n}"}, "2308.09475": {"original_text": "  Robot-assisted surgery has made significant progress, with instrument\nsegmentation being a critical factor in surgical intervention quality. It\nserves as the building block to facilitate surgical robot navigation and\nsurgical education for the next generation of operating intelligence. Although\nexisting methods have achieved accurate instrument segmentation results, they\nsimultaneously generate segmentation masks for all instruments, without the\ncapability to specify a target object and allow an interactive experience. This\nwork explores a new task of Referring Surgical Video Instrument Segmentation\n(RSVIS), which aims to automatically identify and segment the corresponding\nsurgical instruments based on the given language expression. To achieve this,\nwe devise a novel Video-Instrument Synergistic Network (VIS-Net) to learn both\nvideo-level and instrument-level knowledge to boost performance, while previous\nwork only used video-level information. Meanwhile, we design a Graph-based\nRelation-aware Module (GRM) to model the correlation between multi-modal\ninformation (i.e., textual description and video frame) to facilitate the\nextraction of instrument-level information. We are also the first to produce\ntwo RSVIS datasets to promote related research. Our method is verified on these\ndatasets, and experimental results exhibit that the VIS-Net can significantly\noutperform existing state-of-the-art referring segmentation methods. Our code\nand our datasets will be released upon the publication of this work.\n", "rewritten_text": "\\boxed{\nRobot-assisted surgery has made significant progress, with instrument segmentation being a crucial factor in surgical intervention quality. However, existing methods have limitations, such as generating segmentation masks for all instruments without allowing for target object specification or interactive experience. This work introduces a new task, Referring Surgical Video Instrument Segmentation (RSVIS), which aims to automatically identify and segment surgical instruments based on given language expressions. To achieve this, a novel Video-Instrument Synergistic Network (VIS-Net) is proposed to learn both video-level and instrument-level knowledge, and a Graph-based Relation-aware Module (GRM) is designed to model the correlation between textual description and video frame. Two RSVIS datasets are also created to promote related research. Experimental results show that the VIS-Net significantly outperforms existing state-of-the-art referring segmentation methods. The code and datasets will be released upon publication.\n}"}, "1401.119": {"original_text": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n", "rewritten_text": "\\boxed{Recognizing Bangla text in video frames is a challenging task due to complex backgrounds and low resolutions. To address this, we propose a two-stage algorithm. Initially, text lines are segmented into words by analyzing line contours and identifying word gaps using first-order gradient values of text blocks. Then, local binarization is applied to each word, and the text line is reconstructed. Finally, the binarized text block is sent to an Optical Character Recognition (OCR) system for recognition.}"}, "1502.06807": {"original_text": "  We introduce and evaluate several architectures for Convolutional Neural\nNetworks to predict the 3D joint locations of a hand given a depth map. We\nfirst show that a prior on the 3D pose can be easily introduced and\nsignificantly improves the accuracy and reliability of the predictions. We also\nshow how to use context efficiently to deal with ambiguities between fingers.\nThese two contributions allow us to significantly outperform the\nstate-of-the-art on several challenging benchmarks, both in terms of accuracy\nand computation times.\n", "rewritten_text": "\\boxed{We propose and assess various Convolutional Neural Network architectures for predicting 3D hand joint locations from depth maps. By incorporating a prior on 3D pose, we achieve significant improvements in prediction accuracy and reliability. Additionally, we demonstrate how to effectively utilize context to resolve finger ambiguities. These innovations enable us to surpass existing state-of-the-art performance on multiple challenging benchmarks, excelling in both accuracy and computational efficiency.}"}, "1812.01855": {"original_text": "  We aim to dismantle the prevalent black-box neural architectures used in\ncomplex visual reasoning tasks, into the proposed eXplainable and eXplicit\nNeural Modules (XNMs), which advance beyond existing neural module networks\ntowards using scene graphs --- objects as nodes and the pairwise relationships\nas edges --- for explainable and explicit reasoning with structured knowledge.\nXNMs allow us to pay more attention to teach machines how to \"think\",\nregardless of what they \"look\". As we will show in the paper, by using scene\ngraphs as an inductive bias, 1) we can design XNMs in a concise and flexible\nfashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce\nthe number of parameters by 10 to 100 times, and 2) we can explicitly trace the\nreasoning-flow in terms of graph attentions. XNMs are so generic that they\nsupport a wide range of scene graph implementations with various qualities. For\nexample, when the graphs are detected perfectly, XNMs achieve 100% accuracy on\nboth CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound\nfor visual reasoning; when the graphs are noisily detected from real-world\nimages, XNMs are still robust to achieve a competitive 67.5% accuracy on\nVQAv2.0, surpassing the popular bag-of-objects attention models without graph\nstructures.\n", "rewritten_text": "\\boxed{Our goal is to break down complex black-box neural networks used for visual reasoning tasks into transparent and explicit modules, called eXplainable and eXplicit Neural Modules (XNMs). These modules utilize scene graphs, which represent objects as nodes and their relationships as edges, to enable explainable and structured reasoning. By using scene graphs, we can design XNMs in a concise and flexible way, reducing the number of parameters by 10 to 100 times. This approach also allows us to trace the reasoning process explicitly. XNMs are versatile and can be applied to various scene graph implementations, achieving high accuracy in visual reasoning tasks, even with noisy graph detections.}"}, "2401.03340": {"original_text": "  This paper introduces the CowStallNumbers dataset, a collection of images\nextracted from videos focusing on cow teats, designed to advance the field of\ncow stall number detection. The dataset comprises 1042 training images and 261\ntest images, featuring stall numbers ranging from 0 to 60. To enhance the\ndataset, we performed fine-tuning on a YOLO model and applied data augmentation\ntechniques, including random crop, center crop, and random rotation. The\nexperimental outcomes demonstrate a notable 95.4\\% accuracy in recognizing\nstall numbers.\n", "rewritten_text": "\\boxed{This paper presents the CowStallNumbers dataset, a unique collection of images extracted from videos that focus on cow teats, with the goal of improving cow stall number detection. The dataset consists of 1042 training images and 261 test images, covering stall numbers from 0 to 60. To improve the dataset's performance, we fine-tuned a YOLO model and applied various data augmentation techniques, including random cropping, center cropping, and random rotation. Our experiments show a significant accuracy of 95.4% in recognizing stall numbers.}"}, "2307.05158": {"original_text": "  Predicting where a person is looking is a complex task, requiring to\nunderstand not only the person's gaze and scene content, but also the 3D scene\nstructure and the person's situation (are they manipulating? interacting or\nobserving others? attentive?) to detect obstructions in the line of sight or\napply attention priors that humans typically have when observing others. In\nthis paper, we hypothesize that identifying and leveraging such priors can be\nbetter achieved through the exploitation of explicitly derived multimodal cues\nsuch as depth and pose. We thus propose a modular multimodal architecture\nallowing to combine these cues using an attention mechanism. The architecture\ncan naturally be exploited in privacy-sensitive situations such as surveillance\nand health, where personally identifiable information cannot be released. We\nperform extensive experiments on the GazeFollow and VideoAttentionTarget public\ndatasets, obtaining state-of-the-art performance and demonstrating very\ncompetitive results in the privacy setting case.\n", "rewritten_text": "\\boxed{Determining where someone is looking is a challenging task that requires understanding their gaze, the scene's content, 3D structure, and their situation, including any obstructions or attention patterns. This paper proposes a modular architecture that combines multimodal cues like depth and pose to identify and leverage these attention patterns. This approach is particularly useful in privacy-sensitive situations like surveillance and healthcare, where personal information must be protected. Our experiments on public datasets demonstrate state-of-the-art performance and competitive results in privacy settings.}"}, "2302.07027": {"original_text": "  Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.\n", "rewritten_text": "\\boxed{Pretrained language models often require specialization to specific domains, which can be achieved by training an adapter for each domain. However, this approach can be impractical in certain settings. To address this, we propose AdapterSoup, a method that averages the weights of adapters trained on different domains. This approach is highly parallelizable and can improve performance on new domains without requiring additional training. We experiment with various techniques for selecting adapters to combine, including text clustering and semantic similarity, and find that clustering leads to the best results on novel domains. Additionally, we show that averaging adapters trained on the same domain with different hyperparameters can preserve performance on new domains while achieving strong in-domain results.}"}, "1706.09147": {"original_text": "  We address the task of Named Entity Disambiguation (NED) for noisy text. We\npresent WikilinksNED, a large-scale NED dataset of text fragments from the web,\nwhich is significantly noisier and more challenging than existing news-based\ndatasets. To capture the limited and noisy local context surrounding each\nmention, we design a neural model and train it with a novel method for sampling\ninformative negative examples. We also describe a new way of initializing word\nand entity embeddings that significantly improves performance. Our model\nsignificantly outperforms existing state-of-the-art methods on WikilinksNED\nwhile achieving comparable performance on a smaller newswire dataset.\n", "rewritten_text": "\\boxed{This paper tackles the challenge of Named Entity Disambiguation (NED) in noisy text. We introduce WikilinksNED, a large-scale dataset of web-extracted text fragments that is more noisy and difficult than existing news-based datasets. To handle the limited and noisy local context surrounding each mention, we develop a neural model trained with a novel method for selecting informative negative examples. Additionally, we propose a new approach for initializing word and entity embeddings, which significantly boosts performance. Our model outperforms existing state-of-the-art methods on WikilinksNED and achieves comparable results on a smaller newswire dataset.}"}, "2312.16274": {"original_text": "  Recent progress in multi-modal conditioned face synthesis has enabled the\ncreation of visually striking and accurately aligned facial images. Yet,\ncurrent methods still face issues with scalability, limited flexibility, and a\none-size-fits-all approach to control strength, not accounting for the\ndiffering levels of conditional entropy, a measure of unpredictability in data\ngiven some condition, across modalities. To address these challenges, we\nintroduce a novel uni-modal training approach with modal surrogates, coupled\nwith an entropy-aware modal-adaptive modulation, to support flexible, scalable,\nand scalable multi-modal conditioned face synthesis network. Our uni-modal\ntraining with modal surrogate that only leverage uni-modal data, use modal\nsurrogate to decorate condition with modal-specific characteristic and serve as\nlinker for inter-modal collaboration , fully learns each modality control in\nface synthesis process as well as inter-modal collaboration. The entropy-aware\nmodal-adaptive modulation finely adjust diffusion noise according to\nmodal-specific characteristics and given conditions, enabling well-informed\nstep along denoising trajectory and ultimately leading to synthesis results of\nhigh fidelity and quality. Our framework improves multi-modal face synthesis\nunder various conditions, surpassing current methods in image quality and\nfidelity, as demonstrated by our thorough experimental results.\n", "rewritten_text": "\\boxed{\nRecent advances in multi-modal conditioned face synthesis have led to the creation of highly realistic and accurately aligned facial images. However, existing methods still struggle with scalability, limited flexibility, and a one-size-fits-all approach to controlling the strength of conditioning, which fails to account for varying levels of unpredictability across different modalities. To overcome these limitations, we propose a novel approach that involves training a single model on each modality using modal surrogates, and incorporating an entropy-aware modulation that adapts to the specific characteristics of each modality and condition. This approach enables flexible, scalable, and high-quality multi-modal conditioned face synthesis. By fully learning each modality's control and inter-modal collaboration, our method produces highly realistic synthesis results, surpassing existing methods in terms of image quality and fidelity, as demonstrated by our experimental results.\n}"}, "2407.06938": {"original_text": "  We present RodinHD, which can generate high-fidelity 3D avatars from a\nportrait image. Existing methods fail to capture intricate details such as\nhairstyles which we tackle in this paper. We first identify an overlooked\nproblem of catastrophic forgetting that arises when fitting triplanes\nsequentially on many avatars, caused by the MLP decoder sharing scheme. To\novercome this issue, we raise a novel data scheduling strategy and a weight\nconsolidation regularization term, which improves the decoder's capability of\nrendering sharper details. Additionally, we optimize the guiding effect of the\nportrait image by computing a finer-grained hierarchical representation that\ncaptures rich 2D texture cues, and injecting them to the 3D diffusion model at\nmultiple layers via cross-attention. When trained on 46K avatars with a noise\nschedule optimized for triplanes, the resulting model can generate 3D avatars\nwith notably better details than previous methods and can generalize to\nin-the-wild portrait input.\n", "rewritten_text": "\\boxed{We introduce RodinHD, a novel method for generating high-quality 3D avatars from a single portrait image. Unlike existing approaches, our method successfully captures intricate details such as hairstyles. To achieve this, we address the issue of catastrophic forgetting that occurs when fitting triplanes sequentially, by proposing a new data scheduling strategy and a weight consolidation regularization term. Additionally, we enhance the guiding effect of the portrait image by computing a hierarchical representation that captures rich 2D texture cues and injecting them into the 3D diffusion model. Our model, trained on 46,000 avatars with an optimized noise schedule, generates 3D avatars with significantly better details than previous methods and can generalize to real-world portrait inputs.}"}, "2311.08107": {"original_text": "  Large Language Models (LLMs) can justify or critique their predictions\nthrough discussions with other models or humans, thereby enriching their\nintrinsic understanding of instances. While proactive discussions in the\ninference phase have been shown to boost performance, such interactions have\nnot been extensively explored during the training phase. We hypothesize that\nincorporating interactive discussions into the training process can enhance the\nmodels' understanding and improve their reasoning and verbal expression\nabilities during inference. This work introduces the SAIE framework, which\nfacilitates supportive and adversarial discussions between learner and partner\nmodels. The learner model receives responses from the partner, and its\nparameters are then updated based on this discussion. This dynamic adjustment\nprocess continues throughout the training phase, responding to the evolving\noutputs of the learner model. Our empirical evaluation across various tasks,\nincluding math problems, commonsense reasoning, and multi-domain knowledge,\ndemonstrates that models fine-tuned with the SAIE framework outperform those\ntrained with conventional fine-tuning approaches. Furthermore, our method\nenhances the models' reasoning capabilities, improving both individual and\nmulti-agent inference performance.\n", "rewritten_text": "\\boxed{\nLarge Language Models (LLMs) can improve their understanding and performance by engaging in discussions with other models or humans, either to justify or critique their predictions. While this approach has been shown to be effective during the inference phase, it has not been fully explored during the training phase. We propose that incorporating interactive discussions into the training process can enhance the models' understanding, reasoning, and verbal expression abilities. To achieve this, we introduce the SAIE framework, which facilitates discussions between a learner model and a partner model. The learner model receives feedback from the partner model and updates its parameters accordingly. This process continues throughout the training phase, adapting to the learner model's evolving outputs. Our experiments across various tasks, including math problems, commonsense reasoning, and multi-domain knowledge, demonstrate that models fine-tuned with the SAIE framework outperform those trained using traditional methods. Moreover, our approach improves the models' reasoning capabilities, leading to better individual and multi-agent inference performance.\n}"}, "1704.05831": {"original_text": "  We propose a hierarchical approach for making long-term predictions of future\nframes. To avoid inherent compounding errors in recursive pixel-level\nprediction, we propose to first estimate high-level structure in the input\nframes, then predict how that structure evolves in the future, and finally by\nobserving a single frame from the past and the predicted high-level structure,\nwe construct the future frames without having to observe any of the pixel-level\npredictions. Long-term video prediction is difficult to perform by recurrently\nobserving the predicted frames because the small errors in pixel space\nexponentially amplify as predictions are made deeper into the future. Our\napproach prevents pixel-level error propagation from happening by removing the\nneed to observe the predicted frames. Our model is built with a combination of\nLSTM and analogy based encoder-decoder convolutional neural networks, which\nindependently predict the video structure and generate the future frames,\nrespectively. In experiments, our model is evaluated on the Human3.6M and Penn\nAction datasets on the task of long-term pixel-level video prediction of humans\nperforming actions and demonstrate significantly better results than the\nstate-of-the-art.\n", "rewritten_text": "\\boxed{We introduce a hierarchical approach to predict future video frames over long periods of time. To overcome the compounding errors that occur when recursively predicting pixel-level frames, our method first estimates the high-level structure of the input frames, then predicts how this structure evolves, and finally generates the future frames based on a single past frame and the predicted structure. This approach prevents the exponential amplification of pixel-level errors that occurs when making repeated predictions. Our model combines LSTM and analogy-based encoder-decoder convolutional neural networks to independently predict video structure and generate future frames. Experimental results on the Human3.6M and Penn Action datasets demonstrate that our approach outperforms the current state-of-the-art in long-term pixel-level video prediction of human actions.}"}, "1606.05706": {"original_text": "  We study the problem of agreement and disagreement detection in online\ndiscussions. An isotonic Conditional Random Fields (isotonic CRF) based\nsequential model is proposed to make predictions on sentence- or segment-level.\nWe automatically construct a socially-tuned lexicon that is bootstrapped from\nexisting general-purpose sentiment lexicons to further improve the performance.\nWe evaluate our agreement and disagreement tagging model on two disparate\nonline discussion corpora -- Wikipedia Talk pages and online debates. Our model\nis shown to outperform the state-of-the-art approaches in both datasets. For\nexample, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for\nagreement and disagreement detection, when a linear chain CRF obtains 0.58 and\n0.56 for the discussions on Wikipedia Talk pages.\n", "rewritten_text": "\\boxed{This study focuses on detecting agreement and disagreement in online discussions, proposing an isotonic Conditional Random Fields (CRF) model to make predictions at the sentence or segment level. The model is enhanced by a socially-tuned lexicon, automatically constructed from existing sentiment lexicons. Evaluation on two online discussion datasets, Wikipedia Talk pages and online debates, shows that the proposed model outperforms state-of-the-art approaches, achieving F1 scores of 0.74 and 0.67 for agreement and disagreement detection, respectively, compared to 0.58 and 0.56 for a linear chain CRF model on Wikipedia Talk pages.}"}, "2401.13011": {"original_text": "  This paper presents a novel generative model, Collaborative Competitive\nAgents (CCA), which leverages the capabilities of multiple Large Language\nModels (LLMs) based agents to execute complex tasks. Drawing inspiration from\nGenerative Adversarial Networks (GANs), the CCA system employs two equal-status\ngenerator agents and a discriminator agent. The generators independently\nprocess user instructions and generate results, while the discriminator\nevaluates the outputs, and provides feedback for the generator agents to\nfurther reflect and improve the generation results. Unlike the previous\ngenerative model, our system can obtain the intermediate steps of generation.\nThis allows each generator agent to learn from other successful executions due\nto its transparency, enabling a collaborative competition that enhances the\nquality and robustness of the system's results. The primary focus of this study\nis image editing, demonstrating the CCA's ability to handle intricate\ninstructions robustly. The paper's main contributions include the introduction\nof a multi-agent-based generative model with controllable intermediate steps\nand iterative optimization, a detailed examination of agent relationships, and\ncomprehensive experiments on image editing. Code is available at\n\\href{https://github.com/TiankaiHang/CCA}{https://github.com/TiankaiHang/CCA}.\n", "rewritten_text": "\\boxed{This paper introduces a novel generative model called Collaborative Competitive Agents (CCA), which utilizes multiple Large Language Models (LLMs) to perform complex tasks. Inspired by Generative Adversarial Networks (GANs), CCA consists of two generator agents and a discriminator agent. The generators process user instructions independently and produce results, while the discriminator evaluates the outputs and provides feedback to the generators to refine their results. Unlike previous models, CCA allows for intermediate steps of generation to be obtained, enabling each generator to learn from successful executions and engage in collaborative competition, resulting in higher-quality and more robust outputs. The paper focuses on image editing, demonstrating CCA's ability to handle intricate instructions robustly. The main contributions include the introduction of a multi-agent-based generative model with controllable intermediate steps and iterative optimization, an examination of agent relationships, and comprehensive experiments on image editing. The code is available at https://github.com/TiankaiHang/CCA.}"}, "2309.08644": {"original_text": "  3D pose estimation is an invaluable task in computer vision with various\npractical applications. Especially, 3D pose estimation for multi-person from a\nmonocular video (3DMPPE) is particularly challenging and is still largely\nuncharted, far from applying to in-the-wild scenarios yet. We pose three\nunresolved issues with the existing methods: lack of robustness on unseen views\nduring training, vulnerability to occlusion, and severe jittering in the\noutput. As a remedy, we propose POTR-3D, the first realization of a\nsequence-to-sequence 2D-to-3D lifting model for 3DMPPE, powered by a novel\ngeometry-aware data augmentation strategy, capable of generating unbounded data\nwith a variety of views while caring about the ground plane and occlusions.\nThrough extensive experiments, we verify that the proposed model and data\naugmentation robustly generalizes to diverse unseen views, robustly recovers\nthe poses against heavy occlusions, and reliably generates more natural and\nsmoother outputs. The effectiveness of our approach is verified not only by\nachieving the state-of-the-art performance on public benchmarks, but also by\nqualitative results on more challenging in-the-wild videos. Demo videos are\navailable at https://www.youtube.com/@potr3d.\n", "rewritten_text": "\\boxed{\nEstimating 3D poses from monocular videos, particularly for multiple people, is a challenging task in computer vision with many practical applications. However, existing methods have three major limitations: they are not robust to unseen views, vulnerable to occlusion, and produce jittery outputs. To address these issues, we propose POTR-3D, a novel sequence-to-sequence 2D-to-3D lifting model that leverages a geometry-aware data augmentation strategy to generate diverse views while considering the ground plane and occlusions. Our approach demonstrates robust generalization to unseen views, effective recovery of poses from heavy occlusions, and smooth output generation. We validate the effectiveness of our method through state-of-the-art performance on public benchmarks and qualitative results on in-the-wild videos, with demo videos available at https://www.youtube.com/@potr3d.\n}"}, "2008.08735": {"original_text": "  Computer vision (CV) has achieved great success in interpreting semantic\nmeanings from images, yet CV algorithms can be brittle for tasks with adverse\nvision conditions and the ones suffering from data/label pair limitation. One\nof this tasks is in-bed human pose estimation, which has significant values in\nmany healthcare applications. In-bed pose monitoring in natural settings could\ninvolve complete darkness or full occlusion. Furthermore, the lack of publicly\navailable in-bed pose datasets hinders the use of many successful pose\nestimation algorithms for this task. In this paper, we introduce our\nSimultaneously-collected multimodal Lying Pose (SLP) dataset, which includes\nin-bed pose images from 109 participants captured using multiple imaging\nmodalities including RGB, long wave infrared, depth, and pressure map. We also\npresent a physical hyper parameter tuning strategy for ground truth pose label\ngeneration under extreme conditions such as lights off and being fully covered\nby a sheet/blanket. SLP design is compatible with the mainstream human pose\ndatasets, therefore, the state-of-the-art 2D pose estimation models can be\ntrained effectively with SLP data with promising performance as high as 95% at\nPCKh@0.5 on a single modality. The pose estimation performance can be further\nimproved by including additional modalities through collaboration.\n", "rewritten_text": "\\boxed{\nComputer vision has made significant progress in interpreting image semantics, but it can be unreliable when dealing with adverse visual conditions and limited data/label pairs. One such challenging task is estimating human pose while lying in bed, which is crucial in various healthcare applications. However, this task is often hindered by complete darkness, full occlusion, and the lack of publicly available datasets. To address this, we introduce the Simultaneously-collected multimodal Lying Pose (SLP) dataset, which includes images of 109 participants captured using multiple modalities (RGB, long wave infrared, depth, and pressure map). We also propose a physical hyperparameter tuning strategy for generating ground truth pose labels under extreme conditions. The SLP dataset is compatible with mainstream human pose datasets, allowing state-of-the-art 2D pose estimation models to be trained effectively, achieving a performance of up to 95% at PCKh@0.5 using a single modality. Furthermore, incorporating additional modalities can improve pose estimation performance through collaboration.\n}"}, "2206.04846": {"original_text": "  Deep neural networks are capable of learning powerful representations to\ntackle complex vision tasks but expose undesirable properties like the\nover-fitting issue. To this end, regularization techniques like image\naugmentation are necessary for deep neural networks to generalize well.\nNevertheless, most prevalent image augmentation recipes confine themselves to\noff-the-shelf linear transformations like scale, flip, and colorjitter. Due to\ntheir hand-crafted property, these augmentations are insufficient to generate\ntruly hard augmented examples. In this paper, we propose a novel perspective of\naugmentation to regularize the training process. Inspired by the recent success\nof applying masked image modeling to self-supervised learning, we adopt the\nself-supervised masked autoencoder to generate the distorted view of the input\nimages. We show that utilizing such model-based nonlinear transformation as\ndata augmentation can improve high-level recognition tasks. We term the\nproposed method as \\textbf{M}ask-\\textbf{R}econstruct \\textbf{A}ugmentation\n(MRA). The extensive experiments on various image classification benchmarks\nverify the effectiveness of the proposed augmentation. Specifically, MRA\nconsistently enhances the performance on supervised, semi-supervised as well as\nfew-shot classification. The code will be available at\n\\url{https://github.com/haohang96/MRA}.\n", "rewritten_text": "\\boxed{Deep neural networks can learn powerful representations for complex vision tasks, but they often suffer from over-fitting. To address this, regularization techniques like image augmentation are necessary. However, traditional image augmentation methods, which rely on hand-crafted linear transformations, are limited in generating truly challenging examples. This paper proposes a novel approach to augmentation, inspired by masked image modeling in self-supervised learning. The proposed method, called Mask-Reconstruct Augmentation (MRA), uses a self-supervised masked autoencoder to generate distorted views of input images. Experimental results on various image classification benchmarks demonstrate the effectiveness of MRA, consistently improving performance in supervised, semi-supervised, and few-shot classification settings. The code is available at https://github.com/haohang96/MRA.}"}, "1803.10547": {"original_text": "  Text articles with false claims, especially news, have recently become\naggravating for the Internet users. These articles are in wide circulation and\nreaders face difficulty discerning fact from fiction. Previous work on\ncredibility assessment has focused on factual analysis and linguistic features.\nThe task's main challenge is the distinction between the features of true and\nfalse articles. In this paper, we propose a novel approach called Credibility\nOutcome (CREDO) which aims at scoring the credibility of an article in an open\ndomain setting.\n  CREDO consists of different modules for capturing various features\nresponsible for the credibility of an article. These features includes\ncredibility of the article's source and author, semantic similarity between the\narticle and related credible articles retrieved from a knowledge base, and\nsentiments conveyed by the article. A neural network architecture learns the\ncontribution of each of these modules to the overall credibility of an article.\nExperiments on Snopes dataset reveals that CREDO outperforms the\nstate-of-the-art approaches based on linguistic features.\n", "rewritten_text": "\\boxed{The proliferation of false news articles on the internet has become increasingly frustrating for users, who struggle to distinguish fact from fiction. While previous research on credibility assessment has focused on factual analysis and linguistic features, the key challenge lies in differentiating between true and false articles. This paper proposes a novel approach, Credibility Outcome (CREDO), which aims to evaluate the credibility of articles in an open domain setting. CREDO consists of multiple modules that capture various features influencing an article's credibility, including the credibility of the source and author, semantic similarity with credible articles, and the sentiments expressed. A neural network architecture is used to learn the contribution of each module to the overall credibility of an article. Experimental results on the Snopes dataset show that CREDO outperforms existing approaches based on linguistic features.}"}, "1704.01926": {"original_text": "  This paper tackles the problem of semi-supervised video object segmentation,\nthat is, segmenting an object in a sequence given its mask in the first frame.\nOne of the main challenges in this scenario is the change of appearance of the\nobjects of interest. Their semantics, on the other hand, do not vary. This\npaper investigates how to take advantage of such invariance via the\nintroduction of a semantic prior that guides the appearance model.\nSpecifically, given the segmentation mask of the first frame of a sequence, we\nestimate the semantics of the object of interest, and propagate that knowledge\nthroughout the sequence to improve the results based on an appearance model. We\npresent Semantically-Guided Video Object Segmentation (SGV), which improves\nresults over previous state of the art on two different datasets using a\nvariety of evaluation metrics, while running in half a second per frame.\n", "rewritten_text": "\\boxed{This paper addresses the challenge of semi-supervised video object segmentation, where an object's mask is given in the first frame and must be tracked throughout the sequence despite changes in its appearance. To tackle this, the authors propose a novel approach that leverages the invariant semantics of the object to guide an appearance model. By estimating the object's semantics from the first frame and propagating this knowledge throughout the sequence, the authors' method, called Semantically-Guided Video Object Segmentation (SGV), achieves improved results on two datasets with various evaluation metrics, all while operating at a fast pace of half a second per frame.}"}, "1902.06557": {"original_text": "  We propose a novel biophysical and dichromatic reflectance model that\nefficiently characterises spectral skin reflectance. We show how to fit the\nmodel to multispectral face images enabling high quality estimation of diffuse\nand specular shading as well as biophysical parameter maps (melanin and\nhaemoglobin). Our method works from a single image without requiring complex\ncontrolled lighting setups yet provides quantitatively accurate reconstructions\nand qualitatively convincing decomposition and editing.\n", "rewritten_text": "\\boxed{We introduce a new biophysical and dichromatic reflectance model that accurately captures the spectral reflectance of skin. This model can be applied to multispectral face images, allowing for precise estimation of diffuse and specular shading, as well as the creation of detailed maps of biophysical parameters such as melanin and haemoglobin. Notably, our method can achieve high-quality results from a single image, without the need for complex and controlled lighting setups, and produces both quantitatively accurate reconstructions and visually convincing decompositions and edits.}"}, "2211.09379": {"original_text": "  In dialogue state tracking (DST), labeling the dataset involves considerable\nhuman labor. We propose a new self-training framework for few-shot generative\nDST that utilize unlabeled data. Our self-training method iteratively improves\nthe model by pseudo labeling and employs Purpose Preserving Augmentation\n(PPAug) to prevent overfitting. We increaese the few-shot 10% performance by\napproximately 4% on MultiWOZ 2.1 and enhances the slot-recall 8.34% for unseen\nvalues compared to baseline.\n", "rewritten_text": "\\boxed{We introduce a novel self-training framework for few-shot generative dialogue state tracking (DST) that leverages unlabeled data, reducing the need for human annotation. Our approach iteratively refines the model through pseudo-labeling and employs Purpose Preserving Augmentation (PPAug) to prevent overfitting, resulting in a 4% improvement in few-shot 10% performance on MultiWOZ 2.1 and an 8.34% enhancement in slot recall for unseen values compared to the baseline.}"}, "1902.11268": {"original_text": "  Deep neural networks (DNNs), especially deep convolutional neural networks\n(CNNs), have emerged as the powerful technique in various machine learning\napplications. However, the large model sizes of DNNs yield high demands on\ncomputation resource and weight storage, thereby limiting the practical\ndeployment of DNNs. To overcome these limitations, this paper proposes to\nimpose the circulant structure to the construction of convolutional layers, and\nhence leads to circulant convolutional layers (CircConvs) and circulant CNNs.\nThe circulant structure and models can be either trained from scratch or\nre-trained from a pre-trained non-circulant model, thereby making it very\nflexible for different training environments. Through extensive experiments,\nsuch strong structure-imposing approach is proved to be able to substantially\nreduce the number of parameters of convolutional layers and enable significant\nsaving of computational cost by using fast multiplication of the circulant\ntensor.\n", "rewritten_text": "\\boxed{Deep neural networks, particularly convolutional neural networks, have become a powerful tool in machine learning applications. However, their large size requires significant computational resources and storage, limiting their practical use. To address this, this paper proposes the use of circulant structures in convolutional layers, resulting in circulant convolutional layers and circulant CNNs. This approach can be applied to both new and pre-trained models, making it flexible for different training environments. Experimental results show that this method significantly reduces the number of parameters in convolutional layers and computational cost, thanks to the efficient multiplication of circulant tensors.}"}, "1704.00447": {"original_text": "  Purpose: To allow fast and high-quality reconstruction of clinical\naccelerated multi-coil MR data by learning a variational network that combines\nthe mathematical structure of variational models with deep learning.\n  Theory and Methods: Generalized compressed sensing reconstruction formulated\nas a variational model is embedded in an unrolled gradient descent scheme. All\nparameters of this formulation, including the prior model defined by filter\nkernels and activation functions as well as the data term weights, are learned\nduring an offline training procedure. The learned model can then be applied\nonline to previously unseen data.\n  Results: The variational network approach is evaluated on a clinical knee\nimaging protocol. The variational network reconstructions outperform standard\nreconstruction algorithms in terms of image quality and residual artifacts for\nall tested acceleration factors and sampling patterns.\n  Conclusion: Variational network reconstructions preserve the natural\nappearance of MR images as well as pathologies that were not included in the\ntraining data set. Due to its high computational performance, i.e.,\nreconstruction time of 193 ms on a single graphics card, and the omission of\nparameter tuning once the network is trained, this new approach to image\nreconstruction can easily be integrated into clinical workflow.\n", "rewritten_text": "\\boxed{\nThe goal of this project is to develop a fast and high-quality method for reconstructing clinical MRI data that has been accelerated using multiple coils. This is achieved by combining the strengths of variational models and deep learning to create a variational network. The network is trained offline to learn the optimal parameters for reconstruction, including the prior model and data term weights. Once trained, the network can be applied to new, unseen data online. The results show that the variational network approach outperforms standard reconstruction algorithms in terms of image quality and artifact reduction, even at high acceleration factors and with different sampling patterns. The reconstructed images preserve the natural appearance of MR images and any pathologies, even if they were not included in the training data. The method is computationally efficient, taking only 193 ms to reconstruct an image on a single graphics card, and does not require parameter tuning, making it suitable for integration into clinical workflows."}, "2104.1269": {"original_text": "  Data is the engine of modern computer vision, which necessitates collecting\nlarge-scale datasets. This is expensive, and guaranteeing the quality of the\nlabels is a major challenge. In this paper, we investigate efficient annotation\nstrategies for collecting multi-class classification labels for a large\ncollection of images. While methods that exploit learnt models for labeling\nexist, a surprisingly prevalent approach is to query humans for a fixed number\nof labels per datum and aggregate them, which is expensive. Building on prior\nwork on online joint probabilistic modeling of human annotations and\nmachine-generated beliefs, we propose modifications and best practices aimed at\nminimizing human labeling effort. Specifically, we make use of advances in\nself-supervised learning, view annotation as a semi-supervised learning\nproblem, identify and mitigate pitfalls and ablate several key design choices\nto propose effective guidelines for labeling. Our analysis is done in a more\nrealistic simulation that involves querying human labelers, which uncovers\nissues with evaluation using existing worker simulation methods. Simulated\nexperiments on a 125k image subset of the ImageNet100 show that it can be\nannotated to 80% top-1 accuracy with 0.35 annotations per image on average, a\n2.7x and 6.7x improvement over prior work and manual annotation, respectively.\nProject page: https://fidler-lab.github.io/efficient-annotation-cookbook\n", "rewritten_text": "\\boxed{\nCollecting large-scale datasets for computer vision is a costly and challenging task, particularly when it comes to ensuring the quality of labels. This paper explores efficient strategies for annotating multi-class classification labels for a large image collection. While some methods use learned models for labeling, a common approach is to ask humans to provide a fixed number of labels per image and aggregate them, which is expensive. Building on previous work, we propose modifications and best practices to minimize human labeling effort by leveraging advances in self-supervised learning and treating annotation as a semi-supervised learning problem. Our simulated experiments on a 125k image subset of ImageNet100 show that we can achieve 80% top-1 accuracy with an average of 0.35 annotations per image, outperforming prior work and manual annotation by 2.7x and 6.7x, respectively."}, "2206.00923": {"original_text": "  We present a method that achieves state-of-the-art results on challenging\n(few-shot) layout-to-image generation tasks by accurately modeling textures,\nstructures and relationships contained in a complex scene. After compressing\nRGB images into patch tokens, we propose the Transformer with Focal Attention\n(TwFA) for exploring dependencies of object-to-object, object-to-patch and\npatch-to-patch. Compared to existing CNN-based and Transformer-based generation\nmodels that entangled modeling on pixel-level&patch-level and\nobject-level&patch-level respectively, the proposed focal attention predicts\nthe current patch token by only focusing on its highly-related tokens that\nspecified by the spatial layout, thereby achieving disambiguation during\ntraining. Furthermore, the proposed TwFA largely increases the data efficiency\nduring training, therefore we propose the first few-shot complex scene\ngeneration strategy based on the well-trained TwFA. Comprehensive experiments\nshow the superiority of our method, which significantly increases both\nquantitative metrics and qualitative visual realism with respect to\nstate-of-the-art CNN-based and transformer-based methods. Code is available at\nhttps://github.com/JohnDreamer/TwFA.\n", "rewritten_text": "\\boxed{We introduce a novel method that achieves state-of-the-art performance in generating images from layouts, particularly in few-shot scenarios, by effectively capturing textures, structures, and relationships within complex scenes. Our approach involves compressing RGB images into patch tokens and utilizing the Transformer with Focal Attention (TwFA) to model dependencies between objects, patches, and other patches. Unlike existing CNN-based and Transformer-based models that conflate pixel-level, patch-level, and object-level modeling, our focal attention mechanism selectively focuses on highly relevant tokens specified by the spatial layout, enabling disambiguation during training. This approach also significantly improves data efficiency during training, allowing us to propose the first few-shot complex scene generation strategy based on the well-trained TwFA model. Our comprehensive experiments demonstrate the superiority of our method, yielding substantial improvements in both quantitative metrics and qualitative visual realism compared to state-of-the-art CNN-based and transformer-based methods. The code for our approach is available at https://github.com/JohnDreamer/TwFA.}"}, "2204.09841": {"original_text": "  Information from an image occurs over multiple and distinct spatial scales.\nImage pyramid multiresolution representations are a useful data structure for\nimage analysis and manipulation over a spectrum of spatial scales. This paper\nemploys the Gaussian-Laplacian pyramid to treat different spatial frequency\nbands of a texture separately. First, we generate three images corresponding to\nthree levels of the Gaussian-Laplacian pyramid for an input image to capture\nintrinsic details. Then we aggregate features extracted from gray and color\ntexture images using bio-inspired texture descriptors, information-theoretic\nmeasures, gray-level co-occurrence matrix features, and Haralick statistical\nfeatures into a single feature vector. Such an aggregation aims at producing\nfeatures that characterize textures to their maximum extent, unlike employing\neach descriptor separately, which may lose some relevant textural information\nand reduce the classification performance. The experimental results on texture\nand histopathologic image datasets have shown the advantages of the proposed\nmethod compared to state-of-the-art approaches. Such findings emphasize the\nimportance of multiscale image analysis and corroborate that the descriptors\nmentioned above are complementary.\n", "rewritten_text": "\\boxed{Image analysis can be improved by considering multiple spatial scales, which can be achieved through image pyramid multiresolution representations. This approach involves breaking down an image into different frequency bands using the Gaussian-Laplacian pyramid, and then extracting features from each band using various texture descriptors. By combining these features into a single vector, a more comprehensive characterization of textures can be obtained, leading to improved classification performance. Experimental results on texture and histopathologic image datasets demonstrate the effectiveness of this multiscale approach, highlighting the importance of considering multiple scales and the complementary nature of different texture descriptors.}"}, "2207.00744": {"original_text": "  Spatial-Temporal Video Grounding (STVG) is a challenging task which aims to\nlocalize the spatio-temporal tube of the interested object semantically\naccording to a natural language query. Most previous works not only severely\nrely on the anchor boxes extracted by Faster R-CNN, but also simply regard the\nvideo as a series of individual frames, thus lacking their temporal modeling.\nInstead, in this paper, we are the first to propose an anchor-free framework\nfor STVG, called Gaussian Kernel-based Cross Modal Network (GKCMN).\nSpecifically, we utilize the learned Gaussian Kernel-based heatmaps of each\nvideo frame to locate the query-related object. A mixed serial and parallel\nconnection network is further developed to leverage both spatial and temporal\nrelations among frames for better grounding. Experimental results on VidSTG\ndataset demonstrate the effectiveness of our proposed GKCMN.\n", "rewritten_text": "\\boxed{The task of Spatial-Temporal Video Grounding (STVG) involves identifying the spatio-temporal region of an object in a video based on a natural language query. While previous approaches relied heavily on anchor boxes extracted by Faster R-CNN and treated video frames as individual images, our proposed method, Gaussian Kernel-based Cross Modal Network (GKCMN), takes a novel anchor-free approach. We use Gaussian Kernel-based heatmaps to locate the query-related object in each frame and employ a mixed serial and parallel connection network to capture both spatial and temporal relationships between frames. Our experiments on the VidSTG dataset show that GKCMN is an effective solution for STVG.}"}, "2209.00232": {"original_text": "  Capsule networks (CapsNets) aim to parse images into a hierarchy of objects,\nparts, and their relations using a two-step process involving part-whole\ntransformation and hierarchical component routing. However, this hierarchical\nrelationship modeling is computationally expensive, which has limited the wider\nuse of CapsNet despite its potential advantages. The current state of CapsNet\nmodels primarily focuses on comparing their performance with capsule baselines,\nfalling short of achieving the same level of proficiency as deep CNN variants\nin intricate tasks. To address this limitation, we present an efficient\napproach for learning capsules that surpasses canonical baseline models and\neven demonstrates superior performance compared to high-performing convolution\nmodels. Our contribution can be outlined in two aspects: firstly, we introduce\na group of subcapsules onto which an input vector is projected. Subsequently,\nwe present the Hybrid Gromov-Wasserstein framework, which initially quantifies\nthe dissimilarity between the input and the components modeled by the\nsubcapsules, followed by determining their alignment degree through optimal\ntransport. This innovative mechanism capitalizes on new insights into defining\nalignment between the input and subcapsules, based on the similarity of their\nrespective component distributions. This approach enhances CapsNets' capacity\nto learn from intricate, high-dimensional data while retaining their\ninterpretability and hierarchical structure. Our proposed model offers two\ndistinct advantages: (i) its lightweight nature facilitates the application of\ncapsules to more intricate vision tasks, including object detection; (ii) it\noutperforms baseline approaches in these demanding tasks.\n", "rewritten_text": "\\boxed{\nCapsule networks (CapsNets) aim to break down images into a hierarchical structure of objects, parts, and their relationships using a two-stage process. However, this hierarchical modeling is computationally expensive, limiting the widespread adoption of CapsNets despite their potential benefits. Current CapsNet models mainly focus on comparing their performance with capsule baselines, but fail to match the proficiency of deep CNN variants in complex tasks. To overcome this limitation, we propose an efficient approach to learning capsules that outperforms canonical baseline models and even surpasses high-performing convolution models. Our contribution has two key aspects: first, we introduce a set of subcapsules that project an input vector, and second, we present the Hybrid Gromov-Wasserstein framework, which measures the dissimilarity between the input and subcapsules and determines their alignment degree through optimal transport. This innovative mechanism leverages new insights into defining alignment between the input and subcapsules based on the similarity of their component distributions. Our approach enhances CapsNets' ability to learn from complex, high-dimensional data while maintaining their interpretability and hierarchical structure. Our proposed model offers two significant advantages: it is lightweight, making it applicable to more complex vision tasks, including object detection, and it outperforms baseline approaches in these demanding tasks.\n}"}, "2311.12076": {"original_text": "  Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof open-world intelligent systems. Despite the notable advancements in existing\nOOD detection methodologies, our study identifies a significant performance\ndrop under the scarcity of training samples. In this context, we introduce a\nnovel few-shot OOD detection benchmark, carefully constructed to address this\ngap. Our empirical analysis reveals the superiority of ParameterEfficient\nFine-Tuning (PEFT) strategies, such as visual prompt tuning and visual adapter\ntuning, over conventional techniques, including fully fine-tuning and linear\nprobing tuning in the few-shot OOD detection task. Recognizing some crucial\ninformation from the pre-trained model, which is pivotal for OOD detection, may\nbe lost during the fine-tuning process, we propose a method termed\nDomainSpecific and General Knowledge Fusion (DSGF). This approach is designed\nto be compatible with diverse fine-tuning frameworks. Our experiments show that\nthe integration of DSGF significantly enhances the few-shot OOD detection\ncapabilities across various methods and fine-tuning methodologies, including\nfully fine-tuning, visual adapter tuning, and visual prompt tuning. The code\nwill be released.\n", "rewritten_text": "\\boxed{\nEnsuring the reliability of open-world intelligent systems relies heavily on out-of-distribution (OOD) detection. While existing OOD detection methods have made significant progress, they suffer from a substantial performance decline when training samples are scarce. To bridge this gap, we introduce a novel few-shot OOD detection benchmark. Our empirical analysis reveals that ParameterEfficient Fine-Tuning (PEFT) strategies, such as visual prompt tuning and visual adapter tuning, outperform conventional techniques in few-shot OOD detection tasks. However, we recognize that crucial information from pre-trained models may be lost during fine-tuning, which is essential for OOD detection. To address this, we propose the DomainSpecific and General Knowledge Fusion (DSGF) method, compatible with diverse fine-tuning frameworks. Our experiments demonstrate that integrating DSGF significantly enhances few-shot OOD detection capabilities across various methods and fine-tuning methodologies. The code for this approach will be made available.\n}"}, "2104.10117": {"original_text": "  We present a novel deep learning-based framework to generate embedding\nrepresentations of fine-grained emotions that can be used to computationally\ndescribe psychological models of emotions. Our framework integrates a\ncontextualized embedding encoder with a multi-head probing model that enables\nto interpret dynamically learned representations optimized for an emotion\nclassification task. Our model is evaluated on the Empathetic Dialogue dataset\nand shows the state-of-the-art result for classifying 32 emotions. Our layer\nanalysis can derive an emotion graph to depict hierarchical relations among the\nemotions. Our emotion representations can be used to generate an emotion wheel\ndirectly comparable to the one from Plutchik's\\LN model, and also augment the\nvalues of missing emotions in the PAD emotional state model.\n", "rewritten_text": "\\boxed{We introduce a novel deep learning framework that generates embeddings to represent fine-grained emotions, enabling computational descriptions of psychological emotion models. By combining a contextualized encoder with a multi-head probing model, our approach learns interpretable representations optimized for emotion classification tasks. Evaluated on the Empathetic Dialogue dataset, our model achieves state-of-the-art results in classifying 32 emotions. Furthermore, our layer analysis reveals an emotion graph illustrating hierarchical relationships among emotions, and our representations can be used to generate an emotion wheel comparable to Plutchik's model and augment missing emotions in the PAD emotional state model.}"}, "1912.06842": {"original_text": "  The main requisite for fine-grained recognition task is to focus on subtle\ndiscriminative details that make the subordinate classes different from each\nother. We note that existing methods implicitly address this requirement and\nleave it to a data-driven pipeline to figure out what makes a subordinate class\ndifferent from the others. This results in two major limitations: First, the\nnetwork focuses on the most obvious distinctions between classes and overlooks\nmore subtle inter-class variations. Second, the chance of misclassifying a\ngiven sample in any of the negative classes is considered equal, while in fact,\nconfusions generally occur among only the most similar classes. Here, we\npropose to explicitly force the network to find the subtle differences among\nclosely related classes. In this pursuit, we introduce two key novelties that\ncan be easily plugged into existing end-to-end deep learning pipelines. On one\nhand, we introduce diversification block which masks the most salient features\nfor an input to force the network to use more subtle cues for its correct\nclassification. Concurrently, we introduce a gradient-boosting loss function\nthat focuses only on the confusing classes for each sample and therefore moves\nswiftly along the direction on the loss surface that seeks to resolve these\nambiguities. The synergy between these two blocks helps the network to learn\nmore effective feature representations. Comprehensive experiments are performed\non five challenging datasets. Our approach outperforms existing methods using\nsimilar experimental setting on all five datasets.\n", "rewritten_text": "\\boxed{\nFine-grained recognition tasks require focusing on subtle differences between subordinate classes. However, existing methods often overlook these subtleties, leading to two limitations: they focus on obvious distinctions and treat all negative classes as equally likely to be misclassified. To address this, we propose two novel approaches: a diversification block that masks prominent features, forcing the network to use more subtle cues, and a gradient-boosting loss function that targets confusing classes. The combination of these two blocks enables the network to learn more effective feature representations. Our approach outperforms existing methods on five challenging datasets."}, "1310.7447": {"original_text": "  A new method for removing impulse noise from speech in the wavelet transform\ndomain is proposed. The method utilizes the multiresolution property of the\nwavelet transform, which provides finer time resolution at the higher\nfrequencies than the short-time Fourier transform (STFT), to effectively\nidentify and remove impulse noise. It uses two features of speech to\ndiscriminate speech from impulse noise: one is the slow time-varying nature of\nspeech and the other is the Lipschitz regularity of the speech components. On\nthe basis of these features, an algorithm has been developed to identify and\nsuppress wavelet coefficients that correspond to impulse noise. Experiment\nresults show that the new method is able to significantly reduce impulse noise\nwithout degrading the quality of the speech signal or introducing any audible\nartifacts.\n", "rewritten_text": "\\boxed{A novel approach for eliminating impulse noise from speech signals in the wavelet transform domain is introduced. Leveraging the multiresolution property of the wavelet transform, which offers superior time resolution at higher frequencies compared to the short-time Fourier transform, this method effectively detects and removes impulse noise. By exploiting two key characteristics of speech - its slow time-varying nature and Lipschitz regularity of its components - an algorithm is developed to identify and suppress wavelet coefficients associated with impulse noise. Experimental results demonstrate that this new method substantially reduces impulse noise without compromising speech quality or introducing perceptible artifacts.}"}, "2108.06771": {"original_text": "  Quantification of uncertainty in deep-neural-networks (DNN) based image\nregistration algorithms plays a critical role in the deployment of image\nregistration algorithms for clinical applications such as surgical planning,\nintraoperative guidance, and longitudinal monitoring of disease progression or\ntreatment efficacy as well as in research-oriented processing pipelines.\nCurrently available approaches for uncertainty estimation in DNN-based image\nregistration algorithms may result in sub-optimal clinical decision making due\nto potentially inaccurate estimation of the uncertainty of the registration\nstems for the assumed parametric distribution of the registration latent space.\nWe introduce NPBDREG, a fully non-parametric Bayesian framework for uncertainty\nestimation in DNN-based deformable image registration by combining an Adam\noptimizer with stochastic gradient Langevin dynamics (SGLD) to characterize the\nunderlying posterior distribution through posterior sampling. Thus, it has the\npotential to provide uncertainty estimates that are highly correlated with the\npresence of out of distribution data. We demonstrated the added-value of\nNPBDREG, compared to the baseline probabilistic VoxelMorph model (PrVXM), on\nbrain MRI image registration using $390$ image pairs from four publicly\navailable databases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a\nbetter correlation of the predicted uncertainty with out-of-distribution data\n($r>0.95$ vs. $r<0.5$) as well as a 7.3%improvement in the registration\naccuracy (Dice score, $0.74$ vs. $0.69$, $p \\ll 0.01$), and 18% improvement in\nregistration smoothness (percentage of folds in the deformation field, 0.014\nvs. 0.017, $p \\ll 0.01$). Finally, NPBDREG demonstrated a better generalization\ncapability for data corrupted by a mixed structure noise (Dice score of $0.73$\nvs. $0.69$, $p \\ll 0.01$) compared to the baseline PrVXM approach.\n", "rewritten_text": "\\boxed{\nAccurate uncertainty quantification is crucial for the reliable deployment of deep neural network (DNN) based image registration algorithms in clinical applications and research pipelines. Current methods may lead to suboptimal decision making due to inaccurate uncertainty estimation. We propose NPBDREG, a non-parametric Bayesian framework that combines Adam optimization with stochastic gradient Langevin dynamics to estimate uncertainty in DNN-based deformable image registration. NPBDREG outperforms the baseline probabilistic VoxelMorph model in brain MRI image registration, demonstrating better correlation with out-of-distribution data, improved registration accuracy and smoothness, and enhanced generalization capability in the presence of mixed structure noise."}, "2308.13678": {"original_text": "  Reconstructing and tracking deformable surface with little or no texture has\nposed long-standing challenges. Fundamentally, the challenges stem from\ntextureless surfaces lacking features for establishing cross-image\ncorrespondences. In this work, we present a novel type of markers to\nproactively enrich the object's surface features, and thereby ease the 3D\nsurface reconstruction and correspondence tracking. Our markers are made of\nfluorescent dyes, visible only under the ultraviolet (UV) light and invisible\nunder regular lighting condition. Leveraging the markers, we design a\nmulti-camera system that captures surface deformation under the UV light and\nthe visible light in a time multiplexing fashion. Under the UV light, markers\non the object emerge to enrich its surface texture, allowing high-quality 3D\nshape reconstruction and tracking. Under the visible light, markers become\ninvisible, allowing us to capture the object's original untouched appearance.\nWe perform experiments on various challenging scenes, including hand gestures,\nfacial expressions, waving cloth, and hand-object interaction. In all these\ncases, we demonstrate that our system is able to produce robust, high-quality\n3D reconstruction and tracking.\n", "rewritten_text": "\\boxed{Reconstructing and tracking deformable surfaces with minimal or no texture has long been a challenge due to the lack of features for establishing correspondences between images. To address this, we introduce a novel type of marker that enhances the object's surface features, facilitating 3D surface reconstruction and correspondence tracking. These fluorescent markers are only visible under ultraviolet (UV) light and invisible under regular lighting conditions. We design a multi-camera system that captures surface deformation under both UV and visible light, allowing for high-quality 3D shape reconstruction and tracking under UV light and capturing the object's original appearance under visible light. Our system is demonstrated to produce robust, high-quality 3D reconstruction and tracking in various challenging scenarios, including hand gestures, facial expressions, waving cloth, and hand-object interaction.}"}, "2408.12100": {"original_text": "  In recent years Plug-and-Play (PnP) methods have achieved state-of-the-art\nperformance in inverse imaging problems by replacing proximal operators with\ndenoisers. Based on the proximal gradient method, some theoretical results of\nPnP have appeared, where appropriate step size is crucial for convergence\nanalysis. However, in practical applications, applying PnP methods with\ntheoretically guaranteed step sizes is difficult, and these algorithms are\nlimited to Gaussian noise. In this paper,from a perspective of split convex\nfeasibility problems (SCFP), an adaptive PnP algorithm with Projected Landweber\nOperator (PnP-PLO) is proposed to address these issues. Numerical experiments\non image deblurring, super-resolution, and compressed sensing MRI experiments\nillustrate that PnP-PLO with theoretical guarantees outperforms\nstate-of-the-art methods such as RED and RED-PRO.\n", "rewritten_text": "\\boxed{Recent advances in Plug-and-Play (PnP) methods have led to state-of-the-art performance in inverse imaging problems by leveraging denoisers in place of proximal operators. While theoretical results have been established for PnP methods based on the proximal gradient method, their practical application is hindered by the need for theoretically justified step sizes, which are often limited to Gaussian noise. To overcome these limitations, this paper proposes an adaptive PnP algorithm, PnP-PLO, grounded in split convex feasibility problems (SCFP). Numerical experiments demonstrate that PnP-PLO outperforms cutting-edge methods, including RED and RED-PRO, in image deblurring, super-resolution, and compressed sensing MRI, while providing theoretical guarantees.}"}, "2210.13077": {"original_text": "  Novel-view synthesis (NVS) can be tackled through different approaches,\ndepending on the general setting: a single source image to a short video\nsequence, exact or noisy camera pose information, 3D-based information such as\npoint clouds etc. The most challenging scenario, the one where we stand in this\nwork, only considers a unique source image to generate a novel one from another\nviewpoint. However, in such a tricky situation, the latest learning-based\nsolutions often struggle to integrate the camera viewpoint transformation.\nIndeed, the extrinsic information is often passed as-is, through a\nlow-dimensional vector. It might even occur that such a camera pose, when\nparametrized as Euler angles, is quantized through a one-hot representation.\nThis vanilla encoding choice prevents the learnt architecture from inferring\nnovel views on a continuous basis (from a camera pose perspective). We claim it\nexists an elegant way to better encode relative camera pose, by leveraging\n3D-related concepts such as the epipolar constraint. We, therefore, introduce\nan innovative method that encodes the viewpoint transformation as a 2D feature\nimage. Such a camera encoding strategy gives meaningful insights to the network\nregarding how the camera has moved in space between the two views. By encoding\nthe camera pose information as a finite number of coloured epipolar lines, we\ndemonstrate through our experiments that our strategy outperforms vanilla\nencoding.\n", "rewritten_text": "\\boxed{\nNovel-view synthesis (NVS) can be approached in various ways, depending on the specific scenario. The most challenging scenario involves generating a novel image from a single source image, without any additional information. Current learning-based solutions often struggle to incorporate camera viewpoint transformations in this scenario. The typical approach is to represent camera pose as a low-dimensional vector, which can be limiting. We propose a novel method that encodes camera pose as a 2D feature image, using 3D concepts like the epipolar constraint. This approach provides the network with meaningful insights into camera movement between views, and our experiments show that it outperforms traditional encoding methods."}, "2106.00184": {"original_text": "  Encouraging progress in few-shot semantic segmentation has been made by\nleveraging features learned upon base classes with sufficient training data to\nrepresent novel classes with few-shot examples. However, this feature sharing\nmechanism inevitably causes semantic aliasing between novel classes when they\nhave similar compositions of semantic concepts. In this paper, we reformulate\nfew-shot segmentation as a semantic reconstruction problem, and convert base\nclass features into a series of basis vectors which span a class-level semantic\nspace for novel class reconstruction. By introducing contrastive loss, we\nmaximize the orthogonality of basis vectors while minimizing semantic aliasing\nbetween classes. Within the reconstructed representation space, we further\nsuppress interference from other classes by projecting query features to the\nsupport vector for precise semantic activation. Our proposed approach, referred\nto as anti-aliasing semantic reconstruction (ASR), provides a systematic yet\ninterpretable solution for few-shot learning problems. Extensive experiments on\nPASCAL VOC and MS COCO datasets show that ASR achieves strong results compared\nwith the prior works.\n", "rewritten_text": "\\boxed{\nRecent advancements in few-shot semantic segmentation have been achieved by utilizing features learned from base classes with sufficient training data to represent novel classes with limited examples. However, this approach can lead to semantic confusion between novel classes that share similar semantic concepts. To address this, we propose a new framework that reformulates few-shot segmentation as a semantic reconstruction problem. We convert base class features into a set of basis vectors that span a class-level semantic space, enabling the reconstruction of novel classes. By introducing a contrastive loss function, we ensure the orthogonality of these basis vectors, thereby minimizing semantic confusion between classes. Additionally, we project query features onto the support vector to precisely activate semantic information, reducing interference from other classes. Our approach, called Anti-Aliasing Semantic Reconstruction (ASR), provides a systematic and interpretable solution for few-shot learning problems. Experimental results on PASCAL VOC and MS COCO datasets demonstrate that ASR outperforms existing methods.\n}"}, "2310.00274": {"original_text": "  Africa has a very low doctor-to-patient ratio. At very busy clinics, doctors\ncould see 30+ patients per day -- a heavy patient burden compared with\ndeveloped countries -- but productivity tools such as clinical automatic speech\nrecognition (ASR) are lacking for these overworked clinicians. However,\nclinical ASR is mature, even ubiquitous, in developed nations, and\nclinician-reported performance of commercial clinical ASR systems is generally\nsatisfactory. Furthermore, the recent performance of general domain ASR is\napproaching human accuracy. However, several gaps exist. Several publications\nhave highlighted racial bias with speech-to-text algorithms and performance on\nminority accents lags significantly. To our knowledge, there is no publicly\navailable research or benchmark on accented African clinical ASR, and speech\ndata is non-existent for the majority of African accents. We release\nAfriSpeech, 200hrs of Pan-African English speech, 67,577 clips from 2,463\nunique speakers across 120 indigenous accents from 13 countries for clinical\nand general domain ASR, a benchmark test set, with publicly available\npre-trained models with SOTA performance on the AfriSpeech benchmark.\n", "rewritten_text": "\\boxed{\nAfrica faces a significant challenge with a low doctor-to-patient ratio, resulting in clinicians seeing over 30 patients per day. Despite the availability of clinical automatic speech recognition (ASR) tools in developed countries, these tools are lacking in Africa. Moreover, existing ASR systems have been shown to have racial biases and perform poorly on minority accents. To address this gap, we introduce AfriSpeech, a dataset of 200 hours of Pan-African English speech from 2,463 unique speakers across 120 indigenous accents from 13 countries. This dataset provides a benchmark test set and publicly available pre-trained models with state-of-the-art performance on the AfriSpeech benchmark, aiming to improve clinical and general domain ASR for African accents.\n}"}, "2306.02351": {"original_text": "  We present the RSSOD-Bench dataset for salient object detection (SOD) in\noptical remote sensing imagery. While SOD has achieved success in natural scene\nimages with deep learning, research in SOD for remote sensing imagery (RSSOD)\nis still in its early stages. Existing RSSOD datasets have limitations in terms\nof scale, and scene categories, which make them misaligned with real-world\napplications. To address these shortcomings, we construct the RSSOD-Bench\ndataset, which contains images from four different cities in the USA. The\ndataset provides annotations for various salient object categories, such as\nbuildings, lakes, rivers, highways, bridges, aircraft, ships, athletic fields,\nand more. The salient objects in RSSOD-Bench exhibit large-scale variations,\ncluttered backgrounds, and different seasons. Unlike existing datasets,\nRSSOD-Bench offers uniform distribution across scene categories. We benchmark\n23 different state-of-the-art approaches from both the computer vision and\nremote sensing communities. Experimental results demonstrate that more research\nefforts are required for the RSSOD task.\n", "rewritten_text": "\\boxed{We introduce RSSOD-Bench, a comprehensive dataset for salient object detection in optical remote sensing images. Unlike natural scene images, salient object detection in remote sensing imagery (RSSOD) is still in its infancy, with existing datasets limited by their small scale and narrow scene categories. To overcome these limitations, our RSSOD-Bench dataset features images from four US cities, annotated with various salient object categories, including buildings, water bodies, infrastructure, vehicles, and more. The dataset showcases large-scale variations, cluttered backgrounds, and seasonal changes, with a uniform distribution across scene categories. We evaluate 23 state-of-the-art approaches from both computer vision and remote sensing fields, highlighting the need for further research in RSSOD.}"}, "2303.12946": {"original_text": "  We have made a dataset of camouflage object detection mainly for complex\nseabed scenes, and named it UnderWater RGB&Sonar,or UW-RS for short. The UW-RS\ndataset contains a total of 1972 image data. The dataset mainly consists of two\nparts, namely underwater optical data part (UW-R dataset) and underwater sonar\ndata part (UW-S dataset).\n", "rewritten_text": "\\boxed{We have created a comprehensive dataset, dubbed UnderWater RGB&Sonar (UW-RS), specifically designed for detecting camouflaged objects in complex seabed scenes. This dataset comprises 1972 image data, divided into two primary components: the underwater optical data part (UW-R dataset) and the underwater sonar data part (UW-S dataset).}"}, "2308.02738": {"original_text": "  Recently, visual-language learning has shown great potential in enhancing\nvisual-based person re-identification (ReID). Existing visual-language\nlearning-based ReID methods often focus on whole-body scale image-text feature\nalignment, while neglecting supervisions on fine-grained part features. This\nchoice simplifies the learning process but cannot guarantee within-part feature\nsemantic consistency thus hindering the final performance. Therefore, we\npropose to enhance fine-grained visual features with part-informed language\nsupervision for ReID tasks. The proposed method, named Part-Informed\nVisual-language Learning ($\\pi$-VL), suggests that (i) a human parsing-guided\nprompt tuning strategy and (ii) a hierarchical fusion-based visual-language\nalignment paradigm play essential roles in ensuring within-part feature\nsemantic consistency. Specifically, we combine both identity labels and parsing\nmaps to constitute pixel-level text prompts and fuse multi-stage visual\nfeatures with a light-weight auxiliary head to perform fine-grained image-text\nalignment. As a plug-and-play and inference-free solution, our $\\pi$-VL\nachieves substantial improvements over previous state-of-the-arts on four\ncommon-used ReID benchmarks, especially reporting 90.3% Rank-1 and 76.5% mAP\nfor the most challenging MSMT17 database without bells and whistles.\n", "rewritten_text": "\\boxed{Recent advancements in visual-language learning have shown promise in improving person re-identification (ReID) tasks. However, existing methods focus on aligning whole-body image and text features, overlooking the importance of fine-grained part features. To address this, we propose the Part-Informed Visual-language Learning ($\\pi$-VL) method, which enhances fine-grained visual features with part-informed language supervision. Our approach involves a human parsing-guided prompt tuning strategy and a hierarchical fusion-based visual-language alignment paradigm to ensure within-part feature semantic consistency. By combining identity labels and parsing maps to create pixel-level text prompts and fusing multi-stage visual features, our method achieves significant improvements over previous state-of-the-arts on four ReID benchmarks, with notable results on the challenging MSMT17 database.}"}, "1812.05785": {"original_text": "  It is prohibitively expensive to annotate a large-scale video-based person\nre-identification (re-ID) dataset, which makes fully supervised methods\ninapplicable to real-world deployment. How to maximally reduce the annotation\ncost while retaining the re-ID performance becomes an interesting problem. In\nthis paper, we address this problem by integrating an active learning scheme\ninto a deep learning framework. Noticing that the truly matched tracklet-pairs,\nalso denoted as true positives (TP), are the most informative samples for our\nre-ID model, we propose a sampling criterion to choose the most TP-likely\ntracklet-pairs for annotation. A view-aware sampling strategy considering\nview-specific biases is designed to facilitate candidate selection, followed by\nan adaptive resampling step to leave out the selected candidates that are\nunnecessary to annotate. Our method learns the re-ID model and updates the\nannotation set iteratively. The re-ID model is supervised by the tracklets'\npesudo labels that are initialized by treating each tracklet as a distinct\nclass. With the gained annotations of the actively selected candidates, the\ntracklets' pesudo labels are updated by label merging and further used to\nre-train our re-ID model. While being simple, the proposed method demonstrates\nits effectiveness on three video-based person re-ID datasets. Experimental\nresults show that less than 3\\% pairwise annotations are needed for our method\nto reach comparable performance with the fully-supervised setting.\n", "rewritten_text": "\\boxed{\nThe high cost of annotating large-scale video-based person re-identification (re-ID) datasets makes fully supervised methods impractical for real-world use. To address this, we propose an active learning approach that integrates with a deep learning framework to minimize annotation costs while maintaining re-ID performance. Our method selects the most informative tracklet-pairs for annotation based on their likelihood of being true positives, considering view-specific biases and adaptively resampling to eliminate unnecessary annotations. Through iterative learning and annotation updates, our approach achieves comparable performance to fully-supervised methods with less than 3% pairwise annotations on three video-based person re-ID datasets.\n}"}, "2202.05457": {"original_text": "  Sentiment Analysis typically refers to using natural language processing,\ntext analysis and computational linguistics to extract affect and emotion based\ninformation from text data. Our work explores how we can effectively use deep\nneural networks in transfer learning and joint dual input learning settings to\neffectively classify sentiments and detect hate speech in Hindi and Bengali\ndata. We start by training Word2Vec word embeddings for Hindi \\textbf{HASOC\ndataset} and Bengali hate speech and then train LSTM and subsequently, employ\nparameter sharing based transfer learning to Bengali sentiment classifiers by\nreusing and fine-tuning the trained weights of Hindi classifiers with both\nclassifier being used as baseline in our study. Finally, we use BiLSTM with\nself attention in joint dual input learning setting where we train a single\nneural network on Hindi and Bengali dataset simultaneously using their\nrespective embeddings.\n", "rewritten_text": "\\boxed{Sentiment analysis, which involves extracting emotional information from text data using natural language processing and computational linguistics, is the focus of our research. Specifically, we explore the effectiveness of deep neural networks in classifying sentiments and detecting hate speech in Hindi and Bengali data using transfer learning and joint dual input learning approaches. Our methodology involves training word embeddings for Hindi and Bengali datasets, followed by the training of LSTM models and the application of parameter sharing-based transfer learning to fine-tune the models. Finally, we employ a BiLSTM with self-attention in a joint dual input learning setting, where a single neural network is trained on both Hindi and Bengali datasets simultaneously.}"}, "1210.0115": {"original_text": "  A framework of demosaicing and superresolution for color filter array (CFA)\nvia residual image reconstruction and sparse representation is presented.Given\nthe intermediate image produced by certain demosaicing and interpolation\ntechnique, a residual image between the final reconstruction image and the\nintermediate image is reconstructed using sparse representation.The final\nreconstruction image has richer edges and details than that of the intermediate\nimage. Specifically, a generic dictionary is learned from a large set of\ncomposite training data composed of intermediate data and residual data. The\nlearned dictionary implies a mapping between the two data. A specific\ndictionary adaptive to the input CFA is learned thereafter. Using the adaptive\ndictionary, the sparse coefficients of intermediate data are computed and\ntransformed to predict residual image. The residual image is added back into\nthe intermediate image to obtain the final reconstruction image. Experimental\nresults demonstrate the state-of-the-art performance in terms of PSNR and\nsubjective visual perception.\n", "rewritten_text": "\\boxed{A novel framework is proposed for demosaicing and superresolution of color filter array (CFA) images, which leverages residual image reconstruction and sparse representation. The approach involves reconstructing a residual image between the final and intermediate images using sparse representation, resulting in a final image with richer edges and details. A generic dictionary is first learned from a large dataset of intermediate and residual images, and then adapted to the input CFA. The adaptive dictionary is used to compute sparse coefficients, predict the residual image, and ultimately obtain the final reconstruction image. Experimental results show that this approach achieves state-of-the-art performance in terms of PSNR and visual quality.}"}, "2406.01300": {"original_text": "  Text-guided image generation enables the creation of visual content from\ntextual descriptions. However, certain visual concepts cannot be effectively\nconveyed through language alone. This has sparked a renewed interest in\nutilizing the CLIP image embedding space for more visually-oriented tasks\nthrough methods such as IP-Adapter. Interestingly, the CLIP image embedding\nspace has been shown to be semantically meaningful, where linear operations\nwithin this space yield semantically meaningful results. Yet, the specific\nmeaning of these operations can vary unpredictably across different images. To\nharness this potential, we introduce pOps, a framework that trains specific\nsemantic operators directly on CLIP image embeddings. Each pOps operator is\nbuilt upon a pretrained Diffusion Prior model. While the Diffusion Prior model\nwas originally trained to map between text embeddings and image embeddings, we\ndemonstrate that it can be tuned to accommodate new input conditions, resulting\nin a diffusion operator. Working directly over image embeddings not only\nimproves our ability to learn semantic operations but also allows us to\ndirectly use a textual CLIP loss as an additional supervision when needed. We\nshow that pOps can be used to learn a variety of photo-inspired operators with\ndistinct semantic meanings, highlighting the semantic diversity and potential\nof our proposed approach.\n", "rewritten_text": "\\boxed{\nText-guided image generation has limitations, as some visual concepts cannot be effectively conveyed through language alone. To overcome this, researchers have turned to the CLIP image embedding space, which has been shown to be semantically meaningful. However, the meaning of operations within this space can vary unpredictably across different images. To harness this potential, we propose pOps, a framework that trains specific semantic operators directly on CLIP image embeddings. Each pOps operator is built upon a pretrained Diffusion Prior model, which can be fine-tuned to accommodate new input conditions. This approach enables learning semantic operations directly over image embeddings, allowing for the use of textual CLIP loss as additional supervision. We demonstrate that pOps can learn a variety of photo-inspired operators with distinct semantic meanings, showcasing the semantic diversity and potential of our approach.\n}"}, "1805.07477": {"original_text": "  Augmenting neural networks with skip connections, as introduced in the\nso-called ResNet architecture, surprised the community by enabling the training\nof networks of more than 1,000 layers with significant performance gains. This\npaper deciphers ResNet by analyzing the effect of skip connections, and puts\nforward new theoretical results on the advantages of identity skip connections\nin neural networks. We prove that the skip connections in the residual blocks\nfacilitate preserving the norm of the gradient, and lead to stable\nback-propagation, which is desirable from optimization perspective. We also\nshow that, perhaps surprisingly, as more residual blocks are stacked, the\nnorm-preservation of the network is enhanced. Our theoretical arguments are\nsupported by extensive empirical evidence. Can we push for extra\nnorm-preservation? We answer this question by proposing an efficient method to\nregularize the singular values of the convolution operator and making the\nResNet's transition layers extra norm-preserving. Our numerical investigations\ndemonstrate that the learning dynamics and the classification performance of\nResNet can be improved by making it even more norm preserving. Our results and\nthe introduced modification for ResNet, referred to as Procrustes ResNets, can\nbe used as a guide for training deeper networks and can also inspire new deeper\narchitectures.\n", "rewritten_text": "\\boxed{The introduction of skip connections in neural networks, as seen in the ResNet architecture, allowed for the training of extremely deep networks with over 1,000 layers, resulting in significant performance improvements. This paper delves into the effects of skip connections and presents new theoretical findings on the benefits of identity skip connections. We demonstrate that these connections facilitate the preservation of gradient norms, leading to stable back-propagation, which is desirable for optimization. Furthermore, we show that stacking more residual blocks enhances norm preservation. Our theoretical arguments are supported by extensive empirical evidence. We also propose a method to regularize the singular values of the convolution operator, making ResNet's transition layers even more norm-preserving. Our numerical results indicate that this modification, referred to as Procrustes ResNets, can improve the learning dynamics and classification performance of ResNet. Our findings can serve as a guide for training even deeper networks and inspire new architectures.}"}, "2104.08668": {"original_text": "  Communicating new research ideas involves highlighting similarities and\ndifferences with past work. Authors write fluent, often long sections to survey\nthe distinction of a new paper with related work. In this work we model\ngenerating related work sections while being cognisant of the motivation behind\nciting papers. Our content planning model generates a tree of cited papers\nbefore a surface realization model lexicalizes this skeleton. Our model\noutperforms several strong state-of-the-art summarization and multi-document\nsummarization models on generating related work on an ACL Anthology (AA) based\ndataset which we contribute.\n", "rewritten_text": "\\boxed{When presenting new research, it's essential to compare and contrast it with previous studies. Authors typically write detailed sections to distinguish their work from related papers. This study proposes a model that generates these sections by first creating a hierarchical structure of cited papers, taking into account the motivations behind the citations, and then filling in the details. Our approach outperforms existing state-of-the-art summarization models in generating related work sections, as demonstrated on a dataset based on the ACL Anthology.}"}, "1608.08021": {"original_text": "  This paper presents how we can achieve the state-of-the-art accuracy in\nmulti-category object detection task while minimizing the computational cost by\nadapting and combining recent technical innovations. Following the common\npipeline of \"CNN feature extraction + region proposal + RoI classification\", we\nmainly redesign the feature extraction part, since region proposal part is not\ncomputationally expensive and classification part can be efficiently compressed\nwith common techniques like truncated SVD. Our design principle is \"less\nchannels with more layers\" and adoption of some building blocks including\nconcatenated ReLU, Inception, and HyperNet. The designed network is deep and\nthin and trained with the help of batch normalization, residual connections,\nand learning rate scheduling based on plateau detection. We obtained solid\nresults on well-known object detection benchmarks: 83.8% mAP (mean average\nprecision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only\n750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA\nTitan X GPU. Theoretically, our network requires only 12.3% of the\ncomputational cost compared to ResNet-101, the winner on VOC2012.\n", "rewritten_text": "\\boxed{This paper proposes a novel approach to achieve state-of-the-art accuracy in multi-category object detection while minimizing computational cost. By redesigning the feature extraction component of the traditional pipeline, we adopt a \"less channels with more layers\" design principle and incorporate building blocks such as concatenated ReLU, Inception, and HyperNet. Our deep and thin network, trained with batch normalization, residual connections, and learning rate scheduling, achieves solid results on VOC2007 and VOC2012 benchmarks with a significant reduction in computational cost, requiring only 12.3% of the resources needed by ResNet-101.}"}, "2210.05738": {"original_text": "  In this work, we propose to explicitly use the landmarks of prostate to guide\nthe MR-TRUS image registration. We first train a deep neural network to\nautomatically localize a set of meaningful landmarks, and then directly\ngenerate the affine registration matrix from the location of these landmarks.\nFor landmark localization, instead of directly training a network to predict\nthe landmark coordinates, we propose to regress a full-resolution distance map\nof the landmark, which is demonstrated effective in avoiding statistical bias\nto unsatisfactory performance and thus improving performance. We then use the\npredicted landmarks to generate the affine transformation matrix, which\noutperforms the clinicians' manual rigid registration by a significant margin\nin terms of TRE.\n", "rewritten_text": "\\boxed{This study presents a novel approach to MR-TRUS image registration by leveraging prostate landmarks. A deep neural network is trained to automatically detect key landmarks, and then uses these landmarks to directly generate an affine registration matrix. To improve landmark localization, the network is trained to predict a full-resolution distance map, which helps avoid statistical bias and enhances performance. The resulting affine transformation matrix outperforms manual rigid registration by clinicians, achieving a significant reduction in target registration error (TRE).}"}, "2006.08844": {"original_text": "  We tackle the problem of establishing dense pixel-wise correspondences\nbetween a pair of images. In this work, we introduce Dual-Resolution\nCorrespondence Networks (DualRC-Net), to obtain pixel-wise correspondences in a\ncoarse-to-fine manner. DualRC-Net extracts both coarse- and fine- resolution\nfeature maps. The coarse maps are used to produce a full but coarse 4D\ncorrelation tensor, which is then refined by a learnable neighbourhood\nconsensus module. The fine-resolution feature maps are used to obtain the final\ndense correspondences guided by the refined coarse 4D correlation tensor. The\nselected coarse-resolution matching scores allow the fine-resolution features\nto focus only on a limited number of possible matches with high confidence. In\nthis way, DualRC-Net dramatically increases matching reliability and\nlocalisation accuracy, while avoiding to apply the expensive 4D convolution\nkernels on fine-resolution feature maps. We comprehensively evaluate our method\non large-scale public benchmarks including HPatches, InLoc, and Aachen\nDay-Night. It achieves the state-of-the-art results on all of them.\n", "rewritten_text": "\\boxed{This paper addresses the problem of establishing dense pixel-wise correspondences between two images. The proposed Dual-Resolution Correspondence Networks (DualRC-Net) approach involves a coarse-to-fine process, where both coarse and fine-resolution feature maps are extracted. The coarse maps are used to generate a full but coarse 4D correlation tensor, which is then refined by a learnable neighbourhood consensus module. The fine-resolution feature maps are guided by the refined coarse tensor to obtain the final dense correspondences. By focusing on a limited number of high-confidence matches, DualRC-Net improves matching reliability and localization accuracy while avoiding computationally expensive 4D convolution kernels on fine-resolution feature maps. The method is evaluated on large-scale public benchmarks, including HPatches, InLoc, and Aachen Day-Night, and achieves state-of-the-art results on all of them.}"}, "1704.00405": {"original_text": "  As for semantic role labeling (SRL) task, when it comes to utilizing parsing\ninformation, both traditional methods and recent recurrent neural network (RNN)\nbased methods use the feature engineering way. In this paper, we propose Syntax\nAware Long Short Time Memory(SA-LSTM). The structure of SA-LSTM modifies\naccording to dependency parsing information in order to model parsing\ninformation directly in an architecture engineering way instead of feature\nengineering way. We experimentally demonstrate that SA-LSTM gains more\nimprovement from the model architecture. Furthermore, SA-LSTM outperforms the\nstate-of-the-art on CPB 1.0 significantly according to Student t-test\n($p<0.05$).\n", "rewritten_text": "\\boxed{In the context of semantic role labeling, traditional and recent RNN-based methods rely on feature engineering to incorporate parsing information. This paper introduces Syntax Aware Long Short Time Memory (SA-LSTM), a novel architecture that directly integrates dependency parsing information, moving away from feature engineering. Experimental results show that SA-LSTM achieves significant improvements, outperforming the state-of-the-art on CPB 1.0, as confirmed by a Student t-test (p < 0.05).}"}, "2304.08481": {"original_text": "  High-definition (HD) semantic maps are crucial in enabling autonomous\nvehicles to navigate urban environments. The traditional method of creating\noffline HD maps involves labor-intensive manual annotation processes, which are\nnot only costly but also insufficient for timely updates. Recent studies have\nproposed an alternative approach that generates local maps using online sensor\nobservations. However, this approach is limited by the sensor's perception\nrange and its susceptibility to occlusions. In this study, we propose Neural\nMap Prior (NMP), a neural representation of global maps. This representation\nautomatically updates itself and improves the performance of local map\ninference. Specifically, we utilize two approaches to achieve this. Firstly, to\nintegrate a strong map prior into local map inference, we apply\ncross-attention, a mechanism that dynamically identifies correlations between\ncurrent and prior features. Secondly, to update the global neural map prior, we\nutilize a learning-based fusion module that guides the network in fusing\nfeatures from previous traversals. Our experimental results, based on the\nnuScenes dataset, demonstrate that our framework is highly compatible with\nvarious map segmentation and detection architectures. It significantly improves\nmap prediction performance, even in challenging weather conditions and\nsituations with a longer perception range. To the best of our knowledge, this\nis the first learning-based system for creating a global map prior.\n", "rewritten_text": "\\boxed{\nCreating high-definition semantic maps is essential for autonomous vehicles to navigate urban environments. Traditional methods involve manual annotation, which is time-consuming and costly. Recent approaches generate local maps using online sensor observations, but are limited by sensor range and occlusions. This study proposes Neural Map Prior (NMP), a neural representation of global maps that updates itself and improves local map inference. NMP integrates a strong map prior into local map inference using cross-attention and updates the global neural map prior using a learning-based fusion module. Experimental results on the nuScenes dataset show that NMP is compatible with various map segmentation and detection architectures, significantly improving map prediction performance even in challenging weather conditions and longer perception ranges. This is the first learning-based system for creating a global map prior."}, "2202.10108": {"original_text": "  Vision transformers have shown great potential in various computer vision\ntasks owing to their strong capability to model long-range dependency using the\nself-attention mechanism. Nevertheless, they treat an image as a 1D sequence of\nvisual tokens, lacking an intrinsic inductive bias (IB) in modeling local\nvisual structures and dealing with scale variance, which is instead learned\nimplicitly from large-scale training data with longer training schedules. In\nthis paper, we propose a Vision Transformer Advanced by Exploring intrinsic IB\nfrom convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid\nreduction modules to downsample and embed the input image into tokens with rich\nmulti-scale context using multiple convolutions with different dilation rates.\nIn this way, it acquires an intrinsic scale invariance IB and can learn robust\nfeature representation for objects at various scales. Moreover, in each\ntransformer layer, ViTAE has a convolution block parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. The proposed two kinds\nof cells are stacked in both isotropic and multi-stage manners to formulate two\nfamilies of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on\nthe ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and\nAP10K datasets validate the superiority of our models over the baseline\ntransformer models and concurrent works. Besides, we scale up our ViTAE model\nto 644M parameters and obtain the state-of-the-art classification performance,\ni.e., 88.5% Top-1 classification accuracy on ImageNet validation set and the\nbest 91.2% Top-1 accuracy on ImageNet real validation set, without using extra\nprivate data.\n", "rewritten_text": "\\boxed{Vision transformers have shown great potential in computer vision tasks due to their ability to model long-range dependencies using self-attention. However, they lack an intrinsic inductive bias in modeling local visual structures and dealing with scale variance, relying on large-scale training data to learn these aspects implicitly. To address this, we propose ViTAE, a vision transformer that incorporates intrinsic inductive bias from convolutions. ViTAE uses spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context, acquiring scale invariance and robust feature representation for objects at various scales. Additionally, it combines convolution blocks with multi-head self-attention modules to learn local features and global dependencies collaboratively. Our experiments on various datasets demonstrate the superiority of ViTAE over baseline transformer models and concurrent works, achieving state-of-the-art classification performance without using extra private data.}"}, "1809.08371": {"original_text": "  Human pose estimation is an essential yet challenging task in computer\nvision. One of the reasons for this difficulty is that there are many redundant\nregions in the images. In this work, we proposed a convolutional network\narchitecture combined with the novel attention model. We named it attention\nconvolutional neural network (ACNN). ACNN learns to focus on specific regions\nof different input features. It's a multi-stage architecture. Early stages\nfiltrate the \"nothing-regions\", such as background and redundant body parts.\nAnd then, they submit the important regions which contain the joints of the\nhuman body to the following stages to get a more accurate result. What's more,\nit does not require extra manual annotations and self-learning is one of our\nintentions. We separately trained the network because the attention learning\ntask and the pose estimation task are not independent. State-of-the-art\nperformance is obtained on the MPII benchmarks.\n", "rewritten_text": "\\boxed{Estimating human pose in computer vision is a challenging task due to the presence of redundant regions in images. To address this, we propose a novel architecture called Attention Convolutional Neural Network (ACNN), which combines convolutional networks with an attention model. This multi-stage architecture learns to focus on specific regions of input features, filtering out irrelevant areas such as background and redundant body parts in early stages, and submitting important regions containing human joints to later stages for more accurate results. Notably, ACNN does not require manual annotations and is capable of self-learning. By training the network separately for attention learning and pose estimation tasks, we achieve state-of-the-art performance on the MPII benchmarks.}"}, "1310.0302": {"original_text": "  Surface registration is a technique that is used in various areas such as\nobject recognition and 3D model reconstruction. Problem of surface registration\ncan be analyzed as an optimization problem of seeking a rigid motion between\ntwo different views. Genetic algorithms can be used for solving this\noptimization problem, both for obtaining the robust parameter estimation and\nfor its fine-tuning. The main drawback of genetic algorithms is that they are\ntime consuming which makes them unsuitable for online applications. Modern\nacquisition systems enable the implementation of the solutions that would\nimmediately give the information on the rotational angles between the different\nviews, thus reducing the dimension of the optimization problem. The paper gives\nan analysis of the genetic algorithm implemented in the conditions when the\nrotation matrix is known and a comparison of these results with results when\nthis information is not available.\n", "rewritten_text": "\\boxed{Surface registration, a technique used in object recognition and 3D model reconstruction, can be viewed as an optimization problem that seeks a rigid motion between two different views. While genetic algorithms can be employed to solve this problem, providing robust parameter estimation and fine-tuning, they are time-consuming and unsuitable for online applications. However, modern acquisition systems can provide immediate information on rotational angles, reducing the optimization problem's dimension. This paper analyzes the implementation of genetic algorithms when the rotation matrix is known and compares the results with those obtained without this information.}"}, "2008.0194": {"original_text": "  While the progress of machine translation of written text has come far in the\npast several years thanks to the increasing availability of parallel corpora\nand corpora-based training technologies, automatic translation of spoken text\nand dialogues remains challenging even for modern systems. In this paper, we\naim to boost the machine translation quality of conversational texts by\nintroducing a newly constructed Japanese-English business conversation parallel\ncorpus. A detailed analysis of the corpus is provided along with challenging\nexamples for automatic translation. We also experiment with adding the corpus\nin a machine translation training scenario and show how the resulting system\nbenefits from its use.\n", "rewritten_text": "\\boxed{Despite significant advancements in machine translation of written text in recent years, largely due to the growing availability of parallel corpora and corpora-based training technologies, translating spoken text and dialogues remains a difficult task for even the most modern systems. This paper proposes a novel approach to enhance the quality of machine translation for conversational texts by introducing a newly developed Japanese-English business conversation parallel corpus. The paper provides an in-depth analysis of the corpus, highlighting challenging examples for automatic translation, and demonstrates the benefits of incorporating the corpus into a machine translation training scenario.}"}, "2204.08308": {"original_text": "  With the rapid development of multimedia technology, Augmented Reality (AR)\nhas become a promising next-generation mobile platform. The primary theory\nunderlying AR is human visual confusion, which allows users to perceive the\nreal-world scenes and augmented contents (virtual-world scenes) simultaneously\nby superimposing them together. To achieve good Quality of Experience (QoE), it\nis important to understand the interaction between two scenarios, and\nharmoniously display AR contents. However, studies on how this superimposition\nwill influence the human visual attention are lacking. Therefore, in this\npaper, we mainly analyze the interaction effect between background (BG) scenes\nand AR contents, and study the saliency prediction problem in AR. Specifically,\nwe first construct a Saliency in AR Dataset (SARD), which contains 450 BG\nimages, 450 AR images, as well as 1350 superimposed images generated by\nsuperimposing BG and AR images in pair with three mixing levels. A large-scale\neye-tracking experiment among 60 subjects is conducted to collect eye movement\ndata. To better predict the saliency in AR, we propose a vector quantized\nsaliency prediction method and generalize it for AR saliency prediction. For\ncomparison, three benchmark methods are proposed and evaluated together with\nour proposed method on our SARD. Experimental results demonstrate the\nsuperiority of our proposed method on both of the common saliency prediction\nproblem and the AR saliency prediction problem over benchmark methods. Our\ndataset and code are available at: https://github.com/DuanHuiyu/ARSaliency.\n", "rewritten_text": "\\boxed{\nThe rapid growth of multimedia technology has made Augmented Reality (AR) a promising next-generation mobile platform. AR works by superimposing virtual-world scenes onto real-world scenes, relying on human visual confusion. To ensure a good user experience, it's crucial to understand how these two scenarios interact and display AR content harmoniously. However, there's a lack of research on how this superimposition affects human visual attention. This paper addresses this gap by analyzing the interaction between background scenes and AR content, and studying the saliency prediction problem in AR. A dataset of 450 background images, 450 AR images, and 1350 superimposed images was created, and a large-scale eye-tracking experiment was conducted with 60 subjects. A novel vector quantized saliency prediction method was proposed and evaluated, outperforming three benchmark methods. The dataset and code are available online.\n}"}, "2210.16865": {"original_text": "  Explicit decomposition modeling, which involves breaking down complex tasks\ninto more straightforward and often more interpretable sub-tasks, has long been\na central theme in developing robust and interpretable NLU systems. However,\ndespite the many datasets and resources built as part of this effort, the\nmajority have small-scale annotations and limited scope, which is insufficient\nto solve general decomposition tasks. In this paper, we look at large-scale\nintermediate pre-training of decomposition-based transformers using distant\nsupervision from comparable texts, particularly large-scale parallel news. We\nshow that with such intermediate pre-training, developing robust\ndecomposition-based models for a diverse range of tasks becomes more feasible.\nFor example, on semantic parsing, our model, DecompT5, improves 20% to 30% on\ntwo datasets, Overnight and TORQUE, over the baseline language model. We\nfurther use DecompT5 to build a novel decomposition-based QA system named\nDecompEntail, improving over state-of-the-art models, including GPT-3, on both\nHotpotQA and StrategyQA by 8% and 4%, respectively.\n", "rewritten_text": "\\boxed{Breaking down complex tasks into simpler, more understandable sub-tasks has been a key strategy in developing reliable and interpretable natural language understanding (NLU) systems. Despite numerous datasets and resources being built for this purpose, most have limited annotations and scope, making them insufficient for general decomposition tasks. This paper proposes large-scale intermediate pre-training of decomposition-based transformers using distant supervision from comparable texts, such as large-scale parallel news. The results show that this approach enables the development of robust decomposition-based models for a wide range of tasks. For instance, the DecompT5 model improves by 20-30% on two semantic parsing datasets compared to the baseline language model. Additionally, the DecompEntail QA system, built using DecompT5, outperforms state-of-the-art models, including GPT-3, on two QA datasets by 8% and 4%, respectively.}"}, "2103.16364": {"original_text": "  Unsupervised person re-identification (ReID) aims at learning discriminative\nidentity features without annotations. Recently, self-supervised contrastive\nlearning has gained increasing attention for its effectiveness in unsupervised\nrepresentation learning. The main idea of instance contrastive learning is to\nmatch a same instance in different augmented views. However, the relationship\nbetween different instances has not been fully explored in previous contrastive\nmethods, especially for instance-level contrastive loss. To address this issue,\nwe propose Inter-instance Contrastive Encoding (ICE) that leverages\ninter-instance pairwise similarity scores to boost previous class-level\ncontrastive ReID methods. We first use pairwise similarity ranking as one-hot\nhard pseudo labels for hard instance contrast, which aims at reducing\nintra-class variance. Then, we use similarity scores as soft pseudo labels to\nenhance the consistency between augmented and original views, which makes our\nmodel more robust to augmentation perturbations. Experiments on several\nlarge-scale person ReID datasets validate the effectiveness of our proposed\nunsupervised method ICE, which is competitive with even supervised methods.\nCode is made available at https://github.com/chenhao2345/ICE.\n", "rewritten_text": "\\boxed{\nUnsupervised person re-identification (ReID) seeks to learn distinctive identity features without labels. Self-supervised contrastive learning has recently gained popularity for its effectiveness in unsupervised representation learning. While instance contrastive learning matches the same instance across different augmented views, it neglects the relationships between different instances, particularly for instance-level contrastive loss. To address this limitation, we introduce Inter-instance Contrastive Encoding (ICE), which leverages pairwise similarity scores to enhance previous class-level contrastive ReID methods. Our approach uses pairwise similarity ranking as one-hot hard pseudo labels to reduce intra-class variance and similarity scores as soft pseudo labels to improve consistency between augmented and original views, making our model more robust to augmentation perturbations. Experimental results on large-scale person ReID datasets demonstrate the effectiveness of our unsupervised ICE method, which is competitive with supervised methods. The code is available at https://github.com/chenhao2345/ICE.\n}"}, "1607.07604": {"original_text": "  The interpretation and analysis of the wireless capsule endoscopy recording\nis a complex task which requires sophisticated computer aided decision (CAD)\nsystems in order to help physicians with the video screening and, finally, with\nthe diagnosis. Most of the CAD systems in the capsule endoscopy share a common\nsystem design, but use very different image and video representations. As a\nresult, each time a new clinical application of WCE appears, new CAD system has\nto be designed from scratch. This characteristic makes the design of new CAD\nsystems a very time consuming. Therefore, in this paper we introduce a system\nfor small intestine motility characterization, based on Deep Convolutional\nNeural Networks, which avoids the laborious step of designing specific features\nfor individual motility events. Experimental results show the superiority of\nthe learned features over alternative classifiers constructed by using state of\nthe art hand-crafted features. In particular, it reaches a mean classification\naccuracy of 96% for six intestinal motility events, outperforming the other\nclassifiers by a large margin (a 14% relative performance increase).\n", "rewritten_text": "\\boxed{Analyzing wireless capsule endoscopy recordings is a complex task that requires advanced computer-aided decision systems to assist physicians in video screening and diagnosis. While most systems share a similar design, they use different image and video representations, making it time-consuming to design new systems for each new clinical application. This paper proposes a system using Deep Convolutional Neural Networks to characterize small intestine motility, eliminating the need for manual feature design. The results show that the learned features outperform traditional hand-crafted features, achieving a 96% classification accuracy for six intestinal motility events, a 14% improvement over other classifiers.}"}, "2206.12262": {"original_text": "  Microblogs have become a social platform for people to express their emotions\nin real-time, and it is a trend to analyze user emotional tendencies from the\ninformation on Microblogs. The dynamic features of emojis can affect the\nsentiment polarity of microblog texts. Since existing models seldom consider\nthe diversity of emoji sentiment polarity,the paper propose a microblog\nsentiment classification model based on ALBERT-FAET. We obtain text embedding\nvia ALBERT pretraining model and learn the inter-emoji embedding with an\nattention-based LSTM network. In addition, a fine-grained attention mechanism\nis proposed to capture the word-level interactions between plain text and\nemoji. Finally, we concatenate these features and feed them into a CNN\nclassifier to predict the sentiment labels of the microblogs. To verify the\neffectiveness of the model and the fine-grained attention network, we conduct\ncomparison experiments and ablation experiments. The comparison experiments\nshow that the model outperforms previous methods in three evaluation indicators\n(accuracy, precision, and recall) and the model can significantly improve\nsentiment classification. The ablation experiments show that compared with\nALBERT-AET, the proposed model ALBERT-FAET is better in the metrics, indicating\nthat the fine-grained attention network can understand the diversified\ninformation of emoticons.\n", "rewritten_text": "\\boxed{\nMicroblogging has become a popular platform for people to express their emotions in real-time, and analyzing user emotional tendencies from microblog data is a growing trend. The sentiment of microblog texts can be influenced by the dynamic features of emojis. However, existing models often overlook the diverse sentiment polarities of emojis. To address this, we propose a microblog sentiment classification model based on ALBERT-FAET, which combines ALBERT pretraining with an attention-based LSTM network to learn inter-emoji embeddings. Additionally, we introduce a fine-grained attention mechanism to capture word-level interactions between plain text and emojis. The features are then concatenated and fed into a CNN classifier to predict sentiment labels. Experimental results show that our model outperforms previous methods in terms of accuracy, precision, and recall, and significantly improves sentiment classification. Furthermore, ablation experiments demonstrate that the fine-grained attention network is effective in understanding the diverse information conveyed by emojis."}, "1910.07481": {"original_text": "  In Machine Translation, considering the document as a whole can help to\nresolve ambiguities and inconsistencies. In this paper, we propose a simple yet\npromising approach to add contextual information in Neural Machine Translation.\nWe present a method to add source context that capture the whole document with\naccurate boundaries, taking every word into account. We provide this additional\ninformation to a Transformer model and study the impact of our method on three\nlanguage pairs. The proposed approach obtains promising results in the\nEnglish-German, English-French and French-English document-level translation\ntasks. We observe interesting cross-sentential behaviors where the model learns\nto use document-level information to improve translation coherence.\n", "rewritten_text": "\\boxed{This paper introduces a novel approach to incorporate contextual information into Neural Machine Translation, enabling the model to consider the entire document and capture accurate boundaries. By providing this additional information to a Transformer model, we achieve promising results in document-level translation tasks for three language pairs, including English-German, English-French, and French-English. Notably, our method enables the model to learn cross-sentential behaviors, improving translation coherence by leveraging document-level information.}"}, "2007.02424": {"original_text": "  Recent advances in unsupervised domain adaptation for semantic segmentation\nhave shown great potentials to relieve the demand of expensive per-pixel\nannotations. However, most existing works address the domain discrepancy by\naligning the data distributions of two domains at a global image level whereas\nthe local consistencies are largely neglected. This paper presents an\ninnovative local contextual-relation consistent domain adaptation (CrCDA)\ntechnique that aims to achieve local-level consistencies during the\nglobal-level alignment. The idea is to take a closer look at region-wise\nfeature representations and align them for local-level consistencies.\nSpecifically, CrCDA learns and enforces the prototypical local\ncontextual-relations explicitly in the feature space of a labelled source\ndomain while transferring them to an unlabelled target domain via\nbackpropagation-based adversarial learning. An adaptive entropy max-min\nadversarial learning scheme is designed to optimally align these hundreds of\nlocal contextual-relations across domain without requiring discriminator or\nextra computation overhead. The proposed CrCDA has been evaluated extensively\nover two challenging domain adaptive segmentation tasks (e.g., GTA5 to\nCityscapes and SYNTHIA to Cityscapes), and experiments demonstrate its superior\nsegmentation performance as compared with state-of-the-art methods.\n", "rewritten_text": "\\boxed{\nRecent breakthroughs in unsupervised domain adaptation for semantic segmentation have shown promising results in reducing the need for expensive pixel-level annotations. However, most existing approaches focus on aligning global image-level data distributions between two domains, overlooking local consistencies. This paper introduces a novel technique called Local Contextual-Relation Consistent Domain Adaptation (CrCDA), which aims to achieve local-level consistencies while aligning global-level distributions. CrCDA examines region-wise feature representations and aligns them to ensure local-level consistencies. It learns and enforces prototypical local contextual-relations in the feature space of a labeled source domain and transfers them to an unlabeled target domain using backpropagation-based adversarial learning. An adaptive entropy max-min adversarial learning scheme is designed to optimally align hundreds of local contextual-relations across domains without requiring a discriminator or extra computational overhead. The proposed CrCDA has been extensively evaluated on two challenging domain adaptive segmentation tasks (GTA5 to Cityscapes and SYNTHIA to Cityscapes) and has demonstrated superior segmentation performance compared to state-of-the-art methods.\n}"}, "1805.10047": {"original_text": "  Neural machine translation (NMT) has a drawback in that can generate only\nhigh-frequency words owing to the computational costs of the softmax function\nin the output layer.\n  In Japanese-English NMT, Japanese predicate conjugation causes an increase in\nvocabulary size. For example, one verb can have as many as 19 surface\nvarieties. In this research, we focus on predicate conjugation for compressing\nthe vocabulary size in Japanese. The vocabulary list is filled with the various\nforms of verbs. We propose methods using predicate conjugation information\nwithout discarding linguistic information. The proposed methods can generate\nlow-frequency words and deal with unknown words. Two methods were considered to\nintroduce conjugation information: the first considers it as a token\n(conjugation token) and the second considers it as an embedded vector\n(conjugation feature).\n  The results using these methods demonstrate that the vocabulary size can be\ncompressed by approximately 86.1% (Tanaka corpus) and the NMT models can output\nthe words not in the training data set. Furthermore, BLEU scores improved by\n0.91 points in Japanese-to-English translation, and 0.32 points in\nEnglish-to-Japanese translation with ASPEC.\n", "rewritten_text": "\\boxed{\nNeural machine translation (NMT) has a limitation in that it can only generate common words due to the high computational costs of the softmax function in the output layer. In Japanese-English NMT, the various conjugations of Japanese predicates significantly increase the vocabulary size, with a single verb having up to 19 different forms. To address this issue, we propose two methods that utilize predicate conjugation information to compress the vocabulary size without losing linguistic information. Our methods enable the generation of rare words and handling of unknown words. The results show that the vocabulary size can be reduced by approximately 86.1% and the NMT models can produce words not present in the training data. Moreover, the BLEU scores improved by 0.91 points for Japanese-to-English translation and 0.32 points for English-to-Japanese translation using the ASPEC dataset."}, "1703.01028": {"original_text": "  Outlier detection and cluster number estimation is an important issue for\nclustering real data. This paper focuses on spectral clustering, a time-tested\nclustering method, and reveals its important properties related to outliers.\nThe highlights of this paper are the following two mathematical observations:\nfirst, spectral clustering's intrinsic property of an outlier cluster\nformation, and second, the singularity of an outlier cluster with a valid\ncluster number. Based on these observations, we designed a function that\nevaluates clustering and outlier detection results. In experiments, we prepared\ntwo scenarios, face clustering in photo album and person re-identification in a\ncamera network. We confirmed that the proposed method detects outliers and\nestimates the number of clusters properly in both problems. Our method\noutperforms state-of-the-art methods in both the 128-dimensional sparse space\nfor face clustering and the 4,096-dimensional non-sparse space for person\nre-identification.\n", "rewritten_text": "\\boxed{This paper explores the importance of outlier detection and cluster number estimation in clustering real data, particularly in spectral clustering. It highlights two key mathematical observations: the natural formation of outlier clusters and the singularity of outlier clusters with a valid cluster number. Based on these findings, a novel function is designed to evaluate clustering and outlier detection results. Experimental results in face clustering and person re-identification scenarios demonstrate the effectiveness of the proposed method in detecting outliers and estimating cluster numbers, outperforming existing state-of-the-art methods in both sparse and non-sparse spaces.}"}, "2402.19122": {"original_text": "  Gait recognition stands as one of the most pivotal remote identification\ntechnologies and progressively expands across research and industry\ncommunities. However, existing gait recognition methods heavily rely on\ntask-specific upstream driven by supervised learning to provide explicit gait\nrepresentations like silhouette sequences, which inevitably introduce expensive\nannotation costs and potential error accumulation. Escaping from this trend,\nthis work explores effective gait representations based on the all-purpose\nknowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a\nsimple yet efficient gait framework, termed BigGait. Specifically, the Gait\nRepresentation Extractor (GRE) within BigGait draws upon design principles from\nestablished gait representations, effectively transforming all-purpose\nknowledge into implicit gait representations without requiring third-party\nsupervision signals. Experiments on CCPG, CAISA-B* and SUSTech1K indicate that\nBigGait significantly outperforms the previous methods in both within-domain\nand cross-domain tasks in most cases, and provides a more practical paradigm\nfor learning the next-generation gait representation. Finally, we delve into\nprospective challenges and promising directions in LVMs-based gait recognition,\naiming to inspire future work in this emerging topic. The source code is\navailable at https://github.com/ShiqiYu/OpenGait.\n", "rewritten_text": "\\boxed{\nGait recognition is a crucial remote identification technology that is gaining traction in research and industry. However, current methods rely heavily on supervised learning, which requires expensive annotation and can lead to error accumulation. This work proposes a novel approach, called BigGait, which leverages task-agnostic Large Vision Models (LVMs) to extract effective gait representations without the need for third-party supervision. The Gait Representation Extractor (GRE) within BigGait transforms all-purpose knowledge into implicit gait representations, outperforming previous methods in both within-domain and cross-domain tasks. This approach provides a more practical paradigm for learning next-generation gait representation. The paper also discusses future challenges and directions in LVMs-based gait recognition, and the source code is available at https://github.com/ShiqiYu/OpenGait.\n}"}, "1808.03959": {"original_text": "  Deep Learning based stereo matching methods have shown great successes and\nachieved top scores across different benchmarks. However, like most data-driven\nmethods, existing deep stereo matching networks suffer from some well-known\ndrawbacks such as requiring large amount of labeled training data, and that\ntheir performances are fundamentally limited by the generalization ability. In\nthis paper, we propose a novel Recurrent Neural Network (RNN) that takes a\ncontinuous (possibly previously unseen) stereo video as input, and directly\npredicts a depth-map at each frame without a pre-training process, and without\nthe need of ground-truth depth-maps as supervision. Thanks to the recurrent\nnature (provided by two convolutional-LSTM blocks), our network is able to\nmemorize and learn from its past experiences, and modify its inner parameters\n(network weights) to adapt to previously unseen or unfamiliar environments.\nThis suggests a remarkable generalization ability of the net, making it\napplicable in an {\\em open world} setting. Our method works robustly with\nchanges in scene content, image statistics, and lighting and season conditions\n{\\em etc}. By extensive experiments, we demonstrate that the proposed method\nseamlessly adapts between different scenarios. Equally important, in terms of\nthe stereo matching accuracy, it outperforms state-of-the-art deep stereo\napproaches on standard benchmark datasets such as KITTI and Middlebury stereo.\n", "rewritten_text": "\\boxed{Deep learning-based stereo matching methods have achieved great success, but they have limitations, including requiring large amounts of labeled training data and being restricted by their generalization ability. This paper proposes a novel Recurrent Neural Network (RNN) that can take a continuous stereo video as input and predict a depth-map at each frame without pre-training or ground-truth depth-maps. The network's recurrent nature allows it to learn from past experiences and adapt to new environments, demonstrating remarkable generalization ability and robustness to changes in scene content, image statistics, and lighting conditions. Experimental results show that the proposed method outperforms state-of-the-art deep stereo approaches on standard benchmark datasets.}"}, "2206.12571": {"original_text": "  This competition focus on Urban-Sense Segmentation based on the vehicle\ncamera view. Class highly unbalanced Urban-Sense images dataset challenge the\nexisting solutions and further studies. Deep Conventional neural network-based\nsemantic segmentation methods such as encoder-decoder architecture and\nmulti-scale and pyramid-based approaches become flexible solutions applicable\nto real-world applications. In this competition, we mainly review the\nliterature and conduct experiments on transformer-driven methods especially\nSegFormer, to achieve an optimal trade-off between performance and efficiency.\nFor example, SegFormer-B0 achieved 74.6% mIoU with the smallest FLOPS, 15.6G,\nand the largest model, SegFormer- B5 archived 80.2% mIoU. According to multiple\nfactors, including individual case failure analysis, individual class\nperformance, training pressure and efficiency estimation, the final candidate\nmodel for the competition is SegFormer- B2 with 50.6 GFLOPS and 78.5% mIoU\nevaluated on the testing set. Checkout our code implementation at\nhttps://vmv.re/cv3315.\n", "rewritten_text": "\\boxed{\nThis competition revolves around Urban-Sense Segmentation, which involves segmenting images taken from a vehicle's camera view. The highly imbalanced Urban-Sense images dataset poses a significant challenge to existing solutions, necessitating further research. Conventional deep neural network-based semantic segmentation methods, such as encoder-decoder architectures and multi-scale and pyramid-based approaches, offer flexible solutions for real-world applications. This study focuses on transformer-driven methods, particularly SegFormer, to achieve an optimal balance between performance and efficiency. The results show that SegFormer-B0 achieves 74.6% mean Intersection over Union (mIoU) with the lowest FLOPS (15.6G), while SegFormer-B5 achieves 80.2% mIoU. After considering various factors, including individual case failure analysis, individual class performance, training pressure, and efficiency estimation, SegFormer-B2 is selected as the final candidate model, with 50.6 GFLOPS and 78.5% mIoU on the testing set. The code implementation is available at https://vmv.re/cv3315.\n}"}, "2204.11994": {"original_text": "  Digital pathological analysis is run as the main examination used for cancer\ndiagnosis. Recently, deep learning-driven feature extraction from pathology\nimages is able to detect genetic variations and tumor environment, but few\nstudies focus on differential gene expression in tumor cells. In this paper, we\npropose a self-supervised contrastive learning framework, HistCode, to infer\ndifferential gene expressions from whole slide images (WSIs). We leveraged\ncontrastive learning on large-scale unannotated WSIs to derive slide-level\nhistopathological feature in latent space, and then transfer it to tumor\ndiagnosis and prediction of differentially expressed cancer driver genes. Our\nextensive experiments showed that our method outperformed other\nstate-of-the-art models in tumor diagnosis tasks, and also effectively\npredicted differential gene expressions. Interestingly, we found the higher\nfold-changed genes can be more precisely predicted. To intuitively illustrate\nthe ability to extract informative features from pathological images, we\nspatially visualized the WSIs colored by the attentive scores of image tiles.\nWe found that the tumor and necrosis areas were highly consistent with the\nannotations of experienced pathologists. Moreover, the spatial heatmap\ngenerated by lymphocyte-specific gene expression patterns was also consistent\nwith the manually labeled WSI.\n", "rewritten_text": "\\boxed{\nDigital pathology is a crucial tool for cancer diagnosis, and recent advances in deep learning have enabled the detection of genetic variations and tumor environments from pathology images. However, there is a lack of research on identifying differential gene expression in tumor cells. To address this, we propose a self-supervised contrastive learning framework called HistCode, which can infer differential gene expressions from whole slide images (WSIs). By leveraging contrastive learning on large-scale unannotated WSIs, we can derive slide-level histopathological features in latent space and transfer them to tumor diagnosis and prediction of differentially expressed cancer driver genes. Our experiments demonstrate that HistCode outperforms other state-of-the-art models in tumor diagnosis tasks and accurately predicts differential gene expressions, particularly for genes with higher fold changes. We also visualize the informative features extracted from pathological images, which show high consistency with annotations from experienced pathologists and manually labeled WSIs.\n}"}, "2309.07509": {"original_text": "  Generating realistic talking faces is a complex and widely discussed task\nwith numerous applications. In this paper, we present DiffTalker, a novel model\ndesigned to generate lifelike talking faces through audio and landmark\nco-driving. DiffTalker addresses the challenges associated with directly\napplying diffusion models to audio control, which are traditionally trained on\ntext-image pairs. DiffTalker consists of two agent networks: a\ntransformer-based landmarks completion network for geometric accuracy and a\ndiffusion-based face generation network for texture details. Landmarks play a\npivotal role in establishing a seamless connection between the audio and image\ndomains, facilitating the incorporation of knowledge from pre-trained diffusion\nmodels. This innovative approach efficiently produces articulate-speaking\nfaces. Experimental results showcase DiffTalker's superior performance in\nproducing clear and geometrically accurate talking faces, all without the need\nfor additional alignment between audio and image features.\n", "rewritten_text": "\\boxed{This paper introduces DiffTalker, a novel model that generates realistic talking faces by leveraging audio and landmark co-driving. Unlike traditional diffusion models trained on text-image pairs, DiffTalker addresses the challenges of audio control by using two agent networks: a transformer-based network for geometric accuracy and a diffusion-based network for texture details. The model's innovative approach enables the efficient production of articulate-speaking faces with clear and geometrically accurate results, without requiring additional alignment between audio and image features.}"}, "2210.09071": {"original_text": "  Monocular Depth Estimation (MDE) aims to predict pixel-wise depth given a\nsingle RGB image. For both, the convolutional as well as the recent\nattention-based models, encoder-decoder-based architectures have been found to\nbe useful due to the simultaneous requirement of global context and pixel-level\nresolution. Typically, a skip connection module is used to fuse the encoder and\ndecoder features, which comprises of feature map concatenation followed by a\nconvolution operation. Inspired by the demonstrated benefits of attention in a\nmultitude of computer vision problems, we propose an attention-based fusion of\nencoder and decoder features. We pose MDE as a pixel query refinement problem,\nwhere coarsest-level encoder features are used to initialize pixel-level\nqueries, which are then refined to higher resolutions by the proposed Skip\nAttention Module (SAM). We formulate the prediction problem as ordinal\nregression over the bin centers that discretize the continuous depth range and\nintroduce a Bin Center Predictor (BCP) module that predicts bins at the\ncoarsest level using pixel queries. Apart from the benefit of image adaptive\ndepth binning, the proposed design helps learn improved depth embedding in\ninitial pixel queries via direct supervision from the ground truth. Extensive\nexperiments on the two canonical datasets, NYUV2 and KITTI, show that our\narchitecture outperforms the state-of-the-art by 5.3% and 3.9%, respectively,\nalong with an improved generalization performance by 9.4% on the SUNRGBD\ndataset. Code is available at https://github.com/ashutosh1807/PixelFormer.git.\n", "rewritten_text": "\\boxed{\nMonocular Depth Estimation (MDE) involves predicting pixel-wise depth from a single RGB image. Encoder-decoder architectures have been effective in this task, as they can capture both global context and pixel-level resolution. Typically, a skip connection module is used to combine encoder and decoder features. However, we propose an attention-based fusion approach, which we call the Skip Attention Module (SAM). Our method treats MDE as a pixel query refinement problem, where coarse encoder features are used to initialize pixel-level queries, which are then refined to higher resolutions using SAM. We also introduce a Bin Center Predictor (BCP) module to predict depth bins at the coarsest level using pixel queries. Our approach has several benefits, including image-adaptive depth binning and improved depth embedding in initial pixel queries. Experimental results on NYUV2, KITTI, and SUNRGBD datasets show that our architecture outperforms the state-of-the-art by 5.3%, 3.9%, and 9.4%, respectively. The code is available at https://github.com/ashutosh1807/PixelFormer.git.\n}"}, "2312.11716": {"original_text": "  Demand for efficient onboard object detection is increasing due to its key\nrole in autonomous navigation. However, deploying object detection models such\nas YOLO on resource constrained edge devices is challenging due to the high\ncomputational requirements of such models. In this paper, an compressed object\ndetection model named Squeezed Edge YOLO is examined. This model is compressed\nand optimized to kilobytes of parameters in order to fit onboard such edge\ndevices. To evaluate Squeezed Edge YOLO, two use cases - human and shape\ndetection - are used to show the model accuracy and performance. Moreover, the\nmodel is deployed onboard a GAP8 processor with 8 RISC-V cores and an NVIDIA\nJetson Nano with 4GB of memory. Experimental results show Squeezed Edge YOLO\nmodel size is optimized by a factor of 8x which leads to 76% improvements in\nenergy efficiency and 3.3x faster throughout.\n", "rewritten_text": "\\boxed{The growing need for efficient onboard object detection in autonomous navigation has led to the development of compressed models like Squeezed Edge YOLO, which can fit on resource-constrained edge devices. This optimized model reduces the parameter size to kilobytes, enabling deployment on devices like the GAP8 processor and NVIDIA Jetson Nano. Experimental results demonstrate a significant 8x reduction in model size, resulting in 76% improved energy efficiency and a 3.3x increase in throughput, making it a promising solution for object detection in autonomous systems.}"}, "2409.15939": {"original_text": "  3D shape completion is traditionally solved using supervised training or by\ndistribution learning on complete shape examples. Recently self-supervised\nlearning approaches that do not require any complete 3D shape examples have\ngained more interests. In this paper, we propose a non-adversarial\nself-supervised approach for the shape completion task. Our first finding is\nthat completion problems can be formulated as an involutory function trivially,\nwhich implies a special constraint on the completion function G, such that\nG(G(X)) = X. Our second constraint on self-supervised shape completion relies\non the fact that shape completion becomes easier to solve with correspondences\nand similarly, completion can simplify the correspondences problem. We\nformulate a consistency measure in the canonical space in order to supervise\nthe completion function. We efficiently optimize the completion and\ncorrespondence modules using \"freeze and alternate\" strategy. The overall\napproach performs well for rigid shapes in a category as well as dynamic\nnon-rigid shapes. We ablate our design choices and compare our solution against\nstate-of-the-art methods, showing remarkable accuracy approaching supervised\naccuracy in some cases.\n", "rewritten_text": "\\boxed{\nTraditional methods for 3D shape completion rely on supervised training or learning from complete shape examples. However, recent self-supervised approaches that don't require complete shape examples have gained popularity. This paper proposes a non-adversarial self-supervised approach to shape completion. We discover that completion problems can be formulated as an involutory function, implying a constraint on the completion function. Additionally, we find that shape completion and correspondence problems are interconnected, making it easier to solve one when the other is known. We develop a consistency measure to supervise the completion function and optimize it using a \"freeze and alternate\" strategy. Our approach performs well for both rigid and non-rigid shapes, achieving remarkable accuracy comparable to supervised methods.\n}"}, "1807.10552": {"original_text": "  Histology imaging is an essential diagnosis method to finalize the grade and\nstage of cancer of different tissues, especially for breast cancer diagnosis.\nSpecialists often disagree on the final diagnosis on biopsy tissue due to the\ncomplex morphological variety. Although convolutional neural networks (CNN)\nhave advantages in extracting discriminative features in image classification,\ndirectly training a CNN on high resolution histology images is computationally\ninfeasible currently. Besides, inconsistent discriminative features often\ndistribute over the whole histology image, which incurs challenges in\npatch-based CNN classification method. In this paper, we propose a novel\narchitecture for automatic classification of high resolution histology images.\nFirst, an adapted residual network is employed to explore hierarchical features\nwithout attenuation. Second, we develop a robust deep fusion network to utilize\nthe spatial relationship between patches and learn to correct the prediction\nbias generated from inconsistent discriminative feature distribution. The\nproposed method is evaluated using 10-fold cross-validation on 400 high\nresolution breast histology images with balanced labels and reports 95%\naccuracy on 4-class classification and 98.5% accuracy, 99.6% AUC on 2-class\nclassification (carcinoma and non-carcinoma), which substantially outperforms\nprevious methods and close to pathologist performance.\n", "rewritten_text": "\\boxed{Histology imaging is crucial for diagnosing cancer, particularly breast cancer, but can be challenging due to the complexity of tissue morphology. While convolutional neural networks (CNNs) are effective in image classification, they are not feasible for high-resolution histology images. Moreover, the inconsistent distribution of discriminative features across the image makes patch-based CNN classification difficult. To address this, we propose a novel architecture that combines an adapted residual network to extract hierarchical features and a deep fusion network to leverage spatial relationships between patches and correct prediction biases. Our method achieves 95% accuracy in 4-class classification and 98.5% accuracy with 99.6% AUC in 2-class classification, outperforming previous methods and approaching pathologist-level performance.}"}, "2205.11395": {"original_text": "  Hyperspectral anomalous change detection has been a challenging task for its\nemphasis on the dynamics of small and rare objects against the prevalent\nchanges. In this paper, we have proposed a Multi-Temporal spatial-spectral\nComparison Network for hyperspectral anomalous change detection (MTC-NET). The\nwhole model is a deep siamese network, aiming at learning the prevalent\nspectral difference resulting from the complex imaging conditions from the\nhyperspectral images by contrastive learning. A three-dimensional spatial\nspectral attention module is designed to effectively extract the spatial\nsemantic information and the key spectral differences. Then the gaps between\nthe multi-temporal features are minimized, boosting the alignment of the\nsemantic and spectral features and the suppression of the multi-temporal\nbackground spectral difference. The experiments on the \"Viareggio 2013\"\ndatasets demonstrate the effectiveness of proposed MTC-NET.\n", "rewritten_text": "\\boxed{This paper presents a novel approach to hyperspectral anomalous change detection, which is a challenging task due to the emphasis on detecting small and rare objects amidst prevalent changes. The proposed Multi-Temporal spatial-spectral Comparison Network (MTC-NET) is a deep siamese network that learns to identify spectral differences caused by complex imaging conditions through contrastive learning. A 3D spatial spectral attention module is designed to extract spatial semantic information and key spectral differences, and then minimize the gaps between multi-temporal features to align semantic and spectral features and suppress background spectral differences. The effectiveness of MTC-NET is demonstrated through experiments on the \"Viareggio 2013\" datasets.}"}, "2109.01211": {"original_text": "  Reproducibility has become an intensely debated topic in NLP and ML over\nrecent years, but no commonly accepted way of assessing reproducibility, let\nalone quantifying it, has so far emerged. The assumption has been that wider\nscientific reproducibility terminology and definitions are not applicable to\nNLP/ML, with the result that many different terms and definitions have been\nproposed, some diametrically opposed. In this paper, we test this assumption,\nby taking the standard terminology and definitions from metrology and applying\nthem directly to NLP/ML. We find that we are able to straightforwardly derive a\npractical framework for assessing reproducibility which has the desirable\nproperty of yielding a quantified degree of reproducibility that is comparable\nacross different reproduction studies.\n", "rewritten_text": "\\boxed{The reproducibility of NLP and ML results has sparked intense debate, yet a standardized approach to evaluating and quantifying it remains elusive. The prevailing view is that traditional scientific definitions of reproducibility are inapplicable to NLP/ML, leading to a proliferation of conflicting terms and definitions. This paper challenges this assumption by applying established metrology terminology and definitions to NLP/ML, resulting in a practical framework for assessing reproducibility that yields a comparable, quantifiable measure across different studies.}"}, "2101.1065": {"original_text": "  In machine translation field, in both academia and industry, there is a\ngrowing interest in increasingly powerful systems, using corpora of several\nhundred million to several billion examples. These systems represent the\nstate-of-the-art. Here we defend the idea of developing in parallel <<frugal>>\nbilingual translation systems, trained with relatively small corpora. Based on\nthe observation of a standard human professional translator, we estimate that\nthe corpora should be composed at maximum of a monolingual sub-corpus of 75\nmillion examples for the source language, a second monolingual sub-corpus of 6\nmillion examples for the target language, and an aligned bilingual sub-corpus\nof 6 million bi-examples. A less desirable alternative would be an aligned\nbilingual corpus of 47.5 million bi-examples.\n", "rewritten_text": "\\boxed{In the field of machine translation, while there is a growing trend towards developing powerful systems using massive corpora, we argue for the parallel development of \"frugal\" bilingual translation systems that can be trained on relatively small corpora. Based on observations of professional human translators, we propose that these corpora should consist of a maximum of 75 million monolingual examples for the source language, 6 million monolingual examples for the target language, and 6 million aligned bilingual examples, or alternatively, an aligned bilingual corpus of 47.5 million examples.}"}, "2211.10104": {"original_text": "  Stereo images, containing left and right view images with disparity, are\nutilized in solving low-vision tasks recently, e.g., rain removal and\nsuper-resolution. Stereo image restoration methods usually obtain better\nperformance than monocular methods by learning the disparity between dual views\neither implicitly or explicitly. However, existing stereo rain removal methods\nstill cannot make full use of the complementary information between two views,\nand we find it is because: 1) the rain streaks have more complex distributions\nin directions and densities, which severely damage the complementary\ninformation and pose greater challenges; 2) the disparity estimation is not\naccurate enough due to the imperfect fusion mechanism for the features between\ntwo views. To overcome such limitations, we propose a new \\underline{Stereo}\n\\underline{I}mage \\underline{R}ain \\underline{R}emoval method (StereoIRR) via\nsufficient interaction between two views, which incorporates: 1) a new\nDual-view Mutual Attention (DMA) mechanism which generates mutual attention\nmaps by taking left and right views as key information for each other to\nfacilitate cross-view feature fusion; 2) a long-range and cross-view\ninteraction, which is constructed with basic blocks and dual-view mutual\nattention, can alleviate the adverse effect of rain on complementary\ninformation to help the features of stereo images to get long-range and\ncross-view interaction and fusion. Notably, StereoIRR outperforms other related\nmonocular and stereo image rain removal methods on several datasets. Our codes\nand datasets will be released.\n", "rewritten_text": "\\boxed{\nRecently, stereo images have been used to tackle low-vision tasks such as rain removal and super-resolution. These methods typically outperform monocular methods by leveraging the disparity between the left and right views. However, existing stereo rain removal methods fail to fully utilize the complementary information between the two views due to the complex distribution of rain streaks and inaccurate disparity estimation. To address these limitations, we propose a novel Stereo Image Rain Removal (StereoIRR) method that facilitates sufficient interaction between the two views. Our approach incorporates a Dual-view Mutual Attention (DMA) mechanism, which generates attention maps by treating the left and right views as key information for each other, and a long-range and cross-view interaction module that alleviates the adverse effect of rain on complementary information. Experimental results show that StereoIRR outperforms other monocular and stereo image rain removal methods on several datasets. Our code and datasets will be made publicly available.\n}"}, "1704.07293": {"original_text": "  The accuracy of object detectors and trackers is most commonly evaluated by\nthe Intersection over Union (IoU) criterion. To date, most approaches are\nrestricted to axis-aligned or oriented boxes and, as a consequence, many\ndatasets are only labeled with boxes. Nevertheless, axis-aligned or oriented\nboxes cannot accurately capture an object's shape. To address this, a number of\ndensely segmented datasets has started to emerge in both the object detection\nand the object tracking communities. However, evaluating the accuracy of object\ndetectors and trackers that are restricted to boxes on densely segmented data\nis not straightforward. To close this gap, we introduce the relative\nIntersection over Union (rIoU) accuracy measure. The measure normalizes the IoU\nwith the optimal box for the segmentation to generate an accuracy measure that\nranges between 0 and 1 and allows a more precise measurement of accuracies.\nFurthermore, it enables an efficient and easy way to understand scenes and the\nstrengths and weaknesses of an object detection or tracking approach. We\ndisplay how the new measure can be efficiently calculated and present an\neasy-to-use evaluation framework. The framework is tested on the DAVIS and the\nVOT2016 segmentations and has been made available to the community.\n", "rewritten_text": "\\boxed{Evaluating the accuracy of object detectors and trackers is typically done using the Intersection over Union (IoU) criterion, but this method has limitations when dealing with densely segmented datasets. To address this, a new accuracy measure called relative Intersection over Union (rIoU) is introduced, which normalizes IoU with the optimal box for the segmentation, providing a more precise measurement of accuracy. This new measure enables a better understanding of scenes and the strengths and weaknesses of object detection and tracking approaches, and an easy-to-use evaluation framework is provided, tested on the DAVIS and VOT2016 segmentations.}"}, "2306.02819": {"original_text": "  Natural language understanding (NLU) is an essential branch of natural\nlanguage processing, which relies on representations generated by pre-trained\nlanguage models (PLMs). However, PLMs primarily focus on acquiring\nlexico-semantic information, while they may be unable to adequately handle the\nmeaning of constructions. To address this issue, we introduce construction\ngrammar (CxG), which highlights the pairings of form and meaning, to enrich\nlanguage representation. We adopt usage-based construction grammar as the basis\nof our work, which is highly compatible with statistical models such as PLMs.\nThen a HyCxG framework is proposed to enhance language representation through a\nthree-stage solution. First, all constructions are extracted from sentences via\na slot-constraints approach. As constructions can overlap with each other,\nbringing redundancy and imbalance, we formulate the conditional max coverage\nproblem for selecting the discriminative constructions. Finally, we propose a\nrelational hypergraph attention network to acquire representation from\nconstructional information by capturing high-order word interactions among\nconstructions. Extensive experiments demonstrate the superiority of the\nproposed model on a variety of NLU tasks.\n", "rewritten_text": "\\boxed{Natural language understanding (NLU) relies heavily on pre-trained language models (PLMs), but these models often struggle to capture the meaning of constructions. To address this limitation, we incorporate construction grammar (CxG) into language representation, which emphasizes the connection between form and meaning. Our proposed HyCxG framework involves a three-stage process: extracting constructions from sentences using a slot-constraints approach, selecting the most informative constructions using a conditional max coverage problem, and utilizing a relational hypergraph attention network to capture high-order word interactions among constructions. Our experiments demonstrate the effectiveness of our model in various NLU tasks.}"}, "1309.4573": {"original_text": "  This paper is based on an application of smoothing of 3D face images followed\nby feature detection i.e. detecting the nose tip. The present method uses a\nweighted mesh median filtering technique for smoothing. In this present\nsmoothing technique we have built the neighborhood surrounding a particular\npoint in 3D face and replaced that with the weighted value of the surrounding\npoints in 3D face image. After applying the smoothing technique to the 3D face\nimages our experimental results show that we have obtained considerable\nimprovement as compared to the algorithm without smoothing. We have used here\nthe maximum intensity algorithm for detecting the nose-tip and this method\ncorrectly detects the nose-tip in case of any pose i.e. along X, Y, and Z axes.\nThe present technique gave us worked successfully on 535 out of 542 3D face\nimages as compared to the method without smoothing which worked only on 521 3D\nface images out of 542 face images. Thus we have obtained a 98.70% performance\nrate over 96.12% performance rate of the algorithm without smoothing. All the\nexperiments have been performed on the FRAV3D database.\n", "rewritten_text": "\\boxed{\nThis paper presents a novel approach to 3D face image processing, which involves smoothing the images using a weighted mesh median filtering technique, followed by feature detection, specifically the detection of the nose tip. The proposed method constructs a neighborhood around each point in the 3D face and replaces it with a weighted value of the surrounding points. Experimental results show that this approach significantly improves the accuracy of nose-tip detection, particularly in cases where the face is posed along the X, Y, and Z axes. The maximum intensity algorithm is used for nose-tip detection, and the results demonstrate a success rate of 98.70% on the FRAV3D database, outperforming the algorithm without smoothing, which achieved a success rate of 96.12%.\n}"}, "2306.16682": {"original_text": "  Egocentric action anticipation aims to predict the future actions the camera\nwearer will perform from the observation of the past. While predictions about\nthe future should be available before the predicted events take place, most\napproaches do not pay attention to the computational time required to make such\npredictions. As a result, current evaluation schemes assume that predictions\nare available right after the input video is observed, i.e., presuming a\nnegligible runtime, which may lead to overly optimistic evaluations. We propose\na streaming egocentric action evaluation scheme which assumes that predictions\nare performed online and made available only after the model has processed the\ncurrent input segment, which depends on its runtime. To evaluate all models\nconsidering the same prediction horizon, we hence propose that slower models\nshould base their predictions on temporal segments sampled ahead of time. Based\non the observation that model runtime can affect performance in the considered\nstreaming evaluation scenario, we further propose a lightweight action\nanticipation model based on feed-forward 3D CNNs which is optimized using\nknowledge distillation techniques with a novel past-to-future distillation\nloss. Experiments on the three popular datasets EPIC-KITCHENS-55,\nEPIC-KITCHENS-100 and EGTEA Gaze+ show that (i) the proposed evaluation scheme\ninduces a different ranking on state-of-the-art methods as compared to classic\nevaluations, (ii) lightweight approaches tend to outmatch more computationally\nexpensive ones, and (iii) the proposed model based on feed-forward 3D CNNs and\nknowledge distillation outperforms current art in the streaming egocentric\naction anticipation scenario.\n", "rewritten_text": "\\boxed{\nPredicting future actions from past observations is a key goal in egocentric action anticipation. However, most approaches overlook the computational time required to make these predictions, leading to overly optimistic evaluations. To address this, we propose a streaming evaluation scheme that considers the model's runtime and requires slower models to base their predictions on temporal segments sampled ahead of time. We also introduce a lightweight action anticipation model using feed-forward 3D CNNs and knowledge distillation, which outperforms state-of-the-art methods in the streaming scenario. Our experiments on three popular datasets show that the proposed evaluation scheme and model lead to a more accurate ranking of state-of-the-art methods and highlight the importance of considering computational time in egocentric action anticipation."}, "1508.04458": {"original_text": "  Three-dimensional x-ray CT image reconstruction in baggage scanning in\nsecurity applications is an important research field. The variety of materials\nto be reconstructed is broader than medical x-ray imaging. Presence of high\nattenuating materials such as metal may cause artifacts if analytical\nreconstruction methods are used. Statistical modeling and the resultant\niterative algorithms are known to reduce these artifacts and present good\nquantitative accuracy in estimates of linear attenuation coefficients. However,\niterative algorithms may require computations in order to achieve\nquantitatively accurate results. For the case of baggage scanning, in order to\nprovide fast accurate inspection throughput, they must be accelerated\ndrastically. There are many approaches proposed in the literature to increase\nspeed of convergence. This paper presents a new method that estimates the\nwavelet coefficients of the images in the discrete wavelet transform domain\ninstead of the image space itself. Initially, surrogate functions are created\naround approximation coefficients only. As the iterations proceed, the wavelet\ntree on which the updates are made is expanded based on a criterion and detail\ncoefficients at each level are updated and the tree is expanded this way. For\nexample, in the smooth regions of the image the detail coefficients are not\nupdated while the coefficients that represent the high-frequency component\naround edges are being updated, thus saving time by focusing computations where\nthey are needed. This approach is implemented on real data from a SureScan (TM)\nx1000 Explosive Detection System and compared to straightforward implementation\nof the unregularized alternating minimization of O'Sullivan and Benac [1].\n", "rewritten_text": "\\boxed{\nReconstructing 3D X-ray CT images is crucial in security applications for baggage scanning, where a wide range of materials need to be reconstructed. However, high-density materials like metal can cause artifacts when using analytical methods. Statistical modeling and iterative algorithms can reduce these artifacts and provide accurate estimates of linear attenuation coefficients, but they require significant computational power. To achieve fast and accurate inspection, a new method is proposed, which estimates wavelet coefficients in the discrete wavelet transform domain instead of the image space. This approach focuses computations on areas with high-frequency components, such as edges, and skips smooth regions, thereby saving time. The method is tested on real data from a SureScan x1000 Explosive Detection System and compared to a straightforward implementation of the unregularized alternating minimization algorithm.\n}"}, "2301.00184": {"original_text": "  Most existing text-video retrieval methods focus on cross-modal matching\nbetween the visual content of videos and textual query sentences. However, in\nreal-world scenarios, online videos are often accompanied by relevant text\ninformation such as titles, tags, and even subtitles, which can be utilized to\nmatch textual queries. This insight has motivated us to propose a novel\napproach to text-video retrieval, where we directly generate associated\ncaptions from videos using zero-shot video captioning with knowledge from\nweb-scale pre-trained models (e.g., CLIP and GPT-2). Given the generated\ncaptions, a natural question arises: what benefits do they bring to text-video\nretrieval? To answer this, we introduce Cap4Video, a new framework that\nleverages captions in three ways: i) Input data: video-caption pairs can\naugment the training data. ii) Intermediate feature interaction: we perform\ncross-modal feature interaction between the video and caption to produce\nenhanced video representations. iii) Output score: the Query-Caption matching\nbranch can complement the original Query-Video matching branch for text-video\nretrieval. We conduct comprehensive ablation studies to demonstrate the\neffectiveness of our approach. Without any post-processing, Cap4Video achieves\nstate-of-the-art performance on four standard text-video retrieval benchmarks:\nMSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%). The code is\navailable at https://github.com/whwu95/Cap4Video .\n", "rewritten_text": "\\boxed{\nMost text-video retrieval methods focus on matching visual content with textual queries. However, online videos often come with relevant text information like titles, tags, and subtitles that can be used for matching. This insight led us to propose a novel approach that generates captions from videos using pre-trained models like CLIP and GPT-2. We then use these captions in three ways: to augment training data, to interact with video features, and to complement the original query-video matching. Our approach, called Cap4Video, achieves state-of-the-art performance on four benchmarks without post-processing, demonstrating the effectiveness of leveraging captions in text-video retrieval.\n}"}, "1907.08854": {"original_text": "  Document Grounded Conversations is a task to generate dialogue responses when\nchatting about the content of a given document. Obviously, document knowledge\nplays a critical role in Document Grounded Conversations, while existing\ndialogue models do not exploit this kind of knowledge effectively enough. In\nthis paper, we propose a novel Transformer-based architecture for multi-turn\ndocument grounded conversations. In particular, we devise an Incremental\nTransformer to encode multi-turn utterances along with knowledge in related\ndocuments. Motivated by the human cognitive process, we design a two-pass\ndecoder (Deliberation Decoder) to improve context coherence and knowledge\ncorrectness. Our empirical study on a real-world Document Grounded Dataset\nproves that responses generated by our model significantly outperform\ncompetitive baselines on both context coherence and knowledge relevance.\n", "rewritten_text": "\\boxed{Generating dialogue responses based on a given document is a challenging task, as existing dialogue models fail to effectively utilize document knowledge. To address this, we propose a novel Transformer-based architecture for multi-turn document grounded conversations. Our approach involves an Incremental Transformer to encode utterances and related document knowledge, and a two-pass Deliberation Decoder inspired by human cognition to enhance context coherence and knowledge accuracy. Our model outperforms competitive baselines in terms of context coherence and knowledge relevance, as demonstrated by our empirical study on a real-world dataset.}"}, "2406.05023": {"original_text": "  Generative adversarial networks (GANs) are machine learning models that are\nused to estimate the underlying statistical structure of a given dataset and as\na result can be used for a variety of tasks such as image generation or anomaly\ndetection. Despite their initial simplicity, designing an effective loss\nfunction for training GANs remains challenging, and various loss functions have\nbeen proposed aiming to improve the performance and stability of the generative\nmodels. In this study, loss function design for GANs is presented as an\noptimization problem solved using the genetic programming (GP) approach.\nInitial experiments were carried out using small Deep Convolutional GAN (DCGAN)\nmodel and the MNIST dataset, in order to search experimentally for an improved\nloss function. The functions found were evaluated on CIFAR10, with the best\nfunction, named GANetic loss, showing exceptionally better performance and\nstability compared to the losses commonly used for GAN training. To further\nevalute its general applicability on more challenging problems, GANetic loss\nwas applied for two medical applications: image generation and anomaly\ndetection. Experiments were performed with histopathological, gastrointestinal\nor glaucoma images to evaluate the GANetic loss in medical image generation,\nresulting in improved image quality compared to the baseline models. The\nGANetic Loss used for polyp and glaucoma images showed a strong improvement in\nthe detection of anomalies. In summary, the GANetic loss function was evaluated\non multiple datasets and applications where it consistently outperforms\nalternative loss functions. Moreover, GANetic loss leads to stable training and\nreproducible results, a known weak spot of GANs.\n", "rewritten_text": "\\boxed{\nGenerative Adversarial Networks (GANs) are powerful machine learning models that can uncover the underlying statistical structure of a dataset, enabling tasks like image generation and anomaly detection. However, designing an effective loss function for training GANs remains a significant challenge. To address this, this study proposes a novel approach using genetic programming (GP) to optimize loss function design. The proposed method, called GANetic loss, was experimentally validated using a small Deep Convolutional GAN (DCGAN) model and the MNIST dataset. The results showed that GANetic loss outperforms commonly used loss functions in terms of performance and stability. Furthermore, GANetic loss was applied to two medical applications, image generation and anomaly detection, using histopathological, gastrointestinal, and glaucoma images. The results demonstrated improved image quality and strong improvement in anomaly detection. Overall, the GANetic loss function consistently outperformed alternative loss functions across multiple datasets and applications, leading to stable training and reproducible results, a known weakness of GANs.\n}"}, "2208.01633": {"original_text": "  We present UnrealEgo, i.e., a new large-scale naturalistic dataset for\negocentric 3D human pose estimation. UnrealEgo is based on an advanced concept\nof eyeglasses equipped with two fisheye cameras that can be used in\nunconstrained environments. We design their virtual prototype and attach them\nto 3D human models for stereo view capture. We next generate a large corpus of\nhuman motions. As a consequence, UnrealEgo is the first dataset to provide\nin-the-wild stereo images with the largest variety of motions among existing\negocentric datasets. Furthermore, we propose a new benchmark method with a\nsimple but effective idea of devising a 2D keypoint estimation module for\nstereo inputs to improve 3D human pose estimation. The extensive experiments\nshow that our approach outperforms the previous state-of-the-art methods\nqualitatively and quantitatively. UnrealEgo and our source codes are available\non our project web page.\n", "rewritten_text": "\\boxed{We introduce UnrealEgo, a novel, large-scale dataset for egocentric 3D human pose estimation, featuring a virtual prototype of eyeglasses with fisheye cameras that can capture stereo views in real-world environments. This dataset offers the most diverse range of human motions among existing egocentric datasets. Additionally, we propose a new benchmark method that utilizes a 2D keypoint estimation module for stereo inputs, leading to improved 3D human pose estimation. Our approach outperforms previous state-of-the-art methods, and both the dataset and source codes are available on our project website.}"}, "2011.131": {"original_text": "  To assess the knowledge proficiency of a learner, multiple choice question is\nan efficient and widespread form in standard tests. However, the composition of\nthe multiple choice question, especially the construction of distractors is\nquite challenging. The distractors are required to both incorrect and plausible\nenough to confuse the learners who did not master the knowledge. Currently, the\ndistractors are generated by domain experts which are both expensive and\ntime-consuming. This urges the emergence of automatic distractor generation,\nwhich can benefit various standard tests in a wide range of domains. In this\npaper, we propose a question and answer guided distractor generation (EDGE)\nframework to automate distractor generation. EDGE consists of three major\nmodules: (1) the Reforming Question Module and the Reforming Passage Module\napply gate layers to guarantee the inherent incorrectness of the generated\ndistractors; (2) the Distractor Generator Module applies attention mechanism to\ncontrol the level of plausibility. Experimental results on a large-scale public\ndataset demonstrate that our model significantly outperforms existing models\nand achieves a new state-of-the-art.\n", "rewritten_text": "\\boxed{Evaluating a learner's knowledge proficiency often relies on multiple-choice questions in standardized tests. However, crafting these questions, particularly the distractors, can be a difficult task. Distractors must be both incorrect and plausible enough to confuse learners who lack mastery of the subject matter. Currently, experts in the field create these distractors, a process that is both costly and time-consuming. This has led to a growing need for automated distractor generation, which could benefit a wide range of domains and tests. This paper proposes the EDGE framework, a question and answer guided approach to automating distractor generation. The EDGE framework consists of three main components: two modules that ensure the generated distractors are inherently incorrect, and a third module that uses attention mechanisms to control the level of plausibility. Experimental results on a large public dataset show that our model outperforms existing models and sets a new standard.}"}, "2308.14082": {"original_text": "  Reconstructing interacting hands from monocular images is indispensable in\nAR/VR applications. Most existing solutions rely on the accurate localization\nof each skeleton joint. However, these methods tend to be unreliable due to the\nsevere occlusion and confusing similarity among adjacent hand parts. This also\ndefies human perception because humans can quickly imitate an interaction\npattern without localizing all joints. Our key idea is to first construct a\ntwo-hand interaction prior and recast the interaction reconstruction task as\nthe conditional sampling from the prior. To expand more interaction states, a\nlarge-scale multimodal dataset with physical plausibility is proposed. Then a\nVAE is trained to further condense these interaction patterns as latent codes\nin a prior distribution. When looking for image cues that contribute to\ninteraction prior sampling, we propose the interaction adjacency heatmap (IAH).\nCompared with a joint-wise heatmap for localization, IAH assigns denser visible\nfeatures to those invisible joints. Compared with an all-in-one visible\nheatmap, it provides more fine-grained local interaction information in each\ninteraction region. Finally, the correlations between the extracted features\nand corresponding interaction codes are linked by the ViT module. Comprehensive\nevaluations on benchmark datasets have verified the effectiveness of this\nframework. The code and dataset are publicly available at\nhttps://github.com/binghui-z/InterPrior_pytorch\n", "rewritten_text": "\\boxed{\nReconstructing interacting hands from monocular images is crucial for AR/VR applications, but existing methods that rely on accurate joint localization are often unreliable due to occlusion and similarity among hand parts. Inspired by human perception, which can imitate interactions without localizing all joints, we propose a novel approach that constructs a two-hand interaction prior and recasts the reconstruction task as conditional sampling from this prior. We introduce a large-scale multimodal dataset with physical plausibility and train a VAE to condense interaction patterns into latent codes. Our interaction adjacency heatmap (IAH) extracts image cues that contribute to interaction prior sampling, providing more fine-grained local interaction information than traditional heatmaps. Finally, we link the extracted features to interaction codes using a ViT module. Our framework has been validated through comprehensive evaluations on benchmark datasets, and the code and dataset are publicly available.\n}"}, "1910.10223": {"original_text": "  Deep generative models come with the promise to learn an explainable\nrepresentation for visual objects that allows image sampling, synthesis, and\nselective modification. The main challenge is to learn to properly model the\nindependent latent characteristics of an object, especially its appearance and\npose. We present a novel approach that learns disentangled representations of\nthese characteristics and explains them individually. Training requires only\npairs of images depicting the same object appearance, but no pose annotations.\nWe propose an additional classifier that estimates the minimal amount of\nregularization required to enforce disentanglement. Thus both representations\ntogether can completely explain an image while being independent of each other.\nPrevious methods based on adversarial approaches fail to enforce this\nindependence, while methods based on variational approaches lead to\nuninformative representations. In experiments on diverse object categories, the\napproach successfully recombines pose and appearance to reconstruct and\nretarget novel synthesized images. We achieve significant improvements over\nstate-of-the-art methods which utilize the same level of supervision, and reach\nperformances comparable to those of pose-supervised approaches. However, we can\nhandle the vast body of articulated object classes for which no pose\nmodels/annotations are available.\n", "rewritten_text": "\\boxed{Deep generative models have the potential to learn interpretable representations of visual objects, enabling image generation, synthesis, and targeted editing. However, a major challenge is to separately model an object's independent characteristics, such as its appearance and pose. We introduce a novel approach that learns disentangled representations of these characteristics, explaining each individually. Our method requires only image pairs of the same object appearance, without pose annotations, and uses an additional classifier to estimate the necessary regularization for disentanglement. This approach ensures that both representations are independent and can fully explain an image. Unlike previous adversarial and variational methods, our approach successfully recombines pose and appearance to generate novel images, achieving significant improvements over state-of-the-art methods with similar supervision levels and comparable performance to pose-supervised approaches, while handling a wide range of articulated object classes without pose models or annotations.}"}, "2405.17765": {"original_text": "  Video quality assessment (VQA) is a challenging problem due to the numerous\nfactors that can affect the perceptual quality of a video, \\eg, content\nattractiveness, distortion type, motion pattern, and level. However, annotating\nthe Mean opinion score (MOS) for videos is expensive and time-consuming, which\nlimits the scale of VQA datasets, and poses a significant obstacle for deep\nlearning-based methods. In this paper, we propose a VQA method named PTM-VQA,\nwhich leverages PreTrained Models to transfer knowledge from models pretrained\non various pre-tasks, enabling benefits for VQA from different aspects.\n  Specifically, we extract features of videos from different pretrained models\nwith frozen weights and integrate them to generate representation. Since these\nmodels possess various fields of knowledge and are often trained with labels\nirrelevant to quality, we propose an Intra-Consistency and Inter-Divisibility\n(ICID) loss to impose constraints on features extracted by multiple pretrained\nmodels. The intra-consistency constraint ensures that features extracted by\ndifferent pretrained models are in the same unified quality-aware latent space,\nwhile the inter-divisibility introduces pseudo clusters based on the annotation\nof samples and tries to separate features of samples from different clusters.\nFurthermore, with a constantly growing number of pretrained models, it is\ncrucial to determine which models to use and how to use them. To address this\nproblem, we propose an efficient scheme to select suitable candidates. Models\nwith better clustering performance on VQA datasets are chosen to be our\ncandidates. Extensive experiments demonstrate the effectiveness of the proposed\nmethod.\n", "rewritten_text": "\\boxed{\nEvaluating the quality of videos is a complex task due to various factors that influence how viewers perceive them. However, obtaining human opinions on video quality is a time-consuming and costly process, limiting the size of video quality assessment (VQA) datasets and hindering the development of deep learning-based methods. To address this challenge, we propose PTM-VQA, a method that leverages pre-trained models to transfer knowledge from diverse pre-tasks, benefiting VQA from multiple aspects. Specifically, we extract video features from different pre-trained models and integrate them to generate a representation. To ensure consistency and divisibility among features, we introduce an Intra-Consistency and Inter-Divisibility (ICID) loss. Furthermore, we develop an efficient scheme to select suitable pre-trained models based on their clustering performance on VQA datasets. Our experiments demonstrate the effectiveness of the proposed method.\n}"}, "2306.00246": {"original_text": "  The monetary value of a given piece of real estate, a parcel, is often\nreadily available from a geographic information system. However, for many\napplications, such as insurance and urban planning, it is useful to have\nestimates of property value at much higher spatial resolutions. We propose a\nmethod to estimate the distribution over property value at the pixel level from\nremote sensing imagery. We evaluate on a real-world dataset of a major urban\narea. Our results show that the proposed approaches are capable of generating\nfine-level estimates of property values, significantly improving upon a diverse\ncollection of baseline approaches.\n", "rewritten_text": "\\boxed{Estimating property values at a high spatial resolution is crucial for applications like insurance and urban planning. While geographic information systems can provide the monetary value of a parcel, we propose a method to estimate property value at the pixel level using remote sensing imagery. Our approach, tested on a real-world dataset of a major urban area, outperforms a range of baseline methods, generating accurate fine-level property value estimates.}"}, "1606.02147": {"original_text": "  The ability to perform pixel-wise semantic segmentation in real-time is of\nparamount importance in mobile applications. Recent deep neural networks aimed\nat this task have the disadvantage of requiring a large number of floating\npoint operations and have long run-times that hinder their usability. In this\npaper, we propose a novel deep neural network architecture named ENet\n(efficient neural network), created specifically for tasks requiring low\nlatency operation. ENet is up to 18$\\times$ faster, requires 75$\\times$ less\nFLOPs, has 79$\\times$ less parameters, and provides similar or better accuracy\nto existing models. We have tested it on CamVid, Cityscapes and SUN datasets\nand report on comparisons with existing state-of-the-art methods, and the\ntrade-offs between accuracy and processing time of a network. We present\nperformance measurements of the proposed architecture on embedded systems and\nsuggest possible software improvements that could make ENet even faster.\n", "rewritten_text": "\\boxed{Real-time pixel-wise semantic segmentation is crucial for mobile applications, but current deep neural networks are hindered by their high computational requirements and long run-times. To address this, we introduce ENet, a novel architecture designed for low-latency operation, which achieves up to 18 times faster processing, 75 times fewer floating-point operations, and 79 times fewer parameters while maintaining similar or better accuracy than existing models. We evaluate ENet on various datasets, compare it to state-of-the-art methods, and discuss the trade-offs between accuracy and processing time. Additionally, we provide performance measurements on embedded systems and suggest potential software improvements to further optimize ENet's speed.}"}, "2201.11374": {"original_text": "  In this work, we focus on low-resource dependency parsing for multiple\nlanguages. Several strategies are tailored to enhance performance in\nlow-resource scenarios. While these are well-known to the community, it is not\ntrivial to select the best-performing combination of these strategies for a\nlow-resource language that we are interested in, and not much attention has\nbeen given to measuring the efficacy of these strategies. We experiment with 5\nlow-resource strategies for our ensembled approach on 7 Universal Dependency\n(UD) low-resource languages. Our exhaustive experimentation on these languages\nsupports the effective improvements for languages not covered in pretrained\nmodels. We show a successful application of the ensembled system on a truly\nlow-resource language Sanskrit. The code and data are available at:\nhttps://github.com/Jivnesh/SanDP\n", "rewritten_text": "\\boxed{This study concentrates on dependency parsing for multiple languages with limited resources. To improve performance in such scenarios, several strategies are employed. Although these strategies are well-established, selecting the optimal combination for a specific low-resource language can be challenging, and their effectiveness has not been thoroughly evaluated. We investigate the application of five low-resource strategies to an ensemble approach on seven low-resource languages from the Universal Dependency (UD) dataset. Our comprehensive experiments demonstrate significant improvements for languages not included in pre-trained models. Furthermore, we successfully apply our ensemble system to the truly low-resource language of Sanskrit. The code and data are available at https://github.com/Jivnesh/SanDP.}"}, "2407.03841": {"original_text": "  Large Language Models (LLMs) have showcased remarkable capabilities in\nvarious Natural Language Processing tasks. For automatic open-domain dialogue\nevaluation in particular, LLMs have been seamlessly integrated into evaluation\nframeworks, and together with human evaluation, compose the backbone of most\nevaluations. However, existing evaluation benchmarks often rely on outdated\ndatasets and evaluate aspects like Fluency and Relevance, which fail to\nadequately capture the capabilities and limitations of state-of-the-art chatbot\nmodels.\n  This paper critically examines current evaluation benchmarks, highlighting\nthat the use of older response generators and quality aspects fail to\naccurately reflect modern chatbot capabilities. A small annotation experiment\non a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as\nGPT-4 struggle to detect actual deficiencies in dialogues generated by current\nLLM chatbots.\n", "rewritten_text": "\\boxed{Large Language Models (LLMs) have demonstrated impressive performance in various Natural Language Processing tasks, particularly in automatic open-domain dialogue evaluation. However, current evaluation benchmarks rely on outdated datasets and focus on aspects like Fluency and Relevance, which do not adequately capture the capabilities and limitations of modern chatbot models. This paper argues that existing evaluation benchmarks are flawed, as they use older response generators and quality aspects that fail to accurately reflect modern chatbot capabilities. An annotation experiment on a recent LLM-generated dataset (SODA) reveals that even advanced LLM evaluators like GPT-4 struggle to identify actual deficiencies in dialogues generated by current LLM chatbots.}"}, "1502.00377": {"original_text": "  In order to track the moving objects in long range against occlusion,\ninterruption, and background clutter, this paper proposes a unified approach\nfor global trajectory analysis. Instead of the traditional frame-by-frame\ntracking, our method recovers target trajectories based on a short sequence of\nvideo frames, e.g. $15$ frames. We initially calculate a foreground map at each\nframe, as obtained from a state-of-the-art background model. An attribute graph\nis then extracted from the foreground map, where the graph vertices are image\nprimitives represented by the composite features. With this graph\nrepresentation, we pose trajectory analysis as a joint task of spatial graph\npartitioning and temporal graph matching. The task can be formulated by\nmaximizing a posteriori under the Bayesian framework, in which we integrate the\nspatio-temporal contexts and the appearance models. The probabilistic inference\nis achieved by a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given a\nperoid of observed frames, the algorithm simulates a ergodic and aperiodic\nMarkov Chain, and it visits a sequence of solution states in the joint space of\nspatial graph partitioning and temporal graph matching. In the experiments, our\nmethod is tested on several challenging videos from the public datasets of\nvisual surveillance, and it outperforms the state-of-the-art methods.\n", "rewritten_text": "\\boxed{\nThis paper presents a unified approach for analyzing global trajectories of moving objects in long-range videos, despite occlusion, interruption, and background clutter. Unlike traditional frame-by-frame tracking, our method recovers target trajectories from a short sequence of 15 video frames. We first generate a foreground map for each frame using a state-of-the-art background model, and then extract an attribute graph from the foreground map, where vertices represent image primitives with composite features. We formulate trajectory analysis as a joint task of spatial graph partitioning and temporal graph matching, which is solved by maximizing a posteriori probability under the Bayesian framework. This integrates spatio-temporal contexts and appearance models, and is achieved using a data-driven Markov Chain Monte Carlo (MCMC) algorithm. The algorithm simulates a Markov Chain to visit a sequence of solution states in the joint space of spatial graph partitioning and temporal graph matching. Experimental results on public visual surveillance datasets show that our method outperforms state-of-the-art methods.\n}"}, "1607.02204": {"original_text": "  In this paper we introduce a method to overcome one of the main challenges of\nperson re-identification in multi-camera networks, namely cross-view appearance\nchanges. The proposed solution addresses the extreme variability of person\nappearance in different camera views by exploiting multiple feature\nrepresentations. For each feature, Kernel Canonical Correlation Analysis (KCCA)\nwith different kernels is exploited to learn several projection spaces in which\nthe appearance correlation between samples of the same person observed from\ndifferent cameras is maximized. An iterative logistic regression is finally\nused to select and weigh the contributions of each feature projections and\nperform the matching between the two views. Experimental evaluation shows that\nthe proposed solution obtains comparable performance on VIPeR and PRID 450s\ndatasets and improves on PRID and CUHK01 datasets with respect to the state of\nthe art.\n", "rewritten_text": "\\boxed{This paper presents a novel approach to tackle the challenge of cross-view appearance changes in person re-identification across multiple camera networks. By leveraging multiple feature representations and applying Kernel Canonical Correlation Analysis with diverse kernels, our method learns multiple projection spaces that maximize the correlation between samples of the same individual captured from different cameras. An iterative logistic regression is then employed to select and weight the contributions of each feature projection, enabling effective matching between views. Our experimental results demonstrate that our approach achieves comparable performance on VIPeR and PRID 450s datasets and outperforms the state of the art on PRID and CUHK01 datasets.}"}, "2208.03207": {"original_text": "  Learning with noisy labels (LNL) aims at designing strategies to improve\nmodel performance and generalization by mitigating the effects of model\noverfitting to noisy labels. The key success of LNL lies in identifying as many\nclean samples as possible from massive noisy data, while rectifying the wrongly\nassigned noisy labels. Recent advances employ the predicted label distributions\nof individual samples to perform noise verification and noisy label correction,\neasily giving rise to confirmation bias. To mitigate this issue, we propose\nNeighborhood Collective Estimation, in which the predictive reliability of a\ncandidate sample is re-estimated by contrasting it against its feature-space\nnearest neighbors. Specifically, our method is divided into two steps: 1)\nNeighborhood Collective Noise Verification to separate all training samples\ninto a clean or noisy subset, 2) Neighborhood Collective Label Correction to\nrelabel noisy samples, and then auxiliary techniques are used to assist further\nmodel optimization. Extensive experiments on four commonly used benchmark\ndatasets, i.e., CIFAR-10, CIFAR-100, Clothing-1M and Webvision-1.0, demonstrate\nthat our proposed method considerably outperforms state-of-the-art methods.\n", "rewritten_text": "\\boxed{\nThe goal of Learning with Noisy Labels (LNL) is to develop strategies that improve model performance and generalization by reducing the impact of overfitting to noisy labels. The key to success lies in identifying clean samples from noisy data and correcting mislabeled samples. Recent approaches use predicted label distributions to verify and correct noisy labels, but this can lead to confirmation bias. To address this, we propose Neighborhood Collective Estimation, which reassesses the reliability of a sample by comparing it to its nearest neighbors in feature space. Our method consists of two steps: 1) identifying clean and noisy samples through neighborhood-based noise verification, and 2) correcting noisy labels using neighborhood-based label correction. Additional techniques are then used to further optimize the model. Our experiments on four benchmark datasets (CIFAR-10, CIFAR-100, Clothing-1M, and Webvision-1.0) show that our method significantly outperforms state-of-the-art approaches."}, "2305.03112": {"original_text": "  A surge of interest has emerged in weakly supervised semantic segmentation\ndue to its remarkable efficiency in recent years. Existing approaches based on\ntransformers mainly focus on exploring the affinity matrix to boost CAMs with\nglobal relationships. While in this work, we first perform a scrupulous\nexamination towards the impact of successive affinity matrices and discover\nthat they possess an inclination toward sparsification as the network\napproaches convergence, hence disclosing a manifestation of over-smoothing.\nBesides, it has been observed that enhanced attention maps tend to evince a\nsubstantial amount of extraneous background noise in deeper layers. Drawing\nupon this, we posit a daring conjecture that the undisciplined over-smoothing\nphenomenon introduces a noteworthy quantity of semantically irrelevant\nbackground noise, causing performance degradation. To alleviate this issue, we\npropose a novel perspective that highlights the objects of interest by\ninvestigating the regions of the trait, thereby fostering an extensive\ncomprehension of the successive affinity matrix. Consequently, we suggest an\nadaptive re-activation mechanism (AReAM) that alleviates the issue of\nincomplete attention within the object and the unbounded background noise.\nAReAM accomplishes this by supervising high-level attention with shallow\naffinity matrices, yielding promising results. Exhaustive experiments conducted\non the commonly used dataset manifest that segmentation results can be greatly\nimproved through our proposed AReAM, which imposes restrictions on each\naffinity matrix in deep layers to make it attentive to semantic regions.\n", "rewritten_text": "\\boxed{\nRecently, there has been a growing interest in weakly supervised semantic segmentation due to its efficiency. Most existing approaches using transformers focus on exploring the affinity matrix to improve class activation maps (CAMs) by capturing global relationships. However, we found that successive affinity matrices tend to become sparse as the network converges, indicating over-smoothing. Moreover, enhanced attention maps in deeper layers often contain excessive background noise. We hypothesize that over-smoothing introduces irrelevant background noise, leading to performance degradation. To address this, we propose a novel approach that focuses on the regions of interest by analyzing the trait regions, providing a comprehensive understanding of the successive affinity matrix. We introduce an adaptive re-activation mechanism (AReAM) that alleviates incomplete attention within objects and background noise by supervising high-level attention with shallow affinity matrices. Our experiments on a commonly used dataset show that AReAM significantly improves segmentation results by restricting each affinity matrix in deep layers to focus on semantic regions.\n}"}, "2010.10894": {"original_text": "  This paper aims to enhance the few-shot relation classification especially\nfor sentences that jointly describe multiple relations. Due to the fact that\nsome relations usually keep high co-occurrence in the same context, previous\nfew-shot relation classifiers struggle to distinguish them with few annotated\ninstances. To alleviate the above relation confusion problem, we propose CTEG,\na model equipped with two mechanisms to learn to decouple these easily-confused\nrelations. On the one hand, an Entity-Guided Attention (EGA) mechanism, which\nleverages the syntactic relations and relative positions between each word and\nthe specified entity pair, is introduced to guide the attention to filter out\ninformation causing confusion. On the other hand, a Confusion-Aware Training\n(CAT) method is proposed to explicitly learn to distinguish relations by\nplaying a pushing-away game between classifying a sentence into a true relation\nand its confusing relation. Extensive experiments are conducted on the FewRel\ndataset, and the results show that our proposed model achieves comparable and\neven much better results to strong baselines in terms of accuracy. Furthermore,\nthe ablation test and case study verify the effectiveness of our proposed EGA\nand CAT, especially in addressing the relation confusion problem.\n", "rewritten_text": "\\boxed{This paper proposes a novel model, CTEG, to improve few-shot relation classification, particularly for sentences that describe multiple relations. The model tackles the challenge of distinguishing between highly co-occurring relations with limited annotated data. CTEG features two mechanisms: Entity-Guided Attention (EGA), which uses syntactic relations and entity positions to filter out confusing information, and Confusion-Aware Training (CAT), which learns to distinguish relations by playing a \"pushing-away\" game between true and confusing relations. Experimental results on the FewRel dataset show that CTEG achieves comparable or better accuracy than strong baselines, and ablation tests and case studies confirm the effectiveness of EGA and CAT in addressing relation confusion.}"}, "1903.05396": {"original_text": "  This paper introduces improved methods for sub-event detection in social\nmedia streams, by applying neural sequence models not only on the level of\nindividual posts, but also directly on the stream level. Current approaches to\nidentify sub-events within a given event, such as a goal during a soccer match,\nessentially do not exploit the sequential nature of social media streams. We\naddress this shortcoming by framing the sub-event detection problem in social\nmedia streams as a sequence labeling task and adopt a neural sequence\narchitecture that explicitly accounts for the chronological order of posts.\nSpecifically, we (i) establish a neural baseline that outperforms a graph-based\nstate-of-the-art method for binary sub-event detection (2.7% micro-F1\nimprovement), as well as (ii) demonstrate superiority of a recurrent neural\nnetwork model on the posts sequence level for labeled sub-events (2.4%\nbin-level F1 improvement over non-sequential models).\n", "rewritten_text": "\\boxed{This paper presents enhanced methods for detecting sub-events in social media streams by utilizing neural sequence models that consider both individual posts and the overall stream. Unlike current approaches, which neglect the sequential nature of social media streams, our approach frames sub-event detection as a sequence labeling task and employs a neural sequence architecture that takes into account the chronological order of posts. Our results show that our neural baseline outperforms a state-of-the-art graph-based method for binary sub-event detection, with a 2.7% improvement in micro-F1, and that a recurrent neural network model on the post sequence level achieves a 2.4% improvement in bin-level F1 over non-sequential models for labeled sub-events.}"}, "2212.03496": {"original_text": "  Script event prediction aims to predict the subsequent event given the\ncontext. This requires the capability to infer the correlations between events.\nRecent works have attempted to improve event correlation reasoning by using\npretrained language models and incorporating external knowledge~(e.g.,\ndiscourse relations). Though promising results have been achieved, some\nchallenges still remain. First, the pretrained language models adopted by\ncurrent works ignore event-level knowledge, resulting in an inability to\ncapture the correlations between events well. Second, modeling correlations\nbetween events with discourse relations is limited because it can only capture\nexplicit correlations between events with discourse markers, and cannot capture\nmany implicit correlations. To this end, we propose a novel generative approach\nfor this task, in which a pretrained language model is fine-tuned with an\nevent-centric pretraining objective and predicts the next event within a\ngenerative paradigm. Specifically, we first introduce a novel event-level blank\ninfilling strategy as the learning objective to inject event-level knowledge\ninto the pretrained language model, and then design a likelihood-based\ncontrastive loss for fine-tuning the generative model. Instead of using an\nadditional prediction layer, we perform prediction by using sequence\nlikelihoods generated by the generative model. Our approach models correlations\nbetween events in a soft way without any external knowledge. The\nlikelihood-based prediction eliminates the need to use additional networks to\nmake predictions and is somewhat interpretable since it scores each word in the\nevent. Experimental results on the multi-choice narrative cloze~(MCNC) task\ndemonstrate that our approach achieves better results than other\nstate-of-the-art baselines. Our code will be available at\nhttps://github.com/zhufq00/mcnc.\n", "rewritten_text": "\\boxed{\nPredicting subsequent events in a script requires understanding the correlations between events. While recent studies have made progress using pretrained language models and external knowledge, they still face challenges. Current language models neglect event-level knowledge, and modeling correlations using discourse relations is limited to explicit connections. To address this, we propose a novel generative approach that fine-tunes a pretrained language model with an event-centric objective. Our method uses a blank infilling strategy to inject event-level knowledge and a likelihood-based contrastive loss for fine-tuning. This approach models event correlations softly without external knowledge and makes predictions using sequence likelihoods, eliminating the need for additional networks. Our results on the multi-choice narrative cloze task outperform state-of-the-art baselines, and our code is available at https://github.com/zhufq00/mcnc.\n}"}, "1911.12982": {"original_text": "  Recently, Chinese word segmentation (CWS) methods using neural networks have\nmade impressive progress. Most of them regard the CWS as a sequence labeling\nproblem which construct models based on local features rather than considering\nglobal information of input sequence. In this paper, we cast the CWS as a\nsequence translation problem and propose a novel sequence-to-sequence CWS model\nwith an attention-based encoder-decoder framework. The model captures the\nglobal information from the input and directly outputs the segmented sequence.\nIt can also tackle other NLP tasks with CWS jointly in an end-to-end mode.\nExperiments on Weibo, PKU and MSRA benchmark datasets show that our approach\nhas achieved competitive performances compared with state-of-the-art methods.\nMeanwhile, we successfully applied our proposed model to jointly learning CWS\nand Chinese spelling correction, which demonstrates its applicability of\nmulti-task fusion.\n", "rewritten_text": "\\boxed{Recent advancements in Chinese word segmentation (CWS) using neural networks have shown remarkable progress. However, most existing methods approach CWS as a sequence labeling problem, focusing on local features rather than considering the global context of the input sequence. This paper presents a novel approach, framing CWS as a sequence translation problem and introducing a sequence-to-sequence CWS model with an attention-based encoder-decoder framework. This model captures global information from the input and directly generates the segmented sequence, enabling end-to-end learning of CWS and other NLP tasks. Experimental results on Weibo, PKU, and MSRA benchmark datasets demonstrate competitive performance compared to state-of-the-art methods. Furthermore, our proposed model successfully integrates CWS and Chinese spelling correction, showcasing its potential for multi-task fusion.}"}, "2308.14018": {"original_text": "  Few-shot font generation is challenging, as it needs to capture the\nfine-grained stroke styles from a limited set of reference glyphs, and then\ntransfer to other characters, which are expected to have similar styles.\nHowever, due to the diversity and complexity of Chinese font styles, the\nsynthesized glyphs of existing methods usually exhibit visible artifacts, such\nas missing details and distorted strokes. In this paper, we propose a\nVQGAN-based framework (i.e., VQ-Font) to enhance glyph fidelity through token\nprior refinement and structure-aware enhancement. Specifically, we pre-train a\nVQGAN to encapsulate font token prior within a codebook. Subsequently, VQ-Font\nrefines the synthesized glyphs with the codebook to eliminate the domain gap\nbetween synthesized and real-world strokes. Furthermore, our VQ-Font leverages\nthe inherent design of Chinese characters, where structure components such as\nradicals and character components are combined in specific arrangements, to\nrecalibrate fine-grained styles based on references. This process improves the\nmatching and fusion of styles at the structure level. Both modules collaborate\nto enhance the fidelity of the generated fonts. Experiments on a collected font\ndataset show that our VQ-Font outperforms the competing methods both\nquantitatively and qualitatively, especially in generating challenging styles.\n", "rewritten_text": "\\boxed{\nGenerating fonts from a few examples is a difficult task, as it requires capturing the subtle style variations from a limited set of reference characters and applying them to other characters with similar styles. However, existing methods often produce low-quality glyphs with noticeable flaws, such as missing details and distorted strokes, particularly when dealing with the diverse and complex styles of Chinese fonts. To address this, we propose a novel framework called VQ-Font, which utilizes a VQGAN-based approach to improve the quality of generated glyphs. Our method involves pre-training a VQGAN to learn a font token prior, which is then refined to bridge the gap between synthesized and real-world strokes. Additionally, VQ-Font leverages the structural composition of Chinese characters, where radicals and components are combined in specific ways, to fine-tune styles at the structural level. By combining these two modules, our approach enhances the fidelity of generated fonts. Experimental results on a collected font dataset demonstrate that VQ-Font outperforms competing methods, especially in generating challenging font styles."}, "2403.19836": {"original_text": "  Identifying the targets of hate speech is a crucial step in grasping the\nnature of such speech and, ultimately, in improving the detection of offensive\nposts on online forums. Much harmful content on online platforms uses implicit\nlanguage especially when targeting vulnerable and protected groups such as\nusing stereotypical characteristics instead of explicit target names, making it\nharder to detect and mitigate the language. In this study, we focus on\nidentifying implied targets of hate speech, essential for recognizing subtler\nhate speech and enhancing the detection of harmful content on digital\nplatforms. We define a new task aimed at identifying the targets even when they\nare not explicitly stated. To address that task, we collect and annotate target\nspans in three prominent implicit hate speech datasets: SBIC, DynaHate, and\nIHC. We call the resulting merged collection Implicit-Target-Span. The\ncollection is achieved using an innovative pooling method with matching scores\nbased on human annotations and Large Language Models (LLMs). Our experiments\nindicate that Implicit-Target-Span provides a challenging test bed for target\nspan detection methods.\n", "rewritten_text": "\\boxed{Determining the targets of hate speech is a vital step in understanding its nature and improving online hate speech detection. Harmful online content often employs subtle language, particularly when targeting vulnerable groups, making it difficult to detect. This study focuses on identifying implied targets of hate speech, which is crucial for recognizing subtle hate speech and enhancing harmful content detection on digital platforms. We introduce a new task to identify targets even when they are not explicitly mentioned and create a merged dataset, Implicit-Target-Span, by annotating target spans in three prominent implicit hate speech datasets using a novel pooling method combining human annotations and Large Language Models. Our experiments show that Implicit-Target-Span provides a challenging test bed for target span detection methods.}"}, "2303.05955": {"original_text": "  Remote photoplethysmography (rPPG) technology has drawn increasing attention\nin recent years. It can extract Blood Volume Pulse (BVP) from facial videos,\nmaking many applications like health monitoring and emotional analysis more\naccessible. However, as the BVP signal is easily affected by environmental\nchanges, existing methods struggle to generalize well for unseen domains. In\nthis paper, we systematically address the domain shift problem in the rPPG\nmeasurement task. We show that most domain generalization methods do not work\nwell in this problem, as domain labels are ambiguous in complicated\nenvironmental changes. In light of this, we propose a domain-label-free\napproach called NEuron STructure modeling (NEST). NEST improves the\ngeneralization capacity by maximizing the coverage of feature space during\ntraining, which reduces the chance for under-optimized feature activation\nduring inference. Besides, NEST can also enrich and enhance domain invariant\nfeatures across multi-domain. We create and benchmark a large-scale domain\ngeneralization protocol for the rPPG measurement task. Extensive experiments\nshow that our approach outperforms the state-of-the-art methods on both\ncross-dataset and intra-dataset settings.\n", "rewritten_text": "\\boxed{Remote photoplethysmography (rPPG) technology has gained significant attention in recent years due to its ability to extract Blood Volume Pulse (BVP) signals from facial videos, enabling various applications such as health monitoring and emotional analysis. However, the BVP signal is susceptible to environmental changes, making it challenging for existing methods to generalize well across different domains. This paper tackles the domain shift problem in rPPG measurement by proposing a novel approach called NEuron STructure modeling (NEST), which improves generalization capacity by maximizing feature space coverage during training and enriching domain-invariant features. The proposed method outperforms state-of-the-art techniques in both cross-dataset and intra-dataset settings, as demonstrated by extensive experiments on a large-scale domain generalization protocol for rPPG measurement.}"}, "2401.06957": {"original_text": "  As virtual environments continue to advance, the demand for immersive and\nemotionally engaging experiences has grown. Addressing this demand, we\nintroduce Emotion enabled Virtual avatar mapping using Optimized KnowledgE\ndistillation (EVOKE), a lightweight emotion recognition framework designed for\nthe seamless integration of emotion recognition into 3D avatars within virtual\nenvironments. Our approach leverages knowledge distillation involving\nmulti-label classification on the publicly available DEAP dataset, which covers\nvalence, arousal, and dominance as primary emotional classes. Remarkably, our\ndistilled model, a CNN with only two convolutional layers and 18 times fewer\nparameters than the teacher model, achieves competitive results, boasting an\naccuracy of 87% while demanding far less computational resources. This\nequilibrium between performance and deployability positions our framework as an\nideal choice for virtual environment systems. Furthermore, the multi-label\nclassification outcomes are utilized to map emotions onto custom-designed 3D\navatars.\n", "rewritten_text": "\\boxed{As virtual environments evolve, there is a growing need for immersive and emotionally engaging experiences. To meet this demand, we propose EVOKE, a lightweight emotion recognition framework that seamlessly integrates emotion recognition into 3D avatars within virtual environments. Our approach uses knowledge distillation with multi-label classification on the DEAP dataset, which covers valence, arousal, and dominance as primary emotional classes. Notably, our distilled model, a compact CNN with only two convolutional layers, achieves an accuracy of 87% while requiring significantly fewer computational resources. This balance between performance and deployability makes our framework an ideal choice for virtual environment systems. Moreover, the multi-label classification outcomes are used to map emotions onto custom-designed 3D avatars, enabling a more engaging and realistic experience.}"}, "2108.07638": {"original_text": "  Affective Computing is the study of how computers can recognize, interpret\nand simulate human affects. Sentiment Analysis is a common task inNLP related\nto this topic, but it focuses only on emotion valence (positive, negative,\nneutral). An emerging approach in NLP is Emotion Recognition, which relies on\nfined-grained classification. This research describes an approach to create a\nlexical-based weakly supervised corpus for fine-grained emotion in Portuguese.\nWe evaluated our dataset by fine-tuning a transformer-based language model\n(BERT) and validating it on a Gold Standard annotated validation set. Our\nresults (F1-score=.64) suggest lexical-based weak supervision as an appropriate\nstrategy for initial work in low resourced environment.\n", "rewritten_text": "\\boxed{The field of Affective Computing explores how computers can identify, understand, and mimic human emotions. While Sentiment Analysis, a related task in Natural Language Processing (NLP), only examines the positive, negative, or neutral tone of emotions, Emotion Recognition is a more nuanced approach that categorizes emotions with greater precision. This research presents a method for creating a weakly supervised corpus for fine-grained emotion recognition in Portuguese, leveraging lexical-based techniques. The approach was evaluated by fine-tuning a transformer-based language model (BERT) and validated using a Gold Standard annotated dataset, yielding promising results (F1-score = 0.64) that support the use of lexical-based weak supervision in resource-constrained environments.}"}, "2408.01653": {"original_text": "  We introduce Multi-Cylindrical Panoramic Depth Estimation (MCPDepth), a\ntwo-stage framework for omnidirectional depth estimation via stereo matching\nbetween multiple cylindrical panoramas. MCPDepth uses cylindrical panoramas for\ninitial stereo matching and then fuses the resulting depth maps across views. A\ncircular attention module is employed to overcome the distortion along the\nvertical axis. MCPDepth exclusively utilizes standard network components,\nsimplifying deployment to embedded devices and outperforming previous methods\nthat require custom kernels. We theoretically and experimentally compare\nspherical and cylindrical projections for stereo matching, highlighting the\nadvantages of the cylindrical projection. MCPDepth achieves state-of-the-art\nperformance with an 18.8% reduction in mean absolute error (MAE) for depth on\nthe outdoor synthetic dataset Deep360 and a 19.9% reduction on the indoor\nreal-scene dataset 3D60.\n", "rewritten_text": "\\boxed{We propose a novel two-stage framework, Multi-Cylindrical Panoramic Depth Estimation (MCPDepth), which leverages stereo matching between multiple cylindrical panoramas to estimate omnidirectional depth. Our approach first performs initial stereo matching on cylindrical panoramas and then fuses the resulting depth maps across views. To address vertical axis distortion, we employ a circular attention module. Notably, MCPDepth uses standard network components, making it easily deployable to embedded devices and outperforming methods that require custom kernels. We provide a theoretical and experimental comparison of spherical and cylindrical projections for stereo matching, highlighting the benefits of the cylindrical projection. Our approach achieves state-of-the-art performance, with a significant reduction in mean absolute error (MAE) of 18.8% on the Deep360 dataset and 19.9% on the 3D60 dataset.}"}, "2111.09091": {"original_text": "  Monitoring behaviour in smart homes using sensors can offer insights into\nchanges in the independent ability and long-term health of residents. Passive\nInfrared motion sensors (PIRs) are standard, however may not accurately track\nthe full duration of movement. They also require line-of-sight to detect motion\nwhich can restrict performance and ensures they must be visible to residents.\nChannel State Information (CSI) is a low cost, unintrusive form of radio\nsensing which can monitor movement but also offers opportunities to generate\nrich data. We have developed a novel, self-calibrating motion detection system\nwhich uses CSI data collected and processed on a stock Raspberry Pi 4. This\nsystem exploits the correlation between CSI frames, on which we perform\nvariance analysis using our algorithm to accurately measure the full period of\na resident's movement. We demonstrate the effectiveness of this approach in\nseveral real-world environments. Experiments conducted demonstrate that\nactivity start and end time can be accurately detected for motion examples of\ndifferent intensities at different locations.\n", "rewritten_text": "\\boxed{Utilizing sensors in smart homes to monitor behavior can provide valuable insights into residents' independence and long-term health. While traditional Passive Infrared motion sensors (PIRs) have limitations, a novel approach using Channel State Information (CSI) offers a low-cost, non-intrusive solution. By leveraging CSI data and a custom algorithm on a Raspberry Pi 4, our self-calibrating motion detection system can accurately track the full duration of movement, even in various real-world environments. Experimental results demonstrate the effectiveness of this approach in detecting activity start and end times for different motion intensities and locations.}"}, "2405.02296": {"original_text": "  Perspective distortion (PD) causes unprecedented changes in shape, size,\norientation, angles, and other spatial relationships of visual concepts in\nimages. Precisely estimating camera intrinsic and extrinsic parameters is a\nchallenging task that prevents synthesizing perspective distortion.\nNon-availability of dedicated training data poses a critical barrier to\ndeveloping robust computer vision methods. Additionally, distortion correction\nmethods make other computer vision tasks a multi-step approach and lack\nperformance. In this work, we propose mitigating perspective distortion (MPD)\nby employing a fine-grained parameter control on a specific family of M\\\"obius\ntransform to model real-world distortion without estimating camera intrinsic\nand extrinsic parameters and without the need for actual distorted data. Also,\nwe present a dedicated perspectively distorted benchmark dataset, ImageNet-PD,\nto benchmark the robustness of deep learning models against this new dataset.\nThe proposed method outperforms existing benchmarks, ImageNet-E and ImageNet-X.\nAdditionally, it significantly improves performance on ImageNet-PD while\nconsistently performing on standard data distribution. Notably, our method\nshows improved performance on three PD-affected real-world applications crowd\ncounting, fisheye image recognition, and person re-identification and one\nPD-affected challenging CV task: object detection. The source code, dataset,\nand models are available on the project webpage at\nhttps://prakashchhipa.github.io/projects/mpd.\n", "rewritten_text": "\\boxed{\nPerspective distortion in images alters shape, size, orientation, angles, and spatial relationships, making it challenging to estimate camera parameters and develop robust computer vision methods. To address this, we propose a novel approach, Mitigating Perspective Distortion (MPD), which utilizes fine-grained parameter control on a specific M\\\"obius transform to model real-world distortion without requiring camera parameters or actual distorted data. We also introduce a dedicated benchmark dataset, ImageNet-PD, to evaluate the robustness of deep learning models. Our method outperforms existing benchmarks, improves performance on ImageNet-PD, and shows enhanced results in various real-world applications, including crowd counting, fisheye image recognition, person re-identification, and object detection. The source code, dataset, and models are available at https://prakashchhipa.github.io/projects/mpd.\n}"}, "1705.08479": {"original_text": "  This paper introduces a new architectural framework, known as input\nfast-forwarding, that can enhance the performance of deep networks. The main\nidea is to incorporate a parallel path that sends representations of input\nvalues forward to deeper network layers. This scheme is substantially different\nfrom \"deep supervision\" in which the loss layer is re-introduced to earlier\nlayers. The parallel path provided by fast-forwarding enhances the training\nprocess in two ways. First, it enables the individual layers to combine\nhigher-level information (from the standard processing path) with lower-level\ninformation (from the fast-forward path). Second, this new architecture reduces\nthe problem of vanishing gradients substantially because the fast-forwarding\npath provides a shorter route for gradient backpropagation. In order to\nevaluate the utility of the proposed technique, a Fast-Forward Network (FFNet),\nwith 20 convolutional layers along with parallel fast-forward paths, has been\ncreated and tested. The paper presents empirical results that demonstrate\nimproved learning capacity of FFNet due to fast-forwarding, as compared to\nGoogLeNet (with deep supervision) and CaffeNet, which are 4x and 18x larger in\nsize, respectively. All of the source code and deep learning models described\nin this paper will be made available to the entire research community\n", "rewritten_text": "\\boxed{This paper proposes a novel architectural framework called input fast-forwarding, which boosts the performance of deep networks by incorporating a parallel path that sends input representations to deeper layers. Unlike deep supervision, which reintroduces the loss layer to earlier layers, fast-forwarding enables individual layers to combine higher-level and lower-level information, reducing the vanishing gradients problem. The authors evaluate the proposed technique by creating a Fast-Forward Network (FFNet) with 20 convolutional layers and parallel fast-forward paths, demonstrating improved learning capacity compared to larger models like GoogLeNet and CaffeNet. The source code and models will be made available to the research community.}"}, "2002.06515": {"original_text": "  Automatic analysis of highly crowded people has attracted extensive attention\nfrom computer vision research. Previous approaches for crowd counting have\nalready achieved promising performance across various benchmarks. However, to\ndeal with the real situation, we hope the model run as fast as possible while\nkeeping accuracy. In this paper, we propose a compact convolutional neural\nnetwork for crowd counting which learns a more efficient model with a small\nnumber of parameters. With three parallel filters executing the convolutional\noperation on the input image simultaneously at the front of the network, our\nmodel could achieve nearly real-time speed and save more computing resources.\nExperiments on two benchmarks show that our proposed method not only takes a\nbalance between performance and efficiency which is more suitable for actual\nscenes but also is superior to existing light-weight models in speed.\n", "rewritten_text": "\\boxed{The analysis of highly crowded people has garnered significant attention in computer vision research, with previous approaches achieving promising results. However, to tackle real-world scenarios, there is a need for models that balance speed and accuracy. This paper proposes a compact convolutional neural network for crowd counting that learns an efficient model with fewer parameters. By using three parallel filters to execute convolutional operations simultaneously, our model achieves near real-time speed and reduces computational resources. Experimental results on two benchmarks demonstrate that our method strikes a balance between performance and efficiency, making it more suitable for real-world applications, and outperforms existing lightweight models in terms of speed.}"}, "1711.03278": {"original_text": "  Convolution Neural Networks (CNN), known as ConvNets are widely used in many\nvisual imagery application, object classification, speech recognition. After\nthe implementation and demonstration of the deep convolution neural network in\nImagenet classification in 2012 by krizhevsky, the architecture of deep\nConvolution Neural Network is attracted many researchers. This has led to the\nmajor development in Deep learning frameworks such as Tensorflow, caffe, keras,\ntheno. Though the implementation of deep learning is quite possible by\nemploying deep learning frameworks, mathematical theory and concepts are harder\nto understand for new learners and practitioners. This article is intended to\nprovide an overview of ConvNets architecture and to explain the mathematical\ntheory behind it including activation function, loss function, feedforward and\nbackward propagation. In this article, grey scale image is taken as input\ninformation image, ReLU and Sigmoid activation function are considered for\ndeveloping the architecture and cross-entropy loss function are used for\ncomputing the difference between predicted value and actual value. The\narchitecture is developed in such a way that it can contain one convolution\nlayer, one pooling layer, and multiple dense layers\n", "rewritten_text": "\\boxed{\nConvolutional Neural Networks (CNNs), also known as ConvNets, have numerous applications in visual imagery, object classification, and speech recognition. Following the successful demonstration of deep CNNs in the 2012 Imagenet classification by Krizhevsky, the architecture of deep ConvNets has garnered significant attention from researchers, leading to major advancements in deep learning frameworks such as TensorFlow, Caffe, Keras, and Theano. Although these frameworks make implementing deep learning possible, the underlying mathematical theory and concepts can be challenging for new learners and practitioners to grasp. This article aims to provide an overview of the ConvNet architecture and explain the mathematical theory behind it, including activation functions, loss functions, feedforward and backward propagation. Specifically, this article uses a grayscale image as input, employs ReLU and Sigmoid activation functions, and utilizes cross-entropy loss functions to calculate the difference between predicted and actual values. The proposed architecture consists of one convolution layer, one pooling layer, and multiple dense layers.\n}"}, "1610.07995": {"original_text": "  In this paper, the task of recognizing signs made by hearing impaired people\nat sentence level has been addressed. A novel method of extracting spatial\nfeatures to capture hand movements of a signer has been proposed. Frames of a\ngiven video of a sign are preprocessed to extract face and hand components of a\nsigner. The local centroids of the extracted components along with the global\ncentroid are exploited to extract spatial features. The concept of interval\nvalued type symbolic data has been explored to capture variations in the same\nsign made by the different signers at different instances of time. A suitable\nsymbolic similarity measure is studied to establish matching between test and\nreference signs and a simple nearest neighbour classifier is used to recognize\nan unknown sign as one among the known signs by specifying a desired level of\nthreshold. An extensive experimentation is conducted on a considerably large\ndatabase of signs created by us during the course of research work in order to\nevaluate the performance of the proposed system\n", "rewritten_text": "\\boxed{This paper tackles the challenge of recognizing sign language sentences made by hearing-impaired individuals. A new approach is proposed to extract spatial features that capture hand movements of a signer. The method involves preprocessing video frames to isolate face and hand components, and then uses local and global centroids to extract spatial features. The concept of interval-valued symbolic data is explored to account for variations in the same sign made by different signers at different times. A suitable symbolic similarity measure is developed to match test and reference signs, and a simple nearest neighbor classifier is used to identify unknown signs by setting a desired threshold level. The proposed system is extensively tested on a large database of signs created during the research, evaluating its performance.}"}, "1908.04501": {"original_text": "  When a deep neural network is trained on data with only image-level labeling,\nthe regions activated in each image tend to identify only a small region of the\ntarget object. We propose a method of using videos automatically harvested from\nthe web to identify a larger region of the target object by using temporal\ninformation, which is not present in the static image. The temporal variations\nin a video allow different regions of the target object to be activated. We\nobtain an activated region in each frame of a video, and then aggregate the\nregions from successive frames into a single image, using a warping technique\nbased on optical flow. The resulting localization maps cover more of the target\nobject, and can then be used as proxy ground-truth to train a segmentation\nnetwork. This simple approach outperforms existing methods under the same level\nof supervision, and even approaches relying on extra annotations. Based on\nVGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4,\nrespectively, on PASCAL VOC 2012 test images, which represents a new\nstate-of-the-art.\n", "rewritten_text": "\\boxed{When training a deep neural network on image-level labeled data, it tends to focus on a small region of the target object. To overcome this limitation, we propose a method that utilizes web-harvested videos to identify a larger region of the target object by leveraging temporal information. By aggregating activated regions from successive video frames using optical flow-based warping, we generate more comprehensive localization maps that cover a larger area of the target object. These maps can then be used as proxy ground-truth to train a segmentation network, outperforming existing methods with similar supervision levels and even approaching those with extra annotations. Our approach achieves state-of-the-art results, with a mean intersection over union (mIoU) of 65.0 and 67.4 on PASCAL VOC 2012 test images using VGG-16 and ResNet 101 backbones, respectively.}"}, "2203.01532": {"original_text": "  Recently, contrastive learning-based image translation methods have been\nproposed, which contrasts different spatial locations to enhance the spatial\ncorrespondence. However, the methods often ignore the diverse semantic relation\nwithin the images. To address this, here we propose a novel semantic relation\nconsistency (SRC) regularization along with the decoupled contrastive learning,\nwhich utilize the diverse semantics by focusing on the heterogeneous semantics\nbetween the image patches of a single image. To further improve the\nperformance, we present a hard negative mining by exploiting the semantic\nrelation. We verified our method for three tasks: single-modal and multi-modal\nimage translations, and GAN compression task for image translation.\nExperimental results confirmed the state-of-art performance of our method in\nall the three tasks.\n", "rewritten_text": "\\boxed{Recently, image translation methods based on contrastive learning have been developed, which emphasize spatial correspondence by contrasting different locations. However, these methods often overlook the diverse semantic relationships within images. To tackle this limitation, we introduce a novel semantic relation consistency (SRC) regularization, combined with decoupled contrastive learning, which leverages heterogeneous semantics between image patches within a single image. Furthermore, we propose a hard negative mining approach that exploits semantic relationships to enhance performance. Our method has been validated through experiments on three tasks: single-modal and multi-modal image translations, and GAN compression for image translation, demonstrating state-of-the-art results in all three tasks.}"}, "2310.03414": {"original_text": "  Multi-document summarization is a challenging task due to its inherent\nsubjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4\namong DUC-2004 reference summaries. In this work, we aim to enhance the\nobjectivity of news summarization by focusing on the main event of a group of\nrelated news documents and presenting it coherently with sufficient context.\nOur primary objective is to succinctly report the main event, ensuring that the\nsummary remains objective and informative. To achieve this, we employ an\nextract-rewrite approach that incorporates a main-event biased\nmonotone-submodular function for content selection. This enables us to extract\nthe most crucial information related to the main event from the document\ncluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for\nrewriting the extracted content into a coherent text. The evaluation using\nobjective metrics and human evaluators confirms the effectiveness of our\napproach, as it surpasses potential baselines, demonstrating excellence in both\ncontent coverage, coherence, and informativeness.\n", "rewritten_text": "\\boxed{Summarizing multiple documents is a difficult task due to the subjective nature of the process, as shown by the low agreement score of 0.4 among human annotators in the DUC-2004 dataset. To address this issue, we focus on identifying the main event in a cluster of related news articles and present it in a clear and concise manner, providing sufficient context. Our goal is to create a summary that is both objective and informative, reporting the main event accurately. We achieve this by using a two-step approach that first selects the most important information related to the main event using a biased function, and then rewrites the selected content into a coherent text using a fine-tuned language model. Our evaluation results, based on both objective metrics and human assessment, demonstrate the effectiveness of our approach, outperforming baseline methods in terms of content coverage, coherence, and informativeness.}"}, "1608.04314": {"original_text": "  We present a technique for weakly supervised object localization (WSOL),\nbuilding on the observation that WSOL algorithms usually work better on images\nwith bigger objects. Instead of training the object detector on the entire\ntraining set at the same time, we propose a curriculum learning strategy to\nfeed training images into the WSOL learning loop in an order from images\ncontaining bigger objects down to smaller ones. To automatically determine the\norder, we train a regressor to estimate the size of the object given the whole\nimage as input. Furthermore, we use these size estimates to further improve the\nre-localization step of WSOL by assigning weights to object proposals according\nto how close their size matches the estimated object size. We demonstrate the\neffectiveness of using size order and size weighting on the challenging PASCAL\nVOC 2007 dataset, where we achieve a significant improvement over existing\nstate-of-the-art WSOL techniques.\n", "rewritten_text": "\\boxed{We introduce a novel approach to weakly supervised object localization (WSOL) that leverages the insight that WSOL algorithms perform better on images with larger objects. Our method employs a curriculum learning strategy, where training images are fed into the WSOL loop in a specific order, starting with images containing larger objects and gradually moving to smaller ones. To determine this order, we train a regressor to estimate object size based on the entire image. Additionally, we utilize these size estimates to enhance the re-localization step of WSOL by assigning weights to object proposals based on their size similarity to the estimated object size. Our approach yields significant improvements over existing state-of-the-art WSOL techniques on the challenging PASCAL VOC 2007 dataset.}"}, "2402.00160": {"original_text": "  In this work, we introduce the Multiple Embedding Model for EHR (MEME), an\napproach that serializes multimodal EHR tabular data into text using\npseudo-notes, mimicking clinical text generation. This conversion not only\npreserves better representations of categorical data and learns contexts but\nalso enables the effective employment of pretrained foundation models for rich\nfeature representation. To address potential issues with context length, our\nframework encodes embeddings for each EHR modality separately. We demonstrate\nthe effectiveness of MEME by applying it to several decision support tasks\nwithin the Emergency Department across multiple hospital systems. Our findings\nindicate that MEME outperforms traditional machine learning, EHR-specific\nfoundation models, and general LLMs, highlighting its potential as a general\nand extendible EHR representation strategy.\n", "rewritten_text": "\\boxed{We propose the Multiple Embedding Model for EHR (MEME), a novel approach that converts multimodal EHR tabular data into text format using pseudo-notes, similar to clinical text generation. This conversion preserves categorical data representations, learns contexts, and enables the use of pre-trained foundation models for rich feature extraction. To address context length issues, our framework encodes embeddings for each EHR modality separately. We evaluate MEME's effectiveness on various decision support tasks in Emergency Departments across multiple hospital systems, and our results show that MEME outperforms traditional machine learning, EHR-specific foundation models, and general language models, demonstrating its potential as a generalizable and extendable EHR representation strategy.}"}, "2305.11853": {"original_text": "  Large language models (LLMs) with in-context learning have demonstrated\nremarkable capability in the text-to-SQL task. Previous research has prompted\nLLMs with various demonstration-retrieval strategies and intermediate reasoning\nsteps to enhance the performance of LLMs. However, those works often employ\nvaried strategies when constructing the prompt text for text-to-SQL inputs,\nsuch as databases and demonstration examples. This leads to a lack of\ncomparability in both the prompt constructions and their primary contributions.\nFurthermore, selecting an effective prompt construction has emerged as a\npersistent problem for future research. To address this limitation, we\ncomprehensively investigate the impact of prompt constructions across various\nsettings and provide insights into prompt constructions for future text-to-SQL\nstudies.\n", "rewritten_text": "\\boxed{Large language models have shown impressive performance in text-to-SQL tasks, but previous research has used different strategies to construct prompt texts, making it difficult to compare results. This lack of standardization has led to a persistent problem in selecting effective prompt constructions. To overcome this limitation, we conduct a comprehensive study to examine the impact of various prompt constructions across different settings, providing valuable insights for future text-to-SQL research.}"}, "1007.0621": {"original_text": "  In this paper fusion of visual and thermal images in wavelet transformed\ndomain has been presented. Here, Daubechies wavelet transform, called as D2,\ncoefficients from visual and corresponding coefficients computed in the same\nmanner from thermal images are combined to get fused coefficients. After\ndecomposition up to fifth level (Level 5) fusion of coefficients is done.\nInverse Daubechies wavelet transform of those coefficients gives us fused face\nimages. The main advantage of using wavelet transform is that it is well-suited\nto manage different image resolution and allows the image decomposition in\ndifferent kinds of coefficients, while preserving the image information. Fused\nimages thus found are passed through Principal Component Analysis (PCA) for\nreduction of dimensions and then those reduced fused images are classified\nusing a multi-layer perceptron. For experiments IRIS Thermal/Visual Face\nDatabase was used. Experimental results show that the performance of the\napproach presented here achieves maximum success rate of 100% in many cases.\n", "rewritten_text": "\\boxed{This paper proposes a method for fusing visual and thermal images in the wavelet transform domain using Daubechies wavelet transform (D2). The coefficients from visual and thermal images are combined after decomposition up to the fifth level, and the inverse transform yields fused face images. The wavelet transform is advantageous as it can handle different image resolutions and decompose images into various coefficients while preserving information. The fused images are then dimensionally reduced using Principal Component Analysis (PCA) and classified using a multi-layer perceptron. Experimental results on the IRIS Thermal/Visual Face Database demonstrate a maximum success rate of 100% in many cases.}"}, "2210.11725": {"original_text": "  We present AROS, a one-shot learning approach that uses an explicit\nrepresentation of interactions between highly-articulated human poses and 3D\nscenes. The approach is one-shot as the method does not require re-training to\nadd new affordance instances. Furthermore, only one or a small handful of\nexamples of the target pose are needed to describe the interaction. Given a 3D\nmesh of a previously unseen scene, we can predict affordance locations that\nsupport the interactions and generate corresponding articulated 3D human bodies\naround them. We evaluate on three public datasets of scans of real environments\nwith varied degrees of noise. Via rigorous statistical analysis of crowdsourced\nevaluations, results show that our one-shot approach outperforms data-intensive\nbaselines by up to 80\\%.\n", "rewritten_text": "\\boxed{We introduce AROS, a novel one-shot learning method that leverages an explicit representation of human pose interactions with 3D scenes. Notably, our approach does not require retraining to incorporate new affordance instances and only needs a few examples of the target pose to describe the interaction. Given a 3D mesh of a previously unseen scene, AROS can predict affordance locations and generate corresponding articulated 3D human bodies. We evaluate our method on three public datasets of real-world environment scans with varying noise levels, and our rigorous statistical analysis of crowdsourced evaluations shows that AROS outperforms data-intensive baselines by up to 80%.}"}, "1604.06646": {"original_text": "  In this paper we introduce a new method for text detection in natural images.\nThe method comprises two contributions: First, a fast and scalable engine to\ngenerate synthetic images of text in clutter. This engine overlays synthetic\ntext to existing background images in a natural way, accounting for the local\n3D scene geometry. Second, we use the synthetic images to train a\nFully-Convolutional Regression Network (FCRN) which efficiently performs text\ndetection and bounding-box regression at all locations and multiple scales in\nan image. We discuss the relation of FCRN to the recently-introduced YOLO\ndetector, as well as other end-to-end object detection systems based on deep\nlearning. The resulting detection network significantly out performs current\nmethods for text detection in natural images, achieving an F-measure of 84.2%\non the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per\nsecond on a GPU.\n", "rewritten_text": "\\boxed{This paper presents a novel approach to detecting text in natural images, consisting of two key components. Firstly, a fast and scalable engine generates synthetic images of text in cluttered scenes, taking into account local 3D geometry. Secondly, these synthetic images are used to train a Fully-Convolutional Regression Network (FCRN) that efficiently detects text and predicts bounding boxes at multiple scales and locations. The proposed method outperforms existing techniques, achieving an F-measure of 84.2% on the ICDAR 2013 benchmark, and can process 15 images per second on a GPU.}"}, "2204.06028": {"original_text": "  In this paper, we describe our submission to the Simultaneous Speech\nTranslation at IWSLT 2022. We explore strategies to utilize an offline model in\na simultaneous setting without the need to modify the original model. In our\nexperiments, we show that our onlinization algorithm is almost on par with the\noffline setting while being $3\\times$ faster than offline in terms of latency\non the test set. We also show that the onlinized offline model outperforms the\nbest IWSLT2021 simultaneous system in medium and high latency regimes and is\nalmost on par in the low latency regime. We make our system publicly available.\n", "rewritten_text": "\\boxed{This paper presents our submission to the IWSLT 2022 Simultaneous Speech Translation task, where we develop strategies to adapt an offline model for simultaneous translation without modifying the original model. Our experiments demonstrate that our onlinization algorithm achieves similar performance to the offline model while reducing latency by a factor of three. Furthermore, our onlinized offline model outperforms the top IWSLT2021 simultaneous system in medium and high latency settings and is comparable in low latency settings. Our system is publicly available.}"}, "2304.02715": {"original_text": "  This paper presents an unsupervised approach that leverages raw aerial videos\nto learn to estimate planar homographic transformation between consecutive\nvideo frames. Previous learning-based estimators work on pairs of images to\nestimate their planar homographic transformations but suffer from severe\nover-fitting issues, especially when applying over aerial videos. To address\nthis concern, we develop a sequential estimator that directly processes a\nsequence of video frames and estimates their pairwise planar homographic\ntransformations in batches. We also incorporate a set of spatial-temporal\nknowledge to regularize the learning of such a sequence-to-sequence model. We\ncollect a set of challenging aerial videos and compare the proposed method to\nthe alternative algorithms. Empirical studies suggest that our sequential model\nachieves significant improvement over alternative image-based methods and the\nknowledge-rich regularization further boosts our system performance. Our codes\nand dataset could be found at https://github.com/Paul-LiPu/DeepVideoHomography\n", "rewritten_text": "\\boxed{This paper introduces an unsupervised method that uses raw aerial videos to learn planar homographic transformations between consecutive frames. Unlike previous image-based estimators that suffer from over-fitting issues, our approach processes video frames sequentially and estimates transformations in batches. We incorporate spatial-temporal knowledge to regularize the learning process and demonstrate significant improvement over alternative methods through empirical studies. Our code and dataset are available at https://github.com/Paul-LiPu/DeepVideoHomography.}"}, "2306.16925": {"original_text": "  Pretraining with large-scale 3D volumes has a potential for improving the\nsegmentation performance on a target medical image dataset where the training\nimages and annotations are limited. Due to the high cost of acquiring\npixel-level segmentation annotations on the large-scale pretraining dataset,\npretraining with unannotated images is highly desirable. In this work, we\npropose a novel self-supervised learning strategy named Volume Fusion (VF) for\npretraining 3D segmentation models. It fuses several random patches from a\nforeground sub-volume to a background sub-volume based on a predefined set of\ndiscrete fusion coefficients, and forces the model to predict the fusion\ncoefficient of each voxel, which is formulated as a self-supervised\nsegmentation task without manual annotations. Additionally, we propose a novel\nnetwork architecture based on parallel convolution and transformer blocks that\nis suitable to be transferred to different downstream segmentation tasks with\nvarious scales of organs and lesions. The proposed model was pretrained with\n110k unannotated 3D CT volumes, and experiments with different downstream\nsegmentation targets including head and neck organs, thoracic/abdominal organs\nshowed that our pretrained model largely outperformed training from scratch and\nseveral state-of-the-art self-supervised training methods and segmentation\nmodels. The code and pretrained model are available at\nhttps://github.com/openmedlab/MIS-FM.\n", "rewritten_text": "\\boxed{\nPretraining 3D segmentation models with large-scale volumes can improve performance on target medical image datasets with limited training images and annotations. However, acquiring pixel-level annotations for pretraining datasets is costly. To address this, we propose a novel self-supervised learning strategy called Volume Fusion (VF) that pretrains 3D segmentation models without manual annotations. VF fuses random patches from a foreground sub-volume to a background sub-volume using predefined fusion coefficients, and the model predicts the fusion coefficient of each voxel. We also introduce a novel network architecture that combines parallel convolution and transformer blocks, suitable for transfer to various downstream segmentation tasks. Our pretrained model, trained on 110k unannotated 3D CT volumes, outperforms training from scratch and state-of-the-art self-supervised methods on different downstream segmentation targets, including head and neck organs and thoracic/abdominal organs. The code and pretrained model are available at https://github.com/openmedlab/MIS-FM.\n}"}, "2102.0932": {"original_text": "  Event cameras are novel vision sensors that report per-pixel brightness\nchanges as a stream of asynchronous \"events\". They offer significant advantages\ncompared to standard cameras due to their high temporal resolution, high\ndynamic range and lack of motion blur. However, events only measure the varying\ncomponent of the visual signal, which limits their ability to encode scene\ncontext. By contrast, standard cameras measure absolute intensity frames, which\ncapture a much richer representation of the scene. Both sensors are thus\ncomplementary. However, due to the asynchronous nature of events, combining\nthem with synchronous images remains challenging, especially for learning-based\nmethods. This is because traditional recurrent neural networks (RNNs) are not\ndesigned for asynchronous and irregular data from additional sensors. To\naddress this challenge, we introduce Recurrent Asynchronous Multimodal (RAM)\nnetworks, which generalize traditional RNNs to handle asynchronous and\nirregular data from multiple sensors. Inspired by traditional RNNs, RAM\nnetworks maintain a hidden state that is updated asynchronously and can be\nqueried at any time to generate a prediction. We apply this novel architecture\nto monocular depth estimation with events and frames where we show an\nimprovement over state-of-the-art methods by up to 30% in terms of mean\nabsolute depth error. To enable further research on multimodal learning with\nevents, we release EventScape, a new dataset with events, intensity frames,\nsemantic labels, and depth maps recorded in the CARLA simulator.\n", "rewritten_text": "\\boxed{\nEvent cameras, which capture changes in brightness as asynchronous \"events\", offer advantages over standard cameras, including high temporal resolution and lack of motion blur. However, they struggle to encode scene context. Standard cameras, on the other hand, capture absolute intensity frames, providing a richer representation of the scene. Combining these two sensors is challenging, especially for learning-based methods, due to the asynchronous nature of events. To address this, we introduce Recurrent Asynchronous Multimodal (RAM) networks, which can handle asynchronous and irregular data from multiple sensors. We apply this architecture to monocular depth estimation, achieving an improvement of up to 30% over state-of-the-art methods. Additionally, we release EventScape, a new dataset for multimodal learning with events, featuring events, intensity frames, semantic labels, and depth maps recorded in the CARLA simulator.\n}"}, "2410.05963": {"original_text": "  Existing perception models achieve great success by learning from large\namounts of labeled data, but they still struggle with open-world scenarios. To\nalleviate this issue, researchers introduce open-set perception tasks to detect\nor segment unseen objects in the training set. However, these models require\npredefined object categories as inputs during inference, which are not\navailable in real-world scenarios. Recently, researchers pose a new and more\npractical problem, \\textit{i.e.}, open-ended object detection, which discovers\nunseen objects without any object categories as inputs. In this paper, we\npresent VL-SAM, a training-free framework that combines the generalized object\nrecognition model (\\textit{i.e.,} Vision-Language Model) with the generalized\nobject localization model (\\textit{i.e.,} Segment-Anything Model), to address\nthe open-ended object detection and segmentation task. Without additional\ntraining, we connect these two generalized models with attention maps as the\nprompts. Specifically, we design an attention map generation module by\nemploying head aggregation and a regularized attention flow to aggregate and\npropagate attention maps across all heads and layers in VLM, yielding\nhigh-quality attention maps. Then, we iteratively sample positive and negative\npoints from the attention maps with a prompt generation module and send the\nsampled points to SAM to segment corresponding objects. Experimental results on\nthe long-tail instance segmentation dataset (LVIS) show that our method\nsurpasses the previous open-ended method on the object detection task and can\nprovide additional instance segmentation masks. Besides, VL-SAM achieves\nfavorable performance on the corner case object detection dataset (CODA),\ndemonstrating the effectiveness of VL-SAM in real-world applications. Moreover,\nVL-SAM exhibits good model generalization that can incorporate various VLMs and\nSAMs.\n", "rewritten_text": "\\boxed{\nExisting perception models have achieved great success, but they still struggle with detecting unseen objects in real-world scenarios. To address this, researchers have introduced open-set perception tasks, but these models require predefined object categories as inputs, which are not always available. Recently, a new problem called open-ended object detection has been proposed, which involves discovering unseen objects without any prior knowledge of object categories. This paper presents VL-SAM, a novel framework that combines a vision-language model with a segment-anything model to tackle open-ended object detection and segmentation tasks without requiring additional training. VL-SAM uses attention maps to connect the two models and iteratively samples points to segment objects. Experimental results show that VL-SAM outperforms previous methods on object detection tasks and provides additional instance segmentation masks. It also demonstrates good performance on real-world datasets and can incorporate various vision-language models and segment-anything models.\n}"}, "1805.05421": {"original_text": "  Deep learning has made significant improvements at many image processing\ntasks in recent years, such as image classification, object recognition and\nobject detection. Convolutional neural networks (CNN), which is a popular deep\nlearning architecture designed to process data in multiple array form, show\ngreat success to almost all detection \\& recognition problems and computer\nvision tasks. However, the number of parameters in a CNN is too high such that\nthe computers require more energy and larger memory size. In order to solve\nthis problem, we propose a novel energy efficient model Binary Weight and\nHadamard-transformed Image Network (BWHIN), which is a combination of Binary\nWeight Network (BWN) and Hadamard-transformed Image Network (HIN). It is\nobserved that energy efficiency is achieved with a slight sacrifice at\nclassification accuracy. Among all energy efficient networks, our novel\nensemble model outperforms other energy efficient models.\n", "rewritten_text": "\\boxed{In recent years, deep learning has achieved remarkable progress in various image processing tasks, including image classification, object recognition, and object detection. Convolutional neural networks (CNNs) have been particularly successful in these areas, but they require significant computational resources and memory due to their large number of parameters. To address this issue, we propose a novel energy-efficient model, Binary Weight and Hadamard-transformed Image Network (BWHIN), which combines the benefits of Binary Weight Network (BWN) and Hadamard-transformed Image Network (HIN). While our model achieves energy efficiency at the cost of slight accuracy, it outperforms other energy-efficient models in terms of overall performance.}"}, "2001.11561": {"original_text": "  We consider referring image segmentation. It is a problem at the intersection\nof computer vision and natural language understanding. Given an input image and\na referring expression in the form of a natural language sentence, the goal is\nto segment the object of interest in the image referred by the linguistic\nquery. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to\ntackle this problem. Our model consists of an encoder network and a decoder\nnetwork, where ConvLSTM is used in both encoder and decoder networks to capture\nspatial and sequential information. The encoder network extracts visual and\nlinguistic features for each word in the expression sentence, and adopts an\nattention mechanism to focus on words that are more informative in the\nmultimodal interaction. The decoder network integrates the features generated\nby the encoder network at multiple levels as its input and produces the final\nprecise segmentation mask. Experimental results on four challenging datasets\ndemonstrate that the proposed network achieves superior segmentation\nperformance compared with other state-of-the-art methods.\n", "rewritten_text": "\\boxed{We tackle the problem of referring image segmentation, which combines computer vision and natural language understanding. Given an image and a natural language sentence, our goal is to identify the object in the image described by the sentence. To achieve this, we propose a dual ConvLSTM network consisting of an encoder and a decoder. The encoder extracts visual and linguistic features for each word in the sentence, focusing on the most informative words through an attention mechanism. The decoder integrates these features to produce a precise segmentation mask. Our experimental results on four challenging datasets show that our proposed network outperforms other state-of-the-art methods.}"}, "1903.07224": {"original_text": "  This work develops a novel end-to-end deep unsupervised learning method based\non convolutional neural network (CNN) with pseudo-classes for remote sensing\nscene representation. First, we introduce center points as the centers of the\npseudo classes and the training samples can be allocated with pseudo labels\nbased on the center points. Therefore, the CNN model, which is used to extract\nfeatures from the scenes, can be trained supervised with the pseudo labels.\nMoreover, a pseudo-center loss is developed to decrease the variance between\nthe samples and the corresponding pseudo center point. The pseudo-center loss\nis important since it can update both the center points with the training\nsamples and the CNN model with the center points in the training process\nsimultaneously. Finally, joint learning of the pseudo-center loss and the\npseudo softmax loss which is formulated with the samples and the pseudo labels\nis developed for unsupervised remote sensing scene representation to obtain\ndiscriminative representations from the scenes. Experiments are conducted over\ntwo commonly used remote sensing scene datasets to validate the effectiveness\nof the proposed method and the experimental results show the superiority of the\nproposed method when compared with other state-of-the-art methods.\n", "rewritten_text": "\\boxed{\nThis paper presents a novel deep unsupervised learning approach for remote sensing scene representation, leveraging convolutional neural networks (CNNs) with pseudo-classes. The method involves introducing center points as pseudo-class centers, assigning pseudo-labels to training samples based on these centers, and training the CNN model in a supervised manner. A pseudo-center loss is also developed to reduce the variance between samples and their corresponding center points, allowing for simultaneous updates of center points and the CNN model during training. The proposed method combines the pseudo-center loss with a pseudo-softmax loss to learn discriminative representations from remote sensing scenes in an unsupervised manner. Experimental results on two benchmark datasets demonstrate the effectiveness and superiority of the proposed method compared to state-of-the-art approaches."}, "2203.073": {"original_text": "  Current mobile user authentication systems based on PIN codes, fingerprint,\nand face recognition have several shortcomings. Such limitations have been\naddressed in the literature by exploring the feasibility of passive\nauthentication on mobile devices through behavioral biometrics. In this line of\nresearch, this work carries out a comparative analysis of unimodal and\nmultimodal behavioral biometric traits acquired while the subjects perform\ndifferent activities on the phone such as typing, scrolling, drawing a number,\nand tapping on the screen, considering the touchscreen and the simultaneous\nbackground sensor data (accelerometer, gravity sensor, gyroscope, linear\naccelerometer, and magnetometer). Our experiments are performed over HuMIdb,\none of the largest and most comprehensive freely available mobile user\ninteraction databases to date. A separate Recurrent Neural Network (RNN) with\ntriplet loss is implemented for each single modality. Then, the weighted fusion\nof the different modalities is carried out at score level. In our experiments,\nthe most discriminative background sensor is the magnetometer, whereas among\ntouch tasks the best results are achieved with keystroke in a fixed-text\nscenario. In all cases, the fusion of modalities is very beneficial, leading to\nEqual Error Rates (EER) ranging from 4% to 9% depending on the modality\ncombination in a 3-second interval.\n", "rewritten_text": "\\boxed{Current mobile user authentication systems have limitations, which researchers have addressed by exploring passive authentication through behavioral biometrics. This study compares the effectiveness of different behavioral biometric traits, such as typing, scrolling, and tapping, using touchscreen and sensor data from various phone activities. The results, based on a large mobile user interaction database, show that combining multiple modalities leads to improved authentication accuracy, with error rates ranging from 4% to 9% in a 3-second interval. The magnetometer sensor and keystroke data in a fixed-text scenario were found to be the most discriminative.}"}, "2305.14722": {"original_text": "  Most contemporary supervised Remote Sensing (RS) image Change Detection (CD)\napproaches are customized for equal-resolution bitemporal images. Real-world\napplications raise the need for cross-resolution change detection, aka, CD\nbased on bitemporal images with different spatial resolutions. Given training\nsamples of a fixed bitemporal resolution difference (ratio) between the\nhigh-resolution (HR) image and the low-resolution (LR) one, current\ncross-resolution methods may fit a certain ratio but lack adaptation to other\nresolution differences. Toward continuous cross-resolution CD, we propose\nscale-invariant learning to enforce the model consistently predicting HR\nresults given synthesized samples of varying resolution differences.\nConcretely, we synthesize blurred versions of the HR image by random\ndownsampled reconstructions to reduce the gap between HR and LR images. We\nintroduce coordinate-based representations to decode per-pixel predictions by\nfeeding the coordinate query and corresponding multi-level embedding features\ninto an MLP that implicitly learns the shape of land cover changes, therefore\nbenefiting recognizing blurred objects in the LR image. Moreover, considering\nthat spatial resolution mainly affects the local textures, we apply\nlocal-window self-attention to align bitemporal features during the early\nstages of the encoder. Extensive experiments on two synthesized and one\nreal-world different-resolution CD datasets verify the effectiveness of the\nproposed method. Our method significantly outperforms several vanilla CD\nmethods and two cross-resolution CD methods on the three datasets both in\nin-distribution and out-of-distribution settings. The empirical results suggest\nthat our method could yield relatively consistent HR change predictions\nregardless of varying bitemporal resolution ratios. Our code is available at\n\\url{https://github.com/justchenhao/SILI_CD}.\n", "rewritten_text": "\\boxed{\nMost current change detection approaches in remote sensing are designed for images with the same resolution. However, real-world applications require the ability to detect changes between images with different spatial resolutions. Existing methods can only handle specific resolution differences and lack adaptability to other ratios. To address this limitation, we propose a scale-invariant learning approach that enables the model to consistently predict high-resolution results even when given images with varying resolution differences. Our method involves synthesizing blurred versions of high-resolution images, using coordinate-based representations to decode per-pixel predictions, and applying local-window self-attention to align features. Experimental results on three datasets demonstrate the effectiveness of our approach, outperforming several existing methods in both in-distribution and out-of-distribution settings. Our method shows promise in providing consistent high-resolution change predictions regardless of varying bitemporal resolution ratios.\n}"}, "1704.08763": {"original_text": "  We present GazeDirector, a new approach for eye gaze redirection that uses\nmodel-fitting. Our method first tracks the eyes by fitting a multi-part eye\nregion model to video frames using analysis-by-synthesis, thereby recovering\neye region shape, texture, pose, and gaze simultaneously. It then redirects\ngaze by 1) warping the eyelids from the original image using a model-derived\nflow field, and 2) rendering and compositing synthesized 3D eyeballs onto the\noutput image in a photorealistic manner. GazeDirector allows us to change where\npeople are looking without person-specific training data, and with full\narticulation, i.e. we can precisely specify new gaze directions in 3D.\nQuantitatively, we evaluate both model-fitting and gaze synthesis, with\nexperiments for gaze estimation and redirection on the Columbia gaze dataset.\nQualitatively, we compare GazeDirector against recent work on gaze redirection,\nshowing better results especially for large redirection angles. Finally, we\ndemonstrate gaze redirection on YouTube videos by introducing new 3D gaze\ntargets and by manipulating visual behavior.\n", "rewritten_text": "\\boxed{We introduce GazeDirector, a novel approach to redirect eye gaze using model-fitting. This method involves tracking eyes by fitting a multi-part eye region model to video frames, which recovers eye shape, texture, pose, and gaze simultaneously. Then, it redirects gaze by warping eyelids and rendering synthesized 3D eyeballs onto the output image. This approach allows for changing gaze direction without person-specific training data and with precise control. We evaluate the method quantitatively on the Columbia gaze dataset and qualitatively compare it to recent work, demonstrating improved results, especially for large redirection angles. Finally, we apply GazeDirector to YouTube videos, introducing new 3D gaze targets and manipulating visual behavior.}"}, "2110.13385": {"original_text": "  Recently, Transformer-based networks have shown great promise on\nskeleton-based action recognition tasks. The ability to capture global and\nlocal dependencies is the key to success while it also brings quadratic\ncomputation and memory cost. Another problem is that previous studies mainly\nfocus on the relationships among individual joints, which often suffers from\nthe noisy skeleton joints introduced by the noisy inputs of sensors or\ninaccurate estimations. To address the above issues, we propose a novel\nTransformer-based network (IIP-Transformer). Instead of exploiting interactions\namong individual joints, our IIP-Transformer incorporates body joints and parts\ninteractions simultaneously and thus can capture both joint-level (intra-part)\nand part-level (inter-part) dependencies efficiently and effectively. From the\ndata aspect, we introduce a part-level skeleton data encoding that\nsignificantly reduces the computational complexity and is more robust to\njoint-level skeleton noise. Besides, a new part-level data augmentation is\nproposed to improve the performance of the model. On two large-scale datasets,\nNTU-RGB+D 60 and NTU RGB+D 120, the proposed IIP-Transformer achieves\nthe-state-of-art performance with more than 8x less computational complexity\nthan DSTA-Net, which is the SOTA Transformer-based method.\n", "rewritten_text": "\\boxed{\nRecently, Transformer-based networks have demonstrated exceptional performance in skeleton-based action recognition tasks, thanks to their ability to capture global and local dependencies. However, this comes at the cost of quadratic computation and memory usage. Moreover, previous studies have primarily focused on the relationships between individual joints, which can be compromised by noisy skeleton joints resulting from sensor errors or inaccurate estimations. To overcome these limitations, we propose a novel Transformer-based network, IIP-Transformer, which simultaneously incorporates interactions between body joints and parts. This approach efficiently captures both joint-level and part-level dependencies. We also introduce a part-level skeleton data encoding that significantly reduces computational complexity and is more robust to joint-level skeleton noise. Additionally, a new part-level data augmentation is proposed to enhance model performance. Our IIP-Transformer achieves state-of-the-art performance on two large-scale datasets, NTU-RGB+D 60 and NTU RGB+D 120, with a remarkable 8x reduction in computational complexity compared to the current SOTA Transformer-based method, DSTA-Net.\n}"}, "2307.01200": {"original_text": "  Learning-based approaches to monocular motion capture have recently shown\npromising results by learning to regress in a data-driven manner. However, due\nto the challenges in data collection and network designs, it remains\nchallenging for existing solutions to achieve real-time full-body capture while\nbeing accurate in world space. In this work, we introduce ProxyCap, a\nhuman-centric proxy-to-motion learning scheme to learn world-space motions from\na proxy dataset of 2D skeleton sequences and 3D rotational motions. Such proxy\ndata enables us to build a learning-based network with accurate world-space\nsupervision while also mitigating the generalization issues. For more accurate\nand physically plausible predictions in world space, our network is designed to\nlearn human motions from a human-centric perspective, which enables the\nunderstanding of the same motion captured with different camera trajectories.\nMoreover, a contact-aware neural motion descent module is proposed in our\nnetwork so that it can be aware of foot-ground contact and motion misalignment\nwith the proxy observations. With the proposed learning-based solution, we\ndemonstrate the first real-time monocular full-body capture system with\nplausible foot-ground contact in world space even using hand-held moving\ncameras. Our project page is https://zhangyux15.github.io/ProxyCapV2.\n", "rewritten_text": "\\boxed{\nRecent learning-based approaches to monocular motion capture have shown promising results by learning to regress in a data-driven manner. However, existing solutions struggle to achieve real-time full-body capture with accurate world-space results due to data collection and network design challenges. To address this, we propose ProxyCap, a novel learning scheme that uses a proxy dataset of 2D skeleton sequences and 3D rotational motions to learn world-space motions. This approach enables accurate world-space supervision while mitigating generalization issues. Our network is designed to learn human motions from a human-centric perspective, allowing it to understand the same motion captured from different camera trajectories. Additionally, we introduce a contact-aware neural motion descent module to account for foot-ground contact and motion misalignment with proxy observations. Our solution achieves the first real-time monocular full-body capture system with plausible foot-ground contact in world space, even with hand-held moving cameras."}, "1410.7484": {"original_text": "  Stochastic sampling based trackers have shown good performance for abrupt\nmotion tracking so that they have gained popularity in recent years. However,\nconventional methods tend to use a two-stage sampling paradigm, in which the\nsearch space needs to be uniformly explored with an inefficient preliminary\nsampling phase. In this paper, we propose a novel sampling-based method in the\nBayesian filtering framework to address the problem. Within the framework,\nnearest neighbor field estimation is utilized to compute the importance\nproposal probabilities, which guide the Markov chain search towards promising\nregions and thus enhance the sampling efficiency; given the motion priors, a\nsmoothing stochastic sampling Monte Carlo algorithm is proposed to approximate\nthe posterior distribution through a smoothing weight-updating scheme.\nMoreover, to track the abrupt and the smooth motions simultaneously, we develop\nan abrupt-motion detection scheme which can discover the presence of abrupt\nmotions during online tracking. Extensive experiments on challenging image\nsequences demonstrate the effectiveness and the robustness of our algorithm in\nhandling the abrupt motions.\n", "rewritten_text": "\\boxed{Recent years have seen the rise of stochastic sampling-based trackers for abrupt motion tracking due to their good performance. However, traditional methods employ a two-stage sampling approach, which involves an inefficient initial sampling phase that uniformly explores the search space. This paper proposes a novel Bayesian filtering framework-based method to address this issue. The proposed method utilizes nearest neighbor field estimation to compute importance proposal probabilities, guiding the Markov chain search towards promising regions and enhancing sampling efficiency. Additionally, a smoothing stochastic sampling Monte Carlo algorithm is introduced to approximate the posterior distribution using a smoothing weight-updating scheme. Furthermore, an abrupt-motion detection scheme is developed to detect the presence of abrupt motions during online tracking, enabling the simultaneous tracking of abrupt and smooth motions. The effectiveness and robustness of the proposed algorithm are demonstrated through extensive experiments on challenging image sequences.}"}, "2010.06944": {"original_text": "  We present a formulation of the relative depth estimation from a single image\nproblem, as a ranking problem. By reformulating the problem this way, we were\nable to utilize literature on the ranking problem, and apply the existing\nknowledge to achieve better results. To this end, we have introduced a listwise\nranking loss borrowed from ranking literature, weighted ListMLE, to the\nrelative depth estimation problem. We have also brought a new metric which\nconsiders pixel depth ranking accuracy, on which our method is stronger.\n", "rewritten_text": "\\boxed{We reformulate the single-image relative depth estimation problem as a ranking problem, enabling the application of existing ranking literature and techniques. This approach allows us to leverage a listwise ranking loss function, weighted ListMLE, and introduce a new metric that evaluates pixel depth ranking accuracy, where our method demonstrates improved performance.}"}, "2012.02951": {"original_text": "  Visual scene understanding is the core task in making any crucial decision in\nany computer vision system. Although popular computer vision datasets like\nCityscapes, MS-COCO, PASCAL provide good benchmarks for several tasks (e.g.\nimage classification, segmentation, object detection), these datasets are\nhardly suitable for post disaster damage assessments. On the other hand,\nexisting natural disaster datasets include mainly satellite imagery which have\nlow spatial resolution and a high revisit period. Therefore, they do not have a\nscope to provide quick and efficient damage assessment tasks. Unmanned Aerial\nVehicle(UAV) can effortlessly access difficult places during any disaster and\ncollect high resolution imagery that is required for aforementioned tasks of\ncomputer vision. To address these issues we present a high resolution UAV\nimagery, FloodNet, captured after the hurricane Harvey. This dataset\ndemonstrates the post flooded damages of the affected areas. The images are\nlabeled pixel-wise for semantic segmentation task and questions are produced\nfor the task of visual question answering. FloodNet poses several challenges\nincluding detection of flooded roads and buildings and distinguishing between\nnatural water and flooded water. With the advancement of deep learning\nalgorithms, we can analyze the impact of any disaster which can make a precise\nunderstanding of the affected areas. In this paper, we compare and contrast the\nperformances of baseline methods for image classification, semantic\nsegmentation, and visual question answering on our dataset.\n", "rewritten_text": "\\boxed{\nVisual scene understanding is a critical component of computer vision systems, but existing datasets have limitations when it comes to post-disaster damage assessments. Popular datasets like Cityscapes, MS-COCO, and PASCAL are not suitable for this task, while natural disaster datasets are often limited by low-resolution satellite imagery with long revisit periods. To address this gap, we introduce FloodNet, a high-resolution UAV imagery dataset captured after Hurricane Harvey, which provides detailed images of flooded areas with pixel-wise labels for semantic segmentation and questions for visual question answering. This dataset poses challenges such as detecting flooded roads and buildings, and distinguishing between natural and flooded water. By leveraging advanced deep learning algorithms, FloodNet enables precise analysis of disaster impact and affected areas. This paper compares the performance of baseline methods for image classification, semantic segmentation, and visual question answering on our dataset."}, "1905.1062": {"original_text": "  Large-scale face recognition in-the-wild has been recently achieved matured\nperformance in many real work applications. However, such systems are built on\nGPU platforms and mostly deploy heavy deep network architectures. Given a\nhigh-performance heavy network as a teacher, this work presents a simple and\nelegant teacher-student learning paradigm, namely ShrinkTeaNet, to train a\nportable student network that has significantly fewer parameters and\ncompetitive accuracy against the teacher network. Far apart from prior\nteacher-student frameworks mainly focusing on accuracy and compression ratios\nin closed-set problems, our proposed teacher-student network is proved to be\nmore robust against open-set problem, i.e. large-scale face recognition. In\naddition, this work introduces a novel Angular Distillation Loss for distilling\nthe feature direction and the sample distributions of the teacher's hypersphere\nto its student. Then ShrinkTeaNet framework can efficiently guide the student's\nlearning process with the teacher's knowledge presented in both intermediate\nand last stages of the feature embedding. Evaluations on LFW, CFP-FP, AgeDB,\nIJB-B and IJB-C Janus, and MegaFace with one million distractors have\ndemonstrated the efficiency of the proposed approach to learn robust student\nnetworks which have satisfying accuracy and compact sizes. Our ShrinkTeaNet is\nable to support the light-weight architecture achieving high performance with\n99.77% on LFW and 95.64% on large-scale Megaface protocols.\n", "rewritten_text": "\\boxed{\nRecently, large-scale face recognition systems have achieved mature performance in real-world applications, but they rely on heavy deep network architectures and GPU platforms. To address this, we propose a teacher-student learning paradigm called ShrinkTeaNet, which trains a portable student network with significantly fewer parameters while maintaining competitive accuracy with the teacher network. Unlike previous frameworks that focus on accuracy and compression ratios in closed-set problems, our approach is more robust against open-set problems, particularly large-scale face recognition. We introduce a novel Angular Distillation Loss to distill the teacher's knowledge into the student network, guiding its learning process in both intermediate and final stages of feature embedding. Our evaluations on various datasets demonstrate the efficiency of ShrinkTeaNet in learning robust student networks with high accuracy and compact sizes, achieving 99.77% on LFW and 95.64% on large-scale MegaFace protocols.\n}"}, "1601.0763": {"original_text": "  We propose a method that integrates two widely available data sources,\nbuilding footprints from 2D maps and street level images, to derive valuable\ninformation that is generally difficult to acquire -- building heights and\nbuilding facade masks in images. Building footprints are elevated in world\ncoordinates and projected onto images. Building heights are estimated by\nscoring projected footprints based on their alignment with building features in\nimages. Building footprints with estimated heights can be converted to simple\n3D building models, which are projected back to images to identify buildings.\nIn this procedure, accurate camera projections are critical. However, camera\nposition errors inherited from external sensors commonly exist, which adversely\naffect results. We derive a solution to precisely locate cameras on maps using\ncorrespondence between image features and building footprints. Experiments on\nreal-world datasets show the promise of our method.\n", "rewritten_text": "\\boxed{We introduce a novel approach that combines 2D map data and street-level images to extract valuable information, including building heights and facade masks, which are typically hard to obtain. Our method involves elevating building footprints to world coordinates, projecting them onto images, and estimating building heights by scoring the alignment of projected footprints with image features. The resulting 3D building models are then projected back onto images to identify buildings. However, accurate camera projections are crucial, and errors in camera positioning can negatively impact results. To address this, we develop a solution to precisely locate cameras on maps by matching image features with building footprints. Our experiments on real-world datasets demonstrate the effectiveness of our approach.}"}, "2112.12141": {"original_text": "  3D human pose estimation (HPE) in autonomous vehicles (AV) differs from other\nuse cases in many factors, including the 3D resolution and range of data,\nabsence of dense depth maps, failure modes for LiDAR, relative location between\nthe camera and LiDAR, and a high bar for estimation accuracy. Data collected\nfor other use cases (such as virtual reality, gaming, and animation) may\ntherefore not be usable for AV applications. This necessitates the collection\nand annotation of a large amount of 3D data for HPE in AV, which is\ntime-consuming and expensive. In this paper, we propose one of the first\napproaches to alleviate this problem in the AV setting. Specifically, we\npropose a multi-modal approach which uses 2D labels on RGB images as weak\nsupervision to perform 3D HPE. The proposed multi-modal architecture\nincorporates LiDAR and camera inputs with an auxiliary segmentation branch. On\nthe Waymo Open Dataset, our approach achieves a 22% relative improvement over\ncamera-only 2D HPE baseline, and 6% improvement over LiDAR-only model. Finally,\ncareful ablation studies and parts based analysis illustrate the advantages of\neach of our contributions.\n", "rewritten_text": "\\boxed{Estimating 3D human poses in autonomous vehicles is a unique challenge due to factors such as limited 3D resolution, lack of dense depth maps, and high accuracy requirements. Data from other applications like virtual reality or gaming cannot be directly applied, making it necessary to collect and annotate a large amount of 3D data, which is time-consuming and costly. This paper proposes a novel approach to address this issue by using 2D labels on RGB images as weak supervision to perform 3D human pose estimation. The proposed multi-modal architecture combines LiDAR and camera inputs with an auxiliary segmentation branch, achieving a 22% improvement over camera-only 2D human pose estimation and 6% improvement over LiDAR-only models on the Waymo Open Dataset.}"}, "2306.02182": {"original_text": "  Indian court legal texts and processes are essential towards the integrity of\nthe judicial system and towards maintaining the social and political order of\nthe nation. Due to the increase in number of pending court cases, there is an\nurgent need to develop tools to automate many of the legal processes with the\nknowledge of artificial intelligence. In this paper, we employ knowledge\nextraction techniques, specially the named entity extraction of legal entities\nwithin court case judgements. We evaluate several state of the art\narchitectures in the realm of sequence labeling using models trained on a\ncurated dataset of legal texts. We observe that a Bi-LSTM model trained on\nFlair Embeddings achieves the best results, and we also publish the BIO\nformatted dataset as part of this paper.\n", "rewritten_text": "\\boxed{The integrity of India's judicial system and social-political order relies heavily on legal texts and processes. However, the growing backlog of court cases necessitates the development of automation tools leveraging artificial intelligence. This paper focuses on knowledge extraction techniques, specifically named entity extraction of legal entities from court judgments. We assess various state-of-the-art sequence labeling architectures using a curated legal text dataset and find that a Bi-LSTM model with Flair Embeddings yields the best results, which are published alongside this paper in BIO format.}"}, "2011.09563": {"original_text": "  Unsupervised domain adaptation (UDA) is widely used to transfer knowledge\nfrom a labeled source domain to an unlabeled target domain with different data\ndistribution. While extensive studies attested that deep learning models are\nvulnerable to adversarial attacks, the adversarial robustness of models in\ndomain adaptation application has largely been overlooked. This paper points\nout that the inevitable domain distribution deviation in UDA is a critical\nbarrier to model robustness on the target domain. To address the problem, we\npropose a novel Class-consistent Unsupervised Robust Domain Adaptation (CURDA)\nframework for training robust UDA models. With the introduced contrastive\nrobust training and source anchored adversarial contrastive losses, our\nproposed CURDA framework can effectively robustify UDA models by simultaneously\nminimizing the data distribution deviation and the distance between target\ndomain clean-adversarial pairs without creating classification confusion.\nExperiments on several public benchmarks show that CURDA can significantly\nimprove model robustness in the target domain with only minor cost of accuracy\non the clean samples.\n", "rewritten_text": "\\boxed{\nUnsupervised domain adaptation (UDA) is a technique used to transfer knowledge from a labeled source domain to an unlabeled target domain with a different data distribution. However, deep learning models used in UDA are vulnerable to adversarial attacks, which has been largely overlooked. The difference in data distribution between the source and target domains is a major obstacle to achieving robust models in the target domain. To address this issue, we propose a novel framework called Class-consistent Unsupervised Robust Domain Adaptation (CURDA) that trains robust UDA models. Our framework uses contrastive robust training and source anchored adversarial contrastive losses to minimize the difference in data distribution and the distance between clean and adversarial samples in the target domain, without causing classification confusion. Experimental results on several public benchmarks show that CURDA significantly improves model robustness in the target domain with minimal loss of accuracy on clean samples.\n}"}, "1907.00184": {"original_text": "  Since Bahdanau et al. [1] first introduced attention for neural machine\ntranslation, most sequence-to-sequence models made use of attention mechanisms\n[2, 3, 4]. While they produce soft-alignment matrices that could be interpreted\nas alignment between target and source languages, we lack metrics to quantify\ntheir quality, being unclear which approach produces the best alignments. This\npaper presents an empirical evaluation of 3 main sequence-to-sequence models\n(CNN, RNN and Transformer-based) for word discovery from unsegmented phoneme\nsequences. This task consists in aligning word sequences in a source language\nwith phoneme sequences in a target language, inferring from it word\nsegmentation on the target side [5]. Evaluating word segmentation quality can\nbe seen as an extrinsic evaluation of the soft-alignment matrices produced\nduring training. Our experiments in a low-resource scenario on Mboshi and\nEnglish languages (both aligned to French) show that RNNs surprisingly\noutperform CNNs and Transformer for this task. Our results are confirmed by an\nintrinsic evaluation of alignment quality through the use of Average Normalized\nEntropy (ANE). Lastly, we improve our best word discovery model by using an\nalignment entropy confidence measure that accumulates ANE over all the\noccurrences of a given alignment pair in the collection.\n", "rewritten_text": "\\boxed{Since the introduction of attention mechanisms for neural machine translation by Bahdanau et al., most sequence-to-sequence models have utilized these mechanisms. However, there is a lack of metrics to quantify the quality of these attention mechanisms, making it unclear which approach produces the best alignments. This paper presents an empirical evaluation of three main sequence-to-sequence models (CNN, RNN, and Transformer-based) for word discovery from unsegmented phoneme sequences. The task involves aligning word sequences in a source language with phoneme sequences in a target language, and inferring word segmentation on the target side. The quality of the word segmentation can be seen as an extrinsic evaluation of the soft-alignment matrices produced during training. Surprisingly, our experiments show that RNNs outperform CNNs and Transformers in a low-resource scenario on Mboshi and English languages. Our results are confirmed by an intrinsic evaluation of alignment quality using Average Normalized Entropy (ANE). Furthermore, we improve our best word discovery model by using an alignment entropy confidence measure that accumulates ANE over all occurrences of a given alignment pair in the collection.}"}, "1812.09877": {"original_text": "  In multimodal unsupervised image-to-image translation tasks, the goal is to\ntranslate an image from the source domain to many images in the target domain.\nWe present a simple method that produces higher quality images than current\nstate-of-the-art while maintaining the same amount of multimodal diversity.\nPrevious methods follow the unconditional approach of trying to map the latent\ncode directly to a full-size image. This leads to complicated network\narchitectures with several introduced hyperparameters to tune. By treating the\nlatent code as a modifier of the convolutional filters, we produce multimodal\noutput while maintaining the traditional Generative Adversarial Network (GAN)\nloss and without additional hyperparameters. The only tuning required by our\nmethod controls the tradeoff between variability and quality of generated\nimages. Furthermore, we achieve disentanglement between source domain content\nand target domain style for free as a by-product of our formulation. We perform\nqualitative and quantitative experiments showing the advantages of our method\ncompared with the state-of-the art on multiple benchmark image-to-image\ntranslation datasets.\n", "rewritten_text": "\\boxed{\nIn image-to-image translation tasks, the goal is to convert a source image into multiple target images. Our proposed method outperforms current state-of-the-art techniques in terms of image quality while maintaining multimodal diversity. Unlike previous approaches that directly map latent codes to full-size images, our method treats the latent code as a modifier of convolutional filters, producing multimodal outputs with a traditional GAN loss and without additional hyperparameters. Our method requires only one tuning parameter to balance variability and quality of generated images. Additionally, our formulation achieves disentanglement between source domain content and target domain style as a by-product. We demonstrate the advantages of our method through qualitative and quantitative experiments on multiple benchmark datasets.\n}"}, "1807.04047": {"original_text": "  Document image quality assessment (DIQA) is an important and challenging\nproblem in real applications. In order to predict the quality scores of\ndocument images, this paper proposes a novel no-reference DIQA method based on\ncharacter gradient, where the OCR accuracy is used as a ground-truth quality\nmetric. Character gradient is computed on character patches detected with the\nmaximally stable extremal regions (MSER) based method. Character patches are\nessentially significant to character recognition and therefore suitable for use\nin estimating document image quality. Experiments on a benchmark dataset show\nthat the proposed method outperforms the state-of-the-art methods in estimating\nthe quality score of document images.\n", "rewritten_text": "\\boxed{This paper presents a new approach to evaluating the quality of document images without referencing the original document. The proposed method uses character gradient, which is calculated from character patches detected using the Maximally Stable Extremal Regions (MSER) technique, to predict quality scores. The accuracy of Optical Character Recognition (OCR) is used as a benchmark to measure image quality. Experimental results on a standard dataset demonstrate that the proposed method surpasses existing state-of-the-art methods in estimating document image quality.}"}, "2406.01171": {"original_text": "  The concept of persona, originally adopted in dialogue literature, has\nre-surged as a promising framework for tailoring large language models (LLMs)\nto specific context (e.g., personalized search, LLM-as-a-judge). However, the\ngrowing research on leveraging persona in LLMs is relatively disorganized and\nlacks a systematic taxonomy. To close the gap, we present a comprehensive\nsurvey to categorize the current state of the field. We identify two lines of\nresearch, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and\n(2) LLM Personalization, where LLMs take care of user personas. Additionally,\nwe introduce existing methods for LLM personality evaluation. To the best of\nour knowledge, we present the first survey for role-playing and personalization\nin LLMs under the unified view of persona. We continuously maintain a paper\ncollection to foster future endeavors:\nhttps://github.com/MiuLab/PersonaLLM-Survey\n", "rewritten_text": "\\boxed{The concept of persona, initially used in dialogue literature, has seen a resurgence as a promising approach for customizing large language models (LLMs) to specific contexts, such as personalized search and LLM-based judgment. Despite growing research in this area, there is a lack of organization and a systematic taxonomy. To address this gap, we have conducted a comprehensive survey to categorize the current state of the field, identifying two main research directions: (1) assigning personas to LLMs (LLM Role-Playing) and (2) adapting LLMs to user personas (LLM Personalization). We also discuss existing methods for evaluating LLM personality. This survey provides the first unified overview of role-playing and personalization in LLMs from a persona perspective, and we maintain a publicly accessible paper collection to facilitate future research: https://github.com/MiuLab/PersonaLLM-Survey.}"}, "1804.02745": {"original_text": "  Dynamic contrast-enhanced (DCE) MRI is an evolving imaging technique that\nprovides a quantitative measure of pharmacokinetic (PK) parameters in body\ntissues, in which series of T1-weighted images are collected following the\nadministration of a paramagnetic contrast agent. Unfortunately, in many\napplications, conventional clinical DCE-MRI suffers from low spatiotemporal\nresolution and insufficient volume coverage. In this paper, we propose a novel\ndeep learning based approach to directly estimate the PK parameters from\nundersampled DCE-MRI data. Specifically, we design a custom loss function where\nwe incorporate a forward physical model that relates the PK parameters to\ncorrupted image-time series obtained due to subsampling in k-space. This allows\nthe network to directly exploit the knowledge of true contrast agent kinetics\nin the training phase, and hence provide more accurate restoration of PK\nparameters. Experiments on clinical brain DCE datasets demonstrate the efficacy\nof our approach in terms of fidelity of PK parameter reconstruction and\nsignificantly faster parameter inference compared to a model-based iterative\nreconstruction method.\n", "rewritten_text": "\\boxed{A novel deep learning approach is proposed to estimate pharmacokinetic (PK) parameters from undersampled dynamic contrast-enhanced (DCE) MRI data, which typically suffers from low spatiotemporal resolution and limited volume coverage. By incorporating a forward physical model into a custom loss function, the network can leverage knowledge of true contrast agent kinetics to accurately restore PK parameters. Experimental results on clinical brain DCE datasets demonstrate the effectiveness of this approach in terms of PK parameter reconstruction fidelity and significantly faster parameter inference compared to traditional model-based iterative reconstruction methods.}"}, "2207.01164": {"original_text": "  Neural Radiance Field (NeRF) regresses a neural parameterized scene by\ndifferentially rendering multi-view images with ground-truth supervision.\nHowever, when interpolating novel views, NeRF often yields inconsistent and\nvisually non-smooth geometric results, which we consider as a generalization\ngap between seen and unseen views. Recent advances in convolutional neural\nnetworks have demonstrated the promise of advanced robust data augmentations,\neither random or learned, in enhancing both in-distribution and\nout-of-distribution generalization. Inspired by that, we propose Augmented NeRF\n(Aug-NeRF), which for the first time brings the power of robust data\naugmentations into regularizing the NeRF training. Particularly, our proposal\nlearns to seamlessly blend worst-case perturbations into three distinct levels\nof the NeRF pipeline with physical grounds, including (1) the input\ncoordinates, to simulate imprecise camera parameters at image capture; (2)\nintermediate features, to smoothen the intrinsic feature manifold; and (3)\npre-rendering output, to account for the potential degradation factors in the\nmulti-view image supervision. Extensive results demonstrate that Aug-NeRF\neffectively boosts NeRF performance in both novel view synthesis (up to 1.5dB\nPSNR gain) and underlying geometry reconstruction. Furthermore, thanks to the\nimplicit smooth prior injected by the triple-level augmentations, Aug-NeRF can\neven recover scenes from heavily corrupted images, a highly challenging setting\nuntackled before. Our codes are available in\nhttps://github.com/VITA-Group/Aug-NeRF.\n", "rewritten_text": "\\boxed{\nNeural Radiance Field (NeRF) is a technique that uses neural networks to reconstruct a scene from multiple images. However, it often produces inconsistent and visually non-smooth results when generating novel views, which is known as a generalization gap. Inspired by recent advances in convolutional neural networks, we propose Augmented NeRF (Aug-NeRF), which incorporates robust data augmentations into the NeRF training process to improve its performance. Our approach learns to blend worst-case perturbations into three levels of the NeRF pipeline, including input coordinates, intermediate features, and pre-rendering output. Experimental results show that Aug-NeRF significantly improves NeRF's performance in novel view synthesis and geometry reconstruction, and can even recover scenes from heavily corrupted images. Our code is available at https://github.com/VITA-Group/Aug-NeRF.\n}"}, "1511.05296": {"original_text": "  In this paper, we propose a method for ranking fashion images to find the\nones which might be liked by more people. We collect two new datasets from\nimage sharing websites (Pinterest and Polyvore). We represent fashion images\nbased on attributes: semantic attributes and data-driven attributes. To learn\nsemantic attributes from limited training data, we use an algorithm on\nmulti-task convolutional neural networks to share visual knowledge among\ndifferent semantic attribute categories. To discover data-driven attributes\nunsupervisedly, we propose an algorithm to simultaneously discover visual\nclusters and learn fashion-specific feature representations. Given attributes\nas representations, we propose to learn a ranking SPN (sum product networks) to\nrank pairs of fashion images. The proposed ranking SPN can capture the\nhigh-order correlations of the attributes. We show the effectiveness of our\nmethod on our two newly collected datasets.\n", "rewritten_text": "\\boxed{This paper presents a novel approach to rank fashion images based on their likability by a wider audience. We introduce two new datasets gathered from Pinterest and Polyvore, and represent fashion images using both semantic and data-driven attributes. To overcome limited training data, we employ a multi-task convolutional neural network to learn semantic attributes, and propose an unsupervised algorithm to discover data-driven attributes and fashion-specific features. We then use a ranking sum product network to rank fashion image pairs, capturing high-order correlations between attributes. Our method is validated on our two newly collected datasets.}"}, "1901.00049": {"original_text": "  We introduce a new silhouette-based representation for modeling clothed human\nbodies using deep generative models. Our method can reconstruct a complete and\ntextured 3D model of a person wearing clothes from a single input picture.\nInspired by the visual hull algorithm, our implicit representation uses 2D\nsilhouettes and 3D joints of a body pose to describe the immense shape\ncomplexity and variations of clothed people. Given a segmented 2D silhouette of\na person and its inferred 3D joints from the input picture, we first synthesize\nconsistent silhouettes from novel view points around the subject. The\nsynthesized silhouettes which are the most consistent with the input\nsegmentation are fed into a deep visual hull algorithm for robust 3D shape\nprediction. We then infer the texture of the subject's back view using the\nfrontal image and segmentation mask as input to a conditional generative\nadversarial network. Our experiments demonstrate that our silhouette-based\nmodel is an effective representation and the appearance of the back view can be\npredicted reliably using an image-to-image translation network. While classic\nmethods based on parametric models often fail for single-view images of\nsubjects with challenging clothing, our approach can still produce successful\nresults, which are comparable to those obtained from multi-view input.\n", "rewritten_text": "\\boxed{We propose a novel approach to reconstructing 3D models of clothed humans from a single image using deep generative models. Our method leverages 2D silhouettes and 3D joints to capture the complexity of clothed bodies. By synthesizing silhouettes from novel viewpoints and using a deep visual hull algorithm, we can predict the 3D shape of the subject. Additionally, we use a conditional generative adversarial network to infer the texture of the back view from the frontal image and segmentation mask. Our experiments show that our silhouette-based model is effective and can produce reliable results, even for single-view images of subjects with challenging clothing, comparable to those obtained from multi-view input.}"}, "2308.07837": {"original_text": "  In this paper, we present a novel shape reconstruction method leveraging\ndiffusion model to generate 3D sparse point cloud for the object captured in a\nsingle RGB image. Recent methods typically leverage global embedding or local\nprojection-based features as the condition to guide the diffusion model.\nHowever, such strategies fail to consistently align the denoised point cloud\nwith the given image, leading to unstable conditioning and inferior\nperformance. In this paper, we present CCD-3DR, which exploits a novel centered\ndiffusion probabilistic model for consistent local feature conditioning. We\nconstrain the noise and sampled point cloud from the diffusion model into a\nsubspace where the point cloud center remains unchanged during the forward\ndiffusion process and reverse process. The stable point cloud center further\nserves as an anchor to align each point with its corresponding local\nprojection-based features. Extensive experiments on synthetic benchmark\nShapeNet-R2N2 demonstrate that CCD-3DR outperforms all competitors by a large\nmargin, with over 40% improvement. We also provide results on real-world\ndataset Pix3D to thoroughly demonstrate the potential of CCD-3DR in real-world\napplications. Codes will be released soon\n", "rewritten_text": "\\boxed{This paper introduces a novel method, CCD-3DR, for reconstructing 3D shapes from a single RGB image using a diffusion model. Unlike existing methods that rely on global embedding or local projection-based features, CCD-3DR uses a centered diffusion probabilistic model to consistently align the denoised point cloud with the input image. By constraining the noise and sampled point cloud to a subspace, the point cloud center remains stable, serving as an anchor to align each point with its corresponding local features. Experimental results on ShapeNet-R2N2 and Pix3D datasets show that CCD-3DR outperforms existing methods by a significant margin, with over 40% improvement. The code for CCD-3DR will be released soon.}"}, "2312.10610": {"original_text": "  A number of tasks have been proposed recently to facilitate easy access to\ncharts such as chart QA and summarization. The dominant paradigm to solve these\ntasks has been to fine-tune a pretrained model on the task data. However, this\napproach is not only expensive but also not generalizable to unseen tasks. On\nthe other hand, large language models (LLMs) have shown impressive\ngeneralization capabilities to unseen tasks with zero- or few-shot prompting.\nHowever, their application to chart-related tasks is not trivial as these tasks\ntypically involve considering not only the underlying data but also the visual\nfeatures in the chart image. We propose PromptChart, a multimodal few-shot\nprompting framework with LLMs for chart-related applications. By analyzing the\ntasks carefully, we have come up with a set of prompting guidelines for each\ntask to elicit the best few-shot performance from LLMs. We further propose a\nstrategy to inject visual information into the prompts. Our experiments on\nthree different chart-related information consumption tasks show that with\nproperly designed prompts LLMs can excel on the benchmarks, achieving\nstate-of-the-art.\n", "rewritten_text": "\\boxed{Recently, various tasks have been introduced to facilitate easy access to charts, such as chart quality assessment and summarization. While fine-tuning pre-trained models on task data has been the dominant approach, it is costly and lacks generalizability to new tasks. In contrast, large language models (LLMs) have demonstrated impressive generalization capabilities to unseen tasks with minimal prompting. However, applying LLMs to chart-related tasks is challenging due to the need to consider both underlying data and visual features in chart images. To address this, we propose PromptChart, a multimodal few-shot prompting framework that leverages LLMs for chart-related applications. By carefully analyzing tasks, we have developed task-specific prompting guidelines to optimize LLM performance. Additionally, we propose a strategy to incorporate visual information into prompts. Our experiments on three chart-related information consumption tasks demonstrate that LLMs can achieve state-of-the-art results with well-designed prompts.}"}, "1906.03657": {"original_text": "  Group convolution works well with many deep convolutional neural networks\n(CNNs) that can effectively compress the model by reducing the number of\nparameters and computational cost. Using this operation, feature maps of\ndifferent group cannot communicate, which restricts their representation\ncapability. To address this issue, in this work, we propose a novel operation\nnamed Hierarchical Group Convolution (HGC) for creating computationally\nefficient neural networks. Different from standard group convolution which\nblocks the inter-group information exchange and induces the severe performance\ndegradation, HGC can hierarchically fuse the feature maps from each group and\nleverage the inter-group information effectively. Taking advantage of the\nproposed method, we introduce a family of compact networks called HGCNets.\nCompared to networks using standard group convolution, HGCNets have a huge\nimprovement in accuracy at the same model size and complexity level. Extensive\nexperimental results on the CIFAR dataset demonstrate that HGCNets obtain\nsignificant reduction of parameters and computational cost to achieve\ncomparable performance over the prior CNN architectures designed for mobile\ndevices such as MobileNet and ShuffleNet.\n", "rewritten_text": "\\boxed{Group convolution is a technique used in deep convolutional neural networks (CNNs) to reduce the number of parameters and computational cost. However, it has a limitation in that feature maps from different groups cannot communicate with each other, which restricts their representation capability. To overcome this issue, we propose a novel operation called Hierarchical Group Convolution (HGC) that enables the fusion of feature maps from each group, allowing for effective inter-group information exchange. Our proposed method leads to the development of a family of compact networks called HGCNets, which achieve significant improvements in accuracy at the same model size and complexity level compared to standard group convolution-based networks. Experimental results on the CIFAR dataset demonstrate that HGCNets reduce parameters and computational cost while achieving comparable performance to prior CNN architectures designed for mobile devices, such as MobileNet and ShuffleNet.}"}, "2304.01534": {"original_text": "  Bird's eye view (BEV) perception is becoming increasingly important in the\nfield of autonomous driving. It uses multi-view camera data to learn a\ntransformer model that directly projects the perception of the road environment\nonto the BEV perspective. However, training a transformer model often requires\na large amount of data, and as camera data for road traffic are often private,\nthey are typically not shared. Federated learning offers a solution that\nenables clients to collaborate and train models without exchanging data but\nmodel parameters. In this paper, we introduce FedBEVT, a federated transformer\nlearning approach for BEV perception. In order to address two common data\nheterogeneity issues in FedBEVT: (i) diverse sensor poses, and (ii) varying\nsensor numbers in perception systems, we propose two approaches -- Federated\nLearning with Camera-Attentive Personalization (FedCaP) and Adaptive\nMulti-Camera Masking (AMCM), respectively. To evaluate our method in real-world\nsettings, we create a dataset consisting of four typical federated use cases.\nOur findings suggest that FedBEVT outperforms the baseline approaches in all\nfour use cases, demonstrating the potential of our approach for improving BEV\nperception in autonomous driving.\n", "rewritten_text": "\\boxed{The importance of bird's eye view (BEV) perception in autonomous driving is growing, but training transformer models for BEV perception requires large amounts of data, which is often private and not shared. To address this, we propose FedBEVT, a federated learning approach that enables clients to collaborate and train models without sharing data. We also introduce two novel approaches, FedCaP and AMCM, to tackle data heterogeneity issues arising from diverse sensor poses and varying sensor numbers. Our experiments on a real-world dataset demonstrate that FedBEVT outperforms baseline approaches in four typical federated use cases, showcasing its potential to improve BEV perception in autonomous driving.}"}, "1911.10248": {"original_text": "  The rapid development of inexpensive commodity depth sensors has made\nkeypoint detection and matching in the depth image modality an important\nproblem in computer vision. Despite great improvements in recent RGB local\nfeature learning methods, adapting them directly in the depth modality leads to\nunsatisfactory performance. Most of these methods do not explicitly reason\nbeyond the visible pixels in the images. To address the limitations of these\nmethods, we propose a framework ViewSynth, to jointly learn: (1) viewpoint\ninvariant keypoint-descriptor from depth images using a proposed Contrastive\nMatching Loss, and (2) view synthesis of depth images from different viewpoints\nusing the proposed View Synthesis Module and View Synthesis Loss. By learning\nview synthesis, we explicitly encourage the feature extractor to encode\ninformation about not only the visible, but also the occluded parts of the\nscene. We demonstrate that in the depth modality, ViewSynth outperforms the\nstate-of-the-art depth and RGB local feature extraction techniques in the 3D\nkeypoint matching and camera localization tasks on the RGB-D datasets 7-Scenes,\nTUM RGBD and CoRBS in most scenarios. We also show the generalizability of\nViewSynth in 3D keypoint matching across different datasets.\n", "rewritten_text": "\\boxed{The development of affordable depth sensors has made keypoint detection and matching in depth images a crucial problem in computer vision. However, directly applying recent RGB feature learning methods to depth images yields poor results. These methods often fail to consider pixels beyond the visible area. To overcome this limitation, we introduce ViewSynth, a framework that jointly learns viewpoint-invariant keypoint descriptors from depth images using a novel Contrastive Matching Loss, and synthesizes depth images from different viewpoints using the View Synthesis Module and Loss. By learning view synthesis, our approach encourages the feature extractor to capture information about both visible and occluded scene parts. Our experiments demonstrate that ViewSynth outperforms state-of-the-art depth and RGB feature extraction techniques in 3D keypoint matching and camera localization tasks on several RGB-D datasets, and shows strong generalizability across different datasets.}"}, "2211.10018": {"original_text": "  Relation extraction has the potential for large-scale knowledge graph\nconstruction, but current methods do not consider the qualifier attributes for\neach relation triplet, such as time, quantity or location. The qualifiers form\nhyper-relational facts which better capture the rich and complex knowledge\ngraph structure. For example, the relation triplet (Leonard Parker, Educated\nAt, Harvard University) can be factually enriched by including the qualifier\n(End Time, 1967). Hence, we propose the task of hyper-relational extraction to\nextract more specific and complete facts from text. To support the task, we\nconstruct HyperRED, a large-scale and general-purpose dataset. Existing models\ncannot perform hyper-relational extraction as it requires a model to consider\nthe interaction between three entities. Hence, we propose CubeRE, a\ncube-filling model inspired by table-filling approaches and explicitly\nconsiders the interaction between relation triplets and qualifiers. To improve\nmodel scalability and reduce negative class imbalance, we further propose a\ncube-pruning method. Our experiments show that CubeRE outperforms strong\nbaselines and reveal possible directions for future research. Our code and data\nare available at github.com/declare-lab/HyperRED.\n", "rewritten_text": "\\boxed{\nCurrent relation extraction methods have limitations in constructing large-scale knowledge graphs, as they neglect qualifier attributes such as time, quantity, or location that provide more nuanced information. To address this, we introduce the task of hyper-relational extraction, which aims to extract more specific and complete facts from text. We create a large-scale dataset called HyperRED to support this task. Since existing models are incapable of performing hyper-relational extraction, we propose CubeRE, a novel cube-filling model that considers the interactions between relation triplets and qualifiers. Additionally, we develop a cube-pruning method to improve model scalability and mitigate negative class imbalance. Our experiments demonstrate the superiority of CubeRE over strong baselines, highlighting potential avenues for future research. Our code and data are publicly available at github.com/declare-lab/HyperRED.\n}"}, "2210.06246": {"original_text": "  Recently, the community has achieved substantial progress on many commonsense\nreasoning benchmarks. However, it is still unclear what is learned from the\ntraining process: the knowledge, inference capability, or both? We argue that\ndue to the large scale of commonsense knowledge, it is infeasible to annotate a\nlarge enough training set for each task to cover all commonsense for learning.\nThus we should separate the commonsense knowledge acquisition and inference\nover commonsense knowledge as two separate tasks. In this work, we focus on\ninvestigating models' commonsense inference capabilities from two perspectives:\n(1) Whether models can know if the knowledge they have is enough to solve the\ntask; (2) Whether models can develop commonsense inference capabilities that\ngeneralize across commonsense tasks. We first align commonsense tasks with\nrelevant knowledge from commonsense knowledge bases and ask humans to annotate\nwhether the knowledge is enough or not. Then, we convert different commonsense\ntasks into a unified question answering format to evaluate models'\ngeneralization capabilities. We name the benchmark as Commonsense Inference\nwith Knowledge-in-the-loop Question Answering (CIKQA).\n", "rewritten_text": "\\boxed{\nRecently, significant progress has been made on commonsense reasoning benchmarks, but it remains unclear whether models learn knowledge, inference capabilities, or both during training. Due to the vast scale of commonsense knowledge, it is impractical to annotate a large enough training set to cover all commonsense for learning. Therefore, we propose separating commonsense knowledge acquisition and inference over commonsense knowledge into two distinct tasks. This work focuses on investigating models' commonsense inference capabilities from two angles: (1) whether models can recognize when they have sufficient knowledge to solve a task, and (2) whether models can develop generalizable commonsense inference capabilities across tasks. To achieve this, we align commonsense tasks with relevant knowledge from commonsense knowledge bases, annotate whether the knowledge is sufficient, and convert tasks into a unified question answering format to evaluate models' generalization capabilities, which we term Commonsense Inference with Knowledge-in-the-loop Question Answering (CIKQA).\n}"}, "2409.11819": {"original_text": "  6D object pose estimation is the problem of identifying the position and\norientation of an object relative to a chosen coordinate system, which is a\ncore technology for modern XR applications. State-of-the-art 6D object pose\nestimators directly predict an object pose given an object observation. Due to\nthe ill-posed nature of the pose estimation problem, where multiple different\nposes can correspond to a single observation, generating additional plausible\nestimates per observation can be valuable. To address this, we reformulate the\nstate-of-the-art algorithm GDRNPP and introduce EPRO-GDR (End-to-End\nProbabilistic Geometry-Guided Regression). Instead of predicting a single pose\nper detection, we estimate a probability density distribution of the pose.\nUsing the evaluation procedure defined by the BOP (Benchmark for 6D Object Pose\nEstimation) Challenge, we test our approach on four of its core datasets and\ndemonstrate superior quantitative results for EPRO-GDR on LM-O, YCB-V, and\nITODD. Our probabilistic solution shows that predicting a pose distribution\ninstead of a single pose can improve state-of-the-art single-view pose\nestimation while providing the additional benefit of being able to sample\nmultiple meaningful pose candidates.\n", "rewritten_text": "\\boxed{Estimating the 6D object pose, a crucial technology for extended reality (XR) applications, involves identifying an object's position and orientation in a coordinate system. While current state-of-the-art methods predict a single pose from an object observation, we propose a novel approach, EPRO-GDR, which estimates a probability distribution of possible poses. By doing so, we can generate multiple plausible pose estimates from a single observation, addressing the inherent ambiguity of the pose estimation problem. Our method outperforms existing algorithms on several benchmark datasets, providing not only improved pose estimation but also the ability to sample multiple meaningful pose candidates.}"}, "2405.18111": {"original_text": "  Large language models (LLMs) are proven to benefit a lot from\nretrieval-augmented generation (RAG) in alleviating hallucinations confronted\nwith knowledge-intensive questions. RAG adopts information retrieval techniques\nto inject external knowledge from semantic-relevant documents as input\ncontexts. However, since today's Internet is flooded with numerous noisy and\nfabricating content, it is inevitable that RAG systems are vulnerable to these\nnoises and prone to respond incorrectly. To this end, we propose to optimize\nthe retrieval-augmented Generator with an Adversarial Tuning Multi-agent system\n(ATM). The ATM steers the Generator to have a robust perspective of useful\ndocuments for question answering with the help of an auxiliary Attacker agent\nthrough adversarially tuning the agents for several iterations. After rounds of\nmulti-agent iterative tuning, the Generator can eventually better discriminate\nuseful documents amongst fabrications. The experimental results verify the\neffectiveness of ATM and we also observe that the Generator can achieve better\nperformance compared to the state-of-the-art baselines.\n", "rewritten_text": "\\boxed{Large language models can greatly benefit from retrieval-augmented generation (RAG) when faced with knowledge-intensive questions, as it injects relevant external knowledge from documents into the input context. However, the abundance of noisy and fabricated online content makes RAG systems prone to errors. To address this, we propose an Adversarial Tuning Multi-agent system (ATM) that optimizes the generator by pitting it against an auxiliary Attacker agent. Through iterative tuning, the generator learns to distinguish useful documents from fabrications, leading to improved performance. Our experiments confirm the effectiveness of ATM, outperforming state-of-the-art baselines.}"}, "2309.00468": {"original_text": "  Dietary assessment is essential to maintaining a healthy lifestyle. Automatic\nimage-based dietary assessment is a growing field of research due to the\nincreasing prevalence of image capturing devices (e.g. mobile phones). In this\nwork, we estimate food energy from a single monocular image, a difficult task\ndue to the limited hard-to-extract amount of energy information present in an\nimage. To do so, we employ an improved encoder-decoder framework for energy\nestimation; the encoder transforms the image into a representation embedded\nwith food energy information in an easier-to-extract format, which the decoder\nthen extracts the energy information from. To implement our method, we compile\na high-quality food image dataset verified by registered dietitians containing\neating scene images, food-item segmentation masks, and ground truth calorie\nvalues. Our method improves upon previous caloric estimation methods by over\n10\\% and 30 kCal in terms of MAPE and MAE respectively.\n", "rewritten_text": "\\boxed{Accurately assessing dietary intake is crucial for a healthy lifestyle. With the widespread use of image-capturing devices, automatic image-based dietary assessment has become a promising area of research. This study focuses on estimating food energy from a single image, a challenging task due to the limited energy information present in the image. To overcome this, we developed an enhanced encoder-decoder framework that transforms the image into a format that makes energy information easier to extract. We also compiled a high-quality dataset of food images, verified by registered dietitians, which includes eating scene images, food-item segmentation masks, and accurate calorie values. Our method outperforms previous caloric estimation methods, achieving a 10% improvement in mean absolute percentage error (MAPE) and a 30 kCal improvement in mean absolute error (MAE).}"}, "2411.08196": {"original_text": "  Diffusion Transformers (DiTs) have recently achieved remarkable success in\ntext-guided image generation. In image editing, DiTs project text and image\ninputs to a joint latent space, from which they decode and synthesize new\nimages. However, it remains largely unexplored how multimodal information\ncollectively forms this joint space and how they guide the semantics of the\nsynthesized images. In this paper, we investigate the latent space of DiT\nmodels and uncover two key properties: First, DiT's latent space is inherently\nsemantically disentangled, where different semantic attributes can be\ncontrolled by specific editing directions. Second, consistent semantic editing\nrequires utilizing the entire joint latent space, as neither encoded image nor\ntext alone contains enough semantic information. We show that these editing\ndirections can be obtained directly from text prompts, enabling precise\nsemantic control without additional training or mask annotations. Based on\nthese insights, we propose a simple yet effective Encode-Identify-Manipulate\n(EIM) framework for zero-shot fine-grained image editing. Specifically, we\nfirst encode both the given source image and the text prompt that describes the\nimage, to obtain the joint latent embedding. Then, using our proposed Hessian\nScore Distillation Sampling (HSDS) method, we identify editing directions that\ncontrol specific target attributes while preserving other image features. These\ndirections are guided by text prompts and used to manipulate the latent\nembeddings. Moreover, we propose a new metric to quantify the disentanglement\ndegree of the latent space of diffusion models. Extensive experiment results on\nour new curated benchmark dataset and analysis demonstrate DiT's\ndisentanglement properties and effectiveness of the EIM framework.\n", "rewritten_text": "\\boxed{\nRecently, Diffusion Transformers (DiTs) have achieved significant success in generating images guided by text. In image editing, DiTs combine text and image inputs into a shared latent space, from which new images are generated. However, it is not well understood how the multimodal information in this joint space influences the semantics of the generated images. This paper investigates the latent space of DiT models and discovers two key properties: 1) the latent space is semantically disentangled, allowing specific editing directions to control different attributes, and 2) both image and text inputs are necessary for consistent semantic editing. We show that these editing directions can be directly obtained from text prompts, enabling precise control without additional training or annotations. Based on these insights, we propose a simple and effective framework for zero-shot fine-grained image editing, which involves encoding the source image and text prompt, identifying editing directions using our proposed HSDS method, and manipulating the latent embeddings. We also introduce a new metric to measure the disentanglement of the latent space in diffusion models. Our experiments on a new benchmark dataset demonstrate the disentanglement properties of DiTs and the effectiveness of our proposed framework."}, "2403.06444": {"original_text": "  Estimating reliable geometric model parameters from the data with severe\noutliers is a fundamental and important task in computer vision. This paper\nattempts to sample high-quality subsets and select model instances to estimate\nparameters in the multi-structural data. To address this, we propose an\neffective method called Latent Semantic Consensus (LSC). The principle of LSC\nis to preserve the latent semantic consensus in both data points and model\nhypotheses. Specifically, LSC formulates the model fitting problem into two\nlatent semantic spaces based on data points and model hypotheses, respectively.\nThen, LSC explores the distributions of points in the two latent semantic\nspaces, to remove outliers, generate high-quality model hypotheses, and\neffectively estimate model instances. Finally, LSC is able to provide\nconsistent and reliable solutions within only a few milliseconds for general\nmulti-structural model fitting, due to its deterministic fitting nature and\nefficiency. Compared with several state-of-the-art model fitting methods, our\nLSC achieves significant superiority for the performance of both accuracy and\nspeed on synthetic data and real images. The code will be available at\nhttps://github.com/guobaoxiao/LSC.\n", "rewritten_text": "\\boxed{\nEstimating geometric model parameters from data with severe outliers is a crucial task in computer vision. This paper proposes a novel method, Latent Semantic Consensus (LSC), to tackle this challenge. LSC preserves the latent semantic consensus between data points and model hypotheses by formulating the model fitting problem into two latent semantic spaces. It then explores the distributions of points in these spaces to remove outliers, generate high-quality model hypotheses, and estimate model instances effectively. LSC provides consistent and reliable solutions quickly, outperforming state-of-the-art methods in terms of accuracy and speed on both synthetic data and real images. The code is available at https://github.com/guobaoxiao/LSC.\n}"}, "1708.0923": {"original_text": "  Named Entity Recognition and Disambiguation (NERD) systems have recently been\nwidely researched to deal with the significant growth of the Web. NERD systems\nare crucial for several Natural Language Processing (NLP) tasks such as\nsummarization, understanding, and machine translation. However, there is no\nstandard interface specification, i.e. these systems may vary significantly\neither for exporting their outputs or for processing the inputs. Thus, when a\ngiven company desires to implement more than one NERD system, the process is\nquite exhaustive and prone to failure. In addition, industrial solutions demand\ncritical requirements, e.g., large-scale processing, completeness, versatility,\nand licenses. Commonly, these requirements impose a limitation, making good\nNERD models to be ignored by companies. This paper presents TANKER, a\ndistributed architecture which aims to overcome scalability, reliability and\nfailure tolerance limitations related to industrial needs by combining NERD\nsystems. To this end, TANKER relies on a micro-services oriented architecture,\nwhich enables agile development and delivery of complex enterprise\napplications. In addition, TANKER provides a standardized API which makes\npossible to combine several NERD systems at once.\n", "rewritten_text": "\\boxed{\nNamed Entity Recognition and Disambiguation (NERD) systems have gained significant attention in recent years to address the rapid growth of the web. These systems are essential for various Natural Language Processing (NLP) tasks, including summarization, understanding, and machine translation. However, the lack of a standard interface specification hinders their integration, making it challenging for companies to implement multiple NERD systems. This limitation is further exacerbated by industrial requirements such as large-scale processing, completeness, versatility, and licensing. To overcome these limitations, this paper proposes TANKER, a distributed architecture that combines multiple NERD systems to achieve scalability, reliability, and fault tolerance. TANKER's micro-services oriented architecture enables agile development and delivery of complex enterprise applications, while its standardized API facilitates the integration of multiple NERD systems.\n}"}, "1708.01846": {"original_text": "  Low-rank decomposition (LRD) is a state-of-the-art method for visual data\nreconstruction and modelling. However, it is a very challenging problem when\nthe image data contains significant occlusion, noise, illumination variation,\nand misalignment from rotation or viewpoint changes. We leverage the specific\nstructure of data in order to improve the performance of LRD when the data are\nnot ideal. To this end, we propose a new framework that embeds manifold priors\ninto LRD. To implement the framework, we design an alternating direction method\nof multipliers (ADMM) method which efficiently integrates the manifold\nconstraints during the optimization process. The proposed approach is\nsuccessfully used to calculate low-rank models from face images, hand-written\ndigits and planar surface images. The results show a consistent increase of\nperformance when compared to the state-of-the-art over a wide range of\nrealistic image misalignments and corruptions.\n", "rewritten_text": "\\boxed{Low-rank decomposition (LRD) is a powerful tool for visual data reconstruction and modeling, but it struggles with imperfect data, such as images with occlusion, noise, varying illumination, and misalignment due to rotation or viewpoint changes. To overcome these limitations, we developed a new framework that incorporates manifold priors into LRD, which is implemented using an efficient alternating direction method of multipliers (ADMM). Our approach successfully generated low-rank models from various image types, including faces, handwritten digits, and planar surfaces, and consistently outperformed state-of-the-art methods across a range of realistic image corruptions and misalignments.}"}, "2001.0732": {"original_text": "  It's natural these days for people to know the local events from massive\ndocuments. Many texts contain location information, such as city name or road\nname, which is always incomplete or latent. It's significant to extract the\nadministrative area of the text and organize the hierarchy of area, called\nlocation normalization. Existing detecting location systems either exclude\nhierarchical normalization or present only a few specific regions. We propose a\nsystem named ROIBase that normalizes the text by the Chinese hierarchical\nadministrative divisions. ROIBase adopts a co-occurrence constraint as the\nbasic framework to score the hit of the administrative area, achieves the\ninference by special embeddings, and expands the recall by the ROI (region of\ninterest). It has high efficiency and interpretability because it mainly\nestablishes on the definite knowledge and has less complex logic than the\nsupervised models. We demonstrate that ROIBase achieves better performance\nagainst feasible solutions and is useful as a strong support system for\nlocation normalization.\n", "rewritten_text": "\\boxed{In today's digital age, it's common for people to learn about local events from large documents. However, these texts often contain incomplete or implicit location information, such as city or road names. To address this, it's crucial to extract and organize the administrative areas mentioned in the text, a process known as location normalization. Existing systems for detecting locations either neglect this hierarchical normalization or only focus on specific regions. Our proposed system, ROIBase, normalizes text based on China's hierarchical administrative divisions. By using a co-occurrence constraint framework, special embeddings for inference, and ROI (region of interest) to expand recall, ROIBase achieves high efficiency and interpretability. Our system outperforms existing solutions and serves as a reliable support system for location normalization.}"}, "1908.02505": {"original_text": "  This paper presents a scoring system that has shown the top result on the\ntext subset of CALL v3 shared task. The presented system is based on text\nembeddings, namely NNLM~\\cite{nnlm} and BERT~\\cite{Bert}. The distinguishing\nfeature of the given approach is that it does not rely on the reference grammar\nfile for scoring. The model is compared against approaches that use the grammar\nfile and proves the possibility to achieve similar and even higher results\nwithout a predefined set of correct answers.\n  The paper describes the model itself and the data preparation process that\nplayed a crucial role in the model training.\n", "rewritten_text": "\\boxed{This paper introduces a novel scoring system that achieved the highest score on the text subset of the CALL v3 shared task. Unlike other approaches, our system leverages text embeddings, specifically NNLM and BERT, without relying on a reference grammar file for scoring. We demonstrate that comparable or even superior results can be obtained without predefining correct answers. The paper provides a detailed description of the model and the data preparation process, which was instrumental in training the model.}"}, "1604.0801": {"original_text": "  The purpose of this paper is the detection of salient areas in natural video\nby using the new deep learning techniques. Salient patches in video frames are\npredicted first. Then the predicted visual fixation maps are built upon them.\nWe design the deep architecture on the basis of CaffeNet implemented with Caffe\ntoolkit. We show that changing the way of data selection for optimisation of\nnetwork parameters, we can save computation cost up to 12 times. We extend deep\nlearning approaches for saliency prediction in still images with RGB values to\nspecificity of video using the sensitivity of the human visual system to\nresidual motion. Furthermore, we complete primary colour pixel values by\ncontrast features proposed in classical visual attention prediction models. The\nexperiments are conducted on two publicly available datasets. The first is\nIRCCYN video database containing 31 videos with an overall amount of 7300\nframes and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided\n2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, the\naccuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction of\nsaliency of patches show the improvement up to 2% with regard to RGB use only.\nThe resulting accuracy of 76, 6% is obtained. The AUC metric in comparison of\npredicted saliency maps with visual fixation maps shows the increase up to 16%\non a sample of video clips from this dataset.\n", "rewritten_text": "\\boxed{\nThis paper aims to detect salient areas in natural videos using deep learning techniques. The approach involves predicting salient patches in video frames and then constructing visual fixation maps. A deep architecture based on CaffeNet is designed and implemented using the Caffe toolkit. By modifying the data selection method for optimizing network parameters, the computational cost can be reduced by up to 12 times. The paper extends deep learning approaches for saliency prediction in still images to video by incorporating the human visual system's sensitivity to residual motion. Additionally, contrast features from classical visual attention models are used to complement primary color pixel values. Experiments are conducted on two publicly available datasets, IRCCYN and HOLLYWOOD2, with results showing an accuracy of 89.51% on IRCCYN and 76.6% on HOLLYWOOD2. The Area Under the Curve (AUC) metric also shows an improvement of up to 16% when comparing predicted saliency maps with visual fixation maps."}, "2301.03182": {"original_text": "  Existing deep learning-based shadow removal methods still produce images with\nshadow remnants. These shadow remnants typically exist in homogeneous regions\nwith low-intensity values, making them untraceable in the existing\nimage-to-image mapping paradigm. We observe that shadows mainly degrade images\nat the image-structure level (in which humans perceive object shapes and\ncontinuous colors). Hence, in this paper, we propose to remove shadows at the\nimage structure level. Based on this idea, we propose a novel\nstructure-informed shadow removal network (StructNet) to leverage the\nimage-structure information to address the shadow remnant problem.\nSpecifically, StructNet first reconstructs the structure information of the\ninput image without shadows and then uses the restored shadow-free structure\nprior to guiding the image-level shadow removal. StructNet contains two main\nnovel modules: (1) a mask-guided shadow-free extraction (MSFE) module to\nextract image structural features in a non-shadow-to-shadow directional manner,\nand (2) a multi-scale feature & residual aggregation (MFRA) module to leverage\nthe shadow-free structure information to regularize feature consistency. In\naddition, we also propose to extend StructNet to exploit multi-level structure\ninformation (MStructNet), to further boost the shadow removal performance with\nminimum computational overheads. Extensive experiments on three shadow removal\nbenchmarks demonstrate that our method outperforms existing shadow removal\nmethods, and our StructNet can be integrated with existing methods to improve\nthem further.\n", "rewritten_text": "\\boxed{\nCurrent deep learning-based methods for removing shadows from images still leave behind residual shadows, which are particularly difficult to detect in areas with low-intensity values. We propose a new approach that targets the removal of shadows at the level of image structure, where humans perceive object shapes and colors. Our novel network, StructNet, leverages image structure information to address the problem of residual shadows. It first reconstructs the structure of the input image without shadows and then uses this information to guide the removal of shadows at the image level. StructNet consists of two key modules: a mask-guided shadow-free extraction module that extracts structural features in a non-shadow-to-shadow direction, and a multi-scale feature and residual aggregation module that ensures feature consistency. We also extend StructNet to utilize multi-level structure information, resulting in improved performance with minimal additional computational cost. Our experiments demonstrate that our method outperforms existing shadow removal methods and can be integrated with them to further improve their performance.\n}"}, "1709.0586": {"original_text": "  We present a novel method for cell segmentation in microscopy images which is\ninspired by the Generative Adversarial Neural Network (GAN) approach. Our\nframework is built on a pair of two competitive artificial neural networks,\nwith a unique architecture, termed Rib Cage, which are trained simultaneously\nand together define a min-max game resulting in an accurate segmentation of a\ngiven image. Our approach has two main strengths, similar to the GAN, the\nmethod does not require a formulation of a loss function for the optimization\nprocess. This allows training on a limited amount of annotated data in a weakly\nsupervised manner. Promising segmentation results on real fluorescent\nmicroscopy data are presented. The code is freely available at:\nhttps://github.com/arbellea/DeepCellSeg.git\n", "rewritten_text": "\\boxed{We introduce a novel cell segmentation method for microscopy images, inspired by Generative Adversarial Neural Networks (GANs). Our Rib Cage framework consists of two competing neural networks, trained simultaneously to define a min-max game, resulting in accurate image segmentation. Like GANs, our approach doesn't require a predefined loss function, enabling weakly supervised training with limited annotated data. We demonstrate promising results on real fluorescent microscopy data, and provide open-source code at https://github.com/arbellea/DeepCellSeg.git.}"}, "2203.14565": {"original_text": "  Domain adaptation (DA) or domain generalization (DG) for face presentation\nattack detection (PAD) has attracted attention recently with its robustness\nagainst unseen attack scenarios. Existing DA/DG-based PAD methods, however,\nhave not yet fully explored the domain-specific style information that can\nprovide knowledge regarding attack styles (e.g., materials, background,\nillumination and resolution). In this paper, we introduce a novel Style-Guided\nDomain Adaptation (SGDA) framework for inference-time adaptive PAD.\nSpecifically, Style-Selective Normalization (SSN) is proposed to explore the\ndomain-specific style information within the high-order feature statistics. The\nproposed SSN enables the adaptation of the model to the target domain by\nreducing the style difference between the target and the source domains.\nMoreover, we carefully design Style-Aware Meta-Learning (SAML) to boost the\nadaptation ability, which simulates the inference-time adaptation with style\nselection process on virtual test domain. In contrast to previous domain\nadaptation approaches, our method does not require either additional auxiliary\nmodels (e.g., domain adaptors) or the unlabeled target domain during training,\nwhich makes our method more practical to PAD task. To verify our experiments,\nwe utilize the public datasets: MSU-MFSD, CASIA-FASD, OULU-NPU and Idiap\nREPLAYATTACK. In most assessments, the result demonstrates a notable gap of\nperformance compared to the conventional DA/DG-based PAD methods.\n", "rewritten_text": "\\boxed{\nRecently, domain adaptation (DA) and domain generalization (DG) have gained attention for face presentation attack detection (PAD) due to their robustness against unseen attack scenarios. However, existing DA/DG-based PAD methods have not fully utilized domain-specific style information, which can provide insights into attack styles such as materials, background, illumination, and resolution. To address this, we propose a novel Style-Guided Domain Adaptation (SGDA) framework for adaptive PAD at inference time. Our framework features Style-Selective Normalization (SSN), which explores domain-specific style information within high-order feature statistics, and Style-Aware Meta-Learning (SAML), which enhances adaptation ability through style selection on virtual test domains. Unlike previous DA approaches, our method does not require additional auxiliary models or unlabeled target domains during training, making it more practical for PAD tasks. Experimental results on public datasets (MSU-MFSD, CASIA-FASD, OULU-NPU, and Idiap REPLAYATTACK) demonstrate a significant performance gap compared to conventional DA/DG-based PAD methods.\n}"}, "1905.08511": {"original_text": "  Question answering (QA) using textual sources for purposes such as reading\ncomprehension (RC) has attracted much attention. This study focuses on the task\nof explainable multi-hop QA, which requires the system to return the answer\nwith evidence sentences by reasoning and gathering disjoint pieces of the\nreference texts. It proposes the Query Focused Extractor (QFE) model for\nevidence extraction and uses multi-task learning with the QA model. QFE is\ninspired by extractive summarization models; compared with the existing method,\nwhich extracts each evidence sentence independently, it sequentially extracts\nevidence sentences by using an RNN with an attention mechanism on the question\nsentence. It enables QFE to consider the dependency among the evidence\nsentences and cover important information in the question sentence.\nExperimental results show that QFE with a simple RC baseline model achieves a\nstate-of-the-art evidence extraction score on HotpotQA. Although designed for\nRC, it also achieves a state-of-the-art evidence extraction score on FEVER,\nwhich is a recognizing textual entailment task on a large textual database.\n", "rewritten_text": "\\boxed{This study explores the task of explainable multi-hop question answering, which involves providing answers along with supporting evidence sentences from reference texts. The proposed Query Focused Extractor (QFE) model uses a recurrent neural network with attention to sequentially extract evidence sentences, considering their dependencies and importance in the question. Experimental results demonstrate that QFE, combined with a simple reading comprehension baseline model, achieves state-of-the-art evidence extraction scores on both HotpotQA and FEVER datasets.}"}, "2310.18205": {"original_text": "  Claim span identification (CSI) is an important step in fact-checking\npipelines, aiming to identify text segments that contain a checkworthy claim or\nassertion in a social media post. Despite its importance to journalists and\nhuman fact-checkers, it remains a severely understudied problem, and the scarce\nresearch on this topic so far has only focused on English. Here we aim to\nbridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K\nreal-world claims collected from numerous social media platforms in five Indian\nlanguages and English. We report strong baselines with state-of-the-art\nencoder-only language models (e.g., XLM-R) and we demonstrate the benefits of\ntraining on multiple languages over alternative cross-lingual transfer methods\nsuch as zero-shot transfer, or training on translated data, from a\nhigh-resource language such as English. We evaluate generative large language\nmodels from the GPT series using prompting methods on the X-CLAIM dataset and\nwe find that they underperform the smaller encoder-only language models for\nlow-resource languages.\n", "rewritten_text": "\\boxed{Identifying claims in social media posts, known as Claim Span Identification (CSI), is a crucial step in fact-checking pipelines. Despite its significance, CSI has received little attention, with existing research limited to English. To address this gap, we introduce X-CLAIM, a novel dataset of 7,000 real-world claims from multiple social media platforms in five Indian languages and English. Our experiments show that training on multiple languages with state-of-the-art encoder-only language models, such as XLM-R, outperforms alternative methods like zero-shot transfer or training on translated data from English. Additionally, we find that generative large language models from the GPT series, when used with prompting methods, underperform smaller encoder-only language models for low-resource languages.}"}, "2306.08075": {"original_text": "  Current knowledge distillation approaches in semantic segmentation tend to\nadopt a holistic approach that treats all spatial locations equally. However,\nfor dense prediction, students' predictions on edge regions are highly\nuncertain due to contextual information leakage, requiring higher spatial\nsensitivity knowledge than the body regions. To address this challenge, this\npaper proposes a novel approach called boundary-privileged knowledge\ndistillation (BPKD). BPKD distills the knowledge of the teacher model's body\nand edges separately to the compact student model. Specifically, we employ two\ndistinct loss functions: (i) edge loss, which aims to distinguish between\nambiguous classes at the pixel level in edge regions; (ii) body loss, which\nutilizes shape constraints and selectively attends to the inner-semantic\nregions. Our experiments demonstrate that the proposed BPKD method provides\nextensive refinements and aggregation for edge and body regions. Additionally,\nthe method achieves state-of-the-art distillation performance for semantic\nsegmentation on three popular benchmark datasets, highlighting its\neffectiveness and generalization ability. BPKD shows consistent improvements\nacross a diverse array of lightweight segmentation structures, including both\nCNNs and transformers, underscoring its architecture-agnostic adaptability. The\ncode is available at \\url{https://github.com/AkideLiu/BPKD}.\n", "rewritten_text": "\\boxed{\nCurrent knowledge distillation methods for semantic segmentation typically treat all spatial locations equally, but this approach is limited because student models are highly uncertain in edge regions due to contextual information leakage. To overcome this challenge, we propose a novel approach called boundary-privileged knowledge distillation (BPKD), which separately distills the knowledge of the teacher model's body and edges to the compact student model. Our method uses two distinct loss functions: edge loss, which focuses on distinguishing between ambiguous classes in edge regions, and body loss, which utilizes shape constraints and selectively attends to inner-semantic regions. Our experiments show that BPKD provides significant refinements and aggregation for edge and body regions, achieving state-of-the-art distillation performance on three popular benchmark datasets. Moreover, BPKD consistently improves performance across various lightweight segmentation structures, including CNNs and transformers, demonstrating its architecture-agnostic adaptability. The code is available at https://github.com/AkideLiu/BPKD.\n}"}, "2010.05762": {"original_text": "  Scene flow represents the 3D motion of every point in the dynamic\nenvironments. Like the optical flow that represents the motion of pixels in 2D\nimages, 3D motion representation of scene flow benefits many applications, such\nas autonomous driving and service robot. This paper studies the problem of\nscene flow estimation from two consecutive 3D point clouds. In this paper, a\nnovel hierarchical neural network with double attention is proposed for\nlearning the correlation of point features in adjacent frames and refining\nscene flow from coarse to fine layer by layer. The proposed network has a new\nmore-for-less hierarchical architecture. The more-for-less means that the\nnumber of input points is greater than the number of output points for scene\nflow estimation, which brings more input information and balances the precision\nand resource consumption. In this hierarchical architecture, scene flow of\ndifferent levels is generated and supervised respectively. A novel attentive\nembedding module is introduced to aggregate the features of adjacent points\nusing a double attention method in a patch-to-patch manner. The proper layers\nfor flow embedding and flow supervision are carefully considered in our network\ndesignment. Experiments show that the proposed network outperforms the\nstate-of-the-art performance of 3D scene flow estimation on the FlyingThings3D\nand KITTI Scene Flow 2015 datasets. We also apply the proposed network to\nrealistic LiDAR odometry task, which is an key problem in autonomous driving.\nThe experiment results demonstrate that our proposed network can outperform the\nICP-based method and shows the good practical application ability.\n", "rewritten_text": "\\boxed{\nScene flow, which represents the 3D motion of points in dynamic environments, has numerous applications, including autonomous driving and service robots. This paper tackles the problem of estimating scene flow from consecutive 3D point clouds. A novel hierarchical neural network with double attention is proposed to learn point feature correlations and refine scene flow from coarse to fine. The network's more-for-less architecture balances precision and resource consumption by using more input points than output points. The attentive embedding module aggregates adjacent point features using double attention, and the network's design carefully considers flow embedding and supervision layers. Experiments on FlyingThings3D and KITTI Scene Flow 2015 datasets show that the proposed network outperforms state-of-the-art methods, and its application to LiDAR odometry tasks demonstrates its practical potential."}, "2303.14829": {"original_text": "  Generating grammatically and semantically correct captions in video\ncaptioning is a challenging task. The captions generated from the existing\nmethods are either word-by-word that do not align with grammatical structure or\nmiss key information from the input videos. To address these issues, we\nintroduce a novel global-local fusion network, with a Global-Local Fusion Block\n(GLFB) that encodes and fuses features from different parts of speech (POS)\ncomponents with visual-spatial features. We use novel combinations of different\nPOS components - 'determinant + subject', 'auxiliary verb', 'verb', and\n'determinant + object' for supervision of the POS blocks - Det + Subject, Aux\nVerb, Verb, and Det + Object respectively. The novel global-local fusion\nnetwork together with POS blocks helps align the visual features with language\ndescription to generate grammatically and semantically correct captions.\nExtensive qualitative and quantitative experiments on benchmark MSVD and MSRVTT\ndatasets demonstrate that the proposed approach generates more grammatically\nand semantically correct captions compared to the existing methods, achieving\nthe new state-of-the-art. Ablations on the POS blocks and the GLFB demonstrate\nthe impact of the contributions on the proposed method.\n", "rewritten_text": "\\boxed{The task of generating accurate and grammatically correct video captions is a significant challenge. Existing methods often produce captions that are either word-by-word translations or omit crucial information from the input videos. To overcome these limitations, we propose a novel global-local fusion network that combines features from different parts of speech (POS) with visual-spatial features. Our approach utilizes innovative combinations of POS components, including 'determinant + subject', 'auxiliary verb', 'verb', and 'determinant + object', to supervise the POS blocks. The resulting captions are more grammatically and semantically correct, as demonstrated by extensive experiments on benchmark datasets MSVD and MSRVTT. Our approach achieves state-of-the-art results, and ablation studies on the POS blocks and global-local fusion block further highlight the contributions of our method.}"}, "1805.11788": {"original_text": "  Traditional Convolutional Neural Networks (CNNs) typically use the same\nactivation function (usually ReLU) for all neurons with non-linear mapping\noperations. For example, the deep convolutional architecture Inception-v4 uses\nReLU. To improve the classification performance of traditional CNNs, a new\n\"Multi-function Convolutional Neural Network\" (MCNN) is created by using\ndifferent activation functions for different neurons. For $n$ neurons and $m$\ndifferent activation functions, there are a total of $m^n-m$ MCNNs and only $m$\ntraditional CNNs. Therefore, the best model is very likely to be chosen from\nMCNNs because there are $m^n-2m$ more MCNNs than traditional CNNs. For\nperformance analysis, two different datasets for two applications (classifying\nhandwritten digits from the MNIST database and classifying brain MRI images\ninto one of the four stages of Alzheimer's disease (AD)) are used. For both\napplications, an activation function is randomly selected for each layer of a\nMCNN. For the AD diagnosis application, MCNNs using a newly created\nmulti-function Inception-v4 architecture are constructed. Overall, simulations\nshow that MCNNs can outperform traditional CNNs in terms of multi-class\nclassification accuracy for both applications. An important future research\nwork will be to efficiently select the best MCNN from $m^n-m$ candidate MCNNs.\nCurrent CNN software only provides users with partial functionality of MCNNs\nsince different layers can use different activation functions but not\nindividual neurons in the same layer. Thus, modifying current CNN software\nsystems such as ResNets, DenseNets, and Dual Path Networks by using multiple\nactivation functions and developing more effective and faster MCNN software\nsystems and tools would be very useful to solve difficult practical image\nclassification problems.\n", "rewritten_text": "\\boxed{\nTraditional Convolutional Neural Networks (CNNs) typically employ a single activation function, such as ReLU, for all neurons. However, a novel approach, called Multi-function Convolutional Neural Networks (MCNNs), uses different activation functions for different neurons, leading to a vast number of possible models. This approach has been shown to improve classification performance in two applications: handwritten digit recognition and Alzheimer's disease diagnosis. The results demonstrate that MCNNs can outperform traditional CNNs in terms of multi-class classification accuracy. Future research directions include developing efficient methods to select the best MCNN from the vast number of possible models and modifying existing CNN software to support MCNNs, which could lead to more effective and efficient image classification tools.\n}"}, "2108.024": {"original_text": "  Gait data captured by inertial sensors have demonstrated promising results on\nuser authentication. However, most existing approaches stored the enrolled gait\npattern insecurely for matching with the validating pattern, thus, posed\ncritical security and privacy issues. In this study, we present a gait\ncryptosystem that generates from gait data the random key for user\nauthentication, meanwhile, secures the gait pattern. First, we propose a\nrevocable and random binary string extraction method using a deep neural\nnetwork followed by feature-wise binarization. A novel loss function for\nnetwork optimization is also designed, to tackle not only the intrauser\nstability but also the inter-user randomness. Second, we propose a new\nbiometric key generation scheme, namely Irreversible Error Correct and\nObfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme,\nto securely generate from the binary string the random and irreversible key.\nThe model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. We\nshowed that our model could generate the key of 139 bits from 5-second data\nsequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR)\nsmaller than 5.441%. In addition, the security and user privacy analyses showed\nthat our model was secure against existing attacks on biometric template\nprotection, and fulfilled irreversibility and unlinkability.\n", "rewritten_text": "\\boxed{\nGait data from inertial sensors has shown promise for user authentication, but existing methods store gait patterns insecurely, posing security and privacy risks. This study presents a gait cryptosystem that generates a random key for user authentication while securing the gait pattern. The system uses a deep neural network to extract a revocable and random binary string from gait data, followed by feature-wise binarization. A novel loss function is designed to ensure both intra-user stability and inter-user randomness. The binary string is then used to generate a secure and irreversible key using the Irreversible Error Correct and Obfuscate (IECO) scheme. Evaluation on two benchmark datasets shows that the system can generate a 139-bit key from a 5-second data sequence with zero false acceptance rate and a false rejection rate below 5.441%. Security and privacy analyses demonstrate that the system is resistant to existing attacks on biometric template protection and ensures irreversibility and unlinkability.\n}"}, "2404.03443": {"original_text": "  The goal of occluded person re-identification (ReID) is to retrieve specific\npedestrians in occluded situations. However, occluded person ReID still suffers\nfrom background clutter and low-quality local feature representations, which\nlimits model performance. In our research, we introduce a new framework called\nPAB-ReID, which is a novel ReID model incorporating part-attention mechanisms\nto tackle the aforementioned issues effectively. Firstly, we introduce the\nhuman parsing label to guide the generation of more accurate human part\nattention maps. In addition, we propose a fine-grained feature focuser for\ngenerating fine-grained human local feature representations while suppressing\nbackground interference. Moreover, We also design a part triplet loss to\nsupervise the learning of human local features, which optimizes\nintra/inter-class distance. We conducted extensive experiments on specialized\nocclusion and regular ReID datasets, showcasing that our approach outperforms\nthe existing state-of-the-art methods.\n", "rewritten_text": "\\boxed{The objective of occluded person re-identification (ReID) is to locate specific individuals in situations where they are partially obscured. Despite this, current ReID models struggle with background distractions and inadequate local feature representations, leading to subpar performance. To address these limitations, we propose a novel framework called PAB-ReID, which leverages part-attention mechanisms to effectively tackle these issues. Our approach utilizes human parsing labels to generate accurate part attention maps, a fine-grained feature focuser to produce detailed local feature representations while minimizing background interference, and a part triplet loss to optimize intra- and inter-class distances. Through extensive experiments on specialized occlusion and regular ReID datasets, our method demonstrates superior performance compared to existing state-of-the-art approaches.}"}, "2310.05620": {"original_text": "  General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic.\n", "rewritten_text": "\\boxed{\nLarge Language Models (LLMs) have shown impressive performance in LegalAI tasks, but their evaluations are often defined by computer science experts, which may not align with the logical thinking of legal practitioners. To address this, we created LAiW, a Chinese legal LLMs benchmark that assesses LLMs based on the logic of legal practice. We categorized LLMs' legal capabilities into three levels: basic information retrieval, legal foundation inference, and complex legal application. Our automated evaluation of current LLMs on LAiW revealed that they may not align with legal practice logic, exceling in complex tasks but struggling with basic ones. Human evaluation by legal experts confirmed that while LLMs perform well, they still require reinforcement of legal logic to be practically applicable.\n}"}, "2408.12677": {"original_text": "  Traditional volumetric fusion algorithms preserve the spatial structure of 3D\nscenes, which is beneficial for many tasks in computer vision and robotics.\nHowever, they often lack realism in terms of visualization. Emerging 3D\nGaussian splatting bridges this gap, but existing Gaussian-based reconstruction\nmethods often suffer from artifacts and inconsistencies with the underlying 3D\nstructure, and struggle with real-time optimization, unable to provide users\nwith immediate feedback in high quality. One of the bottlenecks arises from the\nmassive amount of Gaussian parameters that need to be updated during\noptimization. Instead of using 3D Gaussian as a standalone map representation,\nwe incorporate it into a volumetric mapping system to take advantage of\ngeometric information and propose to use a quadtree data structure on images to\ndrastically reduce the number of splats initialized. In this way, we\nsimultaneously generate a compact 3D Gaussian map with fewer artifacts and a\nvolumetric map on the fly. Our method, GSFusion, significantly enhances\ncomputational efficiency without sacrificing rendering quality, as demonstrated\non both synthetic and real datasets. Code will be available at\nhttps://github.com/goldoak/GSFusion.\n", "rewritten_text": "\\boxed{\nTraditional volumetric fusion algorithms excel in preserving 3D scene structure, but often fall short in terms of visualization realism. Recently, 3D Gaussian splatting has emerged as a solution, but existing methods are plagued by artifacts, inconsistencies, and slow optimization, making real-time feedback a challenge. A major hurdle is the vast number of Gaussian parameters that require updating during optimization. To overcome this, we integrate 3D Gaussian into a volumetric mapping system, leveraging geometric information and utilizing a quadtree data structure on images to drastically reduce the number of initialized splats. Our approach, GSFusion, generates a compact 3D Gaussian map with fewer artifacts and a volumetric map in real-time, significantly enhancing computational efficiency without compromising rendering quality. Our method has been successfully demonstrated on both synthetic and real datasets, and the code is available at https://github.com/goldoak/GSFusion.\n}"}, "2309.14962": {"original_text": "  All tables can be represented as grids. Based on this observation, we propose\nGridFormer, a novel approach for interpreting unconstrained table structures by\npredicting the vertex and edge of a grid. First, we propose a flexible table\nrepresentation in the form of an MXN grid. In this representation, the vertexes\nand edges of the grid store the localization and adjacency information of the\ntable. Then, we introduce a DETR-style table structure recognizer to\nefficiently predict this multi-objective information of the grid in a single\nshot. Specifically, given a set of learned row and column queries, the\nrecognizer directly outputs the vertexes and edges information of the\ncorresponding rows and columns. Extensive experiments on five challenging\nbenchmarks which include wired, wireless, multi-merge-cell, oriented, and\ndistorted tables demonstrate the competitive performance of our model over\nother methods.\n", "rewritten_text": "\\boxed{We introduce GridFormer, a novel approach to interpret unconstrained table structures by predicting the vertex and edge of a grid. Our method represents tables as flexible MXN grids, where vertexes and edges store localization and adjacency information. A DETR-style recognizer efficiently predicts this information in a single step, using learned row and column queries to output vertexes and edges for corresponding rows and columns. Our approach achieves competitive performance on five challenging benchmarks, outperforming other methods.}"}, "2303.17658": {"original_text": "  Machine learning models deployed in the open world may encounter observations\nthat they were not trained to recognize, and they risk misclassifying such\nobservations with high confidence. Therefore, it is essential that these models\nare able to ascertain what is in-distribution (ID) and out-of-distribution\n(OOD), to avoid this misclassification. In recent years, huge strides have been\nmade in creating models that are robust to this distinction. As a result, the\ncurrent state-of-the-art has reached near perfect performance on relatively\ncoarse-grained OOD detection tasks, such as distinguishing horses from trucks,\nwhile struggling with finer-grained classification, like differentiating models\nof commercial aircraft. In this paper, we describe a new theoretical framework\nfor understanding fine- and coarse-grained OOD detection, we re-conceptualize\nfine grained classification into a three part problem, and we propose a new\nbaseline task for OOD models on two fine-grained hierarchical data sets, two\nnew evaluation methods to differentiate fine- and coarse-grained OOD\nperformance, along with a new loss function for models in this task.\n", "rewritten_text": "\\boxed{Machine learning models face a critical challenge when encountering unfamiliar data in real-world applications, as they may misclassify it with high confidence. To address this, it's crucial for models to distinguish between in-distribution (ID) and out-of-distribution (OOD) data. While significant progress has been made in developing robust models for coarse-grained OOD detection (e.g., distinguishing horses from trucks), fine-grained classification (e.g., differentiating commercial aircraft models) remains a challenge. This paper presents a new theoretical framework for understanding fine- and coarse-grained OOD detection, breaks down fine-grained classification into a three-part problem, and introduces a new baseline task, evaluation methods, and loss function for OOD models on hierarchical data sets.}"}, "2109.00698": {"original_text": "  While conventional wisdom suggests that more aggressively filtering data from\nlow-quality sources like Common Crawl always monotonically improves the quality\nof training data, we find that aggressive filtering can in fact lead to a\ndecrease in model quality on a wide array of downstream tasks for a GPT-like\nlanguage model. We speculate that this is because optimizing sufficiently\nstrongly for a proxy metric harms performance on the true objective, suggesting\na need for more robust filtering objectives when attempting to filter more\naggressively. We hope this work leads to detailed analysis of the effects of\ndataset filtering design choices on downstream model performance in future\nwork.\n", "rewritten_text": "\\boxed{Contrary to popular belief, aggressively filtering out low-quality data sources like Common Crawl can actually decrease the performance of a language model on various tasks. This may be because prioritizing a specific metric too strongly can harm the model's overall performance. Therefore, more robust filtering methods are needed to ensure optimal results. This study highlights the importance of carefully evaluating the impact of dataset filtering on model performance.}"}, "1501.03952": {"original_text": "  Domain adaptation techniques aim at adapting a classifier learnt on a source\ndomain to work on the target domain. Exploiting the subspaces spanned by\nfeatures of the source and target domains respectively is one approach that has\nbeen investigated towards solving this problem. These techniques normally\nassume the existence of a single subspace for the entire source / target\ndomain. In this work, we consider the hierarchical organization of the data and\nconsider multiple subspaces for the source and target domain based on the\nhierarchy. We evaluate different subspace based domain adaptation techniques\nunder this setting and observe that using different subspaces based on the\nhierarchy yields consistent improvement over a non-hierarchical baseline\n", "rewritten_text": "\\boxed{Domain adaptation methods focus on adapting a classifier trained on a source domain to perform well on a target domain. One approach to achieve this is to utilize the subspaces defined by the features of the source and target domains. However, traditional techniques assume a single subspace for each domain. In contrast, this work explores the hierarchical structure of the data and considers multiple subspaces for each domain based on this hierarchy. Experimental results show that using hierarchical subspaces leads to consistent improvements over a non-hierarchical baseline.}"}, "1707.00569": {"original_text": "  This survey presents a deep analysis of the learning and inference\ncapabilities in nine popular trackers. It is neither intended to study the\nwhole literature nor is it an attempt to review all kinds of neural networks\nproposed for visual tracking. We focus instead on Siamese neural networks which\nare a promising starting point for studying the challenging problem of\ntracking. These networks integrate efficiently feature learning and the\ntemporal matching and have so far shown state-of-the-art performance. In\nparticular, the branches of Siamese networks, their layers connecting these\nbranches, specific aspects of training and the embedding of these networks into\nthe tracker are highlighted. Quantitative results from existing papers are\ncompared with the conclusion that the current evaluation methodology shows\nproblems with the reproducibility and the comparability of results. The paper\nproposes a novel Lisp-like formalism for a better comparison of trackers. This\nassumes a certain functional design and functional decomposition of trackers.\nThe paper tries to give foundation for tracker design by a formulation of the\nproblem based on the theory of machine learning and by the interpretation of a\ntracker as a decision function. The work concludes with promising lines of\nresearch and suggests future work.\n", "rewritten_text": "\\boxed{\nThis survey provides an in-depth examination of the learning and inference capabilities of nine popular trackers, with a specific focus on Siamese neural networks, which have demonstrated state-of-the-art performance in visual tracking. The analysis highlights the key components of Siamese networks, including their branches, connecting layers, training aspects, and integration into trackers. A comparison of quantitative results from existing papers reveals issues with reproducibility and comparability. To address this, the paper proposes a novel Lisp-like formalism for better tracker comparison, assuming a functional design and decomposition. The work establishes a foundation for tracker design based on machine learning theory, interpreting a tracker as a decision function. The survey concludes by identifying promising research directions and suggesting future work."}, "2202.04897": {"original_text": "  Knowledge graph embedding (KGE) models learn the representation of entities\nand relations in knowledge graphs. Distance-based methods show promising\nperformance on link prediction task, which predicts the result by the distance\nbetween two entity representations. However, most of these methods represent\nthe head entity and tail entity separately, which limits the model capacity. We\npropose two novel distance-based methods named InterHT and InterHT+ that allow\nthe head and tail entities to interact better and get better entity\nrepresentation. Experimental results show that our proposed method achieves the\nbest results on ogbl-wikikg2 dataset.\n", "rewritten_text": "\\boxed{Knowledge graph embedding models aim to represent entities and relations in knowledge graphs. While distance-based methods have shown promise in link prediction tasks, which involve predicting outcomes based on the distance between entity representations, they often fall short by representing head and tail entities separately, limiting their capacity. To address this, we introduce two novel distance-based methods, InterHT and InterHT+, which enable more effective interaction between head and tail entities, leading to improved entity representation. Our experiments demonstrate that our proposed method achieves superior results on the ogbl-wikikg2 dataset.}"}, "1907.0203": {"original_text": "  Factchecking has always been a part of the journalistic process. However with\nnewsroom budgets shrinking it is coming under increasing pressure just as the\namount of false information circulating is on the rise. We therefore propose a\nmethod to increase the efficiency of the factchecking process, using the latest\ndevelopments in Natural Language Processing (NLP). This method allows us to\ncompare incoming claims to an existing corpus and return similar, factchecked,\nclaims in a live system-allowing factcheckers to work simultaneously without\nduplicating their work.\n", "rewritten_text": "\\boxed{As newsroom budgets decline, the factchecking process, a longstanding component of journalism, is facing growing pressure amidst a surge in false information. To address this, we suggest a novel approach that leverages recent advancements in Natural Language Processing (NLP) to enhance the efficiency of factchecking. Our method enables the comparison of incoming claims with a pre-existing corpus, providing instant access to similar, already-verified claims, thereby allowing multiple factcheckers to work concurrently without duplicating efforts.}"}, "2311.06377": {"original_text": "  Heaps' law is an empirical relation in text analysis that predicts vocabulary\ngrowth as a function of corpus size. While this law has been validated in\ndiverse human-authored text corpora, its applicability to large language model\ngenerated text remains unexplored. This study addresses this gap, focusing on\nthe emulation of corpora using the suite of GPT-Neo large language models. To\nconduct our investigation, we emulated corpora of PubMed abstracts using three\ndifferent parameter sizes of the GPT-Neo model. Our emulation strategy involved\nusing the initial five words of each PubMed abstract as a prompt and\ninstructing the model to expand the content up to the original abstract's\nlength. Our findings indicate that the generated corpora adhere to Heaps' law.\nInterestingly, as the GPT-Neo model size grows, its generated vocabulary\nincreasingly adheres to Heaps' law as as observed in human-authored text. To\nfurther improve the richness and authenticity of GPT-Neo outputs, future\niterations could emphasize enhancing model size or refining the model\narchitecture to curtail vocabulary repetition.\n", "rewritten_text": "\\boxed{This study explores whether Heaps' law, which predicts how vocabulary grows with corpus size, applies to text generated by large language models. Using the GPT-Neo model, we created artificial corpora of PubMed abstracts and found that the generated text follows Heaps' law, with larger models producing vocabulary that more closely resembles that of human-authored text. Our results suggest that increasing model size or refining the model architecture could further improve the quality and authenticity of generated text.}"}, "1809.07099": {"original_text": "  Deep neural networks have exhibited promising performance in image\nsuper-resolution (SR) due to the power in learning the non-linear mapping from\nlow-resolution (LR) images to high-resolution (HR) images. However, most deep\nlearning methods employ feed-forward architectures, and thus the dependencies\nbetween LR and HR images are not fully exploited, leading to limited learning\nperformance. Moreover, most deep learning based SR methods apply the pixel-wise\nreconstruction error as the loss, which, however, may fail to capture\nhigh-frequency information and produce perceptually unsatisfying results,\nwhilst the recent perceptual loss relies on some pre-trained deep model and\nthey may not generalize well. In this paper, we introduce a mask to separate\nthe image into low- and high-frequency parts based on image gradient magnitude,\nand then devise a gradient sensitive loss to well capture the structures in the\nimage without sacrificing the recovery of low-frequency content. Moreover, by\ninvestigating the duality in SR, we develop a dual reconstruction network (DRN)\nto improve the SR performance. We provide theoretical analysis on the\ngeneralization performance of our method and demonstrate its effectiveness and\nsuperiority with thorough experiments.\n", "rewritten_text": "\\boxed{Deep neural networks have shown great potential in image super-resolution (SR) by learning the complex relationship between low-resolution (LR) and high-resolution (HR) images. However, most current methods use simple feed-forward architectures, which fail to fully utilize the connections between LR and HR images, resulting in limited performance. Additionally, traditional pixel-wise reconstruction errors may not capture high-frequency details, leading to unsatisfying results. Recent perceptual losses rely on pre-trained models and may not generalize well. To address these issues, we propose a novel approach that separates images into low- and high-frequency parts using gradient magnitude and introduces a gradient-sensitive loss to capture image structures without sacrificing low-frequency content. Furthermore, we develop a dual reconstruction network (DRN) to enhance SR performance by exploiting the duality in SR. We provide theoretical analysis and experimental results to demonstrate the effectiveness and superiority of our method.}"}, "2310.15052": {"original_text": "  Dataset distillation plays a crucial role in creating compact datasets with\nsimilar training performance compared with original large-scale ones. This is\nessential for addressing the challenges of data storage and training costs.\nPrevalent methods facilitate knowledge transfer by matching the gradients,\nembedding distributions, or training trajectories of synthetic images with\nthose of the sampled original images. Although there are various matching\nobjectives, currently the strategy for selecting original images is limited to\nnaive random sampling. We argue that random sampling overlooks the evenness of\nthe selected sample distribution, which may result in noisy or biased matching\ntargets. Besides, the sample diversity is also not constrained by random\nsampling. Additionally, current methods predominantly focus on\nsingle-dimensional matching, where information is not fully utilized. To\naddress these challenges, we propose a novel matching strategy called Dataset\nDistillation by Bidirectional REpresentAtive Matching (DREAM+), which selects\nrepresentative original images for bidirectional matching. DREAM+ is applicable\nto a variety of mainstream dataset distillation frameworks and significantly\nreduces the number of distillation iterations by more than 15 times without\naffecting performance. Given sufficient training time, DREAM+ can further\nimprove the performance and achieve state-of-the-art results. We have released\nthe code at github.com/NUS-HPC-AI-Lab/DREAM+.\n", "rewritten_text": "\\boxed{\nDataset distillation is a crucial technique for creating compact datasets that maintain similar training performance to large-scale datasets, thereby addressing data storage and training cost challenges. Existing methods achieve this by matching gradients, embedding distributions, or training trajectories between synthetic and original images. However, these methods rely on random sampling of original images, which can lead to noisy or biased targets due to uneven sample distributions and lack of diversity. Furthermore, current methods focus on single-dimensional matching, failing to fully utilize information. To overcome these limitations, we propose DREAM+, a novel matching strategy that selects representative original images for bidirectional matching. DREAM+ is compatible with various dataset distillation frameworks, reduces distillation iterations by over 15 times without performance loss, and achieves state-of-the-art results with sufficient training time. The code is available at github.com/NUS-HPC-AI-Lab/DREAM+.\n}"}, "2207.09625": {"original_text": "  Given an image and a reference caption, the image caption editing task aims\nto correct the misalignment errors and generate a refined caption. However, all\nexisting caption editing works are implicit models, ie, they directly produce\nthe refined captions without explicit connections to the reference captions. In\nthis paper, we introduce a new task: Explicit Caption Editing (ECE). ECE models\nexplicitly generate a sequence of edit operations, and this edit operation\nsequence can translate the reference caption into a refined one. Compared to\nthe implicit editing, ECE has multiple advantages: 1) Explainable: it can trace\nthe whole editing path. 2) Editing Efficient: it only needs to modify a few\nwords. 3) Human-like: it resembles the way that humans perform caption editing,\nand tries to keep original sentence structures. To solve this new task, we\npropose the first ECE model: TIger. TIger is a non-autoregressive\ntransformer-based model, consisting of three modules: Tagger_del, Tagger_add,\nand Inserter. Specifically, Tagger_del decides whether each word should be\npreserved or not, Tagger_add decides where to add new words, and Inserter\npredicts the specific word for adding. To further facilitate ECE research, we\npropose two new ECE benchmarks by re-organizing two existing datasets, dubbed\nCOCO-EE and Flickr30K-EE, respectively. Extensive ablations on both two\nbenchmarks have demonstrated the effectiveness of TIger.\n", "rewritten_text": "\\boxed{\nThe image caption editing task involves correcting errors in a caption to generate a refined version. Existing approaches directly produce the refined caption without explicitly linking it to the original reference caption. This paper introduces a new task, Explicit Caption Editing (ECE), which generates a sequence of edit operations to transform the reference caption into a refined one. ECE has several advantages, including being explainable, efficient, and human-like. To achieve this, the authors propose the TIger model, a non-autoregressive transformer-based model consisting of three modules: Tagger_del, Tagger_add, and Inserter. The authors also introduce two new ECE benchmarks, COCO-EE and Flickr30K-EE, and demonstrate the effectiveness of TIger through extensive experiments.\n}"}, "1805.05503": {"original_text": "  Human faces are one interesting object class with numerous applications.\nWhile significant progress has been made in the generic deblurring problem,\nexisting methods are less effective for blurry face images. The success of the\nstate-of-the-art image deblurring algorithms stems mainly from implicit or\nexplicit restoration of salient edges for kernel estimation. However, existing\nmethods are less effective as only few edges can be restored from blurry face\nimages for kernel estimation. In this paper, we address the problem of\ndeblurring face images by exploiting facial structures. We propose a deblurring\nalgorithm based on an exemplar dataset without using coarse-to-fine strategies\nor heuristic edge selections. In addition, we develop a convolutional neural\nnetwork to restore sharp edges from blurry images for deblurring. Extensive\nexperiments against the state-of-the-art methods demonstrate the effectiveness\nof the proposed algorithms for deblurring face images. In addition, we show the\nproposed algorithms can be applied to image deblurring for other object\nclasses.\n", "rewritten_text": "\\boxed{Human faces are a unique and challenging object class for image deblurring, with existing methods struggling to effectively restore blurry face images. This is because these methods rely on the restoration of prominent edges to estimate the blur kernel, which is difficult to achieve with blurry face images. To address this, we propose a novel deblurring algorithm that leverages facial structures and utilizes a convolutional neural network to restore sharp edges from blurry images. Our approach does not rely on coarse-to-fine strategies or heuristic edge selections, and our experiments demonstrate its effectiveness in deblurring face images, with potential applications to other object classes as well.}"}, "2205.09299": {"original_text": "  Convolutional Neural Networks (CNNs) have achieved promising results in\nmedical image segmentation. However, CNNs require lots of training data and are\nincapable of handling pose and deformation of objects. Furthermore, their\npooling layers tend to discard important information such as positions as well\nas CNNs are sensitive to rotation and affine transformation. Capsule network is\na recent new architecture that has achieved better robustness in part-whole\nrepresentation learning by replacing pooling layers with dynamic routing and\nconvolutional strides, which has shown potential results on popular tasks such\nas digit classification and object segmentation. In this paper, we propose a 3D\nencoder-decoder network with Convolutional Capsule Encoder (called 3DConvCaps)\nto learn lower-level features (short-range attention) with convolutional layers\nwhile modeling the higher-level features (long-range dependence) with capsule\nlayers. Our experiments on multiple datasets including iSeg-2017, Hippocampus,\nand Cardiac demonstrate that our 3D 3DConvCaps network considerably outperforms\nprevious capsule networks and 3D-UNets. We further conduct ablation studies of\nnetwork efficiency and segmentation performance under various configurations of\nconvolution layers and capsule layers at both contracting and expanding paths.\n", "rewritten_text": "\\boxed{Convolutional Neural Networks (CNNs) have shown promise in medical image segmentation, but they have limitations, including requiring large amounts of training data and struggling with object pose and deformation. Additionally, their pooling layers can discard important information and they are sensitive to rotation and affine transformation. To address these issues, this paper proposes a 3D encoder-decoder network, called 3DConvCaps, which combines convolutional layers for learning lower-level features with capsule layers for modeling higher-level features. Experimental results on multiple datasets demonstrate that 3DConvCaps outperforms previous capsule networks and 3D-UNets, and ablation studies provide insights into the network's efficiency and segmentation performance under different configurations.}"}, "2407.08290": {"original_text": "  Recent advances in mobile mapping systems have greatly enhanced the\nefficiency and convenience of acquiring urban 3D data. These systems utilize\nLiDAR sensors mounted on vehicles to capture vast cityscapes. However, a\nsignificant challenge arises due to occlusions caused by roadside parked\nvehicles, leading to the loss of scene information, particularly on the roads,\nsidewalks, curbs, and the lower sections of buildings. In this study, we\npresent a novel approach that leverages deep neural networks to learn a model\ncapable of filling gaps in urban scenes that are obscured by vehicle occlusion.\nWe have developed an innovative technique where we place virtual vehicle models\nalong road boundaries in the gap-free scene and utilize a ray-casting algorithm\nto create a new scene with occluded gaps. This allows us to generate diverse\nand realistic urban point cloud scenes with and without vehicle occlusion,\nsurpassing the limitations of real-world training data collection and\nannotation. Furthermore, we introduce the Scene Gap Completion Network\n(SGC-Net), an end-to-end model that can generate well-defined shape boundaries\nand smooth surfaces within occluded gaps. The experiment results reveal that\n97.66% of the filled points fall within a range of 5 centimeters relative to\nthe high-density ground truth point cloud scene. These findings underscore the\nefficacy of our proposed model in gap completion and reconstructing urban\nscenes affected by vehicle occlusions.\n", "rewritten_text": "\\boxed{\nRecent breakthroughs in mobile mapping technology have made it easier and more efficient to collect 3D data of urban areas. However, a major challenge arises when vehicles parked along roadsides block the view, resulting in missing information about roads, sidewalks, curbs, and building bases. To address this issue, we developed a novel approach that uses deep neural networks to fill in gaps in urban scenes caused by vehicle occlusion. Our method involves placing virtual vehicles along road boundaries in a gap-free scene, then using a ray-casting algorithm to create a new scene with occluded gaps. This allows us to generate realistic urban point cloud scenes with and without vehicle occlusion, overcoming the limitations of real-world data collection and annotation. We also introduced the Scene Gap Completion Network (SGC-Net), a model that can generate well-defined shape boundaries and smooth surfaces within occluded gaps. Our experiment results show that 97.66% of the filled points are within 5 centimeters of the high-density ground truth point cloud scene, demonstrating the effectiveness of our proposed model in completing gaps and reconstructing urban scenes affected by vehicle occlusions.\n}"}, "2004.04699": {"original_text": "  Deep Neural Networks trained in a fully supervised fashion are the dominant\ntechnology in perception-based autonomous driving systems. While collecting\nlarge amounts of unlabeled data is already a major undertaking, only a subset\nof it can be labeled by humans due to the effort needed for high-quality\nannotation. Therefore, finding the right data to label has become a key\nchallenge. Active learning is a powerful technique to improve data efficiency\nfor supervised learning methods, as it aims at selecting the smallest possible\ntraining set to reach a required performance. We have built a scalable\nproduction system for active learning in the domain of autonomous driving. In\nthis paper, we describe the resulting high-level design, sketch some of the\nchallenges and their solutions, present our current results at scale, and\nbriefly describe the open problems and future directions.\n", "rewritten_text": "\\boxed{In autonomous driving systems, fully supervised Deep Neural Networks are the leading technology. However, collecting and labeling large amounts of data is a significant challenge due to the time and effort required for high-quality annotation. To address this, active learning can be used to select the most valuable data to label, improving data efficiency and reducing the required training set. This paper outlines the design and implementation of a scalable active learning system for autonomous driving, discusses the challenges and solutions, and presents current results and future directions.}"}, "1808.01491": {"original_text": "  Single image rain streaks removal has recently witnessed substantial progress\ndue to the development of deep convolutional neural networks. However, existing\ndeep learning based methods either focus on the entrance and exit of the\nnetwork by decomposing the input image into high and low frequency information\nand employing residual learning to reduce the mapping range, or focus on the\nintroduction of cascaded learning scheme to decompose the task of rain streaks\nremoval into multi-stages. These methods treat the convolutional neural network\nas an encapsulated end-to-end mapping module without deepening into the\nrationality and superiority of neural network design. In this paper, we delve\ninto an effective end-to-end neural network structure for stronger feature\nexpression and spatial correlation learning. Specifically, we propose a\nnon-locally enhanced encoder-decoder network framework, which consists of a\npooling indices embedded encoder-decoder network to efficiently learn\nincreasingly abstract feature representation for more accurate rain streaks\nmodeling while perfectly preserving the image detail. The proposed\nencoder-decoder framework is composed of a series of non-locally enhanced dense\nblocks that are designed to not only fully exploit hierarchical features from\nall the convolutional layers but also well capture the long-distance\ndependencies and structural information. Extensive experiments on synthetic and\nreal datasets demonstrate that the proposed method can effectively remove\nrain-streaks on rainy image of various densities while well preserving the\nimage details, which achieves significant improvements over the recent\nstate-of-the-art methods.\n", "rewritten_text": "\\boxed{\nRecent advancements in deep convolutional neural networks have led to significant progress in single image rain streaks removal. However, existing methods have limitations, either focusing on input image decomposition and residual learning or employing cascaded learning schemes. These approaches treat the neural network as a single, end-to-end module without optimizing its design. This paper proposes a novel, end-to-end neural network structure that enhances feature expression and spatial correlation learning. The proposed framework, a non-locally enhanced encoder-decoder network, efficiently learns abstract feature representations for accurate rain streaks modeling while preserving image details. The network consists of dense blocks that capture hierarchical features, long-distance dependencies, and structural information. Experimental results on synthetic and real datasets demonstrate the effectiveness of the proposed method in removing rain streaks from images of various densities while preserving image details, outperforming recent state-of-the-art methods.\n}"}, "2303.15698": {"original_text": "  Standard deep learning models such as convolutional neural networks (CNNs)\nlack the ability of generalizing to domains which have not been seen during\ntraining. This problem is mainly due to the common but often wrong assumption\nof such models that the source and target data come from the same i.i.d.\ndistribution. Recently, Vision Transformers (ViTs) have shown outstanding\nperformance for a broad range of computer vision tasks. However, very few\nstudies have investigated their ability to generalize to new domains. This\npaper presents a first Token-level Feature Stylization (TFS-ViT) approach for\ndomain generalization, which improves the performance of ViTs to unseen data by\nsynthesizing new domains. Our approach transforms token features by mixing the\nnormalization statistics of images from different domains. We further improve\nthis approach with a novel strategy for attention-aware stylization, which uses\nthe attention maps of class (CLS) tokens to compute and mix normalization\nstatistics of tokens corresponding to different image regions. The proposed\nmethod is flexible to the choice of backbone model and can be easily applied to\nany ViT-based architecture with a negligible increase in computational\ncomplexity. Comprehensive experiments show that our approach is able to achieve\nstate-of-the-art performance on five challenging benchmarks for domain\ngeneralization, and demonstrate its ability to deal with different types of\ndomain shifts. The implementation is available at:\nhttps://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.\n", "rewritten_text": "\\boxed{\nDeep learning models, including convolutional neural networks (CNNs), struggle to generalize to new domains unseen during training due to the assumption that source and target data come from the same distribution. Vision Transformers (ViTs) have shown excellent performance in computer vision tasks, but their ability to generalize to new domains has been understudied. This paper introduces Token-level Feature Stylization (TFS-ViT), a novel approach that improves ViTs' performance on unseen data by synthesizing new domains. TFS-ViT transforms token features by mixing normalization statistics from different domains and uses attention maps to compute and mix statistics from different image regions. This approach is flexible, computationally efficient, and achieves state-of-the-art performance on five domain generalization benchmarks, demonstrating its ability to handle various domain shifts. The implementation is available at https://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.\n}"}, "1804.09555": {"original_text": "  Outdoor vision-based systems suffer from atmospheric turbulences, and rain is\none of the worst factors for vision degradation. Current rain removal methods\nshow limitations either for complex dynamic scenes, or under torrential rain\nwith opaque occlusions. We propose a novel derain framework which applies\nsuperpixel (SP) segmentation to decompose the scene into depth consistent\nunits. Alignment of scene contents are done at the SP level, which proves to be\nrobust against rain occlusion interferences and fast camera motion. Two\nalignment output tensors, i.e., optimal temporal match tensor and sorted\nspatial-temporal match tensor, provide informative clues for the location of\nrain streaks and the occluded background contents. Different classical and\nnovel methods such as Robust Principle Component Analysis and Convolutional\nNeural Networks are applied and compared for their respective advantages in\nefficiently exploiting the rich spatial-temporal features provided by the two\ntensors. Extensive evaluations show that advantage of up to 5dB is achieved on\nthe scene restoration PSNR over state-of-the-art methods, and the advantage is\nespecially obvious with highly complex and dynamic scenes. Visual evaluations\nshow that the proposed framework is not only able to suppress heavy and opaque\noccluding rain streaks, but also large semi-transparent regional fluctuations\nand distortions.\n", "rewritten_text": "\\boxed{Outdoor vision systems are often impaired by atmospheric disturbances, with rain being a major culprit in degrading image quality. Existing methods for removing rain from images have limitations, particularly in complex scenes with heavy rain or opaque occlusions. To address this, we propose a novel framework that uses superpixel segmentation to break down the scene into depth-consistent units. By aligning scene contents at the superpixel level, our approach proves robust against rain occlusion interference and fast camera motion. The output tensors provide valuable clues for identifying rain streaks and occluded background content. We compare various methods, including Robust Principle Component Analysis and Convolutional Neural Networks, to efficiently utilize the spatial-temporal features provided by the tensors. Our evaluations show that our framework achieves up to 5dB improvement in scene restoration PSNR over state-of-the-art methods, particularly in complex and dynamic scenes. Visual evaluations demonstrate that our approach effectively suppresses heavy rain streaks, semi-transparent regional fluctuations, and distortions.}"}, "2005.00975": {"original_text": "  In the deep learning (DL) era, parsing models are extremely simplified with\nlittle hurt on performance, thanks to the remarkable capability of multi-layer\nBiLSTMs in context representation. As the most popular graph-based dependency\nparser due to its high efficiency and performance, the biaffine parser directly\nscores single dependencies under the arc-factorization assumption, and adopts a\nvery simple local token-wise cross-entropy training loss. This paper for the\nfirst time presents a second-order TreeCRF extension to the biaffine parser.\nFor a long time, the complexity and inefficiency of the inside-outside\nalgorithm hinder the popularity of TreeCRF. To address this issue, we propose\nan effective way to batchify the inside and Viterbi algorithms for direct large\nmatrix operation on GPUs, and to avoid the complex outside algorithm via\nefficient back-propagation. Experiments and analysis on 27 datasets from 13\nlanguages clearly show that techniques developed before the DL era, such as\nstructural learning (global TreeCRF loss) and high-order modeling are still\nuseful, and can further boost parsing performance over the state-of-the-art\nbiaffine parser, especially for partially annotated training data. We release\nour code at https://github.com/yzhangcs/crfpar.\n", "rewritten_text": "\\boxed{\nIn the deep learning era, parsing models have been greatly simplified without sacrificing performance, thanks to the powerful context representation capabilities of multi-layer BiLSTMs. The biaffine parser, a popular graph-based dependency parser, efficiently scores single dependencies and uses a simple local token-wise cross-entropy training loss. This paper introduces a second-order TreeCRF extension to the biaffine parser, addressing the complexity and inefficiency of the inside-outside algorithm by batchifying the inside and Viterbi algorithms for GPU operation and avoiding the outside algorithm through efficient back-propagation. Experiments on 27 datasets from 13 languages demonstrate that pre-DL era techniques, such as structural learning and high-order modeling, can still improve parsing performance over the state-of-the-art biaffine parser, especially with partially annotated training data. The code is available at https://github.com/yzhangcs/crfpar.\n}"}, "1812.00108": {"original_text": "  With vast amounts of video content being uploaded to the Internet every\nminute, video summarization becomes critical for efficient browsing, searching,\nand indexing of visual content. Nonetheless, the spread of social and\negocentric cameras creates an abundance of sparse scenarios captured by several\ndevices, and ultimately required to be jointly summarized. In this paper, we\ndiscuss the problem of summarizing videos recorded independently by several\ndynamic cameras that intermittently share the field of view. We present a\nrobust framework that (a) identifies a diverse set of important events among\nmoving cameras that often are not capturing the same scene, and (b) selects the\nmost representative view(s) at each event to be included in a universal\nsummary. Due to the lack of an applicable alternative, we collected a new\nmulti-view egocentric dataset, Multi-Ego. Our dataset is recorded\nsimultaneously by three cameras, covering a wide variety of real-life\nscenarios. The footage is annotated by multiple individuals under various\nsummarization configurations, with a consensus analysis ensuring a reliable\nground truth. We conduct extensive experiments on the compiled dataset in\naddition to three other standard benchmarks that show the robustness and the\nadvantage of our approach in both supervised and unsupervised settings.\nAdditionally, we show that our approach learns collectively from data of varied\nnumber-of-views and orthogonal to other summarization methods, deeming it\nscalable and generic.\n", "rewritten_text": "\\boxed{With the rapid growth of video content online, summarizing videos has become essential for efficient browsing, searching, and indexing. However, the increasing use of social and egocentric cameras has created a challenge in jointly summarizing videos recorded by multiple devices with intermittent shared views. This paper proposes a robust framework that identifies key events across multiple moving cameras and selects the most representative views for a universal summary. To evaluate our approach, we created a new multi-view egocentric dataset, Multi-Ego, which consists of real-life scenarios recorded by three cameras and annotated by multiple individuals. Our experiments demonstrate the effectiveness and scalability of our approach in both supervised and unsupervised settings, outperforming other methods and showing its ability to learn from varied data.}"}, "2012.07791": {"original_text": "  We propose real-time, six degrees of freedom (6DoF), 3D face pose estimation\nwithout face detection or landmark localization. We observe that estimating the\n6DoF rigid transformation of a face is a simpler problem than facial landmark\ndetection, often used for 3D face alignment. In addition, 6DoF offers more\ninformation than face bounding box labels. We leverage these observations to\nmake multiple contributions: (a) We describe an easily trained, efficient,\nFaster R-CNN--based model which regresses 6DoF pose for all faces in the photo,\nwithout preliminary face detection. (b) We explain how pose is converted and\nkept consistent between the input photo and arbitrary crops created while\ntraining and evaluating our model. (c) Finally, we show how face poses can\nreplace detection bounding box training labels. Tests on AFLW2000-3D and BIWI\nshow that our method runs at real-time and outperforms state of the art (SotA)\nface pose estimators. Remarkably, our method also surpasses SotA models of\ncomparable complexity on the WIDER FACE detection benchmark, despite not been\noptimized on bounding box labels.\n", "rewritten_text": "\\boxed{We introduce a novel approach for real-time 3D face pose estimation that eliminates the need for face detection and landmark localization. By focusing on the 6DoF rigid transformation of a face, we simplify the problem and gain more information than traditional face bounding box labels. Our contributions include a efficient and easily trained Faster R-CNN-based model that regresses 6DoF pose for all faces in an image, a method to maintain pose consistency across input photos and arbitrary crops, and the use of face poses as a replacement for detection bounding box training labels. Our approach achieves real-time performance and outperforms state-of-the-art face pose estimators on AFLW2000-3D and BIWI datasets, while also surpassing comparable models on the WIDER FACE detection benchmark without optimization on bounding box labels.}"}, "2208.05909": {"original_text": "  Preservation of domain knowledge from the source to target is crucial in any\ntranslation workflow. It is common in the translation industry to receive\nhighly specialized projects, where there is hardly any parallel in-domain data.\nIn such scenarios where there is insufficient in-domain data to fine-tune\nMachine Translation (MT) models, producing translations that are consistent\nwith the relevant context is challenging. In this work, we propose a novel\napproach to domain adaptation leveraging state-of-the-art pretrained language\nmodels (LMs) for domain-specific data augmentation for MT, simulating the\ndomain characteristics of either (a) a small bilingual dataset, or (b) the\nmonolingual source text to be translated. Combining this idea with\nback-translation, we can generate huge amounts of synthetic bilingual in-domain\ndata for both use cases. For our investigation, we use the state-of-the-art\nTransformer architecture. We employ mixed fine-tuning to train models that\nsignificantly improve translation of in-domain texts. More specifically, in\nboth scenarios, our proposed methods achieve improvements of approximately 5-6\nBLEU and 2-3 BLEU, respectively, on the Arabic-to-English and English-to-Arabic\nlanguage pairs. Furthermore, the outcome of human evaluation corroborates the\nautomatic evaluation results.\n", "rewritten_text": "\\boxed{\nMaintaining domain knowledge throughout the translation process is vital. However, in the translation industry, it's common to encounter specialized projects with limited in-domain data. This scarcity of data makes it challenging to produce contextually consistent translations using Machine Translation (MT) models. To address this, we propose a novel domain adaptation approach that leverages pre-trained language models to generate domain-specific data for MT. This approach can simulate the characteristics of either a small bilingual dataset or the monolingual source text. By combining this with back-translation, we can create a large amount of synthetic in-domain data. Using the Transformer architecture, we employ mixed fine-tuning to train models that significantly improve in-domain text translation. Our results show improvements of around 5-6 BLEU and 2-3 BLEU for Arabic-to-English and English-to-Arabic language pairs, respectively. Human evaluation also confirms the automatic evaluation results."}, "1811.09745": {"original_text": "  We propose a new geometric regularization principle for reconstructing vector\nfields based on prior knowledge about their divergence. As one important\nexample of this general idea, we focus on vector fields modelling blood flow\npattern that should be divergent in arteries and convergent in veins. We show\nthat this previously ignored regularization constraint can significantly\nimprove the quality of vessel tree reconstruction particularly around\nbifurcations where non-zero divergence is concentrated. Our divergence prior is\ncritical for resolving (binary) sign ambiguity in flow orientations produced by\nstandard vessel filters, e.g. Frangi. Our vessel tree centerline reconstruction\ncombines divergence constraints with robust curvature regularization. Our\nunsupervised method can reconstruct complete vessel trees with near-capillary\ndetails on synthetic and real 3D volumes.\n", "rewritten_text": "\\boxed{We introduce a novel geometric regularization principle that leverages prior knowledge of vector field divergence to improve reconstruction quality. Specifically, we apply this principle to blood flow modeling, where arteries are expected to be divergent and veins convergent. By incorporating this constraint, we significantly enhance vessel tree reconstruction, particularly around bifurcations where non-zero divergence is prominent. Our approach resolves sign ambiguity in flow orientations and combines divergence constraints with robust curvature regularization to produce complete vessel trees with near-capillary details in both synthetic and real 3D volumes.}"}, "2104.08575": {"original_text": "  Super-resolution (SR) is an ill-posed problem, which means that infinitely\nmany high-resolution (HR) images can be degraded to the same low-resolution\n(LR) image. To study the one-to-many stochastic SR mapping, we implicitly\nrepresent the non-local self-similarity of natural images and develop a\nVariational Sparse framework for Super-Resolution (VSpSR) via neural networks.\nSince every small patch of a HR image can be well approximated by the sparse\nrepresentation of atoms in an over-complete dictionary, we design a two-branch\nmodule, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM\nextracts patch-level basis from the LR input, and the other branch infers\npixel-wise variational distributions with respect to the sparse coefficients.\nBy repeatedly sampling coefficients, we could obtain infinite sparse\nrepresentations, and thus generate diverse HR images. According to the\npreliminary results of NTIRE 2021 challenge on learning SR space, our team\n(FudanZmic21) ranks 7-th in terms of released scores. The implementation of\nVSpSR is released at https://zmiclab.github.io/.\n", "rewritten_text": "\\boxed{\nSuper-resolution is an ill-posed problem, meaning that a single low-resolution image can be derived from multiple high-resolution images. To tackle this one-to-many stochastic mapping, we propose a Variational Sparse framework for Super-Resolution (VSpSR) that leverages neural networks to capture the non-local self-similarity of natural images. Our approach, VSpM, consists of two branches: one extracts patch-level basis from the low-resolution input, while the other infers pixel-wise variational distributions of sparse coefficients. By repeatedly sampling these coefficients, we can generate diverse high-resolution images. Our method, FudanZmic21, achieved 7th place in the NTIRE 2021 challenge on learning SR space, and its implementation is available at https://zmiclab.github.io/.\n}"}, "2206.12505": {"original_text": "  We propose a novel semi-supervised learning approach for classification of\nhistopathology images. We employ strong supervision with patch-level\nannotations combined with a novel co-training loss to create a semi-supervised\nlearning framework. Co-training relies on multiple conditionally independent\nand sufficient views of the data. We separate the hematoxylin and eosin\nchannels in pathology images using color deconvolution to create two views of\neach slide that can partially fulfill these requirements. Two separate CNNs are\nused to embed the two views into a joint feature space. We use a contrastive\nloss between the views in this feature space to implement co-training. We\nevaluate our approach in clear cell renal cell and prostate carcinomas, and\ndemonstrate improvement over state-of-the-art semi-supervised learning methods.\n", "rewritten_text": "\\boxed{We introduce a new semi-supervised learning method for classifying histopathology images, which combines strong supervision with patch-level annotations and a novel co-training loss. By separating the hematoxylin and eosin channels in pathology images using color deconvolution, we create two views of each slide that can be used for co-training. Two separate CNNs are employed to map these views into a shared feature space, where a contrastive loss is applied to implement co-training. Our approach outperforms existing state-of-the-art semi-supervised learning methods in classifying clear cell renal cell and prostate carcinomas.}"}, "2303.08714": {"original_text": "  Adapting the Diffusion Probabilistic Model (DPM) for direct image\nsuper-resolution is wasteful, given that a simple Convolutional Neural Network\n(CNN) can recover the main low-frequency content. Therefore, we present\nResDiff, a novel Diffusion Probabilistic Model based on Residual structure for\nSingle Image Super-Resolution (SISR). ResDiff utilizes a combination of a CNN,\nwhich restores primary low-frequency components, and a DPM, which predicts the\nresidual between the ground-truth image and the CNN predicted image. In\ncontrast to the common diffusion-based methods that directly use LR images to\nguide the noise towards HR space, ResDiff utilizes the CNN's initial prediction\nto direct the noise towards the residual space between HR space and\nCNN-predicted space, which not only accelerates the generation process but also\nacquires superior sample quality. Additionally, a frequency-domain-based loss\nfunction for CNN is introduced to facilitate its restoration, and a\nfrequency-domain guided diffusion is designed for DPM on behalf of predicting\nhigh-frequency details. The extensive experiments on multiple benchmark\ndatasets demonstrate that ResDiff outperforms previous diffusion based methods\nin terms of shorter model convergence time, superior generation quality, and\nmore diverse samples.\n", "rewritten_text": "\\boxed{\nApplying the Diffusion Probabilistic Model (DPM) directly to image super-resolution is inefficient, as a simple Convolutional Neural Network (CNN) can already recover the main low-frequency content. To address this, we propose ResDiff, a novel DPM-based approach that leverages a residual structure for Single Image Super-Resolution (SISR). ResDiff combines a CNN, which restores primary low-frequency components, with a DPM, which predicts the residual between the ground-truth image and the CNN-predicted image. Unlike traditional diffusion-based methods that use low-resolution images to guide noise towards high-resolution space, ResDiff uses the CNN's initial prediction to direct noise towards the residual space, accelerating the generation process and improving sample quality. Furthermore, we introduce a frequency-domain-based loss function for the CNN to facilitate restoration and a frequency-domain guided diffusion for the DPM to predict high-frequency details. Our extensive experiments on multiple benchmark datasets show that ResDiff outperforms previous diffusion-based methods in terms of convergence time, generation quality, and sample diversity.\n}"}, "1607.06797": {"original_text": "  In this paper we proposed an ordered patch based method using Conditional\nRandom Field (CRF) in order to encode local properties and their spatial\nrelationship in images to address texture classification, face recognition, and\nscene classification problems. Typical image classification approaches work\nwithout considering spatial causality among distinctive properties of an image\nfor image representation in feature space. In this method first, each image is\nencoded as a sequence of ordered patches, including local properties. Second,\nthe sequence of these ordered patches is modeled as a probabilistic feature\nvector by CRF to model spatial relationship of these local properties. And\nfinally, image classification is performed on such probabilistic image\nrepresentation. Experimental results on several standard image datasets\nindicate that proposed method outperforms some of existing image classification\nmethods.\n", "rewritten_text": "\\boxed{This paper presents a novel approach to image classification using Conditional Random Fields (CRFs) to capture local properties and their spatial relationships in images. Unlike traditional methods that ignore spatial dependencies, our method first represents each image as a sequence of ordered patches, each capturing local features. Then, it models the sequence of patches as a probabilistic feature vector using CRFs to encode spatial relationships. Finally, image classification is performed on this probabilistic representation. Our experimental results on standard image datasets demonstrate that our proposed method outperforms existing image classification techniques.}"}, "2305.15929": {"original_text": "  Current large language models, such as OpenAI's ChatGPT, have captured the\npublic's attention because how remarkable they are in the use of language.\nHere, I demonstrate that ChatGPT displays phonological biases that are a\nhallmark of human language processing. More concretely, just like humans,\nChatGPT has a consonant bias. That is, the chatbot has a tendency to use\nconsonants over vowels to identify words. This is observed across languages\nthat differ in their relative distribution of consonants and vowels such as\nEnglish and Spanish. Despite the differences in how current artificial\nintelligence language models are trained to process linguistic stimuli and how\nhuman infants acquire language, such training seems to be enough for the\nemergence of a phonological bias in ChatGPT\n", "rewritten_text": "\\boxed{The remarkable language abilities of current large language models, such as OpenAI's ChatGPT, have garnered significant public attention. Notably, I have found that ChatGPT exhibits phonological biases similar to those observed in human language processing, including a consonant bias, where it tends to rely more on consonants than vowels to identify words. This bias is consistent across languages, including English and Spanish, despite their differing consonant and vowel distributions. Interestingly, despite the differences in how AI language models are trained and how humans acquire language, the training process appears to be sufficient for the emergence of this phonological bias in ChatGPT.}"}, "2008.13196": {"original_text": "  The task of spatial-temporal action detection has attracted increasing\nattention among researchers. Existing dominant methods solve this problem by\nrelying on short-term information and dense serial-wise detection on each\nindividual frames or clips. Despite their effectiveness, these methods showed\ninadequate use of long-term information and are prone to inefficiency. In this\npaper, we propose for the first time, an efficient framework that generates\naction tube proposals from video streams with a single forward pass in a\nsparse-to-dense manner. There are two key characteristics in this framework:\n(1) Both long-term and short-term sampled information are explicitly utilized\nin our spatiotemporal network, (2) A new dynamic feature sampling module (DTS)\nis designed to effectively approximate the tube output while keeping the system\ntractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 and\nUCFSports benchmark datasets, achieving promising results that are competitive\nto state-of-the-art methods. The proposed sparse-to-dense strategy rendered our\nframework about 7.6 times more efficient than the nearest competitor.\n", "rewritten_text": "\\boxed{Researchers have been increasingly interested in spatial-temporal action detection, but current methods have limitations. They rely on short-term information and detect actions in each frame or clip separately, which can be inefficient and neglect long-term information. This paper proposes a novel framework that generates action tube proposals from video streams in a single pass, using both long-term and short-term information. The framework features a dynamic feature sampling module that efficiently approximates tube output. Our model achieves competitive results on three benchmark datasets and is about 7.6 times more efficient than the nearest competitor.}"}, "2010.07574": {"original_text": "  Evaluation of grammatical error correction (GEC) systems has primarily\nfocused on essays written by non-native learners of English, which however is\nonly part of the full spectrum of GEC applications. We aim to broaden the\ntarget domain of GEC and release CWEB, a new benchmark for GEC consisting of\nwebsite text generated by English speakers of varying levels of proficiency.\nWebsite data is a common and important domain that contains far fewer\ngrammatical errors than learner essays, which we show presents a challenge to\nstate-of-the-art GEC systems. We demonstrate that a factor behind this is the\ninability of systems to rely on a strong internal language model in low error\ndensity domains. We hope this work shall facilitate the development of\nopen-domain GEC models that generalize to different topics and genres.\n", "rewritten_text": "\\boxed{The evaluation of grammatical error correction systems has traditionally centered on essays written by non-native English speakers, but this is only a limited aspect of their potential applications. To expand the scope of these systems, we introduce CWEB, a new benchmark consisting of website texts written by English speakers with varying proficiency levels. Notably, website data, a common and important domain, contains significantly fewer grammatical errors than learner essays, posing a challenge to current state-of-the-art systems. We identify the inability of these systems to rely on a strong internal language model in low-error domains as a key factor contributing to this challenge. Our work aims to facilitate the development of open-domain grammatical error correction models that can generalize across different topics and genres.}"}, "2306.0045": {"original_text": "  Semantic segmentation is a crucial task in computer vision that involves\nsegmenting images into semantically meaningful regions at the pixel level.\nHowever, existing approaches often rely on expensive human annotations as\nsupervision for model training, limiting their scalability to large, unlabeled\ndatasets. To address this challenge, we present ZeroSeg, a novel method that\nleverages the existing pretrained vision-language (VL) model (e.g. CLIP) to\ntrain open-vocabulary zero-shot semantic segmentation models. Although acquired\nextensive knowledge of visual concepts, it is non-trivial to exploit knowledge\nfrom these VL models to the task of semantic segmentation, as they are usually\ntrained at an image level. ZeroSeg overcomes this by distilling the visual\nconcepts learned by VL models into a set of segment tokens, each summarizing a\nlocalized region of the target image. We evaluate ZeroSeg on multiple popular\nsegmentation benchmarks, including PASCAL VOC 2012, PASCAL Context, and COCO,\nin a zero-shot manner (i.e., no training or adaption on target segmentation\ndatasets). Our approach achieves state-of-the-art performance when compared to\nother zero-shot segmentation methods under the same training data, while also\nperforming competitively compared to strongly supervised methods. Finally, we\nalso demonstrated the effectiveness of ZeroSeg on open-vocabulary segmentation,\nthrough both human studies and qualitative visualizations.\n", "rewritten_text": "\\boxed{\nSemantic segmentation, a fundamental task in computer vision, involves dividing images into meaningful regions at the pixel level. However, current approaches rely heavily on human annotations, limiting their applicability to large, unlabeled datasets. To overcome this, we introduce ZeroSeg, a novel method that utilizes pre-trained vision-language models (e.g., CLIP) to train zero-shot semantic segmentation models without human annotations. Although these models possess extensive knowledge of visual concepts, it's challenging to apply this knowledge to semantic segmentation, as they're typically trained on image-level data. ZeroSeg addresses this by distilling visual concepts into segment tokens, each representing a localized region of the target image. We evaluate ZeroSeg on popular benchmarks (PASCAL VOC 2012, PASCAL Context, and COCO) without training or adaptation on target datasets, achieving state-of-the-art performance compared to other zero-shot methods and competitive results compared to strongly supervised methods. Additionally, we demonstrate ZeroSeg's effectiveness in open-vocabulary segmentation through human studies and visualizations."}, "2401.05166": {"original_text": "  In dyadic interactions, humans communicate their intentions and state of mind\nusing verbal and non-verbal cues, where multiple different facial reactions\nmight be appropriate in response to a specific speaker behaviour. Then, how to\ndevelop a machine learning (ML) model that can automatically generate multiple\nappropriate, diverse, realistic and synchronised human facial reactions from an\npreviously unseen speaker behaviour is a challenging task. Following the\nsuccessful organisation of the first REACT challenge (REACT 2023), this edition\nof the challenge (REACT 2024) employs a subset used by the previous challenge,\nwhich contains segmented 30-secs dyadic interaction clips originally recorded\nas part of the NOXI and RECOLA datasets, encouraging participants to develop\nand benchmark Machine Learning (ML) models that can generate multiple\nappropriate facial reactions (including facial image sequences and their\nattributes) given an input conversational partner's stimulus under various\ndyadic video conference scenarios. This paper presents: (i) the guidelines of\nthe REACT 2024 challenge; (ii) the dataset utilized in the challenge; and (iii)\nthe performance of the baseline systems on the two proposed sub-challenges:\nOffline Multiple Appropriate Facial Reaction Generation and Online Multiple\nAppropriate Facial Reaction Generation, respectively. The challenge baseline\ncode is publicly available at\nhttps://github.com/reactmultimodalchallenge/baseline_react2024.\n", "rewritten_text": "\\boxed{\nDeveloping a machine learning model that can generate multiple, diverse, realistic, and synchronized human facial reactions in response to unseen speaker behavior is a challenging task. To address this, the REACT 2024 challenge has been organized, building on the success of the previous REACT 2023 challenge. The challenge uses a subset of the NOXI and RECOLA datasets, consisting of 30-second dyadic interaction clips, and encourages participants to develop and benchmark machine learning models that can generate multiple appropriate facial reactions given an input conversational partner's stimulus under various dyadic video conference scenarios. This paper provides guidelines for the REACT 2024 challenge, describes the dataset used, and reports the performance of baseline systems on two sub-challenges: offline and online multiple appropriate facial reaction generation. The challenge baseline code is publicly available at https://github.com/reactmultimodalchallenge/baseline_react2024.\n}"}, "2406.10323": {"original_text": "  Most public instruction finetuning datasets are relatively small compared to\nthe closed source datasets used to train industry models. To study questions\nabout finetuning at scale, such as curricula and learning rate cooldown\nschedules, there is a need for industrial-scale datasets. However, this scale\nnecessitates a data generation process that is almost entirely automated. In\nthis work, we study methods for generating large instruction datasets from a\nsingle prompt. With little human oversight, we get LLMs to write diverse sets\nof instruction examples ranging from simple completion tasks to complex\nmulti-turn dialogs across a variety of subject areas. When finetuning a Llama-3\n8B base model, our dataset meets or exceeds both WizardLM and Ultrachat on both\nknowledge-intensive leaderboard tasks as well as conversational evaluations. We\nrelease our dataset, the \"generator\" prompts that created it, and our finetuned\nmodel checkpoints.\n", "rewritten_text": "\\boxed{Most publicly available finetuning datasets are limited in size compared to those used in industry, making it difficult to study large-scale finetuning techniques. To address this, we developed a method to generate large instruction datasets from a single prompt with minimal human oversight, producing diverse examples of instructions across various subjects. Our dataset, created using this method, outperforms existing datasets in both knowledge-intensive and conversational tasks when used to finetune a 8B Llama-3 base model. We are releasing our dataset, the prompts used to generate it, and the finetuned model checkpoints.}"}, "2303.12776": {"original_text": "  One-to-one label assignment in object detection has successfully obviated the\nneed for non-maximum suppression (NMS) as postprocessing and makes the pipeline\nend-to-end. However, it triggers a new dilemma as the widely used sparse\nqueries cannot guarantee a high recall, while dense queries inevitably bring\nmore similar queries and encounter optimization difficulties. As both sparse\nand dense queries are problematic, then what are the expected queries in\nend-to-end object detection? This paper shows that the solution should be Dense\nDistinct Queries (DDQ). Concretely, we first lay dense queries like traditional\ndetectors and then select distinct ones for one-to-one assignments. DDQ blends\nthe advantages of traditional and recent end-to-end detectors and significantly\nimproves the performance of various detectors including FCN, R-CNN, and DETRs.\nMost impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12\nepochs using a ResNet-50 backbone, outperforming all existing detectors in the\nsame setting. DDQ also shares the benefit of end-to-end detectors in crowded\nscenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchers\nto consider the complementarity between traditional methods and end-to-end\ndetectors. The source code can be found at\n\\url{https://github.com/jshilong/DDQ}.\n", "rewritten_text": "\\boxed{\nThe traditional approach of one-to-one label assignment in object detection has eliminated the need for non-maximum suppression, making the pipeline end-to-end. However, this approach raises a new challenge: sparse queries may not achieve high recall, while dense queries introduce optimization difficulties. To address this issue, we propose Dense Distinct Queries (DDQ), which combines the advantages of traditional and end-to-end detectors. DDQ involves laying dense queries and selecting distinct ones for one-to-one assignments, resulting in significant performance improvements in various detectors, including FCN, R-CNN, and DETRs. Notably, DDQ-DETR achieves 52.1 AP on the MS-COCO dataset within 12 epochs using a ResNet-50 backbone, outperforming existing detectors in the same setting. Additionally, DDQ excels in crowded scenes, achieving 93.8 AP on CrowdHuman. We hope that DDQ will inspire researchers to explore the complementarity between traditional methods and end-to-end detectors. The source code is available at https://github.com/jshilong/DDQ.\n}"}, "2303.1112": {"original_text": "  Positional reasoning is the process of ordering unsorted parts contained in a\nset into a consistent structure. We present Positional Diffusion, a\nplug-and-play graph formulation with Diffusion Probabilistic Models to address\npositional reasoning. We use the forward process to map elements' positions in\na set to random positions in a continuous space. Positional Diffusion learns to\nreverse the noising process and recover the original positions through an\nAttention-based Graph Neural Network. We conduct extensive experiments with\nbenchmark datasets including two puzzle datasets, three sentence ordering\ndatasets, and one visual storytelling dataset, demonstrating that our method\noutperforms long-lasting research on puzzle solving with up to +18% compared to\nthe second-best deep learning method, and performs on par against the\nstate-of-the-art methods on sentence ordering and visual storytelling. Our work\nhighlights the suitability of diffusion models for ordering problems and\nproposes a novel formulation and method for solving various ordering tasks.\nProject website at https://iit-pavis.github.io/Positional_Diffusion/\n", "rewritten_text": "\\boxed{\nPositional reasoning, the process of organizing unordered elements into a coherent structure, is addressed through Positional Diffusion, a novel graph-based approach that leverages Diffusion Probabilistic Models. By mapping elements to random positions in a continuous space, Positional Diffusion learns to recover original positions using an Attention-based Graph Neural Network. Experimental results on various benchmark datasets, including puzzle solving, sentence ordering, and visual storytelling, demonstrate the method's superiority, outperforming existing deep learning methods by up to 18% and matching state-of-the-art performance in other tasks. This work showcases the effectiveness of diffusion models for ordering problems and introduces a new formulation and method for solving diverse ordering tasks."}, "2311.13735": {"original_text": "  Recent advances in large language models (LLMs) show potential for clinical\napplications, such as clinical decision support and trial recommendations.\nHowever, the GPT-4 LLM predicts an excessive number of ICD codes for medical\ncoding tasks, leading to high recall but low precision. To tackle this\nchallenge, we introduce LLM-codex, a two-stage approach to predict ICD codes\nthat first generates evidence proposals using an LLM and then employs an\nLSTM-based verification stage. The LSTM learns from both the LLM's high recall\nand human expert's high precision, using a custom loss function. Our model is\nthe only approach that simultaneously achieves state-of-the-art results in\nmedical coding accuracy, accuracy on rare codes, and sentence-level evidence\nidentification to support coding decisions without training on human-annotated\nevidence according to experiments on the MIMIC dataset.\n", "rewritten_text": "\\boxed{Recent breakthroughs in large language models (LLMs) have shown promise in clinical applications, including clinical decision support and trial recommendations. However, the GPT-4 LLM has a tendency to predict an excessive number of ICD codes, resulting in high recall but low precision. To address this issue, we propose LLM-codex, a two-stage approach that first uses an LLM to generate evidence proposals, followed by an LSTM-based verification stage. The LSTM is trained on both the LLM's high recall and human expert's high precision, utilizing a custom loss function. According to experiments on the MIMIC dataset, our model achieves state-of-the-art results in medical coding accuracy, rare code accuracy, and sentence-level evidence identification, all without relying on human-annotated evidence.}"}, "2209.14575": {"original_text": "  In this paper, we investigate the problem of bit allocation in Neural Video\nCompression (NVC). First, we reveal that a recent bit allocation approach\nclaimed to be optimal is, in fact, sub-optimal due to its implementation.\nSpecifically, we find that its sub-optimality lies in the improper application\nof semi-amortized variational inference (SAVI) on latent with non-factorized\nvariational posterior. Then, we show that the corrected version of SAVI on\nnon-factorized latent requires recursively applying back-propagating through\ngradient ascent, based on which we derive the corrected optimal bit allocation\nalgorithm. Due to the computational in-feasibility of the corrected bit\nallocation, we design an efficient approximation to make it practical.\nEmpirical results show that our proposed correction significantly improves the\nincorrect bit allocation in terms of R-D performance and bitrate error, and\noutperforms all other bit allocation methods by a large margin. The source code\nis provided in the supplementary material.\n", "rewritten_text": "\\boxed{\nThis paper re-examines the bit allocation problem in Neural Video Compression (NVC) and debunks a recent approach claimed to be optimal, revealing its sub-optimality due to improper implementation of semi-amortized variational inference (SAVI) on non-factorized latent variables. We correct this by recursively applying back-propagation through gradient ascent, deriving an optimal bit allocation algorithm. However, due to computational limitations, we develop an efficient approximation. Our empirical results demonstrate significant improvements in rate-distortion performance and bitrate error, outperforming existing bit allocation methods by a large margin. The source code is provided in the supplementary material.\n}"}, "2302.03744": {"original_text": "  The ability to perceive and understand 3D scenes is crucial for many\napplications in computer vision and robotics. Inverse graphics is an appealing\napproach to 3D scene understanding that aims to infer the 3D scene structure\nfrom 2D images. In this paper, we introduce probabilistic modeling to the\ninverse graphics framework to quantify uncertainty and achieve robustness in 6D\npose estimation tasks. Specifically, we propose 3D Neural Embedding Likelihood\n(3DNEL) as a unified probabilistic model over RGB-D images, and develop\nefficient inference procedures on 3D scene descriptions. 3DNEL effectively\ncombines learned neural embeddings from RGB with depth information to improve\nrobustness in sim-to-real 6D object pose estimation from RGB-D images.\nPerformance on the YCB-Video dataset is on par with state-of-the-art yet is\nmuch more robust in challenging regimes. In contrast to discriminative\napproaches, 3DNEL's probabilistic generative formulation jointly models\nmultiple objects in a scene, quantifies uncertainty in a principled way, and\nhandles object pose tracking under heavy occlusion. Finally, 3DNEL provides a\nprincipled framework for incorporating prior knowledge about the scene and\nobjects, which allows natural extension to additional tasks like camera pose\ntracking from video.\n", "rewritten_text": "\\boxed{\nUnderstanding 3D scenes is vital in computer vision and robotics. Inverse graphics, which involves inferring 3D scene structure from 2D images, is a promising approach. This paper introduces probabilistic modeling to inverse graphics to quantify uncertainty and improve robustness in 6D pose estimation tasks. The proposed 3D Neural Embedding Likelihood (3DNEL) model combines neural embeddings from RGB images with depth information to enhance robustness in sim-to-real 6D object pose estimation. 3DNEL outperforms state-of-the-art methods on the YCB-Video dataset, particularly in challenging scenarios. Its probabilistic generative formulation allows it to model multiple objects, quantify uncertainty, and track object poses under heavy occlusion. Additionally, 3DNEL provides a framework for incorporating prior knowledge, enabling extension to tasks like camera pose tracking from video."}, "2005.04551": {"original_text": "  A common approach to localize 3D human joints in a synchronized and\ncalibrated multi-view setup consists of two-steps: (1) apply a 2D detector\nseparately on each view to localize joints in 2D, and (2) perform robust\ntriangulation on 2D detections from each view to acquire the 3D joint\nlocations. However, in step 1, the 2D detector is limited to solving\nchallenging cases which could potentially be better resolved in 3D, such as\nocclusions and oblique viewing angles, purely in 2D without leveraging any 3D\ninformation. Therefore, we propose the differentiable \"epipolar transformer\",\nwhich enables the 2D detector to leverage 3D-aware features to improve 2D pose\nestimation. The intuition is: given a 2D location p in the current view, we\nwould like to first find its corresponding point p' in a neighboring view, and\nthen combine the features at p' with the features at p, thus leading to a\n3D-aware feature at p. Inspired by stereo matching, the epipolar transformer\nleverages epipolar constraints and feature matching to approximate the features\nat p'. Experiments on InterHand and Human3.6M show that our approach has\nconsistent improvements over the baselines. Specifically, in the condition\nwhere no external data is used, our Human3.6M model trained with ResNet-50\nbackbone and image size 256 x 256 outperforms state-of-the-art by 4.23 mm and\nachieves MPJPE 26.9 mm.\n", "rewritten_text": "\\boxed{\nA typical method for localizing 3D human joints in a multi-view setup involves two steps: first, applying a 2D detector to each view to localize joints in 2D, and then, performing robust triangulation on the 2D detections to obtain the 3D joint locations. However, this approach has limitations, as the 2D detector struggles to resolve challenging cases, such as occlusions and oblique viewing angles, without leveraging 3D information. To address this, we propose the \"epipolar transformer\", which enables the 2D detector to utilize 3D-aware features to improve 2D pose estimation. The idea is to find the corresponding point in a neighboring view and combine its features with the original point, resulting in a 3D-aware feature. Inspired by stereo matching, the epipolar transformer uses epipolar constraints and feature matching to approximate the features at the corresponding point. Our approach shows consistent improvements over baselines in experiments on InterHand and Human3.6M datasets, with our Human3.6M model achieving a mean per-joint position error (MPJPE) of 26.9 mm, outperforming the state-of-the-art by 4.23 mm.\n}"}, "1912.0643": {"original_text": "  Annotating videos is cumbersome, expensive and not scalable. Yet, many strong\nvideo models still rely on manually annotated data. With the recent\nintroduction of the HowTo100M dataset, narrated videos now offer the\npossibility of learning video representations without manual supervision. In\nthis work we propose a new learning approach, MIL-NCE, capable of addressing\nmisalignments inherent to narrated videos. With this approach we are able to\nlearn strong video representations from scratch, without the need for any\nmanual annotation. We evaluate our representations on a wide range of four\ndownstream tasks over eight datasets: action recognition (HMDB-51, UCF-101,\nKinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization\n(YouTube-8M Segments, CrossTask) and action segmentation (COIN). Our method\noutperforms all published self-supervised approaches for these tasks as well as\nseveral fully supervised baselines.\n", "rewritten_text": "\\boxed{Manual video annotation is a time-consuming, costly, and unscalable process, yet many powerful video models rely on it. However, with the introduction of the HowTo100M dataset, it's now possible to learn video representations without human supervision. This paper proposes a new approach called MIL-NCE, which can handle the misalignments found in narrated videos. Using this method, we can learn strong video representations from scratch without any manual annotation. We evaluate our approach on four downstream tasks across eight datasets, outperforming all published self-supervised methods and several fully supervised baselines.}"}, "2211.09795": {"original_text": "  Diffusion models have become the go-to method for many generative tasks,\nparticularly for image-to-image generation tasks such as super-resolution and\ninpainting. Current diffusion-based methods do not provide statistical\nguarantees regarding the generated results, often preventing their use in\nhigh-stakes situations. To bridge this gap, we construct a confidence interval\naround each generated pixel such that the true value of the pixel is guaranteed\nto fall within the interval with a probability set by the user. Since diffusion\nmodels parametrize the data distribution, a straightforward way of constructing\nsuch intervals is by drawing multiple samples and calculating their bounds.\nHowever, this method has several drawbacks: i) slow sampling speeds ii)\nsuboptimal bounds iii) requires training a diffusion model per task. To\nmitigate these shortcomings we propose Conffusion, wherein we fine-tune a\npre-trained diffusion model to predict interval bounds in a single forward\npass. We show that Conffusion outperforms the baseline method while being three\norders of magnitude faster.\n", "rewritten_text": "\\boxed{Diffusion models have become a popular choice for various generative tasks, especially image-to-image generation tasks like super-resolution and inpainting. However, current diffusion-based methods lack statistical guarantees for their generated results, limiting their use in critical situations. To address this, we develop a method to construct a confidence interval for each generated pixel, ensuring that the true pixel value falls within the interval with a user-set probability. While a simple approach would be to draw multiple samples and calculate their bounds, this method is slow, produces suboptimal bounds, and requires task-specific model training. Instead, we propose Conffusion, which fine-tunes a pre-trained diffusion model to predict interval bounds in a single forward pass, outperforming the baseline method while being significantly faster.}"}, "2404.14248": {"original_text": "  This paper reviews the NTIRE 2024 low light image enhancement challenge,\nhighlighting the proposed solutions and results. The aim of this challenge is\nto discover an effective network design or solution capable of generating\nbrighter, clearer, and visually appealing results when dealing with a variety\nof conditions, including ultra-high resolution (4K and beyond), non-uniform\nillumination, backlighting, extreme darkness, and night scenes. A notable total\nof 428 participants registered for the challenge, with 22 teams ultimately\nmaking valid submissions. This paper meticulously evaluates the\nstate-of-the-art advancements in enhancing low-light images, reflecting the\nsignificant progress and creativity in this field.\n", "rewritten_text": "\\boxed{This paper provides an overview of the NTIRE 2024 low light image enhancement challenge, showcasing the innovative solutions and outcomes. The challenge aims to develop a robust network design or solution that can produce brighter, clearer, and more visually appealing images in diverse conditions, including ultra-high resolution, non-uniform illumination, backlighting, extreme darkness, and night scenes. With 428 participants registered and 22 teams submitting valid entries, this paper thoroughly assesses the cutting-edge advancements in low-light image enhancement, highlighting the remarkable progress and creativity in this field.}"}, "2311.01907": {"original_text": "  Automatic simplification can help laypeople to comprehend complex scientific\ntext. Language models are frequently applied to this task by translating from\ncomplex to simple language. In this paper, we describe our system based on\nLlama 2, which ranked first in the PLABA shared task addressing the\nsimplification of biomedical text. We find that the large portion of shared\ntokens between input and output leads to weak training signals and\nconservatively editing models. To mitigate these issues, we propose\nsentence-level and token-level loss weights. They give higher weight to\nmodified tokens, indicated by edit distance and edit operations, respectively.\nWe conduct an empirical evaluation on the PLABA dataset and find that both\napproaches lead to simplifications closer to those created by human annotators\n(+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x /\n1.8x edit distance) compared to the same model fine-tuned with standard cross\nentropy. We furthermore show that the hyperparameter $\\lambda$ in token-level\nloss weights can be used to control the edit distance and the simplicity level\n(FKGL).\n", "rewritten_text": "\\boxed{Simplifying complex scientific texts can be achieved through automatic simplification, which can be done using language models that translate complex language into simpler language. Our system, based on Llama 2, was ranked first in the PLABA shared task for simplifying biomedical texts. However, we found that the similarity between input and output texts can lead to weak training signals and conservative editing models. To address this, we propose using sentence-level and token-level loss weights, which give more importance to modified tokens. Our evaluation on the PLABA dataset shows that these approaches result in simplifications that are closer to those created by human annotators, use simpler language, and involve more edits. Additionally, we found that the hyperparameter \u03bb in token-level loss weights can be used to control the level of editing and simplicity.}"}, "2402.12923": {"original_text": "  In recent years, 3D point clouds (PCs) have gained significant attention due\nto their diverse applications across various fields, such as computer vision\n(CV), condition monitoring (CM), virtual reality, robotics, autonomous driving,\netc. Deep learning (DL) has proven effective in leveraging 3D PCs to address\nvarious challenges encountered in 2D vision. However, applying deep neural\nnetworks (DNNs) to process 3D PCs presents unique challenges. This paper\nprovides an in-depth review of recent advancements in DL-based industrial CM\nusing 3D PCs, with a specific focus on defect shape classification and\nsegmentation within industrial applications. Recognizing the crucial role of\nthese aspects in industrial maintenance, the paper offers insightful\nobservations on the strengths and limitations of the reviewed DL-based PC\nprocessing methods. This knowledge synthesis aims to contribute to\nunderstanding and enhancing CM processes, particularly within the framework of\nremaining useful life (RUL), in industrial systems.\n", "rewritten_text": "\\boxed{\nIn recent years, 3D point clouds have become increasingly popular across various fields, including computer vision, condition monitoring, virtual reality, robotics, and autonomous driving, due to their diverse applications. While deep learning has shown promise in addressing 2D vision challenges using 3D point clouds, processing these clouds with deep neural networks poses unique challenges. This paper provides a comprehensive review of recent advancements in deep learning-based condition monitoring using 3D point clouds, focusing on defect shape classification and segmentation in industrial applications. The paper highlights the importance of these aspects in industrial maintenance and offers insightful observations on the strengths and limitations of reviewed deep learning-based point cloud processing methods, aiming to enhance condition monitoring processes, particularly in predicting remaining useful life, in industrial systems.\n}"}, "2402.02082": {"original_text": "  Speculative decoding is a relatively new decoding framework that leverages\nsmall and efficient draft models to reduce the latency of LLMs. In this study,\nwe introduce GliDe and CaPE, two low-hassle modifications to vanilla\nspeculative decoding to further improve the decoding speed of a frozen LLM.\nSpecifically, GliDe is a modified draft model architecture that reuses the\ncached keys and values from the target LLM, while CaPE is a proposal expansion\nmethod that uses the draft model's confidence scores to help select additional\ncandidate tokens for verification. Extensive experiments on different\nbenchmarks demonstrate that our proposed GliDe draft model significantly\nreduces the expected decoding latency. Additional evaluation using walltime\nreveals that GliDe can accelerate Vicuna models up to 2.17x and further extend\nthe improvement to 2.61x with CaPE. We will release our code, data, and the\ntrained draft models.\n", "rewritten_text": "\\boxed{This study presents two novel modifications to speculative decoding, a framework that uses small and efficient draft models to reduce the latency of large language models (LLMs). The proposed modifications, GliDe and CaPE, aim to further improve the decoding speed of a frozen LLM. GliDe is a modified draft model architecture that leverages cached keys and values from the target LLM, while CaPE is a proposal expansion method that utilizes the draft model's confidence scores to select additional candidate tokens for verification. Experimental results show that GliDe significantly reduces expected decoding latency, with acceleration of up to 2.17x for Vicuna models, and up to 2.61x when combined with CaPE. The code, data, and trained draft models will be made publicly available.}"}, "2302.14354": {"original_text": "  The cultural heritage buildings (CHB), which are part of mankind's history\nand identity, are in constant danger of damage or in extreme situations total\ndestruction. That being said, it's of utmost importance to preserve them by\nidentifying the existent, or presumptive, defects using novel methods so that\nrenovation processes can be done in a timely manner and with higher accuracy.\nThe main goal of this research is to use new deep learning (DL) methods in the\nprocess of preserving CHBs (situated in Iran); a goal that has been neglected\nespecially in developing countries such as Iran, as these countries still\npreserve their CHBs using manual, and even archaic, methods that need direct\nhuman supervision. Having proven their effectiveness and performance when it\ncomes to processing images, the convolutional neural networks (CNN) are a\nstaple in computer vision (CV) literacy and this paper is not exempt. When\nlacking enough CHB images, training a CNN from scratch would be very difficult\nand prone to overfitting; that's why we opted to use a technique called\ntransfer learning (TL) in which we used pre-trained ResNet, MobileNet, and\nInception networks, for classification. Even more, the Grad-CAM was utilized to\nlocalize the defects to some extent. The final results were very favorable\nbased on those of similar research. The final proposed model can pave the way\nfor moving from manual to unmanned CHB conservation, hence an increase in\naccuracy and a decrease in human-induced errors.\n", "rewritten_text": "\\boxed{\nCultural heritage buildings, which are essential to human history and identity, are vulnerable to damage or destruction. To preserve them, it is crucial to identify defects using innovative methods, enabling timely and accurate renovation. This research aims to utilize deep learning methods, specifically convolutional neural networks (CNN), to preserve cultural heritage buildings in Iran, a country that still relies on manual and outdated methods. Due to the limited availability of images, transfer learning was employed using pre-trained ResNet, MobileNet, and Inception networks for classification. Additionally, Grad-CAM was used to localize defects. The results were promising, comparable to similar studies. The proposed model has the potential to revolutionize cultural heritage building conservation by transitioning from manual to unmanned methods, thereby increasing accuracy and reducing human errors.\n}"}, "2303.0149": {"original_text": "  Language identification is an important first step in many IR and NLP\napplications. Most publicly available language identification datasets,\nhowever, are compiled under the assumption that the gold label of each instance\nis determined by where texts are retrieved from. Research has shown that this\nis a problematic assumption, particularly in the case of very similar languages\n(e.g., Croatian and Serbian) and national language varieties (e.g., Brazilian\nand European Portuguese), where texts may contain no distinctive marker of the\nparticular language or variety. To overcome this important limitation, this\npaper presents DSL True Labels (DSL-TL), the first human-annotated multilingual\ndataset for language variety identification. DSL-TL contains a total of 12,900\ninstances in Portuguese, split between European Portuguese and Brazilian\nPortuguese; Spanish, split between Argentine Spanish and Castilian Spanish; and\nEnglish, split between American English and British English. We trained\nmultiple models to discriminate between these language varieties, and we\npresent the results in detail. The data and models presented in this paper\nprovide a reliable benchmark toward the development of robust and fairer\nlanguage variety identification systems. We make DSL-TL freely available to the\nresearch community.\n", "rewritten_text": "\\boxed{Identifying the language of a text is a crucial step in many information retrieval and natural language processing applications. However, most existing language identification datasets are flawed because they assume that the language of a text can be determined by its source. This assumption is problematic, especially for similar languages or regional variations, where texts may not contain distinctive language markers. To address this limitation, we introduce DSL True Labels (DSL-TL), a novel, human-annotated dataset for identifying language varieties. DSL-TL consists of 12,900 instances in three languages: Portuguese (European and Brazilian), Spanish (Argentine and Castilian), and English (American and British). We trained multiple models to distinguish between these language varieties and present the results in detail. Our dataset and models provide a reliable benchmark for developing robust and fair language variety identification systems, and we make DSL-TL freely available to the research community.}"}, "2004.0827": {"original_text": "  In this paper, we tackle the task of automatically analyzing 3D volumetric\nscans obtained from computed tomography (CT) devices. In particular, we address\na particular task for which data is very limited: the segmentation of ancient\nEgyptian mummies CT scans. We aim at digitally unwrapping the mummy and\nidentify different segments such as body, bandages and jewelry. The problem is\ncomplex because of the lack of annotated data for the different semantic\nregions to segment, thus discouraging the use of strongly supervised\napproaches. We, therefore, propose a weakly supervised and efficient\ninteractive segmentation method to solve this challenging problem. After\nsegmenting the wrapped mummy from its exterior region using histogram analysis\nand template matching, we first design a voxel distance measure to find an\napproximate solution for the body and bandage segments. Here, we use geodesic\ndistances since voxel features as well as spatial relationship among voxels is\nincorporated in this measure. Next, we refine the solution using a GrabCut\nbased segmentation together with a tracking method on the slices of the scan\nthat assigns labels to different regions in the volume, using limited\nsupervision in the form of scribbles drawn by the user. The efficiency of the\nproposed method is demonstrated using visualizations and validated through\nquantitative measures and qualitative unwrapping of the mummy.\n", "rewritten_text": "\\boxed{\nThis paper focuses on automatically analyzing 3D CT scans of ancient Egyptian mummies, with the goal of digitally unwrapping the mummy and identifying different segments such as the body, bandages, and jewelry. Due to the limited availability of annotated data, a weakly supervised and interactive segmentation method is proposed. The approach involves initially separating the wrapped mummy from its exterior region using histogram analysis and template matching. Then, a voxel distance measure based on geodesic distances is used to approximate the body and bandage segments. Finally, the solution is refined using a GrabCut-based segmentation and tracking method, with limited user input in the form of scribbles. The effectiveness of the proposed method is demonstrated through visualizations and validated using quantitative measures and qualitative unwrapping results."}, "1607.06871": {"original_text": "  The \"interpretation through synthesis\" approach to analyze face images,\nparticularly Active Appearance Models (AAMs) method, has become one of the most\nsuccessful face modeling approaches over the last two decades. AAM models have\nability to represent face images through synthesis using a controllable\nparameterized Principal Component Analysis (PCA) model. However, the accuracy\nand robustness of the synthesized faces of AAM are highly depended on the\ntraining sets and inherently on the generalizability of PCA subspaces. This\npaper presents a novel Deep Appearance Models (DAMs) approach, an efficient\nreplacement for AAMs, to accurately capture both shape and texture of face\nimages under large variations. In this approach, three crucial components\nrepresented in hierarchical layers are modeled using the Deep Boltzmann\nMachines (DBM) to robustly capture the variations of facial shapes and\nappearances. DAMs are therefore superior to AAMs in inferencing a\nrepresentation for new face images under various challenging conditions. The\nproposed approach is evaluated in various applications to demonstrate its\nrobustness and capabilities, i.e. facial super-resolution reconstruction,\nfacial off-angle reconstruction or face frontalization, facial occlusion\nremoval and age estimation using challenging face databases, i.e. Labeled Face\nParts in the Wild (LFPW), Helen and FG-NET. Comparing to AAMs and other deep\nlearning based approaches, the proposed DAMs achieve competitive results in\nthose applications, thus this showed their advantages in handling occlusions,\nfacial representation, and reconstruction.\n", "rewritten_text": "\\boxed{\nThe \"interpretation through synthesis\" approach, particularly Active Appearance Models (AAMs), has been a dominant method for face image analysis over the past two decades. AAMs use a parameterized Principal Component Analysis (PCA) model to synthesize face images, but their accuracy and robustness rely heavily on the quality of the training sets and the generalizability of PCA subspaces. This paper introduces a novel alternative, Deep Appearance Models (DAMs), which leverages hierarchical layers and Deep Boltzmann Machines (DBM) to model three key components, enabling robust capture of facial shape and texture variations. DAMs outperform AAMs in inferring representations for new face images under challenging conditions. The proposed approach is evaluated in various applications, including facial super-resolution reconstruction, off-angle reconstruction, facial occlusion removal, and age estimation, using datasets such as LFPW, Helen, and FG-NET. Compared to AAMs and other deep learning-based methods, DAMs achieve competitive results, demonstrating their advantages in handling occlusions, facial representation, and reconstruction.\n}"}, "2105.11872": {"original_text": "  Despite recent advances, standard sequence labeling systems often fail when\nprocessing noisy user-generated text or consuming the output of an Optical\nCharacter Recognition (OCR) process. In this paper, we improve the noise-aware\ntraining method by proposing an empirical error generation approach that\nemploys a sequence-to-sequence model trained to perform translation from\nerror-free to erroneous text. Using an OCR engine, we generated a large\nparallel text corpus for training and produced several real-world noisy\nsequence labeling benchmarks for evaluation. Moreover, to overcome the data\nsparsity problem that exacerbates in the case of imperfect textual input, we\nlearned noisy language model-based embeddings. Our approach outperformed the\nbaseline noise generation and error correction techniques on the erroneous\nsequence labeling data sets. To facilitate future research on robustness, we\nmake our code, embeddings, and data conversion scripts publicly available.\n", "rewritten_text": "\\boxed{This paper presents an improved method for training sequence labeling systems to handle noisy text, such as user-generated content or OCR output. The approach involves training a sequence-to-sequence model to generate errors in text, and using this model to create a large parallel corpus for training. Additionally, the method incorporates noisy language model-based embeddings to overcome data sparsity issues. The proposed approach outperforms existing noise generation and error correction techniques on real-world noisy sequence labeling benchmarks. The code, embeddings, and data conversion scripts are made publicly available to facilitate further research on robustness.}"}, "2102.00062": {"original_text": "  In this paper, we present a method of clothes retargeting; generating the\npotential poses and deformations of a given 3D clothing template model to fit\nonto a person in a single RGB image. The problem is fundamentally ill-posed as\nattaining the ground truth data is impossible, i.e., images of people wearing\nthe different 3D clothing template model at exact same pose. We address this\nchallenge by utilizing large-scale synthetic data generated from physical\nsimulation, allowing us to map 2D dense body pose to 3D clothing deformation.\nWith the simulated data, we propose a semi-supervised learning framework that\nvalidates the physical plausibility of the 3D deformation by matching with the\nprescribed body-to-cloth contact points and clothing silhouette to fit onto the\nunlabeled real images. A new neural clothes retargeting network (CRNet) is\ndesigned to integrate the semi-supervised retargeting task in an end-to-end\nfashion. In our evaluation, we show that our method can predict the realistic\n3D pose and deformation field needed for retargeting clothes models in\nreal-world examples.\n", "rewritten_text": "\\boxed{This paper introduces a method for retargeting clothes, which involves generating possible poses and deformations of a 3D clothing template to fit a person in a single RGB image. Since it's impossible to obtain exact ground truth data, we use large-scale synthetic data from physical simulations to map 2D body pose to 3D clothing deformation. Our semi-supervised learning framework ensures physically plausible 3D deformations by matching with body-to-cloth contact points and clothing silhouettes, and can be applied to unlabeled real images. We propose a neural network, CRNet, that integrates this task in an end-to-end manner, and demonstrate its ability to predict realistic 3D poses and deformation fields for retargeting clothes models in real-world scenarios.}"}, "1810.0578": {"original_text": "  Pose estimation is a widely explored problem, enabling many robotic tasks\nsuch as grasping and manipulation. In this paper, we tackle the problem of pose\nestimation for objects that exhibit rotational symmetry, which are common in\nman-made and industrial environments. In particular, our aim is to infer poses\nfor objects not seen at training time, but for which their 3D CAD models are\navailable at test time. Previous work has tackled this problem by learning to\ncompare captured views of real objects with the rendered views of their 3D CAD\nmodels, by embedding them in a joint latent space using neural networks. We\nshow that sidestepping the issue of symmetry in this scenario during training\nleads to poor performance at test time. We propose a model that reasons about\nrotational symmetry during training by having access to only a small set of\nsymmetry-labeled objects, whereby exploiting a large collection of unlabeled\nCAD models. We demonstrate that our approach significantly outperforms a\nnaively trained neural network on a new pose dataset containing images of tools\nand hardware.\n", "rewritten_text": "\\boxed{This paper addresses the problem of pose estimation for objects with rotational symmetry, which is common in industrial and man-made environments. The goal is to estimate poses for unseen objects at test time, given their 3D CAD models. Previous methods have used neural networks to compare real object views with rendered CAD model views, but they neglect the symmetry issue during training, leading to poor test performance. Our approach overcomes this by incorporating symmetry reasoning during training, using a small set of labeled objects and a large collection of unlabeled CAD models. We show that our method significantly outperforms a naively trained neural network on a new dataset of tool and hardware images.}"}, "2012.15832": {"original_text": "  Increasing the input length has been a driver of progress in language\nmodeling with transformers. We identify conditions where shorter inputs are not\nharmful, and achieve perplexity and efficiency improvements through two new\nmethods that decrease input length. First, we show that initially training a\nmodel on short subsequences before moving on to longer ones both reduces\noverall training time and, surprisingly, substantially improves perplexity.\nSecond, we show how to improve the efficiency of recurrence methods in\ntransformers, which let models condition on previously processed tokens when\ngenerating sequences that exceed the maximal length the transformer can handle\nat once. Existing methods require computationally expensive relative position\nembeddings; we introduce a simple alternative of adding absolute position\nembeddings to queries and keys instead of to word embeddings, which efficiently\nproduces superior results. We show that these recurrent models also benefit\nfrom short input lengths. Combining these techniques speeds up training by a\nfactor of 1.65, reduces memory usage, and substantially improves perplexity on\nWikiText-103, without adding any parameters.\n", "rewritten_text": "\\boxed{While increasing input length has driven progress in language modeling with transformers, we found that shorter inputs can be beneficial under certain conditions. We developed two new methods to reduce input length, leading to improved perplexity and efficiency. The first method involves initially training models on short subsequences before moving to longer ones, which reduces training time and surprisingly improves perplexity. The second method enhances the efficiency of recurrence methods in transformers by adding absolute position embeddings to queries and keys, producing superior results without computationally expensive relative position embeddings. By combining these techniques, we achieved a 1.65x speedup in training, reduced memory usage, and improved perplexity on WikiText-103 without adding any parameters.}"}, "cs/0006021": {"original_text": "  Systems now exist which are able to compile unification grammars into\nlanguage models that can be included in a speech recognizer, but it is so far\nunclear whether non-trivial linguistically principled grammars can be used for\nthis purpose. We describe a series of experiments which investigate the\nquestion empirically, by incrementally constructing a grammar and discovering\nwhat problems emerge when successively larger versions are compiled into finite\nstate graph representations and used as language models for a medium-vocabulary\nrecognition task.\n", "rewritten_text": "\\boxed{Researchers have developed systems that can convert unification grammars into language models for speech recognition, but it's unclear if complex, linguistically-based grammars can be used effectively. To investigate, we conducted a series of experiments where we built a grammar incrementally and identified the challenges that arose when compiling larger versions into finite state graph representations for a medium-sized vocabulary recognition task.}"}, "2209.13351": {"original_text": "  Accurately and timely detecting multiscale small objects that contain tens of\npixels from remote sensing images (RSI) remains challenging. Most of the\nexisting solutions primarily design complex deep neural networks to learn\nstrong feature representations for objects separated from the background, which\noften results in a heavy computation burden. In this article, we propose an\naccurate yet fast object detection method for RSI, named SuperYOLO, which fuses\nmultimodal data and performs high-resolution (HR) object detection on\nmultiscale objects by utilizing the assisted super resolution (SR) learning and\nconsidering both the detection accuracy and computation cost. First, we utilize\na symmetric compact multimodal fusion (MF) to extract supplementary information\nfrom various data for improving small object detection in RSI. Furthermore, we\ndesign a simple and flexible SR branch to learn HR feature representations that\ncan discriminate small objects from vast backgrounds with low-resolution (LR)\ninput, thus further improving the detection accuracy. Moreover, to avoid\nintroducing additional computation, the SR branch is discarded in the inference\nstage, and the computation of the network model is reduced due to the LR input.\nExperimental results show that, on the widely used VEDAI RS dataset, SuperYOLO\nachieves an accuracy of 75.09% (in terms of mAP50 ), which is more than 10%\nhigher than the SOTA large models, such as YOLOv5l, YOLOv5x, and RS designed\nYOLOrs. Meanwhile, the parameter size and GFLOPs of SuperYOLO are about 18\ntimes and 3.8 times less than YOLOv5x. Our proposed model shows a favorable\naccuracy and speed tradeoff compared to the state-of-the-art models. The code\nwill be open-sourced at https://github.com/icey-zhang/SuperYOLO.\n", "rewritten_text": "\\boxed{\nDetecting small objects in remote sensing images (RSI) remains a challenge due to the need for accurate and timely detection. Existing solutions often rely on complex deep neural networks, which can be computationally expensive. To address this, we propose SuperYOLO, a fast and accurate object detection method that fuses multimodal data and performs high-resolution object detection on multiscale objects. Our approach uses a symmetric compact multimodal fusion to extract supplementary information from various data, improving small object detection in RSI. Additionally, we design a simple and flexible super resolution branch to learn high-resolution feature representations that can distinguish small objects from backgrounds with low-resolution input. By discarding the super resolution branch during inference, we reduce the computation cost. Experimental results on the VEDAI RS dataset show that SuperYOLO achieves an accuracy of 75.09% (mAP50), outperforming state-of-the-art models like YOLOv5l, YOLOv5x, and YOLOrs, while requiring significantly fewer parameters and computations. Our proposed model offers a favorable tradeoff between accuracy and speed. The code is available at https://github.com/icey-zhang/SuperYOLO.\n}"}, "2109.01958": {"original_text": "  Transformer-based pre-trained language models boost the performance of\nopen-domain dialogue systems. Prior works leverage Transformer-based\npre-trained language models to generate texts with desired attributes in two\ngeneral approaches: (1) gradient-based methods: updating all latent\nrepresentations of pre-trained models with gradients from attribute models; (2)\nweighted-decoding methods: re-ranking beam candidates from pre-trained models\nwith attribute functions. However, gradient-based methods lead to high\ncomputation cost and can easily get overfitted on small training sets, while\nweighted-decoding methods are inherently constrained by the low-variance\nhigh-bias pre-trained model. In this work, we propose a novel approach to\ncontrol the generation of Transformer-based pre-trained language models: the\nSideControl framework, which leverages a novel control attributes loss to\nincorporate useful control signals, and is shown to perform well with very\nlimited training samples. We evaluate our proposed method on two benchmark\nopen-domain dialogue datasets, and results show that the SideControl framework\nhas better controllability, higher generation quality and better\nsample-efficiency than existing gradient-based and weighted-decoding baselines.\n", "rewritten_text": "\\boxed{Transformer-based pre-trained language models have revolutionized open-domain dialogue systems, but existing methods to generate texts with desired attributes have limitations. Gradient-based methods are computationally expensive and prone to overfitting, while weighted-decoding methods are restricted by the pre-trained model's bias. This paper proposes a novel approach, the SideControl framework, which incorporates control signals through a custom loss function and achieves better controllability, generation quality, and sample efficiency with minimal training data. Experiments on two benchmark datasets demonstrate the superiority of SideControl over existing methods.}"}, "2205.02022": {"original_text": "  Recent advances in the pre-training of language models leverage large-scale\ndatasets to create multilingual models. However, low-resource languages are\nmostly left out in these datasets. This is primarily because many widely spoken\nlanguages are not well represented on the web and therefore excluded from the\nlarge-scale crawls used to create datasets. Furthermore, downstream users of\nthese models are restricted to the selection of languages originally chosen for\npre-training. This work investigates how to optimally leverage existing\npre-trained models to create low-resource translation systems for 16 African\nlanguages. We focus on two questions: 1) How can pre-trained models be used for\nlanguages not included in the initial pre-training? and 2) How can the\nresulting translation models effectively transfer to new domains? To answer\nthese questions, we create a new African news corpus covering 16 languages, of\nwhich eight languages are not part of any existing evaluation dataset. We\ndemonstrate that the most effective strategy for transferring both to\nadditional languages and to additional domains is to fine-tune large\npre-trained models on small quantities of high-quality translation data.\n", "rewritten_text": "\\boxed{\nRecent breakthroughs in language model pre-training rely on massive datasets to develop multilingual models. However, languages with limited resources are often overlooked in these datasets, mainly because many widely spoken languages have a minimal online presence, excluding them from large-scale data crawls. As a result, users of these models are limited to the languages selected during pre-training. This study explores how to optimally utilize existing pre-trained models to create translation systems for 16 African languages with limited resources. We investigate two key questions: 1) How can pre-trained models be adapted for languages not included in the initial training? and 2) How can the resulting translation models effectively adapt to new domains? To address these questions, we create a new African news corpus covering 16 languages, including eight languages not found in any existing evaluation dataset. Our findings show that fine-tuning large pre-trained models with small amounts of high-quality translation data is the most effective strategy for transferring to both additional languages and domains.\n}"}, "2109.12028": {"original_text": "  Human knowledge is collectively encoded in the roughly 6500 languages spoken\naround the world, but it is not distributed equally across languages. Hence,\nfor information-seeking question answering (QA) systems to adequately serve\nspeakers of all languages, they need to operate cross-lingually. In this work\nwe investigate the capabilities of multilingually pre-trained language models\non cross-lingual QA. We find that explicitly aligning the representations\nacross languages with a post-hoc fine-tuning step generally leads to improved\nperformance. We additionally investigate the effect of data size as well as the\nlanguage choice in this fine-tuning step, also releasing a dataset for\nevaluating cross-lingual QA systems. Code and dataset are publicly available\nhere: https://github.com/ffaisal93/aligned_qa\n", "rewritten_text": "\\boxed{The world's collective knowledge is stored in approximately 6500 languages, but it's not evenly distributed among them. To effectively serve speakers of all languages, question answering systems must be able to operate across languages. This study explores the capabilities of multilingual language models in cross-lingual question answering. The results show that aligning language representations through post-hoc fine-tuning generally improves performance. The impact of data size and language choice in fine-tuning is also examined, and a dataset for evaluating cross-lingual question answering systems is released. The code and dataset are available at https://github.com/ffaisal93/aligned_qa.}"}, "2010.00363": {"original_text": "  Long Short-Term Memory recurrent neural network (LSTM) is widely used and\nknown to capture informative long-term syntactic dependencies. However, how\nsuch information are reflected in its internal vectors for natural text has not\nyet been sufficiently investigated. We analyze them by learning a language\nmodel where syntactic structures are implicitly given. We empirically show that\nthe context update vectors, i.e. outputs of internal gates, are approximately\nquantized to binary or ternary values to help the language model to count the\ndepth of nesting accurately, as Suzgun et al. (2019) recently show for\nsynthetic Dyck languages. For some dimensions in the context vector, we show\nthat their activations are highly correlated with the depth of phrase\nstructures, such as VP and NP. Moreover, with an $L_1$ regularization, we also\nfound that it can accurately predict whether a word is inside a phrase\nstructure or not from a small number of components of the context vector. Even\nfor the case of learning from raw text, context vectors are shown to still\ncorrelate well with the phrase structures. Finally, we show that natural\nclusters of the functional words and the part of speeches that trigger phrases\nare represented in a small but principal subspace of the context-update vector\nof LSTM.\n", "rewritten_text": "\\boxed{The internal workings of Long Short-Term Memory (LSTM) recurrent neural networks, widely used for capturing long-term syntactic dependencies, have not been fully explored. This study analyzes LSTM's internal vectors by training a language model with implicit syntactic structures. The results show that LSTM's internal gates output binary or ternary values to accurately count nesting depths, and certain dimensions of the context vector are highly correlated with phrase structures such as VP and NP. Additionally, with regularization, the context vector can predict whether a word is part of a phrase structure. Even with raw text input, the context vectors still correlate with phrase structures. Furthermore, functional words and part-of-speech triggers are represented in a small, principal subspace of the LSTM's context-update vector.}"}, "2005.02877": {"original_text": "  Task-oriented dialog systems rely on dialog state tracking (DST) to monitor\nthe user's goal during the course of an interaction. Multi-domain and\nopen-vocabulary settings complicate the task considerably and demand scalable\nsolutions. In this paper we present a new approach to DST which makes use of\nvarious copy mechanisms to fill slots with values. Our model has no need to\nmaintain a list of candidate values. Instead, all values are extracted from the\ndialog context on-the-fly. A slot is filled by one of three copy mechanisms:\n(1) Span prediction may extract values directly from the user input; (2) a\nvalue may be copied from a system inform memory that keeps track of the\nsystem's inform operations; (3) a value may be copied over from a different\nslot that is already contained in the dialog state to resolve coreferences\nwithin and across domains. Our approach combines the advantages of span-based\nslot filling methods with memory methods to avoid the use of value picklists\naltogether. We argue that our strategy simplifies the DST task while at the\nsame time achieving state of the art performance on various popular evaluation\nsets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55%.\n", "rewritten_text": "\\boxed{This paper proposes a novel approach to dialog state tracking (DST) that utilizes copy mechanisms to fill slots with values extracted from the dialog context in real-time, eliminating the need for candidate value lists. The approach combines span-based slot filling and memory-based methods to achieve state-of-the-art performance on popular evaluation sets, including Multiwoz 2.1, with a joint goal accuracy of over 55%.}"}, "2408.06899": {"original_text": "  We present a novel method for measuring the rate of periodic phenomena (e.g.,\nrotation, flicker, and vibration), by an event camera, a device asynchronously\nreporting brightness changes at independently operating pixels with high\ntemporal resolution. The approach assumes that for a periodic phenomenon, a\nhighly similar set of events is generated within a spatio-temporal window at a\ntime difference corresponding to its period. The sets of similar events are\ndetected by a correlation in the spatio-temporal event stream space. The\nproposed method, EEPPR, is evaluated on a dataset of 12 sequences of periodic\nphenomena, i.e. flashing light and vibration, and periodic motion, e.g.,\nrotation, ranging from 3.2 Hz to 2 kHz (equivalent to 192 - 120 000 RPM). EEPPR\nsignificantly outperforms published methods on this dataset, achieving a mean\nrelative error of 0.1%, setting new state-of-the-art. The dataset and codes are\npublicly available on GitHub.\n", "rewritten_text": "\\boxed{We introduce a novel approach, called EEPPR, for measuring the rate of periodic events, such as rotation, flicker, and vibration, using an event camera that captures brightness changes at high temporal resolution. Our method detects similar patterns of events within a specific time window, corresponding to the period of the phenomenon, by analyzing the spatio-temporal event stream. We evaluate EEPPR on a dataset of 12 sequences, including flashing lights, vibrations, and periodic motion, with frequencies ranging from 3.2 Hz to 2 kHz. Our results show that EEPPR outperforms existing methods, achieving a mean relative error of 0.1%, and sets a new state-of-the-art. The dataset and code are available on GitHub.}"}, "1703.08448": {"original_text": "  We investigate a principle way to progressively mine discriminative object\nregions using classification networks to address the weakly-supervised semantic\nsegmentation problems. Classification networks are only responsive to small and\nsparse discriminative regions from the object of interest, which deviates from\nthe requirement of the segmentation task that needs to localize dense, interior\nand integral regions for pixel-wise inference. To mitigate this gap, we propose\na new adversarial erasing approach for localizing and expanding object regions\nprogressively. Starting with a single small object region, our proposed\napproach drives the classification network to sequentially discover new and\ncomplement object regions by erasing the current mined regions in an\nadversarial manner. These localized regions eventually constitute a dense and\ncomplete object region for learning semantic segmentation. To further enhance\nthe quality of the discovered regions by adversarial erasing, an online\nprohibitive segmentation learning approach is developed to collaborate with\nadversarial erasing by providing auxiliary segmentation supervision modulated\nby the more reliable classification scores. Despite its apparent simplicity,\nthe proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union\n(mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new\nstate-of-the-arts.\n", "rewritten_text": "\\boxed{\nWe propose a novel approach to tackle weakly-supervised semantic segmentation by progressively mining discriminative object regions using classification networks. Our method, called adversarial erasing, starts with a small object region and iteratively discovers new regions by erasing the current ones, driving the classification network to focus on previously overlooked areas. This process eventually yields a dense and complete object region for semantic segmentation. To further improve the quality of the discovered regions, we introduce an online prohibitive segmentation learning approach that provides auxiliary supervision based on reliable classification scores. Our approach achieves state-of-the-art results, with mean Intersection-over-Union (mIoU) scores of 55.0% and 55.7% on the PASCAL VOC 2012 validation and test sets, respectively.\n}"}, "1708.00284": {"original_text": "  Future frame prediction in videos is a promising avenue for unsupervised\nvideo representation learning. Video frames are naturally generated by the\ninherent pixel flows from preceding frames based on the appearance and motion\ndynamics in the video. However, existing methods focus on directly\nhallucinating pixel values, resulting in blurry predictions. In this paper, we\ndevelop a dual motion Generative Adversarial Net (GAN) architecture, which\nlearns to explicitly enforce future-frame predictions to be consistent with the\npixel-wise flows in the video through a dual-learning mechanism. The primal\nfuture-frame prediction and dual future-flow prediction form a closed loop,\ngenerating informative feedback signals to each other for better video\nprediction. To make both synthesized future frames and flows indistinguishable\nfrom reality, a dual adversarial training method is proposed to ensure that the\nfuture-flow prediction is able to help infer realistic future-frames, while the\nfuture-frame prediction in turn leads to realistic optical flows. Our dual\nmotion GAN also handles natural motion uncertainty in different pixel locations\nwith a new probabilistic motion encoder, which is based on variational\nautoencoders. Extensive experiments demonstrate that the proposed dual motion\nGAN significantly outperforms state-of-the-art approaches on synthesizing new\nvideo frames and predicting future flows. Our model generalizes well across\ndiverse visual scenes and shows superiority in unsupervised video\nrepresentation learning.\n", "rewritten_text": "\\boxed{\nPredicting future frames in videos is a promising approach to learning video representations without supervision. Videos naturally exhibit pixel flows between frames based on appearance and motion dynamics. However, current methods focus on directly generating pixel values, resulting in blurry predictions. This paper proposes a dual motion Generative Adversarial Network (GAN) architecture that learns to predict future frames and flows consistently through a dual-learning mechanism. The predicted future frames and flows form a closed loop, providing informative feedback to each other for improved video prediction. A dual adversarial training method is used to ensure that the predicted flows and frames are realistic. The proposed model also handles motion uncertainty using a probabilistic motion encoder based on variational autoencoders. Experimental results show that the dual motion GAN outperforms state-of-the-art approaches in synthesizing new video frames and predicting future flows, generalizing well across diverse visual scenes and demonstrating superiority in unsupervised video representation learning.\n}"}, "1405.6103": {"original_text": "  The Swiss avalanche bulletin is produced twice a day in four languages. Due\nto the lack of time available for manual translation, a fully automated\ntranslation system is employed, based on a catalogue of predefined phrases and\npredetermined rules of how these phrases can be combined to produce sentences.\nThe system is able to automatically translate such sentences from German into\nthe target languages French, Italian and English without subsequent\nproofreading or correction. Our catalogue of phrases is limited to a small\nsublanguage. The reduction of daily translation costs is expected to offset the\ninitial development costs within a few years. After being operational for two\nwinter seasons, we assess here the quality of the produced texts based on an\nevaluation where participants rate real danger descriptions from both origins,\nthe catalogue of phrases versus the manually written and translated texts. With\na mean recognition rate of 55%, users can hardly distinguish between the two\ntypes of texts, and give similar ratings with respect to their language\nquality. Overall, the output from the catalogue system can be considered\nvirtually equivalent to a text written by avalanche forecasters and then\nmanually translated by professional translators. Furthermore, forecasters\ndeclared that all relevant situations were captured by the system with\nsufficient accuracy and within the limited time available.\n", "rewritten_text": "\\boxed{The Swiss avalanche bulletin uses a fully automated translation system to produce reports in four languages, twice a day. This system relies on a catalogue of predefined phrases and rules to combine them into sentences, allowing for automatic translation from German into French, Italian, and English without human proofreading or correction. Despite being limited to a specific sublanguage, the system has reduced daily translation costs, which are expected to offset development costs within a few years. After two winter seasons, an evaluation showed that users can barely distinguish between automated and manually written texts, with a mean recognition rate of 55%. The automated system's output is considered equivalent to manually translated texts in terms of language quality, and forecasters confirmed that it accurately captures all relevant situations within the time available.}"}, "2408.15063": {"original_text": "  Although most existing multi-modal salient object detection (SOD) methods\ndemonstrate effectiveness through training models from scratch, the limited\nmulti-modal data hinders these methods from reaching optimality. In this paper,\nwe propose a novel framework to explore and exploit the powerful feature\nrepresentation and zero-shot generalization ability of the pre-trained Segment\nAnything Model (SAM) for multi-modal SOD. Despite serving as a recent vision\nfundamental model, driving the class-agnostic SAM to comprehend and detect\nsalient objects accurately is non-trivial, especially in challenging scenes. To\nthis end, we develop \\underline{SAM} with se\\underline{m}antic\nf\\underline{e}ature fu\\underline{s}ion guidanc\\underline{e} (Sammese), which\nincorporates multi-modal saliency-specific knowledge into SAM to adapt SAM to\nmulti-modal SOD tasks. However, it is difficult for SAM trained on single-modal\ndata to directly mine the complementary benefits of multi-modal inputs and\ncomprehensively utilize them to achieve accurate saliency prediction. To\naddress these issues, we first design a multi-modal complementary fusion module\nto extract robust multi-modal semantic features by integrating information from\nvisible and thermal or depth image pairs. Then, we feed the extracted\nmulti-modal semantic features into both the SAM image encoder and mask decoder\nfor fine-tuning and prompting, respectively. Specifically, in the image\nencoder, a multi-modal adapter is proposed to adapt the single-modal SAM to\nmulti-modal information. In the mask decoder, a semantic-geometric prompt\ngeneration strategy is proposed to produce corresponding embeddings with\nvarious saliency cues. Extensive experiments on both RGB-D and RGB-T SOD\nbenchmarks show the effectiveness of the proposed framework. The code will be\navailable at \\url{https://github.com/Angknpng/Sammese}.\n", "rewritten_text": "\\boxed{\nMost existing multi-modal salient object detection (SOD) methods train models from scratch, but their performance is limited by the scarcity of multi-modal data. To address this, we propose a novel framework that leverages the powerful feature representation and zero-shot generalization ability of the pre-trained Segment Anything Model (SAM) for multi-modal SOD. However, adapting SAM to accurately detect salient objects in challenging scenes is non-trivial. To overcome this, we develop SAM with semantic feature fusion guidance (Sammese), which incorporates multi-modal saliency-specific knowledge into SAM. We design a multi-modal complementary fusion module to extract robust multi-modal semantic features from visible and thermal or depth image pairs. These features are then fed into the SAM image encoder and mask decoder for fine-tuning and prompting, respectively. Our approach includes a multi-modal adapter to adapt SAM to multi-modal information and a semantic-geometric prompt generation strategy to produce embeddings with various saliency cues. Extensive experiments on RGB-D and RGB-T SOD benchmarks demonstrate the effectiveness of our proposed framework, which will be available at https://github.com/Angknpng/Sammese.\n}"}, "2210.09563": {"original_text": "  With the continuous development of deep learning in the field of image\ngeneration models, a large number of vivid forged faces have been generated and\nspread on the Internet. These high-authenticity artifacts could grow into a\nthreat to society security. Existing face forgery detection methods directly\nutilize the obtained public shared or centralized data for training but ignore\nthe personal privacy and security issues when personal data couldn't be\ncentralizedly shared in real-world scenarios. Additionally, different\ndistributions caused by diverse artifact types would further bring adverse\ninfluences on the forgery detection task. To solve the mentioned problems, the\npaper proposes a novel generalized residual Federated learning for face Forgery\ndetection (FedForgery). The designed variational autoencoder aims to learn\nrobust discriminative residual feature maps to detect forgery faces (with\ndiverse or even unknown artifact types). Furthermore, the general federated\nlearning strategy is introduced to construct distributed detection model\ntrained collaboratively with multiple local decentralized devices, which could\nfurther boost the representation generalization. Experiments conducted on\npublicly available face forgery detection datasets prove the superior\nperformance of the proposed FedForgery. The designed novel generalized face\nforgery detection protocols and source code would be publicly available.\n", "rewritten_text": "\\boxed{\nThe rapid advancement of deep learning in image generation models has led to the proliferation of highly realistic fake faces online, posing a significant threat to societal security. Current face forgery detection methods rely on publicly shared or centralized data, neglecting personal privacy and security concerns. Moreover, diverse artifact types can hinder detection accuracy. To address these issues, this paper introduces FedForgery, a novel federated learning approach for face forgery detection. The proposed method utilizes a variational autoencoder to learn robust features for detecting fake faces, even with unknown artifact types. Additionally, a decentralized federated learning strategy is employed to train a detection model collaboratively across multiple devices, enhancing representation generalization. Experimental results on public datasets demonstrate the superior performance of FedForgery. The proposed protocols and source code will be made publicly available.\n}"}, "2304.05995": {"original_text": "  In recent years, the success of large-scale vision-language models (VLMs)\nsuch as CLIP has led to their increased usage in various computer vision tasks.\nThese models enable zero-shot inference through carefully crafted instructional\ntext prompts without task-specific supervision. However, the potential of VLMs\nfor generalization tasks in remote sensing (RS) has not been fully realized. To\naddress this research gap, we propose a novel image-conditioned prompt learning\nstrategy called the Visual Attention Parameterized Prompts Learning Network\n(APPLeNet). APPLeNet emphasizes the importance of multi-scale feature learning\nin RS scene classification and disentangles visual style and content primitives\nfor domain generalization tasks. To achieve this, APPLeNet combines visual\ncontent features obtained from different layers of the vision encoder and style\nproperties obtained from feature statistics of domain-specific batches. An\nattention-driven injection module is further introduced to generate visual\ntokens from this information. We also introduce an anti-correlation regularizer\nto ensure discrimination among the token embeddings, as this visual information\nis combined with the textual tokens. To validate APPLeNet, we curated four\navailable RS benchmarks and introduced experimental protocols and datasets for\nthree domain generalization tasks. Our results consistently outperform the\nrelevant literature and code is available at\nhttps://github.com/mainaksingha01/APPLeNet\n", "rewritten_text": "\\boxed{\nRecently, large-scale vision-language models (VLMs) like CLIP have gained popularity in various computer vision tasks, enabling zero-shot inference through carefully crafted text prompts without task-specific supervision. However, their potential for generalization tasks in remote sensing (RS) remains underutilized. To bridge this gap, we introduce APPLeNet, a novel image-conditioned prompt learning strategy that emphasizes multi-scale feature learning and disentangles visual style and content primitives for domain generalization tasks in RS scene classification. APPLeNet combines visual content features from different layers of the vision encoder and style properties from domain-specific batches, generating visual tokens through an attention-driven injection module. An anti-correlation regularizer ensures discrimination among token embeddings when combining visual and textual tokens. We validate APPLeNet on four curated RS benchmarks and three domain generalization tasks, outperforming existing literature. The code is available at https://github.com/mainaksingha01/APPLeNet.\n}"}, "1804.03287": {"original_text": "  Despite the noticeable progress in perceptual tasks like detection, instance\nsegmentation and human parsing, computers still perform unsatisfactorily on\nvisually understanding humans in crowded scenes, such as group behavior\nanalysis, person re-identification and autonomous driving, etc. To this end,\nmodels need to comprehensively perceive the semantic information and the\ndifferences between instances in a multi-human image, which is recently defined\nas the multi-human parsing task. In this paper, we present a new large-scale\ndatabase \"Multi-Human Parsing (MHP)\" for algorithm development and evaluation,\nand advances the state-of-the-art in understanding humans in crowded scenes.\nMHP contains 25,403 elaborately annotated images with 58 fine-grained semantic\ncategory labels, involving 2-26 persons per image and captured in real-world\nscenes from various viewpoints, poses, occlusion, interactions and background.\nWe further propose a novel deep Nested Adversarial Network (NAN) model for\nmulti-human parsing. NAN consists of three Generative Adversarial Network\n(GAN)-like sub-nets, respectively performing semantic saliency prediction,\ninstance-agnostic parsing and instance-aware clustering. These sub-nets form a\nnested structure and are carefully designed to learn jointly in an end-to-end\nway. NAN consistently outperforms existing state-of-the-art solutions on our\nMHP and several other datasets, and serves as a strong baseline to drive the\nfuture research for multi-human parsing.\n", "rewritten_text": "\\boxed{\nDespite significant advancements in computer vision tasks such as object detection and segmentation, machines still struggle to understand humans in crowded scenes, including group behavior analysis, person re-identification, and autonomous driving. To address this, a new task called multi-human parsing has been introduced, which requires models to comprehensively perceive semantic information and differences between individuals in images with multiple humans. This paper presents a large-scale database called Multi-Human Parsing (MHP) for developing and evaluating algorithms, which contains 25,403 annotated images with 58 fine-grained labels, featuring 2-26 people per image in various real-world scenarios. A novel deep learning model called Nested Adversarial Network (NAN) is also proposed, consisting of three sub-networks that learn jointly to perform semantic saliency prediction, instance-agnostic parsing, and instance-aware clustering. The NAN model outperforms existing state-of-the-art solutions on the MHP dataset and others, serving as a strong baseline for future research in multi-human parsing.\n}"}, "2206.10779": {"original_text": "  We propose a large-scale dataset of real-world rainy and clean image pairs\nand a method to remove degradations, induced by rain streaks and rain\naccumulation, from the image. As there exists no real-world dataset for\nderaining, current state-of-the-art methods rely on synthetic data and thus are\nlimited by the sim2real domain gap; moreover, rigorous evaluation remains a\nchallenge due to the absence of a real paired dataset. We fill this gap by\ncollecting a real paired deraining dataset through meticulous control of\nnon-rain variations. Our dataset enables paired training and quantitative\nevaluation for diverse real-world rain phenomena (e.g. rain streaks and rain\naccumulation). To learn a representation robust to rain phenomena, we propose a\ndeep neural network that reconstructs the underlying scene by minimizing a\nrain-robust loss between rainy and clean images. Extensive experiments\ndemonstrate that our model outperforms the state-of-the-art deraining methods\non real rainy images under various conditions. Project website:\nhttps://visual.ee.ucla.edu/gt_rain.htm/.\n", "rewritten_text": "\\boxed{We introduce a large-scale dataset of real-world image pairs, featuring rainy and clean images, along with a method to remove rain-induced degradations from images. Currently, deraining methods rely on synthetic data, which is limited by the sim2real domain gap, and evaluation is challenging due to the lack of a real paired dataset. Our dataset fills this gap by providing paired training and quantitative evaluation for diverse real-world rain phenomena. We also propose a deep neural network that learns a robust representation of the underlying scene by minimizing a rain-robust loss between rainy and clean images. Our model outperforms state-of-the-art deraining methods on real rainy images under various conditions, as demonstrated by extensive experiments.}"}, "1812.08115": {"original_text": "  We introduce a model-based deep learning architecture termed MoDL-MUSSELS for\nthe correction of phase errors in multishot diffusion-weighted echo-planar MRI\nimages. The proposed algorithm is a generalization of existing MUSSELS\nalgorithm with similar performance but with significantly reduced computational\ncomplexity. In this work, we show that an iterative re-weighted least-squares\nimplementation of MUSSELS alternates between a multichannel filter bank and the\nenforcement of data consistency. The multichannel filter bank projects the data\nto the signal subspace thus exploiting the phase relations between shots. Due\nto the high computational complexity of self-learned filter bank, we propose to\nreplace it with a convolutional neural network (CNN) whose parameters are\nlearned from exemplary data. The proposed CNN is a hybrid model involving a\nmultichannel CNN in the k-space and another CNN in the image space. The k-space\nCNN exploits the phase relations between the shot images, while the image\ndomain network is used to project the data to an image manifold. The\nexperiments show that the proposed scheme can yield reconstructions that are\ncomparable to state of the art methods while offering several orders of\nmagnitude reduction in run-time.\n", "rewritten_text": "\\boxed{We propose a deep learning architecture, MoDL-MUSSELS, to correct phase errors in multishot diffusion-weighted echo-planar MRI images. Our approach builds upon the existing MUSSELS algorithm, offering similar performance but with significantly reduced computational complexity. By replacing the self-learned filter bank with a convolutional neural network (CNN), we achieve a hybrid model that exploits phase relations between shots in k-space and projects data to an image manifold in image space. Experimental results demonstrate that our method produces reconstructions comparable to state-of-the-art methods, but with a substantial reduction in runtime.}"}, "1912.10644": {"original_text": "  In spite of the recent progresses on classifying 3D point cloud with deep\nCNNs, large geometric transformations like rotation and translation remain\nchallenging problem and harm the final classification performance. To address\nthis challenge, we propose Geometry Sharing Network (GS-Net) which effectively\nlearns point descriptors with holistic context to enhance the robustness to\ngeometric transformations. Compared with previous 3D point CNNs which perform\nconvolution on nearby points, GS-Net can aggregate point features in a more\nglobal way. Specially, GS-Net consists of Geometry Similarity Connection (GSC)\nmodules which exploit Eigen-Graph to group distant points with similar and\nrelevant geometric information, and aggregate features from nearest neighbors\nin both Euclidean space and Eigenvalue space. This design allows GS-Net to\nefficiently capture both local and holistic geometric features such as\nsymmetry, curvature, convexity and connectivity. Theoretically, we show the\nnearest neighbors of each point in Eigenvalue space are invariant to rotation\nand translation. We conduct extensive experiments on public datasets,\nModelNet40, ShapeNet Part. Experiments demonstrate that GS-Net achieves the\nstate-of-the-art performances on major datasets, 93.3% on ModelNet40, and are\nmore robust to geometric transformations.\n", "rewritten_text": "\\boxed{\nDespite recent advancements in classifying 3D point clouds using deep CNNs, large geometric transformations such as rotation and translation remain a significant challenge, negatively impacting classification performance. To tackle this issue, we introduce the Geometry Sharing Network (GS-Net), which learns point descriptors with holistic context to enhance robustness to geometric transformations. Unlike previous 3D point CNNs that perform convolution on nearby points, GS-Net aggregates point features in a more global manner. Specifically, GS-Net consists of Geometry Similarity Connection (GSC) modules that utilize Eigen-Graph to group distant points with similar geometric information and aggregate features from nearest neighbors in both Euclidean and Eigenvalue spaces. This design enables GS-Net to efficiently capture both local and holistic geometric features, including symmetry, curvature, convexity, and connectivity. Theoretically, we prove that the nearest neighbors of each point in Eigenvalue space are invariant to rotation and translation. Our extensive experiments on public datasets, including ModelNet40 and ShapeNet Part, demonstrate that GS-Net achieves state-of-the-art performances, with 93.3% accuracy on ModelNet40, and exhibits improved robustness to geometric transformations.\n}"}, "2111.01515": {"original_text": "  The enormous amount of data being generated on the web and social media has\nincreased the demand for detecting online hate speech. Detecting hate speech\nwill reduce their negative impact and influence on others. A lot of effort in\nthe Natural Language Processing (NLP) domain aimed to detect hate speech in\ngeneral or detect specific hate speech such as religion, race, gender, or\nsexual orientation. Hate communities tend to use abbreviations, intentional\nspelling mistakes, and coded words in their communication to evade detection,\nadding more challenges to hate speech detection tasks. Thus, word\nrepresentation will play an increasingly pivotal role in detecting hate speech.\nThis paper investigates the feasibility of leveraging domain-specific word\nembedding in Bidirectional LSTM based deep model to automatically\ndetect/classify hate speech. Furthermore, we investigate the use of the\ntransfer learning language model (BERT) on hate speech problem as a binary\nclassification task. The experiments showed that domainspecific word embedding\nwith the Bidirectional LSTM based deep model achieved a 93% f1-score while BERT\nachieved up to 96% f1-score on a combined balanced dataset from available hate\nspeech datasets.\n", "rewritten_text": "\\boxed{The surge in online data has created a pressing need for detecting hate speech on the web and social media, as it can have a harmful impact on others. Researchers in the field of Natural Language Processing (NLP) have been working to develop methods for detecting hate speech in general, as well as specific types of hate speech targeting religion, race, gender, or sexual orientation. However, hate groups often use coded language, abbreviations, and intentional spelling mistakes to avoid detection, making it a challenging task. This study explores the effectiveness of using domain-specific word embeddings in a deep learning model based on Bidirectional LSTM to automatically detect and classify hate speech. Additionally, it examines the use of the transfer learning language model BERT for hate speech detection as a binary classification task. The results show that the domain-specific word embedding approach achieved an f1-score of 93%, while BERT achieved an f1-score of up to 96% on a combined balanced dataset from available hate speech datasets.}"}, "2409.17432": {"original_text": "  Reducing the atmospheric haze and enhancing image clarity is crucial for\ncomputer vision applications. The lack of real-life hazy ground truth images\nnecessitates synthetic datasets, which often lack diverse haze types, impeding\neffective haze type classification and dehazing algorithm selection. This\nresearch introduces the HazeSpace2M dataset, a collection of over 2 million\nimages designed to enhance dehazing through haze type classification.\nHazeSpace2M includes diverse scenes with 10 haze intensity levels, featuring\nFog, Cloud, and Environmental Haze (EH). Using the dataset, we introduce a\ntechnique of haze type classification followed by specialized dehazers to clear\nhazy images. Unlike conventional methods, our approach classifies haze types\nbefore applying type-specific dehazing, improving clarity in real-life hazy\nimages. Benchmarking with state-of-the-art (SOTA) models, ResNet50 and AlexNet\nachieve 92.75\\% and 92.50\\% accuracy, respectively, against existing synthetic\ndatasets. However, these models achieve only 80% and 70% accuracy,\nrespectively, against our Real Hazy Testset (RHT), highlighting the challenging\nnature of our HazeSpace2M dataset. Additional experiments show that haze type\nclassification followed by specialized dehazing improves results by 2.41% in\nPSNR, 17.14% in SSIM, and 10.2\\% in MSE over general dehazers. Moreover, when\ntesting with SOTA dehazing models, we found that applying our proposed\nframework significantly improves their performance. These results underscore\nthe significance of HazeSpace2M and our proposed framework in addressing\natmospheric haze in multimedia processing. Complete code and dataset is\navailable on \\href{https://github.com/tanvirnwu/HazeSpace2M}\n{\\textcolor{blue}{\\textbf{GitHub}}}.\n", "rewritten_text": "\\boxed{\nImproving image clarity by reducing atmospheric haze is essential for computer vision applications. However, the lack of real-life hazy images necessitates the use of synthetic datasets, which often lack diverse haze types, making it challenging to classify haze types and select effective dehazing algorithms. To address this, we introduce the HazeSpace2M dataset, a collection of over 2 million images designed to enhance dehazing through haze type classification. This dataset includes diverse scenes with 10 haze intensity levels, featuring Fog, Cloud, and Environmental Haze. We propose a technique that classifies haze types before applying type-specific dehazing, resulting in improved clarity in real-life hazy images. Our approach outperforms conventional methods, achieving higher accuracy and better results in terms of PSNR, SSIM, and MSE. The significance of HazeSpace2M and our proposed framework is highlighted by the improved performance of state-of-the-art dehazing models when tested with our dataset. The complete code and dataset are available on GitHub.\n}"}, "1808.045": {"original_text": "  Medical images with specific pathologies are scarce, but a large amount of\ndata is usually required for a deep convolutional neural network (DCNN) to\nachieve good accuracy. We consider the problem of segmenting the left\nventricular (LV) myocardium on late gadolinium enhancement (LGE) cardiovascular\nmagnetic resonance (CMR) scans of which only some of the scans have scar\ntissue. We propose ScarGAN to simulate scar tissue on healthy myocardium using\nchained generative adversarial networks (GAN). Our novel approach factorizes\nthe simulation process into 3 steps: 1) a mask generator to simulate the shape\nof the scar tissue; 2) a domain-specific heuristic to produce the initial\nsimulated scar tissue from the simulated shape; 3) a refining generator to add\ndetails to the simulated scar tissue. Unlike other approaches that generate\nsamples from scratch, we simulate scar tissue on normal scans resulting in\nhighly realistic samples. We show that experienced radiologists are unable to\ndistinguish between real and simulated scar tissue. Training a U-Net with\nadditional scans with scar tissue simulated by ScarGAN increases the percentage\nof scar pixels correctly included in LV myocardium prediction from 75.9% to\n80.5%.\n", "rewritten_text": "\\boxed{The scarcity of medical images with specific pathologies hinders the training of deep convolutional neural networks (DCNNs), which require large amounts of data to achieve high accuracy. To address this, we propose ScarGAN, a novel approach that simulates scar tissue on healthy myocardium in late gadolinium enhancement cardiovascular magnetic resonance (CMR) scans using chained generative adversarial networks (GAN). Our method involves three steps: generating a scar tissue mask, creating initial simulated scar tissue using a domain-specific heuristic, and refining the simulation with a generator. This approach produces highly realistic samples that are indistinguishable from real scar tissue, even to experienced radiologists. By incorporating these simulated scans into the training of a U-Net, we improve the accuracy of left ventricular myocardium prediction, increasing the correct inclusion of scar pixels from 75.9% to 80.5%.}"}, "2409.00045": {"original_text": "  Colonoscopy is the primary method for examination, detection, and removal of\npolyps. Regular screening helps detect and prevent colorectal cancer at an\nearly curable stage. However, challenges such as variation among the\nendoscopists' skills, bowel quality preparation, and complex nature of the\nlarge intestine which cause large number of polyp miss-rate. These missed\npolyps can develop into cancer later on, which underscores the importance of\nimproving the detection methods. A computer-aided diagnosis system can support\nphysicians by assisting in detecting overlooked polyps. However, one of the\nimportant challenges for developing novel deep learning models for automatic\npolyp detection and segmentation is the lack of publicly available,\nmulti-center large and diverse datasets. To address this gap, we introduce\nPolypDB, a large scale publicly available dataset that contains 3934 still\npolyp images and their corresponding ground truth from real colonoscopy videos\nto design efficient polyp detection and segmentation architectures. The dataset\nhas been developed and verified by a team of 10 gastroenterologists. PolypDB\ncomprises of images from five modalities: Blue Light Imaging (BLI), Flexible\nImaging Color Enhancement (FICE), Linked Color Imaging (LCI), Narrow Band\nImaging (NBI), and White Light Imaging (WLI) and three medical centers from\nNorway, Sweden and Vietnam. Thus, we split the dataset based on modality and\nmedical center for modality-wise and center-wise analysis. We provide a\nbenchmark on each modality using eight popular segmentation methods and six\nstandard benchmark polyp detection methods. Furthermore, we also provide\nbenchmark on center-wise under federated learning settings. Our dataset is\npublic and can be downloaded at \\url{https://osf.io/pr7ms/}.\n", "rewritten_text": "\\boxed{\nColonoscopy is the primary method for examining, detecting, and removing polyps, which helps prevent colorectal cancer. However, challenges such as varying endoscopist skills, bowel preparation, and the complex nature of the large intestine lead to a high polyp miss-rate. To improve detection, a computer-aided diagnosis system can assist physicians in detecting overlooked polyps. A major obstacle in developing deep learning models for automatic polyp detection and segmentation is the lack of publicly available, large, and diverse datasets. To address this, we introduce PolypDB, a large-scale publicly available dataset containing 3934 still polyp images with corresponding ground truth from real colonoscopy videos. The dataset, developed and verified by 10 gastroenterologists, includes images from five modalities and three medical centers. We provide benchmarks for each modality using popular segmentation and detection methods, as well as center-wise analysis under federated learning settings. The dataset is publicly available for download at https://osf.io/pr7ms/.\n}"}, "2201.09724": {"original_text": "  The task of hot-refresh model upgrades of image retrieval systems plays an\nessential role in the industry but has never been investigated in academia\nbefore. Conventional cold-refresh model upgrades can only deploy new models\nafter the gallery is overall backfilled, taking weeks or even months for\nmassive data. In contrast, hot-refresh model upgrades deploy the new model\nimmediately and then gradually improve the retrieval accuracy by backfilling\nthe gallery on-the-fly. Compatible training has made it possible, however, the\nproblem of model regression with negative flips poses a great challenge to the\nstable improvement of user experience. We argue that it is mainly due to the\nfact that new-to-old positive query-gallery pairs may show less similarity than\nnew-to-new negative pairs. To solve the problem, we introduce a\nRegression-Alleviating Compatible Training (RACT) method to properly constrain\nthe feature compatibility while reducing negative flips. The core is to\nencourage the new-to-old positive pairs to be more similar than both the\nnew-to-old negative pairs and the new-to-new negative pairs. An efficient\nuncertainty-based backfilling strategy is further introduced to fasten accuracy\nimprovements. Extensive experiments on large-scale retrieval benchmarks (e.g.,\nGoogle Landmark) demonstrate that our RACT effectively alleviates the model\nregression for one more step towards seamless model upgrades. The code will be\navailable at https://github.com/binjiezhang/RACT_ICLR2022.\n", "rewritten_text": "\\boxed{\nUpgrading image retrieval systems with new models is crucial in the industry, but has not been explored in academia. Traditional methods, known as cold-refresh model upgrades, require a complete update of the gallery, which can take weeks or months for large datasets. In contrast, hot-refresh model upgrades deploy new models immediately and improve retrieval accuracy through on-the-fly backfilling of the gallery. However, this approach is hindered by model regression due to negative flips, which occurs when new-to-old positive query-gallery pairs show less similarity than new-to-new negative pairs. To address this, we propose Regression-Alleviating Compatible Training (RACT), which constrains feature compatibility and reduces negative flips by encouraging new-to-old positive pairs to be more similar than new-to-old and new-to-new negative pairs. We also introduce an efficient uncertainty-based backfilling strategy to accelerate accuracy improvements. Our experiments on large-scale retrieval benchmarks, such as Google Landmark, demonstrate the effectiveness of RACT in alleviating model regression, bringing us closer to seamless model upgrades. The code is available at https://github.com/binjiezhang/RACT_ICLR2022.\n}"}, "2012.14345": {"original_text": "  Deep Learning (DL) based methods for object detection achieve remarkable\nperformance at the cost of computationally expensive training and extensive\ndata labeling. Robots embodiment can be exploited to mitigate this burden by\nacquiring automatically annotated training data via a natural interaction with\na human showing the object of interest, handheld. However, learning solely from\nthis data may introduce biases (the so-called domain shift), and prevents\nadaptation to novel tasks. While Weakly-supervised Learning (WSL) offers a\nwell-established set of techniques to cope with these problems in\ngeneral-purpose Computer Vision, its adoption in challenging robotic domains is\nstill at a preliminary stage. In this work, we target the scenario of a robot\ntrained in a teacher-learner setting to detect handheld objects. The aim is to\nimprove detection performance in different settings by letting the robot\nexplore the environment with a limited human labeling budget. We compare\nseveral techniques for WSL in detection pipelines to reduce model re-training\ncosts without compromising accuracy, proposing solutions which target the\nconsidered robotic scenario. We show that the robot can improve adaptation to\nnovel domains, either by interacting with a human teacher (Active Learning) or\nwith an autonomous supervision (Semi-supervised Learning). We integrate our\nstrategies into an on-line detection method, achieving efficient model update\ncapabilities with few labels. We experimentally benchmark our method on\nchallenging robotic object detection tasks under domain shift.\n", "rewritten_text": "\\boxed{Deep learning-based object detection methods achieve high performance but require extensive training data and computational resources. To address this, robots can be used to collect annotated training data through human-robot interaction, but this may introduce biases and limit adaptability to new tasks. Weakly-supervised learning techniques can help mitigate these issues, but their application in robotics is still in its early stages. This work focuses on a robot trained to detect handheld objects in a teacher-learner setting, aiming to improve detection performance in different environments with limited human labeling. We compare various weakly-supervised learning techniques to reduce re-training costs without compromising accuracy, and propose solutions tailored to the robotic scenario. Our results show that the robot can adapt to new domains through active learning with human teachers or semi-supervised learning with autonomous supervision, enabling efficient model updates with minimal labels. We evaluate our method on challenging robotic object detection tasks with domain shift.}"}, "cs/0310014": {"original_text": "  Spoken Language can be used to provide insights into organisational\nprocesses, unfortunately transcription and coding stages are very time\nconsuming and expensive. The concept of partial transcription and coding is\nproposed in which spoken language is indexed prior to any subsequent\nprocessing. The functional linguistic theory of texture is used to describe the\neffects of partial transcription on observational records. The standard used to\nencode transcript context and metadata is called CHAT, but a previous XML\nschema developed to implement it contains design assumptions that make it\ndifficult to support partial transcription for example. This paper describes a\nmore effective XML schema that overcomes many of these problems and is intended\nfor use in applications that support the rapid development of spoken language\ndeliverables.\n", "rewritten_text": "\\boxed{Analyzing spoken language can reveal valuable insights into organizational processes, but the transcription and coding stages are often too time-consuming and costly. To address this, a new approach called partial transcription and coding is proposed, where spoken language is indexed before further processing. This method is informed by the functional linguistic theory of texture, which examines the effects of partial transcription on observational records. Currently, the CHAT standard is used to encode transcript context and metadata, but its existing XML schema has design limitations that hinder partial transcription. This paper presents a revised XML schema that resolves these issues, facilitating the rapid development of spoken language deliverables in various applications.}"}, "2007.12099": {"original_text": "  Object detection is one of the most important areas in computer vision, which\nplays a key role in various practical scenarios. Due to limitation of hardware,\nit is often necessary to sacrifice accuracy to ensure the infer speed of the\ndetector in practice. Therefore, the balance between effectiveness and\nefficiency of object detector must be considered. The goal of this paper is to\nimplement an object detector with relatively balanced effectiveness and\nefficiency that can be directly applied in actual application scenarios, rather\nthan propose a novel detection model. Considering that YOLOv3 has been widely\nused in practice, we develop a new object detector based on YOLOv3. We mainly\ntry to combine various existing tricks that almost not increase the number of\nmodel parameters and FLOPs, to achieve the goal of improving the accuracy of\ndetector as much as possible while ensuring that the speed is almost unchanged.\nSince all experiments in this paper are conducted based on PaddlePaddle, we\ncall it PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a better\nbalance between effectiveness (45.2% mAP) and efficiency (72.9 FPS), surpassing\nthe existing state-of-the-art detectors such as EfficientDet and YOLOv4.Source\ncode is at https://github.com/PaddlePaddle/PaddleDetection.\n", "rewritten_text": "\\boxed{\nObject detection is a crucial area in computer vision, playing a vital role in various practical scenarios. However, due to hardware limitations, a trade-off between accuracy and speed is often necessary. This paper aims to develop an object detector that balances effectiveness and efficiency, making it suitable for real-world applications. Building upon the widely-used YOLOv3, we create a new detector, PP-YOLO, by combining existing techniques that minimize additional model parameters and FLOPs. Our approach achieves a better balance between accuracy (45.2% mAP) and speed (72.9 FPS), outperforming state-of-the-art detectors like EfficientDet and YOLOv4. The source code is available at https://github.com/PaddlePaddle/PaddleDetection.\n}"}, "2406.15823": {"original_text": "  Understanding the abilities of LLMs to reason about natural language plans,\nsuch as instructional text and recipes, is critical to reliably using them in\ndecision-making systems. A fundamental aspect of plans is the temporal order in\nwhich their steps needs to be executed, which reflects the underlying causal\ndependencies between them. We introduce CaT-Bench, a benchmark of Step Order\nPrediction questions, which test whether a step must necessarily occur before\nor after another in cooking recipe plans. We use this to evaluate how well\nfrontier LLMs understand causal and temporal dependencies. We find that SOTA\nLLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased\ntowards predicting dependence more often, perhaps relying on temporal order of\nsteps as a heuristic. While prompting for explanations and using few-shot\nexamples improve performance, the best F1 result is only 0.73. Further, human\nevaluation of explanations along with answer correctness show that, on average,\nhumans do not agree with model reasoning. Surprisingly, we also find that\nexplaining after answering leads to better performance than normal\nchain-of-thought prompting, and LLM answers are not consistent across questions\nabout the same step pairs. Overall, results show that LLMs' ability to detect\ndependence between steps has significant room for improvement.\n", "rewritten_text": "\\boxed{Evaluating the ability of Large Language Models (LLMs) to understand natural language plans, such as recipes, is crucial for their reliable use in decision-making systems. A key aspect of plans is the temporal order of their steps, which reflects the underlying causal dependencies. We introduce CaT-Bench, a benchmark to test LLMs' ability to predict the correct order of steps in cooking recipes. Our results show that even the best LLMs perform poorly (with an F1 score of only 0.59), and tend to overpredict dependencies. While providing explanations and few-shot examples improve performance, the best result is still only 0.73. Moreover, human evaluation reveals that LLMs' reasoning is often inconsistent and disagrees with human judgment. Surprisingly, explaining after answering leads to better performance, and LLMs' answers are inconsistent across questions about the same step pairs. Overall, our findings highlight the need for significant improvement in LLMs' ability to detect dependencies between steps.}"}, "1907.06007": {"original_text": "  With the development of deep neural networks, the demand for a significant\namount of annotated training data becomes the performance bottlenecks in many\nfields of research and applications. Image synthesis can generate annotated\nimages automatically and freely, which gains increasing attention recently. In\nthis paper, we propose to synthesize scene text images from the 3D virtual\nworlds, where the precise descriptions of scenes, editable\nillumination/visibility, and realistic physics are provided. Different from the\nprevious methods which paste the rendered text on static 2D images, our method\ncan render the 3D virtual scene and text instances as an entirety. In this way,\nreal-world variations, including complex perspective transformations, various\nilluminations, and occlusions, can be realized in our synthesized scene text\nimages. Moreover, the same text instances with various viewpoints can be\nproduced by randomly moving and rotating the virtual camera, which acts as\nhuman eyes. The experiments on the standard scene text detection benchmarks\nusing the generated synthetic data demonstrate the effectiveness and\nsuperiority of the proposed method. The code and synthetic data is available\nat: https://github.com/MhLiao/SynthText3D\n", "rewritten_text": "\\boxed{The need for large amounts of annotated training data has become a major obstacle in many research fields and applications, but image synthesis can provide a solution by generating annotated images automatically. This paper proposes a method for synthesizing scene text images from 3D virtual worlds, which allows for precise control over scene descriptions, lighting, and physics. Unlike previous methods that superimpose text onto 2D images, our approach renders 3D scenes and text as a whole, capturing real-world variations such as perspective transformations, lighting, and occlusions. By moving and rotating the virtual camera, we can generate multiple views of the same text instance, mimicking human vision. Experiments on standard scene text detection benchmarks demonstrate the effectiveness of our method, and the code and synthetic data are available online.}"}, "2006.13017": {"original_text": "  Recently, 3D convolutional networks (3D ConvNets) yield good performance in\naction recognition. However, optical flow stream is still needed to ensure\nbetter performance, the cost of which is very high. In this paper, we propose a\nfast but effective way to extract motion features from videos utilizing\nresidual frames as the input data in 3D ConvNets. By replacing traditional\nstacked RGB frames with residual ones, 35.6% and 26.6% points improvements over\ntop-1 accuracy can be obtained on the UCF101 and HMDB51 datasets when ResNet-18\nmodels are trained from scratch. And we achieved the state-of-the-art results\nin this training mode. Analysis shows that better motion features can be\nextracted using residual frames compared to RGB counterpart. By combining with\na simple appearance path, our proposal can be even better than some methods\nusing optical flow streams.\n", "rewritten_text": "\\boxed{Recently, 3D convolutional networks have shown promising results in action recognition, but they often rely on optical flow streams, which are computationally expensive. This paper proposes a more efficient approach, using residual frames as input data in 3D ConvNets, which achieves significant improvements in top-1 accuracy on the UCF101 and HMDB51 datasets. Specifically, using residual frames instead of traditional RGB frames yields 35.6% and 26.6% points improvements, respectively, when training ResNet-18 models from scratch. This approach even outperforms some methods that use optical flow streams when combined with a simple appearance path.}"}, "2403.15615": {"original_text": "  Conversation is the subject of increasing interest in the social, cognitive,\nand computational sciences. And yet, as conversational datasets continue to\nincrease in size and complexity, researchers lack scalable methods to segment\nspeech-to-text transcripts into conversational turns--the basic building blocks\nof social interaction. We introduce \"NaturalTurn,\" a turn segmentation\nalgorithm designed to accurately capture the dynamics of naturalistic exchange.\nNaturalTurn operates by distinguishing speakers' primary conversational turns\nfrom listeners' secondary utterances, such as backchannels, brief\ninterjections, and other forms of parallel speech that characterize\nconversation. Using data from a large conversation corpus, we show how\nNaturalTurn-derived transcripts demonstrate favorable statistical and\ninferential characteristics compared to transcripts derived from existing\nmethods. The NaturalTurn algorithm represents an improvement in\nmachine-generated transcript processing methods, or \"turn models\" that will\nenable researchers to associate turn-taking dynamics with the broader outcomes\nthat result from social interaction, a central goal of conversation science.\n", "rewritten_text": "\\boxed{There is a growing interest in conversation across various fields, but researchers face a challenge in breaking down large and complex speech-to-text transcripts into individual conversational turns. To address this, we developed \"NaturalTurn\", an algorithm that accurately identifies the primary turns of speakers and distinguishes them from secondary responses, such as backchannels and interjections, that are typical in conversations. Our evaluation using a large conversation dataset shows that NaturalTurn-generated transcripts have better statistical and inferential properties compared to those produced by existing methods. This algorithm improves machine-generated transcript processing, enabling researchers to link turn-taking dynamics to broader social outcomes, a key objective in conversation science.}"}, "1807.00502": {"original_text": "  The use of deep learning for medical imaging has seen tremendous growth in\nthe research community. One reason for the slow uptake of these systems in the\nclinical setting is that they are complex, opaque and tend to fail silently.\nOutside of the medical imaging domain, the machine learning community has\nrecently proposed several techniques for quantifying model uncertainty (i.e.~a\nmodel knowing when it has failed). This is important in practical settings, as\nwe can refer such cases to manual inspection or correction by humans. In this\npaper, we aim to bring these recent results on estimating uncertainty to bear\non two important outputs in deep learning-based segmentation. The first is\nproducing spatial uncertainty maps, from which a clinician can observe where\nand why a system thinks it is failing. The second is quantifying an image-level\nprediction of failure, which is useful for isolating specific cases and\nremoving them from automated pipelines. We also show that reasoning about\nspatial uncertainty, the first output, is a useful intermediate representation\nfor generating segmentation quality predictions, the second output. We propose\na two-stage architecture for producing these measures of uncertainty, which can\naccommodate any deep learning-based medical segmentation pipeline.\n", "rewritten_text": "\\boxed{\nThe application of deep learning in medical imaging has gained significant attention in research, but its adoption in clinical settings has been slow due to the complexity and opacity of these systems, which can fail without warning. Recently, the machine learning community has developed techniques to quantify model uncertainty, enabling models to recognize when they have failed. This is crucial in practical applications, as it allows for manual inspection or correction by humans. This paper aims to apply these uncertainty estimation techniques to deep learning-based segmentation, producing two key outputs: spatial uncertainty maps that highlight areas of potential failure and image-level predictions of failure to identify problematic cases. We propose a two-stage architecture that can be integrated with any deep learning-based medical segmentation pipeline to generate these uncertainty measures, which can also be used to predict segmentation quality.\n}"}, "2112.15439": {"original_text": "  This paper aims to conduct a comprehensive study on facial-sketch synthesis\n(FSS). However, due to the high costs of obtaining hand-drawn sketch datasets,\nthere lacks a complete benchmark for assessing the development of FSS\nalgorithms over the last decade. We first introduce a high-quality dataset for\nFSS, named FS2K, which consists of 2,104 image-sketch pairs spanning three\ntypes of sketch styles, image backgrounds, lighting conditions, skin colors,\nand facial attributes. FS2K differs from previous FSS datasets in difficulty,\ndiversity, and scalability and should thus facilitate the progress of FSS\nresearch. Second, we present the largest-scale FSS investigation by reviewing\n89 classical methods, including 25 handcrafted feature-based facial-sketch\nsynthesis approaches, 29 general translation methods, and 35 image-to-sketch\napproaches. Besides, we elaborate comprehensive experiments on the existing 19\ncutting-edge models. Third, we present a simple baseline for FSS, named FSGAN.\nWith only two straightforward components, i.e., facial-aware masking and\nstyle-vector expansion, FSGAN surpasses the performance of all previous\nstate-of-the-art models on the proposed FS2K dataset by a large margin.\nFinally, we conclude with lessons learned over the past years and point out\nseveral unsolved challenges. Our code is available at\nhttps://github.com/DengPingFan/FSGAN.\n", "rewritten_text": "\\boxed{\nThis paper presents a comprehensive study on facial-sketch synthesis (FSS), a field that has lacked a complete benchmark due to the high cost of obtaining hand-drawn sketch datasets. To address this, we introduce FS2K, a high-quality dataset of 2,104 image-sketch pairs that covers various sketch styles, backgrounds, lighting conditions, skin colors, and facial attributes. We then conduct a large-scale review of 89 FSS methods, including classical approaches and cutting-edge models. Furthermore, we propose a simple yet effective baseline model called FSGAN, which outperforms previous state-of-the-art models on the FS2K dataset. Finally, we reflect on the lessons learned and identify remaining challenges in FSS research. Our code is available at https://github.com/DengPingFan/FSGAN.\n}"}, "1804.09803": {"original_text": "  The inference structures and computational complexity of existing deep neural\nnetworks, once trained, are fixed and remain the same for all test images.\nHowever, in practice, it is highly desirable to establish a progressive\nstructure for deep neural networks which is able to adapt its inference process\nand complexity for images with different visual recognition complexity. In this\nwork, we develop a multi-stage progressive structure with integrated confidence\nanalysis and decision policy learning for deep neural networks. This new\nframework consists of a set of network units to be activated in a sequential\nmanner with progressively increased complexity and visual recognition power.\nOur extensive experimental results on the CIFAR-10 and ImageNet datasets\ndemonstrate that the proposed progressive deep neural network is able to obtain\nmore than 10 fold complexity scalability while achieving the state-of-the-art\nperformance using a single network model satisfying different\ncomplexity-accuracy requirements.\n", "rewritten_text": "\\boxed{Existing deep neural networks have fixed inference structures and computational complexity, which remain the same for all test images. However, it is desirable to have a progressive structure that adapts to images with varying visual recognition complexity. To address this, we propose a multi-stage framework that integrates confidence analysis and decision policy learning, allowing the network to activate units sequentially with increasing complexity and recognition power. Our experiments on CIFAR-10 and ImageNet datasets show that this progressive network achieves state-of-the-art performance with over 10-fold complexity scalability, satisfying different complexity-accuracy requirements with a single model.}"}, "2312.14988": {"original_text": "  Autoregressive and diffusion models drive the recent breakthroughs on\ntext-to-image generation. Despite their huge success of generating\nhigh-realistic images, a common shortcoming of these models is their high\ninference latency - autoregressive models run more than a thousand times\nsuccessively to produce image tokens and diffusion models convert Gaussian\nnoise into images with many hundreds of denoising steps. In this work, we\nexplore non-autoregressive text-to-image models that efficiently generate\nhundreds of image tokens in parallel. We develop many model variations with\ndifferent learning and inference strategies, initialized text encoders, etc.\nCompared with autoregressive baselines that needs to run one thousand times,\nour model only runs 16 times to generate images of competitive quality with an\norder of magnitude lower inference latency. Our non-autoregressive model with\n346M parameters generates an image of 256$\\times$256 with about one second on\none V100 GPU.\n", "rewritten_text": "\\boxed{Recent advancements in text-to-image generation have been driven by autoregressive and diffusion models, but these models suffer from high inference latency. To address this, we propose non-autoregressive text-to-image models that can efficiently generate hundreds of image tokens in parallel. Our approach reduces inference latency by an order of magnitude, requiring only 16 iterations to produce high-quality images, compared to 1,000 iterations needed by autoregressive baselines. Our model, with 346M parameters, can generate a 256x256 image in approximately one second on a single V100 GPU.}"}, "2106.16038": {"original_text": "  Recent pretraining models in Chinese neglect two important aspects specific\nto the Chinese language: glyph and pinyin, which carry significant syntax and\nsemantic information for language understanding. In this work, we propose\nChineseBERT, which incorporates both the {\\it glyph} and {\\it pinyin}\ninformation of Chinese characters into language model pretraining. The glyph\nembedding is obtained based on different fonts of a Chinese character, being\nable to capture character semantics from the visual features, and the pinyin\nembedding characterizes the pronunciation of Chinese characters, which handles\nthe highly prevalent heteronym phenomenon in Chinese (the same character has\ndifferent pronunciations with different meanings). Pretrained on large-scale\nunlabeled Chinese corpus, the proposed ChineseBERT model yields significant\nperformance boost over baseline models with fewer training steps. The porpsoed\nmodel achieves new SOTA performances on a wide range of Chinese NLP tasks,\nincluding machine reading comprehension, natural language inference, text\nclassification, sentence pair matching, and competitive performances in named\nentity recognition. Code and pretrained models are publicly available at\nhttps://github.com/ShannonAI/ChineseBert.\n", "rewritten_text": "\\boxed{Recent Chinese language models have overlooked two crucial aspects of the language: glyphs and pinyin, which contain valuable syntax and semantic information. To address this, we introduce ChineseBERT, a model that incorporates both glyph and pinyin information into language model pretraining. By leveraging font variations to capture character semantics and pronunciation to handle heteronyms, our model achieves significant performance improvements over baseline models with fewer training steps. ChineseBERT sets new state-of-the-art performances on various Chinese NLP tasks, including machine reading comprehension, natural language inference, and text classification, and is competitive in named entity recognition. The code and pretrained models are publicly available at https://github.com/ShannonAI/ChineseBert.}"}, "2103.05342": {"original_text": "  In this paper, we propose a novel data augmentation strategy named\nCut-Thumbnail, that aims to improve the shape bias of the network. We reduce an\nimage to a certain size and replace the random region of the original image\nwith the reduced image. The generated image not only retains most of the\noriginal image information but also has global information in the reduced\nimage. We call the reduced image as thumbnail. Furthermore, we find that the\nidea of thumbnail can be perfectly integrated with Mixed Sample Data\nAugmentation, so we put one image's thumbnail on another image while the ground\ntruth labels are also mixed, making great achievements on various computer\nvision tasks. Extensive experiments show that Cut-Thumbnail works better than\nstate-of-the-art augmentation strategies across classification, fine-grained\nimage classification, and object detection. On ImageNet classification,\nResNet-50 architecture with our method achieves 79.21\\% accuracy, which is more\nthan 2.8\\% improvement on the baseline.\n", "rewritten_text": "\\boxed{This paper introduces a novel data augmentation technique called Cut-Thumbnail, which enhances a network's shape bias by reducing an image to a smaller size (thumbnail) and replacing a random region of the original image with the thumbnail. This approach preserves most of the original image information while incorporating global information from the thumbnail. When combined with Mixed Sample Data Augmentation, the thumbnail is placed on another image with mixed ground truth labels, achieving significant improvements in various computer vision tasks. Experimental results demonstrate that Cut-Thumbnail outperforms state-of-the-art augmentation strategies in classification, fine-grained image classification, and object detection, with a notable 2.8% accuracy improvement on ImageNet classification using ResNet-50 architecture.}"}, "2307.15257": {"original_text": "  The complexity of learning problems, such as Generative Adversarial Network\n(GAN) and its variants, multi-task and meta-learning, hyper-parameter learning,\nand a variety of real-world vision applications, demands a deeper understanding\nof their underlying coupling mechanisms. Existing approaches often address\nthese problems in isolation, lacking a unified perspective that can reveal\ncommonalities and enable effective solutions. Therefore, in this work, we\nproposed a new framework, named Learning with Constraint Learning (LwCL), that\ncan holistically examine challenges and provide a unified methodology to tackle\nall the above-mentioned complex learning and vision problems. Specifically,\nLwCL is designed as a general hierarchical optimization model that captures the\nessence of these diverse learning and vision problems. Furthermore, we develop\na gradient-response based fast solution strategy to overcome optimization\nchallenges of the LwCL framework. Our proposed framework efficiently addresses\na wide range of applications in learning and vision, encompassing three\ncategories and nine different problem types. Extensive experiments on synthetic\ntasks and real-world applications verify the effectiveness of our approach. The\nLwCL framework offers a comprehensive solution for tackling complex machine\nlearning and computer vision problems, bridging the gap between theory and\npractice.\n", "rewritten_text": "\\boxed{\nThe intricacy of various machine learning and computer vision problems, including Generative Adversarial Networks (GANs), multi-task and meta-learning, hyper-parameter learning, and real-world vision applications, necessitates a deeper comprehension of their underlying interconnected mechanisms. Current approaches often tackle these problems individually, lacking a unified perspective that can uncover commonalities and facilitate effective solutions. To address this, we propose a novel framework, Learning with Constraint Learning (LwCL), which provides a holistic approach to examining challenges and offers a unified methodology to tackle these complex problems. LwCL is designed as a general hierarchical optimization model that captures the essence of diverse learning and vision problems. We also develop a gradient-response based fast solution strategy to overcome optimization challenges within the LwCL framework. Our proposed framework efficiently addresses a wide range of applications in learning and vision, encompassing three categories and nine different problem types. Extensive experiments on synthetic tasks and real-world applications demonstrate the effectiveness of our approach. The LwCL framework offers a comprehensive solution for tackling complex machine learning and computer vision problems, bridging the gap between theory and practice.\n}"}, "2309.11119": {"original_text": "  A recent sensor fusion in a Bird's Eye View (BEV) space has shown its utility\nin various tasks such as 3D detection, map segmentation, etc. However, the\napproach struggles with inaccurate camera BEV estimation, and a perception of\ndistant areas due to the sparsity of LiDAR points. In this paper, we propose a\nbroad BEV fusion (BroadBEV) that addresses the problems with a spatial\nsynchronization approach of cross-modality. Our strategy aims to enhance camera\nBEV estimation for a broad-sighted perception while simultaneously improving\nthe completion of LiDAR's sparsity in the entire BEV space. Toward that end, we\ndevise Point-scattering that scatters LiDAR BEV distribution to camera depth\ndistribution. The method boosts the learning of depth estimation of the camera\nbranch and induces accurate location of dense camera features in BEV space. For\nan effective BEV fusion between the spatially synchronized features, we suggest\nColFusion that applies self-attention weights of LiDAR and camera BEV features\nto each other. Our extensive experiments demonstrate that BroadBEV provides a\nbroad-sighted BEV perception with remarkable performance gains.\n", "rewritten_text": "\\boxed{\nRecent advances in sensor fusion in Bird's Eye View (BEV) space have shown promise in tasks like 3D detection and map segmentation. However, existing approaches struggle with inaccurate camera BEV estimation and limited perception of distant areas due to sparse LiDAR points. To address these issues, we propose BroadBEV, a novel fusion approach that leverages spatial synchronization across modalities. Our method enhances camera BEV estimation for a broader field of view while improving LiDAR point density in the entire BEV space. We introduce Point-scattering, which adapts LiDAR BEV distribution to camera depth distribution, boosting camera depth estimation and accurate feature localization. Additionally, we propose ColFusion, which applies self-attention weights to LiDAR and camera BEV features, enabling effective fusion. Our experiments demonstrate that BroadBEV achieves a more comprehensive BEV perception with significant performance improvements.\n}"}, "2403.17920": {"original_text": "  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.\n", "rewritten_text": "\\boxed{Recent text-to-4D generation methods rely on pre-trained text-to-video models to synthesize dynamic 3D scenes, but they are limited by their motion representations, which cannot generate motion beyond the bounding box used for volume rendering. To address this, we propose TC4D, a novel approach that separates motion into global and local components. Our method uses a spline-parameterized trajectory to represent global motion and learns local deformations that conform to this trajectory using supervision from a text-to-video model. This enables the generation of scenes animated along arbitrary trajectories, compositional scene generation, and significant improvements to realism and motion quantity, as demonstrated through qualitative evaluation and a user study.}"}, "2407.11144": {"original_text": "  Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25.\n", "rewritten_text": "\\boxed{Machine learning research on sign languages is hindered by a lack of data, particularly for languages used by Deaf/Hard of Hearing communities worldwide. To address this, we introduce YouTube-SL-25, a large, open-domain corpus of sign language videos with captions from YouTube, featuring over 3000 hours of content across 25 languages. This dataset is three times larger than YouTube-ASL and the largest parallel sign language dataset to date, providing a valuable resource for sign-to-text tasks. We establish baselines for these tasks using a unified multilingual model and demonstrate that multilingual transfer learning benefits both high- and low-resource sign languages within the dataset.}"}, "1702.01776": {"original_text": "  In aspect-based sentiment analysis, most existing methods either focus on\naspect/opinion terms extraction or aspect terms categorization. However, each\ntask by itself only provides partial information to end users. To generate more\ndetailed and structured opinion analysis, we propose a finer-grained problem,\nwhich we call category-specific aspect and opinion terms extraction. This\nproblem involves the identification of aspect and opinion terms within each\nsentence, as well as the categorization of the identified terms. To this end,\nwe propose an end-to-end multi-task attention model, where each task\ncorresponds to aspect/opinion terms extraction for a specific category. Our\nmodel benefits from exploring the commonalities and relationships among\ndifferent tasks to address the data sparsity issue. We demonstrate its\nstate-of-the-art performance on three benchmark datasets.\n", "rewritten_text": "\\boxed{Existing methods in aspect-based sentiment analysis typically focus on either extracting aspect and opinion terms or categorizing aspect terms, but these approaches only provide partial information. To provide more detailed and structured opinion analysis, we introduce a more nuanced problem: category-specific aspect and opinion terms extraction, which involves identifying and categorizing aspect and opinion terms within each sentence. We propose an end-to-end multi-task attention model that tackles this problem by jointly extracting aspect and opinion terms for multiple categories, leveraging the relationships between tasks to overcome data sparsity issues. Our model achieves state-of-the-art performance on three benchmark datasets.}"}, "2301.00399": {"original_text": "  In the present paper, semantic parsing challenges are briefly introduced and\nQDMR formalism in semantic parsing is implemented using sequence to sequence\nmodel with attention but uses only part of speech(POS) as a representation of\nwords of a sentence to make the training as simple and as fast as possible and\nalso avoiding curse of dimensionality as well as overfitting. It is shown how\nsemantic operator prediction could be augmented with other models like the\nCopyNet model or the recursive neural net model.\n", "rewritten_text": "\\boxed{This paper provides an overview of the challenges in semantic parsing and proposes a simplified approach to implementing the QDMR formalism using a sequence-to-sequence model with attention. By only utilizing part-of-speech (POS) tags to represent words, the training process is accelerated and the risks of dimensionality curse and overfitting are mitigated. Additionally, the paper explores the potential of integrating semantic operator prediction with other models, such as CopyNet or recursive neural networks.}"}, "2411.07075": {"original_text": "  To predict upcoming text, language models must in some cases retrieve\nin-context information verbatim. In this report, we investigated how the\nability of language models to retrieve arbitrary in-context nouns developed\nduring training (across time) and as language models trained on the same\ndataset increase in size (across scale). We then asked whether learning of\nin-context retrieval correlates with learning of more challenging zero-shot\nbenchmarks. Furthermore, inspired by semantic effects in human short-term\nmemory, we evaluated the retrieval with respect to a major semantic component\nof target nouns, namely whether they denote a concrete or abstract entity, as\nrated by humans. We show that verbatim in-context retrieval developed in a\nsudden transition early in the training process, after about 1% of the training\ntokens. This was observed across model sizes (from 14M and up to 12B\nparameters), and the transition occurred slightly later for the two smallest\nmodels. We further found that the development of verbatim in-context retrieval\nis positively correlated with the learning of zero-shot benchmarks. Around the\ntransition point, all models showed the advantage of retrieving concrete nouns\nas opposed to abstract nouns. In all but two smallest models, the advantage\ndissipated away toward the end of training.\n", "rewritten_text": "\\boxed{This report explores how language models develop the ability to recall specific nouns from context during training and as they increase in size. The study finds that this ability emerges suddenly early in the training process, around 1% of training tokens, and is correlated with improved performance on more challenging zero-shot benchmarks. Additionally, the research reveals that language models tend to favor recalling concrete nouns over abstract nouns, especially during the early stages of training, although this advantage diminishes for most models as training progresses.}"}, "1911.11351": {"original_text": "  Recently, Human Attribute Recognition (HAR) has become a hot topic due to its\nscientific challenges and application potentials, where localizing attributes\nis a crucial stage but not well handled. In this paper, we propose a novel deep\nlearning approach to HAR, namely Distraction-aware HAR (Da-HAR). It enhances\ndeep CNN feature learning by improving attribute localization through a\ncoarse-to-fine attention mechanism. At the coarse step, a self-mask block is\nbuilt to roughly discriminate and reduce distractions, while at the fine step,\na masked attention branch is applied to further eliminate irrelevant regions.\nThanks to this mechanism, feature learning is more accurate, especially when\nheavy occlusions and complex backgrounds exist. Extensive experiments are\nconducted on the WIDER-Attribute and RAP databases, and state-of-the-art\nresults are achieved, demonstrating the effectiveness of the proposed approach.\n", "rewritten_text": "\\boxed{Human Attribute Recognition (HAR) has gained significant attention due to its scientific challenges and potential applications. However, localizing attributes remains a crucial yet underaddressed stage. To tackle this, we introduce Distraction-aware HAR (Da-HAR), a novel deep learning approach that enhances attribute localization through a coarse-to-fine attention mechanism. This mechanism refines feature learning by initially reducing distractions using a self-mask block, followed by a masked attention branch that eliminates irrelevant regions. Our approach achieves state-of-the-art results on the WIDER-Attribute and RAP databases, demonstrating its effectiveness in accurately learning features, particularly in the presence of heavy occlusions and complex backgrounds.}"}, "2310.08487": {"original_text": "  While multi-modal models have successfully integrated information from image,\nvideo, and audio modalities, integrating graph modality into large language\nmodels (LLMs) remains unexplored. This discrepancy largely stems from the\ninherent divergence between structured graph data and unstructured text data.\nIncorporating graph knowledge provides a reliable source of information,\nenabling potential solutions to address issues in text generation, e.g.,\nhallucination, and lack of domain knowledge. To evaluate the integration of\ngraph knowledge into language models, a dedicated dataset is needed. However,\nthere is currently no benchmark dataset specifically designed for multimodal\ngraph-language models. To address this gap, we propose GraphextQA, a question\nanswering dataset with paired subgraphs, retrieved from Wikidata, to facilitate\nthe evaluation and future development of graph-language models. Additionally,\nwe introduce a baseline model called CrossGNN, which conditions answer\ngeneration on the paired graphs by cross-attending question-aware graph\nfeatures at decoding. The proposed dataset is designed to evaluate\ngraph-language models' ability to understand graphs and make use of it for\nanswer generation. We perform experiments with language-only models and the\nproposed graph-language model to validate the usefulness of the paired graphs\nand to demonstrate the difficulty of the task.\n", "rewritten_text": "\\boxed{\nWhile large language models have successfully combined information from images, videos, and audio, they have yet to incorporate graph data. This is due to the fundamental difference between structured graph data and unstructured text data. Integrating graph knowledge can provide a reliable source of information, helping to address issues in text generation, such as hallucination and lack of domain knowledge. However, a dedicated dataset is needed to evaluate the integration of graph knowledge into language models, but currently, no such benchmark dataset exists. To fill this gap, we introduce GraphextQA, a question-answering dataset with paired subgraphs from Wikidata, designed to evaluate and develop graph-language models. We also propose a baseline model, CrossGNN, which generates answers by cross-attending question-aware graph features. The dataset is designed to test graph-language models' ability to understand graphs and use them for answer generation. Our experiments with language-only models and the proposed graph-language model demonstrate the usefulness of paired graphs and the difficulty of the task.\n}"}, "cs/0010012": {"original_text": "  We describe a new framework for distilling information from word lattices to\nimprove the accuracy of speech recognition and obtain a more perspicuous\nrepresentation of a set of alternative hypotheses. In the standard MAP decoding\napproach the recognizer outputs the string of words corresponding to the path\nwith the highest posterior probability given the acoustics and a language\nmodel. However, even given optimal models, the MAP decoder does not necessarily\nminimize the commonly used performance metric, word error rate (WER). We\ndescribe a method for explicitly minimizing WER by extracting word hypotheses\nwith the highest posterior probabilities from word lattices. We change the\nstandard problem formulation by replacing global search over a large set of\nsentence hypotheses with local search over a small set of word candidates. In\naddition to improving the accuracy of the recognizer, our method produces a new\nrepresentation of the set of candidate hypotheses that specifies the sequence\nof word-level confusions in a compact lattice format. We study the properties\nof confusion networks and examine their use for other tasks, such as lattice\ncompression, word spotting, confidence annotation, and reevaluation of\nrecognition hypotheses using higher-level knowledge sources.\n", "rewritten_text": "\\boxed{We propose a novel framework that refines speech recognition accuracy and provides a clearer representation of alternative hypotheses by extracting high-probability word hypotheses from word lattices. Unlike traditional MAP decoding, which outputs the most likely sentence, our approach directly minimizes word error rate (WER) by locally searching a small set of word candidates. This method not only improves recognition accuracy but also generates a compact lattice representation of word-level confusions, which can be applied to various tasks such as lattice compression, word spotting, confidence annotation, and reevaluation of recognition hypotheses using higher-level knowledge sources.}"}, "2111.03993": {"original_text": "  Skeleton data is of low dimension. However, there is a trend of using very\ndeep and complicated feedforward neural networks to model the skeleton sequence\nwithout considering the complexity in recent year. In this paper, a simple yet\neffective multi-scale semantics-guided neural network (MS-SGN) is proposed for\nskeleton-based action recognition. We explicitly introduce the high level\nsemantics of joints (joint type and frame index) into the network to enhance\nthe feature representation capability of joints. Moreover, a multi-scale\nstrategy is proposed to be robust to the temporal scale variations. In\naddition, we exploit the relationship of joints hierarchically through two\nmodules, i.e., a joint-level module for modeling the correlations of joints in\nthe same frame and a frame-level module for modeling the temporal dependencies\nof frames. With an order of magnitude smaller model size than most previous\nmethods, MSSGN achieves the state-of-the-art performance on the NTU60, NTU120,\nand SYSU datasets.\n", "rewritten_text": "\\boxed{Recent approaches to skeleton-based action recognition have involved using overly complex neural networks, despite the low dimensionality of skeleton data. This paper proposes a simpler yet more effective alternative, the multi-scale semantics-guided neural network (MS-SGN), which incorporates high-level semantics of joints and a multi-scale strategy to handle temporal variations. The MS-SGN model consists of two modules: a joint-level module to capture correlations between joints in the same frame, and a frame-level module to model temporal dependencies between frames. Notably, the MS-SGN achieves state-of-the-art performance on three datasets (NTU60, NTU120, and SYSU) with a significantly smaller model size than previous methods.}"}, "1808.06428": {"original_text": "  This paper presents an approach for automatic detection of Munro's\nMicroabscess in stratum corneum (SC) of human skin biopsy in order to realize a\nmachine assisted diagnosis of Psoriasis. The challenge of detecting neutrophils\nin presence of nucleated cells is solved using the recent advances of deep\nlearning algorithms. Separation of SC layer, extraction of patches from the\nlayer followed by classification of patches with respect to presence or absence\nof neutrophils form the basis of the overall approach which is effected through\nan integration of a U-Net based segmentation network and a capsule network for\nclassification. The novel design of the present capsule net leads to a drastic\nreduction in the number of parameters without any noticeable compromise in the\noverall performance. The research further addresses the challenge of dealing\nwith Mega-pixel images (in 10X) vis-a-vis Giga-pixel ones (in 40X). The\npromising result coming out of an experiment on a dataset consisting of 273\nreal-life images shows that a practical system is possible based on the present\nresearch. The implementation of our system is available at\nhttps://github.com/Anabik/CapsDeMM.\n", "rewritten_text": "\\boxed{This paper proposes a machine-assisted diagnosis system for Psoriasis by automatically detecting Munro's Microabscess in human skin biopsy images. The approach leverages recent deep learning advancements to overcome the challenge of distinguishing neutrophils from nucleated cells. It involves segmenting the stratum corneum layer, extracting patches, and classifying them using a U-Net-based segmentation network and a novel capsule network. The capsule network design achieves a significant reduction in parameters without compromising performance. The system is capable of handling high-resolution images (up to 40X) and demonstrates promising results on a dataset of 273 real-life images. The implementation is available at https://github.com/Anabik/CapsDeMM.}"}, "1302.1422": {"original_text": "  The variation of word meaning according to the context leads us to enrich the\ntype system of our syntactical and semantic analyser of French based on\ncategorial grammars and Montague semantics (or lambda-DRT). The main advantage\nof a deep semantic analyse is too represent meaning by logical formulae that\ncan be easily used e.g. for inferences. Determiners and quantifiers play a\nfundamental role in the construction of those formulae. But in our rich type\nsystem the usual semantic terms do not work. We propose a solution ins- pired\nby the tau and epsilon operators of Hilbert, kinds of generic elements and\nchoice functions. This approach unifies the treatment of the different determi-\nners and quantifiers as well as the dynamic binding of pronouns. Above all,\nthis fully computational view fits in well within the wide coverage parser\nGrail, both from a theoretical and a practical viewpoint.\n", "rewritten_text": "\\boxed{\nThe context-dependent variation in word meaning has led us to enhance the type system of our French syntactical and semantic analyzer, which is based on categorial grammars and Montague semantics. A key benefit of in-depth semantic analysis is the representation of meaning using logical formulas that can be easily utilized for inference purposes. Determiners and quantifiers play a crucial role in constructing these formulas. However, in our enriched type system, traditional semantic terms are insufficient. We propose a solution inspired by Hilbert's tau and epsilon operators, which involve generic elements and choice functions. This approach unifies the treatment of different determiners and quantifiers, as well as the dynamic binding of pronouns. Notably, this fully computational approach integrates seamlessly with the wide-coverage parser Grail, both theoretically and practically."}, "2012.07315": {"original_text": "  The categorical distribution is a natural representation of uncertainty in\nmulti-class segmentations. In the two-class case the categorical distribution\nreduces to the Bernoulli distribution, for which grayscale morphology provides\na range of useful operations. In the general case, applying morphological\noperations on uncertain multi-class segmentations is not straightforward as an\nimage of categorical distributions is not a complete lattice. Although\nmorphology on color images has received wide attention, this is not so for\ncolor-coded or categorical images and even less so for images of categorical\ndistributions. In this work, we establish a set of requirements for morphology\non categorical distributions by combining classic morphology with a\nprobabilistic view. We then define operators respecting these requirements,\nintroduce protected operations on categorical distributions and illustrate the\nutility of these operators on two example tasks: modeling annotator bias in\nbrain tumor segmentations and segmenting vesicle instances from the predictions\nof a multi-class U-Net.\n", "rewritten_text": "\\boxed{The categorical distribution is a natural way to represent uncertainty in multi-class image segmentations. While grayscale morphology provides useful operations for two-class cases, it's not directly applicable to multi-class cases. This is because images of categorical distributions don't form a complete lattice. To address this, we combine classic morphology with a probabilistic approach to establish requirements for morphology on categorical distributions. We then define operators that meet these requirements, introduce protected operations, and demonstrate their utility in two example tasks: modeling annotator bias in brain tumor segmentations and segmenting vesicle instances from multi-class U-Net predictions.}"}, "2410.24219": {"original_text": "  Despite advancements in Text-to-Video (T2V) generation, producing videos with\nrealistic motion remains challenging. Current models often yield static or\nminimally dynamic outputs, failing to capture complex motions described by\ntext. This issue stems from the internal biases in text encoding, which\noverlooks motions, and inadequate conditioning mechanisms in T2V generation\nmodels. To address this, we propose a novel framework called DEcomposed MOtion\n(DEMO), which enhances motion synthesis in T2V generation by decomposing both\ntext encoding and conditioning into content and motion components. Our method\nincludes a content encoder for static elements and a motion encoder for\ntemporal dynamics, alongside separate content and motion conditioning\nmechanisms. Crucially, we introduce text-motion and video-motion supervision to\nimprove the model's understanding and generation of motion. Evaluations on\nbenchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench\ndemonstrate DEMO's superior ability to produce videos with enhanced motion\ndynamics while maintaining high visual quality. Our approach significantly\nadvances T2V generation by integrating comprehensive motion understanding\ndirectly from textual descriptions. Project page:\nhttps://PR-Ryan.github.io/DEMO-project/\n", "rewritten_text": "\\boxed{\nGenerating realistic motion in Text-to-Video (T2V) synthesis remains a significant challenge, as current models often produce static or minimally dynamic videos that fail to capture complex motions described in the text. This limitation is due to biases in text encoding that overlook motion and inadequate conditioning mechanisms in T2V models. To overcome this, we introduce DEMO, a novel framework that enhances motion synthesis by decomposing text encoding and conditioning into content and motion components. Our approach includes separate encoders for static content and temporal motion, as well as distinct conditioning mechanisms. We also introduce text-motion and video-motion supervision to improve the model's understanding and generation of motion. Our evaluations on various benchmarks demonstrate DEMO's ability to produce high-quality videos with enhanced motion dynamics. Our approach significantly advances T2V generation by integrating comprehensive motion understanding directly from textual descriptions."}, "2406.09386": {"original_text": "  Controllable synthetic data generation can substantially lower the annotation\ncost of training data. Prior works use diffusion models to generate driving\nimages conditioned on the 3D object layout. However, those models are trained\non small-scale datasets like nuScenes, which lack appearance and layout\ndiversity. Moreover, overfitting often happens, where the trained models can\nonly generate images based on the layout data from the validation set of the\nsame dataset. In this work, we introduce a simulator-conditioned scene\ngeneration framework called SimGen that can learn to generate diverse driving\nscenes by mixing data from the simulator and the real world. It uses a novel\ncascade diffusion pipeline to address challenging sim-to-real gaps and\nmulti-condition conflicts. A driving video dataset DIVA is collected to enhance\nthe generative diversity of SimGen, which contains over 147.5 hours of\nreal-world driving videos from 73 locations worldwide and simulated driving\ndata from the MetaDrive simulator. SimGen achieves superior generation quality\nand diversity while preserving controllability based on the text prompt and the\nlayout pulled from a simulator. We further demonstrate the improvements brought\nby SimGen for synthetic data augmentation on the BEV detection and segmentation\ntask and showcase its capability in safety-critical data generation.\n", "rewritten_text": "\\boxed{Generating synthetic data in a controlled manner can significantly reduce the cost of annotating training data. While previous studies have used diffusion models to create driving images based on 3D object layouts, these models are limited by their reliance on small-scale datasets that lack diversity in terms of appearance and layout. Moreover, they often suffer from overfitting, only generating images based on the validation set of the same dataset. To address these limitations, we propose a novel framework called SimGen, which combines data from simulators and the real world to generate diverse driving scenes. Our approach uses a cascade diffusion pipeline to overcome the challenges of sim-to-real gaps and multi-condition conflicts. We also introduce a new driving video dataset, DIVA, which comprises over 147.5 hours of real-world driving videos from 73 locations worldwide and simulated driving data from the MetaDrive simulator. SimGen achieves superior generation quality and diversity while maintaining controllability based on text prompts and layouts from simulators. We demonstrate the effectiveness of SimGen in augmenting synthetic data for BEV detection and segmentation tasks, as well as its potential in generating safety-critical data.}"}, "2311.02313": {"original_text": "  Large-scale semantic mapping is crucial for outdoor autonomous agents to\nfulfill high-level tasks such as planning and navigation. This paper proposes a\nnovel method for large-scale 3D semantic reconstruction through implicit\nrepresentations from posed LiDAR measurements alone. We first leverage an\noctree-based and hierarchical structure to store implicit features, then these\nimplicit features are decoded to semantic information and signed distance value\nthrough shallow Multilayer Perceptrons (MLPs). We adopt off-the-shelf\nalgorithms to predict the semantic labels and instance IDs of point clouds. We\nthen jointly optimize the feature embeddings and MLPs parameters with a\nself-supervision paradigm for point cloud geometry and a pseudo-supervision\nparadigm for semantic and panoptic labels. Subsequently, categories and\ngeometric structures for novel points are regressed, and marching cubes are\nexploited to subdivide and visualize the scenes in the inferring stage. For\nscenarios with memory constraints, a map stitching strategy is also developed\nto merge sub-maps into a complete map. Experiments on two real-world datasets,\nSemanticKITTI and SemanticPOSS, demonstrate the superior segmentation\nefficiency and mapping effectiveness of our framework compared to current\nstate-of-the-art 3D LiDAR mapping methods.\n", "rewritten_text": "\\boxed{This paper presents a novel approach for large-scale 3D semantic mapping using posed LiDAR measurements. The method employs an octree-based structure to store implicit features, which are then decoded into semantic information and distance values using shallow neural networks. The approach leverages self-supervision and pseudo-supervision to optimize feature embeddings and network parameters, and can efficiently segment and map scenes. Additionally, a map stitching strategy is developed to merge sub-maps into a complete map, making it suitable for scenarios with memory constraints. Experimental results on two real-world datasets demonstrate the superior performance of the proposed framework compared to existing 3D LiDAR mapping methods.}"}, "2203.09333": {"original_text": "  Perceiving the similarity between images has been a long-standing and\nfundamental problem underlying various visual generation tasks. Predominant\napproaches measure the inter-image distance by computing pointwise absolute\ndeviations, which tends to estimate the median of instance distributions and\nleads to blurs and artifacts in the generated images. This paper presents\nMoNCE, a versatile metric that introduces image contrast to learn a calibrated\nmetric for the perception of multifaceted inter-image distances. Unlike vanilla\ncontrast which indiscriminately pushes negative samples from the anchor\nregardless of their similarity, we propose to re-weight the pushing force of\nnegative samples adaptively according to their similarity to the anchor, which\nfacilitates the contrastive learning from informative negative samples. Since\nmultiple patch-level contrastive objectives are involved in image distance\nmeasurement, we introduce optimal transport in MoNCE to modulate the pushing\nforce of negative samples collaboratively across multiple contrastive\nobjectives. Extensive experiments over multiple image translation tasks show\nthat the proposed MoNCE outperforms various prevailing metrics substantially.\nThe code is available at https://github.com/fnzhan/MoNCE.\n", "rewritten_text": "\\boxed{\nMeasuring the similarity between images is a fundamental problem in visual generation tasks. Current approaches calculate the distance between images by computing pointwise absolute deviations, which can lead to blurry and artifact-ridden generated images. This paper introduces MoNCE, a versatile metric that incorporates image contrast to learn a calibrated metric for perceiving multifaceted inter-image distances. Unlike traditional contrast, MoNCE adaptively re-weights the pushing force of negative samples based on their similarity to the anchor, facilitating contrastive learning from informative negative samples. Additionally, MoNCE uses optimal transport to modulate the pushing force of negative samples across multiple contrastive objectives. Experimental results on multiple image translation tasks demonstrate that MoNCE outperforms prevailing metrics significantly. The code is available at https://github.com/fnzhan/MoNCE.\n}"}, "2402.17292": {"original_text": "  Text-to-Avatar generation has recently made significant strides due to\nadvancements in diffusion models. However, most existing work remains\nconstrained by limited diversity, producing avatars with subtle differences in\nappearance for a given text prompt. We design DivAvatar, a novel framework that\ngenerates diverse avatars, empowering 3D creatives with a multitude of distinct\nand richly varied 3D avatars from a single text prompt. Different from most\nexisting work that exploits scene-specific 3D representations such as NeRF,\nDivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse\navatar generation from simply noise sampling in inference time. DivAvatar has\ntwo key designs that help achieve generation diversity and visual quality. The\nfirst is a noise sampling technique during training phase which is critical in\ngenerating diverse appearances. The second is a semantic-aware zoom mechanism\nand a novel depth loss, the former producing appearances of high textual\nfidelity by separate fine-tuning of specific body parts and the latter\nimproving geometry quality greatly by smoothing the generated mesh in the\nfeatures space. Extensive experiments show that DivAvatar is highly versatile\nin generating avatars of diverse appearances.\n", "rewritten_text": "\\boxed{\nRecent advancements in diffusion models have significantly improved Text-to-Avatar generation. However, existing methods often produce avatars with limited diversity, resulting in subtle appearance differences for a given text prompt. To address this, we propose DivAvatar, a novel framework that generates diverse and richly varied 3D avatars from a single text prompt. Unlike existing methods that rely on scene-specific 3D representations, DivAvatar fine-tunes a 3D generative model (EVA3D) to enable diverse avatar generation through noise sampling during inference. Our framework features two key designs: a noise sampling technique during training to generate diverse appearances, and a semantic-aware zoom mechanism with a novel depth loss to improve visual quality and geometry. Experimental results demonstrate DivAvatar's versatility in generating avatars with diverse appearances.\n}"}, "2208.13078": {"original_text": "  Owing to the lack of corpora for low-resource languages, current works on\ndialogue generation have mainly focused on English. In this paper, we present\nmDIA, the first large-scale multilingual benchmark for dialogue generation\nacross low- to high-resource languages. It covers real-life conversations in 46\nlanguages across 19 language families. We present baseline results obtained by\nfine-tuning the multilingual, non-dialogue-focused pre-trained model mT5 as\nwell as English-centric, dialogue-focused pre-trained chatbot DialoGPT. The\nresults show that mT5-based models perform better on sacreBLEU and BertScore\nbut worse on diversity. Even though promising results are found in few-shot and\nzero-shot scenarios, there is a large gap between the generation quality in\nEnglish and other languages. We hope that the release of mDIA could encourage\nmore works on multilingual dialogue generation to promote language diversity.\n", "rewritten_text": "\\boxed{Due to the scarcity of language corpora, most research on dialogue generation has centered on English. This paper introduces mDIA, a pioneering multilingual benchmark for dialogue generation that spans 46 languages across 19 language families, covering real-life conversations. We provide baseline results using the multilingual pre-trained model mT5 and the English-centric chatbot DialoGPT. Our findings show that mT5-based models outperform on sacreBLEU and BertScore but underperform on diversity. Although promising results are achieved in few-shot and zero-shot scenarios, a significant gap remains between English and other languages in terms of generation quality. We hope that mDIA's release will inspire further research on multilingual dialogue generation, promoting language diversity.}"}, "2310.08908": {"original_text": "  The large language model (LLM) has garnered significant attention due to its\nin-context learning mechanisms and emergent capabilities. The research\ncommunity has conducted several pilot studies to apply LLMs to machine\ntranslation tasks and evaluate their performance from diverse perspectives.\nHowever, previous research has primarily focused on the LLM itself and has not\nexplored human intervention in the inference process of LLM. The\ncharacteristics of LLM, such as in-context learning and prompt engineering,\nclosely mirror human cognitive abilities in language tasks, offering an\nintuitive solution for human-in-the-loop generation. In this study, we propose\na human-in-the-loop pipeline that guides LLMs to produce customized outputs\nwith revision instructions. The pipeline initiates by prompting the LLM to\nproduce a draft translation, followed by the utilization of automatic retrieval\nor human feedback as supervision signals to enhance the LLM's translation\nthrough in-context learning. The human-machine interactions generated in this\npipeline are also stored in an external database to expand the in-context\nretrieval database, enabling us to leverage human supervision in an offline\nsetting. We evaluate the proposed pipeline using GPT-3.5-turbo API on five\ndomain-specific benchmarks for German-English translation. The results\ndemonstrate the effectiveness of the pipeline in tailoring in-domain\ntranslations and improving translation performance compared to direct\ntranslation. Additionally, we discuss the results from the following\nperspectives: 1) the effectiveness of different in-context retrieval methods;\n2) the construction of a retrieval database under low-resource scenarios; 3)\nthe observed domains differences; 4) the quantitative analysis of linguistic\nstatistics; and 5) the qualitative analysis of translation cases. The code and\ndata are available at https://github.com/NLP2CT/HIL-MT/.\n", "rewritten_text": "\\boxed{\nThe large language model (LLM) has gained significant attention for its ability to learn in context and exhibit emergent capabilities. While previous research has focused on applying LLMs to machine translation tasks, it has neglected the role of human intervention in the inference process. However, the characteristics of LLMs, such as in-context learning and prompt engineering, are similar to human cognitive abilities in language tasks, making them suitable for human-in-the-loop generation. This study proposes a human-in-the-loop pipeline that guides LLMs to produce customized outputs with revision instructions. The pipeline involves prompting the LLM to generate a draft translation, followed by automatic retrieval or human feedback to enhance the translation through in-context learning. The human-machine interactions are stored in an external database to expand the in-context retrieval database, enabling human supervision in an offline setting. The proposed pipeline is evaluated using the GPT-3.5-turbo API on five domain-specific benchmarks for German-English translation, demonstrating its effectiveness in tailoring in-domain translations and improving translation performance. The results are discussed from various perspectives, including the effectiveness of different in-context retrieval methods, constructing a retrieval database with limited resources, domain differences, linguistic statistics, and qualitative analysis of translation cases. The code and data are available at https://github.com/NLP2CT/HIL-MT/.\n}"}, "1011.0519": {"original_text": "  It is usual to consider that standards generate mixed feelings among\nscientists. They are often seen as not really reflecting the state of the art\nin a given domain and a hindrance to scientific creativity. Still, scientists\nshould theoretically be at the best place to bring their expertise into\nstandard developments, being even more neutral on issues that may typically be\nrelated to competing industrial interests. Even if it could be thought of as\neven more complex to think about developping standards in the humanities, we\nwill show how this can be made feasible through the experience gained both\nwithin the Text Encoding Initiative consortium and the International\nOrganisation for Standardisation. By taking the specific case of lexical\nresources, we will try to show how this brings about new ideas for designing\nfuture research infrastructures in the human and social sciences.\n", "rewritten_text": "\\boxed{Standards often evoke mixed emotions among scientists, who view them as outdated and restrictive to creativity. However, scientists are well-positioned to contribute their expertise to standard development, bringing neutrality to issues that may be influenced by competing industrial interests. While developing standards in the humanities may seem even more complex, we will demonstrate how it can be achieved through the experiences of the Text Encoding Initiative consortium and the International Organisation for Standardisation. Using the example of lexical resources, we will explore how this leads to innovative ideas for designing future research infrastructures in the human and social sciences.}"}, "2305.04451": {"original_text": "  Virtual try-on attracts increasing research attention as a promising way for\nenhancing the user experience for online cloth shopping. Though existing\nmethods can generate impressive results, users need to provide a well-designed\nreference image containing the target fashion clothes that often do not exist.\nTo support user-friendly fashion customization in full-body portraits, we\npropose a multi-modal interactive setting by combining the advantages of both\ntext and texture for multi-level fashion manipulation. With the carefully\ndesigned fashion editing module and loss functions, FashionTex framework can\nsemantically control cloth types and local texture patterns without annotated\npairwise training data. We further introduce an ID recovery module to maintain\nthe identity of input portrait. Extensive experiments have demonstrated the\neffectiveness of our proposed pipeline.\n", "rewritten_text": "\\boxed{Research on virtual try-on for online clothing shopping is gaining popularity, but current methods require users to provide a reference image of the desired outfit, which may not exist. To address this, we propose a multi-modal interactive system that combines text and texture inputs to enable flexible fashion customization in full-body portraits. Our FashionTex framework can manipulate cloth types and textures without requiring annotated training data, and also preserves the original identity of the input portrait. Experimental results confirm the effectiveness of our approach.}"}, "2310.05989": {"original_text": "  3D object detection plays a pivotal role in autonomous driving and robotics,\ndemanding precise interpretation of Bird's Eye View (BEV) images. The dynamic\nnature of real-world environments necessitates the use of dynamic query\nmechanisms in 3D object detection to adaptively capture and process the complex\nspatio-temporal relationships present in these scenes. However, prior\nimplementations of dynamic queries have often faced difficulties in effectively\nleveraging these relationships, particularly when it comes to integrating\ntemporal information in a computationally efficient manner. Addressing this\nlimitation, we introduce a framework utilizing dynamic query evolution\nstrategy, harnesses K-means clustering and Top-K attention mechanisms for\nrefined spatio-temporal data processing. By dynamically segmenting the BEV\nspace and prioritizing key features through Top-K attention, our model achieves\na real-time, focused analysis of pertinent scene elements. Our extensive\nevaluation on the nuScenes and Waymo dataset showcases a marked improvement in\ndetection accuracy, setting a new benchmark in the domain of query-based BEV\nobject detection. Our dynamic query evolution strategy has the potential to\npush the boundaries of current BEV methods with enhanced adaptability and\ncomputational efficiency. Project page:\nhttps://github.com/Jiawei-Yao0812/QE-BEV\n", "rewritten_text": "\\boxed{\nAccurate 3D object detection is crucial in autonomous driving and robotics, particularly when interpreting Bird's Eye View (BEV) images. However, the dynamic nature of real-world environments requires adaptive processing of complex spatio-temporal relationships. Existing dynamic query mechanisms struggle to effectively integrate temporal information while maintaining computational efficiency. To address this, we propose a framework that utilizes dynamic query evolution, K-means clustering, and Top-K attention mechanisms to refine spatio-temporal data processing. Our model achieves real-time, focused analysis of key scene elements by dynamically segmenting BEV space and prioritizing features through Top-K attention. Our evaluation on nuScenes and Waymo datasets demonstrates a significant improvement in detection accuracy, setting a new benchmark in query-based BEV object detection. Our dynamic query evolution strategy has the potential to enhance adaptability and computational efficiency in BEV methods. (Project page: https://github.com/Jiawei-Yao0812/QE-BEV)\n}"}, "1907.03513": {"original_text": "  Keeping up to date on emerging entities that appear every day is\nindispensable for various applications, such as social-trend analysis and\nmarketing research. Previous studies have attempted to detect unseen entities\nthat are not registered in a particular knowledge base as emerging entities and\nconsequently find non-emerging entities since the absence of entities in\nknowledge bases does not guarantee their emergence. We therefore introduce a\nnovel task of discovering truly emerging entities when they have just been\nintroduced to the public through microblogs and propose an effective method\nbased on time-sensitive distant supervision, which exploits distinctive\nearly-stage contexts of emerging entities. Experimental results with a\nlarge-scale Twitter archive show that the proposed method achieves 83.2%\nprecision of the top 500 discovered emerging entities, which outperforms\nbaselines based on unseen entity recognition with burst detection. Besides\nnotable emerging entities, our method can discover massive long-tail and\nhomographic emerging entities. An evaluation of relative recall shows that the\nmethod detects 80.4% emerging entities newly registered in Wikipedia; 92.4% of\nthem are discovered earlier than their registration in Wikipedia, and the\naverage lead-time is more than one year (571 days).\n", "rewritten_text": "\\boxed{Staying current with newly emerging entities is crucial for applications like social trend analysis and marketing research. While previous studies have tried to identify unseen entities not listed in a knowledge base as emerging, this approach has limitations. We propose a new task: detecting truly emerging entities as soon as they appear in microblogs. Our method, which leverages time-sensitive distant supervision and early-stage contexts, achieves 83.2% precision in identifying top emerging entities on a large Twitter dataset. It outperforms baseline methods and can discover a large number of emerging entities, including those with low popularity and similar names. Our approach detects 80.4% of emerging entities before they are registered on Wikipedia, with an average lead time of over a year.}"}, "2312.04567": {"original_text": "  Recent significant advances in text-to-image models unlock the possibility of\ntraining vision systems using synthetic images, potentially overcoming the\ndifficulty of collecting curated data at scale. It is unclear, however, how\nthese models behave at scale, as more synthetic data is added to the training\nset. In this paper we study the scaling laws of synthetic images generated by\nstate of the art text-to-image models, for the training of supervised models:\nimage classifiers with label supervision, and CLIP with language supervision.\nWe identify several factors, including text prompts, classifier-free guidance\nscale, and types of text-to-image models, that significantly affect scaling\nbehavior. After tuning these factors, we observe that synthetic images\ndemonstrate a scaling trend similar to, but slightly less effective than, real\nimages in CLIP training, while they significantly underperform in scaling when\ntraining supervised image classifiers. Our analysis indicates that the main\nreason for this underperformance is the inability of off-the-shelf\ntext-to-image models to generate certain concepts, a limitation that\nsignificantly impairs the training of image classifiers. Our findings also\nsuggest that scaling synthetic data can be particularly effective in scenarios\nsuch as: (1) when there is a limited supply of real images for a supervised\nproblem (e.g., fewer than 0.5 million images in ImageNet), (2) when the\nevaluation dataset diverges significantly from the training data, indicating\nthe out-of-distribution scenario, or (3) when synthetic data is used in\nconjunction with real images, as demonstrated in the training of CLIP models.\n", "rewritten_text": "\\boxed{\nRecent breakthroughs in text-to-image models have opened up the possibility of training vision systems using synthetic images, which could overcome the challenge of collecting large amounts of curated data. However, it is unclear how these models perform when scaled up with more synthetic data. This study investigates the scaling laws of synthetic images generated by state-of-the-art text-to-image models for training supervised models, including image classifiers and CLIP models. We identify key factors that significantly impact scaling behavior, including text prompts, guidance scale, and model types. After optimizing these factors, we find that synthetic images exhibit a scaling trend similar to, but slightly less effective than, real images when training CLIP models, but significantly underperform when training supervised image classifiers. Our analysis reveals that the main reason for this underperformance is the inability of current text-to-image models to generate certain concepts, which hinders the training of image classifiers. Our findings suggest that scaling synthetic data can be particularly effective in scenarios where real images are limited, the evaluation dataset diverges significantly from the training data, or synthetic data is used in conjunction with real images, as demonstrated in CLIP model training.\n}"}, "1909.1244": {"original_text": "  Recently, pre-trained language models have achieved remarkable success in a\nbroad range of natural language processing tasks. However, in multilingual\nsetting, it is extremely resource-consuming to pre-train a deep language model\nover large-scale corpora for each language. Instead of exhaustively\npre-training monolingual language models independently, an alternative solution\nis to pre-train a powerful multilingual deep language model over large-scale\ncorpora in hundreds of languages. However, the vocabulary size for each\nlanguage in such a model is relatively small, especially for low-resource\nlanguages. This limitation inevitably hinders the performance of these\nmultilingual models on tasks such as sequence labeling, wherein in-depth\ntoken-level or sentence-level understanding is essential.\n  In this paper, inspired by previous methods designed for monolingual\nsettings, we investigate two approaches (i.e., joint mapping and mixture\nmapping) based on a pre-trained multilingual model BERT for addressing the\nout-of-vocabulary (OOV) problem on a variety of tasks, including part-of-speech\ntagging, named entity recognition, machine translation quality estimation, and\nmachine reading comprehension. Experimental results show that using mixture\nmapping is more promising. To the best of our knowledge, this is the first work\nthat attempts to address and discuss the OOV issue in multilingual settings.\n", "rewritten_text": "\\boxed{\nRecently, pre-trained language models have achieved great success in various natural language processing tasks. However, pre-training separate models for each language is resource-intensive. A solution is to pre-train a single multilingual model on large-scale corpora in hundreds of languages. Despite its power, this approach has a limitation: the vocabulary size for each language is relatively small, especially for low-resource languages, which hinders performance on tasks requiring in-depth understanding.\n\nThis paper explores two approaches, joint mapping and mixture mapping, to address the out-of-vocabulary problem in multilingual settings using a pre-trained BERT model. We evaluate these approaches on tasks such as part-of-speech tagging, named entity recognition, machine translation quality estimation, and machine reading comprehension. Our results show that mixture mapping is more promising. To our knowledge, this is the first work to tackle and discuss the out-of-vocabulary issue in multilingual settings.\n}"}, "2311.05821": {"original_text": "  While recent advances have boosted LM proficiency in linguistic benchmarks,\nLMs consistently struggle to reason correctly on complex tasks like\nmathematics. We turn to Reinforcement Learning from Human Feedback (RLHF) as a\nmethod with which to shape model reasoning processes. In particular, we explore\ntwo reward schemes, outcome-supervised reward models (ORMs) and\nprocess-supervised reward models (PRMs), to optimize for logical reasoning. Our\nresults show that the fine-grained reward provided by PRM-based methods\nenhances accuracy on simple mathematical reasoning (GSM8K) while, unexpectedly,\nreducing performance in complex tasks (MATH). Furthermore, we show the critical\nrole reward aggregation functions play in model performance. Providing\npromising avenues for future research, our study underscores the need for\nfurther exploration into fine-grained reward modeling for more reliable\nlanguage models.\n", "rewritten_text": "\\boxed{Recent language models (LMs) have improved in linguistic tasks, but still struggle with complex tasks like mathematics. To address this, we used Reinforcement Learning from Human Feedback (RLHF) to improve model reasoning. We compared two reward schemes: outcome-supervised reward models (ORMs) and process-supervised reward models (PRMs). Our results showed that PRM-based methods improved accuracy in simple math tasks, but surprisingly decreased performance in complex tasks. We also found that the way rewards are aggregated significantly impacts model performance. Our study highlights the need for further research into fine-grained reward modeling to develop more reliable language models.}"}, "1309.6379": {"original_text": "  We propose a large deformation diffeomorphic metric mapping algorithm to\nalign multiple b-value diffusion weighted imaging (mDWI) data, specifically\nacquired via hybrid diffusion imaging (HYDI), denoted as LDDMM-HYDI. We then\npropose a Bayesian model for estimating the white matter atlas from HYDIs. We\nadopt the work given in Hosseinbor et al. (2012) and represent the q-space\ndiffusion signal with the Bessel Fourier orientation reconstruction (BFOR)\nsignal basis. The BFOR framework provides the representation of mDWI in the\nq-space and thus reduces memory requirement. In addition, since the BFOR signal\nbasis is orthonormal, the L2 norm that quantifies the differences in the\nq-space signals of any two mDWI datasets can be easily computed as the sum of\nthe squared differences in the BFOR expansion coefficients. In this work, we\nshow that the reorientation of the $q$-space signal due to spatial\ntransformation can be easily defined on the BFOR signal basis. We incorporate\nthe BFOR signal basis into the LDDMM framework and derive the gradient descent\nalgorithm for LDDMM-HYDI with explicit orientation optimization. Additionally,\nwe extend the previous Bayesian atlas estimation framework for scalar-valued\nimages to HYDIs and derive the expectation-maximization algorithm for solving\nthe HYDI atlas estimation problem. Using real HYDI datasets, we show the\nBayesian model generates the white matter atlas with anatomical details.\nMoreover, we show that it is important to consider the variation of mDWI\nreorientation due to a small change in diffeomorphic transformation in the\nLDDMM-HYDI optimization and to incorporate the full information of HYDI for\naligning mDWI.\n", "rewritten_text": "\\boxed{\nWe introduce a novel algorithm, LDDMM-HYDI, for aligning multiple b-value diffusion weighted imaging (mDWI) data acquired through hybrid diffusion imaging (HYDI). This algorithm leverages a Bayesian model to estimate a white matter atlas from HYDI data. We build upon previous work by Hosseinbor et al. (2012) and utilize the Bessel Fourier orientation reconstruction (BFOR) signal basis to represent q-space diffusion signals. The BFOR framework reduces memory requirements and enables easy computation of signal differences. We incorporate the BFOR signal basis into the LDDMM framework and derive a gradient descent algorithm with explicit orientation optimization. Furthermore, we extend a previous Bayesian atlas estimation framework for scalar-valued images to HYDIs and develop an expectation-maximization algorithm for solving the HYDI atlas estimation problem. Our approach is validated using real HYDI datasets, generating a white matter atlas with anatomical details. We demonstrate the importance of considering mDWI reorientation variations due to small changes in diffeomorphic transformations during LDDMM-HYDI optimization and incorporating full HYDI information for aligning mDWI.\n}"}, "2101.03848": {"original_text": "  Convolutional neural networks (CNNs) have been widely used in various vision\ntasks, e.g. image classification, semantic segmentation, etc. Unfortunately,\nstandard 2D CNNs are not well suited for spherical signals such as panorama\nimages or spherical projections, as the sphere is an unstructured grid. In this\npaper, we present Spherical Transformer which can transform spherical signals\ninto vectors that can be directly processed by standard CNNs such that many\nwell-designed CNNs architectures can be reused across tasks and datasets by\npretraining. To this end, the proposed method first uses local structured\nsampling methods such as HEALPix to construct a transformer grid by using the\ninformation of spherical points and its adjacent points, and then transforms\nthe spherical signals to the vectors through the grid. By building the\nSpherical Transformer module, we can use multiple CNN architectures directly.\nWe evaluate our approach on the tasks of spherical MNIST recognition, 3D object\nclassification and omnidirectional image semantic segmentation. For 3D object\nclassification, we further propose a rendering-based projection method to\nimprove the performance and a rotational-equivariant model to improve the\nanti-rotation ability. Experimental results on three tasks show that our\napproach achieves superior performance over state-of-the-art methods.\n", "rewritten_text": "\\boxed{Convolutional neural networks (CNNs) have been successful in various image-related tasks, but they struggle with spherical signals like panorama images or spherical projections due to the unstructured grid of the sphere. This paper introduces the Spherical Transformer, which converts spherical signals into vectors that can be processed by standard CNNs, allowing for the reuse of well-designed architectures across tasks and datasets through pretraining. The proposed method uses local structured sampling techniques like HEALPix to create a transformer grid, which is then used to transform spherical signals into vectors. By incorporating the Spherical Transformer module, multiple CNN architectures can be utilized directly. The approach is evaluated on spherical MNIST recognition, 3D object classification, and omnidirectional image semantic segmentation tasks, with experimental results showing superior performance compared to state-of-the-art methods.}"}, "2209.02686": {"original_text": "  Image-to-image translation has played an important role in enabling synthetic\ndata for computer vision. However, if the source and target domains have a\nlarge semantic mismatch, existing techniques often suffer from source content\ncorruption aka semantic flipping. To address this problem, we propose a new\nparadigm for image-to-image translation using Vector Symbolic Architectures\n(VSA), a theoretical framework which defines algebraic operations in a\nhigh-dimensional vector (hypervector) space. We introduce VSA-based constraints\non adversarial learning for source-to-target translations by learning a\nhypervector mapping that inverts the translation to ensure consistency with\nsource content. We show both qualitatively and quantitatively that our method\nimproves over other state-of-the-art techniques.\n", "rewritten_text": "\\boxed{Image-to-image translation has revolutionized computer vision by generating synthetic data, but it often struggles with semantic inconsistencies when the source and target domains are vastly different. To tackle this issue, we present a novel approach that leverages Vector Symbolic Architectures (VSA), a theoretical framework that performs algebraic operations in high-dimensional vector spaces. By incorporating VSA-based constraints into adversarial learning, we ensure that the translation process preserves the source content by learning an invertible hypervector mapping. Our method demonstrates significant qualitative and quantitative improvements over existing state-of-the-art techniques.}"}, "2305.14576": {"original_text": "  Pre-trained language models (PLMs) have ignited a surge in demand for\neffective fine-tuning techniques, particularly in low-resource domains and\nlanguages. Active learning (AL), a set of algorithms designed to decrease\nlabeling costs by minimizing label complexity, has shown promise in confronting\nthe labeling bottleneck. In parallel, adapter modules designed for\nparameter-efficient fine-tuning (PEFT) have demonstrated notable potential in\nlow-resource settings. However, the interplay between AL and adapter-based PEFT\nremains unexplored. We present an empirical study of PEFT behavior with AL in\nlow-resource settings for text classification tasks. Our findings affirm the\nsuperiority of PEFT over full-fine tuning (FFT) in low-resource settings and\ndemonstrate that this advantage persists in AL setups. We further examine the\nproperties of PEFT and FFT through the lens of forgetting dynamics and\ninstance-level representations, where we find that PEFT yields more stable\nrepresentations of early and middle layers compared to FFT. Our research\nunderscores the synergistic potential of AL and PEFT in low-resource settings,\npaving the way for advancements in efficient and effective fine-tuning.\n", "rewritten_text": "\\boxed{The growing demand for fine-tuning techniques in low-resource domains and languages has led to the exploration of active learning (AL) and parameter-efficient fine-tuning (PEFT) using adapter modules. While AL aims to reduce labeling costs, PEFT has shown promise in low-resource settings. This study investigates the combination of AL and PEFT for text classification tasks in low-resource settings, confirming that PEFT outperforms full fine-tuning (FFT) and that this advantage holds in AL setups. The analysis of forgetting dynamics and instance-level representations reveals that PEFT produces more stable representations in early and middle layers compared to FFT. The findings highlight the potential synergy between AL and PEFT in low-resource settings, paving the way for more efficient and effective fine-tuning techniques.}"}, "2308.15448": {"original_text": "  The negative effects of online bullying and harassment are increasing with\nInternet popularity, especially in social media. One solution is using natural\nlanguage processing (NLP) and machine learning (ML) methods for the automatic\ndetection of harmful remarks, but these methods are limited in low-resource\nlanguages like the Chittagonian dialect of Bangla.This study focuses on\ndetecting vulgar remarks in social media using supervised ML and deep learning\nalgorithms.Logistic Regression achieved promising accuracy (0.91) while simple\nRNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the\nissue that NN algorithms require more data.\n", "rewritten_text": "\\boxed{As online harassment and bullying escalate with the growing popularity of the Internet, particularly on social media, a potential solution lies in utilizing natural language processing and machine learning techniques to automatically identify harmful comments. However, these methods are hindered by limited resources in languages like Chittagonian Bangla. This study explores the detection of offensive remarks on social media using supervised machine learning and deep learning algorithms, revealing that Logistic Regression achieves a promising accuracy of 0.91, while simple RNN with Word2vec and fastText yields lower accuracy (0.84-0.90), highlighting the need for more data to improve neural network algorithms.}"}, "2305.06052": {"original_text": "  Quantization is a widely adopted technique for deep neural networks to reduce\nthe memory and computational resources required. However, when quantized, most\nmodels would need a suitable calibration process to keep their performance\nintact, which requires data from the target domain, such as a fraction of the\ndataset used in model training and model validation (i.e. calibration dataset).\n  In this study, we investigate the use of synthetic data as a substitute for\nthe calibration with real data for the quantization method. We propose a data\ngeneration method based on Generative Adversarial Networks that are trained\nprior to the model quantization step. We compare the performance of models\nquantized using data generated by StyleGAN2-ADA and our pre-trained DiStyleGAN,\nwith quantization using real data and an alternative data generation method\nbased on fractal images. Overall, the results of our experiments demonstrate\nthe potential of leveraging synthetic data for calibration during the\nquantization process. In our experiments, the percentage of accuracy\ndegradation of the selected models was less than 0.6%, with our best\nperformance achieved on MobileNetV2 (0.05%). The code is available at:\nhttps://github.com/ThanosM97/gsoc2022-openvino\n", "rewritten_text": "\\boxed{\nQuantization is a popular technique used to reduce the memory and computational resources required for deep neural networks. However, this process often requires a calibration step using real data from the target domain to maintain the model's performance. This study explores the use of synthetic data as a substitute for real data in the calibration process. A data generation method based on Generative Adversarial Networks (GANs) is proposed, which is trained before the model quantization step. The performance of models quantized using synthetic data generated by StyleGAN2-ADA and DiStyleGAN is compared to quantization using real data and an alternative method based on fractal images. The results show that synthetic data can be effectively used for calibration during quantization, with an accuracy degradation of less than 0.6% in the selected models, and a best performance of 0.05% achieved on MobileNetV2. The code for this study is available at https://github.com/ThanosM97/gsoc2022-openvino.\n}"}, "2406.15719": {"original_text": "  Convolutional Neural Networks (CNNs) and vision transformers (ViTs) have\nshown excellent capability in complex hyperspectral image (HSI) classification.\nHowever, these models require a significant number of training data and are\ncomputational resources. On the other hand, modern Multi-Layer Perceptrons\n(MLPs) have demonstrated great classification capability. These modern\nMLP-based models require significantly less training data compared to CNNs and\nViTs, achieving the state-of-the-art classification accuracy. Recently,\nKolmogorov-Arnold Networks (KANs) were proposed as viable alternatives for\nMLPs. Because of their internal similarity to splines and their external\nsimilarity to MLPs, KANs are able to optimize learned features with remarkable\naccuracy in addition to being able to learn new features. Thus, in this study,\nwe assess the effectiveness of KANs for complex HSI data classification.\nMoreover, to enhance the HSI classification accuracy obtained by the KANs, we\ndevelop and propose a Hybrid architecture utilizing 1D, 2D, and 3D KANs. To\ndemonstrate the effectiveness of the proposed KAN architecture, we conducted\nextensive experiments on three newly created HSI benchmark datasets:\nQUH-Pingan, QUH-Tangdaowan, and QUH-Qingyun. The results underscored the\ncompetitive or better capability of the developed hybrid KAN-based model across\nthese benchmark datasets over several other CNN- and ViT-based algorithms,\nincluding 1D-CNN, 2DCNN, 3D CNN, VGG-16, ResNet-50, EfficientNet, RNN, and ViT.\nThe code are publicly available at (https://github.com/aj1365/HSIConvKAN)\n", "rewritten_text": "\\boxed{\nConvolutional Neural Networks (CNNs) and vision transformers (ViTs) have achieved excellent performance in complex hyperspectral image (HSI) classification, but they require a large amount of training data and computational resources. In contrast, modern Multi-Layer Perceptrons (MLPs) have demonstrated great classification capability with significantly less training data. Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as a promising alternative to MLPs, offering optimized learned features and the ability to learn new features. This study evaluates the effectiveness of KANs for complex HSI data classification and proposes a hybrid architecture combining 1D, 2D, and 3D KANs to enhance classification accuracy. Experimental results on three newly created HSI benchmark datasets (QUH-Pingan, QUH-Tangdaowan, and QUH-Qingyun) show that the proposed hybrid KAN-based model outperforms several CNN- and ViT-based algorithms, including 1D-CNN, 2DCNN, 3D CNN, VGG-16, ResNet-50, EfficientNet, RNN, and ViT. The code is publicly available at https://github.com/aj1365/HSIConvKAN.\n}"}, "1905.00413": {"original_text": "  We explore the problem of view synthesis from a narrow baseline pair of\nimages, and focus on generating high-quality view extrapolations with plausible\ndisocclusions. Our method builds upon prior work in predicting a multiplane\nimage (MPI), which represents scene content as a set of RGB$\\alpha$ planes\nwithin a reference view frustum and renders novel views by projecting this\ncontent into the target viewpoints. We present a theoretical analysis showing\nhow the range of views that can be rendered from an MPI increases linearly with\nthe MPI disparity sampling frequency, as well as a novel MPI prediction\nprocedure that theoretically enables view extrapolations of up to $4\\times$ the\nlateral viewpoint movement allowed by prior work. Our method ameliorates two\nspecific issues that limit the range of views renderable by prior methods: 1)\nWe expand the range of novel views that can be rendered without depth\ndiscretization artifacts by using a 3D convolutional network architecture along\nwith a randomized-resolution training procedure to allow our model to predict\nMPIs with increased disparity sampling frequency. 2) We reduce the repeated\ntexture artifacts seen in disocclusions by enforcing a constraint that the\nappearance of hidden content at any depth must be drawn from visible content at\nor behind that depth. Please see our results video at:\nhttps://www.youtube.com/watch?v=aJqAaMNL2m4.\n", "rewritten_text": "\\boxed{This paper tackles the problem of generating high-quality view extrapolations with realistic disocclusions from a narrow baseline pair of images. Building on previous work on multiplane images (MPIs), which represent scene content as a set of RGB\u03b1 planes, we present a theoretical analysis showing that the range of renderable views increases linearly with the MPI disparity sampling frequency. We also introduce a novel MPI prediction procedure that enables view extrapolations of up to 4\u00d7 the lateral viewpoint movement allowed by prior work. Our method addresses two key limitations of previous methods: (1) we use a 3D convolutional network and randomized-resolution training to predict MPIs with increased disparity sampling frequency, reducing depth discretization artifacts, and (2) we enforce a constraint to prevent repeated texture artifacts in disocclusions by ensuring that hidden content is drawn from visible content at or behind the same depth. A results video is available at https://www.youtube.com/watch?v=aJqAaMNL2m4.}"}, "1909.10579": {"original_text": "  Neural language models (LMs) perform well on tasks that require sensitivity\nto syntactic structure. Drawing on the syntactic priming paradigm from\npsycholinguistics, we propose a novel technique to analyze the representations\nthat enable such success. By establishing a gradient similarity metric between\nstructures, this technique allows us to reconstruct the organization of the\nLMs' syntactic representational space. We use this technique to demonstrate\nthat LSTM LMs' representations of different types of sentences with relative\nclauses are organized hierarchically in a linguistically interpretable manner,\nsuggesting that the LMs track abstract properties of the sentence.\n", "rewritten_text": "\\boxed{Neural language models have been shown to excel in tasks that require an understanding of sentence structure. Building on the concept of syntactic priming from psycholinguistics, we introduce a new method to examine the internal representations that enable this success. This approach allows us to visualize the organization of the language model's syntactic knowledge, revealing a hierarchical structure that mirrors linguistic principles. Our findings suggest that LSTM language models are capable of capturing abstract properties of sentences, particularly in relation to relative clauses.}"}, "1804.04213": {"original_text": "  We study how to synthesize novel views of human body from a single image.\nThough recent deep learning based methods work well for rigid objects, they\noften fail on objects with large articulation, like human bodies. The core step\nof existing methods is to fit a map from the observable views to novel views by\nCNNs; however, the rich articulation modes of human body make it rather\nchallenging for CNNs to memorize and interpolate the data well. To address the\nproblem, we propose a novel deep learning based pipeline that explicitly\nestimates and leverages the geometry of the underlying human body. Our new\npipeline is a composition of a shape estimation network and an image generation\nnetwork, and at the interface a perspective transformation is applied to\ngenerate a forward flow for pixel value transportation. Our design is able to\nfactor out the space of data variation and makes learning at each step much\neasier. Empirically, we show that the performance for pose-varying objects can\nbe improved dramatically. Our method can also be applied on real data captured\nby 3D sensors, and the flow generated by our methods can be used for generating\nhigh quality results in higher resolution.\n", "rewritten_text": "\\boxed{This study focuses on generating novel views of the human body from a single image. While deep learning methods have been successful for rigid objects, they often struggle with objects that have large articulation, such as the human body. To overcome this challenge, we propose a novel pipeline that combines a shape estimation network and an image generation network, with a perspective transformation applied in between to facilitate pixel value transportation. By explicitly estimating and leveraging the geometry of the human body, our approach can factor out data variation and improve learning. Our method achieves dramatic performance improvements for pose-varying objects and can be applied to real data captured by 3D sensors, generating high-quality results at higher resolutions.}"}, "2401.04354": {"original_text": "  With the explosive growth of video data in real-world applications, a\ncomprehensive representation of videos becomes increasingly important. In this\npaper, we address the problem of video scene recognition, whose goal is to\nlearn a high-level video representation to classify scenes in videos. Due to\nthe diversity and complexity of video contents in realistic scenarios, this\ntask remains a challenge. Most existing works identify scenes for videos only\nfrom visual or textual information in a temporal perspective, ignoring the\nvaluable information hidden in single frames, while several earlier studies\nonly recognize scenes for separate images in a non-temporal perspective. We\nargue that these two perspectives are both meaningful for this task and\ncomplementary to each other, meanwhile, externally introduced knowledge can\nalso promote the comprehension of videos. We propose a novel two-stream\nframework to model video representations from multiple perspectives, i.e.\ntemporal and non-temporal perspectives, and integrate the two perspectives in\nan end-to-end manner by self-distillation. Besides, we design a\nknowledge-enhanced feature fusion and label prediction method that contributes\nto naturally introducing knowledge into the task of video scene recognition.\nExperiments conducted on a real-world dataset demonstrate the effectiveness of\nour proposed method.\n", "rewritten_text": "\\boxed{The rapid growth of video data has created a pressing need for a comprehensive video representation. This paper tackles the challenge of video scene recognition, which involves learning a high-level representation to classify video scenes. Existing approaches focus either on temporal or non-temporal perspectives, neglecting the valuable information in individual frames. We propose a novel two-stream framework that integrates both perspectives through self-distillation, and incorporates external knowledge to enhance video comprehension. Our method, which includes knowledge-enhanced feature fusion and label prediction, is validated through experiments on a real-world dataset.}"}, "2211.08112": {"original_text": "  Active Learning (AL) is a powerful tool for learning with less labeled data,\nin particular, for specialized domains, like legal documents, where unlabeled\ndata is abundant, but the annotation requires domain expertise and is thus\nexpensive. Recent works have shown the effectiveness of AL strategies for\npre-trained language models. However, most AL strategies require a set of\nlabeled samples to start with, which is expensive to acquire. In addition,\npre-trained language models have been shown unstable during fine-tuning with\nsmall datasets, and their embeddings are not semantically meaningful. In this\nwork, we propose a pipeline for effectively using active learning with\npre-trained language models in the legal domain. To this end, we leverage the\navailable unlabeled data in three phases. First, we continue pre-training the\nmodel to adapt it to the downstream task. Second, we use knowledge distillation\nto guide the model's embeddings to a semantically meaningful space. Finally, we\npropose a simple, yet effective, strategy to find the initial set of labeled\nsamples with fewer actions compared to existing methods. Our experiments on\nContract-NLI, adapted to the classification task, and LEDGAR benchmarks show\nthat our approach outperforms standard AL strategies, and is more efficient.\nFurthermore, our pipeline reaches comparable results to the fully-supervised\napproach with a small performance gap, and dramatically reduced annotation\ncost. Code and the adapted data will be made available.\n", "rewritten_text": "\\boxed{\nActive learning (AL) is a valuable technique for learning with limited labeled data, particularly in specialized domains like legal documents where unlabeled data is plentiful but annotation requires expertise and is costly. Recent studies have demonstrated the effectiveness of AL strategies for pre-trained language models. However, most AL strategies require an initial set of labeled samples, which can be expensive to obtain. Moreover, pre-trained language models can be unstable during fine-tuning with small datasets, and their embeddings may not be semantically meaningful. To address these challenges, we propose a three-phase pipeline for effectively using AL with pre-trained language models in the legal domain. We leverage available unlabeled data to adapt the model to the downstream task, guide the model's embeddings to a semantically meaningful space using knowledge distillation, and develop a simple yet effective strategy to select the initial set of labeled samples with reduced annotation effort. Our experiments on Contract-NLI and LEDGAR benchmarks show that our approach outperforms standard AL strategies, is more efficient, and achieves comparable results to fully-supervised approaches with significantly reduced annotation costs. We will make our code and adapted data available.\n}"}, "2102.03115": {"original_text": "  Object detection in natural scenes can be a challenging task. In many\nreal-life situations, the visible spectrum is not suitable for traditional\ncomputer vision tasks. Moving outside the visible spectrum range, such as the\nthermal spectrum or the near-infrared (NIR) images, is much more beneficial in\nlow visibility conditions, NIR images are very helpful for understanding the\nobject's material quality. In this work, we have taken images with both the\nThermal and NIR spectrum for the object detection task. As multi-spectral data\nwith both Thermal and NIR is not available for the detection task, we needed to\ncollect data ourselves. Data collection is a time-consuming process, and we\nfaced many obstacles that we had to overcome. We train the YOLO v3 network from\nscratch to detect an object from multi-spectral images. Also, to avoid\noverfitting, we have done data augmentation and tune hyperparameters.\n", "rewritten_text": "\\boxed{Object detection in natural scenes can be problematic due to limitations of the visible spectrum. To overcome this, we utilized thermal and near-infrared (NIR) images, which are more effective in low-visibility conditions and provide valuable insights into an object's material quality. Since multi-spectral data combining thermal and NIR images was unavailable, we collected our own dataset, overcoming numerous challenges in the process. We then trained a YOLO v3 network from scratch to detect objects in these multi-spectral images, employing data augmentation and hyperparameter tuning to prevent overfitting.}"}, "2207.09157": {"original_text": "  Supervised deep learning-based approaches have been applied to task-oriented\ndialog and have proven to be effective for limited domain and language\napplications when a sufficient number of training examples are available. In\npractice, these approaches suffer from the drawbacks of domain-driven design\nand under-resourced languages. Domain and language models are supposed to grow\nand change as the problem space evolves. On one hand, research on transfer\nlearning has demonstrated the cross-lingual ability of multilingual\nTransformers-based models to learn semantically rich representations. On the\nother, in addition to the above approaches, meta-learning have enabled the\ndevelopment of task and language learning algorithms capable of far\ngeneralization. Through this context, this article proposes to investigate the\ncross-lingual transferability of using synergistically few-shot learning with\nprototypical neural networks and multilingual Transformers-based models.\nExperiments in natural language understanding tasks on MultiATIS++ corpus shows\nthat our approach substantially improves the observed transfer learning\nperformances between the low and the high resource languages. More generally\nour approach confirms that the meaningful latent space learned in a given\nlanguage can be can be generalized to unseen and under-resourced ones using\nmeta-learning.\n", "rewritten_text": "\\boxed{\nDeep learning approaches have been successful in task-oriented dialog systems for specific domains and languages, but they have limitations when dealing with diverse languages and domains. To address this, researchers have explored transfer learning and meta-learning techniques. This article proposes a novel approach that combines few-shot learning with prototypical neural networks and multilingual Transformers-based models to achieve cross-lingual transferability. Experimental results on the MultiATIS++ corpus demonstrate significant improvement in transfer learning performance between low-resource and high-resource languages, confirming that meta-learning can generalize meaningful latent spaces to unseen and under-resourced languages.\n}"}, "2203.1521": {"original_text": "  To learn camera-view invariant features for person Re-IDentification (Re-ID),\nthe cross-camera image pairs of each person play an important role. However,\nsuch cross-view training samples could be unavailable under the ISolated Camera\nSupervised (ISCS) setting, e.g., a surveillance system deployed across distant\nscenes. To handle this challenging problem, a new pipeline is introduced by\nsynthesizing the cross-camera samples in the feature space for model training.\nSpecifically, the feature encoder and generator are end-to-end optimized under\na novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint\nlearning procedure raises concern on the stability of generative model\ntraining. Therefore, a new feature generator, $\\sigma$-Regularized Conditional\nVariational Autoencoder ($\\sigma$-Reg.~CVAE), is proposed with theoretical and\nexperimental analysis on its robustness. Extensive experiments on two ISCS\nperson Re-ID datasets demonstrate the superiority of our CCSFG to the\ncompetitors.\n", "rewritten_text": "\\boxed{\nLearning camera-view invariant features for person Re-Identification (Re-ID) relies heavily on cross-camera image pairs of each person. However, in the ISolated Camera Supervised (ISCS) setting, where cameras are deployed across distant scenes, such cross-view training samples may not be available. To address this challenge, a novel pipeline is proposed, which synthesizes cross-camera samples in the feature space for model training. This pipeline, called Camera-Conditioned Stable Feature Generation (CCSFG), jointly optimizes the feature encoder and generator. To ensure the stability of the generative model training, a new feature generator, $\\sigma$-Regularized Conditional Variational Autoencoder ($\\sigma$-Reg.~CVAE), is introduced, along with theoretical and experimental analysis of its robustness. The proposed CCSFG approach outperforms competitors in extensive experiments on two ISCS person Re-ID datasets."}, "1812.0957": {"original_text": "  In recent years, we have seen the performance of video-based person\nRe-Identification (ReID) methods have improved considerably. However, most of\nthe work in this area has dealt with videos acquired by fixed cameras with\nwider field of view. Recently, widespread use of wearable cameras and recording\ndevices such as cellphones have opened the door to interesting research in\nfirst-person Point-of-view (POV) videos (egocentric videos). Nonetheless,\nanalysis of such videos is challenging due to factors such as poor video\nquality due to ego-motion, blurriness, severe changes in lighting conditions\nand perspective distortions. To facilitate the research towards conquering\nthese challenges, this paper contributes a new dataset called EgoReID. The\ndataset is captured using 3 mobile cellphones with non-overlapping\nfield-of-view. It contains 900 IDs and around 10,200 tracks with a total of\n176,000 detections. The dataset also contains 12-sensor meta data e.g. camera\norientation pitch and rotation for each video.\n  In addition, we propose a new framework which takes advantage of both visual\nand sensor meta data to successfully perform Person ReID. We extend image-based\nre-ID method employing human body parsing trained on ten datasets to\nvideo-based re-ID. In our method, first frame level local features are\nextracted for each semantic region, then 3D convolutions are applied to encode\nthe temporal information in each sequence of semantic regions. Additionally, we\nemploy sensor meta data to predict targets' next camera and their estimated\ntime of arrival, which considerably improves our ReID performance as it\nsignificantly reduces our search space.\n", "rewritten_text": "\\boxed{\nRecent advancements in video-based person Re-Identification (ReID) methods have been significant. However, most research has focused on videos from fixed cameras with a wide field of view. The increasing use of wearable cameras and devices like cellphones has opened up new opportunities for research in first-person Point-of-view (POV) videos, also known as egocentric videos. Despite the challenges posed by poor video quality, blurriness, and lighting changes, this paper introduces a new dataset called EgoReID, which includes 900 IDs, 10,200 tracks, and 176,000 detections from 3 mobile cellphones with non-overlapping fields of view. The dataset also provides 12-sensor metadata, including camera orientation and rotation. Furthermore, a new framework is proposed that leverages both visual and sensor metadata to achieve successful Person ReID. This framework extends image-based re-ID methods to video-based re-ID by extracting local features from semantic regions, applying 3D convolutions to encode temporal information, and utilizing sensor metadata to predict targets' next camera and estimated time of arrival, thereby significantly improving ReID performance.\n}"}, "2112.07928": {"original_text": "  Real-world data often follows a long-tailed distribution, which makes the\nperformance of existing classification algorithms degrade heavily. A key issue\nis that samples in tail categories fail to depict their intra-class diversity.\nHumans can imagine a sample in new poses, scenes, and view angles with their\nprior knowledge even if it is the first time to see this category. Inspired by\nthis, we propose a novel reasoning-based implicit semantic data augmentation\nmethod to borrow transformation directions from other classes. Since the\ncovariance matrix of each category represents the feature transformation\ndirections, we can sample new directions from similar categories to generate\ndefinitely different instances. Specifically, the long-tailed distributed data\nis first adopted to train a backbone and a classifier. Then, a covariance\nmatrix for each category is estimated, and a knowledge graph is constructed to\nstore the relations of any two categories. Finally, tail samples are adaptively\nenhanced via propagating information from all the similar categories in the\nknowledge graph. Experimental results on CIFAR-100-LT, ImageNet-LT, and\niNaturalist 2018 have demonstrated the effectiveness of our proposed method\ncompared with the state-of-the-art methods.\n", "rewritten_text": "\\boxed{\nReal-world data often exhibits a long-tailed distribution, which significantly impairs the performance of existing classification algorithms. A major challenge is that samples from tail categories lack representation of their intra-class diversity. However, humans can imagine a sample in various poses, scenes, and view angles, even if it's the first time they've seen the category, thanks to their prior knowledge. Inspired by this, we propose a novel implicit semantic data augmentation method that leverages transformation directions from other classes through reasoning. By sampling new directions from similar categories, we can generate diverse instances. Our approach involves training a backbone and classifier on long-tailed data, estimating a covariance matrix for each category, and constructing a knowledge graph to store category relationships. Finally, we adaptively enhance tail samples by propagating information from similar categories in the knowledge graph. Our method has demonstrated superior performance compared to state-of-the-art methods on CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018 datasets.\n}"}, "2208.07291": {"original_text": "  One of the possible dangers that older people face in their daily lives is\nfalling. Occlusion is one of the biggest challenges of vision-based fall\ndetection systems and degrades their detection performance considerably. To\ntackle this problem, we synthesize specifically-designed occluded videos for\ntraining fall detection systems using existing datasets. Then, by defining a\nnew cost function, we introduce a framework for weighted training of fall\ndetection models using occluded and un-occluded videos, which can be applied to\nany learnable fall detection system. Finally, we use both a non-deep and deep\nmodel to evaluate the effect of the proposed weighted training method.\nExperiments show that the proposed method can improve the classification\naccuracy by 36% for a non-deep model and 55% for a deep model in occlusion\nconditions. Moreover, it is shown that the proposed training framework can also\nsignificantly improve the detection performance of a deep network on normal\nun-occluded samples.\n", "rewritten_text": "\\boxed{Falls are a significant risk for older adults, and vision-based fall detection systems can be hindered by occlusion, leading to poor performance. To address this, we generate occluded videos from existing datasets to train fall detection systems. We also introduce a weighted training framework that combines occluded and un-occluded videos, which can be applied to any learnable fall detection model. Our experiments demonstrate that this approach improves classification accuracy by 36% for non-deep models and 55% for deep models in occluded conditions, and also enhances detection performance for un-occluded samples.}"}, "2101.0219": {"original_text": "  Object detection and semantic segmentation are two of the most widely adopted\ndeep learning algorithms in agricultural applications. One of the major sources\nof variability in image quality acquired in the outdoors for such tasks is\nchanging lighting condition that can alter the appearance of the objects or the\ncontents of the entire image. While transfer learning and data augmentation to\nsome extent reduce the need for large amount of data to train deep neural\nnetworks, the large variety of cultivars and the lack of shared datasets in\nagriculture makes wide-scale field deployments difficult. In this paper, we\npresent a high throughput robust active lighting-based camera system that\ngenerates consistent images in all lighting conditions. We detail experiments\nthat show the consistency in images quality leading to relatively fewer images\nto train deep neural networks for the task of object detection. We further\npresent results from field experiment under extreme lighting conditions where\nimages without active lighting significantly lack to provide consistent\nresults. The experimental results show that on average, deep nets for object\ndetection trained on consistent data required nearly four times less data to\nachieve similar level of accuracy. This proposed work could potentially provide\npragmatic solutions to computer vision needs in agriculture.\n", "rewritten_text": "\\boxed{Deep learning algorithms, such as object detection and semantic segmentation, are widely used in agricultural applications, but their performance is often hindered by varying lighting conditions that affect image quality. While transfer learning and data augmentation can mitigate this issue, the lack of shared datasets and diverse cultivars in agriculture make it challenging to deploy these algorithms on a large scale. To address this, we propose a high-throughput camera system that uses active lighting to generate consistent images regardless of lighting conditions. Our experiments demonstrate that this system leads to more consistent image quality, reducing the amount of data required to train deep neural networks for object detection by nearly four times. This innovation has the potential to provide practical solutions for computer vision applications in agriculture.}"}, "1911.03828": {"original_text": "  Wasserstein autoencoders are effective for text generation. They do not\nhowever provide any control over the style and topic of the generated sentences\nif the dataset has multiple classes and includes different topics. In this\nwork, we present a semi-supervised approach for generating stylized sentences.\nOur model is trained on a multi-class dataset and learns the latent\nrepresentation of the sentences using a mixture of Gaussian prior without any\nadversarial losses. This allows us to generate sentences in the style of a\nspecified class or multiple classes by sampling from their corresponding prior\ndistributions. Moreover, we can train our model on relatively small datasets\nand learn the latent representation of a specified class by adding external\ndata with other styles/classes to our dataset. While a simple WAE or VAE cannot\ngenerate diverse sentences in this case, generated sentences with our approach\nare diverse, fluent, and preserve the style and the content of the desired\nclasses.\n", "rewritten_text": "\\boxed{Wasserstein autoencoders are effective for text generation, but they lack control over style and topic when dealing with multi-class datasets. To address this, we propose a semi-supervised approach that learns latent sentence representations using a mixture of Gaussian prior without adversarial losses. This enables generating sentences in a specified style or multiple styles by sampling from corresponding prior distributions. Our approach can even handle small datasets by incorporating external data with diverse styles, resulting in diverse, fluent, and style-preserving sentences that outperform traditional WAE or VAE methods.}"}, "2106.01656": {"original_text": "  Many variants of unsupervised domain adaptation (UDA) problems have been\nproposed and solved individually. Its side effect is that a method that works\nfor one variant is often ineffective for or not even applicable to another,\nwhich has prevented practical applications. In this paper, we give a general\nrepresentation of UDA problems, named Generalized Domain Adaptation (GDA). GDA\ncovers the major variants as special cases, which allows us to organize them in\na comprehensive framework. Moreover, this generalization leads to a new\nchallenging setting where existing methods fail, such as when domain labels are\nunknown, and class labels are only partially given to each domain. We propose a\nnovel approach to the new setting. The key to our approach is self-supervised\nclass-destructive learning, which enables the learning of class-invariant\nrepresentations and domain-adversarial classifiers without using any domain\nlabels. Extensive experiments using three benchmark datasets demonstrate that\nour method outperforms the state-of-the-art UDA methods in the new setting and\nthat it is competitive in existing UDA variations as well.\n", "rewritten_text": "\\boxed{Unsupervised domain adaptation (UDA) problems have been tackled individually, resulting in methods that are often ineffective or inapplicable across different variants. This paper introduces a unified framework, Generalized Domain Adaptation (GDA), which encompasses major UDA variants and reveals a new challenging setting where domain labels are unknown and class labels are partially given. A novel approach is proposed, leveraging self-supervised class-destructive learning to learn class-invariant representations and domain-adversarial classifiers without domain labels. Experimental results on three benchmark datasets demonstrate the superiority of our method in the new setting and its competitiveness in existing UDA variations.}"}, "2305.07152": {"original_text": "  The ability to automatically detect and track surgical instruments in\nendoscopic videos can enable transformational interventions. Assessing surgical\nperformance and efficiency, identifying skilled tool use and choreography, and\nplanning operational and logistical aspects of OR resources are just a few of\nthe applications that could benefit. Unfortunately, obtaining the annotations\nneeded to train machine learning models to identify and localize surgical tools\nis a difficult task. Annotating bounding boxes frame-by-frame is tedious and\ntime-consuming, yet large amounts of data with a wide variety of surgical tools\nand surgeries must be captured for robust training. Moreover, ongoing annotator\ntraining is needed to stay up to date with surgical instrument innovation. In\nrobotic-assisted surgery, however, potentially informative data like timestamps\nof instrument installation and removal can be programmatically harvested. The\nability to rely on tool installation data alone would significantly reduce the\nworkload to train robust tool-tracking models. With this motivation in mind we\ninvited the surgical data science community to participate in the challenge,\nSurgToolLoc 2022. The goal was to leverage tool presence data as weak labels\nfor machine learning models trained to detect tools and localize them in video\nframes with bounding boxes. We present the results of this challenge along with\nmany of the team's efforts. We conclude by discussing these results in the\nbroader context of machine learning and surgical data science. The training\ndata used for this challenge consisting of 24,695 video clips with tool\npresence labels is also being released publicly and can be accessed at\nhttps://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022.\n", "rewritten_text": "\\boxed{\nAutomatically detecting and tracking surgical instruments in endoscopic videos can revolutionize surgical interventions. This technology can assess surgical performance, identify skilled tool use, and optimize operating room resources. However, training machine learning models to identify and localize surgical tools requires large amounts of annotated data, which is time-consuming and tedious to obtain. In robotic-assisted surgery, tool installation data can be used as weak labels to train robust tool-tracking models, reducing the workload. To address this challenge, the SurgToolLoc 2022 challenge was launched, inviting the surgical data science community to develop machine learning models that can detect and localize tools in video frames using tool presence data. The results of this challenge are presented, along with the publicly available training data consisting of 24,695 video clips with tool presence labels.\n}"}, "2403.01858": {"original_text": "  We present TMMLU+, a new benchmark designed for Traditional Chinese language\nunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66\nsubjects from elementary to professional level. It is six times larger and\nboasts a more balanced subject distribution than its predecessor, Taiwan\nMassive Multitask Language Understanding (TMMLU). We also benchmark\nclosed-source models and 26 open-weight Chinese large language models (LLMs) of\nparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings reveal\nthat (1.) Traditional Chinese models still trail behind their Simplified\nChinese counterparts, highlighting a need for more focused advancements in LLMs\ncatering to Traditional Chinese. (2.) Current LLMs still fall short of human\nperformance in average scores, indicating a potential need for future research\nto delve deeper into social science and humanities subjects. (3.) Among all the\ntokenization compression metrics examined, we identify that only the fertility\nscore uniquely demonstrates strong correlations with our benchmark results. We\nforesee that TMMLU+ will pinpoint areas for future model improvement, thereby\nnarrowing the gap between machine and human linguistic capabilities and\nsupporting researchers in developing Traditional Chinese LLMs. Our dataset,\nalong with the benchmark source code, is accessible at\nhuggingface.co/datasets/ikala/tmmluplus.\n", "rewritten_text": "\\boxed{\nWe introduce TMMLU+, a novel benchmark for evaluating Traditional Chinese language understanding. This multi-choice question-answering dataset covers 66 subjects across elementary to professional levels, making it six times larger and more balanced than its predecessor, TMMLU. We test 26 open-source Chinese large language models (LLMs) with parameters ranging from 1.8B to 72B on TMMLU+, as well as closed-source models. Our results show that (1) Traditional Chinese models still lag behind their Simplified Chinese counterparts, highlighting the need for more focused advancements. (2) Current LLMs still fall short of human performance, indicating a need for future research in social science and humanities subjects. (3) Among various tokenization compression metrics, only the fertility score shows strong correlations with our benchmark results. We believe TMMLU+ will help identify areas for future model improvement, bridging the gap between machine and human linguistic capabilities and supporting the development of Traditional Chinese LLMs. The dataset and benchmark source code are available at huggingface.co/datasets/ikala/tmmluplus.\n}"}, "2005.04621": {"original_text": "  Deep convolutional neural networks generally perform well in underwater\nobject recognition tasks on both optical and sonar images. Many such methods\nrequire hundreds, if not thousands, of images per class to generalize well to\nunseen examples. However, obtaining and labeling sufficiently large volumes of\ndata can be relatively costly and time-consuming, especially when observing\nrare objects or performing real-time operations. Few-Shot Learning (FSL)\nefforts have produced many promising methods to deal with low data\navailability. However, little attention has been given in the underwater\ndomain, where the style of images poses additional challenges for object\nrecognition algorithms. To the best of our knowledge, this is the first paper\nto evaluate and compare several supervised and semi-supervised Few-Shot\nLearning (FSL) methods using underwater optical and side-scan sonar imagery.\nOur results show that FSL methods offer a significant advantage over the\ntraditional transfer learning methods that fine-tune pre-trained models. We\nhope that our work will help apply FSL to autonomous underwater systems and\nexpand their learning capabilities.\n", "rewritten_text": "\\boxed{Deep neural networks excel in underwater object recognition tasks, but require large amounts of labeled data, which can be costly and time-consuming to obtain. Few-Shot Learning (FSL) methods have shown promise in addressing low data availability, but have been largely unexplored in the underwater domain, where image styles pose unique challenges. This paper evaluates and compares various supervised and semi-supervised FSL methods using underwater optical and sonar images, demonstrating their superiority over traditional transfer learning approaches. The goal is to enable autonomous underwater systems to learn from limited data and expand their capabilities.}"}, "1712.01821": {"original_text": "  Factored neural machine translation (FNMT) is founded on the idea of using\nthe morphological and grammatical decomposition of the words (factors) at the\noutput side of the neural network. This architecture addresses two well-known\nproblems occurring in MT, namely the size of target language vocabulary and the\nnumber of unknown tokens produced in the translation. FNMT system is designed\nto manage larger vocabulary and reduce the training time (for systems with\nequivalent target language vocabulary size). Moreover, we can produce\ngrammatically correct words that are not part of the vocabulary. FNMT model is\nevaluated on IWSLT'15 English to French task and compared to the baseline\nword-based and BPE-based NMT systems. Promising qualitative and quantitative\nresults (in terms of BLEU and METEOR) are reported.\n", "rewritten_text": "\\boxed{Factored neural machine translation (FNMT) is a novel approach that leverages the decomposition of words into morphological and grammatical components at the output side of the neural network. This architecture tackles two major challenges in machine translation: the large target language vocabulary and the occurrence of unknown tokens in the translation. By design, FNMT can handle larger vocabularies and reduce training time, while also generating grammatically correct words not present in the vocabulary. The FNMT model is evaluated on the IWSLT'15 English-to-French task and compared to baseline word-based and BPE-based NMT systems, yielding promising results in terms of BLEU and METEOR scores.}"}, "2206.01988": {"original_text": "  Automatic generation of ophthalmic reports using data-driven neural networks\nhas great potential in clinical practice. When writing a report,\nophthalmologists make inferences with prior clinical knowledge. This knowledge\nhas been neglected in prior medical report generation methods. To endow models\nwith the capability of incorporating expert knowledge, we propose a Cross-modal\nclinical Graph Transformer (CGT) for ophthalmic report generation (ORG), in\nwhich clinical relation triples are injected into the visual features as prior\nknowledge to drive the decoding procedure. However, two major common Knowledge\nNoise (KN) issues may affect models' effectiveness. 1) Existing general\nbiomedical knowledge bases such as the UMLS may not align meaningfully to the\nspecific context and language of the report, limiting their utility for\nknowledge injection. 2) Incorporating too much knowledge may divert the visual\nfeatures from their correct meaning. To overcome these limitations, we design\nan automatic information extraction scheme based on natural language processing\nto obtain clinical entities and relations directly from in-domain training\nreports. Given a set of ophthalmic images, our CGT first restores a sub-graph\nfrom the clinical graph and injects the restored triples into visual features.\nThen visible matrix is employed during the encoding procedure to limit the\nimpact of knowledge. Finally, reports are predicted by the encoded cross-modal\nfeatures via a Transformer decoder. Extensive experiments on the large-scale\nFFA-IR benchmark demonstrate that the proposed CGT is able to outperform\nprevious benchmark methods and achieve state-of-the-art performances.\n", "rewritten_text": "\\boxed{\nThe automatic generation of ophthalmic reports using neural networks has great potential in clinical practice. However, previous methods have neglected the prior clinical knowledge that ophthalmologists use when writing reports. To address this, we propose a Cross-modal Clinical Graph Transformer (CGT) that incorporates expert knowledge into the report generation process. This is achieved by injecting clinical relation triples into visual features, which are obtained through an automatic information extraction scheme based on natural language processing. The CGT model first restores a sub-graph from the clinical graph and injects the restored triples into visual features, then uses a visible matrix to limit the impact of knowledge during encoding. Finally, the model predicts reports using the encoded cross-modal features via a Transformer decoder. Our experiments on the FFA-IR benchmark demonstrate that the proposed CGT outperforms previous methods and achieves state-of-the-art performance."}, "1602.01125": {"original_text": "  We propose a fully automatic method for fitting a 3D morphable model to\nsingle face images in arbitrary pose and lighting. Our approach relies on\ngeometric features (edges and landmarks) and, inspired by the iterated closest\npoint algorithm, is based on computing hard correspondences between model\nvertices and edge pixels. We demonstrate that this is superior to previous work\nthat uses soft correspondences to form an edge-derived cost surface that is\nminimised by nonlinear optimisation.\n", "rewritten_text": "\\boxed{We introduce a fully automated approach to fit a 3D morphable model to single face images with varying pose and lighting conditions. Our method leverages geometric features, such as edges and landmarks, and builds upon the iterated closest point algorithm to establish precise correspondences between model vertices and edge pixels. This approach outperforms existing methods that rely on soft correspondences and edge-derived cost surfaces optimized through nonlinear techniques.}"}, "2405.11862": {"original_text": "  Table structure recognition (TSR) aims to parse the inherent structure of a\ntable from its input image. The `\"split-and-merge\" paradigm is a pivotal\napproach to parse table structure, where the table separation line detection is\ncrucial. However, challenges such as wireless and deformed tables make it\ndemanding. In this paper, we adhere to the \"split-and-merge\" paradigm and\npropose SEMv3 (SEM: Split, Embed and Merge), a method that is both fast and\nrobust for detecting table separation lines. During the split stage, we\nintroduce a Keypoint Offset Regression (KOR) module, which effectively detects\ntable separation lines by directly regressing the offset of each line relative\nto its keypoint proposals. Moreover, in the merge stage, we define a series of\nmerge actions to efficiently describe the table structure based on table grids.\nExtensive ablation studies demonstrate that our proposed KOR module can detect\ntable separation lines quickly and accurately. Furthermore, on public datasets\n(e.g. WTW, ICDAR-2019 cTDaR Historical and iFLYTAB), SEMv3 achieves\nstate-of-the-art (SOTA) performance. The code is available at\nhttps://github.com/Chunchunwumu/SEMv3.\n", "rewritten_text": "\\boxed{\nTable structure recognition (TSR) involves extracting the underlying structure of a table from an image. A key approach to TSR is the \"split-and-merge\" method, which relies on accurately detecting table separation lines. However, this task is challenging due to variations in table formats and deformations. To address this, we propose SEMv3, a fast and robust method that builds upon the \"split-and-merge\" paradigm. Our approach introduces a Keypoint Offset Regression (KOR) module to efficiently detect table separation lines and a merge stage to describe the table structure using grid-based representations. Experimental results demonstrate the effectiveness of our KOR module and SEMv3's state-of-the-art performance on public datasets, including WTW, ICDAR-2019 cTDaR Historical, and iFLYTAB. The implementation is available at https://github.com/Chunchunwumu/SEMv3.\n}"}, "1505.04424": {"original_text": "  In this work, we propose a novel microaneurysm (MA) detection for early\ndiabetic retinopathy screening using color fundus images. Since MA usually the\nfirst lesions to appear as an indicator of diabetic retinopathy, accurate\ndetection of MA is necessary for treatment. Each pixel of the image is\nclassified as either MA or non-MA using a deep neural network with dropout\ntraining procedure using maxout activation function. No preprocessing step or\nmanual feature extraction is required. Substantial improvements over standard\nMA detection method based on the pipeline of preprocessing, feature extraction,\nclassification followed by post processing is achieved. The presented method is\nevaluated in publicly available Retinopathy Online Challenge (ROC) and\nDiaretdb1v2 database and achieved state-of-the-art accuracy.\n", "rewritten_text": "\\boxed{This study introduces a novel approach to detecting microaneurysms (MAs) in color fundus images for early diabetic retinopathy screening. As MAs are typically the first signs of diabetic retinopathy, accurate detection is crucial for effective treatment. Our method utilizes a deep neural network with a maxout activation function and dropout training procedure to classify each image pixel as either MA or non-MA, eliminating the need for preprocessing or manual feature extraction. Compared to traditional MA detection methods, our approach achieves significant improvements. We evaluated our method using the publicly available Retinopathy Online Challenge (ROC) and Diaretdb1v2 databases, achieving state-of-the-art accuracy.}"}, "2206.13156": {"original_text": "  Transformer has been widely used in histopathology whole slide image (WSI)\nclassification for the purpose of tumor grading, prognosis analysis, etc.\nHowever, the design of token-wise self-attention and positional embedding\nstrategy in the common Transformer limits the effectiveness and efficiency in\nthe application to gigapixel histopathology images. In this paper, we propose a\nkernel attention Transformer (KAT) for histopathology WSI classification. The\ninformation transmission of the tokens is achieved by cross-attention between\nthe tokens and a set of kernels related to a set of positional anchors on the\nWSI. Compared to the common Transformer structure, the proposed KAT can better\ndescribe the hierarchical context information of the local regions of the WSI\nand meanwhile maintains a lower computational complexity. The proposed method\nwas evaluated on a gastric dataset with 2040 WSIs and an endometrial dataset\nwith 2560 WSIs, and was compared with 6 state-of-the-art methods. The\nexperimental results have demonstrated the proposed KAT is effective and\nefficient in the task of histopathology WSI classification and is superior to\nthe state-of-the-art methods. The code is available at\nhttps://github.com/zhengyushan/kat.\n", "rewritten_text": "\\boxed{\nThe traditional Transformer architecture has limitations when applied to gigapixel histopathology whole slide images (WSIs) for tasks like tumor grading and prognosis analysis. To address this, we introduce the kernel attention Transformer (KAT), which leverages cross-attention between tokens and kernels related to positional anchors on the WSI to transmit information. This approach better captures hierarchical context information in local regions of the WSI while reducing computational complexity. Our experiments on gastric and endometrial datasets demonstrate the effectiveness and efficiency of KAT, outperforming six state-of-the-art methods. The implementation is available at https://github.com/zhengyushan/kat.\n}"}, "1610.00291": {"original_text": "  We present a novel method for constructing Variational Autoencoder (VAE).\nInstead of using pixel-by-pixel loss, we enforce deep feature consistency\nbetween the input and the output of a VAE, which ensures the VAE's output to\npreserve the spatial correlation characteristics of the input, thus leading the\noutput to have a more natural visual appearance and better perceptual quality.\nBased on recent deep learning works such as style transfer, we employ a\npre-trained deep convolutional neural network (CNN) and use its hidden features\nto define a feature perceptual loss for VAE training. Evaluated on the CelebA\nface dataset, we show that our model produces better results than other methods\nin the literature. We also show that our method can produce latent vectors that\ncan capture the semantic information of face expressions and can be used to\nachieve state-of-the-art performance in facial attribute prediction.\n", "rewritten_text": "\\boxed{We introduce a new approach to building Variational Autoencoders (VAEs) that focuses on preserving spatial correlations and natural visual appearance. By leveraging pre-trained deep convolutional neural networks (CNNs) and their hidden features, we define a feature perceptual loss that enhances VAE training. Our method outperforms existing techniques on the CelebA face dataset and generates latent vectors that capture semantic information about facial expressions, achieving state-of-the-art results in facial attribute prediction.}"}, "2206.00227": {"original_text": "  A data augmentation module is utilized in contrastive learning to transform\nthe given data example into two views, which is considered essential and\nirreplaceable. However, the predetermined composition of multiple data\naugmentations brings two drawbacks. First, the artificial choice of\naugmentation types brings specific representational invariances to the model,\nwhich have different degrees of positive and negative effects on different\ndownstream tasks. Treating each type of augmentation equally during training\nmakes the model learn non-optimal representations for various downstream tasks\nand limits the flexibility to choose augmentation types beforehand. Second, the\nstrong data augmentations used in classic contrastive learning methods may\nbring too much invariance in some cases, and fine-grained information that is\nessential to some downstream tasks may be lost. This paper proposes a general\nmethod to alleviate these two problems by considering where and what to\ncontrast in a general contrastive learning framework. We first propose to learn\ndifferent augmentation invariances at different depths of the model according\nto the importance of each data augmentation instead of learning\nrepresentational invariances evenly in the backbone. We then propose to expand\nthe contrast content with augmentation embeddings to reduce the misleading\neffects of strong data augmentations. Experiments based on several baseline\nmethods demonstrate that we learn better representations for various benchmarks\non classification, detection, and segmentation downstream tasks.\n", "rewritten_text": "\\boxed{\nIn contrastive learning, a data augmentation module is crucial for generating two views of a data example. However, the fixed combination of multiple augmentations has two limitations. Firstly, the arbitrary selection of augmentation types can introduce biases into the model, affecting its performance on different downstream tasks. Secondly, strong augmentations can lead to the loss of fine-grained information essential for some tasks. To address these issues, this paper proposes a novel approach that adapts the contrastive learning framework to learn augmentation invariances at different model depths and incorporates augmentation embeddings to reduce the negative effects of strong augmentations. Experimental results on various benchmarks demonstrate the effectiveness of this approach in learning better representations for classification, detection, and segmentation tasks.\n}"}, "1711.0719": {"original_text": "  We present a stochastic first-order optimization algorithm, named BCSC, that\nadds a cyclic constraint to stochastic block-coordinate descent. It uses\ndifferent subsets of the data to update different subsets of the parameters,\nthus limiting the detrimental effect of outliers in the training set. Empirical\ntests in benchmark datasets show that our algorithm outperforms\nstate-of-the-art optimization methods in both accuracy as well as convergence\nspeed. The improvements are consistent across different architectures, and can\nbe combined with other training techniques and regularization methods.\n", "rewritten_text": "\\boxed{We introduce BCSC, a novel stochastic optimization algorithm that builds upon stochastic block-coordinate descent by incorporating a cyclic constraint. By updating different parameter subsets using distinct data subsets, BCSC mitigates the negative impact of outliers in the training data. Our empirical results on benchmark datasets demonstrate that BCSC surpasses existing state-of-the-art optimization methods in terms of both accuracy and convergence speed, with consistent improvements observed across various architectures and compatible with other training techniques and regularization methods.}"}, "2008.11872": {"original_text": "  In Viticulture, visual inspection of the plant is a necessary task for\nmeasuring relevant variables. In many cases, these visual inspections are\nsusceptible to automation through computer vision methods. Bud detection is one\nsuch visual task, central for the measurement of important variables such as:\nmeasurement of bud sunlight exposure, autonomous pruning, bud counting,\ntype-of-bud classification, bud geometric characterization, internode length,\nbud area, and bud development stage, among others. This paper presents a\ncomputer method for grapevine bud detection based on a Fully Convolutional\nNetworks MobileNet architecture (FCN-MN). To validate its performance, this\narchitecture was compared in the detection task with a strong method for bud\ndetection, Scanning Windows (SW) based on a patch classifier, showing\nimprovements over three aspects of detection: segmentation, correspondence\nidentification and localization. The best version of FCN-MN showed a detection\nF1-measure of $88.6\\%$ (for true positives defined as detected components whose\nintersection-over-union with the true bud is above $0.5$), and false positives\nthat are small and near the true bud. Splits -- false positives overlapping the\ntrue bud -- showed a mean segmentation precision of $89.3\\% (21.7)$, while\nfalse alarms -- false positives not overlapping the true bud -- showed a mean\npixel area of only $8\\%$ the area of a true bud, and a distance (between mass\ncenters) of $1.1$ true bud diameters. The paper concludes by discussing how\nthese results for FCN-MN would produce sufficiently accurate measurements of\nbud variables such as bud number, bud area, and internode length, suggesting a\ngood performance in a practical setup.\n", "rewritten_text": "\\boxed{\nVisual inspection of grapevines is crucial for measuring various variables, but it can be automated using computer vision techniques. One important task is bud detection, which is essential for measuring bud sunlight exposure, autonomous pruning, and other variables. This paper proposes a computer method for grapevine bud detection using a Fully Convolutional Networks MobileNet architecture (FCN-MN). The results show that FCN-MN outperforms a strong method, Scanning Windows (SW), in three aspects of detection: segmentation, correspondence identification, and localization. The best version of FCN-MN achieves a detection F1-measure of 88.6% and produces accurate measurements of bud variables, making it suitable for practical applications."}, "1703.10798": {"original_text": "  We present a system for converting a fully panoramic ($360^\\circ$) video into\na normal field-of-view (NFOV) hyperlapse for an optimal viewing experience. Our\nsystem exploits visual saliency and semantics to non-uniformly sample in space\nand time for generating hyperlapses. In addition, users can optionally choose\nobjects of interest for customizing the hyperlapses. We first stabilize an\ninput $360^\\circ$ video by smoothing the rotation between adjacent frames and\nthen compute regions of interest and saliency scores. An initial hyperlapse is\ngenerated by optimizing the saliency and motion smoothness followed by the\nsaliency-aware frame selection. We further smooth the result using an efficient\n2D video stabilization approach that adaptively selects the motion model to\ngenerate the final hyperlapse. We validate the design of our system by showing\nresults for a variety of scenes and comparing against the state-of-the-art\nmethod through a user study.\n", "rewritten_text": "\\boxed{We introduce a system that transforms 360-degree videos into normal field-of-view hyperlapses, providing an optimal viewing experience. Our approach leverages visual saliency and semantics to selectively sample frames in space and time, allowing for customizable hyperlapses based on user-selected objects of interest. The system stabilizes the input video, identifies regions of interest, and computes saliency scores. An initial hyperlapse is generated by balancing saliency and motion smoothness, followed by saliency-aware frame selection. The final hyperlapse is refined using an efficient 2D video stabilization method that adaptively chooses the motion model. We demonstrate the effectiveness of our system through a user study, showcasing results for various scenes and comparing them to the current state-of-the-art method.}"}, "2001.03182": {"original_text": "  Unsupervised domain adaptation algorithms aim to transfer the knowledge\nlearned from one domain to another (e.g., synthetic to real images). The\nadapted representations often do not capture pixel-level domain shifts that are\ncrucial for dense prediction tasks (e.g., semantic segmentation). In this\npaper, we present a novel pixel-wise adversarial domain adaptation algorithm.\nBy leveraging image-to-image translation methods for data augmentation, our key\ninsight is that while the translated images between domains may differ in\nstyles, their predictions for the task should be consistent. We exploit this\nproperty and introduce a cross-domain consistency loss that enforces our\nadapted model to produce consistent predictions. Through extensive experimental\nresults, we show that our method compares favorably against the\nstate-of-the-art on a wide variety of unsupervised domain adaptation tasks.\n", "rewritten_text": "\\boxed{Unsupervised domain adaptation methods strive to transfer knowledge from one domain to another, but often fail to capture pixel-level differences crucial for tasks like semantic segmentation. This paper proposes a novel pixel-wise adversarial domain adaptation algorithm that leverages image-to-image translation for data augmentation. The key idea is that translated images between domains should have consistent task predictions, despite differing styles. A cross-domain consistency loss is introduced to enforce this consistency, and experimental results show that this method outperforms state-of-the-art approaches in various unsupervised domain adaptation tasks.}"}, "1802.08636": {"original_text": "  Single document summarization is the task of producing a shorter version of a\ndocument while preserving its principal information content. In this paper we\nconceptualize extractive summarization as a sentence ranking task and propose a\nnovel training algorithm which globally optimizes the ROUGE evaluation metric\nthrough a reinforcement learning objective. We use our algorithm to train a\nneural summarization model on the CNN and DailyMail datasets and demonstrate\nexperimentally that it outperforms state-of-the-art extractive and abstractive\nsystems when evaluated automatically and by humans.\n", "rewritten_text": "\\boxed{This paper presents a new approach to single document summarization, which involves condensing a document into a shorter form while retaining its main information. The proposed method treats extractive summarization as a sentence ranking task and introduces a novel training algorithm that optimizes the ROUGE evaluation metric using reinforcement learning. Experimental results on the CNN and DailyMail datasets show that the trained neural summarization model surpasses existing state-of-the-art extractive and abstractive systems, as evaluated by both automatic metrics and human assessors.}"}, "2406.11192": {"original_text": "  Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets faces issues due to inconsistent entity definitions and\nredundant data, limiting LLMs to dataset-specific learning and hindering\nout-of-domain generalization. To address this, we present B2NERD, a cohesive\nand efficient dataset for Open NER, normalized from 54 existing English or\nChinese datasets using a two-step approach. First, we detect inconsistent\nentity definitions across datasets and clarify them by distinguishable label\nnames to construct a universal taxonomy of 400+ entity types. Second, we\naddress redundancy using a data pruning strategy that selects fewer samples\nwith greater category and semantic diversity. Comprehensive evaluation shows\nthat B2NERD significantly improves LLMs' generalization on Open NER. Our B2NER\nmodels, trained on B2NERD, outperform GPT-4 by 6.8-12.0 F1 points and surpass\nprevious methods in 3 out-of-domain benchmarks across 15 datasets and 6\nlanguages.\n", "rewritten_text": "\\boxed{Large Language Models (LLMs) struggle with Open Named Entity Recognition (NER) due to inconsistent entity definitions and redundant data in existing datasets. To overcome this, we introduce B2NERD, a unified and efficient dataset for Open NER, created by normalizing 54 English and Chinese datasets using a two-step approach. This approach involves clarifying inconsistent entity definitions and removing redundant data to construct a universal taxonomy of over 400 entity types. Our evaluation shows that B2NERD significantly improves LLMs' generalization on Open NER, with our B2NER models outperforming GPT-4 by 6.8-12.0 F1 points and surpassing previous methods in 3 out-of-domain benchmarks across 15 datasets and 6 languages.}"}, "2411.01465": {"original_text": "  Despite the outstanding performance in many individual tasks, deep neural\nnetworks suffer from catastrophic forgetting when learning from continuous data\nstreams in real-world scenarios. Current Non-Exemplar Class-Incremental\nLearning (NECIL) methods mitigate forgetting by storing a single prototype per\nclass, which serves to inject previous information when sequentially learning\nnew classes. However, these stored prototypes or their augmented variants often\nfail to simultaneously capture spatial distribution diversity and precision\nneeded for representing old classes. Moreover, as the model acquires new\nknowledge, these prototypes gradually become outdated, making them less\neffective. To overcome these limitations, we propose a more efficient NECIL\nmethod that replaces prototypes with synthesized retrospective features for old\nclasses. Specifically, we model each old class's feature space using a\nmultivariate Gaussian distribution and generate deep representations by\nsampling from high-likelihood regions. Additionally, we introduce a\nsimilarity-based feature compensation mechanism that integrates generated old\nclass features with similar new class features to synthesize robust\nretrospective representations. These retrospective features are then\nincorporated into our incremental learning framework to preserve the decision\nboundaries of previous classes while learning new ones. Extensive experiments\non CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate that our method\nsignificantly improves the efficiency of non-exemplar class-incremental\nlearning and achieves state-of-the-art performance.\n", "rewritten_text": "\\boxed{\nDeep neural networks struggle with catastrophic forgetting when learning from continuous data streams in real-world scenarios, despite their excellent performance in individual tasks. Current methods to mitigate forgetting, known as Non-Exemplar Class-Incremental Learning (NECIL), store a single prototype per class to inject previous information when learning new classes. However, these prototypes often fail to capture the diversity and precision needed to represent old classes, and become outdated as the model acquires new knowledge. To address these limitations, we propose a more efficient NECIL method that replaces prototypes with synthesized retrospective features for old classes. We model each old class's feature space using a multivariate Gaussian distribution and generate deep representations by sampling from high-likelihood regions. Additionally, we introduce a similarity-based feature compensation mechanism to integrate generated old class features with similar new class features, synthesizing robust retrospective representations. Our method incorporates these retrospective features into the incremental learning framework to preserve decision boundaries of previous classes while learning new ones, achieving state-of-the-art performance in extensive experiments on CIFAR-100, TinyImageNet, and ImageNet-Subset.\n}"}, "2404.03654": {"original_text": "  NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel\nview synthesis and 3D reconstruction, but its performance is sensitive to input\nimage quality, which struggles to achieve high-fidelity rendering when provided\nwith low-quality sparse input viewpoints. Previous methods for NeRF restoration\nare tailored for specific degradation type, ignoring the generality of\nrestoration. To overcome this limitation, we propose a generic radiance fields\nrestoration pipeline, named RaFE, which applies to various types of\ndegradations, such as low resolution, blurriness, noise, compression artifacts,\nor their combinations. Our approach leverages the success of off-the-shelf 2D\nrestoration methods to recover the multi-view images individually. Instead of\nreconstructing a blurred NeRF by averaging inconsistencies, we introduce a\nnovel approach using Generative Adversarial Networks (GANs) for NeRF generation\nto better accommodate the geometric and appearance inconsistencies present in\nthe multi-view images. Specifically, we adopt a two-level tri-plane\narchitecture, where the coarse level remains fixed to represent the low-quality\nNeRF, and a fine-level residual tri-plane to be added to the coarse level is\nmodeled as a distribution with GAN to capture potential variations in\nrestoration. We validate RaFE on both synthetic and real cases for various\nrestoration tasks, demonstrating superior performance in both quantitative and\nqualitative evaluations, surpassing other 3D restoration methods specific to\nsingle task. Please see our project website\nhttps://zkaiwu.github.io/RaFE-Project/.\n", "rewritten_text": "\\boxed{\nNeRF, a powerful tool for novel view synthesis and 3D reconstruction, is hindered by its sensitivity to input image quality. Existing methods for NeRF restoration are limited to specific types of degradation, neglecting the need for a more general approach. To address this, we introduce RaFE, a generic radiance fields restoration pipeline that can handle various types of degradations, including low resolution, blurriness, noise, compression artifacts, and their combinations. Our approach leverages 2D restoration methods to recover individual multi-view images and uses Generative Adversarial Networks (GANs) to generate a NeRF that accommodates geometric and appearance inconsistencies. We employ a two-level tri-plane architecture, where a coarse level represents the low-quality NeRF and a fine-level residual tri-plane captures potential variations in restoration. Our experiments demonstrate RaFE's superior performance in both quantitative and qualitative evaluations, outperforming other 3D restoration methods specific to single tasks. For more information, please visit our project website: https://zkaiwu.github.io/RaFE-Project/.\n}"}, "2107.04217": {"original_text": "  This paper studies joint models for selecting correct answer sentences among\nthe top $k$ provided by answer sentence selection (AS2) modules, which are core\ncomponents of retrieval-based Question Answering (QA) systems. Our work shows\nthat a critical step to effectively exploit an answer set regards modeling the\ninterrelated information between pair of answers. For this purpose, we build a\nthree-way multi-classifier, which decides if an answer supports, refutes, or is\nneutral with respect to another one. More specifically, our neural architecture\nintegrates a state-of-the-art AS2 model with the multi-classifier, and a joint\nlayer connecting all components. We tested our models on WikiQA, TREC-QA, and a\nreal-world dataset. The results show that our models obtain the new state of\nthe art in AS2.\n", "rewritten_text": "\\boxed{This research explores the development of joint models that can effectively select the correct answer sentences from the top k options provided by answer sentence selection (AS2) modules, a crucial component of retrieval-based Question Answering (QA) systems. The key to leveraging an answer set lies in modeling the interdependencies between pairs of answers. To achieve this, we design a three-way multi-classifier that determines whether an answer supports, refutes, or is neutral towards another answer. Our neural architecture combines a state-of-the-art AS2 model with the multi-classifier and a joint layer that connects all components. Experimental results on WikiQA, TREC-QA, and a real-world dataset demonstrate that our models achieve a new state of the art in AS2.}"}, "2407.05271": {"original_text": "  Name-based gender prediction has traditionally categorized individuals as\neither female or male based on their names, using a binary classification\nsystem. That binary approach can be problematic in the cases of gender-neutral\nnames that do not align with any one gender, among other reasons. Relying\nsolely on binary gender categories without recognizing gender-neutral names can\nreduce the inclusiveness of gender prediction tasks. We introduce an additional\ngender category, i.e., \"neutral\", to study and address potential gender biases\nin Large Language Models (LLMs). We evaluate the performance of several\nfoundational and large language models in predicting gender based on first\nnames only. Additionally, we investigate the impact of adding birth years to\nenhance the accuracy of gender prediction, accounting for shifting associations\nbetween names and genders over time. Our findings indicate that most LLMs\nidentify male and female names with high accuracy (over 80%) but struggle with\ngender-neutral names (under 40%), and the accuracy of gender prediction is\nhigher for English-based first names than non-English names. The experimental\nresults show that incorporating the birth year does not improve the overall\naccuracy of gender prediction, especially for names with evolving gender\nassociations. We recommend using caution when applying LLMs for gender\nidentification in downstream tasks, particularly when dealing with non-binary\ngender labels.\n", "rewritten_text": "\\boxed{\nTraditional name-based gender prediction uses a binary system, categorizing individuals as either male or female. However, this approach has limitations, particularly with gender-neutral names that don't fit into either category. To address this, we introduce a \"neutral\" gender category to study and mitigate potential biases in Large Language Models (LLMs). We evaluate the performance of various LLMs in predicting gender from first names and explore the impact of adding birth years to improve accuracy. Our results show that LLMs accurately identify male and female names (over 80%) but struggle with gender-neutral names (under 40%). We also find that incorporating birth years does not significantly improve accuracy, especially for names with changing gender associations over time. Therefore, we recommend exercising caution when using LLMs for gender identification, particularly when dealing with non-binary gender labels.\n}"}, "2306.09379": {"original_text": "  In this technical report, we briefly introduce the solution of our team\nVIELab-HUST for coded target restoration through atmospheric turbulence in CVPR\n2023 UG$^2$+ Track 2.2. In this task, we propose an efficient multi-stage\nframework to restore a high quality image from distorted frames. Specifically,\neach distorted frame is initially aligned using image registration to suppress\ngeometric distortion. We subsequently select the sharpest set of registered\nframes by employing a frame selection approach based on image sharpness, and\naverage them to produce an image that is largely free of geometric distortion,\nalbeit with blurriness. A learning-based deblurring method is then applied to\nremove the residual blur in the averaged image. Finally, post-processing\ntechniques are utilized to further enhance the quality of the output image. Our\nframework is capable of handling different kinds of coded target dataset\nprovided in the final testing phase, and ranked 1st on the final leaderboard.\nOur code will be available at https://github.com/xsqhust/Turbulence_Removal.\n", "rewritten_text": "\\boxed{\nThis technical report outlines our team VIELab-HUST's solution for the CVPR 2023 UG$^2$+ Track 2.2 challenge, which involves restoring high-quality images from distorted frames affected by atmospheric turbulence. Our proposed framework consists of four stages: (1) image registration to correct geometric distortion, (2) frame selection based on sharpness to produce a blur-free image, (3) learning-based deblurring to remove residual blur, and (4) post-processing to enhance the output image. Our approach is versatile and can handle various coded target datasets, ultimately ranking 1st on the final leaderboard. The implementation code is available at https://github.com/xsqhust/Turbulence_Removal.\n}"}, "2108.03502": {"original_text": "  Automatic summarization techniques aim to shorten and generalize information\ngiven in the text while preserving its core message and the most relevant\nideas. This task can be approached and treated with a variety of methods,\nhowever, not many attempts have been made to produce solutions specifically for\nthe Russian language despite existing localizations of the state-of-the-art\nmodels. In this paper, we aim to showcase ruGPT3 ability to summarize texts,\nfine-tuning it on the corpora of Russian news with their corresponding\nhuman-generated summaries. Additionally, we employ hyperparameter tuning so\nthat the model's output becomes less random and more tied to the original text.\nWe evaluate the resulting texts with a set of metrics, showing that our\nsolution can surpass the state-of-the-art model's performance without\nadditional changes in architecture or loss function. Despite being able to\nproduce sensible summaries, our model still suffers from a number of flaws,\nnamely, it is prone to altering Named Entities present in the original text\n(such as surnames, places, dates), deviating from facts stated in the given\ndocument, and repeating the information in the summary.\n", "rewritten_text": "\\boxed{The goal of automatic summarization is to condense and generalize text while preserving its essential message and key ideas. Despite the existence of advanced models, few attempts have been made to develop solutions specifically for the Russian language. This paper explores the ability of ruGPT3 to summarize Russian news texts, fine-tuned with human-generated summaries, and optimized through hyperparameter tuning. The results show that our approach outperforms state-of-the-art models without architectural or loss function changes. However, the model still has limitations, including altering named entities, deviating from facts, and repeating information in the summary.}"}, "1901.01574": {"original_text": "  This work systematically analyzes the smoothing effect of vocabulary\nreduction for phrase translation models. We extensively compare various\nword-level vocabularies to show that the performance of smoothing is not\nsignificantly affected by the choice of vocabulary. This result provides\nempirical evidence that the standard phrase translation model is extremely\nsparse. Our experiments also reveal that vocabulary reduction is more effective\nfor smoothing large-scale phrase tables.\n", "rewritten_text": "\\boxed{This study examines the impact of reducing vocabulary on the performance of phrase translation models, finding that the choice of vocabulary has little effect on smoothing results. The findings suggest that traditional phrase translation models are highly sparse, and that vocabulary reduction is particularly effective in improving large-scale phrase tables.}"}, "2210.16579": {"original_text": "  Generating videos is a complex task that is accomplished by generating a set\nof temporally coherent images frame-by-frame. This limits the expressivity of\nvideos to only image-based operations on the individual video frames needing\nnetwork designs to obtain temporally coherent trajectories in the underlying\nimage space. We propose INR-V, a video representation network that learns a\ncontinuous space for video-based generative tasks. INR-V parameterizes videos\nusing implicit neural representations (INRs), a multi-layered perceptron that\npredicts an RGB value for each input pixel location of the video. The INR is\npredicted using a meta-network which is a hypernetwork trained on neural\nrepresentations of multiple video instances. Later, the meta-network can be\nsampled to generate diverse novel videos enabling many downstream video-based\ngenerative tasks. Interestingly, we find that conditional regularization and\nprogressive weight initialization play a crucial role in obtaining INR-V. The\nrepresentation space learned by INR-V is more expressive than an image space\nshowcasing many interesting properties not possible with the existing works.\nFor instance, INR-V can smoothly interpolate intermediate videos between known\nvideo instances (such as intermediate identities, expressions, and poses in\nface videos). It can also in-paint missing portions in videos to recover\ntemporally coherent full videos. In this work, we evaluate the space learned by\nINR-V on diverse generative tasks such as video interpolation, novel video\ngeneration, video inversion, and video inpainting against the existing\nbaselines. INR-V significantly outperforms the baselines on several of these\ndemonstrated tasks, clearly showcasing the potential of the proposed\nrepresentation space.\n", "rewritten_text": "\\boxed{\nGenerating videos is a complex task that involves creating a sequence of coherent images. Current methods are limited to image-based operations, which restricts their expressivity. We propose INR-V, a video representation network that learns a continuous space for video-based generative tasks. INR-V uses implicit neural representations (INRs) to predict RGB values for each pixel in a video, and a meta-network to generate diverse novel videos. Our approach enables various downstream tasks, such as smoothly interpolating between video instances, in-painting missing portions, and generating novel videos. We evaluate INR-V on several tasks and show that it outperforms existing baselines, demonstrating the potential of our proposed representation space.\n}"}, "2101.01843": {"original_text": "  Autonomous driving applications use two types of sensor systems to identify\nvehicles - depth sensing LiDAR and radiance sensing cameras. We compare the\nperformance (average precision) of a ResNet for vehicle detection in complex,\ndaytime, driving scenes when the input is a depth map (D = d(x,y)), a radiance\nimage (L = r(x,y)), or both [D,L]. (1) When the spatial sampling resolution of\nthe depth map and radiance image are equal to typical camera resolutions, a\nResNet detects vehicles at higher average precision from depth than radiance.\n(2) As the spatial sampling of the depth map declines to the range of current\nLiDAR devices, the ResNet average precision is higher for radiance than depth.\n(3) For a hybrid system that combines a depth map and radiance image, the\naverage precision is higher than using depth or radiance alone. We established\nthese observations in simulation and then confirmed them using realworld data.\nThe advantage of combining depth and radiance can be explained by noting that\nthe two type of information have complementary weaknesses. The radiance data\nare limited by dynamic range and motion blur. The LiDAR data have relatively\nlow spatial resolution. The ResNet combines the two data sources effectively to\nimprove overall vehicle detection.\n", "rewritten_text": "\\boxed{Autonomous driving systems utilize two types of sensors, LiDAR for depth sensing and cameras for radiance sensing, to detect vehicles. A comparison of the performance of a ResNet in detecting vehicles in complex daytime scenes reveals that: (1) when the spatial resolution of both sensors is similar to typical camera resolutions, the ResNet detects vehicles more accurately using depth maps than radiance images; (2) however, when the spatial resolution of the depth map decreases to the level of current LiDAR devices, the ResNet performs better with radiance images; and (3) a hybrid system combining both depth maps and radiance images achieves higher accuracy than using either one alone. These findings, initially observed in simulations, were confirmed using real-world data. The advantage of combining depth and radiance data lies in their complementary weaknesses, with radiance data limited by dynamic range and motion blur, and LiDAR data by low spatial resolution, which the ResNet effectively integrates to improve overall vehicle detection.}"}, "1511.03328": {"original_text": "  Deep convolutional neural networks (CNNs) are the backbone of state-of-art\nsemantic image segmentation systems. Recent work has shown that complementing\nCNNs with fully-connected conditional random fields (CRFs) can significantly\nenhance their object localization accuracy, yet dense CRF inference is\ncomputationally expensive. We propose replacing the fully-connected CRF with\ndomain transform (DT), a modern edge-preserving filtering method in which the\namount of smoothing is controlled by a reference edge map. Domain transform\nfiltering is several times faster than dense CRF inference and we show that it\nyields comparable semantic segmentation results, accurately capturing object\nboundaries. Importantly, our formulation allows learning the reference edge map\nfrom intermediate CNN features instead of using the image gradient magnitude as\nin standard DT filtering. This produces task-specific edges in an end-to-end\ntrainable system optimizing the target semantic segmentation quality.\n", "rewritten_text": "\\boxed{Deep convolutional neural networks (CNNs) are the foundation of top-performing semantic image segmentation systems. While combining CNNs with fully-connected conditional random fields (CRFs) can significantly improve object localization accuracy, dense CRF inference is computationally costly. We introduce a novel approach that replaces fully-connected CRFs with domain transform (DT) filtering, a modern edge-preserving method that controls smoothing via a reference edge map. Our method achieves comparable semantic segmentation results, accurately capturing object boundaries, at a significantly faster speed than dense CRF inference. Moreover, our formulation enables learning the reference edge map from intermediate CNN features, producing task-specific edges in an end-to-end trainable system optimized for semantic segmentation quality.}"}, "1610.03155": {"original_text": "  Convolutional Neural Networks (CNN) have demon- strated its successful\napplications in computer vision, speech recognition, and natural language\nprocessing. For object recog- nition, CNNs might be limited by its strict label\nrequirement and an implicit assumption that images are supposed to be target-\nobject-dominated for optimal solutions. However, the labeling procedure,\nnecessitating laying out the locations of target ob- jects, is very tedious,\nmaking high-quality large-scale dataset prohibitively expensive. Data\naugmentation schemes are widely used when deep networks suffer the insufficient\ntraining data problem. All the images produced through data augmentation share\nthe same label, which may be problematic since not all data augmentation\nmethods are label-preserving. In this paper, we propose a weakly supervised CNN\nframework named Multiple Instance Learning Convolutional Neural Networks\n(MILCNN) to solve this problem. We apply MILCNN framework to object recognition\nand report state-of-the-art performance on three benchmark datasets: CIFAR10,\nCIFAR100 and ILSVRC2015 classification dataset.\n", "rewritten_text": "\\boxed{Convolutional Neural Networks (CNNs) have achieved great success in various applications, including computer vision, speech recognition, and natural language processing. However, they have limitations in object recognition, particularly due to the need for strict labeling and the assumption that images are dominated by target objects. The labeling process is time-consuming and expensive, making it challenging to create large-scale datasets. Data augmentation techniques are often used to address the issue of insufficient training data, but they can be problematic since not all methods preserve the original label. To overcome this limitation, we propose a weakly supervised CNN framework called Multiple Instance Learning Convolutional Neural Networks (MILCNN). We apply MILCNN to object recognition and achieve state-of-the-art performance on three benchmark datasets: CIFAR10, CIFAR100, and ILSVRC2015 classification dataset.}"}, "1711.06606": {"original_text": "  To realize the full potential of deep learning for medical imaging, large\nannotated datasets are required for training. Such datasets are difficult to\nacquire because labeled medical images are not usually available due to privacy\nissues, lack of experts available for annotation, underrepresentation of rare\nconditions and poor standardization. Lack of annotated data has been addressed\nin conventional vision applications using synthetic images refined via\nunsupervised adversarial training to look like real images. However, this\napproach is difficult to extend to general medical imaging because of the\ncomplex and diverse set of features found in real human tissues. We propose an\nalternative framework that uses a reverse flow, where adversarial training is\nused to make real medical images more like synthetic images, and hypothesize\nthat clinically-relevant features can be preserved via self-regularization.\nThese domain-adapted images can then be accurately interpreted by networks\ntrained on large datasets of synthetic medical images. We test this approach\nfor the notoriously difficult task of depth-estimation from endoscopy. We train\na depth estimator on a large dataset of synthetic images generated using an\naccurate forward model of an endoscope and an anatomically-realistic colon.\nThis network predicts significantly better depths when using synthetic-like\ndomain-adapted images compared to the real images, confirming that the\nclinically-relevant features of depth are preserved.\n", "rewritten_text": "\\boxed{\nTo fully utilize deep learning in medical imaging, large datasets with annotations are necessary, but these are hard to obtain due to privacy concerns, limited expert availability, underrepresentation of rare conditions, and poor standardization. While synthetic images refined through unsupervised adversarial training have been used to address data scarcity in conventional vision applications, this approach is challenging to apply to medical imaging due to the complexity and diversity of human tissue features. We propose a reverse approach, where adversarial training is used to make real medical images resemble synthetic ones, and hypothesize that self-regularization can preserve clinically relevant features. These adapted images can then be accurately interpreted by networks trained on large synthetic image datasets. We test this approach on the challenging task of depth estimation from endoscopy, training a depth estimator on a large synthetic image dataset generated using an accurate endoscope model and anatomically realistic colon. The results show that the network predicts depths more accurately when using adapted images, confirming that clinically relevant depth features are preserved."}, "2402.17074": {"original_text": "  Digital Image Correlation (DIC) is an optical technique that measures\ndisplacement and strain by tracking pattern movement in a sequence of captured\nimages during testing. DIC has gained recognition in asphalt pavement\nengineering since the early 2000s. However, users often perceive the DIC\ntechnique as an out-of-box tool and lack a thorough understanding of its\noperational and measurement principles. This article presents a state-of-art\nreview of DIC as a crucial tool for laboratory testing of asphalt concrete\n(AC), primarily focusing on the widely utilized 2D-DIC and 3D-DIC techniques.\nTo address frequently asked questions from users, the review thoroughly\nexamines the optimal methods for preparing speckle patterns, configuring\nsingle-camera or dual-camera imaging systems, conducting DIC analyses, and\nexploring various applications. Furthermore, emerging DIC methodologies such as\nDigital Volume Correlation and deep-learning-based DIC are introduced,\nhighlighting their potential for future applications in pavement engineering.\nThe article also provides a comprehensive and reliable flowchart for\nimplementing DIC in AC characterization. Finally, critical directions for\nfuture research are presented.\n", "rewritten_text": "\\boxed{\nDigital Image Correlation (DIC) is a powerful optical technique that measures displacement and strain by tracking pattern movement in a sequence of images captured during testing. Despite its growing popularity in asphalt pavement engineering since the early 2000s, many users lack a deep understanding of its operational and measurement principles. This article provides a comprehensive review of DIC as a vital tool for laboratory testing of asphalt concrete, focusing on 2D-DIC and 3D-DIC techniques. It addresses common user questions by examining optimal methods for preparing speckle patterns, configuring imaging systems, conducting DIC analyses, and exploring various applications. The review also introduces emerging DIC methodologies, including Digital Volume Correlation and deep-learning-based DIC, and provides a reliable flowchart for implementing DIC in asphalt concrete characterization. Finally, it highlights critical directions for future research.\n}"}, "1805.06173": {"original_text": "  Existing deep convolutional neural networks have found major success in image\nderaining, but at the expense of an enormous number of parameters. This limits\ntheir potential application, for example in mobile devices. In this paper, we\npropose a lightweight pyramid of networks (LPNet) for single image deraining.\nInstead of designing a complex network structures, we use domain-specific\nknowledge to simplify the learning process. Specifically, we find that by\nintroducing the mature Gaussian-Laplacian image pyramid decomposition\ntechnology to the neural network, the learning problem at each pyramid level is\ngreatly simplified and can be handled by a relatively shallow network with few\nparameters. We adopt recursive and residual network structures to build the\nproposed LPNet, which has less than 8K parameters while still achieving\nstate-of-the-art performance on rain removal. We also discuss the potential\nvalue of LPNet for other low- and high-level vision tasks.\n", "rewritten_text": "\\boxed{Current deep convolutional neural networks have achieved significant success in image deraining, but at the cost of requiring a large number of parameters, which limits their applicability, particularly in mobile devices. This paper proposes a novel lightweight pyramid of networks (LPNet) for single image deraining, which leverages domain-specific knowledge to simplify the learning process. By incorporating the Gaussian-Laplacian image pyramid decomposition technology into the neural network, the learning problem at each pyramid level is significantly simplified, allowing for a relatively shallow network with few parameters to be used. The proposed LPNet, which has fewer than 8,000 parameters, achieves state-of-the-art performance in rain removal and has potential applications in other low- and high-level vision tasks.}"}, "2208.00919": {"original_text": "  Visual-inertial localization is a key problem in computer vision and robotics\napplications such as virtual reality, self-driving cars, and aerial vehicles.\nThe goal is to estimate an accurate pose of an object when either the\nenvironment or the dynamics are known. Absolute pose regression (APR)\ntechniques directly regress the absolute pose from an image input in a known\nscene using convolutional and spatio-temporal networks. Odometry methods\nperform relative pose regression (RPR) that predicts the relative pose from a\nknown object dynamic (visual or inertial inputs). The localization task can be\nimproved by retrieving information from both data sources for a cross-modal\nsetup, which is a challenging problem due to contradictory tasks. In this work,\nwe conduct a benchmark to evaluate deep multimodal fusion based on pose graph\noptimization and attention networks. Auxiliary and Bayesian learning are\nutilized for the APR task. We show accuracy improvements for the APR-RPR task\nand for the RPR-RPR task for aerial vehicles and hand-held devices. We conduct\nexperiments on the EuRoC MAV and PennCOSYVIO datasets and record and evaluate a\nnovel industry dataset.\n", "rewritten_text": "\\boxed{Determining the precise location and orientation of an object is crucial in computer vision and robotics applications, such as virtual reality, self-driving cars, and aerial vehicles. This task can be approached in two ways: absolute pose regression (APR), which estimates the object's pose from a single image, and relative pose regression (RPR), which predicts the pose change from sequential images or sensor data. Combining these two approaches can lead to improved localization, but it's a challenging task due to the conflicting nature of the two methods. This study benchmarks the performance of deep multimodal fusion techniques, including pose graph optimization and attention networks, and explores the use of auxiliary and Bayesian learning for APR. The results show improved accuracy for both APR-RPR and RPR-RPR tasks in aerial vehicles and hand-held devices, using datasets from EuRoC MAV, PennCOSYVIO, and a novel industry dataset.}"}, "2107.0984": {"original_text": "  Multilingual pre-trained contextual embedding models (Devlin et al., 2019)\nhave achieved impressive performance on zero-shot cross-lingual transfer tasks.\nFinding the most effective fine-tuning strategy to fine-tune these models on\nhigh-resource languages so that it transfers well to the zero-shot languages is\na non-trivial task. In this paper, we propose a novel meta-optimizer to\nsoft-select which layers of the pre-trained model to freeze during fine-tuning.\nWe train the meta-optimizer by simulating the zero-shot transfer scenario.\nResults on cross-lingual natural language inference show that our approach\nimproves over the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al.,\n2020).\n", "rewritten_text": "\\boxed{Multilingual language models have shown remarkable performance in zero-shot cross-lingual transfer tasks. However, fine-tuning these models on high-resource languages to achieve optimal transfer to zero-shot languages remains a challenge. This paper proposes a novel meta-optimizer that dynamically selects which layers of the pre-trained model to freeze during fine-tuning, trained through simulated zero-shot transfer scenarios. The results demonstrate that this approach outperforms simple fine-tuning and X-MAML in cross-lingual natural language inference tasks.}"}, "2010.09298": {"original_text": "  Though deep learning has achieved advanced performance recently, it remains a\nchallenging task in the field of medical imaging, as obtaining reliable labeled\ntraining data is time-consuming and expensive. In this paper, we propose a\ndouble-uncertainty weighted method for semi-supervised segmentation based on\nthe teacher-student model. The teacher model provides guidance for the student\nmodel by penalizing their inconsistent prediction on both labeled and unlabeled\ndata. We train the teacher model using Bayesian deep learning to obtain\ndouble-uncertainty, i.e. segmentation uncertainty and feature uncertainty. It\nis the first to extend segmentation uncertainty estimation to feature\nuncertainty, which reveals the capability to capture information among\nchannels. A learnable uncertainty consistency loss is designed for the\nunsupervised learning process in an interactive manner between prediction and\nuncertainty. With no ground-truth for supervision, it can still incentivize\nmore accurate teacher's predictions and facilitate the model to reduce\nuncertain estimations. Furthermore, our proposed double-uncertainty serves as a\nweight on each inconsistency penalty to balance and harmonize supervised and\nunsupervised training processes. We validate the proposed feature uncertainty\nand loss function through qualitative and quantitative analyses. Experimental\nresults show that our method outperforms the state-of-the-art uncertainty-based\nsemi-supervised methods on two public medical datasets.\n", "rewritten_text": "\\boxed{\nDeep learning has made significant progress in medical imaging, but it remains a challenging task due to the difficulty and expense of obtaining reliable labeled training data. To address this, we propose a novel semi-supervised segmentation method that leverages a teacher-student model with double-uncertainty weighting. The teacher model guides the student model by penalizing inconsistencies in their predictions on both labeled and unlabeled data. We use Bayesian deep learning to train the teacher model, which yields two types of uncertainty: segmentation uncertainty and feature uncertainty. The latter is a novel concept that captures information across channels. We design a learnable uncertainty consistency loss that facilitates unsupervised learning and encourages more accurate predictions. Our double-uncertainty weighting balances supervised and unsupervised training, leading to improved performance. Experimental results on two public medical datasets demonstrate that our method outperforms state-of-the-art uncertainty-based semi-supervised methods.\n}"}, "2108.11575": {"original_text": "  Spatio-temporal representational learning has been widely adopted in various\nfields such as action recognition, video object segmentation, and action\nanticipation. Previous spatio-temporal representational learning approaches\nprimarily employ ConvNets or sequential models,e.g., LSTM, to learn the\nintra-frame and inter-frame features. Recently, Transformer models have\nsuccessfully dominated the study of natural language processing (NLP), image\nclassification, etc. However, the pure-Transformer based spatio-temporal\nlearning can be prohibitively costly on memory and computation to extract\nfine-grained features from a tiny patch. To tackle the training difficulty and\nenhance the spatio-temporal learning, we construct a shifted chunk Transformer\nwith pure self-attention blocks. Leveraging the recent efficient Transformer\ndesign in NLP, this shifted chunk Transformer can learn hierarchical\nspatio-temporal features from a local tiny patch to a global video clip. Our\nshifted self-attention can also effectively model complicated inter-frame\nvariances. Furthermore, we build a clip encoder based on Transformer to model\nlong-term temporal dependencies. We conduct thorough ablation studies to\nvalidate each component and hyper-parameters in our shifted chunk Transformer,\nand it outperforms previous state-of-the-art approaches on Kinetics-400,\nKinetics-600, UCF101, and HMDB51.\n", "rewritten_text": "\\boxed{\nSpatio-temporal representational learning has been widely applied in various fields, including action recognition, video object segmentation, and action anticipation. While previous approaches relied on ConvNets or sequential models like LSTM to learn intra-frame and inter-frame features, recent successes in natural language processing and image classification have led to the adoption of Transformer models. However, pure-Transformer based spatio-temporal learning can be computationally expensive and memory-intensive when extracting fine-grained features from small patches. To address these challenges, we propose a shifted chunk Transformer with pure self-attention blocks, inspired by efficient Transformer designs in NLP. This model can learn hierarchical spatio-temporal features from local patches to global video clips and effectively capture complex inter-frame variations. Additionally, we develop a clip encoder based on Transformer to model long-term temporal dependencies. Through thorough ablation studies, we validate the effectiveness of each component and hyperparameter in our shifted chunk Transformer, which outperforms state-of-the-art approaches on several benchmark datasets, including Kinetics-400, Kinetics-600, UCF101, and HMDB51.\n}"}, "2102.0208": {"original_text": "  We introduce a top-down approach to discourse parsing that is conceptually\nsimpler than its predecessors (Kobayashi et al., 2020; Zhang et al., 2020). By\nframing the task as a sequence labelling problem where the goal is to\niteratively segment a document into individual discourse units, we are able to\neliminate the decoder and reduce the search space for splitting points. We\nexplore both traditional recurrent models and modern pre-trained transformer\nmodels for the task, and additionally introduce a novel dynamic oracle for\ntop-down parsing. Based on the Full metric, our proposed LSTM model sets a new\nstate-of-the-art for RST parsing.\n", "rewritten_text": "\\boxed{We propose a simplified top-down approach to discourse parsing, which involves segmenting a document into individual discourse units through iterative sequence labeling. By eliminating the decoder and reducing the search space for splitting points, our method is more efficient than previous approaches. We experiment with both traditional recurrent models and modern pre-trained transformer models, and introduce a novel dynamic oracle for top-down parsing. Our LSTM model achieves a new state-of-the-art performance on RST parsing, as measured by the Full metric.}"}, "1112.2386": {"original_text": "  This paper proposes a new procedure in order to improve the performance of\nblock matching and 3-D filtering (BM3D) image denoising algorithm. It is\ndemonstrated that it is possible to achieve a better performance than that of\nBM3D algorithm in a variety of noise levels. This method changes BM3D algorithm\nparameter values according to noise level, removes prefiltering, which is used\nin high noise level; therefore Peak Signal-to-Noise Ratio (PSNR) and visual\nquality get improved, and BM3D complexities and processing time are reduced.\nThis improved BM3D algorithm is extended and used to denoise satellite and\ncolor filter array (CFA) images. Output results show that the performance has\nupgraded in comparison with current methods of denoising satellite and CFA\nimages. In this regard this algorithm is compared with Adaptive PCA algorithm,\nthat has led to superior performance for denoising CFA images, on the subject\nof PSNR and visual quality. Also the processing time has decreased\nsignificantly.\n", "rewritten_text": "\\boxed{\nThis paper introduces a novel approach to enhance the performance of the Block Matching and 3-D filtering (BM3D) image denoising algorithm. By adapting BM3D algorithm parameters to noise levels and eliminating prefiltering in high-noise scenarios, the proposed method achieves improved Peak Signal-to-Noise Ratio (PSNR) and visual quality, while reducing computational complexity and processing time. The enhanced BM3D algorithm is then applied to denoise satellite and color filter array (CFA) images, outperforming existing methods in terms of PSNR and visual quality. A comparison with the Adaptive PCA algorithm reveals superior performance in denoising CFA images, with a significant decrease in processing time."}, "2103.06678": {"original_text": "  In this paper, we explore the effects of language variants, data sizes, and\nfine-tuning task types in Arabic pre-trained language models. To do so, we\nbuild three pre-trained language models across three variants of Arabic: Modern\nStandard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a\nfourth language model which is pre-trained on a mix of the three. We also\nexamine the importance of pre-training data size by building additional models\nthat are pre-trained on a scaled-down set of the MSA variant. We compare our\ndifferent models to each other, as well as to eight publicly available models\nby fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest\nthat the variant proximity of pre-training data to fine-tuning data is more\nimportant than the pre-training data size. We exploit this insight in defining\nan optimized system selection model for the studied tasks.\n", "rewritten_text": "\\boxed{This paper investigates the impact of Arabic language variants, dataset sizes, and fine-tuning tasks on pre-trained language models. We develop four models, each trained on a different variant of Arabic (Modern Standard, dialectal, classical, and a mix) and examine the effect of dataset size by training additional models on a reduced Modern Standard Arabic dataset. We compare our models to eight publicly available models by fine-tuning them on 12 datasets across five NLP tasks. Our findings indicate that the similarity between the pre-training and fine-tuning data variants is more crucial than the size of the pre-training dataset. We utilize this insight to develop an optimized model selection system for the tasks studied.}"}, "2408.00331": {"original_text": "  Reliably detecting when a deployed machine learning model is likely to fail\non a given input is crucial for ensuring safe operation. In this work, we\npropose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel\napproach that leverages priors from large language models (LLMs) and\nvision-language models (VLMs) to detect failures in image classification\nmodels. DECIDER utilizes LLMs to specify task-relevant core attributes and\nconstructs a ``debiased'' version of the classifier by aligning its visual\nfeatures to these core attributes using a VLM, and detects potential failure by\nmeasuring disagreement between the original and debiased models. In addition to\nproactively identifying samples on which the model would fail, DECIDER also\nprovides human-interpretable explanations for failure through a novel\nattribute-ablation strategy. Through extensive experiments across diverse\nbenchmarks spanning subpopulation shifts (spurious correlations, class\nimbalance) and covariate shifts (synthetic corruptions, domain shifts), DECIDER\nconsistently achieves state-of-the-art failure detection performance,\nsignificantly outperforming baselines in terms of the overall Matthews\ncorrelation coefficient as well as failure and success recall. Our codes can be\naccessed at~\\url{https://github.com/kowshikthopalli/DECIDER/}\n", "rewritten_text": "\\boxed{\nDetecting potential failures of deployed machine learning models is crucial for safe operation. This work introduces DECIDER, a novel approach that utilizes large language models and vision-language models to detect failures in image classification models. DECIDER identifies task-relevant core attributes using language models and constructs a debiased classifier by aligning visual features to these attributes using a vision-language model. It detects potential failures by measuring disagreement between the original and debiased models. Additionally, DECIDER provides human-interpretable explanations for failure through attribute ablation. Experimental results show that DECIDER achieves state-of-the-art failure detection performance across various benchmarks, outperforming baselines in terms of Matthews correlation coefficient and failure/success recall. The code is available at https://github.com/kowshikthopalli/DECIDER/.\n}"}, "1708.06126": {"original_text": "  This paper presents a novel application to detect counterfeit identity\ndocuments forged by a scan-printing operation. Texture analysis approaches are\nproposed to extract validation features from security background that is\nusually printed in documents as IDs or banknotes. The main contribution of this\nwork is the end-to-end mobile-server architecture, which provides a service for\nnon-expert users and therefore can be used in several scenarios. The system\nalso provides a crowdsourcing mode so labeled images can be gathered,\ngenerating databases for incremental training of the algorithms.\n", "rewritten_text": "\\boxed{This paper introduces a new approach to identify counterfeit identity documents created through scan-printing. It proposes the use of texture analysis to extract verification features from the security background of documents such as IDs and banknotes. The key innovation is a mobile-server system that allows non-expert users to verify documents in various scenarios. Additionally, the system includes a crowdsourcing feature that enables the collection of labeled images, which can be used to improve the algorithms through incremental training.}"}, "2409.12421": {"original_text": "  Camouflaged object detection (COD) aims to segment camouflaged objects which\nexhibit very similar patterns with the surrounding environment. Recent research\nworks have shown that enhancing the feature representation via the frequency\ninformation can greatly alleviate the ambiguity problem between the foreground\nobjects and the background.With the emergence of vision foundation models, like\nInternImage, Segment Anything Model etc, adapting the pretrained model on COD\ntasks with a lightweight adapter module shows a novel and promising research\ndirection. Existing adapter modules mainly care about the feature adaptation in\nthe spatial domain. In this paper, we propose a novel frequency-guided spatial\nadaptation method for COD task. Specifically, we transform the input features\nof the adapter into frequency domain. By grouping and interacting with\nfrequency components located within non overlapping circles in the spectrogram,\ndifferent frequency components are dynamically enhanced or weakened, making the\nintensity of image details and contour features adaptively adjusted. At the\nsame time, the features that are conducive to distinguishing object and\nbackground are highlighted, indirectly implying the position and shape of\ncamouflaged object. We conduct extensive experiments on four widely adopted\nbenchmark datasets and the proposed method outperforms 26 state-of-the-art\nmethods with large margins. Code will be released.\n", "rewritten_text": "\\boxed{\nCamouflaged object detection (COD) is a challenging task that involves segmenting objects that blend in with their surroundings. Recent studies have shown that incorporating frequency information into feature representations can help resolve the ambiguity between foreground objects and background. With the advent of vision foundation models like InternImage and Segment Anything Model, adapting these models to COD tasks using lightweight adapter modules has become a promising research direction. However, existing adapter modules primarily focus on spatial feature adaptation. This paper proposes a novel frequency-guided spatial adaptation method for COD tasks. Specifically, the input features of the adapter are transformed into the frequency domain, where frequency components are dynamically enhanced or weakened based on their location in the spectrogram. This adaptive adjustment of image details and contour features helps highlight the features that distinguish objects from the background, indirectly implying the position and shape of camouflaged objects. Extensive experiments on four benchmark datasets demonstrate that the proposed method outperforms 26 state-of-the-art methods by a significant margin. The code will be made publicly available.\n}"}, "1712.03687": {"original_text": "  Because of affected by weather conditions, camera pose and range, etc.\nObjects are usually small, blur, occluded and diverse pose in the images\ngathered from outdoor surveillance cameras or access control system. It is\nchallenging and important to detect faces precisely for face recognition system\nin the field of public security. In this paper, we design a based on context\nmodeling structure named Feature Hierarchy Encoder-Decoder Network for face\ndetection(FHEDN), which can detect small, blur and occluded face with hierarchy\nby hierarchy from the end to the beginning likes encoder-decoder in a single\nnetwork. The proposed network is consist of multiple context modeling and\nprediction modules, which are in order to detect small, blur, occluded and\ndiverse pose faces. In addition, we analyse the influence of distribution of\ntraining set, scale of default box and receipt field size to detection\nperformance in implement stage. Demonstrated by experiments, Our network\nachieves promising performance on WIDER FACE and FDDB benchmarks.\n", "rewritten_text": "\\boxed{\nFace detection in outdoor surveillance cameras and access control systems is challenging due to weather conditions, camera pose, and range, which result in small, blurry, occluded, and diverse pose faces in images. To address this, we propose a Feature Hierarchy Encoder-Decoder Network (FHEDN) that detects faces with a hierarchical approach, handling small, blurry, and occluded faces in a single network. The network consists of multiple context modeling and prediction modules to detect faces with varying poses. We also analyze the impact of training set distribution, default box scale, and receptive field size on detection performance. Experimental results on WIDER FACE and FDDB benchmarks demonstrate the promising performance of our network.\n}"}, "2010.01305": {"original_text": "  Street view images classification aiming at urban land use analysis is\ndifficult because the class labels (e.g., commercial area), are concepts with\nhigher abstract level compared to the ones of general visual tasks (e.g.,\npersons and cars). Therefore, classification models using only visual features\noften fail to achieve satisfactory performance. In this paper, a novel approach\nbased on a \"Detector-Encoder-Classifier\" framework is proposed. Instead of\nusing visual features of the whole image directly as common image-level models\nbased on convolutional neural networks (CNNs) do, the proposed framework\nfirstly obtains the bounding boxes of buildings in street view images from a\ndetector. Their contextual information such as the co-occurrence patterns of\nbuilding classes and their layout are then encoded into metadata by the\nproposed algorithm \"CODING\" (Context encOding of Detected buildINGs). Finally,\nthese bounding box metadata are classified by a recurrent neural network (RNN).\nIn addition, we made a dual-labeled dataset named \"BEAUTY\" (Building dEtection\nAnd Urban funcTional-zone portraYing) of 19,070 street view images and 38,857\nbuildings based on the existing BIC GSV [1]. The dataset can be used not only\nfor street view image classification, but also for multi-class building\ndetection. Experiments on \"BEAUTY\" show that the proposed approach achieves a\n12.65% performance improvement on macro-precision and 12% on macro-recall over\nimage-level CNN based models. Our code and dataset are available at\nhttps://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/\n", "rewritten_text": "\\boxed{\nClassifying street view images for urban land use analysis is challenging due to the abstract nature of class labels, such as commercial areas. Traditional image-level models using convolutional neural networks (CNNs) often fail to achieve satisfactory performance. To address this, we propose a novel \"Detector-Encoder-Classifier\" framework. This framework first detects building bounding boxes in street view images, then encodes their contextual information, such as co-occurrence patterns and layout, into metadata using the \"CODING\" algorithm. Finally, a recurrent neural network (RNN) classifies the metadata. We also introduce a new dataset, \"BEAUTY\", consisting of 19,070 street view images and 38,857 buildings, which can be used for both street view image classification and multi-class building detection. Experimental results on \"BEAUTY\" show that our approach outperforms image-level CNN-based models by 12.65% in macro-precision and 12% in macro-recall. Our code and dataset are available online."}, "2403.10737": {"original_text": "  Facial action unit (AU) detection is a fundamental block for objective facial\nexpression analysis. Supervised learning approaches require a large amount of\nmanual labeling which is costly. The limited labeled data are also not diverse\nin terms of gender which can affect model fairness. In this paper, we propose\nto use synthetically generated data and multi-source domain adaptation (MSDA)\nto address the problems of the scarcity of labeled data and the diversity of\nsubjects. Specifically, we propose to generate a diverse dataset through\nsynthetic facial expression re-targeting by transferring the expressions from\nreal faces to synthetic avatars. Then, we use MSDA to transfer the AU detection\nknowledge from a real dataset and the synthetic dataset to a target dataset.\nInstead of aligning the overall distributions of different domains, we propose\nPaired Moment Matching (PM2) to align the features of the paired real and\nsynthetic data with the same facial expression. To further improve gender\nfairness, PM2 matches the features of the real data with a female and a male\nsynthetic image. Our results indicate that synthetic data and the proposed\nmodel improve both AU detection performance and fairness across genders,\ndemonstrating its potential to solve AU detection in-the-wild.\n", "rewritten_text": "\\boxed{\nFacial action unit (AU) detection is a crucial component of objective facial expression analysis, but it's hindered by the need for extensive manual labeling, which is time-consuming and costly. Moreover, the limited available labeled data often lack diversity in terms of gender, which can compromise model fairness. To address these issues, we propose a novel approach that leverages synthetically generated data and multi-source domain adaptation (MSDA). Our method involves generating a diverse dataset by transferring real facial expressions to synthetic avatars, and then using MSDA to transfer AU detection knowledge from real and synthetic datasets to a target dataset. Instead of aligning overall distributions, we introduce Paired Moment Matching (PM2) to align features of paired real and synthetic data with the same facial expression, and further improve gender fairness by matching real data with female and male synthetic images. Our results show that synthetic data and the proposed model enhance both AU detection performance and fairness across genders, demonstrating its potential for in-the-wild AU detection.\n}"}, "2401.03129": {"original_text": "  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n", "rewritten_text": "\\boxed{Recent breakthroughs in Large Language Models (LLMs) have demonstrated exceptional capabilities across multiple tasks, leading to a rapid expansion in their development. However, a common approach in LLM development, which involves continuous pre-training on previously fine-tuned models, can result in a significant loss of previously learned information, known as catastrophic forgetting. Our research focuses on examining this phenomenon of forgetting in fine-tuned LLMs during continuous pre-training, assessing its impact on various aspects, including output format, knowledge retention, and reliability. Our experimental results underscore the substantial challenge of mitigating catastrophic forgetting, particularly the issue of repetition.}"}, "2105.07197": {"original_text": "  Modern machine learning models for computer vision exceed humans in accuracy\non specific visual recognition tasks, notably on datasets like ImageNet.\nHowever, high accuracy can be achieved in many ways. The particular decision\nfunction found by a machine learning system is determined not only by the data\nto which the system is exposed, but also the inductive biases of the model,\nwhich are typically harder to characterize. In this work, we follow a recent\ntrend of in-depth behavioral analyses of neural network models that go beyond\naccuracy as an evaluation metric by looking at patterns of errors. Our focus is\non comparing a suite of standard Convolutional Neural Networks (CNNs) and a\nrecently-proposed attention-based network, the Vision Transformer (ViT), which\nrelaxes the translation-invariance constraint of CNNs and therefore represents\na model with a weaker set of inductive biases. Attention-based networks have\npreviously been shown to achieve higher accuracy than CNNs on vision tasks, and\nwe demonstrate, using new metrics for examining error consistency with more\ngranularity, that their errors are also more consistent with those of humans.\nThese results have implications both for building more human-like vision\nmodels, as well as for understanding visual object recognition in humans.\n", "rewritten_text": "\\boxed{While modern computer vision models surpass human accuracy on specific tasks, their decision-making processes are influenced by both data and inductive biases. This study delves beyond accuracy metrics to analyze error patterns in standard Convolutional Neural Networks (CNNs) and the attention-based Vision Transformer (ViT) model, which has weaker inductive biases. The results show that attention-based networks not only achieve higher accuracy but also exhibit error consistency similar to humans, with implications for developing more human-like vision models and understanding human visual object recognition.}"}, "1902.01955": {"original_text": "  In conventional speech recognition, phoneme-based models outperform\ngrapheme-based models for non-phonetic languages such as English. The\nperformance gap between the two typically reduces as the amount of training\ndata is increased. In this work, we examine the impact of the choice of\nmodeling unit for attention-based encoder-decoder models. We conduct\nexperiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, using various\ntarget units (phoneme, grapheme, and word-piece); across all tasks, we find\nthat grapheme or word-piece models consistently outperform phoneme-based\nmodels, even though they are evaluated without a lexicon or an external\nlanguage model. We also investigate model complementarity: we find that we can\nimprove WERs by up to 9% relative by rescoring N-best lists generated from a\nstrong word-piece based baseline with either the phoneme or the grapheme model.\nRescoring an N-best list generated by the phonemic system, however, provides\nlimited improvements. Further analysis shows that the word-piece-based models\nproduce more diverse N-best hypotheses, and thus lower oracle WERs, than\nphonemic models.\n", "rewritten_text": "\\boxed{\nIn contrast to traditional speech recognition, where phoneme-based models excel for non-phonetic languages like English, our study reveals that grapheme-based or word-piece models outperform phoneme-based models in attention-based encoder-decoder models, even without a lexicon or external language model. Moreover, combining models by rescoring N-best lists generated from a strong word-piece baseline with either phoneme or grapheme models improves WERs by up to 9%. This is because word-piece-based models produce more diverse N-best hypotheses, leading to lower oracle WERs."}, "2012.12482": {"original_text": "  We address the problem of crowd localization, i.e., the prediction of dots\ncorresponding to people in a crowded scene. Due to various challenges, a\nlocalization method is prone to spatial semantic errors, i.e., predicting\nmultiple dots within a same person or collapsing multiple dots in a cluttered\nregion. We propose a topological approach targeting these semantic errors. We\nintroduce a topological constraint that teaches the model to reason about the\nspatial arrangement of dots. To enforce this constraint, we define a\npersistence loss based on the theory of persistent homology. The loss compares\nthe topographic landscape of the likelihood map and the topology of the ground\ntruth. Topological reasoning improves the quality of the localization algorithm\nespecially near cluttered regions. On multiple public benchmarks, our method\noutperforms previous localization methods. Additionally, we demonstrate the\npotential of our method in improving the performance in the crowd counting\ntask.\n", "rewritten_text": "\\boxed{This paper tackles the challenge of crowd localization, which involves predicting the locations of individuals in a crowded scene. Existing methods often struggle with spatial semantic errors, such as mistakenly identifying multiple people as one or vice versa. To overcome this, we propose a topological approach that teaches the model to understand the spatial relationships between people. We introduce a novel loss function based on persistent homology, which compares the topological features of the predicted likelihood map with the ground truth. Our method significantly improves the accuracy of crowd localization, particularly in cluttered regions, and outperforms existing methods on multiple benchmarks. Furthermore, we show that our approach can also enhance the performance of crowd counting tasks.}"}, "2303.02982": {"original_text": "  Learning from large-scale contrastive language-image pre-training like CLIP\nhas shown remarkable success in a wide range of downstream tasks recently, but\nit is still under-explored on the challenging few-shot action recognition\n(FSAR) task. In this work, we aim to transfer the powerful multimodal knowledge\nof CLIP to alleviate the inaccurate prototype estimation issue due to data\nscarcity, which is a critical problem in low-shot regimes. To this end, we\npresent a CLIP-guided prototype modulating framework called CLIP-FSAR, which\nconsists of two key components: a video-text contrastive objective and a\nprototype modulation. Specifically, the former bridges the task discrepancy\nbetween CLIP and the few-shot video task by contrasting videos and\ncorresponding class text descriptions. The latter leverages the transferable\ntextual concepts from CLIP to adaptively refine visual prototypes with a\ntemporal Transformer. By this means, CLIP-FSAR can take full advantage of the\nrich semantic priors in CLIP to obtain reliable prototypes and achieve accurate\nfew-shot classification. Extensive experiments on five commonly used benchmarks\ndemonstrate the effectiveness of our proposed method, and CLIP-FSAR\nsignificantly outperforms existing state-of-the-art methods under various\nsettings. The source code and models will be publicly available at\nhttps://github.com/alibaba-mmai-research/CLIP-FSAR.\n", "rewritten_text": "\\boxed{\nRecently, large-scale contrastive language-image pre-training models like CLIP have achieved remarkable success in various downstream tasks. However, their application to few-shot action recognition (FSAR) tasks remains under-explored. This task is challenging due to the inaccurate estimation of prototypes caused by data scarcity. To address this issue, we propose a CLIP-guided prototype modulating framework called CLIP-FSAR. This framework consists of two key components: a video-text contrastive objective that bridges the task discrepancy between CLIP and FSAR, and a prototype modulation that leverages transferable textual concepts from CLIP to adaptively refine visual prototypes using a temporal Transformer. By utilizing the rich semantic priors in CLIP, CLIP-FSAR can obtain reliable prototypes and achieve accurate few-shot classification. Our extensive experiments on five benchmarks demonstrate the effectiveness of our proposed method, outperforming existing state-of-the-art methods under various settings. The source code and models are publicly available at https://github.com/alibaba-mmai-research/CLIP-FSAR.\n}"}, "1905.07826": {"original_text": "  Multi-instance video object segmentation is to segment specific instances\nthroughout a video sequence in pixel level, given only an annotated first\nframe. In this paper, we implement an effective fully convolutional networks\nwith U-Net similar structure built on top of OSVOS fine-tuned layer. We use\ninstance isolation to transform this multi-instance segmentation problem into\nbinary labeling problem, and use weighted cross entropy loss and dice\ncoefficient loss as our loss function. Our best model achieves F mean of 0.467\nand J mean of 0.424 on DAVIS dataset, which is a comparable performance with\nthe State-of-the-Art approach. But case analysis shows this model can achieve a\nsmoother contour and better instance coverage, meaning it better for recall\nfocused segmentation scenario. We also did experiments on other convolutional\nneural networks, including Seg-Net, Mask R-CNN, and provide insightful\ncomparison and discussion.\n", "rewritten_text": "\\boxed{This paper tackles the task of multi-instance video object segmentation, where the goal is to identify specific objects in a video sequence at the pixel level, given only an annotated first frame. We propose a fully convolutional network with a U-Net-like structure, built on top of fine-tuned OSVOS layers, and utilize instance isolation to transform the problem into a binary labeling task. Our model achieves competitive performance on the DAVIS dataset, with an F-mean of 0.467 and J-mean of 0.424, and is particularly well-suited for recall-focused segmentation scenarios, producing smoother contours and better instance coverage. We also experiment with other convolutional neural networks, including Seg-Net and Mask R-CNN, and provide a comparative analysis.}"}, "2003.14282": {"original_text": "  A wide variety of transition-based algorithms are currently used for\ndependency parsers. Empirical studies have shown that performance varies across\ndifferent treebanks in such a way that one algorithm outperforms another on one\ntreebank and the reverse is true for a different treebank. There is often no\ndiscernible reason for what causes one algorithm to be more suitable for a\ncertain treebank and less so for another. In this paper we shed some light on\nthis by introducing the concept of an algorithm's inherent dependency\ndisplacement distribution. This characterises the bias of the algorithm in\nterms of dependency displacement, which quantify both distance and direction of\nsyntactic relations. We show that the similarity of an algorithm's inherent\ndistribution to a treebank's displacement distribution is clearly correlated to\nthe algorithm's parsing performance on that treebank, specifically with highly\nsignificant and substantial correlations for the predominant sentence lengths\nin Universal Dependency treebanks. We also obtain results which show a more\ndiscrete analysis of dependency displacement does not result in any meaningful\ncorrelations.\n", "rewritten_text": "\\boxed{Different dependency parsing algorithms perform variably across treebanks, with no apparent reason why one algorithm excels on one treebank but not another. This paper introduces the concept of an algorithm's inherent dependency displacement distribution, which captures its bias towards certain syntactic relations. We find that the similarity between an algorithm's distribution and a treebank's distribution is strongly correlated with the algorithm's parsing performance on that treebank, particularly for common sentence lengths in Universal Dependency treebanks. In contrast, a more detailed analysis of dependency displacement does not yield meaningful correlations.}"}, "2404.11682": {"original_text": "  Automated methods are becoming increasingly integrated into studies of\nformative feedback on students' science explanation writing. Most of this work,\nhowever, addresses students' responses to short answer questions. We\ninvestigate automated feedback on students' science explanation essays, where\nstudents must articulate multiple ideas. Feedback is based on a rubric that\nidentifies the main ideas students are prompted to include in explanatory\nessays about the physics of energy and mass, given their experiments with a\nsimulated roller coaster. We have found that students generally improve on\nrevised versions of their essays. Here, however, we focus on two factors that\naffect the accuracy of the automated feedback. First, we find that the main\nideas in the rubric differ with respect to how much freedom they afford in\nexplanations of the idea, thus explanation of a natural law is relatively\nconstrained. Students have more freedom in how they explain complex relations\nthey observe in their roller coasters, such as transfer of different forms of\nenergy. Second, by tracing the automated decision process, we can diagnose when\na student's statement lacks sufficient clarity for the automated tool to\nassociate it more strongly with one of the main ideas above all others. This in\nturn provides an opportunity for teachers and peers to help students reflect on\nhow to state their ideas more clearly.\n", "rewritten_text": "\\boxed{\nResearch on automated feedback for students' science writing has primarily focused on short answer questions. This study explores the use of automated feedback on students' science explanation essays, where they must express multiple ideas. The feedback is based on a rubric that identifies key concepts students should include in their essays about energy and mass, based on a simulated roller coaster experiment. While students generally improve their essays with automated feedback, this study examines two factors that affect the accuracy of the feedback. Firstly, the rubric's main ideas vary in the freedom they allow in explanations, with natural laws being more constrained and complex relationships being more open-ended. Secondly, by analyzing the automated decision process, it's possible to identify when a student's statement lacks clarity, providing an opportunity for teachers and peers to help students refine their ideas.\n}"}, "2211.08462": {"original_text": "  Recent years have seen an increasing trend in the volume of personal media\ncaptured by users, thanks to the advent of smartphones and smart glasses,\nresulting in large media collections. Despite conversation being an intuitive\nhuman-computer interface, current efforts focus mostly on single-shot natural\nlanguage based media retrieval to aid users query their media and re-live their\nmemories. This severely limits the search functionality as users can neither\nask follow-up queries nor obtain information without first formulating a\nsingle-turn query.\n  In this work, we propose dialogs for connected memories as a powerful tool to\nempower users to search their media collection through a multi-turn,\ninteractive conversation. Towards this, we collect a new task-oriented dialog\ndataset COMET, which contains $11.5k$ user<->assistant dialogs (totaling $103k$\nutterances), grounded in simulated personal memory graphs. We employ a\nresource-efficient, two-phase data collection pipeline that uses: (1) a novel\nmultimodal dialog simulator that generates synthetic dialog flows grounded in\nmemory graphs, and, (2) manual paraphrasing to obtain natural language\nutterances. We analyze COMET, formulate four main tasks to benchmark meaningful\nprogress, and adopt state-of-the-art language models as strong baselines, in\norder to highlight the multimodal challenges captured by our dataset.\n", "rewritten_text": "\\boxed{\nIn recent years, the widespread use of smartphones and smart glasses has led to a significant increase in personal media collections. However, current methods for searching and retrieving media rely heavily on single-turn natural language queries, which limit the search functionality and do not allow for follow-up questions or spontaneous information retrieval. To address this limitation, we propose the use of multi-turn, interactive conversations, or dialogs, to empower users to search their media collections more effectively. To support this approach, we have developed a new dataset called COMET, which consists of 11,500 user-assistant dialogs (totalling 103,000 utterances) grounded in simulated personal memory graphs. We employed a two-phase data collection pipeline that combines a multimodal dialog simulator with manual paraphrasing to generate natural language utterances. Our analysis of COMET reveals four key tasks that can be used to benchmark progress, and we adopt state-of-the-art language models as baselines to highlight the multimodal challenges captured by our dataset.\n}"}, "1911.03668": {"original_text": "  Natural Language Inference (NLI) aims to determine the logic relationships\n(i.e., entailment, neutral and contradiction) between a pair of premise and\nhypothesis. Recently, the alignment mechanism effectively helps NLI by\ncapturing the aligned parts (i.e., the similar segments) in the sentence pairs,\nwhich imply the perspective of entailment and contradiction. However, these\naligned parts will sometimes mislead the judgment of neutral relations.\nIntuitively, NLI should rely more on multiple perspectives to form a holistic\nview to eliminate bias. In this paper, we propose the Multi-Perspective\nInferrer (MPI), a novel NLI model that reasons relationships from multiple\nperspectives associated with the three relationships. The MPI determines the\nperspectives of different parts of the sentences via a routing-by-agreement\npolicy and makes the final decision from a holistic view. Additionally, we\nintroduce an auxiliary supervised signal to ensure the MPI to learn the\nexpected perspectives. Experiments on SNLI and MultiNLI show that 1) the MPI\nachieves substantial improvements on the base model, which verifies the\nmotivation of multi-perspective inference; 2) visualized evidence verifies that\nthe MPI learns highly interpretable perspectives as expected; 3) more\nimportantly, the MPI is architecture-free and compatible with the powerful\nBERT.\n", "rewritten_text": "\\boxed{\nNatural Language Inference (NLI) seeks to identify the logical relationships between a premise and hypothesis, including entailment, neutral, and contradiction. While alignment mechanisms have improved NLI by highlighting similar segments in sentence pairs, they can sometimes mislead judgments of neutral relationships. To address this, we propose the Multi-Perspective Inferrer (MPI), a novel NLI model that considers multiple perspectives associated with the three relationships. The MPI uses a routing-by-agreement policy to determine perspectives and makes a final decision based on a holistic view. We also introduce an auxiliary supervised signal to ensure the MPI learns expected perspectives. Our experiments on SNLI and MultiNLI demonstrate that the MPI achieves significant improvements over the base model, learns interpretable perspectives, and is compatible with powerful architectures like BERT."}, "2403.04212": {"original_text": "  Providing emotional support through dialogue systems is becoming increasingly\nimportant in today's world, as it can support both mental health and social\ninteractions in many conversation scenarios. Previous works have shown that\nusing persona is effective for generating empathetic and supportive responses.\nThey have often relied on pre-provided persona rather than inferring them\nduring conversations. However, it is not always possible to obtain a user\npersona before the conversation begins. To address this challenge, we propose\nPESS (Persona Extraction through Semantic Similarity), a novel framework that\ncan automatically infer informative and consistent persona from dialogues. We\ndevise completeness loss and consistency loss based on semantic similarity\nscores. The completeness loss encourages the model to generate missing persona\ninformation, and the consistency loss guides the model to distinguish between\nconsistent and inconsistent persona. Our experimental results demonstrate that\nhigh-quality persona information inferred by PESS is effective in generating\nemotionally supportive responses.\n", "rewritten_text": "\\boxed{As the importance of providing emotional support through dialogue systems grows, a key challenge is to infer user personas in real-time conversations. While previous works have shown that using personas can generate empathetic responses, they often rely on pre-provided personas. To address this limitation, we introduce PESS, a novel framework that can automatically extract informative and consistent personas from dialogues. Our approach uses semantic similarity scores to devise completeness and consistency losses, which encourage the model to generate missing persona information and distinguish between consistent and inconsistent personas. Our experiments demonstrate that PESS-inferred personas are effective in generating emotionally supportive responses.}"}, "1810.13391": {"original_text": "  During natural disasters and conflicts, information about what happened is\noften confusing, messy, and distributed across many sources. We would like to\nbe able to automatically identify relevant information and assemble it into\ncoherent narratives of what happened. To make this task accessible to neural\nmodels, we introduce Story Salads, mixtures of multiple documents that can be\ngenerated at scale. By exploiting the Wikipedia hierarchy, we can generate\nsalads that exhibit challenging inference problems. Story salads give rise to a\nnovel, challenging clustering task, where the objective is to group sentences\nfrom the same narratives. We demonstrate that simple bag-of-words similarity\nclustering falls short on this task and that it is necessary to take into\naccount global context and coherence.\n", "rewritten_text": "\\boxed{In the aftermath of natural disasters and conflicts, obtaining accurate information can be difficult due to the abundance of confusing and scattered sources. To address this, we propose the concept of \"Story Salads,\" which involves combining multiple documents to create coherent narratives. By leveraging Wikipedia's hierarchical structure, we can generate these \"salads\" on a large scale, presenting a novel and challenging clustering task. The goal is to group sentences from the same narrative together, which cannot be achieved through simple similarity clustering methods. Instead, it requires considering the global context and coherence of the information.}"}, "2407.02047": {"original_text": "  Multi-view counting (MVC) methods have shown their superiority over\nsingle-view counterparts, particularly in situations characterized by heavy\nocclusion and severe perspective distortions. However, hand-crafted heuristic\nfeatures and identical camera layout requirements in conventional MVC methods\nlimit their applicability and scalability in real-world scenarios.In this work,\nwe propose a concise 3D MVC framework called \\textbf{CountFormer}to elevate\nmulti-view image-level features to a scene-level volume representation and\nestimate the 3D density map based on the volume features. By incorporating a\ncamera encoding strategy, CountFormer successfully embeds camera parameters\ninto the volume query and image-level features, enabling it to handle various\ncamera layouts with significant differences.Furthermore, we introduce a feature\nlifting module capitalized on the attention mechanism to transform image-level\nfeatures into a 3D volume representation for each camera view. Subsequently,\nthe multi-view volume aggregation module attentively aggregates various\nmulti-view volumes to create a comprehensive scene-level volume representation,\nallowing CountFormer to handle images captured by arbitrary dynamic camera\nlayouts. The proposed method performs favorably against the state-of-the-art\napproaches across various widely used datasets, demonstrating its greater\nsuitability for real-world deployment compared to conventional MVC frameworks.\n", "rewritten_text": "\\boxed{\nMulti-view counting (MVC) methods have been shown to outperform single-view methods, especially in situations with heavy occlusion and severe perspective distortions. However, traditional MVC methods are limited by their reliance on hand-crafted features and identical camera layouts, making them less applicable and scalable in real-world scenarios. To address this, we propose CountFormer, a concise 3D MVC framework that elevates image-level features to a scene-level volume representation and estimates the 3D density map. By incorporating camera encoding and attention-based feature lifting, CountFormer can handle various camera layouts and dynamic camera movements. Our method outperforms state-of-the-art approaches on multiple datasets, demonstrating its greater suitability for real-world deployment."}, "2312.07530": {"original_text": "  Weakly supervised 3D object detection aims to learn a 3D detector with lower\nannotation cost, e.g., 2D labels. Unlike prior work which still relies on few\naccurate 3D annotations, we propose a framework to study how to leverage\nconstraints between 2D and 3D domains without requiring any 3D labels.\nSpecifically, we employ visual data from three perspectives to establish\nconnections between 2D and 3D domains. First, we design a feature-level\nconstraint to align LiDAR and image features based on object-aware regions.\nSecond, the output-level constraint is developed to enforce the overlap between\n2D and projected 3D box estimations. Finally, the training-level constraint is\nutilized by producing accurate and consistent 3D pseudo-labels that align with\nthe visual data. We conduct extensive experiments on the KITTI dataset to\nvalidate the effectiveness of the proposed three constraints. Without using any\n3D labels, our method achieves favorable performance against state-of-the-art\napproaches and is competitive with the method that uses 500-frame 3D\nannotations. Code will be made publicly available at\nhttps://github.com/kuanchihhuang/VG-W3D.\n", "rewritten_text": "\\boxed{\nWeakly supervised 3D object detection aims to reduce annotation costs by using lower-dimensional labels, such as 2D labels. Unlike previous approaches that rely on a few accurate 3D annotations, our proposed framework leverages connections between 2D and 3D domains without requiring any 3D labels. We achieve this by employing visual data from three perspectives: feature-level constraints to align LiDAR and image features, output-level constraints to enforce overlap between 2D and projected 3D box estimations, and training-level constraints to produce accurate and consistent 3D pseudo-labels. Our method, validated through extensive experiments on the KITTI dataset, achieves competitive performance against state-of-the-art approaches without using any 3D labels, and is comparable to methods that use 500-frame 3D annotations. The code is available at https://github.com/kuanchihhuang/VG-W3D.\n}"}, "2404.09976": {"original_text": "  Recently, diffusion transformers have gained wide attention with its\nexcellent performance in text-to-image and text-to-vidoe models, emphasizing\nthe need for transformers as backbone for diffusion models. Transformer-based\nmodels have shown better generalization capability compared to CNN-based models\nfor general vision tasks. However, much less has been explored in the existing\nliterature regarding the capabilities of transformer-based diffusion backbones\nand expanding their generative prowess to other datasets. This paper focuses on\nenabling a single pre-trained diffusion transformer model to scale across\nmultiple datasets swiftly, allowing for the completion of diverse generative\ntasks using just one model. To this end, we propose DiffScaler, an efficient\nscaling strategy for diffusion models where we train a minimal amount of\nparameters to adapt to different tasks. In particular, we learn task-specific\ntransformations at each layer by incorporating the ability to utilize the\nlearned subspaces of the pre-trained model, as well as the ability to learn\nadditional task-specific subspaces, which may be absent in the pre-training\ndataset. As these parameters are independent, a single diffusion model with\nthese task-specific parameters can be used to perform multiple tasks\nsimultaneously. Moreover, we find that transformer-based diffusion models\nsignificantly outperform CNN-based diffusion models methods while performing\nfine-tuning over smaller datasets. We perform experiments on four unconditional\nimage generation datasets. We show that using our proposed method, a single\npre-trained model can scale up to perform these conditional and unconditional\ntasks, respectively, with minimal parameter tuning while performing as close as\nfine-tuning an entire diffusion model for that particular task.\n", "rewritten_text": "\\boxed{\nRecently, diffusion transformers have shown exceptional performance in text-to-image and text-to-video models, highlighting the importance of transformers as the backbone for diffusion models. Transformer-based models have demonstrated better generalization capabilities compared to CNN-based models for various vision tasks. However, there is a lack of exploration in the existing literature regarding the capabilities of transformer-based diffusion backbones and their expansion to other datasets. This paper aims to enable a single pre-trained diffusion transformer model to adapt to multiple datasets efficiently, allowing it to perform diverse generative tasks with a single model. We propose DiffScaler, an efficient scaling strategy for diffusion models that involves training a minimal number of parameters to adapt to different tasks. Specifically, we learn task-specific transformations at each layer by leveraging the learned subspaces of the pre-trained model and learning additional task-specific subspaces that may be absent in the pre-training dataset. Since these parameters are independent, a single diffusion model with these task-specific parameters can perform multiple tasks simultaneously. Furthermore, we find that transformer-based diffusion models significantly outperform CNN-based diffusion models when fine-tuning on smaller datasets. We conduct experiments on four unconditional image generation datasets and demonstrate that our proposed method enables a single pre-trained model to adapt to perform conditional and unconditional tasks with minimal parameter tuning, achieving performance comparable to fine-tuning an entire diffusion model for each task.\n}"}, "2406.06843": {"original_text": "  We introduce a data capture system and a new dataset named HO-Cap that can be\nused to study 3D reconstruction and pose tracking of hands and objects in\nvideos. The capture system uses multiple RGB-D cameras and a HoloLens headset\nfor data collection, avoiding the use of expensive 3D scanners or mocap\nsystems. We propose a semi-automatic method to obtain annotations of shape and\npose of hands and objects in the collected videos, which significantly reduces\nthe required annotation time compared to manual labeling. With this system, we\ncaptured a video dataset of humans using objects to perform different tasks, as\nwell as simple pick-and-place and handover of an object from one hand to the\nother, which can be used as human demonstrations for embodied AI and robot\nmanipulation research. Our data capture setup and annotation framework can be\nused by the community to reconstruct 3D shapes of objects and human hands and\ntrack their poses in videos.\n", "rewritten_text": "\\boxed{We present a novel data capture system, HO-Cap, which enables the study of 3D reconstruction and pose tracking of hands and objects in videos. Our system utilizes multiple RGB-D cameras and a HoloLens headset, eliminating the need for costly 3D scanners or motion capture systems. We also propose a semi-automatic annotation method that significantly reduces the time required for labeling shape and pose data. Using this system, we have created a video dataset of humans performing various tasks with objects, including pick-and-place and handover actions, which can serve as demonstrations for embodied AI and robot manipulation research. Our data capture setup and annotation framework can be leveraged by the research community to reconstruct 3D object shapes and track hand and object poses in videos.}"}, "2407.05769": {"original_text": "  In autonomous driving, LiDAR sensors are vital for acquiring 3D point clouds,\nproviding reliable geometric information. However, traditional sampling methods\nof preprocessing often ignore semantic features, leading to detail loss and\nground point interference in 3D object detection. To address this, we propose a\nmulti-branch two-stage 3D object detection framework using a Semantic-aware\nMulti-branch Sampling (SMS) module and multi-view consistency constraints. The\nSMS module includes random sampling, Density Equalization Sampling (DES) for\nenhancing distant objects, and Ground Abandonment Sampling (GAS) to focus on\nnon-ground points. The sampled multi-view points are processed through a\nConsistent KeyPoint Selection (CKPS) module to generate consistent keypoint\nmasks for efficient proposal sampling. The first-stage detector uses\nmulti-branch parallel learning with multi-view consistency loss for feature\naggregation, while the second-stage detector fuses multi-view data through a\nMulti-View Fusion Pooling (MVFP) module to precisely predict 3D objects. The\nexperimental results on the KITTI dataset and Waymo Open Dataset show that our\nmethod achieves excellent detection performance improvement for a variety of\nbackbones, especially for low-performance backbones with the simple network\nstructures.\n", "rewritten_text": "\\boxed{\nIn autonomous driving, LiDAR sensors play a crucial role in capturing 3D point clouds, which provide essential geometric information. However, traditional preprocessing methods often overlook semantic features, resulting in loss of detail and interference from ground points in 3D object detection. To overcome this limitation, we introduce a novel two-stage 3D object detection framework that incorporates a Semantic-aware Multi-branch Sampling (SMS) module and multi-view consistency constraints. The SMS module combines random sampling, Density Equalization Sampling (DES) to enhance distant objects, and Ground Abandonment Sampling (GAS) to focus on non-ground points. The sampled points are then processed through a Consistent KeyPoint Selection (CKPS) module to generate consistent keypoint masks for efficient proposal sampling. The first-stage detector employs multi-branch parallel learning with multi-view consistency loss for feature aggregation, while the second-stage detector fuses multi-view data through a Multi-View Fusion Pooling (MVFP) module to accurately predict 3D objects. Experimental results on the KITTI dataset and Waymo Open Dataset demonstrate that our approach significantly improves detection performance for various backbones, particularly for low-performance backbones with simple network structures.\n}"}, "2211.0089": {"original_text": "  Few-shot learning problem focuses on recognizing unseen classes given a few\nlabeled images. In recent effort, more attention is paid to fine-grained\nfeature embedding, ignoring the relationship among different distance metrics.\nIn this paper, for the first time, we investigate the contributions of\ndifferent distance metrics, and propose an adaptive fusion scheme, bringing\nsignificant improvements in few-shot classification. We start from a naive\nbaseline of confidence summation and demonstrate the necessity of exploiting\nthe complementary property of different distance metrics. By finding the\ncompetition problem among them, built upon the baseline, we propose an Adaptive\nMetrics Module (AMM) to decouple metrics fusion into metric-prediction fusion\nand metric-losses fusion. The former encourages mutual complementary, while the\nlatter alleviates metric competition via multi-task collaborative learning.\nBased on AMM, we design a few-shot classification framework AMTNet, including\nthe AMM and the Global Adaptive Loss (GAL), to jointly optimize the few-shot\ntask and auxiliary self-supervised task, making the embedding features more\nrobust. In the experiment, the proposed AMM achieves 2% higher performance than\nthe naive metrics fusion module, and our AMTNet outperforms the\nstate-of-the-arts on multiple benchmark datasets.\n", "rewritten_text": "\\boxed{\nThe few-shot learning problem involves recognizing new classes with only a few labeled images. Recent research has focused on fine-grained feature embedding, overlooking the relationships between different distance metrics. This paper explores the contributions of various distance metrics and proposes an adaptive fusion scheme, leading to significant improvements in few-shot classification. We begin with a baseline of confidence summation and demonstrate the need to leverage the complementary properties of different distance metrics. By identifying the competition problem among them, we develop an Adaptive Metrics Module (AMM) that separates metrics fusion into metric-prediction fusion and metric-losses fusion. The former promotes mutual complementarity, while the latter mitigates metric competition through multi-task collaborative learning. Our proposed AMTNet framework, which includes AMM and Global Adaptive Loss (GAL), jointly optimizes the few-shot task and auxiliary self-supervised task, resulting in more robust embedding features. Experimental results show that AMM outperforms naive metrics fusion by 2%, and AMTNet surpasses state-of-the-art performance on multiple benchmark datasets.\n}"}, "2407.13520": {"original_text": "  3D deblurring reconstruction techniques have recently seen significant\nadvancements with the development of Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). Although these techniques can recover relatively\nclear 3D reconstructions from blurry image inputs, they still face limitations\nin handling severe blurring and complex camera motion. To address these issues,\nwe propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting\n(EaDeblur-GS), which integrates event camera data to enhance the robustness of\n3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE)\nnetwork to estimate Gaussian center deviations and using novel loss functions,\nEaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating\nperformance comparable to state-of-the-art methods.\n", "rewritten_text": "\\boxed{Recent breakthroughs in 3D deblurring reconstruction have been made with the introduction of Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). However, these methods still struggle with extreme blurring and complex camera movements. To overcome these limitations, we introduce Event-assisted 3D Deblur Reconstruction with Gaussian Splatting (EaDeblur-GS), which leverages event camera data to improve the robustness of 3DGS against motion blur. By utilizing an Adaptive Deviation Estimator (ADE) network to estimate Gaussian center deviations and novel loss functions, EaDeblur-GS achieves high-quality 3D reconstructions in real-time, rivaling the performance of state-of-the-art methods.}"}, "2103.13678": {"original_text": "  Domain Adaptation is widely used in practical applications of neural machine\ntranslation, which aims to achieve good performance on both the general-domain\nand in-domain. However, the existing methods for domain adaptation usually\nsuffer from catastrophic forgetting, domain divergence, and model explosion. To\naddress these three problems, we propose a method of \"divide and conquer\" which\nis based on the importance of neurons or parameters in the translation model.\nIn our method, we first prune the model and only keep the important neurons or\nparameters, making them responsible for both general-domain and in-domain\ntranslation. Then we further train the pruned model supervised by the original\nunpruned model with the knowledge distillation method. Last we expand the model\nto the original size and fine-tune the added parameters for the in-domain\ntranslation. We conduct experiments on different languages and domains and the\nresults show that our method can achieve significant improvements compared with\nseveral strong baselines.\n", "rewritten_text": "\\boxed{\nDomain adaptation is a crucial aspect of neural machine translation, as it enables models to perform well in both general and specific domains. However, existing methods often struggle with three key issues: catastrophic forgetting, domain divergence, and model explosion. To overcome these challenges, we propose a novel \"divide and conquer\" approach that prioritizes important neurons and parameters in the translation model. Our method involves three stages: first, we prune the model to retain only the essential components, which are then responsible for both general and specific domain translations. Next, we use knowledge distillation to further train the pruned model under the supervision of the original model. Finally, we expand the model to its original size and fine-tune the added parameters for specific domain translation. Our experiments across various languages and domains demonstrate significant improvements over several strong baselines.\n}"}, "2003.11536": {"original_text": "  Estimating the actual head orientation from 2D images, with regard to its\nthree degrees of freedom, is a well known problem that is highly significant\nfor a large number of applications involving head pose knowledge. Consequently,\nthis topic has been tackled by a plethora of methods and algorithms the most\npart of which exploits neural networks. Machine learning methods, indeed,\nachieve accurate head rotation values yet require an adequate training stage\nand, to that aim, a relevant number of positive and negative examples. In this\npaper we take a different approach to this topic by using fractal coding theory\nand particularly Partitioned Iterated Function Systems to extract the fractal\ncode from the input head image and to compare this representation to the\nfractal code of a reference model through Hamming distance. According to\nexperiments conducted on both the BIWI and the AFLW2000 databases, the proposed\nPIFS based head pose estimation method provides accurate yaw/pitch/roll angular\nvalues, with a performance approaching that of state of the art of\nmachine-learning based algorithms and exceeding most of non-training based\napproaches.\n", "rewritten_text": "\\boxed{Determining the exact orientation of a head from 2D images, which involves three degrees of freedom, is a crucial problem with numerous applications. While many methods, mostly relying on neural networks, have tackled this issue, they require extensive training data to achieve accurate results. This paper presents a novel approach using fractal coding theory and Partitioned Iterated Function Systems to extract a fractal code from the input head image and compare it to a reference model using Hamming distance. Experimental results on the BIWI and AFLW2000 databases show that this method provides accurate head pose estimation, rivaling machine-learning-based algorithms and outperforming most non-training-based approaches.}"}, "1706.02241": {"original_text": "  Analogy completion has been a popular task in recent years for evaluating the\nsemantic properties of word embeddings, but the standard methodology makes a\nnumber of assumptions about analogies that do not always hold, either in recent\nbenchmark datasets or when expanding into other domains. Through an analysis of\nanalogies in the biomedical domain, we identify three assumptions: that of a\nSingle Answer for any given analogy, that the pairs involved describe the Same\nRelationship, and that each pair is Informative with respect to the other. We\npropose modifying the standard methodology to relax these assumptions by\nallowing for multiple correct answers, reporting MAP and MRR in addition to\naccuracy, and using multiple example pairs. We further present BMASS, a novel\ndataset for evaluating linguistic regularities in biomedical embeddings, and\ndemonstrate that the relationships described in the dataset pose significant\nsemantic challenges to current word embedding methods.\n", "rewritten_text": "\\boxed{Evaluating the semantic properties of word embeddings through analogy completion has been a popular task, but the standard approach relies on three assumptions that don't always hold true, especially in biomedical domains. These assumptions are: 1) a single correct answer exists for each analogy, 2) the paired words describe the same relationship, and 3) each pair is informative about the other. To address these limitations, we propose a modified approach that allows for multiple correct answers, reports additional metrics like MAP and MRR, and uses multiple example pairs. We also introduce BMASS, a new dataset for evaluating biomedical word embeddings, which poses significant semantic challenges to current methods.}"}, "2312.16894": {"original_text": "  The widespread usage of cars and other large, heavy vehicles necessitates the\ndevelopment of an effective parking infrastructure. Additionally, algorithms\nfor detection and recognition of number plates are often used to identify\nautomobiles all around the world where standardized plate sizes and fonts are\nenforced, making recognition an effortless task. As a result, both kinds of\ndata can be combined to develop an intelligent parking system focuses on the\ntechnology of Automatic Number Plate Recognition (ANPR). Retrieving characters\nfrom an inputted number plate image is the sole purpose of ANPR which is a\ncostly procedure. In this article, we propose Chaurah, a minimal cost ANPR\nsystem that relies on a Raspberry Pi 3 that was specifically created for\nparking facilities. The system employs a dual-stage methodology, with the first\nstage being an ANPR system which makes use of two convolutional neural networks\n(CNNs). The primary locates and recognises license plates from a vehicle image,\nwhile the secondary performs Optical Character Recognition (OCR) to identify\nindividualized numbers from the number plate. An application built with Flutter\nand Firebase for database administration and license plate record comparison\nmakes up the second component of the overall solution. The application also\nacts as an user-interface for the billing mechanism based on parking time\nduration resulting in an all-encompassing software deployment of the study.\n", "rewritten_text": "\\boxed{\nThe need for efficient parking infrastructure is driven by the widespread use of cars and heavy vehicles. To address this, a combination of parking infrastructure and number plate recognition algorithms can be used to develop an intelligent parking system. This system, known as Automatic Number Plate Recognition (ANPR), involves retrieving characters from a number plate image, which can be a costly process. This article proposes a low-cost ANPR system, called Chaurah, specifically designed for parking facilities. Chaurah uses a Raspberry Pi 3 and a dual-stage methodology, consisting of two convolutional neural networks (CNNs) that locate and recognize license plates, and perform Optical Character Recognition (OCR) to identify individual numbers. The system also includes a Flutter and Firebase-based application for database management, license plate record comparison, and user interface for billing based on parking time duration, providing a comprehensive software solution.\n}"}, "2110.13032": {"original_text": "  Though there has been a large body of recent works in language modeling (LM)\nfor high resource languages such as English and Chinese, the area is still\nunexplored for low resource languages like Bengali and Hindi. We propose an end\nto end trainable memory efficient CNN architecture named CoCNN to handle\nspecific characteristics such as high inflection, morphological richness,\nflexible word order and phonetical spelling errors of Bengali and Hindi. In\nparticular, we introduce two learnable convolutional sub-models at word and at\nsentence level that are end to end trainable. We show that state-of-the-art\n(SOTA) Transformer models including pretrained BERT do not necessarily yield\nthe best performance for Bengali and Hindi. CoCNN outperforms pretrained BERT\nwith 16X less parameters, and it achieves much better performance than SOTA\nLSTM models on multiple real-world datasets. This is the first study on the\neffectiveness of different architectures drawn from three deep learning\nparadigms - Convolution, Recurrent, and Transformer neural nets for modeling\ntwo widely used languages, Bengali and Hindi.\n", "rewritten_text": "\\boxed{While language modeling has made significant progress for languages like English and Chinese, it remains a relatively unexplored area for low-resource languages such as Bengali and Hindi. To address this, we propose CoCNN, a novel, end-to-end trainable, and memory-efficient CNN architecture designed to handle the unique characteristics of Bengali and Hindi, including high inflection, morphological richness, flexible word order, and phonetical spelling errors. Our approach introduces two learnable convolutional sub-models at the word and sentence levels, which can be trained end-to-end. Notably, our experiments show that state-of-the-art Transformer models, including pre-trained BERT, do not necessarily achieve the best performance for Bengali and Hindi. In contrast, CoCNN outperforms pre-trained BERT with significantly fewer parameters (16 times less) and surpasses the performance of state-of-the-art LSTM models on multiple real-world datasets. This study is the first to investigate the effectiveness of different architectures from three deep learning paradigms - Convolution, Recurrent, and Transformer neural nets - for modeling Bengali and Hindi languages.}"}, "1305.4537": {"original_text": "  We describe a method for visual object detection based on an ensemble of\noptimized decision trees organized in a cascade of rejectors. The trees use\npixel intensity comparisons in their internal nodes and this makes them able to\nprocess image regions very fast. Experimental analysis is provided through a\nface detection problem. The obtained results are encouraging and demonstrate\nthat the method has practical value. Additionally, we analyse its sensitivity\nto noise and show how to perform fast rotation invariant object detection.\nComplete source code is provided at https://github.com/nenadmarkus/pico.\n", "rewritten_text": "\\boxed{We present a novel approach to visual object detection, leveraging an ensemble of optimized decision trees structured in a cascade of rejectors. By utilizing pixel intensity comparisons within the tree nodes, our method enables rapid processing of image regions. We validate our approach through a face detection experiment, yielding promising results that demonstrate its practical applicability. Furthermore, we examine the method's robustness to noise and explore techniques for achieving fast rotation-invariant object detection. The complete source code is available at https://github.com/nenadmarkus/pico.}"}, "2311.18835": {"original_text": "  Empowering models to dynamically accomplish tasks specified through natural\nlanguage instructions represents a promising path toward more capable and\ngeneral artificial intelligence. In this work, we introduce InstructSeq, an\ninstruction-conditioned multi-modal modeling framework that unifies diverse\nvision tasks through flexible natural language control and handling of both\nvisual and textual data. InstructSeq employs a multimodal transformer\narchitecture encompassing visual, language, and sequential modeling. We utilize\na visual encoder to extract image features and a text encoder to encode\ninstructions. An autoregressive transformer fuses the representations and\ngenerates sequential task outputs. By training with LLM-generated natural\nlanguage instructions, InstructSeq acquires a strong comprehension of free-form\ninstructions for specifying visual tasks. This provides an intuitive interface\nfor directing capabilities using flexible natural instructions. Without any\ntask-specific tuning, InstructSeq achieves compelling performance on semantic\nsegmentation, referring expression segmentation/comprehension, and image\ncaptioning. The flexible control and multi-task unification empower the model\nwith more human-like versatility and generalizability for computer vision. The\ncode will be released soon at https://github.com/rongyaofang/InstructSeq.\n", "rewritten_text": "\\boxed{\nEnabling models to dynamically perform tasks based on natural language instructions is a promising approach to creating more capable and general artificial intelligence. We propose InstructSeq, a multi-modal modeling framework that unifies various vision tasks through flexible natural language control and handling of both visual and textual data. Our framework uses a multimodal transformer architecture that combines visual, language, and sequential modeling. We extract image features using a visual encoder and encode instructions using a text encoder. An autoregressive transformer then fuses the representations and generates sequential task outputs. By training on natural language instructions generated by large language models, InstructSeq develops a strong understanding of free-form instructions for specifying visual tasks, providing an intuitive interface for directing capabilities using flexible natural language instructions. Without task-specific fine-tuning, InstructSeq achieves impressive performance on semantic segmentation, referring expression segmentation/comprehension, and image captioning. The flexible control and multi-task unification enable the model to exhibit more human-like versatility and generalizability in computer vision. The code will be available soon at https://github.com/rongyaofang/InstructSeq.\n}"}, "2008.07018": {"original_text": "  We present AutoPose, a novel neural architecture search(NAS) framework that\nis capable of automatically discovering multiple parallel branches of\ncross-scale connections towards accurate and high-resolution 2D human pose\nestimation. Recently, high-performance hand-crafted convolutional networks for\npose estimation show growing demands on multi-scale fusion and high-resolution\nrepresentations. However, current NAS works exhibit limited flexibility on\nscale searching, they dominantly adopt simplified search spaces of\nsingle-branch architectures. Such simplification limits the fusion of\ninformation at different scales and fails to maintain high-resolution\nrepresentations. The presentedAutoPose framework is able to search for\nmulti-branch scales and network depth, in addition to the cell-level\nmicrostructure. Motivated by the search space, a novel bi-level optimization\nmethod is presented, where the network-level architecture is searched via\nreinforcement learning, and the cell-level search is conducted by the\ngradient-based method. Within 2.5 GPU days, AutoPose is able to find very\ncompetitive architectures on the MS COCO dataset, that are also transferable to\nthe MPII dataset. Our code is available at\nhttps://github.com/VITA-Group/AutoPose.\n", "rewritten_text": "\\boxed{We introduce AutoPose, a novel neural architecture search framework that can automatically discover complex network architectures with multiple parallel branches of cross-scale connections, enabling accurate and high-resolution 2D human pose estimation. Unlike existing NAS methods that are limited to simplified search spaces and single-branch architectures, AutoPose can search for multi-branch scales and network depth, as well as cell-level microstructure. Our framework uses a bi-level optimization method, combining reinforcement learning for network-level architecture search and gradient-based methods for cell-level search. In just 2.5 GPU days, AutoPose can find competitive architectures on the MS COCO dataset that are also transferable to the MPII dataset. Our code is available at https://github.com/VITA-Group/AutoPose.}"}, "1902.053": {"original_text": "  Deep learning, due to its unprecedented success in tasks such as image\nclassification, has emerged as a new tool in image reconstruction with\npotential to change the field. In this paper we demonstrate a crucial\nphenomenon: deep learning typically yields unstablemethods for image\nreconstruction. The instabilities usually occur in several forms: (1) tiny,\nalmost undetectable perturbations, both in the image and sampling domain, may\nresult in severe artefacts in the reconstruction, (2) a small structural\nchange, for example a tumour, may not be captured in the reconstructed image\nand (3) (a counterintuitive type of instability) more samples may yield poorer\nperformance. Our new stability test with algorithms and easy to use software\ndetects the instability phenomena. The test is aimed at researchers to test\ntheir networks for instabilities and for government agencies, such as the Food\nand Drug Administration (FDA), to secure safe use of deep learning methods.\n", "rewritten_text": "\\boxed{Deep learning has revolutionized image reconstruction, but it often yields unstable methods that can produce severe artefacts, miss critical details, or even perform worse with more data. This paper highlights these instabilities, which can manifest in three ways: tiny perturbations causing significant errors, small structural changes being overlooked, and increased sampling leading to poorer performance. To address this, we introduce a new stability test, accompanied by algorithms and user-friendly software, to help researchers identify and mitigate these instabilities, ensuring the safe and reliable use of deep learning methods, particularly in regulated industries such as healthcare.}"}, "2203.08543": {"original_text": "  Deep Metric Learning (DML) proposes to learn metric spaces which encode\nsemantic similarities as embedding space distances. These spaces should be\ntransferable to classes beyond those seen during training. Commonly, DML\nmethods task networks to solve contrastive ranking tasks defined over binary\nclass assignments. However, such approaches ignore higher-level semantic\nrelations between the actual classes. This causes learned embedding spaces to\nencode incomplete semantic context and misrepresent the semantic relation\nbetween classes, impacting the generalizability of the learned metric space. To\ntackle this issue, we propose a language guidance objective for visual\nsimilarity learning. Leveraging language embeddings of expert- and\npseudo-classnames, we contextualize and realign visual representation spaces\ncorresponding to meaningful language semantics for better semantic consistency.\nExtensive experiments and ablations provide a strong motivation for our\nproposed approach and show language guidance offering significant,\nmodel-agnostic improvements for DML, achieving competitive and state-of-the-art\nresults on all benchmarks. Code available at\nhttps://github.com/ExplainableML/LanguageGuidance_for_DML.\n", "rewritten_text": "\\boxed{\nDeep Metric Learning (DML) aims to create metric spaces that capture semantic similarities as distances in the embedding space, which should be applicable to unseen classes. However, traditional DML methods, which rely on contrastive ranking tasks with binary class assignments, overlook higher-level semantic relationships between classes. This limitation leads to incomplete semantic context and misrepresentation of semantic relationships, affecting the generalizability of the learned metric space. To address this issue, we introduce a language guidance objective for visual similarity learning, which leverages language embeddings of expert- and pseudo-classnames to contextualize and realign visual representation spaces with meaningful language semantics. Our approach yields significant, model-agnostic improvements for DML, achieving competitive and state-of-the-art results on all benchmarks, as demonstrated by extensive experiments and ablations. The code is available at https://github.com/ExplainableML/LanguageGuidance_for_DML.\n}"}, "1411.5057": {"original_text": "  In this paper, we propose a novel algorithm for analysis-based sparsity\nreconstruction. It can solve the generalized problem by structured sparsity\nregularization with an orthogonal basis and total variation regularization. The\nproposed algorithm is based on the iterative reweighted least squares (IRLS)\nmodel, which is further accelerated by the preconditioned conjugate gradient\nmethod. The convergence rate of the proposed algorithm is almost the same as\nthat of the traditional IRLS algorithms, that is, exponentially fast. Moreover,\nwith the specifically devised preconditioner, the computational cost for each\niteration is significantly less than that of traditional IRLS algorithms, which\nenables our approach to handle large scale problems. In addition to the fast\nconvergence, it is straightforward to apply our method to standard sparsity,\ngroup sparsity, overlapping group sparsity and TV based problems. Experiments\nare conducted on a practical application: compressive sensing magnetic\nresonance imaging. Extensive results demonstrate that the proposed algorithm\nachieves superior performance over 14 state-of-the-art algorithms in terms of\nboth accuracy and computational cost.\n", "rewritten_text": "\\boxed{This paper introduces a new algorithm for reconstructing sparse signals using analysis-based methods. The algorithm combines structured sparsity regularization with an orthogonal basis and total variation regularization, and is based on the iterative reweighted least squares (IRLS) model. By incorporating a preconditioned conjugate gradient method, the algorithm achieves exponential convergence rates similar to traditional IRLS algorithms, but with significantly reduced computational cost per iteration. This enables the algorithm to handle large-scale problems efficiently. The approach is versatile and can be easily applied to various sparsity models, including standard, group, and overlapping group sparsity, as well as TV-based problems. Experimental results on compressive sensing magnetic resonance imaging demonstrate the algorithm's superior performance over 14 state-of-the-art algorithms in terms of both accuracy and computational cost.}"}, "2211.09809": {"original_text": "  Animating portraits using speech has received growing attention in recent\nyears, with various creative and practical use cases. An ideal generated video\nshould have good lip sync with the audio, natural facial expressions and head\nmotions, and high frame quality. In this work, we present SPACE, which uses\nspeech and a single image to generate high-resolution, and expressive videos\nwith realistic head pose, without requiring a driving video. It uses a\nmulti-stage approach, combining the controllability of facial landmarks with\nthe high-quality synthesis power of a pretrained face generator. SPACE also\nallows for the control of emotions and their intensities. Our method\noutperforms prior methods in objective metrics for image quality and facial\nmotions and is strongly preferred by users in pair-wise comparisons. The\nproject website is available at https://deepimagination.cc/SPACE/\n", "rewritten_text": "\\boxed{Recently, animating portraits using speech has gained significant attention, with various creative and practical applications. The ideal outcome is a video with synchronized lip movements, natural facial expressions, and high-quality frames. This work introduces SPACE, a system that generates high-resolution, expressive videos with realistic head movements from a single image and speech input, without requiring a reference video. SPACE employs a multi-stage approach, combining facial landmark control with a pre-trained face generator, and allows for emotional intensity control. Our method surpasses previous techniques in image quality and facial motion metrics, and is strongly preferred by users in comparative evaluations. The project website can be accessed at https://deepimagination.cc/SPACE/.}"}, "2307.03602": {"original_text": "  Stereo vision systems have become popular in computer vision applications,\nsuch as 3D reconstruction, object tracking, and autonomous navigation. However,\ntraditional stereo vision systems that use rectilinear lenses may not be\nsuitable for certain scenarios due to their limited field of view. This has led\nto the popularity of vision systems based on one or multiple fisheye cameras in\ndifferent orientations, which can provide a field of view of 180x180 degrees or\nmore. However, fisheye cameras introduce significant distortion at the edges\nthat affects the accuracy of stereo matching and depth estimation. To overcome\nthese limitations, this paper proposes a method for distortion-removal and\ndepth estimation analysis for stereovision system using orthogonally divergent\nfisheye cameras (ODFC). The proposed method uses two virtual pinhole cameras\n(VPC), each VPC captures a small portion of the original view and presents it\nwithout any lens distortions, emulating the behavior of a pinhole camera. By\ncarefully selecting the captured regions, it is possible to create a stereo\npair using two VPCs. The performance of the proposed method is evaluated in\nboth simulation using virtual environment and experiments using real cameras\nand their results compared to stereo cameras with parallel optical axes. The\nresults demonstrate the effectiveness of the proposed method in terms of\ndistortion removal and depth estimation accuracy.\n", "rewritten_text": "\\boxed{\nStereo vision systems have gained popularity in computer vision applications, including 3D reconstruction, object tracking, and autonomous navigation. However, traditional systems using rectilinear lenses have limited fields of view, making them unsuitable for certain scenarios. To address this, fisheye cameras have been used, offering a wider field of view of up to 180x180 degrees. Nevertheless, fisheye cameras introduce significant distortion at the edges, affecting stereo matching and depth estimation accuracy. This paper proposes a novel method to remove distortion and improve depth estimation in stereovision systems using orthogonally divergent fisheye cameras (ODFC). The approach employs two virtual pinhole cameras, each capturing a small, undistorted portion of the original view, allowing for the creation of a stereo pair. The method's performance is evaluated through simulations and real-camera experiments, demonstrating its effectiveness in removing distortion and improving depth estimation accuracy compared to traditional stereo cameras.\n}"}, "2410.11816": {"original_text": "  The automatic assembly problem has attracted increasing interest due to its\ncomplex challenges that involve 3D representation. This paper introduces\nJigsaw++, a novel generative method designed to tackle the multifaceted\nchallenges of reconstruction for the reassembly problem. Existing approach\nfocusing primarily on piecewise information for both part and fracture\nassembly, often overlooking the integration of complete object prior. Jigsaw++\ndistinguishes itself by learning a category-agnostic shape prior of complete\nobjects. It employs the proposed \"retargeting\" strategy that effectively\nleverages the output of any existing assembly method to generate complete shape\nreconstructions. This capability allows it to function orthogonally to the\ncurrent methods. Through extensive evaluations on Breaking Bad dataset and\nPartNet, Jigsaw++ has demonstrated its effectiveness, reducing reconstruction\nerrors and enhancing the precision of shape reconstruction, which sets a new\ndirection for future reassembly model developments.\n", "rewritten_text": "\\boxed{The automatic assembly problem, which involves complex 3D representation challenges, has gained significant attention. This paper presents Jigsaw++, a novel generative method that addresses the multifaceted reconstruction challenges of the reassembly problem. Unlike existing approaches that focus on piecewise information, Jigsaw++ learns a category-agnostic shape prior of complete objects and employs a \"retargeting\" strategy to leverage the output of any existing assembly method, generating complete shape reconstructions. Through evaluations on the Breaking Bad dataset and PartNet, Jigsaw++ has shown its effectiveness in reducing reconstruction errors and enhancing shape reconstruction precision, paving the way for future reassembly model developments.}"}, "1806.05658": {"original_text": "  Seq2seq learning has produced promising results on summarization. However, in\nmany cases, system summaries still struggle to keep the meaning of the original\nintact. They may miss out important words or relations that play critical roles\nin the syntactic structure of source sentences. In this paper, we present\nstructure-infused copy mechanisms to facilitate copying important words and\nrelations from the source sentence to summary sentence. The approach naturally\ncombines source dependency structure with the copy mechanism of an abstractive\nsentence summarizer. Experimental results demonstrate the effectiveness of\nincorporating source-side syntactic information in the system, and our proposed\napproach compares favorably to state-of-the-art methods.\n", "rewritten_text": "\\boxed{Seq2seq learning has shown promise in summarization, but often fails to preserve the original meaning. To address this, we introduce a novel approach that incorporates source sentence structure into the summarization process, ensuring critical words and relationships are retained. By combining source dependency structure with an abstractive sentence summarizer, our method outperforms state-of-the-art techniques, as demonstrated by experimental results.}"}, "2108.12545": {"original_text": "  Training deep networks for semantic segmentation requires large amounts of\nlabeled training data, which presents a major challenge in practice, as\nlabeling segmentation masks is a highly labor-intensive process. To address\nthis issue, we present a framework for semi-supervised and domain-adaptive\nsemantic segmentation, which is enhanced by self-supervised monocular depth\nestimation (SDE) trained only on unlabeled image sequences.\n  In particular, we utilize SDE as an auxiliary task comprehensively across the\nentire learning framework: First, we automatically select the most useful\nsamples to be annotated for semantic segmentation based on the correlation of\nsample diversity and difficulty between SDE and semantic segmentation. Second,\nwe implement a strong data augmentation by mixing images and labels using the\ngeometry of the scene. Third, we transfer knowledge from features learned\nduring SDE to semantic segmentation by means of transfer and multi-task\nlearning. And fourth, we exploit additional labeled synthetic data with\nCross-Domain DepthMix and Matching Geometry Sampling to align synthetic and\nreal data.\n  We validate the proposed model on the Cityscapes dataset, where all four\ncontributions demonstrate significant performance gains, and achieve\nstate-of-the-art results for semi-supervised semantic segmentation as well as\nfor semi-supervised domain adaptation. In particular, with only 1/30 of the\nCityscapes labels, our method achieves 92% of the fully-supervised baseline\nperformance and even 97% when exploiting additional data from GTA. The source\ncode is available at\nhttps://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth.\n", "rewritten_text": "\\boxed{\nTraining deep networks for semantic segmentation is challenging due to the need for large amounts of labeled training data, which is time-consuming to obtain. To overcome this, we propose a framework that combines semi-supervised and domain-adaptive semantic segmentation with self-supervised monocular depth estimation (SDE) using unlabeled image sequences. Our approach leverages SDE in four ways: (1) selecting the most useful samples for annotation, (2) implementing strong data augmentation, (3) transferring knowledge from SDE to semantic segmentation, and (4) aligning synthetic and real data using Cross-Domain DepthMix and Matching Geometry Sampling. We validate our model on the Cityscapes dataset, achieving state-of-the-art results for semi-supervised semantic segmentation and domain adaptation, with significant performance gains. Notably, our method achieves 92% of the fully-supervised baseline performance with only 1/30 of the Cityscapes labels, and 97% when using additional data from GTA. The source code is available at https://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth.\n}"}, "2405.12833": {"original_text": "  Automatic radiology report generation can alleviate the workload for\nphysicians and minimize regional disparities in medical resources, therefore\nbecoming an important topic in the medical image analysis field. It is a\nchallenging task, as the computational model needs to mimic physicians to\nobtain information from multi-modal input data (i.e., medical images, clinical\ninformation, medical knowledge, etc.), and produce comprehensive and accurate\nreports. Recently, numerous works emerged to address this issue using deep\nlearning-based methods, such as transformers, contrastive learning, and\nknowledge-base construction. This survey summarizes the key techniques\ndeveloped in the most recent works and proposes a general workflow for deep\nlearning-based report generation with five main components, including\nmulti-modality data acquisition, data preparation, feature learning, feature\nfusion/interaction, and report generation. The state-of-the-art methods for\neach of these components are highlighted. Additionally, training strategies,\npublic datasets, evaluation methods, current challenges, and future directions\nin this field are summarized. We have also conducted a quantitative comparison\nbetween different methods under the same experimental setting. This is the most\nup-to-date survey that focuses on multi-modality inputs and data fusion for\nradiology report generation. The aim is to provide comprehensive and rich\ninformation for researchers interested in automatic clinical report generation\nand medical image analysis, especially when using multimodal inputs, and assist\nthem in developing new algorithms to advance the field.\n", "rewritten_text": "\\boxed{\nAutomating radiology report generation is crucial for reducing physicians' workload and bridging regional gaps in medical resources. However, this task is complex, requiring computational models to mimic physicians in extracting information from diverse data sources (medical images, clinical information, medical knowledge, etc.) and producing accurate, comprehensive reports. Recent deep learning-based approaches, including transformers, contrastive learning, and knowledge-base construction, have emerged to tackle this challenge. This survey provides an overview of the latest techniques, proposing a five-component workflow for deep learning-based report generation: data acquisition, preparation, feature learning, feature fusion/interaction, and report generation. The survey highlights state-of-the-art methods for each component, as well as training strategies, public datasets, evaluation methods, current challenges, and future directions. A quantitative comparison of different methods under the same experimental setting is also presented. This comprehensive survey aims to provide valuable insights for researchers in automatic clinical report generation and medical image analysis, particularly when working with multimodal inputs, and to inspire the development of new algorithms to advance the field."}, "1809.10417": {"original_text": "  The tracking-by-detection framework receives growing attentions through the\nintegration with the Convolutional Neural Networks (CNNs). Existing\ntracking-by-detection based methods, however, fail to track objects with severe\nappearance variations. This is because the traditional convolutional operation\nis performed on fixed grids, and thus may not be able to find the correct\nresponse while the object is changing pose or under varying environmental\nconditions. In this paper, we propose a deformable convolution layer to enrich\nthe target appearance representations in the tracking-by-detection framework.\nWe aim to capture the target appearance variations via deformable convolution,\nwhich adaptively enhances its original features. In addition, we also propose a\ngated fusion scheme to control how the variations captured by the deformable\nconvolution affect the original appearance. The enriched feature representation\nthrough deformable convolution facilitates the discrimination of the CNN\nclassifier on the target object and background. Extensive experiments on the\nstandard benchmarks show that the proposed tracker performs favorably against\nstate-of-the-art methods.\n", "rewritten_text": "\\boxed{\nThe tracking-by-detection framework has gained popularity with the integration of Convolutional Neural Networks (CNNs). However, existing methods struggle to track objects with significant appearance changes. This limitation arises from traditional convolutional operations being performed on fixed grids, making it challenging to detect objects under varying poses or environmental conditions. To address this, we introduce a deformable convolution layer to enhance target appearance representations in the tracking-by-detection framework. This layer captures appearance variations by adaptively refining original features. Additionally, we propose a gated fusion scheme to regulate the impact of captured variations on the original appearance. The enriched feature representation enables the CNN classifier to better distinguish between the target object and background. Our experiments on standard benchmarks demonstrate that the proposed tracker outperforms state-of-the-art methods.\n}"}, "2111.15603": {"original_text": "  Modern neural networks are able to perform at least as well as humans in\nnumerous tasks involving object classification and image generation. However,\nsmall perturbations which are imperceptible to humans may significantly degrade\nthe performance of well-trained deep neural networks. We provide a\nDistributionally Robust Optimization (DRO) framework which integrates\nhuman-based image quality assessment methods to design optimal attacks that are\nimperceptible to humans but significantly damaging to deep neural networks.\nThrough extensive experiments, we show that our attack algorithm generates\nbetter-quality (less perceptible to humans) attacks than other state-of-the-art\nhuman imperceptible attack methods. Moreover, we demonstrate that DRO training\nusing our optimally designed human imperceptible attacks can improve group\nfairness in image classification. Towards the end, we provide an algorithmic\nimplementation to speed up DRO training significantly, which could be of\nindependent interest.\n", "rewritten_text": "\\boxed{Neural networks have achieved human-level performance in object classification and image generation tasks, but they can be vulnerable to small, imperceptible perturbations that significantly impact their performance. To address this, we propose a Distributionally Robust Optimization (DRO) framework that incorporates human-based image quality assessment methods to design optimal attacks that are imperceptible to humans but damaging to neural networks. Our experiments show that our attack algorithm generates higher-quality, less perceptible attacks than existing methods, and that DRO training using these attacks can improve group fairness in image classification. Additionally, we provide an algorithmic implementation to significantly speed up DRO training, which has independent value.}"}, "2404.00829": {"original_text": "  Human writers often bookend their writing with ending sentences that relate\nback to the beginning sentences in order to compose a satisfying narrative that\n\"closes the loop.\" Motivated by this observation, we propose RENarGen, a\ncontrollable story-generation paradigm that generates narratives by ensuring\nthe first and last sentences are related and then infilling the middle\nsentences. Our contributions include an initial exploration of how various\nmethods of bookending from Narratology affect language modeling for stories.\nAutomatic and human evaluations indicate RENarGen produces better stories with\nmore narrative closure than current autoregressive models.\n", "rewritten_text": "\\boxed{Inspired by the common literary technique of using related opening and closing sentences to create a cohesive narrative, we introduce RENarGen, a novel approach to story generation that focuses on linking the first and last sentences before filling in the middle. Our research explores how different bookending methods from Narratology impact language modeling for storytelling. Both automated and human evaluations demonstrate that RENarGen produces more engaging stories with greater narrative closure compared to existing autoregressive models.}"}, "1711.01062": {"original_text": "  With the development of depth cameras such as Kinect and Intel Realsense,\nRGB-D based human detection receives continuous research attention due to its\nusage in a variety of applications. In this paper, we propose a new\nMulti-Glimpse LSTM (MG-LSTM) network, in which multi-scale contextual\ninformation is sequentially integrated to promote the human detection\nperformance. Furthermore, we propose a feature fusion strategy based on our\nMG-LSTM network to better incorporate the RGB and depth information. To the\nbest of our knowledge, this is the first attempt to utilize LSTM structure for\nRGB-D based human detection. Our method achieves superior performance on two\npublicly available datasets.\n", "rewritten_text": "\\boxed{The development of depth cameras like Kinect and Intel Realsense has led to ongoing research in RGB-D based human detection, which has various applications. This paper introduces a novel Multi-Glimpse LSTM (MG-LSTM) network that integrates multi-scale contextual information sequentially to enhance human detection performance. Additionally, a feature fusion strategy is proposed to effectively combine RGB and depth information using the MG-LSTM network. This is the first time an LSTM structure has been used for RGB-D based human detection, and our approach achieves superior results on two publicly available datasets.}"}, "1810.07599": {"original_text": "  As facial appearance is subject to significant intra-class variations caused\nby the aging process over time, age-invariant face recognition (AIFR) remains a\nmajor challenge in face recognition community. To reduce the intra-class\ndiscrepancy caused by the aging, in this paper we propose a novel approach\n(namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep\nface features. Specifically, we decompose deep face features into two\northogonal components to represent age-related and identity-related features.\nAs a result, identity-related features that are robust to aging are then used\nfor AIFR. Besides, for complementing the existing cross-age datasets and\nadvancing the research in this field, we construct a brand-new large-scale\nCross-Age Face dataset (CAF). Extensive experiments conducted on the three\npublic domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have\nshown the effectiveness of the proposed approach and the value of the\nconstructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most\npopular general face recognition (GFR) dataset LFW additionally demonstrates\nthe comparable generalization performance on GFR.\n", "rewritten_text": "\\boxed{\nFacial recognition systems struggle to accurately identify individuals as they age due to significant changes in appearance over time. To address this challenge, we propose a novel approach called Orthogonal Embedding CNNs (OE-CNNs) that learns to extract age-invariant deep face features. Our method decomposes face features into two orthogonal components, one representing age-related characteristics and the other representing identity-related features that are robust to aging. We also introduce a new large-scale Cross-Age Face dataset (CAF) to complement existing datasets and advance research in this field. Experimental results on three public face aging datasets and a popular general face recognition dataset demonstrate the effectiveness of our approach and the value of the CAF dataset for age-invariant face recognition.\n}"}, "2107.005": {"original_text": "  Driven by recent advances in object detection with deep neural networks, the\ntracking-by-detection paradigm has gained increasing prevalence in the research\ncommunity of multi-object tracking (MOT). It has long been known that\nappearance information plays an essential role in the detection-to-track\nassociation, which lies at the core of the tracking-by-detection paradigm.\nWhile most existing works consider the appearance distances between the\ndetections and the tracks, they ignore the statistical information implied by\nthe historical appearance distance records in the tracks, which can be\nparticularly useful when a detection has similar distances with two or more\ntracks. In this work, we propose a hybrid track association (HTA) algorithm\nthat models the historical appearance distances of a track with an incremental\nGaussian mixture model (IGMM) and incorporates the derived statistical\ninformation into the calculation of the detection-to-track association cost.\nExperimental results on three MOT benchmarks confirm that HTA effectively\nimproves the target identification performance with a small compromise to the\ntracking speed. Additionally, compared to many state-of-the-art trackers, the\nDeepSORT tracker equipped with HTA achieves better or comparable performance in\nterms of the balance of tracking quality and speed.\n", "rewritten_text": "\\boxed{\nRecent advances in object detection using deep neural networks have led to the popularity of the tracking-by-detection approach in multi-object tracking (MOT) research. The key to this approach is associating detections with tracks, which relies heavily on appearance information. While existing methods focus on the distance between detections and tracks, they overlook the valuable statistical information contained in a track's historical appearance distance records. This omission is particularly problematic when a detection has similar distances to multiple tracks. To address this, we propose a hybrid track association (HTA) algorithm that uses an incremental Gaussian mixture model to capture a track's historical appearance distances and incorporates this statistical information into the detection-to-track association cost calculation. Our experiments on three MOT benchmarks demonstrate that HTA improves target identification performance with a minimal sacrifice in tracking speed. Furthermore, the DeepSORT tracker equipped with HTA achieves comparable or better performance than many state-of-the-art trackers in terms of balancing tracking quality and speed.\n}"}, "1711.08879": {"original_text": "  Objects for detection usually have distinct characteristics in different\nsub-regions and different aspect ratios. However, in prevalent two-stage object\ndetection methods, Region-of-Interest (RoI) features are extracted by RoI\npooling with little emphasis on these translation-variant feature components.\nWe present feature selective networks to reform the feature representations of\nRoIs by exploiting their disparities among sub-regions and aspect ratios. Our\nnetwork produces the sub-region attention bank and aspect ratio attention bank\nfor the whole image. The RoI-based sub-region attention map and aspect ratio\nattention map are selectively pooled from the banks, and then used to refine\nthe original RoI features for RoI classification. Equipped with a light-weight\ndetection subnetwork, our network gets a consistent boost in detection\nperformance based on general ConvNet backbones (ResNet-101, GoogLeNet and\nVGG-16). Without bells and whistles, our detectors equipped with ResNet-101\nachieve more than 3% mAP improvement compared to counterparts on PASCAL VOC\n2007, PASCAL VOC 2012 and MS COCO datasets.\n", "rewritten_text": "\\boxed{Current two-stage object detection methods, which use Region-of-Interest (RoI) pooling, often overlook the unique characteristics of objects in different sub-regions and aspect ratios. To address this, we propose feature selective networks that reform RoI feature representations by leveraging their disparities. Our network generates attention banks for sub-regions and aspect ratios, which are then selectively pooled and used to refine original RoI features for classification. When combined with a lightweight detection subnetwork, our approach consistently improves detection performance using general ConvNet backbones, achieving over 3% mAP improvement on PASCAL VOC and MS COCO datasets without additional modifications.}"}, "2402.02145": {"original_text": "  In today's media landscape, where news outlets play a pivotal role in shaping\npublic opinion, it is imperative to address the issue of sentiment manipulation\nwithin news text. News writers often inject their own biases and emotional\nlanguage, which can distort the objectivity of reporting. This paper introduces\na novel approach to tackle this problem by reducing the polarity of latent\nsentiments in news content. Drawing inspiration from adversarial attack-based\nsentence perturbation techniques and a prompt based method using ChatGPT, we\nemploy transformation constraints to modify sentences while preserving their\ncore semantics. Using three perturbation methods: replacement, insertion, and\ndeletion coupled with a context-aware masked language model, we aim to maximize\nthe desired sentiment score for targeted news aspects through a beam search\nalgorithm. Our experiments and human evaluations demonstrate the effectiveness\nof these two models in achieving reduced sentiment polarity with minimal\nmodifications while maintaining textual similarity, fluency, and grammatical\ncorrectness. Comparative analysis confirms the competitive performance of the\nadversarial attack based perturbation methods and prompt-based methods,\noffering a promising solution to foster more objective news reporting and\ncombat emotional language bias in the media.\n", "rewritten_text": "\\boxed{In today's media, news outlets significantly influence public opinion, making it crucial to address the issue of sentiment manipulation in news articles. Journalists often inject their biases and emotional language, compromising the objectivity of reporting. This study proposes a novel approach to tackle this problem by reducing the emotional tone of latent sentiments in news content. By drawing on techniques used in adversarial attacks and ChatGPT, we modify sentences to preserve their core meaning while minimizing emotional language. Our experiments and human evaluations show that our two models effectively reduce sentiment polarity with minimal changes, maintaining textual similarity, fluency, and grammatical correctness. Our comparative analysis confirms that both models perform competitively, offering a promising solution to promote more objective news reporting and combat emotional language bias in the media.}"}, "2312.10437": {"original_text": "  Tender notices are usually sought by most of the companies at regular\nintervals as a means for obtaining the contracts of various projects. These\nnotices consist of all the required information like description of the work,\nperiod of construction, estimated amount of project, etc. In the context of\nNepal, tender notices are usually published in national as well as local\nnewspapers. The interested bidders should search all the related tender notices\nin newspapers. However, it is very tedious for these companies to manually\nsearch tender notices in every newspaper and figure out which bid is best\nsuited for them. This project is built with the purpose of solving this tedious\ntask of manually searching the tender notices. Initially, the newspapers are\ndownloaded in PDF format using the selenium library of python. After\ndownloading the newspapers, the e-papers are scanned and tender notices are\nautomatically extracted using a neural network. For extraction purposes,\ndifferent architectures of CNN namely ResNet, GoogleNet and Xception are used\nand a model with highest performance has been implemented. Finally, these\nextracted notices are then published on the website and are accessible to the\nusers. This project is helpful for construction companies as well as\ncontractors assuring quality and efficiency. This project has great application\nin the field of competitive bidding as well as managing them in a systematic\nmanner.\n", "rewritten_text": "\\boxed{\nCompanies regularly seek tender notices to secure project contracts, which typically include details such as project descriptions, construction periods, and estimated costs. In Nepal, these notices are usually published in national and local newspapers. However, manually searching for relevant tender notices in every newspaper can be a tedious task for interested bidders. To address this issue, this project aims to automate the process of searching and extracting tender notices. The project involves downloading newspapers in PDF format using Python's selenium library, scanning the e-papers using a neural network, and extracting tender notices using various CNN architectures such as ResNet, GoogleNet, and Xception. The extracted notices are then published on a website, making them easily accessible to users. This project is beneficial for construction companies and contractors, ensuring quality and efficiency in the competitive bidding process.\n}"}, "2405.18132": {"original_text": "  In recent years, the increasing demand for dynamic 3D assets in design and\ngaming applications has given rise to powerful generative pipelines capable of\nsynthesizing high-quality 4D objects. Previous methods generally rely on score\ndistillation sampling (SDS) algorithm to infer the unseen views and motion of\n4D objects, thus leading to unsatisfactory results with defects like\nover-saturation and Janus problem. Therefore, inspired by recent progress of\nvideo diffusion models, we propose to optimize a 4D representation by\nexplicitly generating multi-view videos from one input image. However, it is\nfar from trivial to handle practical challenges faced by such a pipeline,\nincluding dramatic temporal inconsistency, inter-frame geometry and texture\ndiversity, and semantic defects brought by video generation results. To address\nthese issues, we propose DG4D, a novel multi-stage framework that generates\nhigh-quality and consistent 4D assets without score distillation. Specifically,\ncollaborative techniques and solutions are developed, including an attention\ninjection strategy to synthesize temporal-consistent multi-view videos, a\nrobust and efficient dynamic reconstruction method based on Gaussian Splatting,\nand a refinement stage with diffusion prior for semantic restoration. The\nqualitative results and user preference study demonstrate that our framework\noutperforms the baselines in generation quality by a considerable margin. Code\nwill be released at \\url{https://github.com/jasongzy/EG4D}.\n", "rewritten_text": "\\boxed{\nRecently, there has been a growing need for dynamic 3D assets in design and gaming applications, leading to the development of powerful generative pipelines that can create high-quality 4D objects. However, previous methods using score distillation sampling (SDS) have limitations, resulting in unsatisfactory outcomes with issues like over-saturation and the Janus problem. Inspired by video diffusion models, we propose a new approach that optimizes 4D representations by generating multi-view videos from a single input image. To overcome practical challenges such as temporal inconsistency, geometry and texture diversity, and semantic defects, we introduce DG4D, a multi-stage framework that produces high-quality and consistent 4D assets without score distillation. Our framework features collaborative techniques, including attention injection for temporal-consistent video synthesis, robust dynamic reconstruction using Gaussian Splatting, and a refinement stage with diffusion prior for semantic restoration. Our results show that DG4D significantly outperforms baseline methods in generation quality, and our code will be available at https://github.com/jasongzy/EG4D.\n}"}, "2101.04929": {"original_text": "  Motivated by applications from computer vision to bioinformatics, the field\nof shape analysis deals with problems where one wants to analyze geometric\nobjects, such as curves, while ignoring actions that preserve their shape, such\nas translations, rotations, or reparametrizations. Mathematical tools have been\ndeveloped to define notions of distances, averages, and optimal deformations\nfor geometric objects. One such framework, which has proven to be successful in\nmany applications, is based on the square root velocity (SRV) transform, which\nallows one to define a computable distance between spatial curves regardless of\nhow they are parametrized. This paper introduces a supervised deep learning\nframework for the direct computation of SRV distances between curves, which\nusually requires an optimization over the group of reparametrizations that act\non the curves. The benefits of our approach in terms of computational speed and\naccuracy are illustrated via several numerical experiments.\n", "rewritten_text": "\\boxed{Shape analysis, a field with applications in computer vision and bioinformatics, focuses on analyzing geometric objects like curves while disregarding transformations that preserve their shape, such as translations and rotations. To achieve this, mathematical tools have been developed to define distances, averages, and optimal deformations for these objects. This paper presents a deep learning framework that directly computes distances between curves using the square root velocity transform, eliminating the need for optimization over reparametrizations. The approach is shown to improve computational speed and accuracy through numerical experiments.}"}, "1811.07461": {"original_text": "  Humans naturally perceive a 3D scene in front of them through accumulation of\ninformation obtained from multiple interconnected projections of the scene and\nby interpreting their correspondence. This phenomenon has inspired artificial\nintelligence models to extract the depth and view angle of the observed scene\nby modeling the correspondence between different views of that scene. Our paper\nis built upon previous works in the field of unsupervised depth and relative\ncamera pose estimation from temporal consecutive video frames using deep\nlearning (DL) models. Our approach uses a hybrid learning framework introduced\nin a recent work called GeoNet, which leverages geometric constraints in the 3D\nscenes to synthesize a novel view from intermediate DL-based predicted depth\nand relative pose. However, the state-of-the-art unsupervised depth and pose\nestimation DL models are exclusively trained/tested on a few available outdoor\nscene datasets and we have shown they are hardly transferable to new scenes,\nespecially from indoor environments, in which estimation requires higher\nprecision and dealing with probable occlusions. This paper introduces \"Indoor\nGeoNet\", a weakly supervised depth and camera pose estimation model targeted\nfor indoor scenes. In Indoor GeoNet, we take advantage of the availability of\nindoor RGBD datasets collected by human or robot navigators, and added partial\n(i.e. weak) supervision in depth training into the model. Experimental results\nshowed that our model effectively generalizes to new scenes from different\nbuildings. Indoor GeoNet demonstrated significant depth and pose estimation\nerror reduction when compared to the original GeoNet, while showing 3 times\nmore reconstruction accuracy in synthesizing novel views in indoor\nenvironments.\n", "rewritten_text": "\\boxed{\nHumans perceive 3D scenes by combining multiple projections and interpreting their relationships. Inspired by this, AI models extract depth and view angle by modeling correspondence between different views. Building on previous work in unsupervised depth and camera pose estimation from video frames using deep learning, our approach uses a hybrid framework that leverages geometric constraints to synthesize novel views. However, existing models are limited to outdoor scenes and struggle with indoor environments, which require higher precision and occlusion handling. This paper introduces \"Indoor GeoNet\", a weakly supervised model for indoor scenes that utilizes indoor RGBD datasets and partial supervision. Experimental results show that Indoor GeoNet generalizes well to new scenes, reduces estimation errors, and improves novel view synthesis accuracy by 3 times compared to the original GeoNet.\n}"}, "2208.04254": {"original_text": "  Image captioning models are usually trained according to human annotated\nground-truth captions, which could generate accurate but generic captions. In\nthis paper, we focus on generating distinctive captions that can distinguish\nthe target image from other similar images. To evaluate the distinctiveness of\ncaptions, we introduce a series of metrics that use large-scale vision-language\npre-training model CLIP to quantify the distinctiveness. To further improve the\ndistinctiveness of captioning models, we propose a simple and effective\ntraining strategy that trains the model by comparing target image with similar\nimage group and optimizing the group embedding gap. Extensive experiments are\nconducted on various baseline models to demonstrate the wide applicability of\nour strategy and the consistency of metric results with human evaluation. By\ncomparing the performance of our best model with existing state-of-the-art\nmodels, we claim that our model achieves new state-of-the-art towards\ndistinctiveness objective.\n", "rewritten_text": "\\boxed{This paper aims to generate unique image captions that can differentiate the target image from similar ones, rather than just producing accurate but generic descriptions. To achieve this, we introduce a set of metrics that utilize the CLIP model to measure caption distinctiveness. We also propose a training strategy that enhances distinctiveness by comparing the target image with similar images and optimizing the embedding gap. Our approach is widely applicable, and our best model outperforms existing state-of-the-art models in terms of distinctiveness.}"}, "2212.07181": {"original_text": "  Neuromorphic vision or event vision is an advanced vision technology, where\nin contrast to the visible camera that outputs pixels, the event vision\ngenerates neuromorphic events every time there is a brightness change which\nexceeds a specific threshold in the field of view (FOV). This study focuses on\nleveraging neuromorphic event data for roadside object detection. This is a\nproof of concept towards building artificial intelligence (AI) based pipelines\nwhich can be used for forward perception systems for advanced vehicular\napplications. The focus is on building efficient state-of-the-art object\ndetection networks with better inference results for fast-moving forward\nperception using an event camera. In this article, the event-simulated A2D2\ndataset is manually annotated and trained on two different YOLOv5 networks\n(small and large variants). To further assess its robustness, single model\ntesting and ensemble model testing are carried out.\n", "rewritten_text": "\\boxed{This study explores the use of neuromorphic vision, also known as event vision, for detecting objects on the roadside. Unlike traditional cameras, event vision technology generates data in response to changes in brightness, allowing for more efficient and accurate object detection. The goal is to develop artificial intelligence-based pipelines for advanced vehicular applications, such as forward perception systems. To achieve this, the study uses an event camera and trains two YOLOv5 networks on a manually annotated dataset, evaluating their performance through single model and ensemble model testing.}"}, "2402.01217": {"original_text": "  Implicit neural representations, represented by Neural Radiance Fields\n(NeRF), have dominated research in 3D computer vision by virtue of high-quality\nvisual results and data-driven benefits. However, their realistic applications\nare hindered by the need for dense inputs and per-scene optimization. To solve\nthis problem, previous methods implement generalizable NeRFs by extracting\nlocal features from sparse inputs as conditions for the NeRF decoder. However,\nalthough this way can allow feed-forward reconstruction, they suffer from the\ninherent drawback of yielding sub-optimal results caused by erroneous\nreprojected features. In this paper, we focus on this problem and aim to\naddress it by introducing pre-trained generative priors to enable high-quality\ngeneralizable novel view synthesis. Specifically, we propose a novel Indirect\nDiffusion-guided NeRF framework, termed ID-NeRF, which leverages pre-trained\ndiffusion priors as a guide for the reprojected features created by the\nprevious paradigm. Notably, to enable 3D-consistent predictions, the proposed\nID-NeRF discards the way of direct supervision commonly used in prior 3D\ngenerative models and instead adopts a novel indirect prior injection strategy.\nThis strategy is implemented by distilling pre-trained knowledge into an\nimaginative latent space via score-based distillation, and an attention-based\nrefinement module is then proposed to leverage the embedded priors to improve\nreprojected features extracted from sparse inputs. We conduct extensive\nexperiments on multiple datasets to evaluate our method, and the results\ndemonstrate the effectiveness of our method in synthesizing novel views in a\ngeneralizable manner, especially in sparse settings.\n", "rewritten_text": "\\boxed{\nNeural Radiance Fields (NeRF) have achieved impressive results in 3D computer vision, but their practical applications are limited by the need for dense input data and scene-specific optimization. To overcome this, researchers have developed generalizable NeRFs that extract local features from sparse inputs. However, these methods often produce suboptimal results due to errors in the reprojected features. This paper proposes a novel solution, called Indirect Diffusion-guided NeRF (ID-NeRF), which leverages pre-trained generative priors to improve the quality of novel view synthesis. ID-NeRF uses a diffusion-based prior injection strategy, which distills pre-trained knowledge into a latent space and refines reprojected features using an attention-based module. Experimental results demonstrate the effectiveness of ID-NeRF in synthesizing novel views in a generalizable manner, particularly in sparse data settings.\n}"}, "2207.02803": {"original_text": "  Recent advances in face forgery techniques produce nearly visually\nuntraceable deepfake videos, which could be leveraged with malicious\nintentions. As a result, researchers have been devoted to deepfake detection.\nPrevious studies have identified the importance of local low-level cues and\ntemporal information in pursuit to generalize well across deepfake methods,\nhowever, they still suffer from robustness problem against post-processings. In\nthis work, we propose the Local- & Temporal-aware Transformer-based Deepfake\nDetection (LTTD) framework, which adopts a local-to-global learning protocol\nwith a particular focus on the valuable temporal information within local\nsequences. Specifically, we propose a Local Sequence Transformer (LST), which\nmodels the temporal consistency on sequences of restricted spatial regions,\nwhere low-level information is hierarchically enhanced with shallow layers of\nlearned 3D filters. Based on the local temporal embeddings, we then achieve the\nfinal classification in a global contrastive way. Extensive experiments on\npopular datasets validate that our approach effectively spots local forgery\ncues and achieves state-of-the-art performance.\n", "rewritten_text": "\\boxed{Recent breakthroughs in face forgery techniques have made it possible to create highly realistic deepfake videos that can be used for malicious purposes. To combat this, researchers have been working on developing methods to detect deepfakes. While previous studies have shown that local low-level cues and temporal information are important for detecting deepfakes, they still have limitations when it comes to withstanding post-processing attacks. In this study, we propose a new framework called Local- & Temporal-aware Transformer-based Deepfake Detection (LTTD), which uses a local-to-global learning approach to focus on valuable temporal information within local sequences. Our approach uses a Local Sequence Transformer (LST) to model temporal consistency in restricted spatial regions, and then uses global contrastive learning to achieve final classification. Our experiments on popular datasets demonstrate that our method is effective in identifying local forgery cues and achieves state-of-the-art performance.}"}, "2210.16621": {"original_text": "  Transformer-based architectures like BERT have achieved great success in a\nwide range of Natural Language tasks. Despite their decent performance, the\nmodels still have numerous parameters and high computational complexity,\nimpeding their deployment in resource-constrained environments. Post-Training\nQuantization (PTQ), which enables low-bit computations without extra training,\ncould be a promising tool. In this work, we conduct an empirical evaluation of\nthree PTQ methods on BERT-Base and BERT-Large: Linear Quantization (LQ),\nAnalytical Clipping for Integer Quantization (ACIQ), and Outlier Channel\nSplitting (OCS). OCS theoretically surpasses the others in minimizing the Mean\nSquare quantization Error and avoiding distorting the weights' outliers. That\nis consistent with the evaluation results of most language tasks of GLUE\nbenchmark and a reading comprehension task, SQuAD. Moreover, low-bit quantized\nBERT models could outperform the corresponding 32-bit baselines on several\nsmall language tasks, which we attribute to the alleviation of\nover-parameterization. We further explore the limit of quantization bit and\nshow that OCS could quantize BERT-Base and BERT-Large to 3-bits and retain 98%\nand 96% of the performance on the GLUE benchmark accordingly. Moreover, we\nconduct quantization on the whole BERT family, i.e., BERT models in different\nconfigurations, and comprehensively evaluate their performance on the GLUE\nbenchmark and SQuAD, hoping to provide valuable guidelines for their deployment\nin various computation environments.\n", "rewritten_text": "\\boxed{\nTransformer models like BERT have achieved significant success in various natural language processing tasks, but their large number of parameters and high computational complexity make them difficult to deploy in environments with limited resources. To address this issue, Post-Training Quantization (PTQ) methods can be used to reduce the precision of model weights without requiring additional training. This study evaluates the performance of three PTQ methods - Linear Quantization (LQ), Analytical Clipping for Integer Quantization (ACIQ), and Outlier Channel Splitting (OCS) - on BERT-Base and BERT-Large models. The results show that OCS outperforms the other methods in minimizing quantization error and preserving model performance on the GLUE benchmark and SQuAD task. Furthermore, low-bit quantized BERT models can even surpass their 32-bit counterparts on certain small language tasks, likely due to reduced over-parameterization. The study also explores the limits of quantization bit width and finds that OCS can quantize BERT models to 3 bits while retaining 96-98% of their original performance. Finally, the study provides a comprehensive evaluation of the entire BERT model family on the GLUE benchmark and SQuAD task, offering valuable insights for their deployment in various computational environments.\n}"}, "2401.10768": {"original_text": "  While large language models (LLMs) have demonstrated exceptional performance\nacross various tasks following human alignment, they may still generate\nresponses that sound plausible but contradict factual knowledge, a phenomenon\nknown as hallucination. In this paper, we demonstrate the feasibility of\nmitigating hallucinations by verifying and minimizing the inconsistency between\nexternal knowledge present in the alignment data and the intrinsic knowledge\nembedded within foundation LLMs. Specifically, we propose a novel approach\ncalled Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM\nto automatically formulate assessments based on external knowledge to evaluate\nthe knowledge boundaries of foundation LLMs. To address knowledge\ninconsistencies in the alignment data, KCA implements several specific\nstrategies to deal with these data instances. We demonstrate the superior\nefficacy of KCA in reducing hallucinations across six benchmarks, utilizing\nfoundation LLMs of varying backbones and scales. This confirms the\neffectiveness of mitigating hallucinations by reducing knowledge inconsistency.\nOur code, model weights, and data are openly accessible at\n\\url{https://github.com/fanqiwan/KCA}.\n", "rewritten_text": "\\boxed{\nLarge language models (LLMs) can produce responses that seem plausible but contradict factual knowledge, a phenomenon known as hallucination. To address this, we propose a novel approach called Knowledge Consistent Alignment (KCA), which uses a well-aligned LLM to evaluate the knowledge boundaries of foundation LLMs and identify inconsistencies with external knowledge. By implementing strategies to deal with inconsistent data, KCA reduces hallucinations across six benchmarks, demonstrating its effectiveness in mitigating hallucinations by reducing knowledge inconsistency. Our approach and resources are available at https://github.com/fanqiwan/KCA.\n}"}, "2201.01016": {"original_text": "  Recovering detailed facial geometry from a set of calibrated multi-view\nimages is valuable for its wide range of applications. Traditional multi-view\nstereo (MVS) methods adopt an optimization-based scheme to regularize the\nmatching cost. Recently, learning-based methods integrate all these into an\nend-to-end neural network and show superiority of efficiency. In this paper, we\npropose a novel architecture to recover extremely detailed 3D faces within\ndozens of seconds. Unlike previous learning-based methods that regularize the\ncost volume via 3D CNN, we propose to learn an implicit function for regressing\nthe matching cost. By fitting a 3D morphable model from multi-view images, the\nfeatures of multiple images are extracted and aggregated in the mesh-attached\nUV space, which makes the implicit function more effective in recovering\ndetailed facial shape. Our method outperforms SOTA learning-based MVS in\naccuracy by a large margin on the FaceScape dataset. The code and data are\nreleased in https://github.com/zhuhao-nju/mvfr.\n", "rewritten_text": "\\boxed{\nReconstructing detailed 3D facial geometry from multiple calibrated images has numerous applications. Traditional methods use optimization-based schemes to refine the matching cost, while recent learning-based approaches integrate the process into a neural network, achieving greater efficiency. This paper proposes a novel architecture that can recover highly detailed 3D faces in a matter of seconds. Unlike previous learning-based methods that use 3D CNN to regularize the cost volume, our approach learns an implicit function to regress the matching cost. By fitting a 3D morphable model to multi-view images, features from multiple images are extracted and aggregated in the mesh-attached UV space, making the implicit function more effective in recovering detailed facial shapes. Our method significantly outperforms state-of-the-art learning-based MVS methods in terms of accuracy on the FaceScape dataset, and the code and data are available at https://github.com/zhuhao-nju/mvfr.\n}"}, "1809.05752": {"original_text": "  Readmission after discharge from a hospital is disruptive and costly,\nregardless of the reason. However, it can be particularly problematic for\npsychiatric patients, so predicting which patients may be readmitted is\ncritically important but also very difficult. Clinical narratives in\npsychiatric electronic health records (EHRs) span a wide range of topics and\nvocabulary; therefore, a psychiatric readmission prediction model must begin\nwith a robust and interpretable topic extraction component. We created a data\npipeline for using document vector similarity metrics to perform topic\nextraction on psychiatric EHR data in service of our long-term goal of creating\na readmission risk classifier. We show initial results for our topic extraction\nmodel and identify additional features we will be incorporating in the future.\n", "rewritten_text": "\\boxed{Hospital readmissions, regardless of the reason, are costly and disruptive, especially for psychiatric patients. Predicting which patients are likely to be readmitted is crucial but challenging. To address this, we developed a data pipeline that uses document vector similarity metrics to extract topics from psychiatric electronic health records (EHRs). This is a crucial step towards creating a readmission risk classifier. We present initial results from our topic extraction model and outline future improvements.}"}, "2302.1443": {"original_text": "  3D hand tracking methods based on monocular RGB videos are easily affected by\nmotion blur, while event camera, a sensor with high temporal resolution and\ndynamic range, is naturally suitable for this task with sparse output and low\npower consumption. However, obtaining 3D annotations of fast-moving hands is\ndifficult for constructing event-based hand-tracking datasets. In this paper,\nwe provided an event-based speed adaptive hand tracker (ESAHT) to solve the\nhand tracking problem based on event camera. We enabled a CNN model trained on\na hand tracking dataset with slow motion, which enabled the model to leverage\nthe knowledge of RGB-based hand tracking solutions, to work on fast hand\ntracking tasks. To realize our solution, we constructed the first 3D hand\ntracking dataset captured by an event camera in a real-world environment,\nfigured out two data augment methods to narrow the domain gap between slow and\nfast motion data, developed a speed adaptive event stream segmentation method\nto handle hand movements in different moving speeds, and introduced a new\nevent-to-frame representation method adaptive to event streams with different\nlengths. Experiments showed that our solution outperformed RGB-based as well as\nprevious event-based solutions in fast hand tracking tasks, and our codes and\ndataset will be publicly available.\n", "rewritten_text": "\\boxed{\nThis paper presents a novel approach to 3D hand tracking using event cameras, which are better suited for this task than traditional monocular RGB videos due to their high temporal resolution and dynamic range. However, creating event-based hand tracking datasets is challenging, especially for fast-moving hands. To address this, we developed an event-based speed adaptive hand tracker (ESAHT) that leverages knowledge from RGB-based hand tracking solutions. We constructed the first 3D hand tracking dataset using an event camera in a real-world environment and introduced two data augmentation methods to bridge the gap between slow and fast motion data. Our approach also includes a speed adaptive event stream segmentation method and a new event-to-frame representation method. Experimental results show that our solution outperforms both RGB-based and previous event-based solutions in fast hand tracking tasks, and our code and dataset will be publicly available.\n}"}, "2407.15153": {"original_text": "  Video generation has drawn significant interest recently, pushing the\ndevelopment of large-scale models capable of producing realistic videos with\ncoherent motion. Due to memory constraints, these models typically generate\nshort video segments that are then combined into long videos. The merging\nprocess poses a significant challenge, as it requires ensuring smooth\ntransitions and overall consistency. In this paper, we introduce Anchored\nDiffusion, a novel method for synthesizing relatively long and seamless videos.\nWe extend Diffusion Transformers (DiTs) to incorporate temporal information,\ncreating our sequence-DiT (sDiT) model for generating short video segments.\nUnlike previous works, we train our model on video sequences with random\nnon-uniform temporal spacing and incorporate temporal information via external\nguidance, increasing flexibility and allowing it to capture both short and\nlong-term relationships. Furthermore, during inference, we leverage the\ntransformer architecture to modify the diffusion process, generating a batch of\nnon-uniform sequences anchored to a common frame, ensuring consistency\nregardless of temporal distance. To demonstrate our method, we focus on face\nreenactment, the task of creating a video from a source image that replicates\nthe facial expressions and movements from a driving video. Through\ncomprehensive experiments, we show our approach outperforms current techniques\nin producing longer consistent high-quality videos while offering editing\ncapabilities.\n", "rewritten_text": "\\boxed{Recently, there has been significant interest in video generation, leading to the development of large-scale models that can produce realistic videos with coherent motion. However, due to memory constraints, these models typically generate short video segments that need to be combined into longer videos, which can be a challenging task. To address this, we propose a novel method called Anchored Diffusion, which can synthesize long and seamless videos. Our approach extends Diffusion Transformers to incorporate temporal information, allowing it to capture both short and long-term relationships. We train our model on video sequences with random non-uniform temporal spacing and use external guidance to increase flexibility. During inference, we modify the diffusion process to generate a batch of non-uniform sequences anchored to a common frame, ensuring consistency regardless of temporal distance. We demonstrate the effectiveness of our approach on the task of face reenactment, showing that it outperforms current techniques in producing longer, consistent, and high-quality videos while offering editing capabilities.}"}, "2009.03116": {"original_text": "  This paper presents the first Swedish evaluation benchmark for textual\nsemantic similarity. The benchmark is compiled by simply running the English\nSTS-B dataset through the Google machine translation API. This paper discusses\npotential problems with using such a simple approach to compile a Swedish\nevaluation benchmark, including translation errors, vocabulary variation, and\nproductive compounding. Despite some obvious problems with the resulting\ndataset, we use the benchmark to compare the majority of the currently existing\nSwedish text representations, demonstrating that native models outperform\nmultilingual ones, and that simple bag of words performs remarkably well.\n", "rewritten_text": "\\boxed{This paper introduces a Swedish evaluation benchmark for textual semantic similarity, created by translating the English STS-B dataset using Google's machine translation API. While this approach has limitations, including translation errors and vocabulary variations, the benchmark is used to compare various Swedish text representations, revealing that native models outperform multilingual ones and that a simple bag-of-words approach achieves surprisingly good results.}"}, "2106.16138": {"original_text": "  In this paper, we introduce ELECTRA-style tasks to cross-lingual language\nmodel pre-training. Specifically, we present two pre-training tasks, namely\nmultilingual replaced token detection, and translation replaced token\ndetection. Besides, we pretrain the model, named as XLM-E, on both multilingual\nand parallel corpora. Our model outperforms the baseline models on various\ncross-lingual understanding tasks with much less computation cost. Moreover,\nanalysis shows that XLM-E tends to obtain better cross-lingual transferability.\n", "rewritten_text": "\\boxed{This paper proposes a novel approach to cross-lingual language model pre-training by introducing ELECTRA-style tasks. We develop two new pre-training tasks, multilingual replaced token detection and translation replaced token detection, and train a model called XLM-E on both multilingual and parallel corpora. Our results show that XLM-E outperforms baseline models on various cross-lingual understanding tasks while requiring significantly less computational resources. Furthermore, our analysis reveals that XLM-E exhibits improved cross-lingual transferability.}"}, "2101.03929": {"original_text": "  Learning to capture dependencies between spatial positions is essential to\nmany visual tasks, especially the dense labeling problems like scene parsing.\nExisting methods can effectively capture long-range dependencies with\nself-attention mechanism while short ones by local convolution. However, there\nis still much gap between long-range and short-range dependencies, which\nlargely reduces the models' flexibility in application to diverse spatial\nscales and relationships in complicated natural scene images. To fill such a\ngap, we develop a Middle-Range (MR) branch to capture middle-range dependencies\nby restricting self-attention into local patches. Also, we observe that the\nspatial regions which have large correlations with others can be emphasized to\nexploit long-range dependencies more accurately, and thus propose a Reweighed\nLong-Range (RLR) branch. Based on the proposed MR and RLR branches, we build an\nOmni-Range Dependencies Network (ORDNet) which can effectively capture short-,\nmiddle- and long-range dependencies. Our ORDNet is able to extract more\ncomprehensive context information and well adapt to complex spatial variance in\nscene images. Extensive experiments show that our proposed ORDNet outperforms\nprevious state-of-the-art methods on three scene parsing benchmarks including\nPASCAL Context, COCO Stuff and ADE20K, demonstrating the superiority of\ncapturing omni-range dependencies in deep models for scene parsing task.\n", "rewritten_text": "\\boxed{\nCapturing dependencies between spatial positions is crucial for visual tasks, particularly dense labeling problems like scene parsing. While existing methods effectively capture long-range dependencies using self-attention and short-range dependencies using local convolution, there is a significant gap between these two ranges, limiting the models' flexibility in handling diverse spatial scales and relationships in complex natural scene images. To bridge this gap, we propose a Middle-Range (MR) branch that captures middle-range dependencies by restricting self-attention to local patches, and a Reweighed Long-Range (RLR) branch that emphasizes spatial regions with large correlations to exploit long-range dependencies more accurately. Our Omni-Range Dependencies Network (ORDNet) combines these branches to capture short-, middle-, and long-range dependencies, enabling the extraction of more comprehensive context information and adaptation to complex spatial variance in scene images. Experimental results demonstrate that our ORDNet outperforms previous state-of-the-art methods on three scene parsing benchmarks, highlighting the importance of capturing omni-range dependencies in deep models for scene parsing tasks.\n}"}, "2012.08514": {"original_text": "  In this paper, we propose an end-end model for producing furniture layout for\ninterior scene synthesis from the random vector. This proposed model is aimed\nto support professional interior designers to produce the interior decoration\nsolutions more quickly. The proposed model combines a conditional floor-plan\nmodule of the room, a conditional graphical floor-plan module of the room and a\nconditional layout module. As compared with the prior work on scene synthesis,\nour proposed three modules enhance the ability of auto-layout generation given\nthe dimensional category of the room. We conduct our experiments on the\nproposed real-world interior layout dataset that contains $191208$ designs from\nthe professional designers. Our numerical results demonstrate that the proposed\nmodel yields higher-quality layouts in comparison with the state-of-the-art\nmodel. The dataset and code are released\n\\href{https://github.com/CODE-SUBMIT/dataset3}{Dataset,Code}\n", "rewritten_text": "\\boxed{This paper presents an end-to-end model that generates furniture layouts for interior scene synthesis from random vectors, aiming to assist professional interior designers in producing decoration solutions more efficiently. The model consists of three conditional modules: floor-plan, graphical floor-plan, and layout. Compared to previous scene synthesis work, our three-module approach improves auto-layout generation for rooms of various dimensions. We evaluate our model on a real-world interior layout dataset of 191,208 designs from professional designers, and our results show that our model produces higher-quality layouts than the current state-of-the-art model. The dataset and code are available at \\href{https://github.com/CODE-SUBMIT/dataset3}{Dataset, Code}.}"}, "2004.14525": {"original_text": "  Inverted bottleneck layers, which are built upon depthwise convolutions, have\nbeen the predominant building blocks in state-of-the-art object detection\nmodels on mobile devices. In this work, we investigate the optimality of this\ndesign pattern over a broad range of mobile accelerators by revisiting the\nusefulness of regular convolutions. We discover that regular convolutions are a\npotent component to boost the latency-accuracy trade-off for object detection\non accelerators, provided that they are placed strategically in the network via\nneural architecture search. By incorporating regular convolutions in the search\nspace and directly optimizing the network architectures for object detection,\nwe obtain a family of object detection models, MobileDets, that achieve\nstate-of-the-art results across mobile accelerators. On the COCO object\ndetection task, MobileDets outperform MobileNetV3+SSDLite by 1.7 mAP at\ncomparable mobile CPU inference latencies. MobileDets also outperform\nMobileNetV2+SSDLite by 1.9 mAP on mobile CPUs, 3.7 mAP on Google EdgeTPU, 3.4\nmAP on Qualcomm Hexagon DSP and 2.7 mAP on Nvidia Jetson GPU without increasing\nlatency. Moreover, MobileDets are comparable with the state-of-the-art MnasFPN\non mobile CPUs even without using the feature pyramid, and achieve better mAP\nscores on both EdgeTPUs and DSPs with up to 2x speedup. Code and models are\navailable in the TensorFlow Object Detection API:\nhttps://github.com/tensorflow/models/tree/master/research/object_detection.\n", "rewritten_text": "\\boxed{\nResearchers have traditionally used inverted bottleneck layers with depthwise convolutions in object detection models for mobile devices. However, this study explores the effectiveness of regular convolutions in improving the latency-accuracy trade-off for object detection on mobile accelerators. By incorporating regular convolutions into the neural architecture search, the authors develop a family of object detection models called MobileDets, which achieve state-of-the-art results across various mobile accelerators. MobileDets outperform existing models, such as MobileNetV3+SSDLite and MobileNetV2+SSDLite, on multiple platforms, including mobile CPUs, Google EdgeTPU, Qualcomm Hexagon DSP, and Nvidia Jetson GPU, without increasing latency. Additionally, MobileDets are comparable to the state-of-the-art MnasFPN on mobile CPUs and achieve better performance on EdgeTPUs and DSPs with up to 2x speedup. The code and models are available in the TensorFlow Object Detection API.\n}"}, "2310.00031": {"original_text": "  Diffusion models are generative models with impressive text-to-image\nsynthesis capabilities and have spurred a new wave of creative methods for\nclassical machine learning tasks. However, the best way to harness the\nperceptual knowledge of these generative models for visual tasks is still an\nopen question. Specifically, it is unclear how to use the prompting interface\nwhen applying diffusion backbones to vision tasks. We find that automatically\ngenerated captions can improve text-image alignment and significantly enhance a\nmodel's cross-attention maps, leading to better perceptual performance. Our\napproach improves upon the current state-of-the-art (SOTA) in diffusion-based\nsemantic segmentation on ADE20K and the current overall SOTA for depth\nestimation on NYUv2. Furthermore, our method generalizes to the cross-domain\nsetting. We use model personalization and caption modifications to align our\nmodel to the target domain and find improvements over unaligned baselines. Our\ncross-domain object detection model, trained on Pascal VOC, achieves SOTA\nresults on Watercolor2K. Our cross-domain segmentation method, trained on\nCityscapes, achieves SOTA results on Dark Zurich-val and Nighttime Driving.\nProject page: https://www.vision.caltech.edu/tadp/. Code:\nhttps://github.com/damaggu/TADP.\n", "rewritten_text": "\\boxed{\nDiffusion models have shown remarkable capabilities in text-to-image synthesis, but it remains unclear how to effectively utilize their perceptual knowledge for visual tasks. This study explores the use of automatically generated captions to improve text-image alignment and enhance cross-attention maps, leading to better perceptual performance. The proposed approach achieves state-of-the-art results in diffusion-based semantic segmentation on ADE20K and depth estimation on NYUv2, and demonstrates generalizability to cross-domain settings. By personalizing the model and modifying captions, the approach achieves state-of-the-art results in object detection on Watercolor2K and segmentation on Dark Zurich-val and Nighttime Driving. The project page and code are available at https://www.vision.caltech.edu/tadp/ and https://github.com/damaggu/TADP, respectively.\n}"}, "2402.15930": {"original_text": "  The writing examples of English language learners may be different from those\nof native speakers. Given that there is a significant differences in second\nlanguage (L2) learners' error types by their proficiency levels, this paper\nattempts to reduce overcorrection by examining the interaction between LLM's\nperformance and L2 language proficiency. Our method focuses on zero-shot and\nfew-shot prompting and fine-tuning models for GEC for learners of English as a\nforeign language based on the different proficiency. We investigate GEC results\nand find that overcorrection happens primarily in advanced language learners'\nwriting (proficiency C) rather than proficiency A (a beginner level) and\nproficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot\nprompting with writing examples of English learners, actually tend to exhibit\ndecreased recall measures. To make our claim concrete, we conduct a\ncomprehensive examination of GEC outcomes and their evaluation results based on\nlanguage proficiency.\n", "rewritten_text": "\\boxed{This paper explores how to minimize overcorrection in language learners' writing by analyzing the relationship between language model performance and language proficiency. We used zero-shot and few-shot prompting and fine-tuning models for grammar error correction (GEC) tailored to learners of English as a foreign language at different proficiency levels. Our findings show that overcorrection mainly occurs in advanced learners' writing (level C), rather than beginner (level A) or intermediate (level B) levels. Interestingly, fine-tuned language models and few-shot prompting with learner writing examples tend to have lower recall measures. To support our claim, we conducted a thorough examination of GEC outcomes and evaluation results based on language proficiency.}"}, "2306.10963": {"original_text": "  Adversarial patches are still a simple yet powerful white box attack that can\nbe used to fool object detectors by suppressing possible detections. The\npatches of these so-called evasion attacks are computational expensive to\nproduce and require full access to the attacked detector. This paper addresses\nthe problem of computational expensiveness by analyzing 375 generated patches,\ncalculating the principal components of these and show, that linear\ncombinations of the resulting \"eigenpatches\" can be used to fool object\ndetections successfully.\n", "rewritten_text": "\\boxed{This paper tackles the issue of high computational costs associated with generating adversarial patches, which are used to deceive object detectors, by identifying the principal components of 375 generated patches and demonstrating that linear combinations of these \"eigenpatches\" can effectively evade object detection.}"}, "1806.03028": {"original_text": "  Vehicle Make and Model Recognition (MMR) systems provide a fully automatic\nframework to recognize and classify different vehicle models. Several\napproaches have been proposed to address this challenge, however they can\nperform in restricted conditions. Here, we formulate the vehicle make and model\nrecognition as a fine-grained classification problem and propose a new\nconfigurable on-road vehicle make and model recognition framework. We benefit\nfrom the unsupervised feature learning methods and in more details we employ\nLocality constraint Linear Coding (LLC) method as a fast feature encoder for\nencoding the input SIFT features. The proposed method can perform in real\nenvironments of different conditions. This framework can recognize fifty models\nof vehicles and has an advantage to classify every other vehicle not belonging\nto one of the specified fifty classes as an unknown vehicle. The proposed MMR\nframework can be configured to become faster or more accurate based on the\napplication domain. The proposed approach is examined on two datasets including\nIranian on-road vehicle dataset and CompuCar dataset. The Iranian on-road\nvehicle dataset contains images of 50 models of vehicles captured in real\nsituations by traffic cameras in different weather and lighting conditions.\nExperimental results show superiority of the proposed framework over the\nstate-of-the-art methods on Iranian on-road vehicle datatset and comparable\nresults on CompuCar dataset with 97.5% and 98.4% accuracies, respectively.\n", "rewritten_text": "\\boxed{\nVehicle Make and Model Recognition (MMR) systems aim to automatically identify and classify different vehicle models. While various approaches have been proposed, they often have limitations. This study reframes MMR as a fine-grained classification problem and introduces a novel, configurable framework for on-road vehicle make and model recognition. By leveraging unsupervised feature learning methods, specifically Locality constraint Linear Coding (LLC), the proposed method can operate effectively in diverse real-world environments. It can recognize 50 vehicle models and classify unknown vehicles not belonging to these models. The framework's speed and accuracy can be adjusted based on the application. Experimental results on two datasets, including the Iranian on-road vehicle dataset and CompuCar dataset, demonstrate the superiority of the proposed framework, achieving 97.5% and 98.4% accuracies, respectively.\n}"}, "2110.10575": {"original_text": "  Recent research in opinion mining proposed word embedding-based topic\nmodeling methods that provide superior coherence compared to traditional topic\nmodeling. In this paper, we demonstrate how these methods can be used to\ndisplay correlated topic models on social media texts using SocialVisTUM, our\nproposed interactive visualization toolkit. It displays a graph with topics as\nnodes and their correlations as edges. Further details are displayed\ninteractively to support the exploration of large text collections, e.g.,\nrepresentative words and sentences of topics, topic and sentiment\ndistributions, hierarchical topic clustering, and customizable, predefined\ntopic labels. The toolkit optimizes automatically on custom data for optimal\ncoherence. We show a working instance of the toolkit on data crawled from\nEnglish social media discussions about organic food consumption. The\nvisualization confirms findings of a qualitative consumer research study.\nSocialVisTUM and its training procedures are accessible online.\n", "rewritten_text": "\\boxed{This paper presents SocialVisTUM, an interactive visualization toolkit that utilizes word embedding-based topic modeling methods to display correlated topic models on social media texts. The toolkit generates a graph with topics as nodes and their correlations as edges, and provides further details on demand, including representative words and sentences, topic and sentiment distributions, hierarchical topic clustering, and customizable topic labels. SocialVisTUM automatically optimizes for optimal coherence on custom data and is demonstrated on a dataset of English social media discussions about organic food consumption, confirming findings from a qualitative consumer research study. The toolkit and its training procedures are available online.}"}, "2404.00851": {"original_text": "  Pre-trained vision-language models have shown impressive success on various\ncomputer vision tasks with their zero-shot generalizability. Recently, prompt\nlearning approaches have been explored to efficiently and effectively adapt the\nvision-language models to a variety of downstream tasks. However, most existing\nprompt learning methods suffer from task overfitting since the general\nknowledge of the pre-trained vision language models is forgotten while the\nprompts are finetuned on a small data set from a specific target task. To\naddress this issue, we propose a Prompt Meta-Regularization (ProMetaR) to\nimprove the generalizability of prompt learning for vision-language models.\nSpecifically, ProMetaR meta-learns both the regularizer and the soft prompts to\nharness the task-specific knowledge from the downstream tasks and task-agnostic\ngeneral knowledge from the vision-language models. Further, ProMetaR augments\nthe task to generate multiple virtual tasks to alleviate the meta-overfitting.\nIn addition, we provide the analysis to comprehend how ProMetaR improves the\ngeneralizability of prompt tuning in the perspective of the gradient alignment.\nOur extensive experiments demonstrate that our ProMetaR improves the\ngeneralizability of conventional prompt learning methods under\nbase-to-base/base-to-new and domain generalization settings. The code of\nProMetaR is available at https://github.com/mlvlab/ProMetaR.\n", "rewritten_text": "\\boxed{\nPre-trained vision-language models have achieved remarkable success in various computer vision tasks, thanks to their zero-shot generalizability. However, adapting these models to specific downstream tasks using prompt learning approaches often results in task overfitting, where the model forgets its general knowledge. To tackle this issue, we introduce Prompt Meta-Regularization (ProMetaR), a novel method that meta-learns both the regularizer and soft prompts to balance task-specific knowledge and task-agnostic general knowledge. ProMetaR also generates virtual tasks to mitigate meta-overfitting. Our analysis reveals that ProMetaR improves prompt tuning generalizability by aligning gradients. Extensive experiments demonstrate that ProMetaR outperforms conventional prompt learning methods in base-to-base, base-to-new, and domain generalization settings. The ProMetaR code is available at https://github.com/mlvlab/ProMetaR.\n}"}, "2409.12539": {"original_text": "  In radiation therapy (RT), the reliance on pre-treatment computed tomography\n(CT) images encounter challenges due to anatomical changes, necessitating\nadaptive planning. Daily cone-beam CT (CBCT) imaging, pivotal for therapy\nadjustment, falls short in tissue density accuracy. To address this, our\ninnovative approach integrates diffusion models for CT image generation,\noffering precise control over data synthesis. Leveraging a self-training method\nwith knowledge distillation, we maximize CBCT data during therapy, complemented\nby sparse paired fan-beam CTs. This strategy, incorporated into\nstate-of-the-art diffusion-based models, surpasses conventional methods like\nPix2pix and CycleGAN. A meticulously curated dataset of 2800 paired CBCT and CT\nscans, supplemented by 4200 CBCT scans, undergoes preprocessing and teacher\nmodel training, including the Brownian Bridge Diffusion Model (BBDM).\nPseudo-label CT images are generated, resulting in a dataset combining 5600 CT\nimages with corresponding CBCT images. Thorough evaluation using MSE, SSIM,\nPSNR and LPIPS demonstrates superior performance against Pix2pix and CycleGAN.\nOur approach shows promise in generating high-quality CT images from CBCT scans\nin RT.\n", "rewritten_text": "\\boxed{\nIn radiation therapy, pre-treatment CT images are often unreliable due to anatomical changes, requiring adaptive planning. While daily CBCT imaging is crucial for therapy adjustments, it lacks accuracy in tissue density. To overcome this, we developed an innovative approach that combines diffusion models for CT image generation, allowing for precise control over data synthesis. By leveraging self-training with knowledge distillation, we maximize CBCT data during therapy, supplemented by sparse paired fan-beam CTs. Our strategy, integrated into state-of-the-art diffusion-based models, outperforms conventional methods like Pix2pix and CycleGAN. We curated a dataset of 2800 paired CBCT and CT scans, plus 4200 CBCT scans, and trained a teacher model using the Brownian Bridge Diffusion Model (BBDM). We generated pseudo-label CT images, resulting in a dataset of 5600 CT images with corresponding CBCT images. Our approach demonstrated superior performance in generating high-quality CT images from CBCT scans in radiation therapy, as evaluated by MSE, SSIM, PSNR, and LPIPS metrics.\n}"}, "1904.0169": {"original_text": "  We present MonoPSR, a monocular 3D object detection method that leverages\nproposals and shape reconstruction. First, using the fundamental relations of a\npinhole camera model, detections from a mature 2D object detector are used to\ngenerate a 3D proposal per object in a scene. The 3D location of these\nproposals prove to be quite accurate, which greatly reduces the difficulty of\nregressing the final 3D bounding box detection. Simultaneously, a point cloud\nis predicted in an object centered coordinate system to learn local scale and\nshape information. However, the key challenge is how to exploit shape\ninformation to guide 3D localization. As such, we devise aggregate losses,\nincluding a novel projection alignment loss, to jointly optimize these tasks in\nthe neural network to improve 3D localization accuracy. We validate our method\non the KITTI benchmark where we set new state-of-the-art results among\npublished monocular methods, including the harder pedestrian and cyclist\nclasses, while maintaining efficient run-time.\n", "rewritten_text": "\\boxed{We introduce MonoPSR, a novel monocular 3D object detection approach that combines proposal generation and shape reconstruction. By leveraging the principles of a pinhole camera model, our method generates accurate 3D proposals from 2D object detections, simplifying the task of regressing 3D bounding boxes. Additionally, we predict a point cloud in an object-centered coordinate system to capture local scale and shape information. To effectively utilize shape information for 3D localization, we design aggregate losses, including a novel projection alignment loss, to jointly optimize the neural network and improve 3D localization accuracy. Our method achieves state-of-the-art results on the KITTI benchmark, outperforming published monocular methods on pedestrian and cyclist classes while maintaining efficient runtime.}"}, "2408.02291": {"original_text": "  Unsupervised 3D keypoints estimation from Point Cloud Data (PCD) is a complex\ntask, even more challenging when an object shape is deforming. As keypoints\nshould be semantically and geometrically consistent across all the 3D frames -\neach keypoint should be anchored to a specific part of the deforming shape\nirrespective of intrinsic and extrinsic motion. This paper presents, \"SelfGeo\",\na self-supervised method that computes persistent 3D keypoints of non-rigid\nobjects from arbitrary PCDs without the need of human annotations. The gist of\nSelfGeo is to estimate keypoints between frames that respect invariant\nproperties of deforming bodies. Our main contribution is to enforce that\nkeypoints deform along with the shape while keeping constant geodesic distances\namong them. This principle is then propagated to the design of a set of losses\nwhich minimization let emerge repeatable keypoints in specific semantic\nlocations of the non-rigid shape. We show experimentally that the use of\ngeodesic has a clear advantage in challenging dynamic scenes and with different\nclasses of deforming shapes (humans and animals). Code and data are available\nat: https://github.com/IIT-PAVIS/SelfGeo\n", "rewritten_text": "\\boxed{Estimating 3D keypoints from point cloud data (PCD) without supervision is a challenging task, especially when dealing with deforming objects. The goal is to identify keypoints that are semantically and geometrically consistent across all 3D frames, despite intrinsic and extrinsic motion. This paper introduces \"SelfGeo\", a self-supervised method that computes persistent 3D keypoints of non-rigid objects from arbitrary PCDs without human annotations. The key idea is to estimate keypoints that respect invariant properties of deforming bodies, ensuring they deform with the shape while maintaining constant geodesic distances. This principle is used to design a set of losses that, when minimized, produce repeatable keypoints in specific semantic locations of the non-rigid shape. Experimental results demonstrate the advantage of using geodesic distances in dynamic scenes with various classes of deforming shapes, including humans and animals. The code and data are available at https://github.com/IIT-PAVIS/SelfGeo.}"}, "2304.0723": {"original_text": "  Pedestrian attribute recognition (PAR) has received increasing attention\nbecause of its wide application in video surveillance and pedestrian analysis.\nExtracting robust feature representation is one of the key challenges in this\ntask. The existing methods mainly use the convolutional neural network (CNN) as\nthe backbone network to extract features. However, these methods mainly focus\non small discriminative regions while ignoring the global perspective. To\novercome these limitations, we propose a pure transformer-based multi-task PAR\nnetwork named PARFormer, which includes four modules. In the feature extraction\nmodule, we build a transformer-based strong baseline for feature extraction,\nwhich achieves competitive results on several PAR benchmarks compared with the\nexisting CNN-based baseline methods. In the feature processing module, we\npropose an effective data augmentation strategy named batch random mask (BRM)\nblock to reinforce the attentive feature learning of random patches.\nFurthermore, we propose a multi-attribute center loss (MACL) to enhance the\ninter-attribute discriminability in the feature representations. In the\nviewpoint perception module, we explore the impact of viewpoints on pedestrian\nattributes, and propose a multi-view contrastive loss (MCVL) that enables the\nnetwork to exploit the viewpoint information. In the attribute recognition\nmodule, we alleviate the negative-positive imbalance problem to generate the\nattribute predictions. The above modules interact and jointly learn a highly\ndiscriminative feature space, and supervise the generation of the final\nfeatures. Extensive experimental results show that the proposed PARFormer\nnetwork performs well compared to the state-of-the-art methods on several\npublic datasets, including PETA, RAP, and PA100K. Code will be released at\nhttps://github.com/xwf199/PARFormer.\n", "rewritten_text": "\\boxed{\nPedestrian attribute recognition (PAR) has gained significant attention due to its vast applications in video surveillance and pedestrian analysis. A major challenge in PAR is extracting robust feature representations. Existing methods primarily employ convolutional neural networks (CNNs) as the backbone for feature extraction, but they focus on small discriminative regions, neglecting the global perspective. To address these limitations, we introduce PARFormer, a multi-task PAR network based solely on transformers, comprising four modules. The feature extraction module establishes a strong transformer-based baseline, achieving competitive results on PAR benchmarks compared to CNN-based methods. The feature processing module incorporates a batch random mask (BRM) block for attentive feature learning and a multi-attribute center loss (MACL) to enhance inter-attribute discriminability. The viewpoint perception module explores the impact of viewpoints on pedestrian attributes and proposes a multi-view contrastive loss (MCVL) to leverage viewpoint information. The attribute recognition module alleviates the negative-positive imbalance problem to generate attribute predictions. The modules interact and jointly learn a highly discriminative feature space, supervising the generation of final features. Experimental results demonstrate that PARFormer outperforms state-of-the-art methods on public datasets, including PETA, RAP, and PA100K. The code is available at https://github.com/xwf199/PARFormer.\n}"}, "2402.09967": {"original_text": "  Large Language Models (LLMs) excel in generating personalized content and\nfacilitating interactive dialogues, showcasing their remarkable aptitude for a\nmyriad of applications. However, their capabilities in reasoning and providing\nexplainable outputs, especially within the context of reasoning abilities,\nremain areas for improvement. In this study, we delve into the reasoning\nabilities of LLMs, highlighting the current challenges and limitations that\nhinder their effectiveness in complex reasoning scenarios.\n", "rewritten_text": "\\boxed{While Large Language Models (LLMs) demonstrate exceptional skills in generating tailored content and engaging in interactive conversations, they still struggle with logical reasoning and providing transparent explanations, particularly in complex problem-solving situations. This study explores the reasoning capabilities of LLMs, identifying the current obstacles and limitations that impede their performance in these areas.}"}, "2212.05911": {"original_text": "  Deep learning has emerged as an effective solution for solving the task of\nobject detection in images but at the cost of requiring large labeled datasets.\nTo mitigate this cost, semi-supervised object detection methods, which consist\nin leveraging abundant unlabeled data, have been proposed and have already\nshown impressive results. However, most of these methods require linking a\npseudo-label to a ground-truth object by thresholding. In previous works, this\nthreshold value is usually determined empirically, which is time consuming, and\nonly done for a single data distribution. When the domain, and thus the data\ndistribution, changes, a new and costly parameter search is necessary. In this\nwork, we introduce our method Adaptive Self-Training for Object Detection\n(ASTOD), which is a simple yet effective teacher-student method. ASTOD\ndetermines without cost a threshold value based directly on the ground value of\nthe score histogram. To improve the quality of the teacher predictions, we also\npropose a novel pseudo-labeling procedure. We use different views of the\nunlabeled images during the pseudo-labeling step to reduce the number of missed\npredictions and thus obtain better candidate labels. Our teacher and our\nstudent are trained separately, and our method can be used in an iterative\nfashion by replacing the teacher by the student. On the MS-COCO dataset, our\nmethod consistently performs favorably against state-of-the-art methods that do\nnot require a threshold parameter, and shows competitive results with methods\nthat require a parameter sweep search. Additional experiments with respect to a\nsupervised baseline on the DIOR dataset containing satellite images lead to\nsimilar conclusions, and prove that it is possible to adapt the score threshold\nautomatically in self-training, regardless of the data distribution. The code\nis available at https:// github.com/rvandeghen/ASTOD\n", "rewritten_text": "\\boxed{\nDeep learning has proven effective in object detection tasks, but it requires large labeled datasets. To address this limitation, semi-supervised object detection methods have been developed, leveraging unlabeled data. However, these methods often rely on thresholding to link pseudo-labels to ground-truth objects, which can be time-consuming and dataset-specific. We propose Adaptive Self-Training for Object Detection (ASTOD), a teacher-student method that automatically determines the threshold value based on the score histogram. Our approach also introduces a novel pseudo-labeling procedure that uses different views of unlabeled images to reduce missed predictions. Our method outperforms state-of-the-art methods without threshold parameters and is competitive with those that require parameter sweeps on the MS-COCO and DIOR datasets. The code is available at https://github.com/rvandeghen/ASTOD."}, "1507.02772": {"original_text": "  Data encoded as symmetric positive definite (SPD) matrices frequently arise\nin many areas of computer vision and machine learning. While these matrices\nform an open subset of the Euclidean space of symmetric matrices, viewing them\nthrough the lens of non-Euclidean Riemannian geometry often turns out to be\nbetter suited in capturing several desirable data properties. However,\nformulating classical machine learning algorithms within such a geometry is\noften non-trivial and computationally expensive. Inspired by the great success\nof dictionary learning and sparse coding for vector-valued data, our goal in\nthis paper is to represent data in the form of SPD matrices as sparse conic\ncombinations of SPD atoms from a learned dictionary via a Riemannian geometric\napproach. To that end, we formulate a novel Riemannian optimization objective\nfor dictionary learning and sparse coding in which the representation loss is\ncharacterized via the affine invariant Riemannian metric. We also present a\ncomputationally simple algorithm for optimizing our model. Experiments on\nseveral computer vision datasets demonstrate superior classification and\nretrieval performance using our approach when compared to sparse coding via\nalternative non-Riemannian formulations.\n", "rewritten_text": "\\boxed{Symmetric positive definite (SPD) matrices are commonly used in computer vision and machine learning, but they can be better understood through non-Euclidean Riemannian geometry. However, applying traditional machine learning algorithms to SPD matrices can be challenging and computationally expensive. This paper proposes a novel approach to represent SPD matrices as sparse combinations of SPD \"atoms\" from a learned dictionary, using a Riemannian geometric method. The approach is based on a new optimization objective that incorporates the affine invariant Riemannian metric, and is computationally efficient. Experimental results on computer vision datasets show that this approach outperforms alternative methods in classification and retrieval tasks.}"}, "1603.09742": {"original_text": "  Semantic segmentation is critical to image content understanding and object\nlocalization. Recent development in fully-convolutional neural network (FCN)\nhas enabled accurate pixel-level labeling. One issue in previous works is that\nthe FCN based method does not exploit the object boundary information to\ndelineate segmentation details since the object boundary label is ignored in\nthe network training. To tackle this problem, we introduce a double branch\nfully convolutional neural network, which separates the learning of the\ndesirable semantic class labeling with mask-level object proposals guided by\nrelabeled boundaries. This network, called object boundary guided FCN\n(OBG-FCN), is able to integrate the distinct properties of object shape and\nclass features elegantly in a fully convolutional way with a designed masking\narchitecture. We conduct experiments on the PASCAL VOC segmentation benchmark,\nand show that the end-to-end trainable OBG-FCN system offers great improvement\nin optimizing the target semantic segmentation quality.\n", "rewritten_text": "\\boxed{Accurate image understanding and object localization rely on semantic segmentation, which has been improved by fully-convolutional neural networks (FCNs). However, previous FCN-based methods neglect object boundary information, leading to incomplete segmentation details. To address this, we propose a double-branch FCN, called object boundary guided FCN (OBG-FCN), which separates semantic class labeling from mask-level object proposals guided by relabeled boundaries. This network integrates object shape and class features in a fully convolutional manner, resulting in improved semantic segmentation quality, as demonstrated by experiments on the PASCAL VOC segmentation benchmark.}"}, "2004.03677": {"original_text": "  Image manipulation can be considered a special case of image generation where\nthe image to be produced is a modification of an existing image. Image\ngeneration and manipulation have been, for the most part, tasks that operate on\nraw pixels. However, the remarkable progress in learning rich image and object\nrepresentations has opened the way for tasks such as text-to-image or\nlayout-to-image generation that are mainly driven by semantics. In our work, we\naddress the novel problem of image manipulation from scene graphs, in which a\nuser can edit images by merely applying changes in the nodes or edges of a\nsemantic graph that is generated from the image. Our goal is to encode image\ninformation in a given constellation and from there on generate new\nconstellations, such as replacing objects or even changing relationships\nbetween objects, while respecting the semantics and style from the original\nimage. We introduce a spatio-semantic scene graph network that does not require\ndirect supervision for constellation changes or image edits. This makes it\npossible to train the system from existing real-world datasets with no\nadditional annotation effort.\n", "rewritten_text": "\\boxed{Image manipulation can be seen as a specific type of image generation, where an existing image is modified to produce a new one. Traditionally, image generation and manipulation have involved working with raw pixels. However, recent advancements in learning rich image and object representations have enabled tasks like text-to-image and layout-to-image generation, which are driven by semantic meaning. Our research focuses on the novel problem of image manipulation using scene graphs, where users can edit images by simply modifying the nodes or edges of a semantic graph derived from the image. Our objective is to encode image information in a given scene and generate new scenes, such as replacing objects or changing their relationships, while preserving the original image's semantics and style. We propose a spatio-semantic scene graph network that can be trained without direct supervision for scene changes or image edits, allowing it to learn from existing real-world datasets without additional annotation effort.}"}, "2208.09669": {"original_text": "  Contextualized word embeddings in language models have given much advance to\nNLP. Intuitively, sentential information is integrated into the representation\nof words, which can help model polysemy. However, context sensitivity also\nleads to the variance of representations, which may break the semantic\nconsistency for synonyms. We quantify how much the contextualized embeddings of\neach word sense vary across contexts in typical pre-trained models. Results\nshow that contextualized embeddings can be highly consistent across contexts.\nIn addition, part-of-speech, number of word senses, and sentence length have an\ninfluence on the variance of sense representations. Interestingly, we find that\nword representations are position-biased, where the first words in different\ncontexts tend to be more similar. We analyze such a phenomenon and also propose\na simple way to alleviate such bias in distance-based word sense disambiguation\nsettings.\n", "rewritten_text": "\\boxed{The integration of sentential information into word representations in language models has significantly advanced NLP, but it also introduces variability in word meanings, potentially compromising semantic consistency among synonyms. Our study measures the extent to which contextualized word embeddings vary across different contexts in pre-trained models, revealing that they can be surprisingly consistent. We identify factors such as part-of-speech, number of word senses, and sentence length that influence the variability of sense representations. Furthermore, we discover a positional bias in word representations, where initial words in different contexts tend to be more similar, and propose a simple method to mitigate this bias in word sense disambiguation tasks.}"}, "1706.00842": {"original_text": "  We present a fully automatic method employing convolutional neural networks\nbased on the 2D U-net architecture and random forest classifier to solve the\nautomatic liver lesion segmentation problem of the ISBI 2017 Liver Tumor\nSegmentation Challenge (LiTS). In order to constrain the ROI in which the\ntumors could be located, a liver segmentation is performed first. For the organ\nsegmentation, an ensemble of convolutional networks is trained to segment a\nliver using a set of 179 liver CT datasets from liver surgery planning. Inside\nof the liver ROI a neural network, trained using 127 challenge training\ndatasets, identifies tumor candidates, which are subsequently filtered with a\nrandom forest classifier yielding the final tumor segmentation. The evaluation\non the 70 challenge test cases resulted in a mean Dice coefficient of 0.65,\nranking our method in the second place.\n", "rewritten_text": "\\boxed{We propose a fully automated method that combines convolutional neural networks (CNNs) with a random forest classifier to tackle the liver lesion segmentation task in the ISBI 2017 Liver Tumor Segmentation Challenge. Our approach first identifies the liver region using an ensemble of CNNs trained on 179 liver CT datasets, and then uses a separate CNN to detect tumor candidates within the liver region. These candidates are further refined using a random forest classifier to produce the final tumor segmentation. Our method achieved a mean Dice coefficient of 0.65 on the 70 test cases, ranking second in the challenge.}"}, "2108.07848": {"original_text": "  Identifying players in sports videos by recognizing their jersey numbers is a\nchallenging task in computer vision. We have designed and implemented a\nmulti-task learning network for jersey number recognition. In order to train a\nnetwork to recognize jersey numbers, two output label representations are used\n(1) Holistic - considers the entire jersey number as one class, and (2)\nDigit-wise - considers the two digits in a jersey number as two separate\nclasses. The proposed network learns both holistic and digit-wise\nrepresentations through a multi-task loss function. We determine the optimal\nweights to be assigned to holistic and digit-wise losses through an ablation\nstudy. Experimental results demonstrate that the proposed multi-task learning\nnetwork performs better than the constituent holistic and digit-wise\nsingle-task learning networks.\n", "rewritten_text": "\\boxed{Recognizing jersey numbers in sports videos is a difficult computer vision task. To tackle this, we developed a multi-task learning network that can identify jersey numbers in two ways: as a single class (holistic) or as individual digits (digit-wise). Our network learns both representations simultaneously using a multi-task loss function, and we determined the optimal weights for each loss through experimentation. Our results show that our multi-task network outperforms single-task networks that focus on either holistic or digit-wise recognition.}"}, "2104.01136": {"original_text": "  We design a family of image classification architectures that optimize the\ntrade-off between accuracy and efficiency in a high-speed regime. Our work\nexploits recent findings in attention-based architectures, which are\ncompetitive on highly parallel processing hardware. We revisit principles from\nthe extensive literature on convolutional neural networks to apply them to\ntransformers, in particular activation maps with decreasing resolutions. We\nalso introduce the attention bias, a new way to integrate positional\ninformation in vision transformers. As a result, we propose LeVIT: a hybrid\nneural network for fast inference image classification. We consider different\nmeasures of efficiency on different hardware platforms, so as to best reflect a\nwide range of application scenarios. Our extensive experiments empirically\nvalidate our technical choices and show they are suitable to most\narchitectures. Overall, LeViT significantly outperforms existing convnets and\nvision transformers with respect to the speed/accuracy tradeoff. For example,\nat 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on\nCPU. We release the code at https://github.com/facebookresearch/LeViT\n", "rewritten_text": "\\boxed{We have developed a family of image classification architectures that balance accuracy and efficiency for high-speed processing. By combining the strengths of attention-based architectures and convolutional neural networks, we introduce LeVIT, a hybrid neural network that achieves fast inference image classification. Our novel approach incorporates attention bias to integrate positional information in vision transformers and utilizes activation maps with decreasing resolutions. Through extensive experiments, we demonstrate that LeVIT outperforms existing convnets and vision transformers in terms of speed and accuracy, achieving 5 times faster performance than EfficientNet on CPU at 80% ImageNet top-1 accuracy. The code is available at https://github.com/facebookresearch/LeViT.}"}, "2103.02984": {"original_text": "  Abrupt motion of camera or objects in a scene result in a blurry video, and\ntherefore recovering high quality video requires two types of enhancements:\nvisual enhancement and temporal upsampling. A broad range of research attempted\nto recover clean frames from blurred image sequences or temporally upsample\nframes by interpolation, yet there are very limited studies handling both\nproblems jointly. In this work, we present a novel framework for deblurring,\ninterpolating and extrapolating sharp frames from a motion-blurred video in an\nend-to-end manner. We design our framework by first learning the pixel-level\nmotion that caused the blur from the given inputs via optical flow estimation\nand then predict multiple clean frames by warping the decoded features with the\nestimated flows. To ensure temporal coherence across predicted frames and\naddress potential temporal ambiguity, we propose a simple, yet effective\nflow-based rule. The effectiveness and favorability of our approach are\nhighlighted through extensive qualitative and quantitative evaluations on\nmotion-blurred datasets from high speed videos.\n", "rewritten_text": "\\boxed{When a camera or objects in a scene move abruptly, it can result in a blurry video. To recover high-quality video, two types of enhancements are necessary: visual enhancement and temporal upsampling. While many studies have focused on recovering clean frames from blurred images or interpolating frames, few have addressed both issues simultaneously. This work presents a novel framework that deblurs, interpolates, and extrapolates sharp frames from motion-blurred videos in a single, end-to-end process. Our approach involves learning pixel-level motion from input data using optical flow estimation, predicting clean frames by warping decoded features with estimated flows, and ensuring temporal coherence across predicted frames using a flow-based rule. The effectiveness of our approach is demonstrated through extensive qualitative and quantitative evaluations on motion-blurred datasets from high-speed videos.}"}, "1611.09932": {"original_text": "  Compared to earlier multistage frameworks using CNN features, recent\nend-to-end deep approaches for fine-grained recognition essentially enhance the\nmid-level learning capability of CNNs. Previous approaches achieve this by\nintroducing an auxiliary network to infuse localization information into the\nmain classification network, or a sophisticated feature encoding method to\ncapture higher order feature statistics. We show that mid-level representation\nlearning can be enhanced within the CNN framework, by learning a bank of\nconvolutional filters that capture class-specific discriminative patches\nwithout extra part or bounding box annotations. Such a filter bank is well\nstructured, properly initialized and discriminatively learned through a novel\nasymmetric multi-stream architecture with convolutional filter supervision and\na non-random layer initialization. Experimental results show that our approach\nachieves state-of-the-art on three publicly available fine-grained recognition\ndatasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and\nvisualizations are provided to understand our approach.\n", "rewritten_text": "\\boxed{Recent deep learning approaches for fine-grained recognition have improved the mid-level learning capabilities of Convolutional Neural Networks (CNNs) by introducing auxiliary networks or sophisticated feature encoding methods. However, we demonstrate that mid-level representation learning can be enhanced within the CNN framework itself by learning a bank of convolutional filters that capture class-specific discriminative patches without requiring additional part or bounding box annotations. Our novel approach, which utilizes an asymmetric multi-stream architecture with convolutional filter supervision and non-random layer initialization, achieves state-of-the-art results on three publicly available fine-grained recognition datasets.}"}, "1504.01013": {"original_text": "  Recent advances in semantic image segmentation have mostly been achieved by\ntraining deep convolutional neural networks (CNNs). We show how to improve\nsemantic segmentation through the use of contextual information; specifically,\nwe explore `patch-patch' context between image regions, and `patch-background'\ncontext. For learning from the patch-patch context, we formulate Conditional\nRandom Fields (CRFs) with CNN-based pairwise potential functions to capture\nsemantic correlations between neighboring patches. Efficient piecewise training\nof the proposed deep structured model is then applied to avoid repeated\nexpensive CRF inference for back propagation. For capturing the\npatch-background context, we show that a network design with traditional\nmulti-scale image input and sliding pyramid pooling is effective for improving\nperformance. Our experimental results set new state-of-the-art performance on a\nnumber of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC\n2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an\nintersection-over-union score of 78.0 on the challenging PASCAL VOC 2012\ndataset.\n", "rewritten_text": "\\boxed{Recent breakthroughs in semantic image segmentation have been largely driven by the development of deep convolutional neural networks (CNNs). This paper demonstrates how incorporating contextual information can further enhance semantic segmentation. Specifically, it explores two types of context: `patch-patch' context, which examines relationships between neighboring image regions, and `patch-background' context. To leverage the `patch-patch' context, the authors propose a deep structured model that combines Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between adjacent patches. They also introduce an efficient piecewise training method to avoid repetitive and costly CRF inference during back propagation. For the `patch-background' context, the authors show that a network design featuring traditional multi-scale image input and sliding pyramid pooling is effective in improving performance. The proposed approach achieves state-of-the-art results on several popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow, with a notable intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012 dataset.}"}, "2106.1262": {"original_text": "  The self-attention-based model, transformer, is recently becoming the leading\nbackbone in the field of computer vision. In spite of the impressive success\nmade by transformers in a variety of vision tasks, it still suffers from heavy\ncomputation and intensive memory costs. To address this limitation, this paper\npresents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).\nWe start by observing a large amount of redundant computation, mainly spent on\nuncorrelated input patches, and then introduce an interpretable module to\ndynamically and gracefully drop these redundant patches. This novel framework\nis then extended to a hierarchical structure, where uncorrelated tokens at\ndifferent stages are gradually removed, resulting in a considerable shrinkage\nof computational cost. We include extensive experiments on both image and video\ntasks, where our method could deliver up to 1.4x speed-up for state-of-the-art\nmodels like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy.\nMore importantly, contrary to other acceleration approaches, our method is\ninherently interpretable with substantial visual evidence, making vision\ntransformer closer to a more human-understandable architecture while being\nlighter. We demonstrate that the interpretability that naturally emerged in our\nframework can outperform the raw attention learned by the original visual\ntransformer, as well as those generated by off-the-shelf interpretation\nmethods, with both qualitative and quantitative results. Project Page:\nhttp://people.csail.mit.edu/bpan/ia-red/.\n", "rewritten_text": "\\boxed{\nThe transformer model, a self-attention-based architecture, has become a leading approach in computer vision, achieving impressive success in various tasks. However, it is limited by its high computational cost and memory requirements. To address this, we propose the Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$), which identifies and dynamically removes redundant input patches, reducing computational cost. We extend this framework to a hierarchical structure, gradually removing uncorrelated tokens at different stages, resulting in significant speed-up without sacrificing accuracy. Our experiments on image and video tasks demonstrate up to 1.4x speed-up for state-of-the-art models like DeiT and TimeSformer, with minimal accuracy loss. Moreover, our method provides inherent interpretability, making vision transformers more human-understandable, and outperforms raw attention and off-the-shelf interpretation methods."}, "1803.03852": {"original_text": "  Tracking the pose of instruments is a central problem in image-guided\nsurgery. For microscopic scenarios, optical coherence tomography (OCT) is\nincreasingly used as an imaging modality. OCT is suitable for accurate pose\nestimation due to its micrometer range resolution and volumetric field of view.\nHowever, OCT image processing is challenging due to speckle noise and\nreflection artifacts in addition to the images' 3D nature. We address pose\nestimation from OCT volume data with a new deep learning-based tracking\nframework. For this purpose, we design a new 3D convolutional neural network\n(CNN) architecture to directly predict the 6D pose of a small marker geometry\nfrom OCT volumes. We use a hexapod robot to automatically acquire labeled data\npoints which we use to train 3D CNN architectures for multi-output regression.\nWe use this setup to provide an in-depth analysis on deep learning-based pose\nestimation from volumes. Specifically, we demonstrate that exploiting volume\ninformation for pose estimation yields higher accuracy than relying on 2D\nrepresentations with depth information. Supporting this observation, we provide\nquantitative and qualitative results that 3D CNNs effectively exploit the depth\nstructure of marker objects. Regarding the deep learning aspect, we present\nefficient design principles for 3D CNNs, making use of insights from the 2D\ndeep learning community. In particular, we present Inception3D as a new\narchitecture which performs best for our application. We show that our deep\nlearning approach reaches errors at our ground-truth label's resolution. We\nachieve a mean average error of $\\SI{14.89 \\pm 9.3}{\\micro\\metre}$ and\n$\\SI{0.096 \\pm 0.072}{\\degree}$ for position and orientation learning,\nrespectively.\n", "rewritten_text": "\\boxed{\nEstimating the pose of instruments is a crucial task in image-guided surgery, and optical coherence tomography (OCT) is increasingly being used for this purpose due to its high resolution and 3D imaging capabilities. However, processing OCT images is challenging due to noise and artifacts. To address this, we developed a deep learning-based framework that uses a 3D convolutional neural network (CNN) to predict the 6D pose of a marker geometry from OCT volumes. We trained the network using labeled data acquired with a hexapod robot and demonstrated that using 3D volume information yields higher accuracy than relying on 2D representations with depth information. Our results show that the 3D CNN effectively exploits the depth structure of marker objects, achieving a mean average error of 14.89 \u00b1 9.3 \u03bcm for position and 0.096 \u00b1 0.072\u00b0 for orientation learning. We also introduced a new architecture, Inception3D, which performed best for our application."}, "2104.04986": {"original_text": "  Aspect-based Sentiment Analysis (ABSA), aiming at predicting the polarities\nfor aspects, is a fine-grained task in the field of sentiment analysis.\nPrevious work showed syntactic information, e.g. dependency trees, can\neffectively improve the ABSA performance. Recently, pre-trained models (PTMs)\nalso have shown their effectiveness on ABSA. Therefore, the question naturally\narises whether PTMs contain sufficient syntactic information for ABSA so that\nwe can obtain a good ABSA model only based on PTMs. In this paper, we firstly\ncompare the induced trees from PTMs and the dependency parsing trees on several\npopular models for the ABSA task, showing that the induced tree from fine-tuned\nRoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis\nexperiments reveal that the FT-RoBERTa Induced Tree is more\nsentiment-word-oriented and could benefit the ABSA task. The experiments also\nshow that the pure RoBERTa-based model can outperform or approximate to the\nprevious SOTA performances on six datasets across four languages since it\nimplicitly incorporates the task-oriented syntactic information.\n", "rewritten_text": "\\boxed{Aspect-based Sentiment Analysis (ABSA) is a nuanced task that involves predicting the sentiment of specific aspects. While previous studies have shown that incorporating syntactic information, such as dependency trees, can enhance ABSA performance, recent advancements in pre-trained models (PTMs) have also demonstrated their effectiveness in this task. This raises the question of whether PTMs inherently contain sufficient syntactic information to achieve good ABSA results without additional syntactic features. Our research compares the induced trees from PTMs with traditional dependency parsing trees and finds that the fine-tuned RoBERTa-induced tree outperforms the parser-provided tree. Further analysis reveals that the RoBERTa-induced tree is more focused on sentiment-bearing words, which benefits ABSA. Moreover, our experiments show that a RoBERTa-based model can achieve or approximate state-of-the-art performances on six datasets across four languages, suggesting that it can implicitly incorporate task-oriented syntactic information.}"}, "2012.11581": {"original_text": "  Humans live within a 3D space and constantly interact with it to perform\ntasks. Such interactions involve physical contact between surfaces that is\nsemantically meaningful. Our goal is to learn how humans interact with scenes\nand leverage this to enable virtual characters to do the same. To that end, we\nintroduce a novel Human-Scene Interaction (HSI) model that encodes proximal\nrelationships, called POSA for \"Pose with prOximitieS and contActs\". The\nrepresentation of interaction is body-centric, which enables it to generalize\nto new scenes. Specifically, POSA augments the SMPL-X parametric human body\nmodel such that, for every mesh vertex, it encodes (a) the contact probability\nwith the scene surface and (b) the corresponding semantic scene label. We learn\nPOSA with a VAE conditioned on the SMPL-X vertices, and train on the PROX\ndataset, which contains SMPL-X meshes of people interacting with 3D scenes, and\nthe corresponding scene semantics from the PROX-E dataset. We demonstrate the\nvalue of POSA with two applications. First, we automatically place 3D scans of\npeople in scenes. We use a SMPL-X model fit to the scan as a proxy and then\nfind its most likely placement in 3D. POSA provides an effective representation\nto search for \"affordances\" in the scene that match the likely contact\nrelationships for that pose. We perform a perceptual study that shows\nsignificant improvement over the state of the art on this task. Second, we show\nthat POSA's learned representation of body-scene interaction supports monocular\nhuman pose estimation that is consistent with a 3D scene, improving on the\nstate of the art. Our model and code are available for research purposes at\nhttps://posa.is.tue.mpg.de.\n", "rewritten_text": "\\boxed{\nHumans interact with their 3D environment in meaningful ways, and our goal is to enable virtual characters to do the same. To achieve this, we propose a novel Human-Scene Interaction (HSI) model called POSA, which captures the relationships between the human body and the scene. POSA is a body-centric representation that can generalize to new scenes and is learned using a VAE conditioned on the SMPL-X parametric human body model. We train POSA on the PROX dataset, which contains 3D scans of people interacting with scenes, and demonstrate its value in two applications. Firstly, we use POSA to automatically place 3D scans of people in scenes, achieving significant improvement over the state of the art. Secondly, we show that POSA supports monocular human pose estimation that is consistent with a 3D scene, outperforming existing methods. Our model and code are available at https://posa.is.tue.mpg.de.\n}"}, "1609.04079": {"original_text": "  We present a single-shot system to recover surface geometry of objects with\nspatially-varying albedos, from images captured under a calibrated RGB\nphotometric stereo setup---with three light directions multiplexed across\ndifferent color channels in the observed RGB image. Since the problem is\nill-posed point-wise, we assume that the albedo map can be modeled as\npiece-wise constant with a restricted number of distinct albedo values. We show\nthat under ideal conditions, the shape of a non-degenerate local constant\nalbedo surface patch can theoretically be recovered exactly. Moreover, we\npresent a practical and efficient algorithm that uses this model to robustly\nrecover shape from real images. Our method first reasons about shape locally in\na dense set of patches in the observed image, producing shape distributions for\nevery patch. These local distributions are then combined to produce a single\nconsistent surface normal map. We demonstrate the efficacy of the approach\nthrough experiments on both synthetic renderings as well as real captured\nimages.\n", "rewritten_text": "\\boxed{We propose a single-shot system to reconstruct the surface geometry of objects with varying albedos from RGB images captured under a calibrated photometric stereo setup. By assuming the albedo map can be modeled as piece-wise constant with a limited number of distinct values, we show that the shape of a local surface patch can be exactly recovered under ideal conditions. Our practical algorithm recovers shape from real images by first estimating shape distributions locally in a dense set of patches, and then combining them to produce a consistent surface normal map. We validate our approach through experiments on both synthetic and real-world images.}"}, "2305.15699": {"original_text": "  Understanding action recognition in egocentric videos has emerged as a vital\nresearch topic with numerous practical applications. With the limitation in the\nscale of egocentric data collection, learning robust deep learning-based action\nrecognition models remains difficult. Transferring knowledge learned from the\nlarge-scale exocentric data to the egocentric data is challenging due to the\ndifference in videos across views. Our work introduces a novel cross-view\nlearning approach to action recognition (CVAR) that effectively transfers\nknowledge from the exocentric to the selfish view. First, we present a novel\ngeometric-based constraint into the self-attention mechanism in Transformer\nbased on analyzing the camera positions between two views. Then, we propose a\nnew cross-view self-attention loss learned on unpaired cross-view data to\nenforce the self-attention mechanism learning to transfer knowledge across\nviews. Finally, to further improve the performance of our cross-view learning\napproach, we present the metrics to measure the correlations in videos and\nattention maps effectively. Experimental results on standard egocentric action\nrecognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, and\nEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-art\nperformance.\n", "rewritten_text": "\\boxed{\nRecognizing actions in egocentric videos is a crucial research area with many practical applications. However, collecting large-scale egocentric data is limited, making it challenging to develop robust deep learning-based action recognition models. A major obstacle is transferring knowledge from large-scale exocentric data to egocentric data due to differences in video views. To address this, we propose a novel cross-view learning approach for action recognition (CVAR) that effectively transfers knowledge from exocentric to egocentric views. Our approach introduces a geometric-based constraint into the self-attention mechanism of Transformers, analyzing camera positions between views. We also propose a cross-view self-attention loss to enforce knowledge transfer across views. Additionally, we develop metrics to measure video and attention map correlations, further improving our approach's performance. Our method achieves state-of-the-art results on standard egocentric action recognition benchmarks, including Charades-Ego, EPIC-Kitchens-55, and EPIC-Kitchens-100."}, "2407.10753": {"original_text": "  Accurate depth information is crucial for enhancing the performance of\nmulti-view 3D object detection. Despite the success of some existing multi-view\n3D detectors utilizing pixel-wise depth supervision, they overlook two\nsignificant phenomena: 1) the depth supervision obtained from LiDAR points is\nusually distributed on the surface of the object, which is not so friendly to\nexisting DETR-based 3D detectors due to the lack of the depth of 3D object\ncenter; 2) for distant objects, fine-grained depth estimation of the whole\nobject is more challenging. Therefore, we argue that the object-wise depth (or\n3D center of the object) is essential for accurate detection. In this paper, we\npropose a new multi-view 3D object detector named OPEN, whose main idea is to\neffectively inject object-wise depth information into the network through our\nproposed object-wise position embedding. Specifically, we first employ an\nobject-wise depth encoder, which takes the pixel-wise depth map as a prior, to\naccurately estimate the object-wise depth. Then, we utilize the proposed\nobject-wise position embedding to encode the object-wise depth information into\nthe transformer decoder, thereby producing 3D object-aware features for final\ndetection. Extensive experiments verify the effectiveness of our proposed\nmethod. Furthermore, OPEN achieves a new state-of-the-art performance with\n64.4% NDS and 56.7% mAP on the nuScenes test benchmark.\n", "rewritten_text": "\\boxed{\nAccurate depth information is vital for improving multi-view 3D object detection. However, existing detectors that use pixel-wise depth supervision overlook two key issues: 1) LiDAR points only provide surface-level depth information, which is insufficient for DETR-based detectors that require object center depth; and 2) estimating depth for distant objects is particularly challenging. To address this, we propose a new detector called OPEN, which incorporates object-wise depth information into the network using a novel object-wise position embedding. Our approach involves estimating object-wise depth using an encoder that takes pixel-wise depth maps as input, and then encoding this information into the transformer decoder to produce 3D object-aware features. Experimental results demonstrate the effectiveness of our method, with OPEN achieving state-of-the-art performance on the nuScenes test benchmark with 64.4% NDS and 56.7% mAP."}, "2308.10445": {"original_text": "  Incremental learning aims to overcome catastrophic forgetting when learning\ndeep networks from sequential tasks. With impressive learning efficiency and\nperformance, prompt-based methods adopt a fixed backbone to sequential tasks by\nlearning task-specific prompts. However, existing prompt-based methods heavily\nrely on strong pretraining (typically trained on ImageNet-21k), and we find\nthat their models could be trapped if the potential gap between the pretraining\ntask and unknown future tasks is large. In this work, we develop a learnable\nAdaptive Prompt Generator (APG). The key is to unify the prompt retrieval and\nprompt learning processes into a learnable prompt generator. Hence, the whole\nprompting process can be optimized to reduce the negative effects of the gap\nbetween tasks effectively. To make our APG avoid learning ineffective\nknowledge, we maintain a knowledge pool to regularize APG with the feature\ndistribution of each class. Extensive experiments show that our method\nsignificantly outperforms advanced methods in exemplar-free incremental\nlearning without (strong) pretraining. Besides, under strong retraining, our\nmethod also has comparable performance to existing prompt-based models, showing\nthat our method can still benefit from pretraining. Codes can be found at\nhttps://github.com/TOM-tym/APG\n", "rewritten_text": "\\boxed{\nIncremental learning faces the challenge of catastrophic forgetting when training deep networks on sequential tasks. Prompt-based methods have shown impressive efficiency and performance by using a fixed backbone and learning task-specific prompts. However, these methods rely heavily on strong pretraining, typically on ImageNet-21k, and may struggle when the gap between pretraining and future tasks is large. To address this, we propose a learnable Adaptive Prompt Generator (APG) that unifies prompt retrieval and learning into a single generator. This approach optimizes the prompting process to reduce the negative effects of task gaps. We also maintain a knowledge pool to regularize APG and prevent ineffective knowledge learning. Our experiments demonstrate that APG outperforms advanced methods in exemplar-free incremental learning without strong pretraining, and still benefits from pretraining when retrained. The code is available at https://github.com/TOM-tym/APG.\n}"}, "2409.18355": {"original_text": "  Cone Beam Computed Tomography (CBCT) finds diverse applications in medicine.\nEnsuring high image quality in CBCT scans is essential for accurate diagnosis\nand treatment delivery. Yet, the susceptibility of CBCT images to noise and\nartifacts undermines both their usefulness and reliability. Existing methods\ntypically address CBCT artifacts through image-to-image translation approaches.\nThese methods, however, are limited by the artifact types present in the\ntraining data, which may not cover the complete spectrum of CBCT degradations\nstemming from variations in imaging protocols. Gathering additional data to\nencompass all possible scenarios can often pose a challenge. To address this,\nwe present SinoSynth, a physics-based degradation model that simulates various\nCBCT-specific artifacts to generate a diverse set of synthetic CBCT images from\nhigh-quality CT images without requiring pre-aligned data. Through extensive\nexperiments, we demonstrate that several different generative networks trained\non our synthesized data achieve remarkable results on heterogeneous\nmulti-institutional datasets, outperforming even the same networks trained on\nactual data. We further show that our degradation model conveniently provides\nan avenue to enforce anatomical constraints in conditional generative models,\nyielding high-quality and structure-preserving synthetic CT images.\n", "rewritten_text": "\\boxed{\nCone Beam Computed Tomography (CBCT) is a valuable medical tool, but its images are often compromised by noise and artifacts, which can lead to inaccurate diagnoses and treatments. Current methods for addressing these issues are limited by the types of artifacts present in the training data and may not account for all possible variations in imaging protocols. To overcome this, we developed SinoSynth, a physics-based model that simulates CBCT-specific artifacts to generate synthetic images from high-quality CT scans without requiring pre-aligned data. Our experiments show that generative networks trained on these synthetic images outperform those trained on actual data, even on diverse, multi-institutional datasets. Additionally, our model allows for the enforcement of anatomical constraints, resulting in high-quality, structure-preserving synthetic CT images.\n}"}, "1904.07904": {"original_text": "  Spoken question answering (SQA) is challenging due to complex reasoning on\ntop of the spoken documents. The recent studies have also shown the\ncatastrophic impact of automatic speech recognition (ASR) errors on SQA.\nTherefore, this work proposes to mitigate the ASR errors by aligning the\nmismatch between ASR hypotheses and their corresponding reference\ntranscriptions. An adversarial model is applied to this domain adaptation task,\nwhich forces the model to learn domain-invariant features the QA model can\neffectively utilize in order to improve the SQA results. The experiments\nsuccessfully demonstrate the effectiveness of our proposed model, and the\nresults are better than the previous best model by 2% EM score.\n", "rewritten_text": "\\boxed{Answering spoken questions is difficult due to the complexity of spoken language and the errors that occur during speech recognition. To address this, we propose a method to reduce these errors by aligning the recognized speech with the correct transcriptions. We use an adversarial model to adapt to the differences between the two, allowing our question answering model to learn features that are not specific to a particular domain and improving the overall performance. Our experiments show that this approach is effective, outperforming the previous best model by 2% in terms of exact match score.}"}, "2310.00936": {"original_text": "  Recent studies on StyleGAN variants show promising performances for various\ngeneration tasks. In these models, latent codes have traditionally been\nmanipulated and searched for the desired images. However, this approach\nsometimes suffers from a lack of photorealism in generated images due to a lack\nof knowledge about the geometry of the trained latent space. In this paper, we\nshow a simple unsupervised method that provides well-trained local latent\nsubspace, enabling latent code navigation while preserving the photorealism of\nthe generated images. Specifically, the method identifies densely mapped latent\nspaces and restricts latent manipulations within the local latent subspace.\nExperimental results demonstrate that images generated within the local latent\nsubspace maintain photorealism even when the latent codes are significantly and\nrepeatedly manipulated. Moreover, experiments show that the method can be\napplied to latent code optimization for various types of style-based models.\nOur empirical evidence of the method will benefit applications in style-based\nmodels.\n", "rewritten_text": "\\boxed{\nRecent research on StyleGAN variants has achieved impressive results in image generation tasks. However, manipulating latent codes to produce desired images can sometimes lead to a lack of photorealism due to limited understanding of the latent space geometry. This paper presents a simple, unsupervised method that identifies well-defined local latent subspaces, enabling latent code navigation while maintaining photorealism. By restricting latent manipulations within these subspaces, our approach generates high-quality images even when latent codes are significantly and repeatedly modified. Furthermore, our method can be applied to optimize latent codes for various style-based models, with promising implications for applications in this field.\n}"}, "2310.00796": {"original_text": "  Strong inductive biases enable learning from little data and help\ngeneralization outside of the training distribution. Popular neural\narchitectures such as Transformers lack strong structural inductive biases for\nseq2seq NLP tasks on their own. Consequently, they struggle with systematic\ngeneralization beyond the training distribution, e.g. with extrapolating to\nlonger inputs, even when pre-trained on large amounts of text. We show how a\nstructural inductive bias can be efficiently injected into a seq2seq model by\npre-training it to simulate structural transformations on synthetic data.\nSpecifically, we inject an inductive bias towards Finite State Transducers\n(FSTs) into a Transformer by pre-training it to simulate FSTs given their\ndescriptions. Our experiments show that our method imparts the desired\ninductive bias, resulting in improved systematic generalization and better\nfew-shot learning for FST-like tasks. Our analysis shows that fine-tuned models\naccurately capture the state dynamics of the unseen underlying FSTs, suggesting\nthat the simulation process is internalized by the fine-tuned model.\n", "rewritten_text": "\\boxed{Neural architectures like Transformers, despite being powerful, lack inherent structural biases that enable learning from limited data and generalizing beyond their training datasets. This limitation hinders their ability to extrapolate to new, unseen scenarios, such as longer input sequences. To address this, we propose a method to inject a structural inductive bias into a seq2seq model by pre-training it on synthetic data that simulates structural transformations. Specifically, we pre-train a Transformer to mimic Finite State Transducers (FSTs) based on their descriptions. Our results show that this approach successfully imparts the desired bias, leading to improved generalization and few-shot learning capabilities for FST-like tasks. Furthermore, our analysis reveals that fine-tuned models accurately capture the underlying state dynamics of unseen FSTs, indicating that the simulation process is effectively internalized by the model.}"}, "2309.11566": {"original_text": "  We introduce SignBank+, a clean version of the SignBank dataset, optimized\nfor machine translation between spoken language text and SignWriting, a\nphonetic sign language writing system. In addition to previous work that\nemploys complex factorization techniques to enable translation between text and\nSignWriting, we show that a traditional text-to-text translation approach\nperforms equally effectively on the cleaned SignBank+ dataset. Our evaluation\nresults indicate that models trained on SignBank+ surpass those on the original\ndataset, establishing a new benchmark for SignWriting-based sign language\ntranslation and providing an open resource for future research.\n", "rewritten_text": "\\boxed{We present SignBank+, a refined version of the SignBank dataset, specifically designed to facilitate machine translation between spoken language text and SignWriting, a phonetic sign language notation system. Unlike previous approaches that rely on complex factorization techniques, we demonstrate that a conventional text-to-text translation method achieves comparable performance on the cleaned SignBank+ dataset. Our evaluation results reveal that models trained on SignBank+ outperform those trained on the original dataset, setting a new standard for SignWriting-based sign language translation and providing a valuable resource for future research.}"}, "2212.00244": {"original_text": "  Domain adaptation for Cross-LiDAR 3D detection is challenging due to the\nlarge gap on the raw data representation with disparate point densities and\npoint arrangements. By exploring domain-invariant 3D geometric characteristics\nand motion patterns, we present an unsupervised domain adaptation method that\novercomes above difficulties. First, we propose the Spatial Geometry Alignment\nmodule to extract similar 3D shape geometric features of the same object class\nto align two domains, while eliminating the effect of distinct point\ndistributions. Second, we present Temporal Motion Alignment module to utilize\nmotion features in sequential frames of data to match two domains. Prototypes\ngenerated from two modules are incorporated into the pseudo-label reweighting\nprocedure and contribute to our effective self-training framework for the\ntarget domain. Extensive experiments show that our method achieves\nstate-of-the-art performance on cross-device datasets, especially for the\ndatasets with large gaps captured by mechanical scanning LiDARs and solid-state\nLiDARs in various scenes. Project homepage is at\nhttps://github.com/4DVLab/CL3D.git\n", "rewritten_text": "\\boxed{\nDomain adaptation for 3D object detection using Cross-LiDAR data is a challenging task due to the significant differences in raw data representation, including varying point densities and arrangements. To address this issue, we propose an unsupervised domain adaptation method that focuses on extracting domain-invariant 3D geometric features and motion patterns. Our approach consists of two modules: Spatial Geometry Alignment, which extracts similar 3D shape features to align the two domains and eliminate the effect of distinct point distributions, and Temporal Motion Alignment, which utilizes motion features in sequential frames to match the two domains. The prototypes generated from these modules are incorporated into a pseudo-label reweighting procedure, contributing to an effective self-training framework for the target domain. Our method achieves state-of-the-art performance on cross-device datasets, particularly for datasets with large gaps captured by mechanical scanning LiDARs and solid-state LiDARs in various scenes. The project's code is available at https://github.com/4DVLab/CL3D.git.\n}"}, "2210.10239": {"original_text": "  This paper aims to investigate representation learning for large scale visual\nplace recognition, which consists of determining the location depicted in a\nquery image by referring to a database of reference images. This is a\nchallenging task due to the large-scale environmental changes that can occur\nover time (i.e., weather, illumination, season, traffic, occlusion). Progress\nis currently challenged by the lack of large databases with accurate ground\ntruth. To address this challenge, we introduce GSV-Cities, a new image dataset\nproviding the widest geographic coverage to date with highly accurate ground\ntruth, covering more than 40 cities across all continents over a 14-year\nperiod. We subsequently explore the full potential of recent advances in deep\nmetric learning to train networks specifically for place recognition, and\nevaluate how different loss functions influence performance. In addition, we\nshow that performance of existing methods substantially improves when trained\non GSV-Cities. Finally, we introduce a new fully convolutional aggregation\nlayer that outperforms existing techniques, including GeM, NetVLAD and\nCosPlace, and establish a new state-of-the-art on large-scale benchmarks, such\nas Pittsburgh, Mapillary-SLS, SPED and Nordland. The dataset and code are\navailable for research purposes at https://github.com/amaralibey/gsv-cities.\n", "rewritten_text": "\\boxed{\nThis paper explores the task of visual place recognition, which involves identifying the location of a query image by comparing it to a database of reference images. This task is challenging due to environmental changes over time, such as weather, illumination, and season. Currently, the lack of large databases with accurate ground truth hinders progress. To address this, we introduce GSV-Cities, a new dataset with the widest geographic coverage to date, featuring highly accurate ground truth and covering over 40 cities across 14 years. We utilize recent advances in deep metric learning to train networks for place recognition and evaluate the impact of different loss functions on performance. Our results show that existing methods improve significantly when trained on GSV-Cities. Furthermore, we propose a new fully convolutional aggregation layer that outperforms existing techniques and sets a new state-of-the-art on large-scale benchmarks. The dataset and code are available at https://github.com/amaralibey/gsv-cities."}, "2202.04101": {"original_text": "  Photoplethysmography (PPG) signals have become a key technology in many\nfields, such as medicine, well-being, or sports. Our work proposes a set of\npipelines to extract remote PPG signals (rPPG) from the face robustly,\nreliably, and configurable. We identify and evaluate the possible choices in\nthe critical steps of unsupervised rPPG methodologies. We assess a\nstate-of-the-art processing pipeline in six different datasets, incorporating\nimportant corrections in the methodology that ensure reproducible and fair\ncomparisons. In addition, we extend the pipeline by proposing three novel\nideas; 1) a new method to stabilize the detected face based on a rigid mesh\nnormalization; 2) a new method to dynamically select the different regions in\nthe face that provide the best raw signals, and 3) a new RGB to rPPG\ntransformation method, called Orthogonal Matrix Image Transformation (OMIT)\nbased on QR decomposition, that increases robustness against compression\nartifacts. We show that all three changes introduce noticeable improvements in\nretrieving rPPG signals from faces, obtaining state-of-the-art results compared\nwith unsupervised, non-learning-based methodologies and, in some databases,\nvery close to supervised, learning-based methods. We perform a comparative\nstudy to quantify the contribution of each proposed idea. In addition, we\ndepict a series of observations that could help in future implementations.\n", "rewritten_text": "\\boxed{\nThis research proposes a set of pipelines to extract remote photoplethysmography (rPPG) signals from facial images in a robust, reliable, and configurable manner. The study evaluates various unsupervised rPPG methodologies and identifies key steps that impact signal extraction. A state-of-the-art processing pipeline is assessed on six datasets, with modifications to ensure reproducible and fair comparisons. Three novel ideas are introduced to improve signal extraction: a face stabilization method using rigid mesh normalization, a dynamic region selection approach, and an Orthogonal Matrix Image Transformation (OMIT) method to increase robustness against compression artifacts. The results show significant improvements in retrieving rPPG signals, achieving state-of-the-art performance compared to unsupervised methods and approaching the performance of supervised learning-based methods. A comparative study is conducted to quantify the contribution of each proposed idea, and observations are provided to inform future implementations."}, "1803.05449": {"original_text": "  We introduce SentEval, a toolkit for evaluating the quality of universal\nsentence representations. SentEval encompasses a variety of tasks, including\nbinary and multi-class classification, natural language inference and sentence\nsimilarity. The set of tasks was selected based on what appears to be the\ncommunity consensus regarding the appropriate evaluations for universal\nsentence representations. The toolkit comes with scripts to download and\npreprocess datasets, and an easy interface to evaluate sentence encoders. The\naim is to provide a fairer, less cumbersome and more centralized way for\nevaluating sentence representations.\n", "rewritten_text": "\\boxed{We present SentEval, a comprehensive toolkit designed to assess the quality of universal sentence representations. This toolkit offers a range of evaluation tasks, including binary and multi-class classification, natural language inference, and sentence similarity, which are widely recognized as essential benchmarks for universal sentence representations. SentEval provides user-friendly scripts for dataset download and preprocessing, as well as a straightforward interface for evaluating sentence encoders, with the goal of facilitating a more standardized, efficient, and fair evaluation process.}"}, "1506.05929": {"original_text": "  Previous work has shown that the artist of an artwork can be identified by\nuse of computational methods that analyse digital images. However, the\ndigitised artworks are often investigated at a coarse scale discarding many of\nthe important details that may define an artist's style. In recent years high\nresolution images of artworks have become available, which, combined with\nincreased processing power and new computational techniques, allow us to\nanalyse digital images of artworks at a very fine scale. In this work we train\nand evaluate a Convolutional Neural Network (CNN) on the task of artist\nattribution using artwork images of varying resolutions. To this end, we\ncombine two existing methods to enable the application of high resolution\nimages to CNNs. By comparing the attribution performances obtained at different\nscales, we find that in most cases finer scales are beneficial to the\nattribution performance, whereas for a minority of the artists, coarser scales\nappear to be preferable. We conclude that artist attribution would benefit from\na multi-scale CNN approach which vastly expands the possibilities for\ncomputational art forensics.\n", "rewritten_text": "\\boxed{Previous research has demonstrated that computational methods can identify an artwork's creator by analyzing digital images. However, these methods often overlook crucial details that define an artist's style by examining artworks at a coarse scale. With the recent availability of high-resolution artwork images and advancements in processing power and computational techniques, we can now analyze digital images at a finer scale. This study trains and evaluates a Convolutional Neural Network (CNN) to attribute artworks to their creators using images of varying resolutions. By combining two existing methods, we enable the application of high-resolution images to CNNs. Our results show that finer scales generally improve attribution performance, although some artists' styles are better captured at coarser scales. We conclude that a multi-scale CNN approach would significantly enhance computational art forensics.}"}, "2112.03803": {"original_text": "  Despite the great progress in video understanding made by deep convolutional\nneural networks, feature representation learned by existing methods may be\nbiased to static visual cues. To address this issue, we propose a novel method\nto suppress static visual cues (SSVC) based on probabilistic analysis for\nself-supervised video representation learning. In our method, video frames are\nfirst encoded to obtain latent variables under standard normal distribution via\nnormalizing flows. By modelling static factors in a video as a random variable,\nthe conditional distribution of each latent variable becomes shifted and scaled\nnormal. Then, the less-varying latent variables along time are selected as\nstatic cues and suppressed to generate motion-preserved videos. Finally,\npositive pairs are constructed by motion-preserved videos for contrastive\nlearning to alleviate the problem of representation bias to static cues. The\nless-biased video representation can be better generalized to various\ndownstream tasks. Extensive experiments on publicly available benchmarks\ndemonstrate that the proposed method outperforms the state of the art when only\nsingle RGB modality is used for pre-training.\n", "rewritten_text": "\\boxed{Deep convolutional neural networks have made significant progress in video understanding, but existing methods may prioritize static visual cues. To overcome this limitation, we introduce a novel approach called Suppressing Static Visual Cues (SSVC) for self-supervised video representation learning. Our method involves encoding video frames into latent variables using normalizing flows, modeling static factors as a random variable, and selecting less-varying latent variables as static cues to suppress. This generates motion-preserved videos, which are then used to construct positive pairs for contrastive learning, reducing representation bias towards static cues. Our approach leads to a more generalizable video representation, outperforming state-of-the-art methods on publicly available benchmarks when using a single RGB modality for pre-training.}"}, "2103.16889": {"original_text": "  Current state-of-the-art visual recognition systems usually rely on the\nfollowing pipeline: (a) pretraining a neural network on a large-scale dataset\n(e.g., ImageNet) and (b) finetuning the network weights on a smaller,\ntask-specific dataset. Such a pipeline assumes the sole weight adaptation is\nable to transfer the network capability from one domain to another domain,\nbased on a strong assumption that a fixed architecture is appropriate for all\ndomains. However, each domain with a distinct recognition target may need\ndifferent levels/paths of feature hierarchy, where some neurons may become\nredundant, and some others are re-activated to form new network structures. In\nthis work, we prove that dynamically adapting network architectures tailored\nfor each domain task along with weight finetuning benefits in both efficiency\nand effectiveness, compared to the existing image recognition pipeline that\nonly tunes the weights regardless of the architecture. Our method can be easily\ngeneralized to an unsupervised paradigm by replacing supernet training with\nself-supervised learning in the source domain tasks and performing linear\nevaluation in the downstream tasks. This further improves the search efficiency\nof our method. Moreover, we also provide principled and empirical analysis to\nexplain why our approach works by investigating the ineffectiveness of existing\nneural architecture search. We find that preserving the joint distribution of\nthe network architecture and weights is of importance. This analysis not only\nbenefits image recognition but also provides insights for crafting neural\nnetworks. Experiments on five representative image recognition tasks such as\nperson re-identification, age estimation, gender recognition, image\nclassification, and unsupervised domain adaptation demonstrate the\neffectiveness of our method.\n", "rewritten_text": "\\boxed{\nCurrent visual recognition systems typically follow a two-step process: pretraining a neural network on a large dataset and fine-tuning it on a smaller, task-specific dataset. However, this approach assumes that a fixed architecture is suitable for all domains, which may not be the case. Each domain may require a unique feature hierarchy, making some neurons redundant and others essential for new network structures. This paper proposes a novel approach that dynamically adapts network architectures for each domain task, in addition to fine-tuning weights. This approach is shown to be more efficient and effective than the traditional pipeline. Furthermore, the method can be extended to unsupervised learning by replacing supernet training with self-supervised learning and linear evaluation. The paper also provides an analysis of why this approach works, highlighting the importance of preserving the joint distribution of network architecture and weights. Experimental results on five image recognition tasks demonstrate the effectiveness of this method.\n}"}, "2003.13239": {"original_text": "  Cross view feature fusion is the key to address the occlusion problem in\nhuman pose estimation. The current fusion methods need to train a separate\nmodel for every pair of cameras making them difficult to scale. In this work,\nwe introduce MetaFuse, a pre-trained fusion model learned from a large number\nof cameras in the Panoptic dataset. The model can be efficiently adapted or\nfinetuned for a new pair of cameras using a small number of labeled images. The\nstrong adaptation power of MetaFuse is due in large part to the proposed\nfactorization of the original fusion model into two parts (1) a generic fusion\nmodel shared by all cameras, and (2) lightweight camera-dependent\ntransformations. Furthermore, the generic model is learned from many cameras by\na meta-learning style algorithm to maximize its adaptation capability to\nvarious camera poses. We observe in experiments that MetaFuse finetuned on the\npublic datasets outperforms the state-of-the-arts by a large margin which\nvalidates its value in practice.\n", "rewritten_text": "\\boxed{\nAddressing the occlusion problem in human pose estimation requires effective cross-view feature fusion. However, current methods are limited by the need to train a separate model for each camera pair, making them difficult to scale. To overcome this, we propose MetaFuse, a pre-trained fusion model learned from a large number of cameras in the Panoptic dataset. This model can be efficiently adapted to new camera pairs using a small number of labeled images, thanks to its strong adaptation power. This power is attributed to the factorization of the original fusion model into a generic fusion model shared by all cameras and lightweight camera-dependent transformations. The generic model is learned through a meta-learning style algorithm to maximize its adaptation capability to various camera poses. Experimental results on public datasets demonstrate that MetaFuse outperforms state-of-the-art methods by a significant margin, validating its practical value.\n}"}, "2011.12528": {"original_text": "  We propose a novel reference-based video colorization framework with\nspatiotemporal correspondence. Reference-based methods colorize grayscale\nframes referencing a user input color frame. Existing methods suffer from the\ncolor leakage between objects and the emergence of average colors, derived from\nnon-local semantic correspondence in space. To address this issue, we warp\ncolors only from the regions on the reference frame restricted by\ncorrespondence in time. We propagate masks as temporal correspondences, using\ntwo complementary tracking approaches: off-the-shelf instance tracking for high\nperformance segmentation, and newly proposed dense tracking to track various\ntypes of objects. By restricting temporally-related regions for referencing\ncolors, our approach propagates faithful colors throughout the video.\nExperiments demonstrate that our method outperforms state-of-the-art methods\nquantitatively and qualitatively.\n", "rewritten_text": "\\boxed{We introduce a new video colorization framework that uses a reference image and spatiotemporal correspondence to accurately colorize grayscale frames. Unlike existing methods that suffer from color bleeding between objects and the creation of average colors, our approach restricts color transfer to regions with temporal correspondence, ensuring faithful color propagation throughout the video. We achieve this by combining two tracking approaches: instance tracking for high-performance segmentation and a novel dense tracking method for various object types. Our experiments show that our method outperforms current state-of-the-art methods in both quantitative and qualitative evaluations.}"}}