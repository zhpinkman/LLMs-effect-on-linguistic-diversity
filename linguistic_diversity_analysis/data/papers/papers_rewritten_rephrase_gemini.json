{"1812.10788": {"original_text": "  Hyperspectral remote sensing is a prominent research topic in data\nprocessing. Most of the spectral unmixing algorithms are developed by adopting\nthe linear mixing models. Nonnegative matrix factorization (NMF) and its\ndevelopments are used widely for estimation of signatures and fractional\nabundances in the SU problem. Sparsity constraints was added to NMF, and was\nregularized by $ L_ {q} $ norm. In this paper, at first hyperspectral images\nare clustered by fuzzy c- means method, and then a new algorithm based on\nsparsity constrained distributed optimization is used for spectral unmixing. In\nthe proposed algorithm, a network including clusters is employed. Each pixel in\nthe hyperspectral images considered as a node in this network. The proposed\nalgorithm is optimized with diffusion LMS strategy, and then the update\nequations for fractional abundance and signature matrices are obtained.\nSimulation results based on defined performance metrics illustrate advantage of\nthe proposed algorithm in spectral unmixing of hyperspectral data compared with\nother methods.\n", "rewritten_text": "This paper addresses spectral unmixing (SU) in hyperspectral remote sensing, a field dominated by linear mixing models.  While Nonnegative Matrix Factorization (NMF) and its variants are commonly used for estimating spectral signatures and fractional abundances, this work proposes a novel algorithm incorporating sparsity constraints regularized by the Lq norm.  The approach begins by clustering the hyperspectral image using fuzzy c-means.  A distributed optimization algorithm is then applied, representing each pixel as a node in a network of clusters.  Optimized using a diffusion least mean squares (LMS) strategy, this method yields updated equations for fractional abundance and signature matrices.  Simulation results demonstrate the superior performance of the proposed algorithm compared to existing methods, as measured by established performance metrics.\n"}, "2305.04628": {"original_text": "  In this paper, we introduce a novel framework for the challenging problem of\nOne-Shot Unsupervised Domain Adaptation (OSUDA), which aims to adapt to a\ntarget domain with only a single unlabeled target sample. Unlike existing\napproaches that rely on large labeled source and unlabeled target data, our\nTarget-driven One-Shot UDA (TOS-UDA) approach employs a learnable augmentation\nstrategy guided by the target sample's style to align the source distribution\nwith the target distribution. Our method consists of three modules: an\naugmentation module, a style alignment module, and a classifier. Unlike\nexisting methods, our augmentation module allows for strong transformations of\nthe source samples, and the style of the single target sample available is\nexploited to guide the augmentation by ensuring perceptual similarity.\nFurthermore, our approach integrates augmentation with style alignment,\neliminating the need for separate pre-training on additional datasets. Our\nmethod outperforms or performs comparably to existing OS-UDA methods on the\nDigits and DomainNet benchmarks.\n", "rewritten_text": "This paper presents TOS-UDA, a novel framework for One-Shot Unsupervised Domain Adaptation (OSUDA).  Addressing the challenge of adapting to a target domain using only one unlabeled target sample, TOS-UDA leverages a learnable augmentation strategy guided by the target sample's style to align source and target distributions.  Unlike methods requiring large labeled source and unlabeled target datasets, our three-module approach (augmentation, style alignment, and classification) uses strong, target-style-guided source data transformations.  This integrated approach eliminates the need for separate pre-training, achieving performance comparable to or exceeding existing OS-UDA methods on Digits and DomainNet benchmarks.\n"}, "2011.14589": {"original_text": "  Monocular 3D object detection, with the aim of predicting the geometric\nproperties of on-road objects, is a promising research topic for the\nintelligent perception systems of autonomous driving. Most state-of-the-art\nmethods follow a keypoint-based paradigm, where the keypoints of objects are\npredicted and employed as the basis for regressing the other geometric\nproperties. In this work, a unified network named as FADNet is presented to\naddress the task of monocular 3D object detection. In contrast to previous\nkeypoint-based methods, we propose to divide the output modalities into\ndifferent groups according to the estimation difficulty of object properties.\nDifferent groups are treated differently and sequentially associated by a\nconvolutional Gated Recurrent Unit. Another contribution of this work is the\nstrategy of depth hint augmentation. To provide characterized depth patterns as\nhints for depth estimation, a dedicated depth hint module is designed to\ngenerate row-wise features named as depth hints, which are explicitly\nsupervised in a bin-wise manner. The contributions of this work are validated\nby conducting experiments and ablation study on the KITTI benchmark. Without\nutilizing depth priors, post optimization, or other refinement modules, our\nnetwork performs competitively against state-of-the-art methods while\nmaintaining a decent running speed.\n", "rewritten_text": "This paper introduces FADNet, a novel monocular 3D object detection network for autonomous driving.  Unlike existing keypoint-based approaches, FADNet employs a unified network that strategically groups object properties by estimation difficulty, processing them sequentially using a convolutional Gated Recurrent Unit.  Furthermore, a dedicated depth hint module generates row-wise features to improve depth estimation, employing bin-wise supervision.  Experiments on the KITTI benchmark demonstrate FADNet's competitive performance against state-of-the-art methods, achieving this without depth priors, post-processing, or refinement modules, while maintaining efficient runtime.\n"}, "2307.13958": {"original_text": "  Recently, vision transformer based multimodal learning methods have been\nproposed to improve the robustness of face anti-spoofing (FAS) systems.\nHowever, multimodal face data collected from the real world is often imperfect\ndue to missing modalities from various imaging sensors. Recently,\nflexible-modal FAS~\\cite{yu2023flexible} has attracted more attention, which\naims to develop a unified multimodal FAS model using complete multimodal face\ndata but is insensitive to test-time missing modalities. In this paper, we\ntackle one main challenge in flexible-modal FAS, i.e., when missing modality\noccurs either during training or testing in real-world situations. Inspired by\nthe recent success of the prompt learning in language models, we propose\n\\textbf{V}isual \\textbf{P}rompt flexible-modal \\textbf{FAS} (VP-FAS), which\nlearns the modal-relevant prompts to adapt the frozen pre-trained foundation\nmodel to downstream flexible-modal FAS task. Specifically, both vanilla visual\nprompts and residual contextual prompts are plugged into multimodal\ntransformers to handle general missing-modality cases, while only requiring\nless than 4\\% learnable parameters compared to training the entire model.\nFurthermore, missing-modality regularization is proposed to force models to\nlearn consistent multimodal feature embeddings when missing partial modalities.\nExtensive experiments conducted on two multimodal FAS benchmark datasets\ndemonstrate the effectiveness of our VP-FAS framework that improves the\nperformance under various missing-modality cases while alleviating the\nrequirement of heavy model re-training.\n", "rewritten_text": "To enhance the robustness of face anti-spoofing (FAS) systems, recent research uses vision transformer-based multimodal learning.  However, real-world multimodal face data is often incomplete.  Flexible-modal FAS methods, which aim for a unified model robust to missing modalities at test time, have gained prominence. This paper addresses the challenge of missing modalities during both training and testing.  Inspired by prompt learning, we introduce VP-FAS, a Visual Prompt flexible-modal FAS system. VP-FAS uses visual and residual contextual prompts to adapt a pre-trained model to the flexible-modal FAS task, requiring fewer than 4% learnable parameters.  A novel missing-modality regularization further ensures consistent feature embeddings.  Experiments on two benchmark datasets demonstrate VP-FAS's effectiveness in handling missing modalities with minimal retraining.\n"}, "1803.0634": {"original_text": "  We present a method for estimating detailed scene illumination using human\nfaces in a single image. In contrast to previous works that estimate lighting\nin terms of low-order basis functions or distant point lights, our technique\nestimates illumination at a higher precision in the form of a non-parametric\nenvironment map. Based on the observation that faces can exhibit strong\nhighlight reflections from a broad range of lighting directions, we propose a\ndeep neural network for extracting highlights from faces, and then trace these\nreflections back to the scene to acquire the environment map. Since real\ntraining data for highlight extraction is very limited, we introduce an\nunsupervised scheme for finetuning the network on real images, based on the\nconsistent diffuse chromaticity of a given face seen in multiple real images.\nIn tracing the estimated highlights to the environment, we reduce the blurring\neffect of skin reflectance on reflected light through a deconvolution\ndetermined by prior knowledge on face material properties. Comparisons to\nprevious techniques for highlight extraction and illumination estimation show\nthe state-of-the-art performance of this approach on a variety of indoor and\noutdoor scenes.\n", "rewritten_text": "This paper introduces a novel method for reconstructing detailed scene illumination from a single image using facial highlights.  Unlike previous methods relying on simplified lighting models (e.g., low-order basis functions or point lights), our approach generates a high-resolution, non-parametric environment map.  We leverage the rich highlight information present on faces, training a deep neural network to extract these highlights.  To overcome the scarcity of labeled training data, we employ an unsupervised fine-tuning strategy based on consistent diffuse chromaticity across multiple images of the same face.  Finally, we mitigate the blurring effect of skin reflectance during highlight back-projection using a deconvolution process informed by known facial material properties.  Our results demonstrate state-of-the-art performance on diverse indoor and outdoor scenes.\n"}, "1902.00595": {"original_text": "  Intuitively, human readers cope easily with errors in text; typos,\nmisspelling, word substitutions, etc. do not unduly disrupt natural reading.\nPrevious work indicates that letter transpositions result in increased reading\ntimes, but it is unclear if this effect generalizes to more natural errors. In\nthis paper, we report an eye-tracking study that compares two error types\n(letter transpositions and naturally occurring misspelling) and two error rates\n(10% or 50% of all words contain errors). We find that human readers show\nunimpaired comprehension in spite of these errors, but error words cause more\nreading difficulty than correct words. Also, transpositions are more difficult\nthan misspellings, and a high error rate increases difficulty for all words,\nincluding correct ones. We then present a computational model that uses\ncharacter-based (rather than traditional word-based) surprisal to account for\nthese results. The model explains that transpositions are harder than\nmisspellings because they contain unexpected letter combinations. It also\nexplains the error rate effect: upcoming words are more difficultto predict\nwhen the context is degraded, leading to increased surprisal.\n", "rewritten_text": "Humans readily understand text containing errors like typos and misspellings. While letter transpositions slow reading, their impact compared to more natural errors is unclear.  Our eye-tracking study compared letter transpositions and naturally occurring misspellings at 10% and 50% error rates.  Comprehension remained largely unaffected, but error words, especially transpositions, caused reading difficulties. Higher error rates increased difficulty for all words.  A novel character-based computational model, using surprisal, explains these findings: transpositions disrupt expected letter sequences, and high error rates reduce contextual predictability, increasing processing difficulty.\n"}, "cs/0406031": {"original_text": "  This paper describes a standalone, publicly-available implementation of the\nResolution of Anaphora Procedure (RAP) given by Lappin and Leass (1994). The\nRAP algorithm resolves third person pronouns, lexical anaphors, and identifies\npleonastic pronouns. Our implementation, JavaRAP, fills a current need in\nanaphora resolution research by providing a reference implementation that can\nbe benchmarked against current algorithms. The implementation uses the\nstandard, publicly available Charniak (2000) parser as input, and generates a\nlist of anaphora-antecedent pairs as output. Alternately, an in-place\nannotation or substitution of the anaphors with their antecedents can be\nproduced. Evaluation on the MUC-6 co-reference task shows that JavaRAP has an\naccuracy of 57.9%, similar to the performance given previously in the\nliterature (e.g., Preiss 2002).\n", "rewritten_text": "This paper presents JavaRAP, a publicly available, standalone implementation of Lappin and Leass's (1994) Resolution of Anaphora Procedure (RAP).  JavaRAP resolves third-person pronouns and lexical anaphors, and identifies pleonastic pronouns.  Using the Charniak (2000) parser as input, it outputs anaphora-antecedent pairs, or optionally, directly annotates/substitutes anaphors.  Its 57.9% accuracy on the MUC-6 coreference task is comparable to results reported in previous research (e.g., Preiss 2002), providing a valuable benchmark for current anaphora resolution algorithms.\n"}, "2010.13508": {"original_text": "  The SHApe Recovery from Partial textured 3D scans challenge, SHARP 2020, is\nthe first edition of a challenge fostering and benchmarking methods for\nrecovering complete textured 3D scans from raw incomplete data. SHARP 2020 is\norganised as a workshop in conjunction with ECCV 2020. There are two\ncomplementary challenges, the first one on 3D human scans, and the second one\non generic objects. Challenge 1 is further split into two tracks, focusing,\nfirst, on large body and clothing regions, and, second, on fine body details. A\nnovel evaluation metric is proposed to quantify jointly the shape\nreconstruction, the texture reconstruction and the amount of completed data.\nAdditionally, two unique datasets of 3D scans are proposed, to provide raw\nground-truth data for the benchmarks. The datasets are released to the\nscientific community. Moreover, an accompanying custom library of software\nroutines is also released to the scientific community. It allows for processing\n3D scans, generating partial data and performing the evaluation. Results of the\ncompetition, analysed in comparison to baselines, show the validity of the\nproposed evaluation metrics, and highlight the challenging aspects of the task\nand of the datasets. Details on the SHARP 2020 challenge can be found at\nhttps://cvi2.uni.lu/sharp2020/.\n", "rewritten_text": "The SHARP 2020 challenge, held as a workshop alongside ECCV 2020, was the inaugural competition benchmarking methods for completing textured 3D scans from incomplete data.  It featured two challenges: one focusing on human scans (divided into tracks for body/clothing and fine details), and another on generic objects.  A novel evaluation metric assessed shape, texture, and completion quality.  Two new datasets and a supporting software library for processing, data generation, and evaluation were released publicly.  Competition results, compared to baselines, validated the metric and highlighted the challenge's difficulty.  Further details are available at https://cvi2.uni.lu/sharp2020/.\n"}, "2305.08844": {"original_text": "  Despite their unprecedented success, even the largest language models make\nmistakes. Similar to how humans learn and improve using feedback, previous work\nproposed providing language models with natural language feedback to guide them\nin repairing their outputs. Because human-generated critiques are expensive to\nobtain, researchers have devised learned critique generators in lieu of human\ncritics while assuming one can train downstream models to utilize generated\nfeedback. However, this approach does not apply to black-box or limited access\nmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\nlarge general-purpose language agents, fine-tuning is neither computationally\nnor spatially efficient as it results in multiple copies of the network. In\nthis work, we introduce RL4F (Reinforcement Learning for Feedback), a\nmulti-agent collaborative framework where the critique generator is trained to\nmaximize end-task performance of GPT-3, a fixed model more than 200 times its\nsize. RL4F produces critiques that help GPT-3 revise its outputs. We study\nthree datasets for action planning, summarization and alphabetization and show\nrelative improvements up to 10% in multiple text similarity metrics over other\nlearned, retrieval-augmented or prompting-based critique generators.\n", "rewritten_text": "Even the most advanced language models, like GPT-3, are fallible.  While feedback-driven improvement is effective, human feedback is costly.  Existing methods using learned critique generators are unsuitable for black-box models like ChatGPT, which cannot be fine-tuned, and are inefficient for large language models due to the resource demands of fine-tuning.  This paper introduces RL4F, a reinforcement learning framework that trains a critique generator to improve the performance of a fixed, large language model (GPT-3) without fine-tuning.  Evaluated on three diverse tasks, RL4F demonstrates up to a 10% improvement in text similarity metrics compared to other critique generation methods.\n"}, "2312.14697": {"original_text": "  Polarization information of the light can provide rich cues for computer\nvision and scene understanding tasks, such as the type of material, pose, and\nshape of the objects. With the advent of new and cheap polarimetric sensors,\nthis imaging modality is becoming accessible to a wider public for solving\nproblems such as pose estimation, 3D reconstruction, underwater navigation, and\ndepth estimation. However, we observe several limitations regarding the usage\nof this sensorial modality, as well as a lack of standards and publicly\navailable tools to analyze polarization images. Furthermore, although\npolarization camera manufacturers usually provide acquisition tools to\ninterface with their cameras, they rarely include processing algorithms that\nmake use of the polarization information. In this paper, we review recent\nadvances in applications that involve polarization imaging, including a\ncomprehensive survey of recent advances on polarization for vision and robotics\nperception tasks. We also introduce a complete software toolkit that provides\ncommon standards to communicate with and process information from most of the\nexisting micro-grid polarization cameras on the market. The toolkit also\nimplements several image processing algorithms for this modality, and it is\npublicly available on GitHub: https://github.com/vibot-lab/Pola4all_JEI_2023.\n", "rewritten_text": "Polarization imaging offers valuable cues for computer vision, enabling improved material identification, pose estimation, and 3D reconstruction.  The decreasing cost of polarimetric sensors is broadening access to this technology for applications like underwater navigation and depth estimation.  However, current limitations include a lack of standardization, readily available analysis tools, and integrated processing algorithms from camera manufacturers.  This paper reviews recent advancements in polarization-based vision and robotics, and introduces Pola4all, a publicly available software toolkit (https://github.com/vibot-lab/Pola4all_JEI_2023) providing standardized communication and processing capabilities for common micro-grid polarization cameras.  The toolkit includes several implemented image processing algorithms.\n"}, "1809.04191": {"original_text": "  To realize the promise of ubiquitous embedded deep network inference, it is\nessential to seek limits of energy and area efficiency. To this end,\nlow-precision networks offer tremendous promise because both energy and area\nscale down quadratically with the reduction in precision. Here we demonstrate\nResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on\nthe ImageNet classification benchmark that, at 8-bit precision exceed the\naccuracy of the full-precision baseline networks after one epoch of finetuning,\nthereby leveraging the availability of pretrained models. We also demonstrate\nResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn 4-bit models that match\nthe accuracy of the full-precision baseline networks -- the highest scores to\ndate. Surprisingly, the weights of the low-precision networks are very close\n(in cosine similarity) to the weights of the corresponding baseline networks,\nmaking training from scratch unnecessary.\n  We find that gradient noise due to quantization during training increases\nwith reduced precision, and seek ways to overcome this noise. The number of\niterations required by SGD to achieve a given training error is related to the\nsquare of (a) the distance of the initial solution from the final plus (b) the\nmaximum variance of the gradient estimates. Therefore, we (a) reduce solution\ndistance by starting with pretrained fp32 precision baseline networks and\nfine-tuning, and (b) combat gradient noise introduced by quantization by\ntraining longer and reducing learning rates. Sensitivity analysis indicates\nthat these simple techniques, coupled with proper activation function range\ncalibration to take full advantage of the limited precision, are sufficient to\ndiscover low-precision networks, if they exist, close to fp32 precision\nbaseline networks. The results herein provide evidence that 4-bits suffice for\nclassification.\n", "rewritten_text": "To achieve truly ubiquitous embedded deep learning, energy and area efficiency are paramount.  Low-precision networks offer a significant advantage, as energy and area consumption scale quadratically with precision reduction.  We demonstrate that 8-bit versions of several popular network architectures (ResNet-18, -34, -50, -152, Inception-v3, DenseNet-161, and VGG-16bn) surpass full-precision accuracy on ImageNet after a single epoch of fine-tuning from pre-trained models.  Furthermore, our 4-bit models (ResNet-18, -34, -50, -152, DenseNet-161, and VGG-16bn) achieve accuracy matching full-precision baselines\u2014a new state-of-the-art.  Surprisingly, these low-precision networks exhibit high weight similarity to their full-precision counterparts, eliminating the need for training from scratch.  While quantization introduces increased gradient noise with lower precision, we mitigate this by leveraging pre-trained models for fine-tuning and employing longer training schedules with reduced learning rates.  Sensitivity analysis confirms that these methods, combined with careful activation function range calibration, effectively discover high-performing low-precision networks. Our findings strongly suggest that 4-bit precision is sufficient for image classification.\n"}, "2103.14799": {"original_text": "  Image representation is an important topic in computer vision and pattern\nrecognition. It plays a fundamental role in a range of applications towards\nunderstanding visual contents. Moment-based image representation has been\nreported to be effective in satisfying the core conditions of semantic\ndescription due to its beneficial mathematical properties, especially geometric\ninvariance and independence. This paper presents a comprehensive survey of the\northogonal moments for image representation, covering recent advances in\nfast/accurate calculation, robustness/invariance optimization, definition\nextension, and application. We also create a software package for a variety of\nwidely-used orthogonal moments and evaluate such methods in a same base. The\npresented theory analysis, software implementation, and evaluation results can\nsupport the community, particularly in developing novel techniques and\npromoting real-world applications.\n", "rewritten_text": "This paper provides a comprehensive survey of orthogonal moments for image representation in computer vision and pattern recognition.  We examine recent advancements in their fast and accurate computation, enhanced robustness and invariance, extended definitions, and diverse applications.  The efficacy of moment-based representation stems from its inherent geometric invariance and independence, making it well-suited for semantic image description.  Furthermore, we present a software package implementing various widely-used orthogonal moments, enabling comparative evaluation and facilitating the development of novel techniques and real-world applications.\n"}, "2406.19297": {"original_text": "  Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we\ndemonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)\napproach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.\n", "rewritten_text": "This paper investigates the impact of multimodal input on continual learning in Visual Question Answering (VQA).  We find that different modalities (e.g., visual and textual) learn at varying rates across sequential tasks, a phenomenon observed in both established and state-of-the-art vision-and-language (V&L) models.  To address this, we introduce Modality-Aware Feature Distillation (MAFED), a novel approach that surpasses existing methods across various model scales and continual learning scenarios.  Our ablation studies confirm that MAFED effectively complements experience replay.  These findings highlight the crucial role of modality-specific learning dynamics in mitigating catastrophic forgetting within multimodal continual learning.\n"}, "2410.15865": {"original_text": "  Grammatical features such as number and gender serve two central functions in\nhuman languages. While they encode salient semantic attributes like numerosity\nand animacy, they also offload sentence processing cost by predictably linking\nwords together via grammatical agreement. Grammars exhibit consistent\norganizational patterns across diverse languages, invariably rooted in a\nsemantic foundation, a widely confirmed but still theoretically unexplained\nphenomenon. To explain the basis of universal grammatical patterns, we unify\ntwo fundamental properties of grammar, semantic encoding and agreement-based\npredictability, into a single information-theoretic objective under cognitive\nconstraints. Our analyses reveal that grammatical organization provably\ninherits from perceptual attributes, but that grammars empirically prioritize\nfunctional goals, promoting efficient language processing over semantic\nencoding.\n", "rewritten_text": "Number and gender in grammar encode meaning (like quantity and animacy) and streamline sentence processing through agreement.  While grammars across languages share consistent, semantically-rooted patterns\u2014a well-established but unexplained phenomenon\u2014we propose an information-theoretic model. This model unifies semantic encoding and agreement-based predictability under cognitive constraints, showing that while grammar originates in perceptual features, it prioritizes efficient processing over purely semantic representation.\n"}, "2306.05119": {"original_text": "  Factuality is important to dialogue summarization. Factual error correction\n(FEC) of model-generated summaries is one way to improve factuality. Current\nFEC evaluation that relies on factuality metrics is not reliable and detailed\nenough. To address this problem, we are the first to manually annotate a FEC\ndataset for dialogue summarization containing 4000 items and propose FERRANTI,\na fine-grained evaluation framework based on reference correction that\nautomatically evaluates the performance of FEC models on different error\ncategories. Using this evaluation framework, we conduct sufficient experiments\nwith FEC approaches under a variety of settings and find the best training\nmodes and significant differences in the performance of the existing approaches\non different factual error categories.\n", "rewritten_text": "Dialogue summarization requires high factuality.  While factual error correction (FEC) aims to improve this, current evaluation methods based solely on factuality metrics are insufficient.  To remedy this, we introduce FERRANTI, a novel, fine-grained evaluation framework.  FERRANTI uses a manually annotated dataset of 4000 dialogue summaries (the first of its kind) and leverages reference corrections to automatically assess FEC model performance across various error types.  Our extensive experiments using FERRANTI reveal optimal training strategies and highlight significant performance variations among existing FEC approaches across different factual error categories.\n"}, "2410.16190": {"original_text": "  This work explores how human judgement about salient regions of an image can\nbe introduced into deep convolutional neural network (DCNN) training.\nTraditionally, training of DCNNs is purely data-driven. This often results in\nlearning features of the data that are only coincidentally correlated with\nclass labels. Human saliency can guide network training using our proposed new\ncomponent of the loss function that ConveYs Brain Oversight to Raise\nGeneralization (CYBORG) and penalizes the model for using non-salient regions.\nThis mechanism produces DCNNs achieving higher accuracy and generalization\ncompared to using the same training data without human salience. Experimental\nresults demonstrate that CYBORG applies across multiple network architectures\nand problem domains (detection of synthetic faces, iris presentation attacks\nand anomalies in chest X-rays), while requiring significantly less data than\ntraining without human saliency guidance. Visualizations show that\nCYBORG-trained models' saliency is more consistent across independent training\nruns than traditionally-trained models, and also in better agreement with\nhumans. To lower the cost of collecting human annotations, we also explore\nusing deep learning to provide automated annotations. CYBORG training of CNNs\naddresses important issues such as reducing the appetite for large training\nsets, increasing interpretability, and reducing fragility by generalizing\nbetter to new types of data.\n", "rewritten_text": "This research integrates human perception of image importance into deep convolutional neural network (DCNN) training.  Unlike traditional data-driven DCNN training, which can lead to learning spurious correlations, our proposed CYBORG (ConveYs Brain Oversight to Raise Generalization) loss function penalizes reliance on non-salient image regions.  This results in DCNNs with improved accuracy and generalization across diverse tasks (synthetic face detection, iris presentation attack detection, and chest X-ray anomaly detection), requiring significantly less training data.  Experiments show CYBORG produces models with more consistent and human-aligned saliency maps across training runs.  Furthermore, to reduce annotation costs, we investigate automated annotation methods using deep learning.  Overall, CYBORG addresses key challenges in DCNN training: reducing data requirements, enhancing interpretability, and improving robustness through better generalization.\n"}, "2311.07611": {"original_text": "  In this study we intentionally introduce biases into large language model\nresponses in an attempt to create specific personas for interactive media\npurposes. We explore the differences between open source models such as\nFalcon-7b and the GPT-4 model from Open AI, and we quantify some differences in\nresponses afforded by the two systems. We find that the guardrails in the GPT-4\nmixture of experts models with a supervisor, while useful in assuring AI\nalignment in general, are detrimental in trying to construct personas with a\nvariety of uncommon viewpoints. This study aims to set the groundwork for\nfuture exploration in intentional biases of large language models such that\nthese practices can be applied in the creative field, and new forms of media.\n", "rewritten_text": "This study investigates the impact of intentionally introducing biases into large language models (LLMs) to create specific personas for interactive media.  We compare the open-source Falcon-7b and the proprietary GPT-4, quantifying differences in their responses to biased prompts.  Our findings suggest that GPT-4's safety mechanisms, while beneficial for general AI alignment, hinder the creation of personas with diverse and unconventional viewpoints.  This research lays the foundation for future work on leveraging biased LLMs for creative applications and novel media forms.\n"}, "2409.01522": {"original_text": "  Long-term motion generation is a challenging task that requires producing\ncoherent and realistic sequences over extended durations. Current methods\nprimarily rely on framewise motion representations, which capture only static\nspatial details and overlook temporal dynamics. This approach leads to\nsignificant redundancy across the temporal dimension, complicating the\ngeneration of effective long-term motion. To overcome these limitations, we\nintroduce the novel concept of Lagrangian Motion Fields, specifically designed\nfor long-term motion generation. By treating each joint as a Lagrangian\nparticle with uniform velocity over short intervals, our approach condenses\nmotion representations into a series of \"supermotions\" (analogous to\nsuperpixels). This method seamlessly integrates static spatial information with\ninterpretable temporal dynamics, transcending the limitations of existing\nnetwork architectures and motion sequence content types. Our solution is\nversatile and lightweight, eliminating the need for neural network\npreprocessing. Our approach excels in tasks such as long-term music-to-dance\ngeneration and text-to-motion generation, offering enhanced efficiency,\nsuperior generation quality, and greater diversity compared to existing\nmethods. Additionally, the adaptability of Lagrangian Motion Fields extends to\napplications like infinite motion looping and fine-grained controlled motion\ngeneration, highlighting its broad utility. Video demonstrations are available\nat \\url{https://plyfager.github.io/LaMoG}.\n", "rewritten_text": "Generating realistic, coherent long-term motion is difficult because current methods rely on frame-by-frame representations, neglecting temporal dynamics and leading to redundancy.  We introduce Lagrangian Motion Fields, a novel approach that represents motion as a series of \"supermotions\" \u2013 essentially, treating each joint as a particle with constant velocity over short intervals. This efficiently integrates spatial and temporal information, surpassing existing methods in both quality and efficiency.  Our lightweight, preprocessing-free approach excels in tasks like music-to-dance and text-to-motion generation, enabling infinite looping and fine-grained control.  See our video demonstrations at [https://plyfager.github.io/LaMoG](https://plyfager.github.io/LaMoG).\n"}, "1704.06756": {"original_text": "  We have developed convolutional neural networks (CNN) for a facial expression\nrecognition task. The goal is to classify each facial image into one of the\nseven facial emotion categories considered in this study. We trained CNN models\nwith different depth using gray-scale images. We developed our models in Torch\nand exploited Graphics Processing Unit (GPU) computation in order to expedite\nthe training process. In addition to the networks performing based on raw pixel\ndata, we employed a hybrid feature strategy by which we trained a novel CNN\nmodel with the combination of raw pixel data and Histogram of Oriented\nGradients (HOG) features. To reduce the overfitting of the models, we utilized\ndifferent techniques including dropout and batch normalization in addition to\nL2 regularization. We applied cross validation to determine the optimal\nhyper-parameters and evaluated the performance of the developed models by\nlooking at their training histories. We also present the visualization of\ndifferent layers of a network to show what features of a face can be learned by\nCNN models.\n", "rewritten_text": "This study uses convolutional neural networks (CNNs) of varying depths, trained on grayscale facial images, to classify seven basic emotions.  Leveraging GPU acceleration within the Torch framework, we trained models on both raw pixel data and a hybrid approach incorporating Histogram of Oriented Gradients (HOG) features.  Techniques such as dropout, batch normalization, and L2 regularization mitigated overfitting.  Hyperparameter optimization was achieved through cross-validation, and model performance was assessed via training history analysis.  Finally, we visualize network layers to illustrate the learned facial features.\n"}, "2312.17292": {"original_text": "  Word embedding methods (WEMs) are extensively used for representing text\ndata. The dimensionality of these embeddings varies across various tasks and\nimplementations. The effect of dimensionality change on the accuracy of the\ndownstream task is a well-explored question. However, how the dimensionality\nchange affects the bias of word embeddings needs to be investigated. Using the\nEnglish Wikipedia corpus, we study this effect for two static (Word2Vec and\nfastText) and two context-sensitive (ElMo and BERT) WEMs. We have two\nobservations. First, there is a significant variation in the bias of word\nembeddings with the dimensionality change. Second, there is no uniformity in\nhow the dimensionality change affects the bias of word embeddings. These\nfactors should be considered while selecting the dimensionality of word\nembeddings.\n", "rewritten_text": "This study investigates how dimensionality affects the bias of word embeddings (WEMs), a crucial aspect often overlooked despite extensive research on their accuracy.  Using the English Wikipedia corpus, we analyzed four WEMs\u2014Word2Vec, fastText, ElMo, and BERT\u2014finding significant and inconsistent bias variations across different dimensionalities.  Our findings highlight the importance of carefully considering dimensionality when selecting WEMs for downstream tasks.\n"}, "2401.07745": {"original_text": "  Open-vocabulary 3D instance segmentation is cutting-edge for its ability to\nsegment 3D instances without predefined categories. However, progress in 3D\nlags behind its 2D counterpart due to limited annotated 3D data. To address\nthis, recent works first generate 2D open-vocabulary masks through 2D models\nand then merge them into 3D instances based on metrics calculated between two\nneighboring frames. In contrast to these local metrics, we propose a novel\nmetric, view consensus rate, to enhance the utilization of multi-view\nobservations. The key insight is that two 2D masks should be deemed part of the\nsame 3D instance if a significant number of other 2D masks from different views\ncontain both these two masks. Using this metric as edge weight, we construct a\nglobal mask graph where each mask is a node. Through iterative clustering of\nmasks showing high view consensus, we generate a series of clusters, each\nrepresenting a distinct 3D instance. Notably, our model is training-free.\nThrough extensive experiments on publicly available datasets, including\nScanNet++, ScanNet200 and MatterPort3D, we demonstrate that our method achieves\nstate-of-the-art performance in open-vocabulary 3D instance segmentation. Our\nproject page is at https://pku-epic.github.io/MaskClustering.\n", "rewritten_text": "Open-vocabulary 3D instance segmentation, while promising, suffers from a lack of annotated 3D data.  Current methods overcome this by leveraging 2D open-vocabulary segmentation and merging 2D masks across frames using local metrics.  We introduce a novel, training-free approach that uses a \"view consensus rate\" metric to improve 3D instance segmentation. This metric globally assesses the consistency of 2D mask pairings across multiple viewpoints, creating a graph where nodes are masks and edges represent consensus.  Iterative clustering of this graph yields 3D instances.  Experiments on ScanNet++, ScanNet200, and MatterPort3D demonstrate state-of-the-art performance.  See our project page at https://pku-epic.github.io/MaskClustering for details.\n"}, "2404.05916": {"original_text": "  Echocardiography segmentation for cardiac analysis is time-consuming and\nresource-intensive due to the variability in image quality and the necessity to\nprocess scans from various standard views. While current automated segmentation\nmethods in echocardiography show promising performance, they are trained on\nspecific scan views to analyze corresponding data. However, this solution has a\nlimitation as the number of required models increases with the number of\nstandard views. To address this, in this paper, we present a prompt-driven\nuniversal method for view-agnostic echocardiography analysis. Considering the\ndomain shift between standard views, we first introduce a method called prompt\nmatching, aimed at learning prompts specific to different views by matching\nprompts and querying input embeddings using a pre-trained vision model. Then,\nwe utilized a pre-trained medical language model to align textual information\nwith pixel data for accurate segmentation. Extensive experiments on three\nstandard views showed that our approach significantly outperforms the\nstate-of-the-art universal methods and achieves comparable or even better\nperformances over the segmentation model trained and tested on same views.\n", "rewritten_text": "Echocardiography segmentation is labor-intensive due to image variability and the need to process multiple standard views.  Existing automated methods, while promising, require separate models for each view, leading to scalability issues.  This paper introduces a novel prompt-driven, view-agnostic approach.  By employing prompt matching to learn view-specific prompts and aligning textual and visual information using pre-trained models, our method achieves superior performance compared to existing universal methods and rivals or surpasses single-view trained models, as demonstrated through extensive experiments across three standard views.\n"}, "1908.09884": {"original_text": "  We consider the problem of discovering novel object categories in an image\ncollection. While these images are unlabelled, we also assume prior knowledge\nof related but different image classes. We use such prior knowledge to reduce\nthe ambiguity of clustering, and improve the quality of the newly discovered\nclasses. Our contributions are twofold. The first contribution is to extend\nDeep Embedded Clustering to a transfer learning setting; we also improve the\nalgorithm by introducing a representation bottleneck, temporal ensembling, and\nconsistency. The second contribution is a method to estimate the number of\nclasses in the unlabelled data. This also transfers knowledge from the known\nclasses, using them as probes to diagnose different choices for the number of\nclasses in the unlabelled subset. We thoroughly evaluate our method,\nsubstantially outperforming state-of-the-art techniques in a large number of\nbenchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN.\n", "rewritten_text": "This paper addresses the unsupervised discovery of novel object categories within an image collection, leveraging prior knowledge of related, labeled classes.  We enhance Deep Embedded Clustering through transfer learning, incorporating a representation bottleneck, temporal ensembling, and consistency regularization.  Furthermore, we introduce a novel method for estimating the number of unknown classes, using the labeled classes as probes.  Our approach significantly outperforms existing methods on extensive benchmarks including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN.\n"}, "2212.08334": {"original_text": "  Scene understanding has made tremendous progress over the past few years, as\ndata acquisition systems are now providing an increasing amount of data of\nvarious modalities (point cloud, depth, RGB...). However, this improvement\ncomes at a large cost on computation resources and data annotation\nrequirements. To analyze geometric information and images jointly, many\napproaches rely on both a 2D loss and 3D loss, requiring not only 2D per\npixel-labels but also 3D per-point labels. However, obtaining a 3D groundtruth\nis challenging, time-consuming and error-prone. In this paper, we show that\nimage segmentation can benefit from 3D geometric information without requiring\na 3D groundtruth, by training the geometric feature extraction and the 2D\nsegmentation network jointly, in an end-to-end fashion, using only the 2D\nsegmentation loss. Our method starts by extracting a map of 3D features\ndirectly from a provided point cloud by using a lightweight 3D neural network.\nThe 3D feature map, merged with the RGB image, is then used as an input to a\nclassical image segmentation network. Our method can be applied to many 2D\nsegmentation networks, improving significantly their performance with only a\nmarginal network weight increase and light input dataset requirements, since no\n3D groundtruth is required.\n", "rewritten_text": "Recent advancements in scene understanding, fueled by increasingly rich multi-modal data (RGB, depth, point clouds), have been hampered by high computational costs and demanding annotation requirements, particularly the need for 3D ground truth data.  This paper presents a novel approach that leverages 3D geometric information to improve 2D image segmentation *without* requiring 3D annotations.  We achieve this by jointly training a lightweight 3D feature extraction network (operating on point clouds) and a standard 2D segmentation network in an end-to-end fashion, using only a 2D segmentation loss.  The 3D features are extracted from the point cloud and fused with the RGB image before being fed into the 2D segmentation network.  This method significantly enhances 2D segmentation performance across various architectures with minimal added computational overhead and data annotation burden.\n"}, "2309.09379": {"original_text": "  Nowadays, photogrammetrically derived point clouds are widely used in many\ncivilian applications due to their low cost and flexibility in acquisition.\nTypically, photogrammetric point clouds are assessed through reference data\nsuch as LiDAR point clouds. However, when reference data are not available, the\nassessment of photogrammetric point clouds may be challenging. Since these\npoint clouds are algorithmically derived, their accuracies and precisions are\nhighly varying with the camera networks, scene complexity, and dense image\nmatching (DIM) algorithms, and there is no standard error metric to determine\nper-point errors. The theory of internal reliability of camera networks has\nbeen well studied through first-order error estimation of Bundle Adjustment\n(BA), which is used to understand the errors of 3D points assuming known\nmeasurement errors. However, the measurement errors of the DIM algorithms are\nintricate to an extent that every single point may have its error function\ndetermined by factors such as pixel intensity, texture entropy, and surface\nsmoothness. Despite the complexity, there exist a few common metrics that may\naid the process of estimating the posterior reliability of the derived points,\nespecially in a multi-view stereo (MVS) setup when redundancies are present. In\nthis paper, by using an aerial oblique photogrammetric block with LiDAR\nreference data, we analyze several internal matching metrics within a common\nMVS framework, including statistics in ray convergence, intersection angles,\nDIM energy, etc.\n", "rewritten_text": "Photogrammetry's low cost and ease of data acquisition have made point clouds widely used in civilian applications.  While typically assessed against LiDAR data, evaluating photogrammetric point clouds without reference data is challenging.  Accuracy and precision vary significantly depending on camera network, scene complexity, and dense image matching (DIM) algorithms, lacking a standardized per-point error metric.  While Bundle Adjustment (BA) provides first-order error estimation,  DIM algorithm errors are complex, varying per point based on factors like pixel intensity, texture, and surface smoothness.  However, several metrics can estimate point reliability, particularly in multi-view stereo (MVS) with its inherent redundancy. This paper analyzes various internal matching metrics (ray convergence statistics, intersection angles, DIM energy) within a common MVS framework using an aerial oblique photogrammetric block with LiDAR ground truth.\n"}, "2306.05061": {"original_text": "  Multi-task visual perception has a wide range of applications in scene\nunderstanding such as autonomous driving. In this work, we devise an efficient\nunified framework to solve multiple common perception tasks, including instance\nsegmentation, semantic segmentation, monocular 3D detection, and depth\nestimation. Simply sharing the same visual feature representations for these\ntasks impairs the performance of tasks, while independent task-specific feature\nextractors lead to parameter redundancy and latency. Thus, we design two\nfeature-merge branches to learn feature basis, which can be useful to, and thus\nshared by, multiple perception tasks. Then, each task takes the corresponding\nfeature basis as the input of the prediction task head to fulfill a specific\ntask. In particular, one feature merge branch is designed for instance-level\nrecognition the other for dense predictions. To enhance inter-branch\ncommunication, the instance branch passes pixel-wise spatial information of\neach instance to the dense branch using efficient dynamic convolution\nweighting. Moreover, a simple but effective dynamic routing mechanism is\nproposed to isolate task-specific features and leverage common properties among\ntasks. Our proposed framework, termed D2BNet, demonstrates a unique approach to\nparameter-efficient predictions for multi-task perception. In addition, as\ntasks benefit from co-training with each other, our solution achieves on par\nresults on partially labeled settings on nuScenes and outperforms previous\nworks for 3D detection and depth estimation on the Cityscapes dataset with full\nsupervision.\n", "rewritten_text": "This paper introduces D2BNet, an efficient unified framework for multi-task visual perception, addressing challenges in autonomous driving and scene understanding.  Unlike approaches that suffer from performance degradation through shared features or redundancy from independent feature extractors, D2BNet employs two feature-merge branches\u2014one for instance-level, the other for dense predictions\u2014to learn shared feature bases for tasks like instance and semantic segmentation, monocular 3D detection, and depth estimation.  Inter-branch communication is enhanced via dynamic convolution weighting, and a dynamic routing mechanism isolates task-specific features while leveraging shared properties.  D2BNet achieves comparable performance to state-of-the-art methods on partially labeled nuScenes data and surpasses them on fully supervised Cityscapes for 3D detection and depth estimation, demonstrating its parameter efficiency and the benefits of co-training.\n"}, "2107.00977": {"original_text": "  Cardiac ultrasound imaging is used to diagnose various heart diseases. Common\nanalysis pipelines involve manual processing of the video frames by expert\nclinicians. This suffers from intra- and inter-observer variability. We propose\na novel approach to ultrasound video analysis using a transformer architecture\nbased on a Residual Auto-Encoder Network and a BERT model adapted for token\nclassification. This enables videos of any length to be processed. We apply our\nmodel to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection\nand the automated computation of the left ventricular ejection fraction. We\nachieve an average frame distance of 3.36 frames for the ES and 7.17 frames for\nthe ED on videos of arbitrary length. Our end-to-end learnable approach can\nestimate the ejection fraction with a MAE of 5.95 and $R^2$ of 0.52 in 0.15s\nper video, showing that segmentation is not the only way to predict ejection\nfraction. Code and models are available at https://github.com/HReynaud/UVT.\n", "rewritten_text": "We present a novel transformer-based approach for automated analysis of cardiac ultrasound videos, addressing the limitations of manual analysis.  Our method, combining a residual autoencoder and a BERT-adapted token classifier, processes videos of any length to detect end-systolic and end-diastolic frames and compute left ventricular ejection fraction (LVEF).  We achieve a mean average error (MAE) of 5.95 and an R\u00b2 of 0.52 in LVEF estimation, with average frame detection errors of 3.36 and 7.17 frames for end-systole and end-diastole, respectively, all within 0.15 seconds per video.  This demonstrates a viable alternative to segmentation-based LVEF prediction.  Code and models are available at https://github.com/HReynaud/UVT.\n"}, "1710.05994": {"original_text": "  Recent advancements in neutron and X-ray sources, instrumentation and data\ncollection modes have significantly increased the experimental data size (which\ncould easily contain 10$^{8}$ -- 10$^{10}$ data points), so that conventional\nvolumetric visualization approaches become inefficient for both still imaging\nand interactive OpenGL rendition in a 3D setting. We introduce a new approach\nbased on the unsupervised machine learning algorithm, Density-Based Spatial\nClustering of Applications with Noise (DBSCAN), to efficiently analyze and\nvisualize large volumetric datasets. Here we present two examples of analyzing\nand visualizing datasets from the diffuse scattering experiment of a single\ncrystal sample and the tomographic reconstruction of a neutron scanning of a\nturbine blade. We found that by using the intensity as the weighting factor in\nthe clustering process, DBSCAN becomes very effective in de-noising and\nfeature/boundary detection, and thus enables better visualization of the\nhierarchical internal structures of the neutron scattering data.\n", "rewritten_text": "The explosion of data from advanced neutron and X-ray sources (datasets reaching 10<sup>8</sup>-10<sup>10</sup> data points) overwhelms traditional volumetric visualization methods.  We propose a novel approach using the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) machine learning algorithm for efficient analysis and visualization of these large datasets.  We demonstrate this approach on two examples: diffuse scattering from a single crystal and tomographic reconstruction of a neutron-scanned turbine blade.  Using intensity as a weighting factor, DBSCAN effectively removes noise, highlights features and boundaries, and reveals the hierarchical internal structures within the neutron scattering data, leading to improved visualization.\n"}, "1812.0081": {"original_text": "  It is well known that the generative adversarial nets (GANs) are remarkably\ndifficult to train. The recently proposed Wasserstein GAN (WGAN) creates\nprincipled research directions towards addressing these issues. But we found in\npractice that gradient penalty WGANs (GP-WGANs) still suffer from training\ninstability. In this paper, we combine a Total Variational (TV) regularizing\nterm into the WGAN formulation instead of weight clipping or gradient penalty,\nwhich implies that the Lipschitz constraint is enforced on the critic network.\nOur proposed method is more stable at training than GP-WGANs and works well\nacross varied GAN architectures. We also present a method to control the\ntrade-off between image diversity and visual quality. It does not bring any\ncomputation burden.\n", "rewritten_text": "Training Generative Adversarial Networks (GANs) is notoriously challenging. While Wasserstein GANs (WGANs) offered a promising solution,  gradient penalty WGANs (GP-WGANs) remain prone to instability.  This paper introduces a novel approach that incorporates a Total Variational (TV) regularization term into the WGAN framework, replacing weight clipping and gradient penalties to enforce the Lipschitz constraint on the critic network.  Our method demonstrates improved training stability compared to GP-WGANs, performs well across diverse GAN architectures, and offers a computationally inexpensive mechanism to balance image diversity and visual quality.\n"}, "1709.06247": {"original_text": "  Most of convolutional neural networks share the same characteristic: each\nconvolutional layer is followed by a nonlinear activation layer where Rectified\nLinear Unit (ReLU) is the most widely used. In this paper, we argue that the\ndesigned structure with the equal ratio between these two layers may not be the\nbest choice since it could result in the poor generalization ability. Thus, we\ntry to investigate a more suitable method on using ReLU to explore the better\nnetwork architectures. Specifically, we propose a proportional module to keep\nthe ratio between convolution and ReLU amount to be N:M (N>M). The proportional\nmodule can be applied in almost all networks with no extra computational cost\nto improve the performance. Comprehensive experimental results indicate that\nthe proposed method achieves better performance on different benchmarks with\ndifferent network architectures, thus verify the superiority of our work.\n", "rewritten_text": "Many convolutional neural networks (CNNs) use a ReLU activation layer after each convolutional layer.  However, this 1:1 ratio may hinder generalization.  This paper proposes a proportional module that adjusts the ratio of convolutional to ReLU layers to N:M (N>M), improving performance without significant computational overhead.  Experiments across various CNN architectures and benchmarks demonstrate the effectiveness of this approach.\n"}, "2201.06674": {"original_text": "  Providing feedback on the argumentation of the learner is essential for\ndeveloping critical thinking skills, however, it requires a lot of time and\neffort. To mitigate the overload on teachers, we aim to automate a process of\nproviding feedback, especially giving diagnostic comments which point out the\nweaknesses inherent in the argumentation. It is recommended to give specific\ndiagnostic comments so that learners can recognize the diagnosis without\nmisinterpretation. However, it is not obvious how the task of providing\nspecific diagnostic comments should be formulated. We present a formulation of\nthe task as template selection and slot filling to make an automatic evaluation\neasier and the behavior of the model more tractable. The key to the formulation\nis the possibility of creating a template set that is sufficient for practical\nuse. In this paper, we define three criteria that a template set should\nsatisfy: expressiveness, informativeness, and uniqueness, and verify the\nfeasibility of creating a template set that satisfies these criteria as a first\ntrial. We will show that it is feasible through an annotation study that\nconverts diagnostic comments given in a text to a template format. The corpus\nused in the annotation study is publicly available.\n", "rewritten_text": "Automating feedback on student arguments is crucial for fostering critical thinking, but is time-consuming for educators.  This paper proposes automating diagnostic feedback by framing the task as template selection and slot filling.  This approach simplifies automated evaluation and model behavior, contingent on creating a practically sufficient template set.  We define three criteria for this set \u2013 expressiveness, informativeness, and uniqueness \u2013 and demonstrate their feasibility through a pilot annotation study using a publicly available corpus.  This study converts existing textual diagnostic comments into our template format.\n"}, "2008.01167": {"original_text": "  Current anchor-free object detectors label all the features that spatially\nfall inside a predefined central region of a ground-truth box as positive. This\napproach causes label noise during training, since some of these positively\nlabeled features may be on the background or an occluder object, or they are\nsimply not discriminative features. In this paper, we propose a new labeling\nstrategy aimed to reduce the label noise in anchor-free detectors. We sum-pool\npredictions stemming from individual features into a single prediction. This\nallows the model to reduce the contributions of non-discriminatory features\nduring training. We develop a new one-stage, anchor-free object detector,\nPPDet, to employ this labeling strategy during training and a similar\nprediction pooling method during inference. On the COCO dataset, PPDet achieves\nthe best performance among anchor-free top-down detectors and performs on-par\nwith the other state-of-the-art methods. It also outperforms all major\none-stage and two-stage methods in small object detection (${AP}_{S}$ $31.4$).\nCode is available at https://github.com/nerminsamet/ppdet\n", "rewritten_text": "Existing anchor-free object detectors suffer from label noise due to their inclusive positive labeling strategy.  This paper introduces PPDet, a novel one-stage, anchor-free detector that mitigates this issue.  PPDet employs a new labeling strategy and a prediction pooling mechanism to suppress the influence of non-discriminative features.  Evaluated on the COCO dataset, PPDet achieves state-of-the-art performance among anchor-free top-down detectors, rivals leading methods overall, and significantly outperforms them in small object detection (AP<sub>S</sub> 31.4).  Code is available at https://github.com/nerminsamet/ppdet.\n"}, "1807.10657": {"original_text": "  Saliency map estimation in computer vision aims to estimate the locations\nwhere people gaze in images. Since people tend to look at objects in images,\nthe parameters of the model pretrained on ImageNet for image classification are\nuseful for the saliency map estimation. However, there is no research on the\nrelationship between the image classification accuracy and the performance of\nthe saliency map estimation. In this paper, it is shown that there is a strong\ncorrelation between image classification accuracy and saliency map estimation\naccuracy. We also investigated the effective architecture based on multi scale\nimages and the upsampling layers to refine the saliency-map resolution. Our\nmodel achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003\ndatasets. In the MIT Saliency Benchmark, our model achieved the best\nperformance in some metrics and competitive results in the other metrics.\n", "rewritten_text": "This paper investigates the relationship between image classification accuracy and saliency map estimation performance.  Leveraging the observation that human gaze often focuses on objects, we utilize ImageNet pre-trained models.  Contrary to prior research, we demonstrate a strong correlation between image classification accuracy and the accuracy of saliency map estimation.  Furthermore, we propose a novel architecture incorporating multi-scale images and upsampling layers to improve saliency map resolution.  Our model achieves state-of-the-art results on PASCAL-S, OSIE, and MIT1003 datasets, and achieves top performance or highly competitive results on the MIT Saliency Benchmark.\n"}, "1812.05642": {"original_text": "  Unsupervised learning for geometric perception (depth, optical flow, etc.) is\nof great interest to autonomous systems. Recent works on unsupervised learning\nhave made considerable progress on perceiving geometry; however, they usually\nignore the coherence of objects and perform poorly under scenarios with dark\nand noisy environments. In contrast, supervised learning algorithms, which are\nrobust, require large labeled geometric dataset. This paper introduces SIGNet,\na novel framework that provides robust geometry perception without requiring\ngeometrically informative labels. Specifically, SIGNet integrates semantic\ninformation to make depth and flow predictions consistent with objects and\nrobust to low lighting conditions. SIGNet is shown to improve upon the\nstate-of-the-art unsupervised learning for depth prediction by 30% (in squared\nrelative error). In particular, SIGNet improves the dynamic object class\nperformance by 39% in depth prediction and 29% in flow prediction. Our code\nwill be made available at https://github.com/mengyuest/SIGNet\n", "rewritten_text": "Autonomous systems greatly benefit from unsupervised learning for geometric perception (depth, optical flow). While recent unsupervised methods have advanced this field, they often lack object coherence and struggle in dark or noisy conditions.  Supervised methods, though robust, demand extensive labeled data.  This paper presents SIGNet, a novel framework for robust geometric perception without requiring labeled geometric data.  By integrating semantic information, SIGNet produces depth and flow predictions consistent with object boundaries and resilient to low light.  SIGNet surpasses state-of-the-art unsupervised depth prediction by 30% (in squared relative error), achieving even greater improvements (39% for depth and 29% for flow) in dynamic object performance.  Our code is available at https://github.com/mengyuest/SIGNet.\n"}, "2212.09098": {"original_text": "  Fine-grained semantic segmentation of a person's face and head, including\nfacial parts and head components, has progressed a great deal in recent years.\nHowever, it remains a challenging task, whereby considering ambiguous\nocclusions and large pose variations are particularly difficult. To overcome\nthese difficulties, we propose a novel framework termed Mask-FPAN. It uses a\nde-occlusion module that learns to parse occluded faces in a semi-supervised\nway. In particular, face landmark localization, face occlusionstimations, and\ndetected head poses are taken into account. A 3D morphable face model combined\nwith the UV GAN improves the robustness of 2D face parsing. In addition, we\nintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for face\nparing work. The proposed Mask-FPAN framework addresses the face parsing\nproblem in the wild and shows significant performance improvements with MIOU\nfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging face\ndatasets.\n", "rewritten_text": "Recent advancements have significantly improved fine-grained semantic segmentation of faces and heads, but challenges remain, especially handling occlusions and varying poses.  To address this, we introduce Mask-FPAN, a novel framework incorporating a semi-supervised de-occlusion module that leverages face landmarks, occlusion estimations, and head pose.  A 3D morphable face model integrated with a UV GAN enhances 2D face parsing robustness.  Furthermore, we contribute two new high-quality datasets, FaceOccMask-HQ and CelebAMaskOcc-HQ.  Mask-FPAN demonstrates substantial performance gains, achieving a Mean Intersection over Union (mIoU) of 0.9013, a significant improvement over the state-of-the-art (0.7353) on challenging datasets.\n"}, "2206.08347": {"original_text": "  By leveraging contrastive learning, clustering, and other pretext tasks,\nunsupervised methods for learning image representations have reached impressive\nresults on standard benchmarks. The result has been a crowded field - many\nmethods with substantially different implementations yield results that seem\nnearly identical on popular benchmarks, such as linear evaluation on ImageNet.\nHowever, a single result does not tell the whole story. In this paper, we\ncompare methods using performance-based benchmarks such as linear evaluation,\nnearest neighbor classification, and clustering for several different datasets,\ndemonstrating the lack of a clear front-runner within the current\nstate-of-the-art. In contrast to prior work that performs only supervised vs.\nunsupervised comparison, we compare several different unsupervised methods\nagainst each other. To enrich this comparison, we analyze embeddings with\nmeasurements such as uniformity, tolerance, and centered kernel alignment\n(CKA), and propose two new metrics of our own: nearest neighbor graph\nsimilarity and linear prediction overlap. We reveal through our analysis that\nin isolation, single popular methods should not be treated as though they\nrepresent the field as a whole, and that future work ought to consider how to\nleverage the complimentary nature of these methods. We also leverage CKA to\nprovide a framework to robustly quantify augmentation invariance, and provide a\nreminder that certain types of invariance will be undesirable for downstream\ntasks.\n", "rewritten_text": "Unsupervised image representation learning, boosted by contrastive learning and other pretext tasks, has yielded remarkably similar results across numerous methods on standard benchmarks like ImageNet linear evaluation.  This paper challenges the notion of a single dominant approach.  By comprehensively comparing leading unsupervised methods using diverse evaluation metrics (linear evaluation, nearest neighbor classification, clustering, and novel metrics like nearest neighbor graph similarity and linear prediction overlap), we demonstrate a lack of clear superiority among them.  Furthermore, we analyze embedding properties (uniformity, tolerance, CKA) to reveal the limitations of relying on single benchmark scores.  Our findings highlight the complementary strengths of different methods and advocate for future research exploring their combined potential.  Finally, we utilize CKA to rigorously assess augmentation invariance, emphasizing that certain invariances may hinder downstream task performance.\n"}, "2108.08432": {"original_text": "  Deep learning has achieved remarkable success in medicalimage segmentation,\nbut it usually requires a large numberof images labeled with fine-grained\nsegmentation masks, andthe annotation of these masks can be very expensive\nandtime-consuming. Therefore, recent methods try to use un-supervised domain\nadaptation (UDA) methods to borrow in-formation from labeled data from other\ndatasets (source do-mains) to a new dataset (target domain). However, due tothe\nabsence of labels in the target domain, the performance ofUDA methods is much\nworse than that of the fully supervisedmethod. In this paper, we propose a\nweakly supervised do-main adaptation setting, in which we can partially label\nnewdatasets with bounding boxes, which are easier and cheaperto obtain than\nsegmentation masks. Accordingly, we proposea new weakly-supervised domain\nadaptation method calledBox-Adapt, which fully explores the fine-grained\nsegmenta-tion mask in the source domain and the weak bounding boxin the target\ndomain. Our Box-Adapt is a two-stage methodthat first performs joint training\non the source and target do-mains, and then conducts self-training with the\npseudo-labelsof the target domain. We demonstrate the effectiveness of\nourmethod in the liver segmentation task. Weakly supervised do-main adaptation\n", "rewritten_text": "Deep learning excels at medical image segmentation, but its reliance on extensively labeled data is costly and time-consuming.  Unsupervised domain adaptation (UDA) offers a solution by leveraging labeled data from other datasets, but its performance lags behind fully supervised methods due to the lack of target domain labels.  This paper introduces a weakly supervised domain adaptation approach, Box-Adapt, which utilizes readily available bounding box annotations in the target domain alongside fully labeled source domain data.  Box-Adapt employs a two-stage process: joint source-target training followed by self-training with target domain pseudo-labels generated from the bounding boxes.  We validate Box-Adapt's effectiveness through liver segmentation experiments.\n"}, "2203.02231": {"original_text": "  Light field disparity estimation is an essential task in computer vision with\nvarious applications. Although supervised learning-based methods have achieved\nboth higher accuracy and efficiency than traditional optimization-based\nmethods, the dependency on ground-truth disparity for training limits the\noverall generalization performance not to say for real-world scenarios where\nthe ground-truth disparity is hard to capture. In this paper, we argue that\nunsupervised methods can achieve comparable accuracy, but, more importantly,\nmuch higher generalization capacity and efficiency than supervised methods.\nSpecifically, we present the Occlusion Pattern Aware Loss, named OPAL, which\nsuccessfully extracts and encodes the general occlusion patterns inherent in\nthe light field for loss calculation. OPAL enables: i) accurate and robust\nestimation by effectively handling occlusions without using any ground-truth\ninformation for training and ii) much efficient performance by significantly\nreducing the network parameters required for accurate inference. Besides, a\ntransformer-based network and a refinement module are proposed for achieving\neven more accurate results. Extensive experiments demonstrate our method not\nonly significantly improves the accuracy compared with the SOTA unsupervised\nmethods, but also possesses strong generalization capacity, even for real-world\ndata, compared with supervised methods. Our code will be made publicly\navailable.\n", "rewritten_text": "Light field disparity estimation is crucial in computer vision. While supervised learning methods offer high accuracy and efficiency, their reliance on ground-truth disparity hinders generalization, especially in real-world applications.  This paper introduces an unsupervised approach, leveraging an Occlusion Pattern Aware Loss (OPAL), which effectively encodes inherent light field occlusion patterns.  OPAL enables accurate and robust disparity estimation without ground-truth data, significantly reducing network complexity.  Further improvements are achieved through a transformer-based network and a refinement module.  Extensive experiments show our method surpasses state-of-the-art unsupervised methods in accuracy and exhibits superior generalization capabilities, even outperforming supervised methods on real-world data.  Our code will be publicly released.\n"}, "2006.1038": {"original_text": "  Video semantic segmentation is active in recent years benefited from the\ngreat progress of image semantic segmentation. For such a task, the per-frame\nimage segmentation is generally unacceptable in practice due to high\ncomputation cost. To tackle this issue, many works use the flow-based feature\npropagation to reuse the features of previous frames. However, the optical flow\nestimation inevitably suffers inaccuracy and then causes the propagated\nfeatures distorted. In this paper, we propose distortion-aware feature\ncorrection to alleviate the issue, which improves video segmentation\nperformance by correcting distorted propagated features. To be specific, we\nfirstly propose to transfer distortion patterns from feature into image space\nand conduct effective distortion map prediction. Benefited from the guidance of\ndistortion maps, we proposed Feature Correction Module (FCM) to rectify\npropagated features in the distorted areas. Our proposed method can\nsignificantly boost the accuracy of video semantic segmentation at a low price.\nThe extensive experimental results on Cityscapes and CamVid show that our\nmethod outperforms the recent state-of-the-art methods.\n", "rewritten_text": "Recent advancements in image semantic segmentation have fueled progress in video semantic segmentation.  However, frame-by-frame processing is computationally expensive.  Existing methods often address this by propagating features from previous frames using optical flow, but inaccuracies in optical flow lead to feature distortion.  This paper introduces a distortion-aware feature correction method to improve video segmentation accuracy.  We achieve this by transferring distortion patterns from the feature space to the image space, predicting a distortion map, and then using a Feature Correction Module (FCM) to rectify distorted propagated features.  Our approach significantly improves accuracy with minimal computational overhead, outperforming state-of-the-art methods on Cityscapes and CamVid datasets.\n"}, "2101.11251": {"original_text": "  Junctions reflect the important geometrical structure information of the\nimage, and are of primary significance to applications such as image matching\nand motion analysis. Previous event-based feature extraction methods are mainly\nfocused on corners, which mainly find their locations, however, ignoring the\ngeometrical structure information like orientations and scales of edges. This\npaper adapts the frame-based a-contrario junction detector(ACJ) to event data,\nproposing the event-based a-contrario junction detector(e-ACJ), which yields\njunctions' locations while giving the scales and orientations of their\nbranches. The proposed method relies on an a-contrario model and can operate on\nasynchronous events directly without generating synthesized event frames. We\nevaluate the performance on public event datasets. The result shows our method\nsuccessfully finds the orientations and scales of branches, while maintaining\nhigh accuracy in junction's location.\n", "rewritten_text": "Image junctions encode crucial geometric information vital for applications like image matching and motion analysis.  Existing event-based feature extraction methods primarily focus on corner detection, neglecting crucial geometric details such as edge orientation and scale. This paper introduces e-ACJ, an event-based adaptation of the frame-based a-contrario junction detector (ACJ).  e-ACJ directly processes asynchronous event streams without frame synthesis, identifying junction locations and providing branch scale and orientation information.  Evaluated on public event datasets, e-ACJ demonstrates high accuracy in locating junctions while accurately determining branch orientations and scales.\n"}, "2210.16239": {"original_text": "  The Hyperspectral image (HSI) contains several hundred bands of the same\nregion called the Ground Truth (GT). The bands are taken in juxtaposed\nfrequencies, but some of them are noisily measured or contain no information.\nFor the classification, the selection of bands, affects significantly the\nresults of classification, in fact, using a subset of relevant bands, these\nresults can be better than those obtained using all bands, from which the need\nto reduce the dimensionality of the HSI. In this paper, a categorization of\ndimensionality reduction methods, according to the generation process, is\npresented. Furthermore, we reproduce an algorithm based on mutual information\n(MI) to reduce dimensionality by features selection and we introduce an\nalgorithm using mutual information and homogeneity. The two schemas are a\nfilter strategy. Finally, to validate this, we consider the case study AVIRIS\nHSI 92AV3C.\n  Keywords: Hyperspectrale images; classification; features selection; mutual\ninformation; homogeneity\n", "rewritten_text": "Hyperspectral images (HSIs) contain hundreds of spectral bands covering the same ground truth (GT) area.  However, some bands are noisy or uninformative.  Band selection significantly impacts classification accuracy; using a subset of relevant bands often outperforms using all bands, necessitating dimensionality reduction. This paper categorizes dimensionality reduction methods and presents two novel filter-based algorithms.  One algorithm uses mutual information (MI) for feature selection, while the other incorporates both MI and homogeneity.  The AVIRIS 92AV3C HSI dataset is used for validation.\n"}, "2108.07127": {"original_text": "  We translate a closed text that is known in advance and available in many\nlanguages into a new and severely low resource language. Most human translation\nefforts adopt a portion-based approach to translate consecutive pages/chapters\nin order, which may not suit machine translation. We compare the portion-based\napproach that optimizes coherence of the text locally with the random sampling\napproach that increases coverage of the text globally. Our results show that\nthe random sampling approach performs better. When training on a seed corpus of\n~1,000 lines from the Bible and testing on the rest of the Bible (~30,000\nlines), random sampling gives a performance gain of +11.0 BLEU using English as\na simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a\nMayan language. Furthermore, we compare three ways of updating machine\ntranslation models with increasing amount of human post-edited data through\niterations. We find that adding newly post-edited data to training after\nvocabulary update without self-supervision performs the best. We propose an\nalgorithm for human and machine to work together seamlessly to translate a\nclosed text into a severely low resource language.\n", "rewritten_text": "This study investigates optimal strategies for machine translation (MT) of a known, multilingual text into a severely low-resource language.  We compare two approaches: a sequential, portion-based translation (mimicking human workflows) and a random sampling approach prioritizing global text coverage.  Experiments using a ~30,000-line Bible corpus (with a 1,000-line seed) showed random sampling significantly outperformed the portion-based method, yielding a +11.0 BLEU improvement using simulated low-resource English and +4.9 BLEU using Eastern Pokomchi.  Additionally, we evaluated three iterative human post-editing strategies, finding that incorporating post-edited data after vocabulary updates (without self-supervision) yielded the best results.  Based on our findings, we propose a novel algorithm for efficient human-machine collaboration in low-resource language translation of closed texts.\n"}, "2103.10978": {"original_text": "  This paper addresses the problem of 3D human body shape and pose estimation\nfrom RGB images. Recent progress in this field has focused on single images,\nvideo or multi-view images as inputs. In contrast, we propose a new task: shape\nand pose estimation from a group of multiple images of a human subject, without\nconstraints on subject pose, camera viewpoint or background conditions between\nimages in the group. Our solution to this task predicts distributions over SMPL\nbody shape and pose parameters conditioned on the input images in the group. We\nprobabilistically combine predicted body shape distributions from each image to\nobtain a final multi-image shape prediction. We show that the additional body\nshape information present in multi-image input groups improves 3D human shape\nestimation metrics compared to single-image inputs on the SSP-3D dataset and a\nprivate dataset of tape-measured humans. In addition, predicting distributions\nover 3D bodies allows us to quantify pose prediction uncertainty, which is\nuseful when faced with challenging input images with significant occlusion. Our\nmethod demonstrates meaningful pose uncertainty on the 3DPW dataset and is\ncompetitive with the state-of-the-art in terms of pose estimation metrics.\n", "rewritten_text": "This paper introduces a novel approach to 3D human body shape and pose estimation from multiple RGB images.  Unlike existing methods relying on single images, videos, or multi-view images with controlled conditions, our method handles unconstrained groups of images depicting the same subject, regardless of pose, viewpoint, or background.  We predict probability distributions over SMPL body shape and pose parameters for each image, probabilistically fusing these distributions to generate a final, refined 3D shape estimate.  Experiments on the SSP-3D and a private dataset demonstrate improved 3D shape estimation accuracy compared to single-image methods.  Furthermore, our approach quantifies pose prediction uncertainty, proving beneficial in handling occluded images.  Evaluated on the 3DPW dataset, our method achieves state-of-the-art pose estimation accuracy while providing meaningful uncertainty estimates.\n"}, "2307.01447": {"original_text": "  Accurately matching local features between a pair of images is a challenging\ncomputer vision task. Previous studies typically use attention based graph\nneural networks (GNNs) with fully-connected graphs over keypoints within/across\nimages for visual and geometric information reasoning. However, in the context\nof feature matching, considerable keypoints are non-repeatable due to occlusion\nand failure of the detector, and thus irrelevant for message passing. The\nconnectivity with non-repeatable keypoints not only introduces redundancy,\nresulting in limited efficiency, but also interferes with the representation\naggregation process, leading to limited accuracy. Targeting towards high\naccuracy and efficiency, we propose MaKeGNN, a sparse attention-based GNN\narchitecture which bypasses non-repeatable keypoints and leverages matchable\nones to guide compact and meaningful message passing. More specifically, our\nBilateral Context-Aware Sampling Module first dynamically samples two small\nsets of well-distributed keypoints with high matchability scores from the image\npair. Then, our Matchable Keypoint-Assisted Context Aggregation Module regards\nsampled informative keypoints as message bottlenecks and thus constrains each\nkeypoint only to retrieve favorable contextual information from intra- and\ninter- matchable keypoints, evading the interference of irrelevant and\nredundant connectivity with non-repeatable ones. Furthermore, considering the\npotential noise in initial keypoints and sampled matchable ones, the MKACA\nmodule adopts a matchability-guided attentional aggregation operation for purer\ndata-dependent context propagation. By these means, we achieve the\nstate-of-the-art performance on relative camera estimation, fundamental matrix\nestimation, and visual localization, while significantly reducing computational\nand memory complexity compared to typical attentional GNNs.\n", "rewritten_text": "Matching local features in images is a difficult computer vision problem.  Existing attention-based graph neural networks (GNNs) often suffer from inefficiency and inaccuracy due to processing many non-repeatable keypoints (caused by occlusion or detector failure).  To address this, we introduce MaKeGNN, a sparse attention-based GNN.  MaKeGNN uses a Bilateral Context-Aware Sampling Module to select a small subset of highly matchable keypoints from each image.  A Matchable Keypoint-Assisted Context Aggregation (MKACA) module then uses these keypoints to guide message passing, focusing on relevant contextual information and avoiding interference from non-repeatable keypoints.  The MKACA module also incorporates matchability-guided attention to mitigate noise.  This approach achieves state-of-the-art results on relative camera estimation, fundamental matrix estimation, and visual localization, while significantly improving efficiency compared to previous GNN methods.\n"}, "1703.00087": {"original_text": "  Lesion segmentation is the first step in most automatic melanoma recognition\nsystems. Deficiencies and difficulties in dermoscopic images such as color\ninconstancy, hair occlusion, dark corners and color charts make lesion\nsegmentation an intricate task. In order to detect the lesion in the presence\nof these problems, we propose a supervised saliency detection method tailored\nfor dermoscopic images based on the discriminative regional feature integration\n(DRFI). DRFI method incorporates multi-level segmentation, regional contrast,\nproperty, background descriptors, and a random forest regressor to create\nsaliency scores for each region in the image. In our improved saliency\ndetection method, mDRFI, we have added some new features to regional property\ndescriptors. Also, in order to achieve more robust regional background\ndescriptors, a thresholding algorithm is proposed to obtain a new\npseudo-background region. Findings reveal that mDRFI is superior to DRFI in\ndetecting the lesion as the salient object in dermoscopic images. The proposed\noverall lesion segmentation framework uses detected saliency map to construct\nan initial mask of the lesion through thresholding and post-processing\noperations. The initial mask is then evolving in a level set framework to fit\nbetter on the lesion's boundaries. The results of evaluation tests on three\npublic datasets show that our proposed segmentation method outperforms the\nother conventional state-of-the-art segmentation algorithms and its performance\nis comparable with most recent approaches that are based on deep convolutional\nneural networks.\n", "rewritten_text": "Accurate lesion segmentation is crucial for automated melanoma diagnosis, but challenges like inconsistent color, hair, shadows, and color charts in dermoscopic images complicate this process.  This paper presents mDRFI, an improved supervised saliency detection method for dermoscopic images.  Building upon the discriminative regional feature integration (DRFI) method, mDRFI enhances regional property descriptors and employs a novel thresholding algorithm for more robust background descriptors.  This leads to superior lesion detection compared to DRFI.  The resulting saliency map initiates lesion segmentation via thresholding and post-processing, followed by level set refinement for precise boundary delineation.  Evaluated on three public datasets, our method surpasses conventional techniques and achieves performance comparable to leading deep learning approaches.\n"}, "2405.12209": {"original_text": "  Recent advancements in large language models (LLMs) have showcased\nsignificant improvements in mathematics. However, traditional math benchmarks\nlike GSM8k offer a unidimensional perspective, falling short in providing a\nholistic assessment of the LLMs' math capabilities. To address this gap, we\nintroduce MathBench, a new benchmark that rigorously assesses the mathematical\ncapabilities of large language models. MathBench spans a wide range of\nmathematical disciplines, offering a detailed evaluation of both theoretical\nunderstanding and practical problem-solving skills. The benchmark progresses\nthrough five distinct stages, from basic arithmetic to college mathematics, and\nis structured to evaluate models at various depths of knowledge. Each stage\nincludes theoretical questions and application problems, allowing us to measure\na model's mathematical proficiency and its ability to apply concepts in\npractical scenarios. MathBench aims to enhance the evaluation of LLMs'\nmathematical abilities, providing a nuanced view of their knowledge\nunderstanding levels and problem solving skills in a bilingual context. The\nproject is released at https://github.com/open-compass/MathBench .\n", "rewritten_text": "Large language models (LLMs) have shown progress in mathematics, but existing benchmarks like GSM8k provide an incomplete picture.  To offer a more comprehensive evaluation, we introduce MathBench, a new benchmark covering a broad spectrum of mathematical disciplines, from arithmetic to college-level mathematics.  MathBench's five stages assess both theoretical understanding and practical application through diverse problem types.  This bilingual benchmark provides a nuanced evaluation of LLMs' mathematical proficiency and problem-solving skills.  The project is available at https://github.com/open-compass/MathBench.\n"}, "2407.03993": {"original_text": "  Natural language counterfactual generation aims to minimally modify a given\ntext such that the modified text will be classified into a different class. The\ngenerated counterfactuals provide insight into the reasoning behind a model's\npredictions by highlighting which words significantly influence the outcomes.\nAdditionally, they can be used to detect model fairness issues and augment the\ntraining data to enhance the model's robustness. A substantial amount of\nresearch has been conducted to generate counterfactuals for various NLP tasks,\nemploying different models and methodologies. With the rapid growth of studies\nin this field, a systematic review is crucial to guide future researchers and\ndevelopers. To bridge this gap, this survey provides a comprehensive overview\nof textual counterfactual generation methods, particularly those based on Large\nLanguage Models. We propose a new taxonomy that systematically categorizes the\ngeneration methods into four groups and summarizes the metrics for evaluating\nthe generation quality. Finally, we discuss ongoing research challenges and\noutline promising directions for future work.\n", "rewritten_text": "This survey systematically reviews natural language counterfactual generation, focusing on methods leveraging Large Language Models.  By minimally altering text to change its predicted class, counterfactual generation reveals model biases, improves robustness through data augmentation, and illuminates prediction reasoning.  We categorize existing methods into a novel four-group taxonomy, summarize evaluation metrics, and identify key challenges and future research directions in this rapidly expanding field.\n"}, "1909.13411": {"original_text": "  Mesoscale eddies play a significant role in marine energy transport, marine\nbiological environment and marine climate. Due to their huge impact on the\nocean, mesoscale eddy detection has become a hot research area in recent years.\nTherefore, more and more people are entering the field of mesoscale eddy\ndetection. However, the existing detection methods mainly based on traditional\ndetection methods typically only use Sea Surface Height (SSH) as a variable to\ndetect, resulting in inaccurate performance. In this paper, we propose a\nmesoscale eddy detection method based on multivariate fusion data to solve this\nproblem. We not only use the SSH variable, but also add the two variables: Sea\nSurface Temperature (SST) and velocity of flow, achieving a multivariate\ninformation fusion input. We design a novel symmetric network, which merges\nlow-level feature maps from the downsampling pathway and high-level feature\nmaps from the upsampling pathway by lateral connection. In addition, we apply\ndilated convolutions to network structure to increase the receptive field and\nobtain more contextual information in the case of constant parameter. In the\nend, we demonstrate the effectiveness of our method on dataset provided by us,\nachieving the test set performance of 97.06% , greatly improved the performance\nof previous methods of mesoscale eddy detection.\n", "rewritten_text": "Mesoscale eddies significantly influence ocean energy transport, marine life, and climate, making their detection a rapidly growing research field.  Existing methods, primarily relying on Sea Surface Height (SSH), suffer from inaccuracy.  This paper introduces a novel mesoscale eddy detection method using multivariate data fusion.  By incorporating SSH, Sea Surface Temperature (SST), and flow velocity, our approach leverages a symmetric network architecture with lateral connections merging low-level and high-level feature maps.  Dilated convolutions expand the receptive field, enhancing contextual information.  Our method achieves a 97.06% accuracy on our dataset, significantly outperforming previous techniques.\n"}, "1808.07733": {"original_text": "  We analyze the performance of different sentiment classification models on\nsyntactically complex inputs like A-but-B sentences. The first contribution of\nthis analysis addresses reproducible research: to meaningfully compare\ndifferent models, their accuracies must be averaged over far more random seeds\nthan what has traditionally been reported. With proper averaging in place, we\nnotice that the distillation model described in arXiv:1603.06318v4 [cs.LG],\nwhich incorporates explicit logic rules for sentiment classification, is\nineffective. In contrast, using contextualized ELMo embeddings\n(arXiv:1802.05365v2 [cs.CL]) instead of logic rules yields significantly better\nperformance. Additionally, we provide analysis and visualizations that\ndemonstrate ELMo's ability to implicitly learn logic rules. Finally, a\ncrowdsourced analysis reveals how ELMo outperforms baseline models even on\nsentences with ambiguous sentiment labels.\n", "rewritten_text": "This study evaluates the performance of various sentiment classification models on complex sentences, specifically those with A-but-B structures.  We demonstrate the critical need for extensive random seed averaging to ensure reproducible results when comparing models.  This rigorous evaluation reveals that a logic-rule-based distillation model (arXiv:1603.06318v4) performs poorly, unlike models using contextualized ELMo embeddings (arXiv:1802.05365v2), which achieve significantly better accuracy.  Furthermore, our analysis, including visualizations, suggests ELMo implicitly learns the logic rules.  Finally, crowdsourced data confirms ELMo's superiority, even on sentences with ambiguous sentiment.\n"}, "2011.09634": {"original_text": "  In this paper, we teach machines to understand visuals and natural language\nby learning the mapping between sentences and noisy video snippets without\nexplicit annotations. Firstly, we define a self-supervised learning framework\nthat captures the cross-modal information. A novel adversarial learning module\nis then introduced to explicitly handle the noises in the natural videos, where\nthe subtitle sentences are not guaranteed to be strongly corresponded to the\nvideo snippets. For training and evaluation, we contribute a new dataset\n`ApartmenTour' that contains a large number of online videos and subtitles. We\ncarry out experiments on the bidirectional retrieval tasks between sentences\nand videos, and the results demonstrate that our proposed model achieves the\nstate-of-the-art performance on both retrieval tasks and exceeds several strong\nbaselines. The dataset can be downloaded at https://github.com/zyj-13/WAL.\n", "rewritten_text": "This paper presents a novel self-supervised learning framework that enables machines to understand the relationship between natural language and visual data.  The framework leverages a new adversarial learning module to mitigate the impact of noise inherent in unaligned video-subtitle pairs.  We introduce a new dataset, ApartmentTour, comprising numerous online videos and their subtitles, used to train and evaluate our model.  Experiments on bidirectional video-sentence retrieval demonstrate state-of-the-art performance, surpassing existing methods. The ApartmentTour dataset is available at https://github.com/zyj-13/WAL.\n"}, "1708.01654": {"original_text": "  We demonstrate the use of shape-from-shading (SfS) to improve both the\nquality and the robustness of 3D reconstruction of dynamic objects captured by\na single camera. Unlike previous approaches that made use of SfS as a\npost-processing step, we offer a principled integrated approach that solves\ndynamic object tracking and reconstruction and SfS as a single unified cost\nfunction. Moving beyond Lambertian S f S , we propose a general approach that\nmodels both specularities and shading while simultaneously tracking and\nreconstructing general dynamic objects. Solving these problems jointly prevents\nthe kinds of tracking failures which can not be recovered from by pipeline\napproaches. We show state-of-the-art results both qualitatively and\nquantitatively.\n", "rewritten_text": "This paper presents a novel, integrated approach to 3D reconstruction of dynamic objects from monocular video.  Unlike previous methods that apply shape-from-shading (SfS) as a post-processing step, our method jointly optimizes object tracking, reconstruction, and SfS within a unified cost function.  This approach handles both Lambertian and specular reflections, significantly improving robustness and accuracy compared to pipeline methods that are vulnerable to unrecoverable tracking failures.  We demonstrate state-of-the-art qualitative and quantitative results.\n"}, "1711.07721": {"original_text": "  Post capture refocusing effect in smartphone cameras is achievable by using\nfocal stacks. However, the accuracy of this effect is totally dependent on the\ncombination of the depth layers in the stack. The accuracy of the extended\ndepth of field effect in this application can be improved significantly by\ncomputing an accurate depth map which has been an open issue for decades. To\ntackle this issue, in this paper, a framework is proposed based on\nPreconditioned Alternating Direction Method of Multipliers (PADMM) for depth\nfrom the focal stack and synthetic defocus application. In addition to its\nability to provide high structural accuracy and occlusion handling, the\noptimization function of the proposed method can, in fact, converge faster and\nbetter than state of the art methods. The evaluation has been done on 21 sets\nof focal stacks and the optimization function has been compared against 5 other\nmethods. Preliminary results indicate that the proposed method has a better\nperformance in terms of structural accuracy and optimization in comparison to\nthe current state of the art methods.\n", "rewritten_text": "Smartphone cameras can achieve post-capture refocusing using focal stacks, but accuracy depends heavily on how these depth layers are combined.  Improving the extended depth of field effect requires a precise depth map, a long-standing challenge. This paper proposes a novel framework based on the Preconditioned Alternating Direction Method of Multipliers (PADMM) to generate depth maps from focal stacks and apply synthetic defocus.  Our method offers superior structural accuracy, better occlusion handling, and faster convergence than existing state-of-the-art techniques, as demonstrated by evaluations on 21 focal stack datasets and comparisons with five other methods.  Preliminary results show improved performance in both structural accuracy and optimization speed.\n"}, "2004.06376": {"original_text": "  Understanding the shape of a scene from a single color image is a formidable\ncomputer vision task. However, most methods aim to predict the geometry of\nsurfaces that are visible to the camera, which is of limited use when planning\npaths for robots or augmented reality agents. Such agents can only move when\ngrounded on a traversable surface, which we define as the set of classes which\nhumans can also walk over, such as grass, footpaths and pavement. Models which\npredict beyond the line of sight often parameterize the scene with voxels or\nmeshes, which can be expensive to use in machine learning frameworks.\n  We introduce a model to predict the geometry of both visible and occluded\ntraversable surfaces, given a single RGB image as input. We learn from stereo\nvideo sequences, using camera poses, per-frame depth and semantic segmentation\nto form training data, which is used to supervise an image-to-image network. We\ntrain models from the KITTI driving dataset, the indoor Matterport dataset, and\nfrom our own casually captured stereo footage. We find that a surprisingly low\nbar for spatial coverage of training scenes is required. We validate our\nalgorithm against a range of strong baselines, and include an assessment of our\npredictions for a path-planning task.\n", "rewritten_text": "Predicting scene geometry from a single color image is a challenging computer vision problem.  Existing methods primarily focus on visible surfaces, insufficient for robot or augmented reality navigation, which requires knowledge of traversable surfaces (e.g., grass, paths, pavement).  Representing the entire scene using voxels or meshes is computationally expensive.  This paper presents a novel model that predicts the geometry of both visible and occluded traversable surfaces from a single RGB image.  Trained on stereo video data (KITTI, Matterport, and custom datasets) incorporating camera poses, depth, and semantic segmentation, our image-to-image network surprisingly requires minimal spatial training data coverage.  We benchmark our model against state-of-the-art methods and demonstrate its effectiveness in a path-planning application.\n"}, "2107.08173": {"original_text": "  This ability to learn consecutive tasks without forgetting how to perform\npreviously trained problems is essential for developing an online dialogue\nsystem. This paper proposes an effective continual learning for the\ntask-oriented dialogue system with iterative network pruning, expanding and\nmasking (TPEM), which preserves performance on previously encountered tasks\nwhile accelerating learning progress on subsequent tasks. Specifically, TPEM\n(i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts\nnetwork expanding to create free weights for new tasks, and (iii) introduces\ntask-specific network masking to alleviate the negative impact of fixed weights\nof old tasks on new tasks. We conduct extensive experiments on seven different\ntasks from three benchmark datasets and show empirically that TPEM leads to\nsignificantly improved results over the strong competitors. For\nreproducibility, we submit the code and data at:\nhttps://github.com/siat-nlp/TPEM\n", "rewritten_text": "Continual learning is crucial for developing robust online dialogue systems.  This paper introduces TPEM, a novel continual learning method for task-oriented dialogue systems that uses iterative network pruning, expansion, and masking.  TPEM preserves performance on previously learned tasks while accelerating learning on new ones by (i) pruning to retain knowledge from old tasks, (ii) expanding the network to accommodate new tasks, and (iii) masking to mitigate interference between old and new task weights.  Extensive experiments across seven tasks from three benchmark datasets demonstrate TPEM's significant performance improvements over existing methods.  Code and data are available at: https://github.com/siat-nlp/TPEM\n"}, "1811.09789": {"original_text": "  There has been much recent work on image captioning models that describe the\nfactual aspects of an image. Recently, some models have incorporated\nnon-factual aspects into the captions, such as sentiment or style. However,\nsuch models typically have difficulty in balancing the semantic aspects of the\nimage and the non-factual dimensions of the caption; in addition, it can be\nobserved that humans may focus on different aspects of an image depending on\nthe chosen sentiment or style of the caption. To address this, we design an\nattention-based model to better add sentiment to image captions. The model\nembeds and learns sentiment with respect to image-caption data, and uses both\nhigh-level and word-level sentiment information during the learning process.\nThe model outperforms the state-of-the-art work in image captioning with\nsentiment using standard evaluation metrics. An analysis of generated captions\nalso shows that our model does this by a better selection of the\nsentiment-bearing adjectives and adjective-noun pairs.\n", "rewritten_text": "Recent image captioning models primarily focus on factual descriptions.  While some now incorporate non-factual elements like sentiment or style, they struggle to balance these with the image's semantic content, and human focus varies depending on the desired style.  To overcome this, we propose a novel attention-based model that effectively integrates sentiment into image captions.  This model learns sentiment embeddings from image-caption data, leveraging both high-level and word-level sentiment information.  Our model surpasses existing state-of-the-art sentiment-aware image captioning models, as demonstrated by standard evaluation metrics and an analysis revealing improved selection of sentiment-laden adjectives and adjective-noun pairs.\n"}, "2310.15724": {"original_text": "  Pre-trained language models (PLMs) have achieved remarkable results on NLP\ntasks but at the expense of huge parameter sizes and the consequent\ncomputational costs. In this paper, we propose Variator, a parameter-efficient\nacceleration method that enhances computational efficiency through\nplug-and-play compression plugins. Compression plugins are designed to reduce\nthe sequence length via compressing multiple hidden vectors into one and\ntrained with original PLMs frozen. Different from traditional model\nacceleration methods, which compress PLMs to smaller sizes, Variator offers two\ndistinct advantages: (1) In real-world applications, the plug-and-play nature\nof our compression plugins enables dynamic selection of different compression\nplugins with varying acceleration ratios based on the current workload. (2) The\ncompression plugin comprises a few compact neural network layers with minimal\nparameters, significantly saving storage and memory overhead, particularly in\nscenarios with a growing number of tasks. We validate the effectiveness of\nVariator on seven datasets. Experimental results show that Variator can save\n53% computational costs using only 0.9% additional parameters with a\nperformance drop of less than 2%. Moreover, when the model scales to billions\nof parameters, Variator matches the strong performance of uncompressed PLMs.\n", "rewritten_text": "Large language models (LLMs) achieve impressive performance but are computationally expensive due to their massive size.  This paper introduces Variator, a novel parameter-efficient acceleration method.  Variator uses plug-and-play compression plugins that reduce sequence length by aggregating hidden vectors, trained independently while leaving the LLM frozen. Unlike methods that shrink model size, Variator offers dynamic compression plugin selection for optimal acceleration based on workload, and its minimal parameter overhead remains small even with many tasks.  Evaluated on seven datasets, Variator reduced computational cost by 53% with only a 0.9% parameter increase and less than 2% performance degradation.  Furthermore, it maintains strong performance even with billion-parameter models.\n"}, "2110.02929": {"original_text": "  Event-based dynamic vision sensors provide very sparse output in the form of\nspikes, which makes them suitable for low-power applications. Convolutional\nspiking neural networks model such event-based data and develop their full\nenergy-saving potential when deployed on asynchronous neuromorphic hardware.\nEvent-based vision being a nascent field, the sensitivity of spiking neural\nnetworks to potentially malicious adversarial attacks has received little\nattention so far. We show how white-box adversarial attack algorithms can be\nadapted to the discrete and sparse nature of event-based visual data, and\ndemonstrate smaller perturbation magnitudes at higher success rates than the\ncurrent state-of-the-art algorithms. For the first time, we also verify the\neffectiveness of these perturbations directly on neuromorphic hardware.\nFinally, we discuss the properties of the resulting perturbations, the effect\nof adversarial training as a defense strategy, and future directions.\n", "rewritten_text": "Event-based dynamic vision sensors' sparse, spike-based output is ideal for low-power applications, especially when paired with energy-efficient convolutional spiking neural networks (SNNs) on asynchronous neuromorphic hardware.  However, the vulnerability of these SNNs to adversarial attacks remains largely unexplored.  This work adapts existing white-box attack algorithms to the unique characteristics of event-based data, achieving higher success rates with smaller perturbations than current methods.  We further demonstrate these attacks' effectiveness on neuromorphic hardware for the first time, and analyze the resulting perturbations, the efficacy of adversarial training, and future research directions.\n"}, "2405.20259": {"original_text": "  The proliferation of deep learning solutions and the scarcity of large\nannotated datasets pose significant challenges in real-world applications.\nVarious strategies have been explored to overcome this challenge, with data\naugmentation (DA) approaches emerging as prominent solutions. DA approaches\ninvolve generating additional examples by transforming existing labeled data,\nthereby enriching the dataset and helping deep learning models achieve improved\ngeneralization without succumbing to overfitting. In real applications, where\nsolutions based on deep learning are widely used, there is facial expression\nrecognition (FER), which plays an essential role in human communication,\nimproving a range of knowledge areas (e.g., medicine, security, and marketing).\nIn this paper, we propose a simple and comprehensive face data augmentation\napproach based on mixed face component regularization that outperforms the\nclassical DA approaches from the literature, including the MixAugment which is\na specific approach for the target task in two well-known FER datasets existing\nin the literature.\n", "rewritten_text": "Deep learning's widespread use is hampered by the limited availability of large, labeled datasets.  Data augmentation (DA) offers a key solution, generating synthetic training data from existing examples to improve model generalization and prevent overfitting.  Facial expression recognition (FER), crucial in fields like medicine, security, and marketing, exemplifies this challenge. This paper introduces a novel, efficient face data augmentation technique based on mixed face component regularization.  Our method surpasses existing DA approaches, including MixAugment, on two benchmark FER datasets.\n"}, "2308.09983": {"original_text": "  Early detection of dysplasia of the cervix is critical for cervical cancer\ntreatment. However, automatic cervical dysplasia diagnosis via visual\ninspection, which is more appropriate in low-resource settings, remains a\nchallenging problem. Though promising results have been obtained by recent deep\nlearning models, their performance is significantly hindered by the limited\nscale of the available cervix datasets. Distinct from previous methods that\nlearn from a single dataset, we propose to leverage cross-domain cervical\nimages that were collected in different but related clinical studies to improve\nthe model's performance on the targeted cervix dataset. To robustly learn the\ntransferable information across datasets, we propose a novel prototype-based\nknowledge filtering method to estimate the transferability of cross-domain\nsamples. We further optimize the shared feature space by aligning the\ncross-domain image representations simultaneously on domain level with early\nalignment and class level with supervised contrastive learning, which endows\nmodel training and knowledge transfer with stronger robustness. The empirical\nresults on three real-world benchmark cervical image datasets show that our\nproposed method outperforms the state-of-the-art cervical dysplasia visual\ninspection by an absolute improvement of 4.7% in top-1 accuracy, 7.0% in\nprecision, 1.4% in recall, 4.6% in F1 score, and 0.05 in ROC-AUC.\n", "rewritten_text": "Early detection of cervical dysplasia is crucial for effective cancer treatment.  Automated visual inspection, particularly valuable in resource-limited settings, remains challenging due to limited training data.  This paper introduces a novel approach to improve deep learning models for cervical dysplasia diagnosis by leveraging cross-domain cervical images from multiple related studies.  A prototype-based knowledge filtering method assesses the transferability of these images, while early alignment and supervised contrastive learning optimize the shared feature space for robust knowledge transfer.  Results on three benchmark datasets demonstrate significant improvements over existing methods, achieving a 4.7% absolute increase in top-1 accuracy, along with substantial gains in precision, recall, F1 score, and ROC-AUC.\n"}, "1704.02268": {"original_text": "  Unsupervised learning of visual similarities is of paramount importance to\ncomputer vision, particularly due to lacking training data for fine-grained\nsimilarities. Deep learning of similarities is often based on relationships\nbetween pairs or triplets of samples. Many of these relations are unreliable\nand mutually contradicting, implying inconsistencies when trained without\nsupervision information that relates different tuples or triplets to each\nother. To overcome this problem, we use local estimates of reliable\n(dis-)similarities to initially group samples into compact surrogate classes\nand use local partial orders of samples to classes to link classes to each\nother. Similarity learning is then formulated as a partial ordering task with\nsoft correspondences of all samples to classes. Adopting a strategy of\nself-supervision, a CNN is trained to optimally represent samples in a mutually\nconsistent manner while updating the classes. The similarity learning and\ngrouping procedure are integrated in a single model and optimized jointly. The\nproposed unsupervised approach shows competitive performance on detailed pose\nestimation and object classification.\n", "rewritten_text": "Unsupervised visual similarity learning is crucial for computer vision, especially given the scarcity of training data for fine-grained distinctions.  Existing deep learning methods often rely on unreliable and contradictory pairwise or triplet comparisons.  To address this, we introduce a novel approach that leverages locally estimated reliable similarities to initially cluster samples into surrogate classes.  These classes are then interconnected using local partial orderings, framing similarity learning as a partial ordering task with soft sample-to-class assignments.  A self-supervised CNN is trained to consistently represent samples while iteratively refining these classes.  This integrated, jointly optimized model achieves competitive results on detailed pose estimation and object classification.\n"}, "1909.07598": {"original_text": "  Multi-hop question answering (QA) requires an information retrieval (IR)\nsystem that can find \\emph{multiple} supporting evidence needed to answer the\nquestion, making the retrieval process very challenging. This paper introduces\nan IR technique that uses information of entities present in the initially\nretrieved evidence to learn to `\\emph{hop}' to other relevant evidence. In a\nsetting, with more than \\textbf{5 million} Wikipedia paragraphs, our approach\nleads to significant boost in retrieval performance. The retrieved evidence\nalso increased the performance of an existing QA model (without any training)\non the \\hotpot benchmark by \\textbf{10.59} F1.\n", "rewritten_text": "Answering multi-hop questions demands retrieving multiple supporting documents.  This paper presents a novel information retrieval technique that leverages entities within initially retrieved evidence to iteratively discover further relevant information.  Evaluated on a dataset of over 5 million Wikipedia paragraphs, our method significantly improves retrieval performance and, without further training, boosts the F1 score of a state-of-the-art question answering model on the HotpotQA benchmark by 10.59 points.\n"}, "2211.06726": {"original_text": "  Vision Transformer (ViT) is a pioneering deep learning framework that can\naddress real-world computer vision issues, such as image classification and\nobject recognition. Importantly, ViTs are proven to outperform traditional deep\nlearning models, such as convolutional neural networks (CNNs). Relatively\nrecently, a number of ViT mutations have been transplanted into the field of\nmedical imaging, thereby resolving a variety of critical classification and\nsegmentation challenges, especially in terms of brain imaging data. In this\nwork, we provide a novel multimodal deep learning pipeline, MultiCrossViT,\nwhich is capable of analyzing both structural MRI (sMRI) and static functional\nnetwork connectivity (sFNC) data for the prediction of schizophrenia disease.\nOn a dataset with minimal training subjects, our novel model can achieve an AUC\nof 0.832. Finally, we visualize multiple brain regions and covariance patterns\nmost relevant to schizophrenia based on the resulting ViT attention maps by\nextracting features from transformer encoders.\n", "rewritten_text": "This paper introduces MultiCrossViT, a novel multimodal deep learning pipeline leveraging Vision Transformers (ViTs) to predict schizophrenia from structural MRI (sMRI) and static functional network connectivity (sFNC) data.  Unlike traditional convolutional neural networks (CNNs), ViTs have demonstrated superior performance in computer vision and have recently shown promise in medical imaging.  MultiCrossViT achieves an AUC of 0.832 on a dataset with limited training samples.  Furthermore, by analyzing attention maps from the transformer encoders, we identify key brain regions and covariance patterns associated with schizophrenia.\n"}, "1607.02504": {"original_text": "  We present a method to predict image deformations based on patch-wise image\nappearance. Specifically, we design a patch-based deep encoder-decoder network\nwhich learns the pixel/voxel-wise mapping between image appearance and\nregistration parameters. Our approach can predict general deformation\nparameterizations, however, we focus on the large deformation diffeomorphic\nmetric mapping (LDDMM) registration model. By predicting the LDDMM\nmomentum-parameterization we retain the desirable theoretical properties of\nLDDMM, while reducing computation time by orders of magnitude: combined with\npatch pruning, we achieve a 1500x/66x speed up compared to GPU-based\noptimization for 2D/3D image registration. Our approach has better prediction\naccuracy than predicting deformation or velocity fields and results in\ndiffeomorphic transformations. Additionally, we create a Bayesian probabilistic\nversion of our network, which allows evaluation of deformation field\nuncertainty through Monte Carlo sampling using dropout at test time. We show\nthat deformation uncertainty highlights areas of ambiguous deformations. We\ntest our method on the OASIS brain image dataset in 2D and 3D.\n", "rewritten_text": "This paper introduces a novel patch-based deep learning method for fast and accurate image deformation prediction.  Our encoder-decoder network learns a pixel/voxel-wise mapping between image appearance and large deformation diffeomorphic metric mapping (LDDMM) registration parameters, specifically predicting the LDDMM momentum. This approach significantly accelerates LDDMM registration (1500x/66x speedup for 2D/3D on GPU) while preserving its theoretical advantages, thanks to patch pruning.  Unlike methods predicting deformation or velocity fields directly, our approach yields diffeomorphic transformations and achieves superior accuracy.  Furthermore, a Bayesian extension enables uncertainty quantification of the deformation field via Monte Carlo dropout sampling, effectively highlighting ambiguous regions.  We demonstrate the efficacy of our method on 2D and 3D OASIS brain images.\n"}, "1911.08007": {"original_text": "  The classification of streets on road networks has been focused on the\nvehicular transportational features of streets such as arterials, major roads,\nminor roads and so forth based on their transportational use. City authorities\non the other hand have been shifting to more urban inclusive planning of\nstreets, encompassing the side use of a street combined with the\ntransportational features of a street. In such classification schemes, streets\nare labeled for example as commercial throughway, residential neighborhood,\npark etc. This modern approach to urban planning has been adopted by major\ncities such as the city of San Francisco, the states of Florida and\nPennsylvania among many others. Currently, the process of labeling streets\naccording to their contexts is manual and hence is tedious and time consuming.\nIn this paper, we propose an approach to collect and label imagery data then\ndeploy advancements in computer vision towards modern urban planning. We\ncollect and label street imagery then train deep convolutional neural networks\n(CNN) to perform the classification of street context. We show that CNN models\ncan perform well achieving accuracies in the 81% to 87%, we then visualize\nsamples from the embedding space of streets using the t-SNE method and apply\nclass activation mapping methods to interpret the features in street imagery\ncontributing to output classification from a model.\n", "rewritten_text": "Traditional road network classification prioritizes vehicular traffic, categorizing streets as arterials, major roads, etc.  However, modern urban planning increasingly incorporates broader street uses, classifying them by context (e.g., commercial throughway, residential neighborhood).  This approach, adopted by cities like San Francisco and states like Florida and Pennsylvania, currently relies on manual, time-consuming labeling.  This paper presents a novel automated approach using computer vision. We collect and label street imagery, train deep convolutional neural networks (CNNs) to classify street context (achieving 81-87% accuracy), and utilize t-SNE visualization and class activation mapping to interpret the model's learned features.\n"}, "1811.11387": {"original_text": "  The success of deep neural networks generally requires a vast amount of\ntraining data to be labeled, which is expensive and unfeasible in scale,\nespecially for video collections. To alleviate this problem, in this paper, we\npropose 3DRotNet: a fully self-supervised approach to learn spatiotemporal\nfeatures from unlabeled videos. A set of rotations are applied to all videos,\nand a pretext task is defined as prediction of these rotations. When\naccomplishing this task, 3DRotNet is actually trained to understand the\nsemantic concepts and motions in videos. In other words, it learns a\nspatiotemporal video representation, which can be transferred to improve video\nunderstanding tasks in small datasets. Our extensive experiments successfully\ndemonstrate the effectiveness of the proposed framework on action recognition,\nleading to significant improvements over the state-of-the-art self-supervised\nmethods. With the self-supervised pre-trained 3DRotNet from large datasets, the\nrecognition accuracy is boosted up by 20.4% on UCF101 and 16.7% on HMDB51\nrespectively, compared to the models trained from scratch.\n", "rewritten_text": "Training deep neural networks for video analysis typically requires vast quantities of labeled data, a costly and often impractical undertaking.  This paper introduces 3DRotNet, a self-supervised method that learns spatiotemporal features from unlabeled videos.  By predicting rotations applied to video clips (a pretext task), 3DRotNet implicitly learns semantic concepts and motion, creating a transferable spatiotemporal representation.  Extensive experiments on action recognition demonstrate 3DRotNet's effectiveness, significantly outperforming existing self-supervised methods.  Pre-training 3DRotNet on large datasets and then fine-tuning on smaller datasets like UCF101 and HMDB51 yielded accuracy improvements of 20.4% and 16.7%, respectively, compared to models trained from scratch.\n"}, "2402.01619": {"original_text": "  Program induction (PI) has become a promising paradigm for using knowledge\nbases (KBs) to help large language models (LLMs) answer complex\nknowledge-intensive questions. Nonetheless, PI typically relies on a large\nnumber of parallel question-program pairs to make the LLM aware of the schema\nof the given KB, and is thus challenging for many low-resourced KBs that lack\nannotated data. To this end, we propose KB-Plugin, a plug-and-play framework\nthat enables LLMs to induce programs over any low-resourced KB. Firstly,\nKB-Plugin adopts self-supervised learning to encode the detailed schema\ninformation of a given KB into a pluggable module, namely schema plugin.\nSecondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB\nto train another pluggable module, namely PI plugin, which can help the LLM\nextract question-relevant schema information from the schema plugin of any KB\nand utilize this information to induce programs over this KB. Experiments on\nfive heterogeneous KBQA datasets show that KB-Plugin achieves better or\ncomparable performance with 25$\\times$ smaller backbone LLM compared to SoTA PI\nmethods for low-resourced KBs, and even approaches the performance of\nsupervised methods. Our code and data are available at\nhttps://github.com/THU-KEG/KB-Plugin.\n", "rewritten_text": "Program induction (PI) offers a promising approach to leveraging knowledge bases (KBs) for answering complex questions with large language models (LLMs).  However, PI's reliance on extensive parallel question-program data limits its applicability to low-resource KBs lacking annotations.  To address this, we introduce KB-Plugin, a flexible framework enabling LLMs to generate programs for any KB, regardless of its size.  KB-Plugin uses self-supervised learning to encode KB schema into a \"schema plugin\" module and leverages data from a high-resource KB to train a \"PI plugin\" module. This PI plugin helps the LLM extract relevant schema information and generate programs.  Experiments across five diverse KBQA datasets demonstrate that KB-Plugin, using a 25 times smaller LLM, achieves comparable or superior performance to state-of-the-art PI methods on low-resource KBs, even approaching the performance of supervised methods.  Our code and data are available at https://github.com/THU-KEG/KB-Plugin.\n"}, "1812.01458": {"original_text": "  Recent advances in deep learning have shown exciting promise in filling large\nholes and lead to another orientation for image inpainting. However, existing\nlearning-based methods often create artifacts and fallacious textures because\nof insufficient cognition understanding. Previous generative networks are\nlimited with single receptive type and give up pooling in consideration of\ndetail sharpness. Human cognition is constant regardless of the target\nattribute. As multiple receptive fields improve the ability of abstract image\ncharacterization and pooling can keep feature invariant, specifically, deep\ninception learning is adopted to promote high-level feature representation and\nenhance model learning capacity for local patches. Moreover, approaches for\ngenerating diverse mask images are introduced and a random mask dataset is\ncreated. We benchmark our methods on ImageNet, Places2 dataset, and CelebA-HQ.\nExperiments for regular, irregular, and custom regions completion are all\nperformed and free-style image inpainting is also presented. Quantitative\ncomparisons with previous state-of-the-art methods show that ours obtain much\nmore natural image completions.\n", "rewritten_text": "Deep learning has significantly advanced image inpainting, but existing methods often produce artifacts due to limitations in understanding image context.  These limitations stem from using single receptive fields and avoiding pooling to preserve detail, neglecting the consistent nature of human visual perception.  To address this, we leverage deep inception learning to improve high-level feature representation and enhance local patch learning through multiple receptive fields and pooling.  We also introduce diverse mask generation techniques and a new random mask dataset.  Our method, evaluated on ImageNet, Places2, and CelebA-HQ datasets with various mask types (regular, irregular, custom), including free-style inpainting, demonstrates superior performance and significantly more natural image completions compared to state-of-the-art techniques.\n"}, "cs/0207005": {"original_text": "  We present a broad coverage Japanese grammar written in the HPSG formalism\nwith MRS semantics. The grammar is created for use in real world applications,\nsuch that robustness and performance issues play an important role. It is\nconnected to a POS tagging and word segmentation tool. This grammar is being\ndeveloped in a multilingual context, requiring MRS structures that are easily\ncomparable across languages.\n", "rewritten_text": "This paper introduces a comprehensive Japanese grammar, formalized using HPSG and annotated with MRS semantics, designed for robust real-world applications.  Its development prioritizes performance and integrates with a POS tagger and word segmenter.  Furthermore, its creation within a multilingual framework necessitates easily cross-linguistically comparable MRS structures.\n"}, "2307.10696": {"original_text": "  Improving the feature representation ability is the foundation of many whole\nslide pathological image (WSIs) tasks. Recent works have achieved great success\nin pathological-specific self-supervised learning (SSL). However, most of them\nonly focus on learning patch-level representations, thus there is still a gap\nbetween pretext and slide-level downstream tasks, e.g., subtyping, grading and\nstaging. Aiming towards slide-level representations, we propose Slide-Level\nPrototypical Distillation (SLPD) to explore intra- and inter-slide semantic\nstructures for context modeling on WSIs. Specifically, we iteratively perform\nintra-slide clustering for the regions (4096x4096 patches) within each WSI to\nyield the prototypes and encourage the region representations to be closer to\nthe assigned prototypes. By representing each slide with its prototypes, we\nfurther select similar slides by the set distance of prototypes and assign the\nregions by cross-slide prototypes for distillation. SLPD achieves\nstate-of-the-art results on multiple slide-level benchmarks and demonstrates\nthat representation learning of semantic structures of slides can make a\nsuitable proxy task for WSI analysis. Code will be available at\nhttps://github.com/Carboxy/SLPD.\n", "rewritten_text": "Effective whole slide image (WSI) analysis hinges on robust feature representation. While recent self-supervised learning (SSL) methods have shown promise in pathology, they primarily focus on patch-level features, limiting their applicability to slide-level tasks like subtyping, grading, and staging.  To address this, we introduce Slide-Level Prototypical Distillation (SLPD), a novel SSL approach that leverages intra- and inter-slide semantic structures.  SLPD iteratively clusters 4096x4096 patches within each WSI to generate slide-level prototypes, pulling region representations towards their assigned prototypes.  Similar slides are then identified based on prototype similarity, enabling cross-slide distillation.  SLPD achieves state-of-the-art performance on multiple slide-level benchmarks, demonstrating the effectiveness of learning slide-level semantic structures for WSI analysis.  Code is available at https://github.com/Carboxy/SLPD.\n"}, "2205.05869": {"original_text": "  We address the task of view synthesis, generating novel views of a scene\ngiven a set of images as input. In many recent works such as NeRF (Mildenhall\net al., 2020), the scene geometry is parameterized using neural implicit\nrepresentations (i.e., MLPs). Implicit neural representations have achieved\nimpressive visual quality but have drawbacks in computational efficiency. In\nthis work, we propose a new approach that performs view synthesis using point\nclouds. It is the first point-based method that achieves better visual quality\nthan NeRF while being 100x faster in rendering speed. Our approach builds on\nexisting works on differentiable point-based rendering but introduces a novel\ntechnique we call \"Sculpted Neural Points (SNP)\", which significantly improves\nthe robustness to errors and holes in the reconstructed point cloud. We further\npropose to use view-dependent point features based on spherical harmonics to\ncapture non-Lambertian surfaces, and new designs in the point-based rendering\npipeline that further boost the performance. Finally, we show that our system\nsupports fine-grained scene editing. Code is available at\nhttps://github.com/princeton-vl/SNP.\n", "rewritten_text": "This paper introduces a novel point-cloud-based approach to view synthesis, significantly outperforming neural radiance fields (NeRFs) in both speed and visual quality.  Unlike NeRF's reliance on computationally expensive neural implicit representations, our method leverages a new technique, Sculpted Neural Points (SNP), to robustly handle incomplete or noisy point clouds.  Further enhancements, including view-dependent spherical harmonic features and optimized rendering pipeline designs, achieve a 100x speedup over NeRF.  Our system also enables fine-grained scene editing.  Code is available at https://github.com/princeton-vl/SNP.\n"}, "1910.03484": {"original_text": "  In Natural Language Generation (NLG), End-to-End (E2E) systems trained\nthrough deep learning have recently gained a strong interest. Such deep models\nneed a large amount of carefully annotated data to reach satisfactory\nperformance. However, acquiring such datasets for every new NLG application is\na tedious and time-consuming task. In this paper, we propose a semi-supervised\ndeep learning scheme that can learn from non-annotated data and annotated data\nwhen available. It uses an NLG and a Natural Language Understanding (NLU)\nsequence-to-sequence models which are learned jointly to compensate for the\nlack of annotation. Experiments on two benchmark datasets show that, with\nlimited amount of annotated data, the method can achieve very competitive\nresults while not using any pre-processing or re-scoring tricks. These findings\nopen the way to the exploitation of non-annotated datasets which is the current\nbottleneck for the E2E NLG system development to new applications.\n", "rewritten_text": "Deep learning-based end-to-end (E2E) Natural Language Generation (NLG) systems are attracting significant attention, but require large, meticulously annotated datasets \u2013 a costly and time-consuming process for each new application.  This paper introduces a semi-supervised deep learning approach that leverages both annotated and unannotated data, employing jointly trained NLG and Natural Language Understanding (NLU) sequence-to-sequence models to mitigate annotation scarcity.  Experiments on two benchmark datasets demonstrate that this method achieves highly competitive results with limited annotated data, without relying on preprocessing or rescoring techniques. This breakthrough addresses the current bottleneck in E2E NLG development, enabling the exploitation of readily available unannotated datasets for new applications.\n"}, "2109.09883": {"original_text": "  Few-shot classification aims at classifying categories of a novel task by\nlearning from just a few (typically, 1 to 5) labelled examples. An effective\napproach to few-shot classification involves a prior model trained on a\nlarge-sample base domain, which is then finetuned over the novel few-shot task\nto yield generalizable representations. However, task-specific finetuning is\nprone to overfitting due to the lack of enough training examples. To alleviate\nthis issue, we propose a new finetuning approach based on contrastive learning\nthat reuses unlabelled examples from the base domain in the form of\ndistractors. Unlike the nature of unlabelled data used in prior works,\ndistractors belong to classes that do not overlap with the novel categories. We\ndemonstrate for the first time that inclusion of such distractors can\nsignificantly boost few-shot generalization. Our technical novelty includes a\nstochastic pairing of examples sharing the same category in the few-shot task\nand a weighting term that controls the relative influence of task-specific\nnegatives and distractors. An important aspect of our finetuning objective is\nthat it is agnostic to distractor labels and hence applicable to various base\ndomain settings. Compared to state-of-the-art approaches, our method shows\naccuracy gains of up to $12\\%$ in cross-domain and up to $5\\%$ in unsupervised\nprior-learning settings.\n", "rewritten_text": "Few-shot classification, which trains on only a handful of labeled examples (1-5), benefits from fine-tuning a pre-trained model.  However, this fine-tuning is susceptible to overfitting.  We address this by introducing a novel contrastive learning approach that incorporates unlabeled examples from the base domain as distractors \u2013 samples from classes *unrelated* to the novel task.  This is a departure from previous methods.  Our method, which stochastically pairs same-class examples and uses a weighted loss function to balance task-specific negatives and distractors, is label-agnostic regarding distractors, making it broadly applicable.  Experiments show significant improvements, achieving up to 12% higher accuracy in cross-domain scenarios and up to 5% in unsupervised pre-training settings compared to state-of-the-art methods.\n"}, "2003.12137": {"original_text": "  We explore novel approaches to the task of image generation from their\nrespective captions, building on state-of-the-art GAN architectures.\nParticularly, we baseline our models with the Attention-based GANs that learn\nattention mappings from words to image features. To better capture the features\nof the descriptions, we then built a novel cyclic design that learns an inverse\nfunction to maps the image back to original caption. Additionally, we\nincorporated recently developed BERT pretrained word embeddings as our initial\ntext featurizer and observe a noticeable improvement in qualitative and\nquantitative performance compared to the Attention GAN baseline.\n", "rewritten_text": "This work advances image generation from captions by improving upon state-of-the-art Attention-based GANs.  We introduce a novel cyclic architecture that learns to map images back to their captions, enhancing feature capture from descriptive text.  Furthermore, leveraging pre-trained BERT embeddings for text processing significantly improves both qualitative and quantitative results compared to our Attention GAN baseline.\n"}, "2003.02683": {"original_text": "  We introduce the first method for automatic image generation from scene-level\nfreehand sketches. Our model allows for controllable image generation by\nspecifying the synthesis goal via freehand sketches. The key contribution is an\nattribute vector bridged Generative Adversarial Network called EdgeGAN, which\nsupports high visual-quality object-level image content generation without\nusing freehand sketches as training data. We have built a large-scale composite\ndataset called SketchyCOCO to support and evaluate the solution. We validate\nour approach on the tasks of both object-level and scene-level image generation\non SketchyCOCO. Through quantitative, qualitative results, human evaluation and\nablation studies, we demonstrate the method's capacity to generate realistic\ncomplex scene-level images from various freehand sketches.\n", "rewritten_text": "This paper presents the first method for automatically generating images from freehand scene sketches.  Our novel EdgeGAN, an attribute vector-bridged Generative Adversarial Network, enables controllable image synthesis based on these sketches.  Unlike previous approaches, EdgeGAN generates high-quality object-level images without requiring freehand sketches during training.  We introduce SketchyCOCO, a large-scale dataset, to support and evaluate our method, demonstrating its effectiveness in generating realistic, complex scene-level images from diverse freehand sketches through quantitative and qualitative analysis, human evaluation, and ablation studies.\n"}, "2409.00942": {"original_text": "  Normalizing flows, a category of probabilistic models famed for their\ncapabilities in modeling complex data distributions, have exhibited remarkable\nefficacy in unsupervised anomaly detection. This paper explores the potential\nof normalizing flows in multi-class anomaly detection, wherein the normal data\nis compounded with multiple classes without providing class labels. Through the\nintegration of vector quantization (VQ), we empower the flow models to\ndistinguish different concepts of multi-class normal data in an unsupervised\nmanner, resulting in a novel flow-based unified method, named VQ-Flow.\nSpecifically, our VQ-Flow leverages hierarchical vector quantization to\nestimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for\nconcept distinction and its concomitant Concept-Specific Pattern Codebook\n(CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow\nare conditioned on the concept-specific patterns captured in CSPC, capable of\nmodeling specific normal patterns associated with different concepts. Moreover,\nCPC further enables our VQ-Flow for concept-aware distribution modeling,\nfaithfully mimicking the intricate multi-class normal distribution through a\nmixed Gaussian distribution reparametrized on the conceptual prototypes.\nThrough the introduction of vector quantization, the proposed VQ-Flow advances\nthe state-of-the-art in multi-class anomaly detection within a unified training\nscheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase\nis publicly available at https://github.com/cool-xuan/vqflow.\n", "rewritten_text": "Normalizing flows excel at modeling complex data distributions, making them effective for unsupervised anomaly detection.  This paper introduces VQ-Flow, a novel method for multi-class anomaly detection without class labels.  VQ-Flow uses hierarchical vector quantization to learn two codebooks: a Conceptual Prototype Codebook (CPC) to distinguish different normal data concepts, and a Concept-Specific Pattern Codebook (CSPC) to capture patterns within each concept.  Conditioned on the CSPC, the normalizing flows model the specific patterns of each concept.  The CPC allows VQ-Flow to model the overall multi-class distribution as a mixture of Gaussians centered on the conceptual prototypes.  This unified approach achieves state-of-the-art results (Det./Loc. AUROC of 99.5%/98.3% on MVTec AD) and is available at https://github.com/cool-xuan/vqflow.\n"}, "1708.00801": {"original_text": "  We study the impact of big models (in terms of the degree of lexicalization)\nand big data (in terms of the training corpus size) on dependency grammar\ninduction. We experimented with L-DMV, a lexicalized version of Dependency\nModel with Valence and L-NDMV, our lexicalized extension of the Neural\nDependency Model with Valence. We find that L-DMV only benefits from very small\ndegrees of lexicalization and moderate sizes of training corpora. L-NDMV can\nbenefit from big training data and lexicalization of greater degrees,\nespecially when enhanced with good model initialization, and it achieves a\nresult that is competitive with the current state-of-the-art.\n", "rewritten_text": "This study investigates how the size of language models (lexicalization) and training datasets affect dependency grammar induction.  Using L-DMV (a lexicalized Dependency Model with Valence) and L-NDMV (our lexicalized Neural Dependency Model with Valence), we found that L-DMV performs best with minimal lexicalization and moderately sized training data.  In contrast, L-NDMV benefits significantly from both large datasets and extensive lexicalization, particularly with effective model initialization, achieving state-of-the-art results.\n"}, "2303.1309": {"original_text": "  Recent trends in semi-supervised learning have significantly boosted the\nperformance of 3D semi-supervised medical image segmentation. Compared with 2D\nimages, 3D medical volumes involve information from different directions, e.g.,\ntransverse, sagittal, and coronal planes, so as to naturally provide\ncomplementary views. These complementary views and the intrinsic similarity\namong adjacent 3D slices inspire us to develop a novel annotation way and its\ncorresponding semi-supervised model for effective segmentation. Specifically,\nwe firstly propose the orthogonal annotation by only labeling two orthogonal\nslices in a labeled volume, which significantly relieves the burden of\nannotation. Then, we perform registration to obtain the initial pseudo labels\nfor sparsely labeled volumes. Subsequently, by introducing unlabeled volumes,\nwe propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that\nexploits dense pseudo labels in early stage and sparse labels in later stage\nand meanwhile forces consistent output of two networks. Experimental results on\nthree benchmark datasets validated our effectiveness in performance and\nefficiency in annotation. For example, with only 10 annotated slices, our\nmethod reaches a Dice up to 86.93% on KiTS19 dataset.\n", "rewritten_text": "Leveraging recent advances in semi-supervised learning, we present a novel approach to 3D medical image segmentation that significantly improves performance and reduces annotation effort.  Our method exploits the inherent multi-planar information (transverse, sagittal, coronal) within 3D volumes and the similarity between adjacent slices.  We introduce orthogonal annotation, requiring labels for only two orthogonal slices per volume, and use registration to generate initial pseudo-labels for sparsely annotated volumes.  A dual-network architecture, Dense-Sparse Co-training (DeSCO), then integrates these pseudo-labels with unlabeled data, enforcing consistent predictions between the two networks.  Evaluated on three benchmark datasets, our method achieves high accuracy\u2014for instance, reaching a Dice score of 86.93% on KiTS19 with only 10 annotated slices\u2014demonstrating both effectiveness and efficiency.\n"}, "2401.08123": {"original_text": "  Guided depth super-resolution (GDSR) involves restoring missing depth details\nusing the high-resolution RGB image of the same scene. Previous approaches have\nstruggled with the heterogeneity and complementarity of the multi-modal inputs,\nand neglected the issues of modal misalignment, geometrical misalignment, and\nfeature selection. In this study, we rethink some essential components in GDSR\nnetworks and propose a simple yet effective Dynamic Dual Alignment and\nAggregation network (D2A2). D2A2 mainly consists of 1) a dynamic dual alignment\nmodule that adapts to alleviate the modal misalignment via a learnable domain\nalignment block and geometrically align cross-modal features by learning the\noffset; and 2) a mask-to-pixel feature aggregate module that uses the gated\nmechanism and pixel attention to filter out irrelevant texture noise from RGB\nfeatures and combine the useful features with depth features. By combining the\nstrengths of RGB and depth features while minimizing disturbance introduced by\nthe RGB image, our method with simple reuse and redesign of basic components\nachieves state-of-the-art performance on multiple benchmark datasets. The code\nis available at https://github.com/JiangXinni/D2A2.\n", "rewritten_text": "This paper introduces Dynamic Dual Alignment and Aggregation (D2A2), a novel approach to guided depth super-resolution (GDSR).  Unlike previous methods that struggle with misalignment and irrelevant RGB information, D2A2 addresses these challenges directly.  It employs a dynamic dual alignment module to correct both modal and geometric misalignments between RGB and depth images, and a mask-to-pixel feature aggregation module to selectively integrate useful RGB features while suppressing noise.  This simple yet effective design achieves state-of-the-art results on multiple benchmark datasets.  The code is available at https://github.com/JiangXinni/D2A2.\n"}, "2112.09414": {"original_text": "  By highlighting the regions of the input image that contribute the most to\nthe decision, saliency maps have become a popular method to make neural\nnetworks interpretable. In medical imaging, they are particularly well-suited\nto explain neural networks in the context of abnormality localization. However,\nfrom our experiments, they are less suited to classification problems where the\nfeatures that allow to distinguish between the different classes are spatially\ncorrelated, scattered and definitely non-trivial. In this paper we thus propose\na new paradigm for better interpretability. To this end we provide the user\nwith relevant and easily interpretable information so that he can form his own\nopinion. We use Disentangled Variational Auto-Encoders which latent\nrepresentation is divided into two components: the non-interpretable part and\nthe disentangled part. The latter accounts for the categorical variables\nexplicitly representing the different classes of interest. In addition to\nproviding the class of a given input sample, such a model offers the\npossibility to transform the sample from a given class to a sample of another\nclass, by modifying the value of the categorical variables in the latent\nrepresentation. This paves the way to easier interpretation of class\ndifferences. We illustrate the relevance of this approach in the context of\nautomatic sex determination from hip bones in forensic medicine. The features\nencoded by the model, that distinguish the different classes were found to be\nconsistent with expert knowledge.\n", "rewritten_text": "Saliency maps, while popular for interpreting neural networks by highlighting influential image regions, are limited in classifying images with spatially dispersed and complex distinguishing features.  This paper introduces a novel approach for improved interpretability using Disentangled Variational Auto-Encoders (VAEs).  These VAEs separate latent representations into interpretable and non-interpretable components. The interpretable component explicitly represents class distinctions, enabling not only classification but also class-to-class sample transformations by manipulating latent variables.  This facilitates understanding class differences.  We demonstrate the method's effectiveness in forensic medicine, specifically automatic sex determination from hip bones, where the model's learned distinguishing features align with expert knowledge.\n"}, "1910.06431": {"original_text": "  There has been great success recently in tackling challenging NLP tasks by\nneural networks which have been pre-trained and fine-tuned on large amounts of\ntask data. In this paper, we investigate one such model, BERT for\nquestion-answering, with the aim to analyze why it is able to achieve\nsignificantly better results than other models. We run DeepLIFT on the model\npredictions and test the outcomes to monitor shift in the attention values for\ninput. We also cluster the results to analyze any possible patterns similar to\nhuman reasoning depending on the kind of input paragraph and question the model\nis trying to answer.\n", "rewritten_text": "Recent advancements in natural language processing (NLP) leverage pre-trained and fine-tuned neural networks, achieving remarkable success on complex tasks.  This paper analyzes BERT, a prominent question-answering model, to understand its superior performance.  We employ DeepLIFT to examine model predictions, tracking changes in attention weights and clustering results to identify patterns potentially analogous to human reasoning processes across various question-paragraph pairs.\n"}, "2410.15277": {"original_text": "  Retrieval-augmented generation (RAG) can supplement large language models\n(LLMs) by integrating external knowledge. However, as the number of retrieved\ndocuments increases, the input length to LLMs grows linearly, causing a\ndramatic increase in latency and a degradation in long-context understanding.\nThis is particularly serious for multi-hop questions that require a chain of\nreasoning across documents. To accelerate inference, reduce costs, and minimize\ndistractions, this paper presents BRIEF (Bridging Retrieval and Inference\nthrough Evidence Fusion), a lightweight approach that performs query-aware\nmulti-hop reasoning by compressing retrieved documents into highly dense\ntextual summaries to integrate into in-context learning. To enable learning\ncompression for multi-hop reasoning, we curate synthetic data by extracting\natomic proposition expressions that encapsulate distinct factoids from the\nsource documents to compose synthetic summaries. Based on our synthetic data\nbuilt entirely by open-source models, BRIEF generates more concise summaries\nand enables a range of LLMs to achieve exceptional open-domain question\nanswering (QA) performance. For example, on HotpotQA, BRIEF improves the\ncompression rate by 2 times compared to the state-of-the-art baseline, while\noutperforming it by 3.00% EM and 4.16% F1 with Flan-UL2 as the reader LM. It\nalso generates more concise summaries than proprietary GPT-3.5, while\ndemonstrating nearly identical QA performance.\n", "rewritten_text": "Retrieval-augmented generation (RAG) suffers from increased latency and reduced accuracy as the number of retrieved documents grows, especially for complex, multi-hop questions.  To address this, BRIEF (Bridging Retrieval and Inference through Evidence Fusion) compresses retrieved documents into concise summaries before feeding them to a large language model (LLM).  BRIEF uses synthetic data, generated from open-source models, to train its compression technique.  This results in significantly improved efficiency and accuracy.  On HotpotQA, BRIEF doubled the compression rate compared to the state-of-the-art, achieving a 3.00% EM and 4.16% F1 improvement with Flan-UL2, and matched the performance of GPT-3.5 with more concise summaries.\n"}, "2403.17512": {"original_text": "  Improving the efficiency of current neural networks and modeling them in\nbiological neural systems have become popular research directions in recent\nyears. Pulse-coupled neural network (PCNN) is a well applicated model for\nimitating the computation characteristics of the human brain in computer vision\nand neural network fields. However, differences between the PCNN and biological\nneural systems remain: limited neural connection, high computational cost, and\nlack of stochastic property. In this study, random-coupled neural network\n(RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic\ncomputing via a random inactivation process. This process randomly closes some\nneural connections in the RCNN model, realized by the random inactivation\nweight matrix of link input. This releases the computational burden of PCNN,\nmaking it affordable to achieve vast neural connections. Furthermore, the image\nand video processing mechanisms of RCNN are researched. It encodes constant\nstimuli as periodic spike trains and periodic stimuli as chaotic spike trains,\nthe same as biological neural information encoding characteristics. Finally,\nthe RCNN is applicated to image segmentation, fusion, and pulse shape\ndiscrimination subtasks. It is demonstrated to be robust, efficient, and highly\nanti-noised, with outstanding performance in all applications mentioned above.\n", "rewritten_text": "Recent research focuses on enhancing neural network efficiency and mirroring biological neural systems.  Pulse-coupled neural networks (PCNNs), while widely used in computer vision for mimicking brain computation, suffer from limitations: restricted connectivity, high computational cost, and a lack of stochasticity.  This study introduces the random-coupled neural network (RCNN), addressing these limitations through a random inactivation process.  This process, implemented via a random inactivation weight matrix, reduces computational burden, enabling large-scale connectivity.  The RCNN encodes constant stimuli as periodic spikes and periodic stimuli as chaotic spikes, mirroring biological encoding.  Applied to image segmentation, fusion, and pulse shape discrimination, the RCNN demonstrates robustness, efficiency, noise resistance, and superior performance across all tasks.\n"}, "1907.0916": {"original_text": "  Facial Micro-Expressions (MEs) are spontaneous, involuntary facial movements\nwhen a person experiences an emotion but deliberately or unconsciously attempts\nto conceal his or her genuine emotions. Recently, ME recognition has attracted\nincreasing attention due to its potential applications such as clinical\ndiagnosis, business negotiation, interrogations, and security. However, it is\nexpensive to build large scale ME datasets, mainly due to the difficulty of\ninducing spontaneous MEs. This limits the application of deep learning\ntechniques which require lots of training data. In this paper, we propose a\nsimple, efficient yet robust descriptor called Extended Local Binary Patterns\non Three Orthogonal Planes (ELBPTOP) for ME recognition. ELBPTOP consists of\nthree complementary binary descriptors: LBPTOP and two novel ones Radial\nDifference LBPTOP (RDLBPTOP) and Angular Difference LBPTOP (ADLBPTOP), which\nexplore the local second order information along the radial and angular\ndirections contained in ME video sequences. ELBPTOP is a novel ME descriptor\ninspired by unique and subtle facial movements. It is computationally efficient\nand only marginally increases the cost of computing LBPTOP, yet is extremely\neffective for ME recognition. In addition, by firstly introducing Whitened\nPrincipal Component Analysis (WPCA) to ME recognition, we can further obtain\nmore compact and discriminative feature representations, then achieve\nsignificantly computational savings. Extensive experimental evaluation on three\npopular spontaneous ME datasets SMIC, CASME II and SAMM show that our proposed\nELBPTOP approach significantly outperforms the previous state-of-the-art on all\nthree single evaluated datasets and achieves promising results on\ncross-database recognition.Our code will be made available.\n", "rewritten_text": "This paper introduces ELBPTOP, a novel, efficient, and robust descriptor for facial micro-expression (ME) recognition.  ELBPTOP combines three complementary binary descriptors (LBPTOP, RDLBPTOP, and ADLBPTOP) to capture subtle, second-order local information from ME video sequences.  Addressing the challenge of limited training data in ME recognition, ELBPTOP's computational efficiency allows its use with deep learning techniques, even with smaller datasets.  Further enhancing performance, we integrate Whitened Principal Component Analysis (WPCA) for more compact and discriminative feature representation.  Extensive experiments on SMIC, CASME II, and SAMM datasets demonstrate that ELBPTOP significantly surpasses existing state-of-the-art methods in both single-dataset and cross-database recognition.  The code will be publicly released.\n"}, "2402.11431": {"original_text": "  To address the issue of increased triangulation uncertainty caused by\nselecting views with small camera baselines in Structure from Motion (SFM) view\nselection, this paper proposes a robust error-resistant view selection method.\nThe method utilizes a triangulation-based computation to obtain an\nerror-resistant model, which is then used to construct an error-resistant\nmatrix. The sorting results of each row in the error-resistant matrix determine\nthe candidate view set for each view. By traversing the candidate view sets of\nall views and completing the missing views based on the error-resistant matrix,\nthe integrity of 3D reconstruction is ensured. Experimental comparisons between\nthis method and the exhaustive method with the highest accuracy in the COLMAP\nprogram are conducted in terms of average reprojection error and absolute\ntrajectory error in the reconstruction results. The proposed method\ndemonstrates an average reduction of 29.40% in reprojection error accuracy and\n5.07% in absolute trajectory error on the TUM dataset and DTU dataset.\n", "rewritten_text": "This paper presents a novel view selection method for Structure from Motion (SFM) that mitigates the increased triangulation uncertainty associated with short camera baselines.  The method employs a triangulation-based approach to create an error-resistant matrix, which guides the selection of optimal views.  This ensures 3D reconstruction integrity by identifying and addressing missing views.  Experiments on the TUM and DTU datasets, comparing the proposed method to COLMAP's most accurate exhaustive approach, show a significant improvement: a 29.40% average reduction in reprojection error and a 5.07% reduction in absolute trajectory error.\n"}, "2306.12693": {"original_text": "  This paper gives an Indic-to-Indic (IL-IL) MNMT baseline model for 11 ILs\nimplemented on the Samanantar corpus and analyzed on the Flores-200 corpus. All\nthe models are evaluated using the BLEU score. In addition, the languages are\nclassified under three groups namely East Indo- Aryan (EI), Dravidian (DR), and\nWest Indo-Aryan (WI). The effect of language relatedness on MNMT model\nefficiency is studied. Owing to the presence of large corpora from English (EN)\nto ILs, MNMT IL-IL models using EN as a pivot are also built and examined. To\nachieve this, English- Indic (EN-IL) models are also developed, with and\nwithout the usage of related languages. Results reveal that using related\nlanguages is beneficial for the WI group only, while it is detrimental for the\nEI group and shows an inconclusive effect on the DR group, but it is useful for\nEN-IL models. Thus, related language groups are used to develop pivot MNMT\nmodels. Furthermore, the IL corpora are transliterated from the corresponding\nscripts to a modified ITRANS script, and the best MNMT models from the previous\napproaches are built on the transliterated corpus. It is observed that the\nusage of pivot models greatly improves MNMT baselines with AS-TA achieving the\nminimum BLEU score and PA-HI achieving the maximum score. Among languages, AS,\nML, and TA achieve the lowest BLEU score, whereas HI, PA, and GU perform the\nbest. Transliteration also helps the models with few exceptions. The best\nincrement of scores is observed in ML, TA, and BN and the worst average\nincrement is observed in KN, HI, and PA, across all languages. The best model\nobtained is the PA-HI language pair trained on PAWI transliterated corpus which\ngives 24.29 BLEU.\n", "rewritten_text": "This study presents a baseline multilingual neural machine translation (MNMT) model for 11 Indic languages (ILs), trained on the Samanantar corpus and evaluated on Flores-200 using BLEU score.  The ILs were grouped into East Indo-Aryan (EI), Dravidian (DR), and West Indo-Aryan (WI) families to investigate the impact of language relatedness on model performance.  We also developed pivot MNMT models using English (EN) as an intermediary, leveraging existing large EN-IL corpora, both with and without related language data.  Results show that incorporating related languages benefits WI models but harms EI models, with inconclusive results for DR models; however, it proved beneficial for EN-IL models.  Consequently, related language data was used to build pivot MNMT models.  Additionally, we transliterated the IL corpora into a modified ITRANS script and retrained the best-performing models.  Pivot models significantly improved baseline performance, with Assamese-Tamil (AS-TA) achieving the lowest and Punjabi-Hindi (PA-HI) the highest BLEU score (24.29).  While Assamese, Malayalam, and Tamil showed the lowest overall scores, Hindi, Punjabi, and Gujarati performed best.  Transliteration generally improved results, with the largest gains seen in Malayalam, Tamil, and Bengali, and the smallest in Kannada, Hindi, and Punjabi.  The best overall model was the PA-HI pair trained on transliterated Punjabi-West Indo-Aryan data.\n"}, "2403.08487": {"original_text": "  Diffusion models pose risks of privacy breaches and copyright disputes,\nprimarily stemming from the potential utilization of unauthorized data during\nthe training phase. The Training Membership Inference (TMI) task aims to\ndetermine whether a specific sample has been used in the training process of a\ntarget model, representing a critical tool for privacy violation verification.\nHowever, the increased stochasticity inherent in diffusion renders traditional\nshadow-model-based or metric-based methods ineffective when applied to\ndiffusion models. Moreover, existing methods only yield binary classification\nlabels which lack necessary comprehensibility in practical applications. In\nthis paper, we explore a novel perspective for the TMI task by leveraging the\nintrinsic generative priors within the diffusion model. Compared with unseen\nsamples, training samples exhibit stronger generative priors within the\ndiffusion model, enabling the successful reconstruction of substantially\ndegraded training images. Consequently, we propose the Degrade Restore Compare\n(DRC) framework. In this framework, an image undergoes sequential degradation\nand restoration, and its membership is determined by comparing it with the\nrestored counterpart. Experimental results verify that our approach not only\nsignificantly outperforms existing methods in terms of accuracy but also\nprovides comprehensible decision criteria, offering evidence for potential\nprivacy violations.\n", "rewritten_text": "Diffusion models, while powerful, risk privacy breaches and copyright infringement due to the potential use of unauthorized training data.  Verifying these violations requires effective Training Membership Inference (TMI).  However, the inherent stochasticity of diffusion models renders existing TMI methods, typically based on shadow models or metrics, ineffective.  Furthermore, these methods only provide binary classifications, lacking practical interpretability.  This paper introduces a novel TMI approach leveraging the generative priors within diffusion models.  Training samples, unlike unseen samples, exhibit stronger generative priors, allowing for the reconstruction of heavily degraded versions.  Our proposed Degrade-Restore-Compare (DRC) framework sequentially degrades and restores an image, determining membership based on a comparison with the restored image.  Experiments demonstrate DRC's superior accuracy and provides interpretable results, offering clear evidence of potential privacy violations.\n"}, "2309.03390": {"original_text": "  In this paper, we seek a new method in designing an iris recognition system.\nIn this method, first the Haar wavelet features are extracted from iris images.\nThe advantage of using these features is the high-speed extraction, as well as\nbeing unique to each iris. Then the back propagation neural network (BPNN) is\nused as a classifier. In this system, the BPNN parallel algorithms and their\nimplementation on GPUs have been used by the aid of CUDA in order to speed up\nthe learning process. Finally, the system performance and the speeding outcomes\nin a way that this algorithm is done in series are presented.\n", "rewritten_text": "This paper proposes a novel iris recognition system.  It employs Haar wavelet features for their fast and unique extraction from iris images, followed by classification using a backpropagation neural network (BPNN).  To accelerate training, the BPNN utilizes parallel algorithms implemented on GPUs via CUDA.  The paper concludes by presenting the system's performance and speed improvements compared to a serial implementation.\n"}, "2311.10436": {"original_text": "  Since their inception, embeddings have become a primary ingredient in many\nflavours of Natural Language Processing (NLP) tasks supplanting earlier types\nof representation. Even though multilingual embeddings have been used for the\nincreasing number of multilingual tasks, due to the scarcity of parallel\ntraining data, low-resource languages such as Sinhala, tend to focus more on\nmonolingual embeddings. Then when it comes to the aforementioned multi-lingual\ntasks, it is challenging to utilize these monolingual embeddings given that\neven if the embedding spaces have a similar geometric arrangement due to an\nidentical training process, the embeddings of the languages considered are not\naligned. This is solved by the embedding alignment task. Even in this,\nhigh-resource language pairs are in the limelight while low-resource languages\nsuch as Sinhala which is in dire need of help seem to have fallen by the\nwayside. In this paper, we try to align Sinhala and English word embedding\nspaces based on available alignment techniques and introduce a benchmark for\nSinhala language embedding alignment. In addition to that, to facilitate the\nsupervised alignment, as an intermediate task, we also introduce\nSinhala-English alignment datasets. These datasets serve as our anchor datasets\nfor supervised word embedding alignment. Even though we do not obtain results\ncomparable to the high-resource languages such as French, German, or Chinese,\nwe believe our work lays the groundwork for more specialized alignment between\nEnglish and Sinhala embeddings.\n", "rewritten_text": "Word embeddings are now fundamental to Natural Language Processing (NLP), replacing earlier representation methods.  While multilingual embeddings are increasingly used, the scarcity of parallel training data limits their application to low-resource languages like Sinhala, which often rely on monolingual embeddings.  This poses a challenge for multilingual tasks because, even with similar training, monolingual embeddings aren't inherently aligned across languages.  Embedding alignment addresses this, but research predominantly focuses on high-resource language pairs, neglecting low-resource languages like Sinhala.  This paper addresses this gap by aligning Sinhala and English word embeddings using existing techniques, establishing a benchmark for Sinhala embedding alignment.  We also introduce Sinhala-English alignment datasets to facilitate supervised alignment.  Although our results don't match those achieved with high-resource languages, our work provides a foundation for future, more specialized English-Sinhala embedding alignment.\n"}, "2301.08245": {"original_text": "  Estimating depth from images nowadays yields outstanding results, both in\nterms of in-domain accuracy and generalization. However, we identify two main\nchallenges that remain open in this field: dealing with non-Lambertian\nmaterials and effectively processing high-resolution images. Purposely, we\npropose a novel dataset that includes accurate and dense ground-truth labels at\nhigh resolution, featuring scenes containing several specular and transparent\nsurfaces. Our acquisition pipeline leverages a novel deep space-time stereo\nframework, enabling easy and accurate labeling with sub-pixel precision. The\ndataset is composed of 606 samples collected in 85 different scenes, each\nsample includes both a high-resolution pair (12 Mpx) as well as an unbalanced\nstereo pair (Left: 12 Mpx, Right: 1.1 Mpx), typical of modern mobile devices\nthat mount sensors with different resolutions. Additionally, we provide\nmanually annotated material segmentation masks and 15K unlabeled samples. The\ndataset is composed of a train set and two test sets, the latter devoted to the\nevaluation of stereo and monocular depth estimation networks. Our experiments\nhighlight the open challenges and future research directions in this field.\n", "rewritten_text": "Modern image-based depth estimation achieves impressive accuracy and generalizability.  However, challenges remain in handling non-Lambertian surfaces (like specular and transparent materials) and processing high-resolution images.  To address this, we introduce a new dataset with accurate, dense, high-resolution depth ground truth.  Acquired using a novel deep space-time stereo framework, it provides sub-pixel precision labeling.  The dataset comprises 606 samples from 85 scenes, each including a high-resolution stereo pair (12 MP) and a low-resolution unbalanced pair (12 MP/1.1 MP), mimicking modern mobile device setups.  It also includes material segmentation masks and 15,000 unlabeled samples, split into training and two test sets (one for stereo, one for monocular depth estimation).  Our experiments reveal ongoing challenges and future research directions in depth estimation.\n"}, "1709.00813": {"original_text": "  Various text analysis techniques exist, which attempt to uncover unstructured\ninformation from text. In this work, we explore using statistical dependence\nmeasures for textual classification, representing text as word vectors. Student\nsatisfaction scores on a 3-point scale and their free text comments written\nabout university subjects are used as the dataset. We have compared two textual\nrepresentations: a frequency word representation and term frequency\nrelationship to word vectors, and found that word vectors provide a greater\naccuracy. However, these word vectors have a large number of features which\naggravates the burden of computational complexity. Thus, we explored using a\nnon-linear dependency measure for feature selection by maximizing the\ndependence between the text reviews and corresponding scores. Our quantitative\nand qualitative analysis on a student satisfaction dataset shows that our\napproach achieves comparable accuracy to the full feature vector, while being\nan order of magnitude faster in testing. These text analysis and feature\nreduction techniques can be used for other textual data applications such as\nsentiment analysis.\n", "rewritten_text": "This study investigates the use of statistical dependence measures for text classification, specifically applying them to student satisfaction data.  We represented text as word vectors, comparing a simple frequency-based approach with a more sophisticated term frequency-inverse document frequency (TF-IDF) word vector representation. While word vectors yielded higher accuracy, their dimensionality posed computational challenges.  To address this, we employed a non-linear dependency measure for feature selection, maximizing the relationship between student comments and satisfaction scores.  Our results demonstrate that this approach achieves accuracy comparable to using all features, but with a significant (order-of-magnitude) speed improvement during testing.  This methodology holds promise for other text analysis tasks, including sentiment analysis.\n"}, "1704.0803": {"original_text": "  Some lung diseases are related to bronchial airway structures and morphology.\nAlthough airway segmentation from chest CT volumes is an important task in the\ncomputer-aided diagnosis and surgery assistance systems for the chest, complete\n3-D airway structure segmentation is a quite challenging task due to its\ncomplex tree-like structure. In this paper, we propose a new airway\nsegmentation method from 3D chest CT volumes based on volume of interests (VOI)\nusing gradient vector flow (GVF). This method segments the bronchial regions by\napplying the cavity enhancement filter (CEF) to trace the bronchial tree\nstructure from the trachea. It uses the CEF in the VOI to segment each branch.\nAnd a tube-likeness function based on GVF and the GVF magnitude map in each VOI\nare utilized to assist predicting the positions and directions of child\nbranches. By calculating the tube-likeness function based on GVF and the GVF\nmagnitude map, the airway-like candidate structures are identified and their\ncentrelines are extracted. Based on the extracted centrelines, we can detect\nthe branch points of the bifurcations and directions of the airway branches in\nthe next level. At the same time, a leakage detection is performed to avoid the\nleakage by analysing the pixel information and the shape information of airway\ncandidate regions extracted in the VOI. Finally, we unify all of the extracted\nbronchial regions to form an integrated airway tree. Preliminary experiments\nusing four cases of chest CT volumes demonstrated that the proposed method can\nextract more bronchial branches in comparison with other methods.\n", "rewritten_text": "Accurate segmentation of the complex, tree-like bronchial airway structure in 3D chest CT scans is crucial for computer-aided diagnosis and surgery planning.  This paper presents a novel airway segmentation method employing volume of interest (VOI) analysis and gradient vector flow (GVF).  The method traces the bronchial tree from the trachea using a cavity enhancement filter (CEF), iteratively segmenting branches within each VOI.  A tube-likeness function, derived from GVF and its magnitude map, predicts branch positions and orientations.  This allows for identification of airway-like structures, centerline extraction, bifurcation point detection, and branch direction determination.  A leakage detection mechanism, using pixel and shape analysis, prevents segmentation errors.  Finally, the segmented bronchial regions are integrated into a complete airway tree.  Preliminary results on four chest CT scans show superior bronchial branch extraction compared to existing methods.\n"}, "1906.03249": {"original_text": "  Word embeddings are traditionally trained on a large corpus in an\nunsupervised setting, with no specific design for incorporating domain\nknowledge. This can lead to unsatisfactory performances when training data\noriginate from heterogeneous domains. In this paper, we propose two novel\nmechanisms for domain-aware word embedding training, namely domain indicator\nand domain attention, which integrate domain-specific knowledge into the widely\nused SG and CBOW models, respectively. The two methods are based on a joint\nlearning paradigm and ensure that words in a target domain are intensively\nfocused when trained on a source domain corpus. Qualitative and quantitative\nevaluation confirm the validity and effectiveness of our models. Compared to\nbaseline methods, our method is particularly effective in near-cold-start\nscenarios.\n", "rewritten_text": "Traditional word embedding methods, trained unsupervisedly on large corpora, lack domain awareness, hindering performance when data comes from diverse sources.  This paper introduces two novel domain-aware training mechanisms \u2013 domain indicator and domain attention \u2013 integrating domain knowledge into Skip-gram (SG) and Continuous Bag-of-Words (CBOW) models.  These methods, employing a joint learning approach, prioritize target domain words even when training on a source domain corpus.  Qualitative and quantitative results demonstrate their effectiveness, particularly in low-data (near-cold-start) scenarios, outperforming baseline methods.\n"}, "2305.17975": {"original_text": "  Automated assembly of 3D fractures is essential in orthopedics, archaeology,\nand our daily life. This paper presents Jigsaw, a novel framework for\nassembling physically broken 3D objects from multiple pieces. Our approach\nleverages hierarchical features of global and local geometry to match and align\nthe fracture surfaces. Our framework consists of four components: (1) front-end\npoint feature extractor with attention layers, (2) surface segmentation to\nseparate fracture and original parts, (3) multi-parts matching to find\ncorrespondences among fracture surface points, and (4) robust global alignment\nto recover the global poses of the pieces. We show how to jointly learn\nsegmentation and matching and seamlessly integrate feature matching and\nrigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and\nachieve superior performance compared to state-of-the-art methods. Our method\nalso generalizes well to diverse fracture modes, objects, and unseen instances.\nTo the best of our knowledge, this is the first learning-based method designed\nspecifically for 3D fracture assembly over multiple pieces. Our code is\navailable at https://jiaxin-lu.github.io/Jigsaw/.\n", "rewritten_text": "This paper introduces Jigsaw, a novel, learning-based framework for assembling broken 3D objects from multiple fragments.  Crucial in fields ranging from orthopedics and archaeology to everyday applications, Jigsaw uses a hierarchical approach, leveraging both global and local geometric features to match and align fractured surfaces.  The framework comprises four key components:  a point feature extractor with attention layers, fracture surface segmentation, multi-part matching, and robust global alignment.  Jigsaw jointly learns segmentation and matching, integrating feature matching with rigidity constraints.  Outperforming existing methods on the Breaking Bad dataset, Jigsaw demonstrates strong generalization across diverse fracture types, objects, and unseen examples.  This is the first learning-based approach specifically designed for multi-piece 3D fracture assembly; the code is publicly available at https://jiaxin-lu.github.io/Jigsaw/.\n"}, "2312.08367": {"original_text": "  In this work, we propose an efficient Video-Language Alignment (ViLA)\nnetwork. Our ViLA model addresses both efficient frame sampling and effective\ncross-modal alignment in a unified way. In our ViLA network, we design a new\nlearnable text-guided Frame-Prompter together with a new cross-modal\ndistillation (QFormer-Distiller) module. Pre-trained large image-language\nmodels have shown promising results on problems such as visual question\nanswering (VQA). However, how to efficiently and effectively sample video\nframes when adapting pre-trained large image-language model to video-language\nalignment is still the major challenge. Compared with prior work, our ViLA\nmodel demonstrates the capability of selecting key frames with critical\ncontents, thus improving the video-language alignment accuracy while reducing\nthe inference latency +3.3% on NExT-QA Temporal with 3.0X speed up). Overall,\nour ViLA network outperforms the state-of-the-art methods on the video\nquestion-answering benchmarks: +4.6% on STAR Interaction, +2.2% on STAR average\nwith 3.0X speed up, ours 2-frames out-perform SeViLA 4-frames on the VLEP\ndataset with 4.2X speed-up. The code will be available at\nhttps://github.com/xijun-cs/ViLA.\n", "rewritten_text": "This paper introduces ViLA, a novel and efficient Video-Language Alignment network.  ViLA simultaneously optimizes frame sampling and cross-modal alignment using a learnable text-guided Frame-Prompter and a QFormer-Distiller module.  Addressing the challenge of adapting large image-language models to video, ViLA efficiently selects key video frames, significantly improving video-language alignment accuracy and reducing inference time.  Compared to existing methods, ViLA achieves state-of-the-art performance on video question-answering benchmarks, including a 4.6% improvement on STAR Interaction, a 2.2% improvement on STAR average (both with 3x speedup), and superior performance to SeViLA on VLEP (with 4.2x speedup using only 2 frames versus SeViLA's 4).  The code is available at https://github.com/xijun-cs/ViLA.\n"}, "1809.10692": {"original_text": "  In this paper, we target the problem of fracture classification from clinical\nX-Ray images towards an automated Computer Aided Diagnosis (CAD) system.\nAlthough primarily dealing with an image classification problem, we argue that\nlocalizing the fracture in the image is crucial to make good class predictions.\nTherefore, we propose and thoroughly analyze several schemes for simultaneous\nfracture localization and classification. We show that using an auxiliary\nlocalization task, in general, improves the classification performance.\nMoreover, it is possible to avoid the need for additional localization\nannotations thanks to recent advancements in weakly-supervised deep learning\napproaches. Among such approaches, we investigate and adapt Spatial\nTransformers (ST), Self-Transfer Learning (STL), and localization from global\npooling layers. We provide a detailed quantitative and qualitative validation\non a dataset of 1347 femur fractures images and report high accuracy with\nregard to inter-expert correlation values reported in the literature. Our\ninvestigations show that i) lesion localization improves the classification\noutcome, ii) weakly-supervised methods improve baseline classification without\nany additional cost, iii) STL guides feature activations and boost performance.\nWe plan to make both the dataset and code available.\n", "rewritten_text": "This paper presents a novel approach to automated fracture classification in clinical X-ray images for Computer-Aided Diagnosis (CAD).  We demonstrate that accurate fracture localization significantly improves classification accuracy.  Therefore, we propose and evaluate several methods for simultaneous fracture localization and classification, leveraging weakly-supervised deep learning techniques such as Spatial Transformers (ST), Self-Transfer Learning (STL), and localization from global pooling layers.  These methods avoid the need for additional localization annotations.  Our results on a dataset of 1347 femur fracture images show high accuracy, comparable to inter-expert agreement reported in the literature.  We confirm that localization enhances classification, weakly-supervised methods improve performance cost-effectively, and STL specifically boosts performance by guiding feature activations.  The dataset and code will be publicly released.\n"}, "2309.16039": {"original_text": "  We present a series of long-context LLMs that support effective context\nwindows of up to 32,768 tokens. Our model series are built through continual\npretraining from Llama 2 with longer training sequences and on a dataset where\nlong texts are upsampled. We perform extensive evaluation on language modeling,\nsynthetic context probing tasks, and a wide range of research benchmarks. On\nresearch benchmarks, our models achieve consistent improvements on most regular\ntasks and significant improvements on long-context tasks over Llama 2. Notably,\nwith a cost-effective instruction tuning procedure that does not require\nhuman-annotated long instruction data, the 70B variant can already surpass\ngpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.\nAlongside these results, we provide an in-depth analysis on the individual\ncomponents of our method. We delve into Llama's position encodings and discuss\nits limitation in modeling long dependencies. We also examine the impact of\nvarious design choices in the pretraining process, including the data mix and\nthe training curriculum of sequence lengths -- our ablation experiments suggest\nthat having abundant long texts in the pretrain dataset is not the key to\nachieving strong performance, and we empirically verify that long context\ncontinual pretraining is more efficient and similarly effective compared to\npretraining from scratch with long sequences.\n", "rewritten_text": "This paper introduces a family of large language models (LLMs) with context windows up to 32,768 tokens, built by continually pre-training Llama 2 on a dataset oversampling long texts and using longer training sequences.  Extensive evaluation across language modeling, synthetic context probes, and various benchmarks demonstrates consistent improvements over Llama 2, particularly on long-context tasks.  Remarkably, a 70B parameter model, using a cost-effective instruction tuning method requiring no human-annotated long instructions, outperforms gpt-3.5-turbo-16k on a suite of long-context benchmarks.  The paper further analyzes the model's components, including Llama's positional encoding limitations and the impact of pretraining choices (data mix and sequence length curriculum).  Ablation studies reveal that abundant long texts during pretraining aren't crucial for strong performance, and that continual pre-training on long contexts is more efficient than training from scratch with long sequences.\n"}, "2403.02211": {"original_text": "  Popular methods usually use a degradation model in a supervised way to learn\na watermark removal model. However, it is true that reference images are\ndifficult to obtain in the real world, as well as collected images by cameras\nsuffer from noise. To overcome these drawbacks, we propose a perceptive\nself-supervised learning network for noisy image watermark removal (PSLNet) in\nthis paper. PSLNet depends on a parallel network to remove noise and\nwatermarks. The upper network uses task decomposition ideas to remove noise and\nwatermarks in sequence. The lower network utilizes the degradation model idea\nto simultaneously remove noise and watermarks. Specifically, mentioned paired\nwatermark images are obtained in a self supervised way, and paired noisy images\n(i.e., noisy and reference images) are obtained in a supervised way. To enhance\nthe clarity of obtained images, interacting two sub-networks and fusing\nobtained clean images are used to improve the effects of image watermark\nremoval in terms of structural information and pixel enhancement. Taking into\ntexture information account, a mixed loss uses obtained images and features to\nachieve a robust model of noisy image watermark removal. Comprehensive\nexperiments show that our proposed method is very effective in comparison with\npopular convolutional neural networks (CNNs) for noisy image watermark removal.\nCodes can be obtained at https://github.com/hellloxiaotian/PSLNet.\n", "rewritten_text": "Existing watermark removal methods typically rely on supervised learning with a degradation model, but require hard-to-obtain clean reference images.  To address this, we introduce PSLNet, a perceptive self-supervised learning network for removing watermarks from noisy images.  PSLNet employs a parallel architecture: an upper network sequentially removes noise and watermarks using task decomposition, while a lower network simultaneously removes both using a degradation model, leveraging self-supervised paired watermark images and supervised paired noisy images.  The outputs of these networks are fused to enhance image clarity, considering both structural and pixel-level information, and a mixed loss function incorporating texture information ensures robustness.  Extensive experiments demonstrate PSLNet's superior performance compared to existing CNN-based methods. Code is available at https://github.com/hellloxiaotian/PSLNet.\n"}, "2104.09386": {"original_text": "  Mapping a single exposure low dynamic range (LDR) image into a high dynamic\nrange (HDR) is considered among the most strenuous image to image translation\ntasks due to exposure-related missing information. This study tackles the\nchallenges of single-shot LDR to HDR mapping by proposing a novel two-stage\ndeep network. Notably, our proposed method aims to reconstruct an HDR image\nwithout knowing hardware information, including camera response function (CRF)\nand exposure settings. Therefore, we aim to perform image enhancement task like\ndenoising, exposure correction, etc., in the first stage. Additionally, the\nsecond stage of our deep network learns tone mapping and bit-expansion from a\nconvex set of data samples. The qualitative and quantitative comparisons\ndemonstrate that the proposed method can outperform the existing LDR to HDR\nworks with a marginal difference. Apart from that, we collected an LDR image\ndataset incorporating different camera systems. The evaluation with our\ncollected real-world LDR images illustrates that the proposed method can\nreconstruct plausible HDR images without presenting any visual artefacts. Code\navailable: https://github. com/sharif-apu/twostageHDR_NTIRE21.\n", "rewritten_text": "Converting a single low dynamic range (LDR) image to a high dynamic range (HDR) image is a challenging image translation problem because of missing exposure information.  This paper introduces a novel two-stage deep neural network to address this challenge.  Unlike many methods, ours requires no camera hardware information (e.g., camera response function, exposure settings). The first stage performs image enhancement (denoising, exposure correction, etc.), while the second stage learns tone mapping and bit-expansion from a convex dataset.  Our results show competitive performance compared to existing LDR-to-HDR methods, with minimal visual artifacts, as demonstrated on both synthetic and a newly collected real-world LDR image dataset.  Code is available at https://github.com/sharif-apu/twostageHDR_NTIRE21.\n"}, "2409.09170": {"original_text": "  The diagnosis and treatment of individuals with communication disorders\noffers many opportunities for the application of speech technology, but\nresearch so far has not adequately considered: the diversity of conditions, the\nrole of pragmatic deficits, and the challenges of limited data. This paper\nexplores how a general-purpose model of perceived pragmatic similarity may\novercome these limitations. It explains how it might support several use cases\nfor clinicians and clients, and presents evidence that a simple model can\nprovide value, and in particular can capture utterance aspects that are\nrelevant to diagnoses of autism and specific language impairment.\n", "rewritten_text": "Speech technology holds significant potential for diagnosing and treating communication disorders. However, current research overlooks the diversity of these disorders, the impact of pragmatic deficits, and data scarcity. This paper proposes a general-purpose model of perceived pragmatic similarity to address these limitations.  We demonstrate its potential utility for clinicians and clients across various applications, showing that even a simple model can effectively capture utterance features relevant to diagnosing autism and specific language impairment.\n"}, "2309.04462": {"original_text": "  Real-world application of chest X-ray abnormality classification requires\ndealing with several challenges: (i) limited training data; (ii) training and\nevaluation sets that are derived from different domains; and (iii) classes that\nappear during training may have partial overlap with classes of interest during\nevaluation. To address these challenges, we present an integrated framework\ncalled Generalized Cross-Domain Multi-Label Few-Shot Learning (GenCDML-FSL).\nThe framework supports overlap in classes during training and evaluation,\ncross-domain transfer, adopts meta-learning to learn using few training\nsamples, and assumes each chest X-ray image is either normal or associated with\none or more abnormalities. Furthermore, we propose Generalized Episodic\nTraining (GenET), a training strategy that equips models to operate with\nmultiple challenges observed in the GenCDML-FSL scenario. Comparisons with\nwell-established methods such as transfer learning, hybrid transfer learning,\nand multi-label meta-learning on multiple datasets show the superiority of our\napproach.\n", "rewritten_text": "Chest X-ray abnormality classification faces challenges including limited training data, domain discrepancies between training and evaluation sets, and class overlap.  To overcome these, we introduce GenCDML-FSL, a novel framework for generalized cross-domain multi-label few-shot learning.  GenCDML-FSL handles class overlap, facilitates cross-domain transfer, employs meta-learning for efficient learning from limited samples, and models chest X-rays as either normal or exhibiting one or more abnormalities.  Our proposed Generalized Episodic Training (GenET) strategy further enhances performance in this complex scenario.  Benchmarking against established methods (transfer learning, hybrid transfer learning, multi-label meta-learning) across multiple datasets demonstrates GenCDML-FSL's superior performance.\n"}, "1811.071": {"original_text": "  Few-shot deep learning is a topical challenge area for scaling visual\nrecognition to open ended growth of unseen new classes with limited labeled\nexamples. A promising approach is based on metric learning, which trains a deep\nembedding to support image similarity matching. Our insight is that effective\ngeneral purpose matching requires non-linear comparison of features at multiple\nabstraction levels. We thus propose a new deep comparison network comprised of\nembedding and relation modules that learn multiple non-linear distance metrics\nbased on different levels of features simultaneously. Furthermore, to reduce\nover-fitting and enable the use of deeper embeddings, we represent images as\ndistributions rather than vectors via learning parameterized Gaussian noise\nregularization. The resulting network achieves excellent performance on both\nminiImageNet and tieredImageNet.\n", "rewritten_text": "Scaling visual recognition to handle continuously emerging, unseen classes with limited training data (few-shot learning) is a significant challenge.  We address this by proposing a novel deep comparison network for metric learning.  Unlike methods relying on single-level feature comparisons, our network uses embedding and relation modules to learn multiple non-linear distance metrics across different feature abstraction levels.  To mitigate overfitting and allow for deeper embeddings, we represent images as distributions using parameterized Gaussian noise regularization.  Our approach demonstrates superior performance on miniImageNet and tieredImageNet benchmarks.\n"}, "2306.07279": {"original_text": "  We introduce Cap3D, an automatic approach for generating descriptive text for\n3D objects. This approach utilizes pretrained models from image captioning,\nimage-text alignment, and LLM to consolidate captions from multiple views of a\n3D asset, completely side-stepping the time-consuming and costly process of\nmanual annotation. We apply Cap3D to the recently introduced large-scale 3D\ndataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted\nusing 41k human annotations from the same dataset, demonstrates that Cap3D\nsurpasses human-authored descriptions in terms of quality, cost, and speed.\nThrough effective prompt engineering, Cap3D rivals human performance in\ngenerating geometric descriptions on 17k collected annotations from the ABO\ndataset. Finally, we finetune Text-to-3D models on Cap3D and human captions,\nand show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E,\nand DreamFusion.\n", "rewritten_text": "Cap3D is a novel automated system for generating descriptive text for 3D objects.  Leveraging pre-trained image captioning, image-text alignment, and large language models (LLMs), Cap3D efficiently creates captions from multiple viewpoints, eliminating the need for manual annotation.  Applied to the Objaverse dataset, Cap3D generated 660,000 3D-text pairs, exceeding human-authored descriptions in quality, cost, and speed, as validated by 41,000 human annotations.  Furthermore, Cap3D's geometric descriptions rival human performance on the ABO dataset (17k annotations).  Finally, fine-tuning Text-to-3D models with Cap3D's output outperforms models trained on human captions and surpasses state-of-the-art methods like Point-E, Shape-E, and DreamFusion.\n"}, "2311.08469": {"original_text": "  Language technologies that accurately model the dynamics of events must\nperform commonsense reasoning. Existing work evaluating commonsense reasoning\nfocuses on making inferences about common, everyday situations. To instead\ninvestigate the ability to model unusual, unexpected, and unlikely situations,\nwe explore the task of uncommonsense abductive reasoning. Given a piece of\ncontext with an unexpected outcome, this task requires reasoning abductively to\ngenerate an explanation that makes the unexpected outcome more likely in the\ncontext. To this end, we curate and release a new English language corpus\ncalled UNcommonsense. We characterize the performance differences between human\nexplainers and the best-performing large language models, finding that\nmodel-enhanced human-written explanations achieve the highest quality by\ntrading off between specificity and diversity. Finally, we experiment with\nseveral imitation learning algorithms to train open and accessible language\nmodels on this task. When compared with the vanilla supervised fine-tuning\napproach, these methods consistently reduce lose rates on both common and\nuncommonsense abductive reasoning judged by human evaluators.\n", "rewritten_text": "Accurate language models require commonsense reasoning, but current evaluations focus on typical scenarios.  To assess reasoning in unusual situations, we introduce the task of uncommonsense abductive reasoning: explaining unexpected outcomes.  We created a new corpus, UNcommonsense, to support this task and compared human and large language model (LLM) explanations, finding that human-LLM collaborations yield the highest quality.  Finally, we explored imitation learning methods for training LLMs on this task, demonstrating improved performance over standard supervised fine-tuning on both common and uncommonsense reasoning, as judged by human evaluation.\n"}, "2305.1374": {"original_text": "  Tense inconsistency frequently occurs in machine translation. However, there\nare few criteria to assess the model's mastery of tense prediction from a\nlinguistic perspective. In this paper, we present a parallel tense test set,\ncontaining French-English 552 utterances. We also introduce a corresponding\nbenchmark, tense prediction accuracy. With the tense test set and the\nbenchmark, researchers are able to measure the tense consistency performance of\nmachine translation systems for the first time.\n", "rewritten_text": "Machine translation often suffers from tense inconsistencies, but lacks robust linguistic evaluation metrics.  This paper introduces a new parallel French-English tense test set (552 utterances) and a corresponding benchmark, tense prediction accuracy. This allows researchers to quantitatively assess the tense consistency of machine translation systems for the first time.\n"}, "2410.19294": {"original_text": "  Vision-language models, such as CLIP, have shown impressive generalization\ncapacities when using appropriate text descriptions. While optimizing prompts\non downstream labeled data has proven effective in improving performance, these\nmethods entail labor costs for annotations and are limited by their quality.\nAdditionally, since CLIP is pre-trained on highly imbalanced Web-scale data, it\nsuffers from inherent label bias that leads to suboptimal performance. To\ntackle the above challenges, we propose a label-Free prompt distribution\nlearning and bias correction framework, dubbed as **Frolic**, which boosts\nzero-shot performance without the need for labeled data. Specifically, our\nFrolic learns distributions over prompt prototypes to capture diverse visual\nrepresentations and adaptively fuses these with the original CLIP through\nconfidence matching. This fused model is further enhanced by correcting label\nbias via a label-free logit adjustment. Notably, our method is not only\ntraining-free but also circumvents the necessity for hyper-parameter tuning.\nExtensive experimental results across 16 datasets demonstrate the efficacy of\nour approach, particularly outperforming the state-of-the-art by an average of\n$2.6\\%$ on 10 datasets with CLIP ViT-B/16 and achieving an average margin of\n$1.5\\%$ on ImageNet and its five distribution shifts with CLIP ViT-B/16. Codes\nare available in https://github.com/zhuhsingyuu/Frolic.\n", "rewritten_text": "Vision-language models like CLIP, despite their strong generalization abilities, suffer from limitations stemming from reliance on labeled data and inherent biases in their pre-training.  To address this, we introduce Frolic, a label-free framework for prompt distribution learning and bias correction.  Frolic enhances zero-shot performance by learning distributions over prompt prototypes to capture diverse visual representations, fusing them with CLIP via confidence matching, and correcting label bias through a label-free logit adjustment.  Importantly, Frolic requires no training or hyperparameter tuning.  Extensive experiments across 16 datasets show significant improvements, surpassing state-of-the-art results by an average of 2.6% on 10 datasets and 1.5% on ImageNet and its distribution shifts, using CLIP ViT-B/16.  Code is available at https://github.com/zhuhsingyuu/Frolic.\n"}, "1809.01219": {"original_text": "  A novel graph-to-tree conversion mechanism called the deep-tree generation\n(DTG) algorithm is first proposed to predict text data represented by graphs.\nThe DTG method can generate a richer and more accurate representation for nodes\n(or vertices) in graphs. It adds flexibility in exploring the vertex\nneighborhood information to better reflect the second order proximity and\nhomophily equivalence in a graph. Then, a Deep-Tree Recursive Neural Network\n(DTRNN) method is presented and used to classify vertices that contains text\ndata in graphs. To demonstrate the effectiveness of the DTRNN method, we apply\nit to three real-world graph datasets and show that the DTRNN method\noutperforms several state-of-the-art benchmarking methods.\n", "rewritten_text": "This paper introduces a novel graph-to-tree conversion algorithm, Deep-Tree Generation (DTG), for improved representation of text data within graphs.  DTG enriches node representations by more effectively capturing second-order proximity and homophily.  A Deep-Tree Recursive Neural Network (DTRNN) classifier, leveraging these enhanced representations, is then proposed and evaluated on three real-world datasets, demonstrating superior performance compared to existing state-of-the-art methods.\n"}, "2403.09636": {"original_text": "  Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.\n", "rewritten_text": "Large language models (LLMs) rely heavily on transformers, but their generation speed is hampered by the linearly scaling memory requirements of the key-value cache used to track past tokens.  To address this, we introduce Dynamic Memory Compression (DMC), an inference-time method for online compression of this cache.  Crucially, DMC learns to adjust compression ratios across different transformer heads and layers.  By retrofitting DMC into pre-trained LLMs like Llama 2 (7B, 13B, and 70B) \u2013 requiring only minimal continued pre-training on a small fraction of the original data and adding no new parameters \u2013 we achieve up to a 7x throughput increase on an NVIDIA H100 GPU during autoregressive inference.  Even with up to 4x cache compression, DMC maintains original performance, surpassing existing techniques like grouped-query attention (GQA) and key-value eviction methods (H\u2082O, TOVA).  Furthermore, DMC and GQA can be combined for even greater improvements.  Therefore, DMC offers a readily integrable solution for enhancing existing LLMs, enabling them to handle longer contexts and larger batches within existing memory constraints.\n"}, "2108.06536": {"original_text": "  We address the problem of generalized zero-shot semantic segmentation (GZS3)\npredicting pixel-wise semantic labels for seen and unseen classes. Most GZS3\nmethods adopt a generative approach that synthesizes visual features of unseen\nclasses from corresponding semantic ones (e.g., word2vec) to train novel\nclassifiers for both seen and unseen classes. Although generative methods show\ndecent performance, they have two limitations: (1) the visual features are\nbiased towards seen classes; (2) the classifier should be retrained whenever\nnovel unseen classes appear. We propose a discriminative approach to address\nthese limitations in a unified framework. To this end, we leverage visual and\nsemantic encoders to learn a joint embedding space, where the semantic encoder\ntransforms semantic features to semantic prototypes that act as centers for\nvisual features of corresponding classes. Specifically, we introduce\nboundary-aware regression (BAR) and semantic consistency (SC) losses to learn\ndiscriminative features. Our approach to exploiting the joint embedding space,\ntogether with BAR and SC terms, alleviates the seen bias problem. At test time,\nwe avoid the retraining process by exploiting semantic prototypes as a\nnearest-neighbor (NN) classifier. To further alleviate the bias problem, we\nalso propose an inference technique, dubbed Apollonius calibration (AC), that\nmodulates the decision boundary of the NN classifier to the Apollonius circle\nadaptively. Experimental results demonstrate the effectiveness of our\nframework, achieving a new state of the art on standard benchmarks.\n", "rewritten_text": "This paper tackles generalized zero-shot semantic segmentation (GZS3), aiming to predict pixel-level semantic labels for both known and unknown classes.  Existing generative GZS3 methods, which synthesize visual features for unseen classes from semantic embeddings (like word2vec), suffer from bias towards known classes and require retraining for new classes.  We introduce a novel discriminative approach that avoids these limitations.  Our method uses visual and semantic encoders to create a joint embedding space, where semantic features are mapped to prototypes acting as class centers.  Boundary-aware regression (BAR) and semantic consistency (SC) losses ensure discriminative feature learning.  At inference time, a nearest-neighbor classifier using these semantic prototypes eliminates the need for retraining.  Furthermore, our Apollonius calibration (AC) technique refines the classifier's decision boundary, mitigating bias.  Our approach achieves state-of-the-art results on standard benchmarks.\n"}, "1907.10815": {"original_text": "  Improvements in data-capture and face modeling techniques have enabled us to\ncreate high-fidelity realistic face models. However, driving these realistic\nface models requires special input data, e.g. 3D meshes and unwrapped textures.\nAlso, these face models expect clean input data taken under controlled lab\nenvironments, which is very different from data collected in the wild. All\nthese constraints make it challenging to use the high-fidelity models in\ntracking for commodity cameras. In this paper, we propose a self-supervised\ndomain adaptation approach to enable the animation of high-fidelity face models\nfrom a commodity camera. Our approach first circumvents the requirement for\nspecial input data by training a new network that can directly drive a face\nmodel just from a single 2D image. Then, we overcome the domain mismatch\nbetween lab and uncontrolled environments by performing self-supervised domain\nadaptation based on \"consecutive frame texture consistency\" based on the\nassumption that the appearance of the face is consistent over consecutive\nframes, avoiding the necessity of modeling the new environment such as lighting\nor background. Experiments show that we are able to drive a high-fidelity face\nmodel to perform complex facial motion from a cellphone camera without\nrequiring any labeled data from the new domain.\n", "rewritten_text": "High-fidelity face models, while realistic, demand specialized 3D data and controlled environments, hindering their use with standard cameras.  This paper presents a self-supervised domain adaptation method to animate these models using only a commodity camera's 2D input.  We address the data limitations by training a network to directly drive the model from single images, and mitigate the domain gap between lab and real-world conditions using consecutive frame texture consistency. This approach, requiring no labeled real-world data, enables complex facial animation from a cellphone camera.\n"}, "2311.10651": {"original_text": "  Analysis of the 3D Texture is indispensable for various tasks, such as\nretrieval, segmentation, classification, and inspection of sculptures, knitted\nfabrics, and biological tissues. A 3D texture is a locally repeated surface\nvariation independent of the surface's overall shape and can be determined\nusing the local neighborhood and its characteristics. Existing techniques\ntypically employ computer vision techniques that analyze a 3D mesh globally,\nderive features, and then utilize the obtained features for retrieval or\nclassification. Several traditional and learning-based methods exist in the\nliterature, however, only a few are on 3D texture, and nothing yet, to the best\nof our knowledge, on the unsupervised schemes. This paper presents an original\nframework for the unsupervised segmentation of the 3D texture on the mesh\nmanifold. We approach this problem as binary surface segmentation, partitioning\nthe mesh surface into textured and non-textured regions without prior\nannotation. We devise a mutual transformer-based system comprising a label\ngenerator and a cleaner. The two models take geometric image representations of\nthe surface mesh facets and label them as texture or non-texture across an\niterative mutual learning scheme. Extensive experiments on three publicly\navailable datasets with diverse texture patterns demonstrate that the proposed\nframework outperforms standard and SOTA unsupervised techniques and competes\nreasonably with supervised methods.\n", "rewritten_text": "Unsupervised 3D texture segmentation is crucial for applications involving sculptures, textiles, and biological tissues.  While existing methods often rely on global 3D mesh analysis and supervised learning,  this paper introduces a novel unsupervised framework.  Our approach uses a mutual transformer network \u2013 a label generator and a cleaner \u2013 operating iteratively on geometric image representations of the mesh facets to segment textured and non-textured regions without prior labeling.  Experiments on three public datasets show that our method surpasses existing unsupervised techniques and achieves competitive results compared to supervised approaches.\n"}, "1906.05416": {"original_text": "  We introduce a novel method of generating synthetic question answering\ncorpora by combining models of question generation and answer extraction, and\nby filtering the results to ensure roundtrip consistency. By pretraining on the\nresulting corpora we obtain significant improvements on SQuAD2 and NQ,\nestablishing a new state-of-the-art on the latter. Our synthetic data\ngeneration models, for both question generation and answer extraction, can be\nfully reproduced by finetuning a publicly available BERT model on the\nextractive subsets of SQuAD2 and NQ. We also describe a more powerful variant\nthat does full sequence-to-sequence pretraining for question generation,\nobtaining exact match and F1 at less than 0.1% and 0.4% from human performance\non SQuAD2.\n", "rewritten_text": "This paper presents a novel method for creating synthetic question-answering datasets.  We combine question generation and answer extraction models, filtering the output to ensure consistency.  Pretraining on this synthetic data significantly improves performance on SQuAD2 and NQ, achieving state-of-the-art results on NQ.  Our models are reproducible, using a publicly available BERT model fine-tuned on extractive subsets of SQuAD2 and NQ.  A more advanced sequence-to-sequence model for question generation achieves near-human performance on SQuAD2, with Exact Match and F1 scores within 0.1% and 0.4% of human performance, respectively.\n"}, "2404.02393": {"original_text": "  While multilingual machine translation (MNMT) systems hold substantial\npromise, they also have security vulnerabilities. Our research highlights that\nMNMT systems can be susceptible to a particularly devious style of backdoor\nattack, whereby an attacker injects poisoned data into a low-resource language\npair to cause malicious translations in other languages, including\nhigh-resource languages. Our experimental results reveal that injecting less\nthan 0.01% poisoned data into a low-resource language pair can achieve an\naverage 20% attack success rate in attacking high-resource language pairs. This\ntype of attack is of particular concern, given the larger attack surface of\nlanguages inherent to low-resource settings. Our aim is to bring attention to\nthese vulnerabilities within MNMT systems with the hope of encouraging the\ncommunity to address security concerns in machine translation, especially in\nthe context of low-resource languages.\n", "rewritten_text": "Multilingual machine translation (MNMT) systems, while promising, are vulnerable to sophisticated backdoor attacks.  Our research demonstrates that injecting a tiny amount (less than 0.01%) of poisoned data into a low-resource language pair can trigger malicious translations in high-resource languages, achieving a 20% success rate on average.  This highlights the significant security risk posed by the expanded attack surface inherent in low-resource language settings.  We aim to raise awareness of these vulnerabilities to encourage the development of more secure MNMT systems, particularly for low-resource languages.\n"}, "2104.00556": {"original_text": "  Two-view structure-from-motion (SfM) is the cornerstone of 3D reconstruction\nand visual SLAM. Existing deep learning-based approaches formulate the problem\nby either recovering absolute pose scales from two consecutive frames or\npredicting a depth map from a single image, both of which are ill-posed\nproblems. In contrast, we propose to revisit the problem of deep two-view SfM\nby leveraging the well-posedness of the classic pipeline. Our method consists\nof 1) an optical flow estimation network that predicts dense correspondences\nbetween two frames; 2) a normalized pose estimation module that computes\nrelative camera poses from the 2D optical flow correspondences, and 3) a\nscale-invariant depth estimation network that leverages epipolar geometry to\nreduce the search space, refine the dense correspondences, and estimate\nrelative depth maps. Extensive experiments show that our method outperforms all\nstate-of-the-art two-view SfM methods by a clear margin on KITTI depth, KITTI\nVO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth\nestimation.\n", "rewritten_text": "Deep learning approaches to two-view structure-from-motion (SfM) often struggle with ill-posed problems like absolute scale recovery or single-image depth prediction.  This work proposes a novel deep two-view SfM method that leverages the robustness of traditional pipelines.  Our approach comprises three stages:  dense optical flow estimation, normalized relative pose computation from the optical flow, and scale-invariant depth estimation using epipolar geometry to refine correspondences and reduce search space.  Extensive experiments on standard benchmarks (KITTI depth, KITTI VO, MVS, Scenes11, and SUN3D) demonstrate superior performance in both relative pose and depth estimation compared to existing state-of-the-art methods.\n"}, "2309.03903": {"original_text": "  Training data for video segmentation are expensive to annotate. This impedes\nextensions of end-to-end algorithms to new video segmentation tasks, especially\nin large-vocabulary settings. To 'track anything' without training on video\ndata for every individual task, we develop a decoupled video segmentation\napproach (DEVA), composed of task-specific image-level segmentation and\nclass/task-agnostic bi-directional temporal propagation. Due to this design, we\nonly need an image-level model for the target task (which is cheaper to train)\nand a universal temporal propagation model which is trained once and\ngeneralizes across tasks. To effectively combine these two modules, we use\nbi-directional propagation for (semi-)online fusion of segmentation hypotheses\nfrom different frames to generate a coherent segmentation. We show that this\ndecoupled formulation compares favorably to end-to-end approaches in several\ndata-scarce tasks including large-vocabulary video panoptic segmentation,\nopen-world video segmentation, referring video segmentation, and unsupervised\nvideo object segmentation. Code is available at:\nhttps://hkchengrex.github.io/Tracking-Anything-with-DEVA\n", "rewritten_text": "The high cost of annotating video segmentation training data limits the applicability of end-to-end methods, particularly for large-vocabulary tasks.  To address this, we introduce DEVA, a decoupled video segmentation approach. DEVA separates task-specific image-level segmentation from a universal, class-agnostic temporal propagation model. This design requires only a (cheaply trained) image-level model per task and a single, reusable temporal model.  Bi-directional propagation fuses segmentation hypotheses across frames, creating coherent segmentations.  DEVA outperforms end-to-end methods on several data-scarce tasks, including large-vocabulary panoptic, open-world, referring, and unsupervised video object segmentation.  Code is available at: https://hkchengrex.github.io/Tracking-Anything-with-DEVA\n"}, "2312.08869": {"original_text": "  We are living in a world surrounded by diverse and \"smart\" devices with rich\nmodalities of sensing ability. Conveniently capturing the interactions between\nus humans and these objects remains far-reaching. In this paper, we present\nI'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the\nhuman and object in a novel setting: using a minimal amount of RGB camera and\nobject-mounted Inertial Measurement Unit (IMU). It combines general motion\ninference and category-aware refinement. For the former, we introduce a\nholistic human-object tracking method to fuse the IMU signals and the RGB\nstream and progressively recover the human motions and subsequently the\ncompanion object motions. For the latter, we tailor a category-aware motion\ndiffusion model, which is conditioned on both the raw IMU observations and the\nresults from the previous stage under over-parameterization representation. It\nsignificantly refines the initial results and generates vivid body, hand, and\nobject motions. Moreover, we contribute a large dataset with ground truth human\nand object motions, dense RGB inputs, and rich object-mounted IMU measurements.\nExtensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid\ncapture setting. Our dataset and code will be released to the community.\n", "rewritten_text": "This paper introduces I'm-HOI, a novel monocular system for accurately capturing 3D human-object interaction (HOI).  Using only a minimal setup\u2014an RGB camera and an object-mounted IMU\u2014I'm-HOI combines general motion inference with category-aware refinement.  A holistic tracking method fuses IMU and RGB data to progressively recover human and object motions.  This is followed by a category-aware motion diffusion model, conditioned on IMU data and initial motion estimates, to refine the results and generate realistic 3D motions.  We also contribute a large-scale dataset with ground truth motion, RGB data, and IMU measurements.  Extensive experiments validate I'm-HOI's effectiveness, and our code and dataset are publicly available.\n"}, "1810.04864": {"original_text": "  We present a comparison of word-based and character-based\nsequence-to-sequence models for data-to-text natural language generation, which\ngenerate natural language descriptions for structured inputs. On the datasets\nof two recent generation challenges, our models achieve comparable or better\nautomatic evaluation results than the best challenge submissions. Subsequent\ndetailed statistical and human analyses shed light on the differences between\nthe two input representations and the diversity of the generated texts. In a\ncontrolled experiment with synthetic training data generated from templates, we\ndemonstrate the ability of neural models to learn novel combinations of the\ntemplates and thereby generalize beyond the linguistic structures they were\ntrained on.\n", "rewritten_text": "This paper compares word-based and character-based sequence-to-sequence models for data-to-text generation.  Our models, evaluated on two recent challenge datasets, achieved performance comparable to or exceeding the best submissions.  Further statistical and human evaluations reveal the impact of input representation on text diversity.  Finally, experiments using synthetic template data demonstrate the models' ability to generalize beyond their training data by learning novel template combinations.\n"}, "2405.16116": {"original_text": "  Scene Graph Generation (SGG) can extract abstract semantic relations between\nentities in images as graph representations. This task holds strong promises\nfor other downstream tasks such as the embodied cognition of an autonomous\nagent. However, to power such applications, SGG needs to solve the gap of\nreal-time latency. In this work, we propose to investigate the bottlenecks of\ncurrent approaches for real-time constraint applications. Then, we propose a\nsimple yet effective implementation of a real-time SGG approach using YOLOV8 as\nan object detection backbone. Our implementation is the first to obtain more\nthan 48 FPS for the task with no loss of accuracy, successfully outperforming\nany other lightweight approaches. Our code is freely available at\nhttps://github.com/Maelic/SGG-Benchmark.\n", "rewritten_text": "This paper addresses the real-time performance limitations of Scene Graph Generation (SGG), a crucial task for applications like autonomous agent embodiment.  We analyze current SGG bottlenecks and present a novel, fast and accurate approach leveraging YOLOv8 for object detection.  Our implementation achieves over 48 FPS without sacrificing accuracy, surpassing all existing lightweight SGG methods.  The code is publicly available at https://github.com/Maelic/SGG-Benchmark.\n"}, "2012.08549": {"original_text": "  Voice Assistants such as Alexa, Siri, and Google Assistant typically use a\ntwo-stage Spoken Language Understanding pipeline; first, an Automatic Speech\nRecognition (ASR) component to process customer speech and generate text\ntranscriptions, followed by a Natural Language Understanding (NLU) component to\nmap transcriptions to an actionable hypothesis. An end-to-end (E2E) system that\ngoes directly from speech to a hypothesis is a more attractive option. These\nsystems were shown to be smaller, faster, and better optimized. However, they\nrequire massive amounts of end-to-end training data and in addition, don't take\nadvantage of the already available ASR and NLU training data.\n  In this work, we propose an E2E system that is designed to jointly train on\nmultiple speech-to-text tasks, such as ASR (speech-transcription) and SLU\n(speech-hypothesis), and text-to-text tasks, such as NLU (text-hypothesis). We\ncall this the Audio-Text All-Task (AT-AT) Model and we show that it beats the\nperformance of E2E models trained on individual tasks, especially ones trained\non limited data. We show this result on an internal music dataset and two\npublic datasets, FluentSpeech and SNIPS Audio, where we achieve\nstate-of-the-art results. Since our model can process both speech and text\ninput sequences and learn to predict a target sequence, it also allows us to do\nzero-shot E2E SLU by training on only text-hypothesis data (without any speech)\nfrom a new domain. We evaluate this ability of our model on the Facebook TOP\ndataset and set a new benchmark for zeroshot E2E performance. We will soon\nrelease the audio data collected for the TOP dataset for future research.\n", "rewritten_text": "Current voice assistants use a two-stage process: automatic speech recognition (ASR) followed by natural language understanding (NLU).  While end-to-end (E2E) systems, which directly map speech to action, offer advantages in size, speed, and optimization, they require vast amounts of training data and don't leverage existing ASR and NLU resources.  This paper introduces the Audio-Text All-Task (AT-AT) model, an E2E system that jointly trains on speech-to-text (ASR and speech-level understanding (SLU)) and text-to-text (NLU) tasks.  AT-AT outperforms single-task E2E models, particularly with limited data, achieving state-of-the-art results on internal and public datasets (FluentSpeech, SNIPS Audio).  Furthermore, its ability to process both speech and text enables zero-shot E2E SLU, demonstrated by setting a new benchmark on the Facebook TOP dataset using only text-based training data.  The TOP dataset's audio component will be publicly released to facilitate future research.\n"}, "2207.09059": {"original_text": "  Few-shot open-set recognition aims to classify both seen and novel images\ngiven only limited training data of seen classes. The challenge of this task is\nthat the model is required not only to learn a discriminative classifier to\nclassify the pre-defined classes with few training data but also to reject\ninputs from unseen classes that never appear at training time. In this paper,\nwe propose to solve the problem from two novel aspects. First, instead of\nlearning the decision boundaries between seen classes, as is done in standard\nclose-set classification, we reserve space for unseen classes, such that images\nlocated in these areas are recognized as the unseen classes. Second, to\neffectively learn such decision boundaries, we propose to utilize the\nbackground features from seen classes. As these background regions do not\nsignificantly contribute to the decision of close-set classification, it is\nnatural to use them as the pseudo unseen classes for classifier learning. Our\nextensive experiments show that our proposed method not only outperforms\nmultiple baselines but also sets new state-of-the-art results on three popular\nbenchmarks, namely tieredImageNet, miniImageNet, and Caltech-USCD\nBirds-200-2011 (CUB).\n", "rewritten_text": "This paper addresses the challenge of few-shot open-set recognition, where a model must classify known and unknown images with limited training data.  Unlike standard closed-set classification, our approach explicitly reserves decision space for unseen classes, identifying unknown images as belonging to this space.  Furthermore, we leverage background features from known classes \u2013 typically ignored in closed-set classification \u2013 as pseudo-unseen class examples to improve the model's ability to distinguish between known and unknown images.  Our method achieves state-of-the-art performance on tieredImageNet, miniImageNet, and CUB-200-2011 benchmarks.\n"}, "1710.04943": {"original_text": "  In this paper, we report on our efforts for using Deep Learning for\nclassifying artifacts and their features in digital visuals as a part of the\nNeoclassica framework. It was conceived to provide scholars with new methods\nfor analyzing and classifying artifacts and aesthetic forms from the era of\nClassicism. The framework accommodates both traditional knowledge\nrepresentation as a formal ontology and data-driven knowledge discovery, where\ncultural patterns will be identified by means of algorithms in statistical\nanalysis and machine learning. We created a Deep Learning approach trained on\nphotographs to classify the objects inside these photographs. In a next step,\nwe will apply a different Deep Learning approach. It is capable of locating\nmultiple objects inside an image and classifying them with a high accuracy.\n", "rewritten_text": "This paper details the application of deep learning within the Neoclassica framework to classify artifacts and their features in digital images.  Neoclassica aims to provide scholars with novel methods for analyzing Classical-era artifacts and aesthetics.  The framework integrates traditional ontologies with data-driven discovery, using machine learning algorithms to identify cultural patterns.  We developed a deep learning model trained on photographs to classify depicted objects.  Future work will involve a more advanced model capable of accurately identifying and classifying multiple objects within a single image.\n"}, "2111.07239": {"original_text": "  Object detection has achieved promising performance on clean datasets, but\nhow to achieve better tradeoff between the adversarial robustness and clean\nprecision is still under-explored. Adversarial training is the mainstream\nmethod to improve robustness, but most of the works will sacrifice clean\nprecision to gain robustness than standard training. In this paper, we propose\nUnified Decoupled Feature Alignment (UDFA), a novel fine-tuning paradigm which\nachieves better performance than existing methods, by fully exploring the\ncombination between self-knowledge distillation and adversarial training for\nobject detection. We first use decoupled fore/back-ground features to construct\nself-knowledge distillation branch between clean feature representation from\npretrained detector (served as teacher) and adversarial feature representation\nfrom student detector. Then we explore the self-knowledge distillation from a\nnew angle by decoupling original branch into a self-supervised learning branch\nand a new self-knowledge distillation branch. With extensive experiments on the\nPASCAL-VOC and MS-COCO benchmarks, the evaluation results show that UDFA can\nsurpass the standard training and state-of-the-art adversarial training methods\nfor object detection. For example, compared with teacher detector, our approach\non GFLV2 with ResNet-50 improves clean precision by 2.2 AP on PASCAL-VOC;\ncompared with SOTA adversarial training methods, our approach improves clean\nprecision by 1.6 AP, while improving adversarial robustness by 0.5 AP. Our code\nwill be available at https://github.com/grispeut/udfa.\n", "rewritten_text": "Object detectors perform well on clean data, but balancing robustness against adversarial attacks with clean accuracy remains a challenge.  While adversarial training is the common approach to enhance robustness, it often reduces clean accuracy. This paper introduces Unified Decoupled Feature Alignment (UDFA), a novel fine-tuning method that leverages self-knowledge distillation and adversarial training to improve object detection performance.  UDFA uses decoupled foreground/background features to create a self-knowledge distillation branch between a pretrained detector (teacher) and a student detector trained with adversarial examples.  Further enhancing this, UDFA decouples the original branch into self-supervised and self-knowledge distillation branches.  Extensive experiments on PASCAL-VOC and MS-COCO demonstrate that UDFA outperforms standard and state-of-the-art adversarial training methods.  Specifically, on PASCAL-VOC using GFLV2 with ResNet-50, UDFA improves clean precision by 2.2 AP over the teacher detector and by 1.6 AP over existing adversarial training methods, while simultaneously improving adversarial robustness by 0.5 AP.  Code is available at https://github.com/grispeut/udfa.\n"}, "2304.05694": {"original_text": "  Self-attention modules have demonstrated remarkable capabilities in capturing\nlong-range relationships and improving the performance of point cloud tasks.\nHowever, point cloud objects are typically characterized by complex,\ndisordered, and non-Euclidean spatial structures with multiple scales, and\ntheir behavior is often dynamic and unpredictable. The current self-attention\nmodules mostly rely on dot product multiplication and dimension alignment among\nquery-key-value features, which cannot adequately capture the multi-scale\nnon-Euclidean structures of point cloud objects. To address these problems,\nthis paper proposes a self-attention plug-in module with its variants,\nMulti-scale Geometry-aware Transformer (MGT). MGT processes point cloud data\nwith multi-scale local and global geometric information in the following three\naspects. At first, the MGT divides point cloud data into patches with multiple\nscales. Secondly, a local feature extractor based on sphere mapping is proposed\nto explore the geometry inner each patch and generate a fixed-length\nrepresentation for each patch. Thirdly, the fixed-length representations are\nfed into a novel geodesic-based self-attention to capture the global\nnon-Euclidean geometry between patches. Finally, all the modules are integrated\ninto the framework of MGT with an end-to-end training scheme. Experimental\nresults demonstrate that the MGT vastly increases the capability of capturing\nmulti-scale geometry using the self-attention mechanism and achieves strong\ncompetitive performance on mainstream point cloud benchmarks.\n", "rewritten_text": "Self-attention mechanisms excel at capturing long-range dependencies in point cloud data, but struggle with the inherent complexities of these data:  their multi-scale, disordered, non-Euclidean structure and dynamic nature.  Existing self-attention methods, relying on dot products, are insufficient for this task.  This paper introduces the Multi-scale Geometry-aware Transformer (MGT), a novel self-attention module and its variants.  MGT leverages multi-scale geometric information by: 1) partitioning the point cloud into multi-scale patches; 2) employing a sphere-mapping-based local feature extractor to generate fixed-length patch representations; and 3) utilizing a geodesic-based self-attention mechanism to capture global non-Euclidean relationships between patches.  This end-to-end trainable framework significantly improves the capture of multi-scale geometry within a self-attention context, achieving state-of-the-art performance on standard point cloud benchmarks.\n"}, "2108.07708": {"original_text": "  Can language models learn grounded representations from text distribution\nalone? This question is both central and recurrent in natural language\nprocessing; authors generally agree that grounding requires more than textual\ndistribution. We propose to experimentally test this claim: if any two words\nhave different meanings and yet cannot be distinguished from distribution\nalone, then grounding is out of the reach of text-based models. To that end, we\npresent early work on an online game for the collection of human judgments on\nthe distributional similarity of word pairs in five languages. We further\nreport early results of our data collection campaign.\n", "rewritten_text": "The ability of language models to learn meaningful representations solely from text data is a fundamental and ongoing question in NLP.  While consensus suggests that contextual grounding requires more than just textual distribution, we empirically investigate this.  Our hypothesis is that if distributional analysis cannot differentiate between words with distinct meanings, then text-based models cannot achieve true grounding.  This paper presents preliminary findings from an online game designed to collect human judgments on the distributional similarity of word pairs across five languages, along with early results from our data collection effort.\n"}, "2302.03194": {"original_text": "  We propose two methods to make unsupervised domain adaptation (UDA) more\nparameter efficient using adapters, small bottleneck layers interspersed with\nevery layer of the large-scale pre-trained language model (PLM). The first\nmethod deconstructs UDA into a two-step process: first by adding a domain\nadapter to learn domain-invariant information and then by adding a task adapter\nthat uses domain-invariant information to learn task representations in the\nsource domain. The second method jointly learns a supervised classifier while\nreducing the divergence measure. Compared to strong baselines, our simple\nmethods perform well in natural language inference (MNLI) and the cross-domain\nsentiment classification task. We even outperform unsupervised domain\nadaptation methods such as DANN and DSN in sentiment classification, and we are\nwithin 0.85% F1 for natural language inference task, by fine-tuning only a\nfraction of the full model parameters. We release our code at\nhttps://github.com/declare-lab/domadapter\n", "rewritten_text": "This paper introduces two novel, parameter-efficient methods for unsupervised domain adaptation (UDA) in large-scale pre-trained language models (PLMs).  Both methods leverage small \"adapter\" layers inserted between existing PLM layers.  The first method sequentially learns domain-invariant features with a domain adapter, followed by task-specific representations with a task adapter. The second method simultaneously trains a supervised classifier while minimizing domain divergence.  Experiments on natural language inference (MNLI) and cross-domain sentiment classification demonstrate strong performance, exceeding or closely matching (within 0.85% F1 score on MNLI) state-of-the-art UDA methods like DANN and DSN, while only fine-tuning a small fraction of the model's parameters.  Code is available at https://github.com/declare-lab/domadapter.\n"}, "2103.04059": {"original_text": "  Few-shot class incremental learning (FSCIL) portrays the problem of learning\nnew concepts gradually, where only a few examples per concept are available to\nthe learner. Due to the limited number of examples for training, the techniques\ndeveloped for standard incremental learning cannot be applied verbatim to\nFSCIL. In this work, we introduce a distillation algorithm to address the\nproblem of FSCIL and propose to make use of semantic information during\ntraining. To this end, we make use of word embeddings as semantic information\nwhich is cheap to obtain and which facilitate the distillation process.\nFurthermore, we propose a method based on an attention mechanism on multiple\nparallel embeddings of visual data to align visual and semantic vectors, which\nreduces issues related to catastrophic forgetting. Via experiments on\nMiniImageNet, CUB200, and CIFAR100 dataset, we establish new state-of-the-art\nresults by outperforming existing approaches.\n", "rewritten_text": "This paper addresses few-shot class incremental learning (FSCIL), a challenging scenario where new classes are learned sequentially with limited training examples per class.  Standard incremental learning techniques are insufficient for FSCIL due to this data scarcity.  We propose a novel distillation algorithm leveraging readily available word embeddings as semantic information to improve learning.  This approach incorporates an attention mechanism to align visual and semantic representations, mitigating catastrophic forgetting.  Experiments on MiniImageNet, CUB200, and CIFAR100 datasets demonstrate state-of-the-art performance.\n"}, "1710.07395": {"original_text": "  In the wake of a polarizing election, the cyber world is laden with hate\nspeech. Context accompanying a hate speech text is useful for identifying hate\nspeech, which however has been largely overlooked in existing datasets and hate\nspeech detection models. In this paper, we provide an annotated corpus of hate\nspeech with context information well kept. Then we propose two types of hate\nspeech detection models that incorporate context information, a logistic\nregression model with context features and a neural network model with learning\ncomponents for context. Our evaluation shows that both models outperform a\nstrong baseline by around 3% to 4% in F1 score and combining these two models\nfurther improve the performance by another 7% in F1 score.\n", "rewritten_text": "Following a divisive election, online hate speech proliferated.  Existing hate speech detection datasets and models largely ignore the crucial contextual information surrounding such speech. This paper introduces a new annotated corpus of hate speech including rich contextual data. We then propose two novel context-aware hate speech detection models: a logistic regression model using explicit context features and a neural network model learning context implicitly.  Our evaluation demonstrates that both models surpass a strong baseline by 3-4% F1 score, with a further 7% F1 score improvement achieved by combining them.\n"}, "2401.17597": {"original_text": "  Multi-turn dialogues are characterized by their extended length and the\npresence of turn-taking conversations. Traditional language models often\noverlook the distinct features of these dialogues by treating them as regular\ntext. In this paper, we propose a speaker-enhanced pre-training method for long\ndialogue summarization, which leverages the inherent structure of multiple-turn\ndialogues. To support our study, we curate a diverse dataset that includes\ntranscripts from real-world scenarios, movie or TV show transcripts, and\ndialogues generated by a Large Language Model. We then perform a pre-training,\nwhich encompasses the detection of speaker changes, and masked utterance\ngeneration. Experimental results of fine-tuned models demonstrate that our\nmodel achieves state-of-the-art performance on downstream benchmarks with long\ncontext, surpassing baseline models and highlighting the effectiveness of our\napproach. Our findings highlight the importance of curating pre-training\ndatasets that exhibit diversity and variations in length distribution to ensure\neffective alignment with downstream datasets.\n", "rewritten_text": "This paper introduces a novel speaker-enhanced pre-training method for long dialogue summarization.  Unlike traditional methods that treat multi-turn dialogues as standard text, our approach leverages the inherent turn-taking structure.  We trained our model on a diverse dataset comprising real-world transcripts, movie/TV dialogue, and large language model-generated conversations.  The pre-training incorporates speaker change detection and masked utterance generation.  Our fine-tuned model achieves state-of-the-art results on long-context benchmarks, outperforming baselines and demonstrating the importance of diverse, length-varied pre-training data for effective long dialogue summarization.\n"}, "2309.01036": {"original_text": "  Spatial transcriptomics is an emerging technology that aligns histopathology\nimages with spatially resolved gene expression profiling. It holds the\npotential for understanding many diseases but faces significant bottlenecks\nsuch as specialized equipment and domain expertise. In this work, we present\nSEPAL, a new model for predicting genetic profiles from visual tissue\nappearance. Our method exploits the biological biases of the problem by\ndirectly supervising relative differences with respect to mean expression, and\nleverages local visual context at every coordinate to make predictions using a\ngraph neural network. This approach closes the gap between complete locality\nand complete globality in current methods. In addition, we propose a novel\nbenchmark that aims to better define the task by following current best\npractices in transcriptomics and restricting the prediction variables to only\nthose with clear spatial patterns. Our extensive evaluation in two different\nhuman breast cancer datasets indicates that SEPAL outperforms previous\nstate-of-the-art methods and other mechanisms of including spatial context.\n", "rewritten_text": "This paper introduces SEPAL, a novel model for predicting gene expression profiles from histopathology images.  Addressing the limitations of current spatial transcriptomics technologies, which require expensive equipment and specialized knowledge, SEPAL leverages a graph neural network to predict gene expression by analyzing local visual context and incorporating biologically informed relative expression differences.  Unlike existing methods, SEPAL effectively balances local and global information.  Furthermore, we introduce a new benchmark dataset, designed to improve task definition and focus on spatially patterned genes.  Our results on two human breast cancer datasets demonstrate that SEPAL significantly outperforms existing state-of-the-art methods.\n"}, "2011.0212": {"original_text": "  Diabetic retinopathy (DR) is one of the leading causes of blindness. However,\nno specific symptoms of early DR lead to a delayed diagnosis, which results in\ndisease progression in patients. To determine the disease severity levels,\nophthalmologists need to focus on the discriminative parts of the fundus\nimages. In recent years, deep learning has achieved great success in medical\nimage analysis. However, most works directly employ algorithms based on\nconvolutional neural networks (CNNs), which ignore the fact that the difference\namong classes is subtle and gradual. Hence, we consider automatic image grading\nof DR as a fine-grained classification task, and construct a bilinear model to\nidentify the pathologically discriminative areas. In order to leverage the\nordinal information among classes, we use an ordinal regression method to\nobtain the soft labels. In addition, other than only using a categorical loss\nto train our network, we also introduce the metric loss to learn a more\ndiscriminative feature space. Experimental results demonstrate the superior\nperformance of the proposed method on two public IDRiD and DeepDR datasets.\n", "rewritten_text": "Diabetic retinopathy (DR), a leading cause of blindness, often goes undiagnosed due to a lack of early symptoms.  Accurate severity assessment requires ophthalmologists to analyze subtle details in fundus images. While deep learning offers promise in medical image analysis, existing convolutional neural network (CNN)-based approaches struggle with the gradual, nuanced differences between DR severity levels.  This work addresses this challenge by framing DR grading as a fine-grained classification problem. We propose a bilinear model to identify key pathological features, incorporate ordinal regression for handling the ordered nature of DR severity, and employ a metric loss alongside categorical loss to enhance feature discrimination.  Our method demonstrates superior performance on the IDRiD and DeepDR datasets.\n"}, "1905.02462": {"original_text": "  Recently, image super-resolution has been widely studied and achieved\nsignificant progress by leveraging the power of deep convolutional neural\nnetworks. However, there has been limited advancement in video super-resolution\n(VSR) due to the complex temporal patterns in videos. In this paper, we\ninvestigate how to adapt state-of-the-art methods of image super-resolution for\nvideo super-resolution. The proposed adapting method is straightforward. The\ninformation among successive frames is well exploited, while the overhead on\nthe original image super-resolution method is negligible. Furthermore, we\npropose a learning-based method to ensemble the outputs from multiple\nsuper-resolution models. Our methods show superior performance and rank second\nin the NTIRE2019 Video Super-Resolution Challenge Track 1.\n", "rewritten_text": "Deep learning has significantly advanced image super-resolution (ISR), but video super-resolution (VSR) lags behind due to the complexities of temporal information.  This paper presents a novel approach that effectively adapts leading ISR techniques to VSR.  Our method efficiently leverages inter-frame information with minimal computational overhead.  Additionally, we introduce a learning-based ensemble method combining outputs from multiple super-resolution models.  These combined techniques achieved a second-place ranking in the NTIRE2019 Video Super-Resolution Challenge Track 1.\n"}, "2312.14157": {"original_text": "  3D hand tracking from a monocular video is a very challenging problem due to\nhand interactions, occlusions, left-right hand ambiguity, and fast motion. Most\nexisting methods rely on RGB inputs, which have severe limitations under\nlow-light conditions and suffer from motion blur. In contrast, event cameras\ncapture local brightness changes instead of full image frames and do not suffer\nfrom the described effects. Unfortunately, existing image-based techniques\ncannot be directly applied to events due to significant differences in the data\nmodalities. In response to these challenges, this paper introduces the first\nframework for 3D tracking of two fast-moving and interacting hands from a\nsingle monocular event camera. Our approach tackles the left-right hand\nambiguity with a novel semi-supervised feature-wise attention mechanism and\nintegrates an intersection loss to fix hand collisions. To facilitate advances\nin this research domain, we release a new synthetic large-scale dataset of two\ninteracting hands, Ev2Hands-S, and a new real benchmark with real event streams\nand ground-truth 3D annotations, Ev2Hands-R. Our approach outperforms existing\nmethods in terms of the 3D reconstruction accuracy and generalises to real data\nunder severe light conditions.\n", "rewritten_text": "Tracking 3D hand movements from a single video camera is difficult due to factors like hand interaction, occlusion, ambiguity between left and right hands, and rapid motion.  Existing RGB-based methods struggle in low light and with motion blur.  Event cameras, which record changes in brightness rather than full frames, overcome these limitations. However, existing techniques are not directly applicable to event camera data.  This paper presents the first framework for 3D tracking of two interacting hands from a single event camera, addressing hand ambiguity with a novel semi-supervised attention mechanism and resolving collisions with an intersection loss.  We also introduce two new datasets: Ev2Hands-S, a large synthetic dataset, and Ev2Hands-R, a real-world benchmark with 3D ground truth. Our method surpasses existing approaches in 3D reconstruction accuracy and robustly handles challenging real-world low-light conditions.\n"}, "2010.08243": {"original_text": "  3D object detectors based only on LiDAR point clouds hold the\nstate-of-the-art on modern street-view benchmarks. However, LiDAR-based\ndetectors poorly generalize across domains due to domain shift. In the case of\nLiDAR, in fact, domain shift is not only due to changes in the environment and\nin the object appearances, as for visual data from RGB cameras, but is also\nrelated to the geometry of the point clouds (e.g., point density variations).\nThis paper proposes SF-UDA$^{3D}$, the first Source-Free Unsupervised Domain\nAdaptation (SF-UDA) framework to domain-adapt the state-of-the-art PointRCNN 3D\ndetector to target domains for which we have no annotations (unsupervised),\nneither we hold images nor annotations of the source domain (source-free).\nSF-UDA$^{3D}$ is novel on both aspects. Our approach is based on\npseudo-annotations, reversible scale-transformations and motion coherency.\nSF-UDA$^{3D}$ outperforms both previous domain adaptation techniques based on\nfeatures alignment and state-of-the-art 3D object detection methods which\nadditionally use few-shot target annotations or target annotation statistics.\nThis is demonstrated by extensive experiments on two large-scale datasets,\ni.e., KITTI and nuScenes.\n", "rewritten_text": "While LiDAR-based 3D object detectors achieve peak performance on current street-view benchmarks, their performance suffers significantly from domain shift.  Unlike camera-based systems, this shift in LiDAR data stems not only from environmental and object appearance changes, but also from variations in point cloud geometry (e.g., density).  This paper introduces SF-UDA\u00b3D, the first source-free unsupervised domain adaptation framework for 3D object detection.  SF-UDA\u00b3D adapts the state-of-the-art PointRCNN detector to unlabeled target domains without requiring any source domain data or annotations.  This novel approach leverages pseudo-annotations, reversible scale transformations, and motion coherency.  Extensive experiments on KITTI and nuScenes datasets demonstrate that SF-UDA\u00b3D surpasses existing domain adaptation methods (which rely on feature alignment) and even state-of-the-art 3D detectors that utilize limited target annotations.\n"}, "2305.09407": {"original_text": "  Visual quality inspection in high performance manufacturing can benefit from\nautomation, due to cost savings and improved rigor. Deep learning techniques\nare the current state of the art for generic computer vision tasks like\nclassification and object detection. Manufacturing data can pose a challenge\nfor deep learning because data is highly repetitive and there are few images of\ndefects or deviations to learn from. Deep learning models trained with such\ndata can be fragile and sensitive to context, and can under-detect new defects\nnot found in the training data. In this work, we explore training defect\ndetection models to learn specific defects out of context, so that they are\nmore likely to be detected in new situations. We demonstrate how models trained\non diverse images containing a common defect type can pick defects out in new\ncircumstances. Such generic models could be more robust to new defects not\nfound data collected for training, and can reduce data collection impediments\nto implementing visual inspection on production lines. Additionally, we\ndemonstrate that object detection models trained to predict a label and\nbounding box outperform classifiers that predict a label only on held out test\ndata typical of manufacturing inspection tasks. Finally, we studied the factors\nthat affect generalization in order to train models that work under a wider\nrange of conditions.\n", "rewritten_text": "Automating visual quality inspection in high-performance manufacturing offers significant cost savings and improved accuracy. While deep learning excels at general computer vision tasks, its application to manufacturing is hampered by repetitive data and a scarcity of defect images. This leads to models prone to errors and unable to detect novel defects.  This research investigates training defect detection models on diverse, out-of-context images of a specific defect type to enhance robustness. We show that these models effectively identify defects in new situations, overcoming data limitations and improving deployment on production lines.  Furthermore, we demonstrate that object detection models (predicting both defect location and type) outperform simple classifiers (predicting only defect type) in realistic manufacturing scenarios. Finally, we analyze factors influencing model generalization to achieve broader applicability.\n"}, "2406.09858": {"original_text": "  The visual quality of an image is confounded by a number of intertwined\nfactors including its semantic content, distortion characteristics and\nappearance properties such as brightness, contrast, sharpness, and\ncolourfulness. Distilling high level knowledge about all these quality bearing\nattributes is crucial for developing objective Image Quality Assessment\n(IQA).While existing solutions have modeled some of these aspects, a\ncomprehensive solution that involves all these important quality related\nattributes has not yet been developed. In this paper, we present a new blind\nIQA (BIQA) model termed Self-supervision and Vision-Language supervision Image\nQUality Evaluator (SLIQUE) that features a joint vision-language and visual\ncontrastive representation learning framework for acquiring high level\nknowledge about the images semantic contents, distortion characteristics and\nappearance properties for IQA. For training SLIQUE, we have developed a\nsystematic approach to constructing a first of its kind large image database\nannotated with all three categories of quality relevant texts. The Text\nAnnotated Distortion, Appearance and Content (TADAC) database has over 1.6\nmillion images annotated with textual descriptions of their semantic contents,\ndistortion characteristics and appearance properties. The method for\nconstructing TADAC and the database itself will be particularly useful for\nexploiting vision-language modeling for advanced IQA applications. Extensive\nexperimental results show that SLIQUE has superior performances over state of\nthe art, demonstrating the soundness of its design principle and the\neffectiveness of its implementation.\n", "rewritten_text": "Image quality is complex, depending on interwoven factors like semantic content, distortions, and visual properties (brightness, contrast, etc.).  Objective Image Quality Assessment (IQA) requires understanding all these attributes.  While existing IQA models address some aspects, a comprehensive solution has been lacking.  This paper introduces SLIQUE, a novel blind IQA (BIQA) model using a joint vision-language and contrastive learning framework to learn high-level knowledge about image content, distortions, and appearance.  SLIQUE is trained on TADAC, a new, large-scale image database (1.6M+ images) with textual annotations describing content, distortions, and appearance.  TADAC and its creation method are valuable resources for future vision-language IQA research.  Extensive experiments demonstrate SLIQUE's superior performance compared to state-of-the-art methods, validating its design and implementation.\n"}, "2406.16860": {"original_text": "  We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, addressing the difficulties involved in consolidating and\ninterpreting results from various tasks, and introduce a new vision-centric\nbenchmark, CV-Bench. To further improve visual grounding, we propose the\nSpatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that\nintegrates high-resolution vision features with LLMs while reducing the number\nof tokens. Additionally, we discuss the curation of high-quality visual\ninstruction-tuning data from publicly available sources, emphasizing the\nimportance of data source balancing and distribution ratio. Collectively,\nCambrian-1 not only achieves state-of-the-art performance but also serves as a\ncomprehensive, open cookbook for instruction-tuned MLLMs. We provide model\nweights, code, supporting tools, datasets, and detailed instruction-tuning and\nevaluation recipes. We hope our release will inspire and accelerate\nadvancements in multimodal systems and visual representation learning.\n", "rewritten_text": "Cambrian-1, a new family of vision-centric multimodal large language models (MLLMs), addresses the limitations of current approaches by prioritizing robust visual understanding.  Unlike many MLLMs that underemphasize visual representation learning, Cambrian-1 systematically evaluates over 20 vision encoders using a novel visual instruction tuning framework.  This research reveals key insights into different visual model architectures and exposes weaknesses in existing MLLM benchmarks.  To address these shortcomings, we introduce CV-Bench, a new vision-centric benchmark, and the Spatial Vision Aggregator (SVA), a novel module improving high-resolution image integration with LLMs.  Furthermore, we detail our process for curating high-quality visual instruction-tuning data, emphasizing data balancing.  Cambrian-1 achieves state-of-the-art performance and is released as an open-source resource, including model weights, code, tools, datasets, and detailed training and evaluation recipes, to foster advancements in multimodal AI and visual representation learning.\n"}, "2403.09593": {"original_text": "  Names are essential to both human cognition and vision-language models.\nOpen-vocabulary models utilize class names as text prompts to generalize to\ncategories unseen during training. However, the precision of these names is\noften overlooked in existing datasets. In this paper, we address this\nunderexplored problem by presenting a framework for \"renovating\" names in\nopen-vocabulary segmentation benchmarks (RENOVATE). Our framework features a\nrenaming model that enhances the quality of names for each visual segment.\nThrough experiments, we demonstrate that our renovated names help train\nstronger open-vocabulary models with up to 15% relative improvement and\nsignificantly enhance training efficiency with improved data quality. We also\nshow that our renovated names improve evaluation by better measuring\nmisclassification and enabling fine-grained model analysis. We will provide our\ncode and relabelings for several popular segmentation datasets (MS COCO,\nADE20K, Cityscapes) to the research community.\n", "rewritten_text": "Accurate object names are crucial for both human understanding and the performance of vision-language models.  Open-vocabulary models rely on these names, yet existing datasets often contain imprecise labels.  This paper introduces RENOVATE, a framework for improving the quality of these names in open-vocabulary segmentation benchmarks.  RENOVATE uses a renaming model to refine segment labels, leading to up to a 15% relative improvement in model performance, increased training efficiency, and more accurate evaluation.  Furthermore, it facilitates finer-grained model analysis.  The code and improved labels for MS COCO, ADE20K, and Cityscapes datasets will be publicly released.\n"}, "2310.13201": {"original_text": "  Early identification of abnormalities in plants is an important task for\nensuring proper growth and achieving high yields from crops. Precision\nagriculture can significantly benefit from modern computer vision tools to make\nfarming strategies addressing these issues efficient and effective. As farming\nlands are typically quite large, farmers have to manually check vast areas to\ndetermine the status of the plants and apply proper treatments. In this work,\nwe consider the problem of automatically identifying abnormal regions in maize\nplants from images captured by a UAV. Using deep learning techniques, we have\ndeveloped a methodology which can detect different levels of abnormality (i.e.,\nlow, medium, high or no abnormality) in maize plants independently of their\ngrowth stage. The primary goal is to identify anomalies at the earliest\npossible stage in order to maximize the effectiveness of potential treatments.\nAt the same time, the proposed system can provide valuable information to human\nannotators for ground truth data collection by helping them to focus their\nattention on a much smaller set of images only. We have experimented with two\ndifferent but complimentary approaches, the first considering abnormality\ndetection as a classification problem and the second considering it as a\nregression problem. Both approaches can be generalized to different types of\nabnormalities and do not make any assumption about the abnormality occurring at\nan early plant growth stage which might be easier to detect due to the plants\nbeing smaller and easier to separate. As a case study, we have considered a\npublicly available data set which exhibits mostly Nitrogen deficiency in maize\nplants of various growth stages. We are reporting promising preliminary results\nwith an 88.89\\% detection accuracy of low abnormality and 100\\% detection\naccuracy of no abnormality.\n", "rewritten_text": "Early detection of plant abnormalities is crucial for maximizing crop yields.  This work addresses this challenge by using computer vision and deep learning to automatically identify abnormal regions in maize plants from UAV imagery.  Our methodology detects varying levels of abnormality (low, medium, high, or none) regardless of plant growth stage, enabling early intervention for optimal treatment effectiveness.  The system also assists human annotators by prioritizing images requiring detailed inspection, thus improving data collection efficiency.  We explored two complementary approaches: classification and regression.  Both are generalizable to various abnormalities and are not limited to early-stage detection.  Preliminary results on a publicly available dataset (primarily showing nitrogen deficiency) show promising accuracy, achieving 88.89% detection accuracy for low abnormality and 100% for no abnormality.\n"}, "1711.0664": {"original_text": "  We investigate the problem of producing structured graph representations of\nvisual scenes. Our work analyzes the role of motifs: regularly appearing\nsubstructures in scene graphs. We present new quantitative insights on such\nrepeated structures in the Visual Genome dataset. Our analysis shows that\nobject labels are highly predictive of relation labels but not vice-versa. We\nalso find that there are recurring patterns even in larger subgraphs: more than\n50% of graphs contain motifs involving at least two relations. Our analysis\nmotivates a new baseline: given object detections, predict the most frequent\nrelation between object pairs with the given labels, as seen in the training\nset. This baseline improves on the previous state-of-the-art by an average of\n3.6% relative improvement across evaluation settings. We then introduce Stacked\nMotif Networks, a new architecture designed to capture higher order motifs in\nscene graphs that further improves over our strong baseline by an average 7.1%\nrelative gain. Our code is available at github.com/rowanz/neural-motifs.\n", "rewritten_text": "This paper addresses the creation of structured graph representations for visual scenes, focusing on the significance of recurring substructures (motifs) within scene graphs.  Analyzing the Visual Genome dataset, we reveal that object labels strongly predict relation labels, but not conversely, and that over half of the graphs contain motifs with at least two relations.  This analysis led to a novel baseline: predicting the most frequent relation between detected objects based on their labels. This baseline surpasses the previous state-of-the-art by 3.6% on average.  We further introduce Stacked Motif Networks, a new architecture capturing higher-order motifs, achieving an additional 7.1% average relative improvement over our baseline.  Our code is publicly available at github.com/rowanz/neural-motifs.\n"}, "2207.01932": {"original_text": "  Image Coding for Machines (ICM) aims to compress images for AI tasks analysis\nrather than meeting human perception. Learning a kind of feature that is both\ngeneral (for AI tasks) and compact (for compression) is pivotal for its\nsuccess. In this paper, we attempt to develop an ICM framework by learning\nuniversal features while also considering compression. We name such features as\nomnipotent features and the corresponding framework as Omni-ICM. Considering\nself-supervised learning (SSL) improves feature generalization, we integrate it\nwith the compression task into the Omni-ICM framework to learn omnipotent\nfeatures. However, it is non-trivial to coordinate semantics modeling in SSL\nand redundancy removing in compression, so we design a novel information\nfiltering (IF) module between them by co-optimization of instance\ndistinguishment and entropy minimization to adaptively drop information that is\nweakly related to AI tasks (e.g., some texture redundancy). Different from\nprevious task-specific solutions, Omni-ICM could directly support AI tasks\nanalysis based on the learned omnipotent features without joint training or\nextra transformation. Albeit simple and intuitive, Omni-ICM significantly\noutperforms existing traditional and learning-based codecs on multiple\nfundamental vision tasks.\n", "rewritten_text": "This paper introduces Omni-ICM, a novel Image Coding for Machines (ICM) framework designed to efficiently compress images for AI tasks.  Unlike traditional image compression focusing on human perception, Omni-ICM prioritizes learning compact, generalizable features \u2013 termed \"omnipotent features\" \u2013 suitable for diverse AI applications.  Leveraging self-supervised learning (SSL), Omni-ICM integrates compression and feature learning through a novel information filtering (IF) module. This module co-optimizes instance distinction and entropy minimization, selectively discarding information irrelevant to AI tasks (e.g., texture redundancy).  Unlike task-specific methods, Omni-ICM directly supports various AI tasks using its learned omnipotent features without retraining or additional transformations.  Despite its simplicity, Omni-ICM significantly outperforms existing codecs on multiple benchmark vision tasks.\n"}, "1711.07183": {"original_text": "  Generating adversarial examples is an intriguing problem and an important way\nof understanding the working mechanism of deep neural networks. Most existing\napproaches generated perturbations in the image space, i.e., each pixel can be\nmodified independently. However, in this paper we pay special attention to the\nsubset of adversarial examples that correspond to meaningful changes in 3D\nphysical properties (like rotation and translation, illumination condition,\netc.). These adversaries arguably pose a more serious concern, as they\ndemonstrate the possibility of causing neural network failure by easy\nperturbations of real-world 3D objects and scenes.\n  In the contexts of object classification and visual question answering, we\naugment state-of-the-art deep neural networks that receive 2D input images with\na rendering module (either differentiable or not) in front, so that a 3D scene\n(in the physical space) is rendered into a 2D image (in the image space), and\nthen mapped to a prediction (in the output space). The adversarial\nperturbations can now go beyond the image space, and have clear meanings in the\n3D physical world. Though image-space adversaries can be interpreted as\nper-pixel albedo change, we verify that they cannot be well explained along\nthese physically meaningful dimensions, which often have a non-local effect.\nBut it is still possible to successfully attack beyond the image space on the\nphysical space, though this is more difficult than image-space attacks,\nreflected in lower success rates and heavier perturbations required.\n", "rewritten_text": "Deep neural networks' vulnerability to adversarial examples is a key research area.  While most research focuses on pixel-level image perturbations, this paper investigates adversarial examples stemming from meaningful 3D physical changes (rotation, translation, lighting, etc.).  These pose a greater threat, as they demonstrate the potential for easily manipulated real-world objects to fool networks.\n\nWe augment object classification and visual question answering networks with a rendering module (differentiable or not) to process 3D scenes into 2D images before prediction. This allows adversarial attacks to manipulate the 3D scene directly, rather than just individual pixels.  While pixel-level attacks can be interpreted as albedo changes, they lack the non-local effects of physically meaningful 3D manipulations.  Although more challenging and requiring larger perturbations,  3D-space attacks successfully compromise the networks, highlighting a significant vulnerability.\n"}, "2105.02039": {"original_text": "  In this paper, we fill the research gap by adopting state-of-the-art computer\nvision techniques for the data extraction stage in a data mining system. As\nshown in Fig.1, this stage contains two subtasks, namely, plot element\ndetection and data conversion. For building a robust box detector, we\ncomprehensively compare different deep learning-based methods and find a\nsuitable method to detect box with high precision. For building a robust point\ndetector, a fully convolutional network with feature fusion module is adopted,\nwhich can distinguish close points compared to traditional methods. The\nproposed system can effectively handle various chart data without making\nheuristic assumptions. For data conversion, we translate the detected element\ninto data with semantic value. A network is proposed to measure feature\nsimilarities between legends and detected elements in the legend matching\nphase. Furthermore, we provide a baseline on the competition of Harvesting raw\ntables from Infographics. Some key factors have been found to improve the\nperformance of each stage. Experimental results demonstrate the effectiveness\nof the proposed system.\n", "rewritten_text": "This paper addresses a research gap in data mining by applying advanced computer vision techniques to data extraction.  Our system, illustrated in Figure 1, comprises plot element detection (boxes and points) and data conversion.  We developed a high-precision box detector by comparing various deep learning methods, and a novel point detector using a fully convolutional network with feature fusion to accurately identify closely spaced points.  Unlike heuristic approaches, our system robustly handles diverse chart types.  Data conversion involves semantically enriching detected elements, including a novel legend-matching network.  We also establish a baseline for the \"Harvesting raw tables from Infographics\" competition, identifying key performance-enhancing factors.  Experimental results validate the system's effectiveness.\n"}, "2401.08508": {"original_text": "  Sentiment analysis and emotion detection are important research topics in\nnatural language processing (NLP) and benefit many downstream tasks. With the\nwidespread application of LLMs, researchers have started exploring the\napplication of LLMs based on instruction-tuning in the field of sentiment\nanalysis. However, these models only focus on single aspects of affective\nclassification tasks (e.g. sentimental polarity or categorical emotions), and\noverlook the regression tasks (e.g. sentiment strength or emotion intensity),\nwhich leads to poor performance in downstream tasks. The main reason is the\nlack of comprehensive affective instruction tuning datasets and evaluation\nbenchmarks, which cover various affective classification and regression tasks.\nMoreover, although emotional information is useful for downstream tasks,\nexisting downstream datasets lack high-quality and comprehensive affective\nannotations. In this paper, we propose EmoLLMs, the first series of\nopen-sourced instruction-following LLMs for comprehensive affective analysis\nbased on fine-tuning various LLMs with instruction data, the first multi-task\naffective analysis instruction dataset (AAID) with 234K data samples based on\nvarious classification and regression tasks to support LLM instruction tuning,\nand a comprehensive affective evaluation benchmark (AEB) with 14 tasks from\nvarious sources and domains to test the generalization ability of LLMs. We\npropose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various\naffective instruction tasks. We compare our model with a variety of LLMs on\nAEB, where our models outperform all other open-sourced LLMs, and surpass\nChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve\nthe ChatGPT-level and GPT-4-level generalization capabilities on affective\nanalysis tasks, and demonstrates our models can be used as affective annotation\ntools.\n", "rewritten_text": "Instruction-tuned large language models (LLMs) show promise for sentiment analysis and emotion detection, but current approaches are limited by their focus on single-aspect classification tasks and a lack of suitable datasets and benchmarks for regression tasks (e.g., sentiment strength).  To address this, we introduce EmoLLMs, a series of open-source, instruction-following LLMs.  These models are fine-tuned using AAID, a new 234K-sample multi-task affective analysis instruction dataset covering both classification and regression tasks.  We also present AEB, a comprehensive evaluation benchmark with 14 tasks.  EmoLLMs outperform existing open-source LLMs on AEB and achieve performance comparable to ChatGPT and GPT-4 on most tasks, demonstrating their strong generalization capabilities and potential as affective annotation tools.\n"}, "2210.05564": {"original_text": "  Semantic segmentation is a fundamental topic in computer vision. Several deep\nlearning methods have been proposed for semantic segmentation with outstanding\nresults. However, these models require a lot of densely annotated images. To\naddress this problem, we propose a new algorithm that uses HyperGraph\nConvolutional Networks for Weakly-supervised Semantic Segmentation\n(HyperGCN-WSS). Our algorithm constructs spatial and k-Nearest Neighbor (k-NN)\ngraphs from the images in the dataset to generate the hypergraphs. Then, we\ntrain a specialized HyperGraph Convolutional Network (HyperGCN) architecture\nusing some weak signals. The outputs of the HyperGCN are denominated\npseudo-labels, which are later used to train a DeepLab model for semantic\nsegmentation. HyperGCN-WSS is evaluated on the PASCAL VOC 2012 dataset for\nsemantic segmentation, using scribbles or clicks as weak signals. Our algorithm\nshows competitive performance against previous methods.\n", "rewritten_text": "This paper introduces HyperGCN-WSS, a novel weakly-supervised semantic segmentation algorithm leveraging HyperGraph Convolutional Networks (HyperGCNs).  Addressing the scarcity of fully annotated data in semantic segmentation, HyperGCN-WSS constructs hypergraphs from image data using spatial and k-Nearest Neighbor relationships.  A specialized HyperGCN then generates pseudo-labels from weak supervision (scribbles or clicks), which are subsequently used to train a DeepLab model.  Experiments on the PASCAL VOC 2012 dataset demonstrate competitive performance compared to existing methods.\n"}, "2112.0618": {"original_text": "  We present 360-DFPE, a sequential floor plan estimation method that directly\ntakes 360-images as input without relying on active sensors or 3D information.\nOur approach leverages a loosely coupled integration between a monocular visual\nSLAM solution and a monocular 360-room layout approach, which estimate camera\nposes and layout geometries, respectively. Since our task is to sequentially\ncapture the floor plan using monocular images, the entire scene structure, room\ninstances, and room shapes are unknown. To tackle these challenges, we first\nhandle the scale difference between visual odometry and layout geometry via\nformulating an entropy minimization process, which enables us to directly align\n360-layouts without knowing the entire scene in advance. Second, to\nsequentially identify individual rooms, we propose a novel room identification\nalgorithm that tracks every room along the camera exploration using geometry\ninformation. Lastly, to estimate the final shape of the room, we propose a\nshortest path algorithm with an iterative coarse-to-fine strategy, which\nimproves prior formulations with higher accuracy and faster run-time. Moreover,\nwe collect a new floor plan dataset with challenging large-scale scenes,\nproviding both point clouds and sequential 360-image information. Experimental\nresults show that our monocular solution achieves favorable performance against\nthe current state-of-the-art algorithms that rely on active sensors and require\nthe entire scene reconstruction data in advance.\n", "rewritten_text": "360-DFPE is a novel method for sequentially estimating floor plans directly from 360\u00b0 images, requiring no active sensors or pre-existing 3D data.  It integrates a monocular visual SLAM system (for camera pose estimation) with a monocular 360\u00b0 room layout estimation approach.  To address the challenges of unknown scene structure and scale discrepancies, 360-DFPE employs: 1) an entropy minimization process for aligning 360\u00b0 layouts; 2) a novel room identification algorithm tracking rooms throughout camera movement; and 3) a refined shortest-path algorithm with iterative coarse-to-fine optimization for accurate and efficient room shape estimation.  Furthermore, we introduce a new large-scale floor plan dataset with 360\u00b0 image sequences and point cloud ground truth.  Our monocular approach outperforms existing methods that rely on active sensors and complete scene reconstruction.\n"}, "2205.08811": {"original_text": "  Object pose estimation is crucial for robotic applications and augmented\nreality. Beyond instance level 6D object pose estimation methods, estimating\ncategory-level pose and shape has become a promising trend. As such, a new\nresearch field needs to be supported by well-designed datasets. To provide a\nbenchmark with high-quality ground truth annotations to the community, we\nintroduce a multimodal dataset for category-level object pose estimation with\nphotometrically challenging objects termed PhoCaL. PhoCaL comprises 60 high\nquality 3D models of household objects over 8 categories including highly\nreflective, transparent and symmetric objects. We developed a novel\nrobot-supported multi-modal (RGB, depth, polarisation) data acquisition and\nannotation process. It ensures sub-millimeter accuracy of the pose for opaque\ntextured, shiny and transparent objects, no motion blur and perfect camera\nsynchronisation. To set a benchmark for our dataset, state-of-the-art RGB-D and\nmonocular RGB methods are evaluated on the challenging scenes of PhoCaL.\n", "rewritten_text": "Accurate object pose estimation is vital for robotics and augmented reality.  While instance-level 6D pose estimation is well-established, category-level pose and shape estimation is emerging as a key area of research.  To advance this field, we introduce PhoCaL, a new multimodal dataset featuring high-quality ground truth annotations for photometrically challenging objects.  PhoCaL contains 60 detailed 3D models of household objects across 8 categories, including reflective, transparent, and symmetric objects.  Our novel robot-assisted data acquisition process uses RGB, depth, and polarization imaging to achieve sub-millimeter pose accuracy, eliminating motion blur and ensuring perfect camera synchronization.  We benchmark state-of-the-art RGB-D and monocular RGB methods on PhoCaL's challenging scenes.\n"}, "2005.14439": {"original_text": "  Dynamic routing networks, aimed at finding the best routing paths in the\nnetworks, have achieved significant improvements to neural networks in terms of\naccuracy and efficiency. In this paper, we see dynamic routing networks in a\nfresh light, formulating a routing method as a mapping from a sample space to a\nrouting space. From the perspective of space mapping, prevalent methods of\ndynamic routing didn't consider how inference paths would be distributed in the\nrouting space. Thus, we propose a novel method, termed CoDiNet, to model the\nrelationship between a sample space and a routing space by regularizing the\ndistribution of routing paths with the properties of consistency and diversity.\nSpecifically, samples with similar semantics should be mapped into the same\narea in routing space, while those with dissimilar semantics should be mapped\ninto different areas. Moreover, we design a customizable dynamic routing\nmodule, which can strike a balance between accuracy and efficiency. When\ndeployed upon ResNet models, our method achieves higher performance and\neffectively reduces average computational cost on four widely used datasets.\n", "rewritten_text": "Dynamic routing significantly improves neural network accuracy and efficiency.  This paper re-examines dynamic routing by framing it as a mapping between sample and routing spaces.  Existing methods neglect the distribution of inference paths within the routing space.  Therefore, we introduce CoDiNet, a novel method that regularizes this distribution, ensuring consistency (similar samples map to similar routing paths) and diversity (dissimilar samples map to distinct paths).  Our customizable dynamic routing module balances accuracy and efficiency, achieving superior performance and reduced computational cost on four benchmark datasets when integrated into ResNet models.\n"}, "2110.05594": {"original_text": "  We present a modern solution to the multi-view photometric stereo problem\n(MVPS). Our work suitably exploits the image formation model in a MVPS\nexperimental setup to recover the dense 3D reconstruction of an object from\nimages. We procure the surface orientation using a photometric stereo (PS)\nimage formation model and blend it with a multi-view neural radiance field\nrepresentation to recover the object's surface geometry. Contrary to the\nprevious multi-staged framework to MVPS, where the position, iso-depth\ncontours, or orientation measurements are estimated independently and then\nfused later, our method is simple to implement and realize. Our method performs\nneural rendering of multi-view images while utilizing surface normals estimated\nby a deep photometric stereo network. We render the MVPS images by considering\nthe object's surface normals for each 3D sample point along the viewing\ndirection rather than explicitly using the density gradient in the volume space\nvia 3D occupancy information. We optimize the proposed neural radiance field\nrepresentation for the MVPS setup efficiently using a fully connected deep\nnetwork to recover the 3D geometry of an object. Extensive evaluation on the\nDiLiGenT-MV benchmark dataset shows that our method performs better than the\napproaches that perform only PS or only multi-view stereo (MVS) and provides\ncomparable results against the state-of-the-art multi-stage fusion methods.\n", "rewritten_text": "This paper introduces a novel, streamlined approach to multi-view photometric stereo (MVPS).  Unlike previous multi-stage methods that separately estimate and fuse position, depth, or orientation, our single-stage method directly integrates a photometric stereo (PS) model with a neural radiance field (NeRF) representation.  We leverage a deep PS network to estimate surface normals, which are then used to render multi-view images within the NeRF framework, bypassing the need for explicit 3D occupancy information.  This efficient, fully connected network architecture is optimized for MVPS and achieves state-of-the-art performance on the DiLiGenT-MV benchmark, outperforming methods relying solely on PS or multi-view stereo (MVS) and matching the accuracy of complex multi-stage fusion techniques.\n"}, "2206.13263": {"original_text": "  Robust maritime obstacle detection is critical for safe navigation of\nautonomous boats and timely collision avoidance. The current state-of-the-art\nis based on deep segmentation networks trained on large datasets. However,\nper-pixel ground truth labeling of such datasets is labor-intensive and\nexpensive. We propose a new scaffolding learning regime (SLR) that leverages\nweak annotations consisting of water edges, the horizon location, and obstacle\nbounding boxes to train segmentation-based obstacle detection networks, thereby\nreducing the required ground truth labeling effort by a factor of twenty. SLR\ntrains an initial model from weak annotations and then alternates between\nre-estimating the segmentation pseudo-labels and improving the network\nparameters. Experiments show that maritime obstacle segmentation networks\ntrained using SLR on weak annotations not only match but outperform the same\nnetworks trained with dense ground truth labels, which is a remarkable result.\nIn addition to the increased accuracy, SLR also increases domain generalization\nand can be used for domain adaptation with a low manual annotation load. The\nSLR code and pre-trained models are available at\nhttps://github.com/lojzezust/SLR .\n", "rewritten_text": "Safe autonomous boat navigation requires robust maritime obstacle detection.  While deep segmentation networks currently achieve state-of-the-art performance, their training relies on expensive, labor-intensive pixel-level annotations.  We introduce Scaffolding Learning Regime (SLR), a novel training method using significantly cheaper weak annotations (water edges, horizon, and bounding boxes).  SLR iteratively refines pseudo-labels and network parameters, reducing annotation effort by 95%.  Surprisingly, experiments demonstrate that SLR achieves accuracy exceeding that of models trained with full annotations, while also improving domain generalization and enabling efficient domain adaptation.  Our code and pre-trained models are publicly available at https://github.com/lojzezust/SLR.\n"}, "2204.07126": {"original_text": "  Recent development of neural implicit function has shown tremendous success\non high-quality 3D shape reconstruction. However, most works divide the space\ninto inside and outside of the shape, which limits their representing power to\nsingle-layer and watertight shapes. This limitation leads to tedious data\nprocessing (converting non-watertight raw data to watertight) as well as the\nincapability of representing general object shapes in the real world. In this\nwork, we propose a novel method to represent general shapes including\nnon-watertight shapes and shapes with multi-layer surfaces. We introduce\nGeneral Implicit Function for 3D Shape (GIFS), which models the relationships\nbetween every two points instead of the relationships between points and\nsurfaces. Instead of dividing 3D space into predefined inside-outside regions,\nGIFS encodes whether two points are separated by any surface. Experiments on\nShapeNet show that GIFS outperforms previous state-of-the-art methods in terms\nof reconstruction quality, rendering efficiency, and visual fidelity. Project\npage is available at https://jianglongye.com/gifs .\n", "rewritten_text": "Neural implicit functions have achieved remarkable success in high-quality 3D shape reconstruction.  However, their reliance on inside/outside classifications restricts them to single-layer, watertight shapes, necessitating laborious data preprocessing and hindering representation of complex real-world objects.  This paper introduces GIFS (General Implicit Function for 3D Shape), a novel method that overcomes these limitations.  Unlike previous approaches, GIFS models the relationships between all point pairs, encoding their separation by any surface rather than relying on predefined inside/outside regions.  Experiments on ShapeNet demonstrate GIFS' superior reconstruction quality, rendering efficiency, and visual fidelity compared to state-of-the-art methods.  See https://jianglongye.com/gifs for more details.\n"}, "2103.03166": {"original_text": "  Annotated medical images are typically rarer than labeled natural images\nsince they are limited by domain knowledge and privacy constraints. Recent\nadvances in transfer and contrastive learning have provided effective solutions\nto tackle such issues from different perspectives. The state-of-the-art\ntransfer learning (e.g., Big Transfer (BiT)) and contrastive learning (e.g.,\nSimple Siamese Contrastive Learning (SimSiam)) approaches have been\ninvestigated independently, without considering the complementary nature of\nsuch techniques. It would be appealing to accelerate contrastive learning with\ntransfer learning, given that slow convergence speed is a critical limitation\nof modern contrastive learning approaches. In this paper, we investigate the\nfeasibility of aligning BiT with SimSiam. From empirical analyses, different\nnormalization techniques (Group Norm in BiT vs. Batch Norm in SimSiam) are the\nkey hurdle of adapting BiT to SimSiam. When combining BiT with SimSiam, we\nevaluated the performance of using BiT, SimSiam, and BiT+SimSiam on CIFAR-10\nand HAM10000 datasets. The results suggest that the BiT models accelerate the\nconvergence speed of SimSiam. When used together, the model gives superior\nperformance over both of its counterparts. We hope this study will motivate\nresearchers to revisit the task of aggregating big pre-trained models with\ncontrastive learning models for image analysis.\n", "rewritten_text": "Medical image annotation is scarce due to expertise and privacy limitations, unlike natural images.  While transfer learning (e.g., Big Transfer (BiT)) and contrastive learning (e.g., SimSiam) offer solutions, their complementary potential remains largely unexplored.  This paper investigates combining BiT's pre-trained weights with SimSiam to address contrastive learning's slow convergence.  We found that differing normalization techniques (Group Norm in BiT, Batch Norm in SimSiam) presented a key challenge.  Experiments on CIFAR-10 and HAM10000 datasets demonstrated that integrating BiT significantly accelerates SimSiam's convergence and yields superior performance compared to either method alone.  This work encourages further research into combining large pre-trained models with contrastive learning for improved image analysis.\n"}, "2401.00642": {"original_text": "  During times of increasing antibiotic resistance and the spread of infectious\ndiseases like COVID-19, it is important to classify genes related to antibiotic\nresistance. As natural language processing has advanced with transformer-based\nlanguage models, many language models that learn characteristics of nucleotide\nsequences have also emerged. These models show good performance in classifying\nvarious features of nucleotide sequences. When classifying nucleotide\nsequences, not only the sequence itself, but also various background knowledge\nis utilized. In this study, we use not only a nucleotide sequence-based\nlanguage model but also a text language model based on PubMed articles to\nreflect more biological background knowledge in the model. We propose a method\nto fine-tune the nucleotide sequence language model and the text language model\nbased on various databases of antibiotic resistance genes. We also propose an\nLLM-based augmentation technique to supplement the data and an ensemble method\nto effectively combine the two models. We also propose a benchmark for\nevaluating the model. Our method achieved better performance than the\nnucleotide sequence language model in the drug resistance class prediction.\n", "rewritten_text": "The rise of antibiotic resistance and pandemics like COVID-19 underscores the critical need for accurate classification of antibiotic resistance genes.  Leveraging advancements in transformer-based language models, we developed a novel approach that combines a nucleotide sequence language model with a text-based model trained on PubMed articles to incorporate richer biological context.  This method, which includes LLM-based data augmentation and an ensemble technique to integrate both models, significantly outperforms a nucleotide-sequence-only model in predicting antibiotic resistance gene classes.  We also introduce a new benchmark for evaluating such models.\n"}, "2406.15718": {"original_text": "  As large language models (LLMs) increasingly permeate daily lives, there is a\ngrowing demand for real-time interactions that mirror human conversations.\nTraditional turn-based chat systems driven by LLMs prevent users from verbally\ninteracting with the system while it is generating responses. To overcome these\nlimitations, we adapt existing LLMs to \\textit{duplex models} so that these\nLLMs can listen for users while generating output and dynamically adjust\nthemselves to provide users with instant feedback. % such as in response to\ninterruptions. Specifically, we divide the queries and responses of\nconversations into several time slices and then adopt a\ntime-division-multiplexing (TDM) encoding-decoding strategy to\npseudo-simultaneously process these slices. Furthermore, to make LLMs\nproficient enough to handle real-time conversations, we build a fine-tuning\ndataset consisting of alternating time slices of queries and responses as well\nas covering typical feedback types in instantaneous interactions. Our\nexperiments show that although the queries and responses of conversations are\nsegmented into incomplete slices for processing, LLMs can preserve their\noriginal performance on standard benchmarks with a few fine-tuning steps on our\ndataset. Automatic and human evaluation indicate that duplex models make\nuser-AI interactions more natural and human-like, and greatly improve user\nsatisfaction compared to vanilla LLMs. Our duplex model and dataset will be\nreleased.\n", "rewritten_text": "To address the limitations of current LLMs in real-time conversational interactions, we introduce a duplex model architecture.  This allows LLMs to simultaneously listen and generate responses, providing users with immediate feedback.  We achieve this using a time-division-multiplexing (TDM) strategy that processes conversation slices concurrently.  A new fine-tuning dataset, featuring alternating query and response slices and common real-time feedback types, enhances the LLM's ability to handle these interactions.  Our results demonstrate that, despite processing incomplete conversational segments, the LLM maintains performance on standard benchmarks after minimal fine-tuning.  Both automatic and human evaluations show our duplex model significantly improves the naturalness, human-likeness, and user satisfaction of LLM interactions compared to traditional methods.  The model and dataset are publicly available.\n"}, "2011.01535": {"original_text": "  3D-LaneNet+ is a camera-based DNN method for anchor free 3D lane detection\nwhich is able to detect 3d lanes of any arbitrary topology such as splits,\nmerges, as well as short and perpendicular lanes. We follow recently proposed\n3D-LaneNet, and extend it to enable the detection of these previously\nunsupported lane topologies. Our output representation is an anchor free,\nsemi-local tile representation that breaks down lanes into simple lane segments\nwhose parameters can be learnt. In addition we learn, per lane instance,\nfeature embedding that reasons for the global connectivity of locally detected\nsegments to form full 3d lanes. This combination allows 3D-LaneNet+ to avoid\nusing lane anchors, non-maximum suppression, and lane model fitting as in the\noriginal 3D-LaneNet. We demonstrate the efficacy of 3D-LaneNet+ using both\nsynthetic and real world data. Results show significant improvement relative to\nthe original 3D-LaneNet that can be attributed to better generalization to\ncomplex lane topologies, curvatures and surface geometries.\n", "rewritten_text": "3D-LaneNet+ is an improved, camera-based deep learning method for 3D lane detection that overcomes limitations of its predecessor, 3D-LaneNet.  Unlike 3D-LaneNet, it handles complex lane topologies (splits, merges, short, and perpendicular lanes) without anchors, non-maximum suppression, or explicit lane model fitting.  This is achieved through an anchor-free, semi-local tile representation that learns parameters for individual lane segments, and a learned feature embedding that connects these segments to reconstruct complete 3D lane instances.  Evaluated on synthetic and real-world data, 3D-LaneNet+ shows significant performance gains, particularly in generalizing to challenging lane configurations and geometries.\n"}, "1612.06496": {"original_text": "  Image segmentation is a popular area of research in computer vision that has\nmany applications in automated image processing. A recent technique called\npiecewise flat embeddings (PFE) has been proposed for use in image\nsegmentation; PFE transforms image pixel data into a lower dimensional\nrepresentation where similar pixels are pulled close together and dissimilar\npixels are pushed apart. This technique has shown promising results, but its\noriginal formulation is not computationally feasible for large images. We\npropose two improvements to the algorithm for computing PFE: first, we\nreformulate portions of the algorithm to enable various linear algebra\noperations to be performed in parallel; second, we propose utilizing an\niterative linear solver (preconditioned conjugate gradient) to quickly solve a\nlinear least-squares problem that occurs in the inner loop of a nested\niteration. With these two computational improvements, we show on a publicly\navailable image database that PFE can be sped up by an order of magnitude\nwithout sacrificing segmentation performance. Our results make this technique\nmore practical for use on large data sets, not only for image segmentation, but\nfor general data clustering problems.\n", "rewritten_text": "Piecewise Flat Embeddings (PFE) is a promising image segmentation technique that transforms image data into a lower-dimensional representation, grouping similar pixels.  However, its original implementation is computationally expensive for large images.  We present two key algorithmic improvements: parallelization of linear algebra operations and the use of a preconditioned conjugate gradient iterative solver for a key least-squares problem.  These enhancements result in a tenfold speedup on a public image dataset, without compromising segmentation accuracy. This makes PFE significantly more practical for large-scale image segmentation and general data clustering tasks.\n"}, "1511.06783": {"original_text": "  We present a novel dataset and a novel algorithm for recognizing activities\nof daily living (ADL) from a first-person wearable camera. Handled objects are\ncrucially important for egocentric ADL recognition. For specific examination of\nobjects related to users' actions separately from other objects in an\nenvironment, many previous works have addressed the detection of handled\nobjects in images captured from head-mounted and chest-mounted cameras.\nNevertheless, detecting handled objects is not always easy because they tend to\nappear small in images. They can be occluded by a user's body. As described\nherein, we mount a camera on a user's wrist. A wrist-mounted camera can capture\nhandled objects at a large scale, and thus it enables us to skip object\ndetection process. To compare a wrist-mounted camera and a head-mounted camera,\nwe also develop a novel and publicly available dataset that includes videos and\nannotations of daily activities captured simultaneously by both cameras.\nAdditionally, we propose a discriminative video representation that retains\nspatial and temporal information after encoding frame descriptors extracted by\nConvolutional Neural Networks (CNN).\n", "rewritten_text": "This paper introduces a new dataset and algorithm for recognizing activities of daily living (ADLs) using a wrist-mounted camera.  Because handled objects are key to ADL recognition, previous methods often focused on detecting these objects in head-mounted or chest-mounted camera footage, a challenging task due to small object size and occlusion.  Our wrist-mounted camera captures handled objects at a larger scale, eliminating the need for explicit object detection.  To compare wrist- and head-mounted perspectives, we've created a publicly available dataset with synchronized video and annotations from both.  Furthermore, we propose a novel video representation method that effectively encodes spatial and temporal information from CNN-extracted frame descriptors for improved ADL recognition.\n"}, "2305.11806": {"original_text": "  Neural metrics for machine translation evaluation, such as COMET, exhibit\nsignificant improvements in their correlation with human judgments, as compared\nto traditional metrics based on lexical overlap, such as BLEU. Yet, neural\nmetrics are, to a great extent, \"black boxes\" returning a single sentence-level\nscore without transparency about the decision-making process. In this work, we\ndevelop and compare several neural explainability methods and demonstrate their\neffectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our\nstudy reveals that these metrics leverage token-level information that can be\ndirectly attributed to translation errors, as assessed through comparison of\ntoken-level neural saliency maps with Multidimensional Quality Metrics (MQM)\nannotations and with synthetically-generated critical translation errors. To\nease future research, we release our code at:\nhttps://github.com/Unbabel/COMET/tree/explainable-metrics.\n", "rewritten_text": "While neural machine translation evaluation metrics like COMET significantly outperform traditional metrics (e.g., BLEU) in aligning with human judgment, their lack of transparency hinders understanding.  This work introduces and evaluates several neural explainability methods to interpret these advanced metrics.  Our findings demonstrate that these metrics utilize token-level information directly linked to translation errors, as confirmed by comparing token saliency maps with Multidimensional Quality Metrics (MQM) annotations and synthetic error data.  For reproducibility, our code is publicly available at https://github.com/Unbabel/COMET/tree/explainable-metrics.\n"}, "1906.11143": {"original_text": "  Accurate segmentation of the optic disc (OD) and cup (OC)in fundus images\nfrom different datasets is critical for glaucoma disease screening. The\ncross-domain discrepancy (domain shift) hinders the generalization of deep\nneural networks to work on different domain datasets.In this work, we present\nan unsupervised domain adaptation framework,called Boundary and Entropy-driven\nAdversarial Learning (BEAL), to improve the OD and OC segmentation performance,\nespecially on the ambiguous boundary regions. In particular, our proposed BEAL\nframe-work utilizes the adversarial learning to encourage the boundary\nprediction and mask probability entropy map (uncertainty map) of the target\ndomain to be similar to the source ones, generating more accurate boundaries\nand suppressing the high uncertainty predictions of OD and OC segmentation. We\nevaluate the proposed BEAL framework on two public retinal fundus image\ndatasets (Drishti-GS and RIM-ONE-r3), and the experiment results demonstrate\nthat our method outperforms the state-of-the-art unsupervised domain adaptation\nmethods. Codes will be available at https://github.com/EmmaW8/BEAL.\n", "rewritten_text": "Accurate optic disc (OD) and cup (OC) segmentation in fundus images is crucial for glaucoma screening.  However, differences between datasets hinder the generalizability of deep learning models.  This paper introduces Boundary and Entropy-driven Adversarial Learning (BEAL), an unsupervised domain adaptation framework that improves OD/OC segmentation, particularly at ambiguous boundaries.  BEAL uses adversarial learning to align boundary predictions and uncertainty maps between source and target domains, resulting in more precise boundaries and reduced uncertainty.  Evaluated on Drishti-GS and RIM-ONE-r3 datasets, BEAL outperforms existing unsupervised domain adaptation methods. Code is available at https://github.com/EmmaW8/BEAL.\n"}, "2108.08109": {"original_text": "  Illustrations are an essential transmission instrument. For an historian, the\nfirst step in studying their evolution in a corpus of similar manuscripts is to\nidentify which ones correspond to each other. This image collation task is\ndaunting for manuscripts separated by many lost copies, spreading over\ncenturies, which might have been completely re-organized and greatly modified\nto adapt to novel knowledge or belief and include hundreds of illustrations.\nOur contributions in this paper are threefold. First, we introduce the task of\nillustration collation and a large annotated public dataset to evaluate\nsolutions, including 6 manuscripts of 2 different texts with more than 2 000\nillustrations and 1 200 annotated correspondences. Second, we analyze state of\nthe art similarity measures for this task and show that they succeed in simple\ncases but struggle for large manuscripts when the illustrations have undergone\nvery significant changes and are discriminated only by fine details. Finally,\nwe show clear evidence that significant performance boosts can be expected by\nexploiting cycle-consistent correspondences. Our code and data are available on\nhttp://imagine.enpc.fr/~shenx/ImageCollation.\n", "rewritten_text": "Historical illustrations are crucial for understanding manuscript evolution.  Identifying corresponding illustrations across numerous, widely separated manuscripts\u2014potentially altered and reorganized over centuries\u2014presents a significant challenge. This paper addresses this \"illustration collation\" task in three ways:  (1) We introduce the task and a large, publicly available annotated dataset (6 manuscripts, 2 texts, >2000 illustrations, >1200 annotated correspondences). (2) We evaluate existing similarity measures, finding them effective for simple cases but inadequate for heavily modified illustrations in large manuscripts. (3) We demonstrate that leveraging cycle-consistent correspondences significantly improves performance.  Our code and data are available at http://imagine.enpc.fr/~shenx/ImageCollation.\n"}, "2204.01062": {"original_text": "  Several popular computer vision (CV) datasets, specifically employed for\nObject Detection (OD) in autonomous driving tasks exhibit biases due to a range\nof factors including weather and lighting conditions. These biases may impair a\nmodel's generalizability, rendering it ineffective for OD in novel and unseen\ndatasets. Especially, in autonomous driving, it may prove extremely high risk\nand unsafe for the vehicle and its surroundings. This work focuses on\nunderstanding these datasets better by identifying such \"good-weather\" bias.\nMethods to mitigate such bias which allows the OD models to perform better and\nimprove the robustness are also demonstrated. A simple yet effective OD\nframework for studying bias mitigation is proposed. Using this framework, the\nperformance on popular datasets is analyzed and a significant difference in\nmodel performance is observed. Additionally, a knowledge transfer technique and\na synthetic image corruption technique are proposed to mitigate the identified\nbias. Finally, using the DAWN dataset, the findings are validated on the OD\ntask, demonstrating the effectiveness of our techniques in mitigating\nreal-world \"good-weather\" bias. The experiments show that the proposed\ntechniques outperform baseline methods by averaged fourfold improvement.\n", "rewritten_text": "This research investigates \"good-weather bias\" in popular computer vision datasets used for object detection in autonomous driving.  This bias, stemming from factors like weather and lighting, hinders model generalizability and poses significant safety risks.  We propose a novel framework for analyzing and mitigating this bias, employing knowledge transfer and synthetic image corruption techniques.  Experiments on standard datasets, validated using the DAWN dataset, demonstrate a fourfold performance improvement over baseline methods, highlighting the effectiveness of our approach in enhancing the robustness and reliability of object detection models in real-world, varied conditions.\n"}, "2302.01097": {"original_text": "  Tree kernels have been proposed to be used in many areas as the automatic\nlearning of natural language applications. In this paper, we propose a new\nlinear time algorithm based on the concept of weighted tree automata for\nSubTree kernel computation. First, we introduce a new class of weighted tree\nautomata, called Root-Weighted Tree Automata, and their associated formal tree\nseries. Then we define, from this class, the SubTree automata that represent\ncompact computational models for finite tree languages. This allows us to\ndesign a theoretically guaranteed linear-time algorithm for computing the\nSubTree Kernel based on weighted tree automata intersection. The key idea\nbehind the proposed algorithm is to replace DAG reduction and nodes sorting\nsteps used in previous approaches by states equivalence classes computation\nallowed in the weighted tree automata approach. Our approach has three major\nadvantages: it is output-sensitive, it is free sensitive from the tree types\n(ordered trees versus unordered trees), and it is well adapted to any\nincremental tree kernel based learning methods. Finally, we conduct a variety\nof comparative experiments on a wide range of synthetic tree languages datasets\nadapted for a deep algorithm analysis. The obtained results show that the\nproposed algorithm outperforms state-of-the-art methods.\n", "rewritten_text": "This paper presents a novel linear-time algorithm for SubTree kernel computation, leveraging a new class of weighted tree automata called Root-Weighted Tree Automata.  These automata provide compact representations of finite tree languages, enabling efficient computation via intersection.  Unlike previous methods relying on DAG reduction and node sorting, our algorithm utilizes state equivalence class computation. This results in an output-sensitive algorithm, independent of tree ordering (ordered vs. unordered), and readily adaptable to incremental tree kernel learning.  Extensive experiments on synthetic datasets demonstrate superior performance compared to existing state-of-the-art algorithms.\n"}, "2404.10877": {"original_text": "  In this paper, we aim to generate text classification data given arbitrary\nclass definitions (i.e., user instruction), so one can train a small text\nclassifier without any human annotation or raw corpus. Compared with pioneer\nattempts, our proposed Incubator is the first framework that can handle\ncomplicated and even mutually dependent classes (e.g., \"TED Talk given by\nEducator\" and \"Other\"). Specifically, Incubator is an LLM firstly tuned on the\ninstruction-to-data mappings that we obtained from classification datasets and\ndescriptions on HuggingFace together with in-context augmentation by GPT-4. We\nthen refine Incubator by learning on the cluster centers of semantic textual\nembeddings to emphasize the uniformity and semantic diversity in generations.\nWe compare Incubator on various classification tasks with strong baselines such\nas direct LLM-based inference and training data generation by prompt\nengineering. Experiments show Incubator is able to (1) perform well on\ntraditional benchmarks, (2) take label dependency and user preference into\nconsideration, and (3) enable logical text mining by incubating multiple\nclassifiers.\n", "rewritten_text": "This paper introduces Incubator, a novel framework for generating text classification datasets from user-defined classes, eliminating the need for human annotation or large corpora.  Unlike previous methods, Incubator handles complex, interdependent class definitions.  It leverages an LLM fine-tuned on instruction-data mappings from HuggingFace datasets, augmented with GPT-4 in-context learning.  Further refinement uses semantic embedding clustering to ensure generated data uniformity and semantic diversity.  Comparative experiments against strong baselines (including direct LLM inference and prompt-engineered data generation) demonstrate Incubator's superior performance on standard benchmarks, its ability to manage label dependencies and user preferences, and its capacity to facilitate logical text mining through the simultaneous training of multiple classifiers.\n"}, "2210.09782": {"original_text": "  This paper focuses on developing a more effective method of hierarchical\npropagation for semi-supervised Video Object Segmentation (VOS). Based on\nvision transformers, the recently-developed Associating Objects with\nTransformers (AOT) approach introduces hierarchical propagation into VOS and\nhas shown promising results. The hierarchical propagation can gradually\npropagate information from past frames to the current frame and transfer the\ncurrent frame feature from object-agnostic to object-specific. However, the\nincrease of object-specific information will inevitably lead to the loss of\nobject-agnostic visual information in deep propagation layers. To solve such a\nproblem and further facilitate the learning of visual embeddings, this paper\nproposes a Decoupling Features in Hierarchical Propagation (DeAOT) approach.\nFirstly, DeAOT decouples the hierarchical propagation of object-agnostic and\nobject-specific embeddings by handling them in two independent branches.\nSecondly, to compensate for the additional computation from dual-branch\npropagation, we propose an efficient module for constructing hierarchical\npropagation, i.e., Gated Propagation Module, which is carefully designed with\nsingle-head attention. Extensive experiments show that DeAOT significantly\noutperforms AOT in both accuracy and efficiency. On YouTube-VOS, DeAOT can\nachieve 86.0% at 22.4fps and 82.0% at 53.4fps. Without test-time augmentations,\nwe achieve new state-of-the-art performance on four benchmarks, i.e.,\nYouTube-VOS (86.2%), DAVIS 2017 (86.2%), DAVIS 2016 (92.9%), and VOT 2020\n(0.622). Project page: https://github.com/z-x-yang/AOT.\n", "rewritten_text": "This paper presents DeAOT, a novel approach to semi-supervised video object segmentation (VOS) that improves upon the existing Associating Objects with Transformers (AOT) method.  AOT uses hierarchical propagation, transferring information between video frames, but suffers from the loss of object-agnostic information in deeper layers. DeAOT addresses this by decoupling the propagation of object-agnostic and object-specific features into separate branches.  Furthermore, a new Gated Propagation Module, employing single-head attention, efficiently handles this dual-branch propagation.  Extensive experiments demonstrate DeAOT's superior accuracy and efficiency compared to AOT, achieving state-of-the-art results on YouTube-VOS (86.2% without test-time augmentation), DAVIS 2017 (86.2%), DAVIS 2016 (92.9%), and VOT 2020 (0.622).  DeAOT also achieves 86.0% accuracy at 22.4fps and 82.0% at 53.4fps on YouTube-VOS.  Project page: https://github.com/z-x-yang/AOT.\n"}, "2406.18173": {"original_text": "  Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases.\n", "rewritten_text": "Large language models struggle with long texts due to limited context window sizes.  This paper presents UIO-LLMs, a novel method for training memory-enhanced transformers to handle significantly longer contexts.  UIO-LLMs use an encoder-decoder architecture with shared weights, incrementally processing text segments and storing information in memory.  By framing this as a fully-connected recurrent neural network and employing Truncated Backpropagation Through Time (TBPTT) with unbiased incremental optimization, UIO-LLMs efficiently manage long sequences.  This approach achieves a substantial increase in context window size (e.g., from 4K to 100K tokens for Llama2-7b-chat with only a 2% parameter increase) while maintaining near-linear inference cost.\n"}, "2306.07713": {"original_text": "  Segment anything model (SAM), as the name suggests, is claimed to be capable\nof cutting out any object and demonstrates impressive zero-shot transfer\nperformance with the guidance of prompts. However, there is currently a lack of\ncomprehensive evaluation regarding its robustness under various corruptions.\nUnderstanding the robustness of SAM across different corruption scenarios is\ncrucial for its real-world deployment. Prior works show that SAM is biased\ntowards texture (style) rather than shape, motivated by which we start by\ninvestigating its robustness against style transfer, which is synthetic\ncorruption. Following by interpreting the effects of synthetic corruption as\nstyle changes, we proceed to conduct a comprehensive evaluation for its\nrobustness against 15 types of common corruption. These corruptions mainly fall\ninto categories such as digital, noise, weather, and blur, and within each\ncorruption category, we explore 5 severity levels to simulate real-world\ncorruption scenarios. Beyond the corruptions, we further assess the robustness\nof SAM against local occlusion and local adversarial patch attacks. To the best\nof our knowledge, our work is the first of its kind to evaluate the robustness\nof SAM under style change, local occlusion, and local adversarial patch\nattacks. Given that patch attacks visible to human eyes are easily detectable,\nwe further assess its robustness against global adversarial attacks that are\nimperceptible to human eyes. Overall, this work provides a comprehensive\nempirical study of the robustness of SAM, evaluating its performance under\nvarious corruptions and extending the assessment to critical aspects such as\nlocal occlusion, local adversarial patch attacks, and global adversarial\nattacks. These evaluations yield valuable insights into the practical\napplicability and effectiveness of SAM in addressing real-world challenges.\n", "rewritten_text": "The Segment Anything Model (SAM) boasts impressive zero-shot object segmentation capabilities, but its robustness under real-world conditions remains unexplored.  This study comprehensively evaluates SAM's resilience to various corruptions, including 15 common image corruptions (digital, noise, weather, blur) at five severity levels each, style transfer, local occlusion, and local and global adversarial attacks.  Unlike previous work, we specifically investigate SAM's vulnerability to style changes and adversarial attacks, providing the first such analysis. Our findings offer crucial insights into SAM's practical limitations and potential for real-world deployment.\n"}, "2103.1399": {"original_text": "  A fundamental challenge faced by existing Fine-Grained Sketch-Based Image\nRetrieval (FG-SBIR) models is the data scarcity -- model performances are\nlargely bottlenecked by the lack of sketch-photo pairs. Whilst the number of\nphotos can be easily scaled, each corresponding sketch still needs to be\nindividually produced. In this paper, we aim to mitigate such an upper-bound on\nsketch data, and study whether unlabelled photos alone (of which they are many)\ncan be cultivated for performances gain. In particular, we introduce a novel\nsemi-supervised framework for cross-modal retrieval that can additionally\nleverage large-scale unlabelled photos to account for data scarcity. At the\ncentre of our semi-supervision design is a sequential photo-to-sketch\ngeneration model that aims to generate paired sketches for unlabelled photos.\nImportantly, we further introduce a discriminator guided mechanism to guide\nagainst unfaithful generation, together with a distillation loss based\nregularizer to provide tolerance against noisy training samples. Last but not\nleast, we treat generation and retrieval as two conjugate problems, where a\njoint learning procedure is devised for each module to mutually benefit from\neach other. Extensive experiments show that our semi-supervised model yields\nsignificant performance boost over the state-of-the-art supervised\nalternatives, as well as existing methods that can exploit unlabelled photos\nfor FG-SBIR.\n", "rewritten_text": "Fine-grained sketch-based image retrieval (FG-SBIR) suffers from a critical lack of training data: creating sketch-photo pairs is laborious.  This paper addresses this limitation by proposing a novel semi-supervised framework that leverages abundant unlabeled photos.  The core of our approach is a sequential photo-to-sketch generation model, guided by a discriminator to ensure fidelity and a distillation loss to handle noisy data.  We jointly train the generation and retrieval models, allowing them to mutually improve performance.  Extensive experiments demonstrate significant improvements over existing supervised and semi-supervised FG-SBIR methods.\n"}, "1412.7854": {"original_text": "  Traditional object recognition approaches apply feature extraction, part\ndeformation handling, occlusion handling and classification sequentially while\nthey are independent from each other. Ouyang and Wang proposed a model for\njointly learning of all of the mentioned processes using one deep neural\nnetwork. We utilized, and manipulated their toolbox in order to apply it in car\ndetection scenarios where it had not been tested. Creating a single deep\narchitecture from these components, improves the interaction between them and\ncan enhance the performance of the whole system. We believe that the approach\ncan be used as a general purpose object detection toolbox. We tested the\nalgorithm on UIUC car dataset, and achieved an outstanding result. The accuracy\nof our method was 97 % while the previously reported results showed an accuracy\nof up to 91 %. We strongly believe that having an experiment on a larger\ndataset can show the advantage of using deep models over shallow ones.\n", "rewritten_text": "Unlike traditional sequential object recognition methods, which treat feature extraction, part deformation handling, occlusion handling, and classification as independent steps, Ouyang and Wang's model jointly learns these processes within a single deep neural network.  We adapted and applied this model to car detection, a previously untested domain.  This unified deep architecture fosters improved interaction between components, leading to enhanced performance.  Our results on the UIUC car dataset demonstrate a significant improvement, achieving 97% accuracy compared to the previous best of 91%.  Further testing on larger datasets would likely further highlight the superiority of this deep learning approach over traditional shallow methods.\n"}, "1905.03672": {"original_text": "  In this paper, we are interested in boosting the representation capability of\nconvolution neural networks which utilizing the inverted residual structure.\nBased on the success of Inverted Residual structure[Sandler et al. 2018] and\nInterleaved Low-Rank Group Convolutions[Sun et al. 2018], we rethink this two\npattern of neural network structure, rather than NAS(Neural architecture\nsearch) method[Zoph and Le 2017; Pham et al. 2018; Liu et al. 2018b], we\nintroduce uneven point-wise group convolution, which provide a novel search\nspace for designing basic blocks to obtain better trade-off between\nrepresentation capability and computational cost. Meanwhile, we propose two\nnovel information flow patterns that will enable cross-group information flow\nfor multiple group convolution layers with and without any channel\npermute/shuffle operation. Dense experiments on image classification task show\nthat our proposed model, named Seesaw-Net, achieves state-of-the-art(SOTA)\nperformance with limited computation and memory cost. Our code will be\nopen-source and available together with pre-trained models.\n", "rewritten_text": "This paper improves the representational power of convolutional neural networks (CNNs) using inverted residual structures.  Instead of relying on neural architecture search (NAS), we propose a novel approach: uneven point-wise group convolutions. This introduces a new search space for designing efficient building blocks that balance representational capability and computational cost.  Furthermore, we introduce two novel information flow patterns facilitating cross-group information exchange in multiple group convolution layers, with and without channel shuffling.  Extensive image classification experiments demonstrate that our resulting model, Seesaw-Net, achieves state-of-the-art performance with reduced computational and memory overhead.  The code and pre-trained models will be publicly released.\n"}, "1912.1163": {"original_text": "  Person re-identification has attracted many researchers' attention for its\nwide application, but it is still a very challenging task because only part of\nthe image information can be used for personnel matching. Most of current\nmethods uses CNN to learn to embeddings that can capture semantic similarity\ninformation among data points. Many of the state-of-the-arts methods use\ncomplex network structures with multiple branches that fuse multiple features\nwhile training or testing, using classification loss, Triplet loss or a\ncombination of the two as loss function. However, the method that using Triplet\nloss as loss function converges slowly, and the method in which pull features\nof the same class as close as possible in features space leads to poor feature\nstability. This paper will combine the ranking motivated structured loss,\nproposed a new metric learning loss function that make the features of the same\nclass are sparsely distributed into the range of small hyperspheres and the\nfeatures of different classes are uniformly distributed at a clearly angle. And\nadopted a new single-branch network structure that only using global feature\ncan also get great performance. The validity of our method is verified on the\nMarket1501 and DukeMTMC-ReID person re-identification datasets. Finally\nacquires 90.9% rank-1 accuracy and 80.8% mAP on DukeMTMC-reID, 95.3% rank-1\naccuracy and 88.7% mAP on Market1501. Codes and models are available in\nGithub.https://github.com/Qidian213/Ranked_Person_ReID.\n", "rewritten_text": "Person re-identification, while widely applicable, remains challenging due to limited usable image information for matching.  Current methods predominantly employ Convolutional Neural Networks (CNNs) to learn embeddings capturing semantic similarity.  Many state-of-the-art approaches utilize complex multi-branch networks, fusing multiple features and employing classification and/or triplet loss functions. However, triplet loss suffers from slow convergence and can lead to unstable features due to its focus on minimizing intra-class distances.\n\nThis paper introduces a novel metric learning loss function based on a ranking-motivated structured loss. This loss function promotes sparse distribution of same-class features within small hyperspheres and uniform distribution of different-class features at distinct angles.  Furthermore, we propose a simpler single-branch network architecture relying solely on global features, achieving strong performance.  Our method achieves 90.9% rank-1 accuracy and 80.8% mAP on DukeMTMC-ReID, and 95.3% rank-1 accuracy and 88.7% mAP on Market1501. Code and models are available on GitHub: [https://github.com/Qidian213/Ranked_Person_ReID](https://github.com/Qidian213/Ranked_Person_ReID).\n"}, "1907.1144": {"original_text": "  Pooling is one of the main elements in convolutional neural networks. The\npooling reduces the size of the feature map, enabling training and testing with\na limited amount of computation. This paper proposes a new pooling method named\nuniversal pooling. Unlike the existing pooling methods such as average pooling,\nmax pooling, and stride pooling with fixed pooling function, universal pooling\ngenerates any pooling function, depending on a given problem and dataset.\nUniversal pooling was inspired by attention methods and can be considered as a\nchannel-wise form of local spatial attention. Universal pooling is trained\njointly with the main network and it is shown that it includes the existing\npooling methods. Finally, when applied to two benchmark problems, the proposed\nmethod outperformed the existing pooling methods and performed with the\nexpected diversity, adapting to the given problem.\n", "rewritten_text": "Convolutional neural networks (CNNs) rely heavily on pooling to reduce feature map size and computational cost.  This paper introduces universal pooling, a novel pooling method that dynamically generates its pooling function based on the specific dataset and problem, unlike fixed-function methods like average, max, and stride pooling.  Inspired by attention mechanisms, universal pooling acts as a channel-wise local spatial attention mechanism, trained concurrently with the main network.  Experiments on two benchmark problems demonstrate its superior performance over existing methods, showcasing its adaptability and achieving expected diversity in its learned pooling functions.\n"}, "2406.17236": {"original_text": "  Although recent years have witnessed significant advancements in image\nediting thanks to the remarkable progress of text-to-image diffusion models,\nthe problem of non-rigid image editing still presents its complexities and\nchallenges. Existing methods often fail to achieve consistent results due to\nthe absence of unique identity characteristics. Thus, learning a personalized\nidentity prior might help with consistency in the edited results. In this\npaper, we explore a novel task: learning the personalized identity prior for\ntext-based non-rigid image editing. To address the problems in jointly learning\nprior and editing the image, we present LIPE, a two-stage framework designed to\ncustomize the generative model utilizing a limited set of images of the same\nsubject, and subsequently employ the model with learned prior for non-rigid\nimage editing. Experimental results demonstrate the advantages of our approach\nin various editing scenarios over past related leading methods in qualitative\nand quantitative ways.\n", "rewritten_text": "Text-to-image diffusion models have significantly advanced image editing, but non-rigid editing remains challenging due to inconsistencies stemming from a lack of personalized identity preservation.  This paper introduces LIPE, a novel two-stage framework that learns a personalized identity prior for improved text-based non-rigid image editing.  LIPE customizes a generative model using a limited set of subject images and then applies this learned prior to achieve more consistent editing results.  Experiments demonstrate LIPE's superior qualitative and quantitative performance compared to existing state-of-the-art methods across various editing scenarios.\n"}, "2406.12679": {"original_text": "  Large Language Models (LLMs) are increasingly being used in educational and\nlearning applications. Research has demonstrated that controlling for style, to\nfit the needs of the learner, fosters increased understanding, promotes\ninclusion, and helps with knowledge distillation. To understand the\ncapabilities and limitations of contemporary LLMs in style control, we\nevaluated five state-of-the-art models: GPT-3.5, GPT-4, GPT-4o, Llama-3, and\nMistral-instruct- 7B across two style control tasks. We observed significant\ninconsistencies in the first task, with model performances averaging between\n5th and 8th grade reading levels for tasks intended for first-graders, and\nstandard deviations up to 27.6. For our second task, we observed a\nstatistically significant improvement in performance from 0.02 to 0.26.\nHowever, we find that even without stereotypes in reference texts, LLMs often\ngenerated culturally insensitive content during their tasks. We provide a\nthorough analysis and discussion of the results.\n", "rewritten_text": "This study investigates the capabilities of five leading Large Language Models (LLMs) \u2013 GPT-3.5, GPT-4, GPT-4o, Llama-3, and Mistral-instruct-7B \u2013 to adapt their writing style to suit different learning needs.  While style control is crucial for effective educational applications, our evaluation across two tasks revealed significant inconsistencies.  In the first task (targeting first-graders), model performance varied widely, averaging between 5th and 8th-grade reading levels, with substantial variability.  Although the second task showed statistically significant improvement, ranging from 0.02 to 0.26,  all models exhibited culturally insensitive output, even without biased input data.  Our analysis provides a comprehensive examination of these findings and their implications.\n"}, "1907.06882": {"original_text": "  Majority of state-of-the-art monocular depth estimation methods are\nsupervised learning approaches. The success of such approaches heavily depends\non the high-quality depth labels which are expensive to obtain. Some recent\nmethods try to learn depth networks by leveraging unsupervised cues from\nmonocular videos which are easier to acquire but less reliable. In this paper,\nwe propose to resolve this dilemma by transferring knowledge from synthetic\nvideos with easily obtainable ground-truth depth labels. Due to the stylish\ndifference between synthetic and real images, we propose a\ntemporally-consistent domain adaptation (TCDA) approach that simultaneously\nexplores labels in the synthetic domain and temporal constraints in the videos\nto improve style transfer and depth prediction. Furthermore, we make use of the\nground-truth optical flow and pose information in the synthetic data to learn\nmoving mask and pose prediction networks. The learned moving masks can filter\nout moving regions that produces erroneous temporal constraints and the\nestimated poses provide better initializations for estimating temporal\nconstraints. Experimental results demonstrate the effectiveness of our method\nand comparable performance against state-of-the-art.\n", "rewritten_text": "Most current monocular depth estimation methods rely on supervised learning, requiring expensive high-quality depth labels.  Unsupervised methods using monocular videos are cheaper but less accurate.  This paper addresses this limitation by transferring knowledge from synthetic videos with readily available ground truth depth.  To bridge the visual gap between synthetic and real images, we introduce a temporally-consistent domain adaptation (TCDA) approach that leverages both synthetic labels and temporal video constraints.  We further utilize synthetic optical flow and pose information to train networks for moving mask and pose prediction, improving temporal constraint accuracy by filtering out moving regions and providing better initialization.  Our method achieves state-of-the-art performance.\n"}, "2104.14839": {"original_text": "  Recently, various neural encoder-decoder models pioneered by Seq2Seq\nframework have been proposed to achieve the goal of generating more abstractive\nsummaries by learning to map input text to output text. At a high level, such\nneural models can freely generate summaries without any constraint on the words\nor phrases used. Moreover, their format is closer to human-edited summaries and\noutput is more readable and fluent. However, the neural model's abstraction\nability is a double-edged sword. A commonly observed problem with the generated\nsummaries is the distortion or fabrication of factual information in the\narticle. This inconsistency between the original text and the summary has\ncaused various concerns over its applicability, and the previous evaluation\nmethods of text summarization are not suitable for this issue. In response to\nthe above problems, the current research direction is predominantly divided\ninto two categories, one is to design fact-aware evaluation metrics to select\noutputs without factual inconsistency errors, and the other is to develop new\nsummarization systems towards factual consistency. In this survey, we focus on\npresenting a comprehensive review of these fact-specific evaluation methods and\ntext summarization models.\n", "rewritten_text": "Recent neural encoder-decoder models, based on the Seq2Seq framework, generate more abstractive summaries by directly mapping input to output text.  This allows for greater fluency and readability, resembling human-written summaries. However, this abstractive ability can lead to factual inaccuracies and hallucinations in the generated summaries.  This inconsistency challenges existing evaluation methods and limits applicability.  Current research addresses this by focusing on two main approaches: developing fact-aware evaluation metrics to identify accurate summaries and creating new summarization models that prioritize factual consistency. This survey comprehensively reviews these fact-focused evaluation methods and summarization models.\n"}, "1701.01619": {"original_text": "  We present an approach to effectively use millions of images with noisy\nannotations in conjunction with a small subset of cleanly-annotated images to\nlearn powerful image representations. One common approach to combine clean and\nnoisy data is to first pre-train a network using the large noisy dataset and\nthen fine-tune with the clean dataset. We show this approach does not fully\nleverage the information contained in the clean set. Thus, we demonstrate how\nto use the clean annotations to reduce the noise in the large dataset before\nfine-tuning the network using both the clean set and the full set with reduced\nnoise. The approach comprises a multi-task network that jointly learns to clean\nnoisy annotations and to accurately classify images. We evaluate our approach\non the recently released Open Images dataset, containing ~9 million images,\nmultiple annotations per image and over 6000 unique classes. For the small\nclean set of annotations we use a quarter of the validation set with ~40k\nimages. Our results demonstrate that the proposed approach clearly outperforms\ndirect fine-tuning across all major categories of classes in the Open Image\ndataset. Further, our approach is particularly effective for a large number of\nclasses with wide range of noise in annotations (20-80% false positive\nannotations).\n", "rewritten_text": "This paper introduces a novel method for leveraging large, noisy image datasets alongside smaller, cleanly annotated datasets to build robust image representations.  Unlike traditional pre-training and fine-tuning approaches, which underutilize the clean data, our method first uses a multi-task network to reduce noise in the large dataset using the clean annotations.  This denoised dataset, combined with the clean dataset, is then used to fine-tune the network.  Evaluated on the Open Images dataset (\u22489 million images, 6000+ classes), using a clean subset of \u224840,000 images, our approach significantly outperforms standard fine-tuning, especially for classes with high annotation noise (20-80% false positives).\n"}, "1905.04215": {"original_text": "  We study the problem of unsupervised domain adaptation which aims to adapt\nmodels trained on a labeled source domain to a completely unlabeled target\ndomain. Recently, the cluster assumption has been applied to unsupervised\ndomain adaptation and achieved strong performance. One critical factor in\nsuccessful training of the cluster assumption is to impose the\nlocally-Lipschitz constraint to the model. Existing methods only impose the\nlocally-Lipschitz constraint around the training points while miss the other\nareas, such as the points in-between training data. In this paper, we address\nthis issue by encouraging the model to behave linearly in-between training\npoints. We propose a new regularization method called Virtual Mixup Training\n(VMT), which is able to incorporate the locally-Lipschitz constraint to the\nareas in-between training data. Unlike the traditional mixup model, our method\nconstructs the combination samples without using the label information,\nallowing it to apply to unsupervised domain adaptation. The proposed method is\ngeneric and can be combined with most existing models such as the recent\nstate-of-the-art model called VADA. Extensive experiments demonstrate that VMT\nsignificantly improves the performance of VADA on six domain adaptation\nbenchmark datasets. For the challenging task of adapting MNIST to SVHN, VMT can\nimprove the accuracy of VADA by over 30\\%. Code is available at\n\\url{https://github.com/xudonmao/VMT}.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) aims to transfer models trained on labeled data to unlabeled target domains.  Recent UDA methods leverage the cluster assumption, but their success hinges on enforcing a locally-Lipschitz constraint on the model.  Existing methods only enforce this constraint near training points, neglecting the space between them.  This paper introduces Virtual Mixup Training (VMT), a novel regularization technique that addresses this limitation by promoting linear model behavior between training points.  Unlike standard mixup, VMT doesn't rely on labels, making it suitable for UDA.  VMT is a general method compatible with existing models, and experiments on six benchmark datasets show significant performance improvements, particularly for the challenging MNIST-to-SVHN adaptation task (over 30% accuracy gain when combined with the state-of-the-art VADA model).  Code is available at [https://github.com/xudonmao/VMT](https://github.com/xudonmao/VMT).\n"}, "2311.17590": {"original_text": "  Achieving high synchronization in the synthesis of realistic, speech-driven\ntalking head videos presents a significant challenge. Traditional Generative\nAdversarial Networks (GAN) struggle to maintain consistent facial identity,\nwhile Neural Radiance Fields (NeRF) methods, although they can address this\nissue, often produce mismatched lip movements, inadequate facial expressions,\nand unstable head poses. A lifelike talking head requires synchronized\ncoordination of subject identity, lip movements, facial expressions, and head\nposes. The absence of these synchronizations is a fundamental flaw, leading to\nunrealistic and artificial outcomes. To address the critical issue of\nsynchronization, identified as the \"devil\" in creating realistic talking heads,\nwe introduce SyncTalk. This NeRF-based method effectively maintains subject\nidentity, enhancing synchronization and realism in talking head synthesis.\nSyncTalk employs a Face-Sync Controller to align lip movements with speech and\ninnovatively uses a 3D facial blendshape model to capture accurate facial\nexpressions. Our Head-Sync Stabilizer optimizes head poses, achieving more\nnatural head movements. The Portrait-Sync Generator restores hair details and\nblends the generated head with the torso for a seamless visual experience.\nExtensive experiments and user studies demonstrate that SyncTalk outperforms\nstate-of-the-art methods in synchronization and realism. We recommend watching\nthe supplementary video: https://ziqiaopeng.github.io/synctalk\n", "rewritten_text": "Creating realistic, speech-driven talking head videos is hampered by synchronization challenges.  Existing methods, like GANs and NeRFs, struggle to consistently maintain facial identity and accurate lip synchronization, often resulting in unnatural expressions and head poses.  To overcome this \"devil\" in the details, we introduce SyncTalk, a novel NeRF-based approach.  SyncTalk uses a Face-Sync Controller for precise lip movements, a 3D blendshape model for accurate facial expressions, and a Head-Sync Stabilizer for natural head poses.  Furthermore, a Portrait-Sync Generator seamlessly integrates hair and torso details.  Our results, supported by extensive experiments and user studies, demonstrate SyncTalk's superior synchronization and realism compared to state-of-the-art techniques.  See our supplementary video for a demonstration: https://ziqiaopeng.github.io/synctalk\n"}, "1901.07656": {"original_text": "  Word vector representations are well developed tools for various NLP and\nMachine Learning tasks and are known to retain significant semantic and\nsyntactic structure of languages. But they are prone to carrying and amplifying\nbias which can perpetrate discrimination in various applications. In this work,\nwe explore new simple ways to detect the most stereotypically gendered words in\nan embedding and remove the bias from them. We verify how names are masked\ncarriers of gender bias and then use that as a tool to attenuate bias in\nembeddings. Further, we extend this property of names to show how names can be\nused to detect other types of bias in the embeddings such as bias based on\nrace, ethnicity, and age.\n", "rewritten_text": "Word embeddings, while powerful tools for NLP and machine learning, often reflect and amplify societal biases, leading to discriminatory outcomes.  This work introduces novel, straightforward methods for identifying and mitigating gender bias in word embeddings, leveraging the observation that names act as significant carriers of such bias.  We demonstrate how this approach can be extended to detect and reduce biases related to race, ethnicity, and age.\n"}, "2210.02318": {"original_text": "  Recently, two-stage Deformable DETR introduced the query-based two-stage\nhead, a new type of two-stage head different from the region-based two-stage\nheads of classical detectors as Faster R-CNN. In query-based two-stage heads,\nthe second stage selects one feature per detection processed by a transformer,\ncalled the query, as opposed to pooling a rectangular grid of features\nprocessed by CNNs as in region-based detectors. In this work, we improve the\nquery-based head by improving the prior of the cross-attention operation with\nanchors, significantly speeding up the convergence while increasing its\nperformance. Additionally, we empirically show that by improving the\ncross-attention prior, auxiliary losses and iterative bounding box mechanisms\ntypically used by DETR-based detectors are no longer needed. By combining the\nbest of both the classical and the DETR-based detectors, our FQDet head peaks\nat 45.4 AP on the 2017 COCO validation set when using a ResNet-50+TPN backbone,\nonly after training for 12 epochs using the 1x schedule. We outperform other\nhigh-performing two-stage heads such as e.g. Cascade R-CNN, while using the\nsame backbone and while being computationally cheaper. Additionally, when using\nthe large ResNeXt-101-DCN+TPN backbone and multi-scale testing, our FQDet head\nachieves 52.9 AP on the 2017 COCO test-dev set after only 12 epochs of\ntraining. Code is released at https://github.com/CedricPicron/FQDet .\n", "rewritten_text": "This paper introduces FQDet, a novel two-stage detection head that improves upon the query-based approach of Deformable DETR.  Unlike region-based methods like Faster R-CNN, FQDet uses transformer queries, processing one feature per detection, rather than pooling features from a rectangular region.  By enhancing the cross-attention mechanism with anchors, FQDet significantly accelerates training convergence and boosts performance, eliminating the need for auxiliary losses and iterative bounding box refinement typically employed in DETR-based detectors.  Using a ResNet-50+TPN backbone, FQDet achieves a remarkable 45.4 AP on the COCO 2017 validation set after just 12 epochs of training (1x schedule), surpassing other high-performing two-stage detectors such as Cascade R-CNN with improved computational efficiency.  Furthermore, with a ResNeXt-101-DCN+TPN backbone and multi-scale testing, FQDet reaches 52.9 AP on the COCO 2017 test-dev set.  The code is publicly available at https://github.com/CedricPicron/FQDet.\n"}, "1411.634": {"original_text": "  While widely acknowledged as highly effective in computer vision, multi-label\nMRFs with non-convex priors are difficult to optimize. To tackle this, we\nintroduce an algorithm that iteratively approximates the original energy with\nan appropriately weighted surrogate energy that is easier to minimize. Our\nalgorithm guarantees that the original energy decreases at each iteration. In\nparticular, we consider the scenario where the global minimizer of the weighted\nsurrogate energy can be obtained by a multi-label graph cut algorithm, and show\nthat our algorithm then lets us handle of large variety of non-convex priors.\nWe demonstrate the benefits of our method over state-of-the-art MRF energy\nminimization techniques on stereo and inpainting problems.\n", "rewritten_text": "Multi-label Markov Random Fields (MRFs) with non-convex priors are powerful tools in computer vision, but their optimization is challenging.  We present a novel iterative algorithm that addresses this by successively minimizing weighted surrogate energies, each simpler to optimize than the original non-convex energy.  Our algorithm guarantees monotonic energy reduction.  Leveraging efficient multi-label graph cut algorithms for surrogate energy minimization, we effectively handle a wide range of non-convex priors.  Experiments on stereo and inpainting tasks demonstrate superior performance compared to existing MRF optimization methods.\n"}, "2009.02649": {"original_text": "  Causality visualization can help people understand temporal chains of events,\nsuch as messages sent in a distributed system, cause and effect in a historical\nconflict, or the interplay between political actors over time. However, as the\nscale and complexity of these event sequences grows, even these visualizations\ncan become overwhelming to use. In this paper, we propose the use of textual\nnarratives as a data-driven storytelling method to augment causality\nvisualization. We first propose a design space for how textual narratives can\nbe used to describe causal data. We then present results from a crowdsourced\nuser study where participants were asked to recover causality information from\ntwo causality visualizations--causal graphs and Hasse diagrams--with and\nwithout an associated textual narrative. Finally, we describe CAUSEWORKS, a\ncausality visualization system for understanding how specific interventions\ninfluence a causal model. The system incorporates an automatic textual\nnarrative mechanism based on our design space. We validate CAUSEWORKS through\ninterviews with experts who used the system for understanding complex events.\n", "rewritten_text": "This paper explores augmenting causality visualizations with data-driven textual narratives to improve comprehension of complex temporal event sequences.  We define a design space for integrating narratives into visualizations, then present a crowdsourced study comparing causal graph and Hasse diagram comprehension with and without accompanying narratives.  Finally, we introduce CAUSEWORKS, a system incorporating automated narrative generation based on our design space, and validate its effectiveness through expert interviews.  This system aids understanding of how interventions affect causal models, addressing the limitations of visualizations when dealing with large-scale, complex events.\n"}, "2204.07183": {"original_text": "  We propose an interactive approach for 3D instance segmentation, where users\ncan iteratively collaborate with a deep learning model to segment objects in a\n3D point cloud directly. Current methods for 3D instance segmentation are\ngenerally trained in a fully-supervised fashion, which requires large amounts\nof costly training labels, and does not generalize well to classes unseen\nduring training. Few works have attempted to obtain 3D segmentation masks using\nhuman interactions. Existing methods rely on user feedback in the 2D image\ndomain. As a consequence, users are required to constantly switch between 2D\nimages and 3D representations, and custom architectures are employed to combine\nmultiple input modalities. Therefore, integration with existing standard 3D\nmodels is not straightforward. The core idea of this work is to enable users to\ninteract directly with 3D point clouds by clicking on desired 3D objects of\ninterest~(or their background) to interactively segment the scene in an\nopen-world setting. Specifically, our method does not require training data\nfrom any target domain, and can adapt to new environments where no appropriate\ntraining sets are available. Our system continuously adjusts the object\nsegmentation based on the user feedback and achieves accurate dense 3D\nsegmentation masks with minimal human effort (few clicks per object). Besides\nits potential for efficient labeling of large-scale and varied 3D datasets, our\napproach, where the user directly interacts with the 3D environment, enables\nnew applications in AR/VR and human-robot interaction.\n", "rewritten_text": "This paper introduces an interactive approach to 3D instance segmentation that directly integrates user input with a deep learning model.  Unlike existing fully-supervised methods, which require extensive and costly labeled data and struggle with unseen classes, our approach allows users to iteratively segment objects within a 3D point cloud via simple clicks.  Unlike previous interactive methods that rely on 2D image feedback and complex multi-modal architectures, our system offers direct 3D interaction, eliminating the need for switching between 2D and 3D views and simplifying integration with existing 3D models.  This zero-shot learning approach adapts to new environments without requiring training data, achieving accurate, dense 3D segmentation masks with minimal user effort.  Beyond efficient large-scale data labeling, this direct 3D interaction opens up new possibilities for augmented/virtual reality and human-robot collaboration.\n"}, "2012.0536": {"original_text": "  Semantic aware reconstruction is more advantageous than geometric-only\nreconstruction for future robotic and AR/VR applications because it represents\nnot only where things are, but also what things are. Object-centric mapping is\na task to build an object-level reconstruction where objects are separate and\nmeaningful entities that convey both geometry and semantic information. In this\npaper, we present MOLTR, a solution to object-centric mapping using only\nmonocular image sequences and camera poses. It is able to localise, track, and\nreconstruct multiple objects in an online fashion when an RGB camera captures a\nvideo of the surrounding. Given a new RGB frame, MOLTR firstly applies a\nmonocular 3D detector to localise objects of interest and extract their shape\ncodes that represent the object shapes in a learned embedding space. Detections\nare then merged to existing objects in the map after data association. Motion\nstate (i.e. kinematics and the motion status) of each object is tracked by a\nmultiple model Bayesian filter and object shape is progressively refined by\nfusing multiple shape code. We evaluate localisation, tracking, and\nreconstruction on benchmarking datasets for indoor and outdoor scenes, and show\nsuperior performance over previous approaches.\n", "rewritten_text": "Object-centric mapping, crucial for advanced robotics and AR/VR, requires understanding both object location (\"where\") and identity (\"what\").  Unlike purely geometric methods, semantic awareness offers significant advantages.  This paper introduces MOLTR, a novel monocular object-centric mapping system.  Using only RGB video and camera poses, MOLTR performs online localization, tracking, and reconstruction of multiple objects.  It leverages a 3D detector to identify objects and extract shape codes, associating these with existing map objects.  A Bayesian filter tracks object motion, while shape refinement occurs through shape code fusion.  Benchmarking on indoor and outdoor datasets demonstrates MOLTR's superior performance compared to existing techniques.\n"}, "1605.06052": {"original_text": "  Similarity scores in face recognition represent the proximity between pairs\nof images as computed by a matching algorithm. Given a large set of images and\nthe proximities between all pairs, a similarity score space is defined. Cluster\nanalysis was applied to the similarity score space to develop various\ntaxonomies. Given the number of subjects in the dataset, we used hierarchical\nmethods to aggregate images of the same subject. We also explored the hierarchy\nabove and below the subject level, including clusters that reflect gender and\nethnicity. Evidence supports the existence of clustering by race, gender,\nsubject, and illumination condition.\n", "rewritten_text": "Face recognition algorithms generate similarity scores indicating the likeness between image pairs.  These scores form a similarity space, which we analyzed using cluster analysis to create taxonomies.  Employing hierarchical clustering methods, we grouped images of the same individual, and further investigated higher and lower-level groupings, revealing clusters based on gender, ethnicity, and illumination conditions.  Our findings demonstrate the presence of distinct clusters corresponding to race, gender, individual subjects, and lighting.\n"}, "2401.00653": {"original_text": "  Deceptive images can be shared in seconds with social networking services,\nposing substantial risks. Tampering traces, such as boundary artifacts and\nhigh-frequency information, have been significantly emphasized by massive\nnetworks in the Image Manipulation Localization (IML) field. However, they are\nprone to image post-processing operations, which limit the generalization and\nrobustness of existing methods. We present a novel Prompt-IML framework. We\nobserve that humans tend to discern the authenticity of an image based on both\nsemantic and high-frequency information, inspired by which, the proposed\nframework leverages rich semantic knowledge from pre-trained visual foundation\nmodels to assist IML. We are the first to design a framework that utilizes\nvisual foundation models specially for the IML task. Moreover, we design a\nFeature Alignment and Fusion module to align and fuse features of semantic\nfeatures with high-frequency features, which aims at locating tampered regions\nfrom multiple perspectives. Experimental results demonstrate that our model can\nachieve better performance on eight typical fake image datasets and outstanding\nrobustness.\n", "rewritten_text": "The rapid spread of manipulated images via social media presents significant challenges.  While existing Image Manipulation Localization (IML) methods focus on detecting tampering traces like artifacts, these are easily obscured by post-processing.  This paper introduces Prompt-IML, a novel framework that leverages the semantic understanding of pre-trained visual foundation models, a first-of-its-kind approach for IML.  By combining semantic information with traditional high-frequency analysis via a Feature Alignment and Fusion module, Prompt-IML achieves superior performance and robustness across eight benchmark datasets, mimicking human image authenticity assessment which considers both visual detail and overall context.\n"}, "1809.08495": {"original_text": "  Earlier work demonstrates the promise of deep-learning-based approaches for\npoint cloud segmentation; however, these approaches need to be improved to be\npractically useful. To this end, we introduce a new model SqueezeSegV2 that is\nmore robust to dropout noise in LiDAR point clouds. With improved model\nstructure, training loss, batch normalization and additional input channel,\nSqueezeSegV2 achieves significant accuracy improvement when trained on real\ndata. Training models for point cloud segmentation requires large amounts of\nlabeled point-cloud data, which is expensive to obtain. To sidestep the cost of\ncollection and annotation, simulators such as GTA-V can be used to create\nunlimited amounts of labeled, synthetic data. However, due to domain shift,\nmodels trained on synthetic data often do not generalize well to the real\nworld. We address this problem with a domain-adaptation training pipeline\nconsisting of three major components: 1) learned intensity rendering, 2)\ngeodesic correlation alignment, and 3) progressive domain calibration. When\ntrained on real data, our new model exhibits segmentation accuracy improvements\nof 6.0-8.6% over the original SqueezeSeg. When training our new model on\nsynthetic data using the proposed domain adaptation pipeline, we nearly double\ntest accuracy on real-world data, from 29.0% to 57.4%. Our source code and\nsynthetic dataset will be open-sourced.\n", "rewritten_text": "Deep learning shows promise for point cloud segmentation, but existing methods lack practical robustness.  This paper introduces SqueezeSegV2, a novel model significantly improving accuracy on real-world LiDAR data by enhancing model architecture, loss function, batch normalization, and input channels.  Addressing the high cost of real-world labeled data, we leverage synthetic data from GTA-V, mitigating the resulting domain gap via a three-stage domain adaptation pipeline: learned intensity rendering, geodesic correlation alignment, and progressive domain calibration.  SqueezeSegV2 achieves a 6.0-8.6% accuracy increase over its predecessor when trained on real data, and nearly doubles (from 29.0% to 57.4%) real-world accuracy when trained on synthetic data using our adaptation pipeline.  Our code and synthetic dataset are publicly available.\n"}, "2209.11214": {"original_text": "  Automatic tomato disease recognition from leaf images is vital to avoid crop\nlosses by applying control measures on time. Even though recent deep\nlearning-based tomato disease recognition methods with classical training\nprocedures showed promising recognition results, they demand large labelled\ndata and involve expensive training. The traditional deep learning models\nproposed for tomato disease recognition also consume high memory and storage\nbecause of a high number of parameters. While lightweight networks overcome\nsome of these issues to a certain extent, they continue to show low performance\nand struggle to handle imbalanced data. In this paper, a novel Siamese\nnetwork-based lightweight framework is proposed for automatic tomato leaf\ndisease recognition. This framework achieves the highest accuracy of 96.97% on\nthe tomato subset obtained from the PlantVillage dataset and 95.48% on the\nTaiwan tomato leaf disease dataset. Experimental results further confirm that\nthe proposed framework is effective with imbalanced and small data. The\nbackbone deep network integrated with this framework is lightweight with\napproximately 2.9629 million trainable parameters, which is way lower than\nexisting lightweight deep networks.\n", "rewritten_text": "Early detection of tomato diseases via leaf image analysis is crucial for preventing crop failure.  While deep learning offers promising results, existing methods require extensive labeled data, costly training, and significant computational resources due to their large parameter counts.  Even lightweight networks struggle with performance and imbalanced datasets.  This paper introduces a novel, lightweight Siamese network achieving 96.97% accuracy on a PlantVillage subset and 95.48% on a Taiwanese dataset.  Its efficiency stems from a significantly reduced parameter count (approximately 2.96 million), making it effective even with limited and imbalanced data.\n"}, "1912.01496": {"original_text": "  Stories are diverse and highly personalized, resulting in a large possible\noutput space for story generation. Existing end-to-end approaches produce\nmonotonous stories because they are limited to the vocabulary and knowledge in\na single training dataset. This paper introduces KG-Story, a three-stage\nframework that allows the story generation model to take advantage of external\nKnowledge Graphs to produce interesting stories. KG-Story distills a set of\nrepresentative words from the input prompts, enriches the word set by using\nexternal knowledge graphs, and finally generates stories based on the enriched\nword set. This distill-enrich-generate framework allows the use of external\nresources not only for the enrichment phase, but also for the distillation and\ngeneration phases. In this paper, we show the superiority of KG-Story for\nvisual storytelling, where the input prompt is a sequence of five photos and\nthe output is a short story. Per the human ranking evaluation, stories\ngenerated by KG-Story are on average ranked better than that of the\nstate-of-the-art systems. Our code and output stories are available at\nhttps://github.com/zychen423/KE-VIST.\n", "rewritten_text": "Story generation faces the challenge of producing diverse narratives due to the limitations of single training datasets.  This paper presents KG-Story, a three-stage framework leveraging external Knowledge Graphs (KGs) to overcome this limitation.  KG-Story first extracts key words from input prompts, then expands this vocabulary using KGs, and finally generates a story based on the enriched word set.  This framework integrates KG information throughout the entire process \u2013 from keyword selection to story generation.  Evaluated on visual storytelling (using five photos as input), KG-Story outperforms state-of-the-art methods in human-judged rankings.  The code and generated stories are available at https://github.com/zychen423/KE-VIST.\n"}, "2307.16825": {"original_text": "  With sufficient paired training samples, the supervised deep learning methods\nhave attracted much attention in image denoising because of their superior\nperformance. However, it is still very challenging to widely utilize the\nsupervised methods in real cases due to the lack of paired noisy-clean images.\nMeanwhile, most self-supervised denoising methods are ineffective as well when\napplied to the real-world denoising tasks because of their strict assumptions\nin applications. For example, as a typical method for self-supervised\ndenoising, the original blind spot network (BSN) assumes that the noise is\npixel-wise independent, which is much different from the real cases. To solve\nthis problem, we propose a novel self-supervised real image denoising framework\nnamed Sampling Difference As Perturbation (SDAP) based on Random Sub-samples\nGeneration (RSG) with a cyclic sample difference loss. Specifically, we dig\ndeeper into the properties of BSN to make it more suitable for real noise.\nSurprisingly, we find that adding an appropriate perturbation to the training\nimages can effectively improve the performance of BSN. Further, we propose that\nthe sampling difference can be considered as perturbation to achieve better\nresults. Finally we propose a new BSN framework in combination with our RSG\nstrategy. The results show that it significantly outperforms other\nstate-of-the-art self-supervised denoising methods on real-world datasets. The\ncode is available at https://github.com/p1y2z3/SDAP.\n", "rewritten_text": "Supervised deep learning excels at image denoising, but its reliance on paired noisy-clean image data limits real-world applicability.  Existing self-supervised methods, often making unrealistic assumptions (e.g., pixel-wise independent noise as in Blind Spot Networks \u2013 BSN), also fall short.  To address this, we introduce SDAP, a novel self-supervised framework leveraging Random Sub-samples Generation (RSG) and a cyclic sample difference loss.  By analyzing BSN's limitations, we discovered that strategically perturbing training images improves performance, leading us to propose using sampling differences as this perturbation.  Our enhanced BSN, incorporating RSG, significantly outperforms state-of-the-art self-supervised methods on real-world datasets.  Code is available at https://github.com/p1y2z3/SDAP.\n"}, "2409.16685": {"original_text": "  Integrating aerial imagery-based scene generation into applications like\nautonomous driving and gaming enhances realism in 3D environments, but\nchallenges remain in creating detailed content for occluded areas and ensuring\nreal-time, consistent rendering. In this paper, we introduce Skyeyes, a novel\nframework that can generate photorealistic sequences of ground view images\nusing only aerial view inputs, thereby creating a ground roaming experience.\nMore specifically, we combine a 3D representation with a view consistent\ngeneration model, which ensures coherence between generated images. This method\nallows for the creation of geometrically consistent ground view images, even\nwith large view gaps. The images maintain improved spatial-temporal coherence\nand realism, enhancing scene comprehension and visualization from aerial\nperspectives. To the best of our knowledge, there are no publicly available\ndatasets that contain pairwise geo-aligned aerial and ground view imagery.\nTherefore, we build a large, synthetic, and geo-aligned dataset using Unreal\nEngine. Both qualitative and quantitative analyses on this synthetic dataset\ndisplay superior results compared to other leading synthesis approaches. See\nthe project page for more results:\nhttps://chaoren2357.github.io/website-skyeyes/.\n", "rewritten_text": "Generating realistic 3D environments from aerial imagery is crucial for applications like autonomous driving and gaming, but accurately rendering occluded areas in real-time remains a challenge.  This paper presents Skyeyes, a novel framework that generates photorealistic ground-level image sequences from aerial views alone, enabling immersive ground-roaming experiences.  Skyeyes combines 3D scene representation with a view-consistent generation model, ensuring coherent and geometrically accurate images even across large viewing gaps.  This improved spatial-temporal coherence and realism enhances scene understanding.  Due to the lack of publicly available geo-aligned aerial and ground imagery datasets, we created a large synthetic dataset using Unreal Engine.  Our results, both qualitative and quantitative, demonstrate Skyeyes' superiority over existing methods.  See our project page for further details: https://chaoren2357.github.io/website-skyeyes/.\n"}, "2210.02844": {"original_text": "  In this paper, we explore the following question: Are synonym substitution\nattacks really synonym substitution attacks (SSAs)? We approach this question\nby examining how SSAs replace words in the original sentence and show that\nthere are still unresolved obstacles that make current SSAs generate invalid\nadversarial samples. We reveal that four widely used word substitution methods\ngenerate a large fraction of invalid substitution words that are ungrammatical\nor do not preserve the original sentence's semantics. Next, we show that the\nsemantic and grammatical constraints used in SSAs for detecting invalid word\nreplacements are highly insufficient in detecting invalid adversarial samples.\n", "rewritten_text": "This paper investigates the validity of synonym substitution attacks (SSAs).  By analyzing how SSAs replace words, we demonstrate that current methods frequently produce invalid adversarial examples due to unresolved limitations.  We show that four common word substitution techniques generate a significant number of ungrammatical or semantically inappropriate replacements. Furthermore, we find that existing semantic and grammatical constraints within SSAs are inadequate for reliably identifying these invalid attacks.\n"}, "2307.09769": {"original_text": "  Unsupervised domain adaptation (UDA) has increasingly gained interests for\nits capacity to transfer the knowledge learned from a labeled source domain to\nan unlabeled target domain. However, typical UDA methods require concurrent\naccess to both the source and target domain data, which largely limits its\napplication in medical scenarios where source data is often unavailable due to\nprivacy concern. To tackle the source data-absent problem, we present a novel\ntwo-stage source-free domain adaptation (SFDA) framework for medical image\nsegmentation, where only a well-trained source segmentation model and unlabeled\ntarget data are available during domain adaptation. Specifically, in the\nprototype-anchored feature alignment stage, we first utilize the weights of the\npre-trained pixel-wise classifier as source prototypes, which preserve the\ninformation of source features. Then, we introduce the bi-directional transport\nto align the target features with class prototypes by minimizing its expected\ncost. On top of that, a contrastive learning stage is further devised to\nutilize those pixels with unreliable predictions for a more compact target\nfeature distribution. Extensive experiments on a cross-modality medical\nsegmentation task demonstrate the superiority of our method in large domain\ndiscrepancy settings compared with the state-of-the-art SFDA approaches and\neven some UDA methods. Code is available at\nhttps://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA.\n", "rewritten_text": "Source-free domain adaptation (SFDA) offers a solution to the limitations of unsupervised domain adaptation (UDA) in medical imaging, where source data privacy concerns often prevent access.  This paper introduces a novel two-stage SFDA framework for medical image segmentation that operates solely with a pre-trained source model and unlabeled target data.  The first stage aligns target features with source prototypes (derived from the pre-trained model's weights) using bi-directional transport to minimize alignment cost.  The second stage employs contrastive learning to refine the target feature distribution, focusing on unreliable predictions.  Experiments on a cross-modality medical segmentation task demonstrate superior performance compared to existing SFDA and even some UDA methods, particularly in scenarios with significant domain discrepancies.  Code is available at https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA.\n"}, "1811.08015": {"original_text": "  This paper introduces the problem of automatic font pairing. Font pairing is\nan important design task that is difficult for novices. Given a font selection\nfor one part of a document (e.g., header), our goal is to recommend a font to\nbe used in another part (e.g., body) such that the two fonts used together look\nvisually pleasing. There are three main challenges in font pairing. First, this\nis a fine-grained problem, in which the subtle distinctions between fonts may\nbe important. Second, rules and conventions of font pairing given by human\nexperts are difficult to formalize. Third, font pairing is an asymmetric\nproblem in that the roles played by header and body fonts are not\ninterchangeable. To address these challenges, we propose automatic font pairing\nthrough learning visual relationships from large-scale human-generated font\npairs. We introduce a new database for font pairing constructed from millions\nof PDF documents available on the Internet. We propose two font pairing\nalgorithms: dual-space k-NN and asymmetric similarity metric learning (ASML).\nThese two methods automatically learn fine-grained relationships from\nlarge-scale data. We also investigate several baseline methods based on the\nrules from professional designers. Experiments and user studies demonstrate the\neffectiveness of our proposed dataset and methods.\n", "rewritten_text": "This paper tackles the challenging design problem of automatic font pairing, a task difficult for non-experts.  Given a font for one document section (e.g., header), our goal is to automatically recommend a visually complementary font for another (e.g., body).  This is challenging due to the subtle distinctions between fonts, the difficulty of formalizing expert design rules, and the asymmetrical relationship between font roles.  To overcome these challenges, we leverage a novel, large-scale dataset of human-paired fonts extracted from millions of online PDFs.  We propose two novel algorithms\u2014dual-space k-NN and asymmetric similarity metric learning (ASML)\u2014which learn fine-grained relationships from this data.  We compare these to baseline methods based on established design rules, demonstrating their effectiveness through experiments and user studies.\n"}, "2403.15119": {"original_text": "  Person re-identification (ReID) has made great strides thanks to the\ndata-driven deep learning techniques. However, the existing benchmark datasets\nlack diversity, and models trained on these data cannot generalize well to\ndynamic wild scenarios. To meet the goal of improving the explicit\ngeneralization of ReID models, we develop a new Open-World, Diverse,\nCross-Spatial-Temporal dataset named OWD with several distinct features. 1)\nDiverse collection scenes: multiple independent open-world and highly dynamic\ncollecting scenes, including streets, intersections, shopping malls, etc. 2)\nDiverse lighting variations: long time spans from daytime to nighttime with\nabundant illumination changes. 3) Diverse person status: multiple camera\nnetworks in all seasons with normal/adverse weather conditions and diverse\npedestrian appearances (e.g., clothes, personal belongings, poses, etc.). 4)\nProtected privacy: invisible faces for privacy critical applications. To\nimprove the implicit generalization of ReID, we further propose a Latent Domain\nExpansion (LDE) method to develop the potential of source data, which decouples\ndiscriminative identity-relevant and trustworthy domain-relevant features and\nimplicitly enforces domain-randomized identity feature space expansion with\nricher domain diversity to facilitate domain invariant representations. Our\ncomprehensive evaluations with most benchmark datasets in the community are\ncrucial for progress, although this work is far from the grand goal toward\nopen-world and dynamic wild applications.\n", "rewritten_text": "Deep learning has significantly advanced person re-identification (ReID), but existing datasets lack diversity, hindering real-world performance.  To address this, we introduce OWD, a new open-world, diverse, cross-spatial-temporal ReID dataset featuring diverse scenes (streets, malls, etc.), lighting conditions (day/night), weather, and pedestrian appearances (clothing, poses), all while protecting privacy through facial anonymization.  Furthermore, we propose Latent Domain Expansion (LDE), a method to improve model generalization by decoupling identity and domain features, expanding the feature space, and promoting domain-invariant representations.  Extensive benchmarking against existing datasets demonstrates progress towards robust, open-world ReID, though significant challenges remain.\n"}, "1906.03731": {"original_text": "  Attention mechanisms have recently boosted performance on a range of NLP\ntasks. Because attention layers explicitly weight input components'\nrepresentations, it is also often assumed that attention can be used to\nidentify information that models found important (e.g., specific contextualized\nword tokens). We test whether that assumption holds by manipulating attention\nweights in already-trained text classification models and analyzing the\nresulting differences in their predictions. While we observe some ways in which\nhigher attention weights correlate with greater impact on model predictions, we\nalso find many ways in which this does not hold, i.e., where gradient-based\nrankings of attention weights better predict their effects than their\nmagnitudes. We conclude that while attention noisily predicts input components'\noverall importance to a model, it is by no means a fail-safe indicator.\n", "rewritten_text": "Recent advancements in NLP leverage attention mechanisms to improve performance.  While attention weights are often interpreted as indicators of a model's focus on specific input elements, our experiments challenge this assumption.  By manipulating attention weights in pre-trained text classification models, we found a weak correlation between weight magnitude and prediction impact.  Gradient-based rankings of attention weights proved a more reliable predictor of influence.  Therefore, although attention provides some insight into model behavior, it's an unreliable measure of input importance.\n"}, "1611.08134": {"original_text": "  Color based re-identification methods usually rely on a distance function to\nmeasure the similarity between individuals. In this paper we study the behavior\nof several histogram distance measures in different color spaces. We wonder\nwhether there is a particular histogram distance measure better than others,\nlikewise also, if there is a color space that present better discrimination\nfeatures. Several experiments are designed and evaluated in several images to\nobtain measures against various color spaces. We test in several image\ndatabases. A measure ranking is generated to calculate the area under the CMC,\nthis area is the indicator used to evaluate which distance measure and color\nspace present the best performance for the considered databases. Also, other\nparameters such as the image division in horizontal stripes and number of\nhistogram bins, have been studied.\n", "rewritten_text": "This paper investigates the performance of various histogram distance measures and color spaces for color-based person re-identification.  We compare several distance metrics across different color spaces to determine which combination yields the best re-identification accuracy.  Experiments using multiple image databases evaluate the impact of factors such as the number of histogram bins and image partitioning (horizontal stripes).  Performance is assessed using the area under the Cumulative Matching Characteristic (CMC) curve, ranking the methods and identifying the optimal color space and distance measure for improved re-identification.\n"}, "2003.05065": {"original_text": "  For many of the physical phenomena around us, we have developed sophisticated\nmodels explaining their behavior. Nevertheless, measuring physical properties\nfrom visual observations is challenging due to the high number of causally\nunderlying physical parameters -- including material properties and external\nforces. In this paper, we propose to measure latent physical properties for\ncloth in the wind without ever having seen a real example before. Our solution\nis an iterative refinement procedure with simulation at its core. The algorithm\ngradually updates the physical model parameters by running a simulation of the\nobserved phenomenon and comparing the current simulation to a real-world\nobservation. The correspondence is measured using an embedding function that\nmaps physically similar examples to nearby points. We consider a case study of\ncloth in the wind, with curling flags as our leading example -- a seemingly\nsimple phenomena but physically highly involved. Based on the physics of cloth\nand its visual manifestation, we propose an instantiation of the embedding\nfunction. For this mapping, modeled as a deep network, we introduce a spectral\nlayer that decomposes a video volume into its temporal spectral power and\ncorresponding frequencies. Our experiments demonstrate that the proposed method\ncompares favorably to prior work on the task of measuring cloth material\nproperties and external wind force from a real-world video.\n", "rewritten_text": "We've developed accurate models for many physical phenomena, but visually measuring their properties remains difficult due to numerous underlying factors.  This paper presents a novel method for inferring latent physical properties, specifically the material properties and wind forces affecting a flag, without prior real-world examples.  Our iterative approach uses simulation, refining model parameters by comparing simulated and real-world video observations.  Similarity is assessed via an embedding function, implemented as a deep neural network with a spectral layer that analyzes temporal frequencies in video data.  This method, tested on the complex dynamics of a flag in the wind, outperforms existing techniques for estimating cloth material properties and wind forces from video.\n"}, "2408.14860": {"original_text": "  This paper presents DiffSurf, a transformer-based denoising diffusion model\nfor generating and reconstructing 3D surfaces. Specifically, we design a\ndiffusion transformer architecture that predicts noise from noisy 3D surface\nvertices and normals. With this architecture, DiffSurf is able to generate 3D\nsurfaces in various poses and shapes, such as human bodies, hands, animals and\nman-made objects. Further, DiffSurf is versatile in that it can address various\n3D downstream tasks including morphing, body shape variation and 3D human mesh\nfitting to 2D keypoints. Experimental results on 3D human model benchmarks\ndemonstrate that DiffSurf can generate shapes with greater diversity and higher\nquality than previous generative models. Furthermore, when applied to the task\nof single-image 3D human mesh recovery, DiffSurf achieves accuracy comparable\nto prior techniques at a near real-time rate.\n", "rewritten_text": "DiffSurf, a novel transformer-based denoising diffusion model, generates and reconstructs high-quality 3D surfaces.  By predicting noise from noisy vertex and normal data, DiffSurf produces diverse 3D models of humans, animals, and objects.  Its versatility extends to applications like shape morphing, body variation modeling, and real-time 2D-to-3D human mesh fitting.  Benchmark results show DiffSurf surpasses existing generative models in both diversity and quality, achieving near real-time accuracy comparable to state-of-the-art methods in single-image 3D human mesh recovery.\n"}, "2111.1341": {"original_text": "  Manual annotation of medical images is highly subjective, leading to\ninevitable and huge annotation biases. Deep learning models may surpass human\nperformance on a variety of tasks, but they may also mimic or amplify these\nbiases. Although we can have multiple annotators and fuse their annotations to\nreduce stochastic errors, we cannot use this strategy to handle the bias caused\nby annotators' preferences. In this paper, we highlight the issue of\nannotator-related biases on medical image segmentation tasks, and propose a\nPreference-involved Annotation Distribution Learning (PADL) framework to\naddress it from the perspective of disentangling an annotator's preference from\nstochastic errors using distribution learning so as to produce not only a meta\nsegmentation but also the segmentation possibly made by each annotator. Under\nthis framework, a stochastic error modeling (SEM) module estimates the meta\nsegmentation map and average stochastic error map, and a series of human\npreference modeling (HPM) modules estimate each annotator's segmentation and\nthe corresponding stochastic error. We evaluated our PADL framework on two\nmedical image benchmarks with different imaging modalities, which have been\nannotated by multiple medical professionals, and achieved promising performance\non all five medical image segmentation tasks.\n", "rewritten_text": "Medical image annotation suffers from significant subjective biases introduced by human annotators, which deep learning models can inadvertently perpetuate or exacerbate. While multiple annotators can mitigate random errors, inherent biases remain.  This paper addresses this problem by introducing a Preference-involved Annotation Distribution Learning (PADL) framework. PADL disentangles annotator bias from random errors using distribution learning, generating a consensus segmentation (\"meta-segmentation\") alongside individual annotator-specific segmentations.  This is achieved through a stochastic error modeling (SEM) module and multiple human preference modeling (HPM) modules.  Evaluated on two diverse medical image datasets with multi-annotator ground truth, PADL demonstrates promising results across five segmentation tasks.\n"}, "2312.06598": {"original_text": "  Early action recognition is an important and challenging problem that enables\nthe recognition of an action from a partially observed video stream where the\nactivity is potentially unfinished or even not started. In this work, we\npropose a novel model that learns a prototypical representation of the full\naction for each class and uses it to regularize the architecture and the visual\nrepresentations of the partial observations. Our model is very simple in design\nand also efficient. We decompose the video into short clips, where a visual\nencoder extracts features from each clip independently. Later, a decoder\naggregates together in an online fashion features from all the clips for the\nfinal class prediction. During training, for each partial observation, the\nmodel is jointly trained to both predict the label as well as the action\nprototypical representation which acts as a regularizer. We evaluate our method\non multiple challenging real-world datasets and outperform the current\nstate-of-the-art by a significant margin. For example, on early recognition\nobserving only the first 10% of each video, our method improves the SOTA by\n+2.23 Top-1 accuracy on Something-Something-v2, +3.55 on UCF-101, +3.68 on\nSSsub21, and +5.03 on EPIC-Kitchens-55, where prior work used either\nmulti-modal inputs (e.g. optical-flow) or batched inference. Finally, we also\npresent exhaustive ablation studies to motivate the design choices we made, as\nwell as gather insights regarding what our model is learning semantically.\n", "rewritten_text": "This paper introduces a novel, efficient model for early action recognition\u2014identifying actions from incomplete video segments.  The model learns a prototypical representation of each action, using it to regularize both the architecture and the features extracted from short video clips.  A visual encoder processes each clip independently, and an online decoder aggregates these features for final classification.  Joint training predicts both the action label and its prototype.  Our method significantly outperforms the state-of-the-art on several challenging datasets, achieving substantial accuracy gains (e.g., +2.23 Top-1 accuracy on Something-Something-v2 using only the first 10% of videos) even surpassing methods relying on multi-modal data or batched inference.  Comprehensive ablation studies validate our design choices and provide insights into the model's learned semantic representations.\n"}, "2007.1562": {"original_text": "  Named Entity Recognition (NER) is a fundamental NLP task, commonly formulated\nas classification over a sequence of tokens. Morphologically-Rich Languages\n(MRLs) pose a challenge to this basic formulation, as the boundaries of Named\nEntities do not necessarily coincide with token boundaries, rather, they\nrespect morphological boundaries. To address NER in MRLs we then need to answer\ntwo fundamental questions, namely, what are the basic units to be labeled, and\nhow can these units be detected and classified in realistic settings, i.e.,\nwhere no gold morphology is available. We empirically investigate these\nquestions on a novel NER benchmark, with parallel tokenlevel and morpheme-level\nNER annotations, which we develop for Modern Hebrew, a morphologically\nrich-and-ambiguous language. Our results show that explicitly modeling\nmorphological boundaries leads to improved NER performance, and that a novel\nhybrid architecture, in which NER precedes and prunes morphological\ndecomposition, greatly outperforms the standard pipeline, where morphological\ndecomposition strictly precedes NER, setting a new performance bar for both\nHebrew NER and Hebrew morphological decomposition tasks.\n", "rewritten_text": "Named Entity Recognition (NER) in morphologically rich languages (MRLs) is complicated because entity boundaries often don't align with word boundaries but instead with morphological units.  This paper addresses this challenge by investigating optimal labeling units and detection methods for MRL NER, particularly in scenarios lacking pre-existing morphological analyses.  A new Hebrew NER benchmark with parallel token- and morpheme-level annotations is introduced.  Experiments demonstrate that explicitly incorporating morphological boundaries improves NER accuracy.  Furthermore, a novel hybrid architecture, where NER precedes and informs morphological decomposition, significantly surpasses the traditional pipeline (decomposition before NER), establishing a new state-of-the-art for both Hebrew NER and morphological analysis.\n"}, "2102.12452": {"original_text": "  Probing classifiers have emerged as one of the prominent methodologies for\ninterpreting and analyzing deep neural network models of natural language\nprocessing. The basic idea is simple -- a classifier is trained to predict some\nlinguistic property from a model's representations -- and has been used to\nexamine a wide variety of models and properties. However, recent studies have\ndemonstrated various methodological limitations of this approach. This article\ncritically reviews the probing classifiers framework, highlighting their\npromises, shortcomings, and advances.\n", "rewritten_text": "Probing classifiers are a leading method for interpreting natural language processing (NLP) deep learning models.  These classifiers predict linguistic properties from a model's internal representations, offering insights into model behavior. While widely used, recent research reveals significant methodological flaws. This article provides a critical assessment of the probing classifier framework, examining its strengths, weaknesses, and recent developments.\n"}, "2408.11567": {"original_text": "  Point cloud analysis has achieved significant development and is\nwell-performed in multiple downstream tasks like point cloud classification and\nsegmentation, etc. Being conscious of the simplicity of the position encoding\nstructure in Transformer-based architectures, we attach importance to the\nposition encoding as a high-dimensional part and the patch encoder to offer\nmulti-scale information. Together with the sequential Transformer, the whole\nmodule with position encoding comprehensively constructs a multi-scale feature\nabstraction module that considers both the local parts from the patch and the\nglobal parts from center points as position encoding. With only a few\nparameters, the position embedding module fits the setting of PEFT\n(Parameter-Efficient Fine-Tuning) tasks pretty well. Thus we unfreeze these\nparameters as a fine-tuning part. At the same time, we review the existing\nprompt and adapter tuning methods, proposing a fresh way of prompts and\nsynthesizing them with adapters as dynamic adjustments. Our Proposed method of\nPEFT tasks, namely PPT, with only 1.05% of parameters for training, gets\nstate-of-the-art results in several mainstream datasets, such as 95.01%\naccuracy in the ScanObjectNN OBJ_BG dataset. Codes will be released at\nhttps://github.com/zsc000722/PPT.\n", "rewritten_text": "Transformer-based point cloud analysis has seen significant advancements, but their simple positional encoding limits performance.  Our novel approach, PPT, addresses this by incorporating a high-dimensional positional encoding and a multi-scale patch encoder within a sequential Transformer architecture. This creates a feature abstraction module capturing both local (patch) and global (center point) context.  PPT's efficiency, using only 1.05% of parameters for training, makes it ideal for parameter-efficient fine-tuning (PEFT).  We further improve performance by introducing a new prompt synthesis method combined with adapter tuning.  This achieves state-of-the-art results on benchmark datasets, such as 95.01% accuracy on ScanObjectNN OBJ_BG.  Code is available at https://github.com/zsc000722/PPT.\n"}, "1607.0003": {"original_text": "  Human evaluation of machine translation normally uses sentence-level measures\nsuch as relative ranking or adequacy scales. However, these provide no insight\ninto possible errors, and do not scale well with sentence length. We argue for\na semantics-based evaluation, which captures what meaning components are\nretained in the MT output, thus providing a more fine-grained analysis of\ntranslation quality, and enabling the construction and tuning of\nsemantics-based MT. We present a novel human semantic evaluation measure, Human\nUCCA-based MT Evaluation (HUME), building on the UCCA semantic representation\nscheme. HUME covers a wider range of semantic phenomena than previous methods\nand does not rely on semantic annotation of the potentially garbled MT output.\nWe experiment with four language pairs, demonstrating HUME's broad\napplicability, and report good inter-annotator agreement rates and correlation\nwith human adequacy scores.\n", "rewritten_text": "Traditional human evaluation of machine translation (MT) relies on sentence-level metrics like ranking or adequacy scales, offering limited error analysis and scaling poorly with sentence length.  This paper proposes a novel semantics-based evaluation method, Human UCCA-based MT Evaluation (HUME), leveraging the UCCA semantic representation.  HUME provides a more granular analysis of translation quality by identifying retained meaning components, facilitating the development and refinement of semantics-based MT systems.  Unlike previous methods, HUME analyzes a broader range of semantic phenomena and avoids the need for annotating potentially flawed MT output.  Experiments across four language pairs demonstrate HUME's versatility, strong inter-annotator agreement, and correlation with human adequacy ratings.\n"}, "2205.0809": {"original_text": "  Event cameras are bio-inspired sensors that capture per-pixel asynchronous\nintensity change rather than the synchronous absolute intensity frames captured\nby a classical camera sensor. Such cameras are ideal for robotics applications\nsince they have high temporal resolution, high dynamic range and low latency.\nHowever, due to their high temporal resolution, event cameras are particularly\nsensitive to flicker such as from fluorescent or LED lights. During every cycle\nfrom bright to dark, pixels that image a flickering light source generate many\nevents that provide little or no useful information for a robot, swamping the\nuseful data in the scene. In this paper, we propose a novel linear filter to\npreprocess event data to remove unwanted flicker events from an event stream.\nThe proposed algorithm achieves over 4.6 times relative improvement in the\nsignal-to-noise ratio when compared to the raw event stream due to the\neffective removal of flicker from fluorescent lighting. Thus, it is ideally\nsuited to robotics applications that operate in indoor settings or scenes\nilluminated by flickering light sources.\n", "rewritten_text": "Unlike traditional cameras that capture full image frames, bio-inspired event cameras record only changes in pixel intensity.  This asynchronous approach offers high temporal resolution, dynamic range, and low latency, making them well-suited for robotics. However, their sensitivity to flickering light sources (like fluorescent or LED lights) generates a flood of irrelevant data.  This paper introduces a novel linear filter to preprocess event camera data, effectively removing flicker-induced noise.  Tests show a more than 4.6-fold improvement in signal-to-noise ratio under fluorescent lighting, significantly enhancing the usability of event cameras in indoor robotics applications.\n"}, "2205.00627": {"original_text": "  White matter fiber clustering is an important strategy for white matter\nparcellation, which enables quantitative analysis of brain connections in\nhealth and disease. In combination with expert neuroanatomical labeling,\ndata-driven white matter fiber clustering is a powerful tool for creating\natlases that can model white matter anatomy across individuals. While widely\nused fiber clustering approaches have shown good performance using classical\nunsupervised machine learning techniques, recent advances in deep learning\nreveal a promising direction toward fast and effective fiber clustering. In\nthis work, we propose a novel deep learning framework for white matter fiber\nclustering, Deep Fiber Clustering (DFC), which solves the unsupervised\nclustering problem as a self-supervised learning task with a domain-specific\npretext task to predict pairwise fiber distances. This process learns a\nhigh-dimensional embedding feature representation for each fiber, regardless of\nthe order of fiber points reconstructed during tractography. We design a novel\nnetwork architecture that represents input fibers as point clouds and allows\nthe incorporation of additional sources of input information from gray matter\nparcellation to improve anatomical coherence of clusters. In addition, DFC\nconducts outlier removal naturally by rejecting fibers with low cluster\nassignment probability. We evaluate DFC on three independently acquired\ncohorts, including data from 220 individuals across genders, ages (young and\nelderly adults), and different health conditions (healthy control and multiple\nneuropsychiatric disorders). We compare DFC to several state-of-the-art white\nmatter fiber clustering algorithms. Experimental results demonstrate superior\nperformance of DFC in terms of cluster compactness, generalization ability,\nanatomical coherence, and computational efficiency.\n", "rewritten_text": "Deep Fiber Clustering (DFC), a novel deep learning framework, offers a fast and effective solution for white matter fiber clustering, a crucial step in analyzing brain connectivity.  Unlike traditional unsupervised machine learning methods, DFC employs a self-supervised approach, predicting pairwise fiber distances to learn a robust, order-invariant fiber representation.  Its unique architecture handles fiber data as point clouds and integrates gray matter parcellation information for improved anatomical accuracy.  DFC also inherently removes outliers.  Evaluated across three diverse cohorts (220 individuals with varying ages, genders, and health statuses), DFC outperforms existing methods in cluster compactness, generalizability, anatomical coherence, and speed.\n"}, "1707.08401": {"original_text": "  In the last two decades Computer Aided Diagnostics (CAD) systems were\ndeveloped to help radiologists analyze screening mammograms. The benefits of\ncurrent CAD technologies appear to be contradictory and they should be improved\nto be ultimately considered useful. Since 2012 deep convolutional neural\nnetworks (CNN) have been a tremendous success in image recognition, reaching\nhuman performance. These methods have greatly surpassed the traditional\napproaches, which are similar to currently used CAD solutions. Deep CNN-s have\nthe potential to revolutionize medical image analysis. We propose a CAD system\nbased on one of the most successful object detection frameworks, Faster R-CNN.\nThe system detects and classifies malignant or benign lesions on a mammogram\nwithout any human intervention. The proposed method sets the state of the art\nclassification performance on the public INbreast database, AUC = 0.95 . The\napproach described here has achieved the 2nd place in the Digital Mammography\nDREAM Challenge with AUC = 0.85 . When used as a detector, the system reaches\nhigh sensitivity with very few false positive marks per image on the INbreast\ndataset. Source code, the trained model and an OsiriX plugin are availaible\nonline at https://github.com/riblidezso/frcnn_cad .\n", "rewritten_text": "Computer-aided diagnosis (CAD) systems for mammogram analysis have shown mixed results over the past two decades.  However, the recent success of deep convolutional neural networks (CNNs) in image recognition offers significant potential for improvement.  This paper presents a novel CAD system based on the Faster R-CNN object detection framework, capable of autonomously detecting and classifying malignant and benign lesions in mammograms.  Our system achieved state-of-the-art performance on the INbreast dataset (AUC = 0.95) and placed second in the Digital Mammography DREAM Challenge (AUC = 0.85).  Furthermore, it demonstrates high sensitivity with low false positives.  The source code, trained model, and an OsiriX plugin are publicly available at [https://github.com/riblidezso/frcnn_cad](https://github.com/riblidezso/frcnn_cad).\n"}, "1604.02975": {"original_text": "  We propose a novel Coupled Projection multi-task Metric Learning (CP-mtML)\nmethod for large scale face retrieval. In contrast to previous works which were\nlimited to low dimensional features and small datasets, the proposed method\nscales to large datasets with high dimensional face descriptors. It utilises\npairwise (dis-)similarity constraints as supervision and hence does not require\nexhaustive class annotation for every training image. While, traditionally,\nmulti-task learning methods have been validated on same dataset but different\ntasks, we work on the more challenging setting with heterogeneous datasets and\ndifferent tasks. We show empirical validation on multiple face image datasets\nof different facial traits, e.g. identity, age and expression. We use classic\nLocal Binary Pattern (LBP) descriptors along with the recent Deep Convolutional\nNeural Network (CNN) features. The experiments clearly demonstrate the\nscalability and improved performance of the proposed method on the tasks of\nidentity and age based face image retrieval compared to competitive existing\nmethods, on the standard datasets and with the presence of a million distractor\nface images.\n", "rewritten_text": "This paper introduces CP-mtML, a novel coupled projection multi-task metric learning method for large-scale face retrieval.  Unlike previous methods limited to small datasets and low-dimensional features, CP-mtML handles high-dimensional descriptors from large datasets using pairwise similarity constraints, eliminating the need for exhaustive image annotation.  Unlike typical multi-task learning approaches using homogeneous datasets and tasks, CP-mtML tackles the more challenging scenario of heterogeneous datasets and diverse tasks (e.g., identity, age, and expression recognition).  Experiments using LBP and deep CNN features on standard datasets, including a million distractor images, demonstrate CP-mtML's superior scalability and performance in identity and age-based face retrieval compared to state-of-the-art methods.\n"}, "1705.10872": {"original_text": "  We introduce a novel loss for learning local feature descriptors which is\ninspired by the Lowe's matching criterion for SIFT. We show that the proposed\nloss that maximizes the distance between the closest positive and closest\nnegative patch in the batch is better than complex regularization methods; it\nworks well for both shallow and deep convolution network architectures.\nApplying the novel loss to the L2Net CNN architecture results in a compact\ndescriptor -- it has the same dimensionality as SIFT (128) that shows\nstate-of-art performance in wide baseline stereo, patch verification and\ninstance retrieval benchmarks. It is fast, computing a descriptor takes about 1\nmillisecond on a low-end GPU.\n", "rewritten_text": "This paper presents a new loss function for training local feature descriptors, inspired by Lowe's SIFT matching criterion.  This simple loss, which maximizes the distance between the nearest positive and nearest negative patch within a training batch, outperforms complex regularization techniques for both shallow and deep convolutional neural networks.  When applied to the L2Net architecture, it yields a compact 128-dimensional descriptor (matching SIFT's dimensionality) achieving state-of-the-art results on wide-baseline stereo, patch verification, and instance retrieval benchmarks.  Furthermore, descriptor computation is fast, requiring approximately 1 millisecond on a low-end GPU.\n"}, "2103.17107": {"original_text": "  In this paper, the multi-task learning of lightweight convolutional neural\nnetworks is studied for face identification and classification of facial\nattributes (age, gender, ethnicity) trained on cropped faces without margins.\nThe necessity to fine-tune these networks to predict facial expressions is\nhighlighted. Several models are presented based on MobileNet, EfficientNet and\nRexNet architectures. It was experimentally demonstrated that they lead to near\nstate-of-the-art results in age, gender and race recognition on the UTKFace\ndataset and emotion classification on the AffectNet dataset. Moreover, it is\nshown that the usage of the trained models as feature extractors of facial\nregions in video frames leads to 4.5% higher accuracy than the previously known\nstate-of-the-art single models for the AFEW and the VGAF datasets from the\nEmotiW challenges. The models and source code are publicly available at\nhttps://github.com/HSE-asavchenko/face-emotion-recognition.\n", "rewritten_text": "This paper investigates multi-task learning with lightweight convolutional neural networks (CNNs) \u2013 based on MobileNet, EfficientNet, and RexNet architectures \u2013 for simultaneous face identification and attribute classification (age, gender, ethnicity).  Trained on margin-free cropped faces, these models are shown to require fine-tuning for facial expression prediction.  Experiments on UTKFace (age, gender, race) and AffectNet (emotion) datasets demonstrate near state-of-the-art performance.  Furthermore, using these models as feature extractors for video frames improves accuracy by 4.5% over existing single-model approaches on the AFEW and VGAF datasets (EmotiW challenges).  The models and code are publicly available at https://github.com/HSE-asavchenko/face-emotion-recognition.\n"}, "2310.06474": {"original_text": "  While large language models (LLMs) exhibit remarkable capabilities across a\nwide range of tasks, they pose potential safety concerns, such as the\n``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to\nexhibit undesirable behavior. Although several preventive measures have been\ndeveloped to mitigate the potential risks associated with LLMs, they have\nprimarily focused on English. In this study, we reveal the presence of\nmultilingual jailbreak challenges within LLMs and consider two potential risky\nscenarios: unintentional and intentional. The unintentional scenario involves\nusers querying LLMs using non-English prompts and inadvertently bypassing the\nsafety mechanisms, while the intentional scenario concerns malicious users\ncombining malicious instructions with multilingual prompts to deliberately\nattack LLMs. The experimental results reveal that in the unintentional\nscenario, the rate of unsafe content increases as the availability of languages\ndecreases. Specifically, low-resource languages exhibit about three times the\nlikelihood of encountering harmful content compared to high-resource languages,\nwith both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts\ncan exacerbate the negative impact of malicious instructions, with\nastonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for\nGPT-4. To handle such a challenge in the multilingual context, we propose a\nnovel \\textsc{Self-Defense} framework that automatically generates multilingual\ntraining data for safety fine-tuning. Experimental results show that ChatGPT\nfine-tuned with such data can achieve a substantial reduction in unsafe content\ngeneration. Data is available at\n\\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.\n", "rewritten_text": "Large language models (LLMs) like ChatGPT and GPT-4, while powerful, are vulnerable to \"jailbreaks\"\u2014malicious prompts eliciting undesirable behavior.  Existing safety measures primarily target English, leaving LLMs susceptible to multilingual attacks.  This study reveals that non-English prompts can unintentionally bypass safety mechanisms, with low-resource languages showing a threefold increase in unsafe content compared to high-resource languages.  Intentionally malicious multilingual prompts are even more effective, resulting in unsafe outputs 80.92% of the time for ChatGPT and 40.71% for GPT-4.  To address this, we introduce a novel Self-Defense framework that automatically generates multilingual training data for improved safety.  Our experiments demonstrate that this approach significantly reduces unsafe content generation in ChatGPT.  The dataset is available at [https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs](https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs).\n"}, "2110.13793": {"original_text": "  With camera resolution ever increasing and the need to rapidly recalibrate\nrobotic platforms in less than ideal environments, there is a need for faster\nand more robust chessboard fiducial marker detectors. A new chessboard detector\nis proposed that is specifically designed for: high resolution images,\nfocus/motion blur, harsh lighting conditions, and background clutter. This is\naccomplished using a new x-corner detector, where for the first time blur is\nestimated and used in a novel way to enhance corner localization, edge\nvalidation, and connectivity. Performance is measured and compared against\nother libraries using a diverse set of images created by combining multiple\nthird party datasets and including new specially crafted scenarios designed to\nstress the state-of-the-art. The proposed detector has the best F1- Score of\n0.97, runs 1.9x faster than next fastest, and is a top performer for corner\naccuracy, while being the only detector to have consistent good performance in\nall scenarios.\n", "rewritten_text": "High-resolution cameras and the demand for rapid recalibration of robots in challenging environments necessitate faster, more robust chessboard detection.  This paper introduces a novel chessboard detector optimized for high-resolution images, handling blur, harsh lighting, and cluttered backgrounds.  A new x-corner detector, uniquely incorporating blur estimation to improve corner localization, edge validation, and connectivity, underpins this approach.  Evaluated against existing libraries on a diverse, rigorously tested dataset, our detector achieves a best-in-class F1-score of 0.97, is 1.9 times faster than the next fastest competitor, and exhibits superior corner accuracy and consistent performance across all scenarios.\n"}, "2309.13492": {"original_text": "  Today's image style transfer methods have difficulty retaining humans face\nindividual features after the whole stylizing process. This occurs because the\nfeatures like face geometry and people's expressions are not captured by the\ngeneral-purpose image classifiers like the VGG-19 pre-trained models. This\npaper proposes the use of embeddings from an auxiliary pre-trained face\nrecognition model to encourage the algorithm to propagate human face features\nfrom the content image to the final stylized result.\n", "rewritten_text": "Current image style transfer techniques struggle to preserve individual facial features.  This is because standard image classifiers, such as VGG-19, don't adequately capture facial geometry and expressions.  This paper introduces a novel approach: leveraging embeddings from a pre-trained face recognition model to guide the style transfer process and ensure accurate transfer of facial features from the source image to the stylized output.\n"}, "2206.04242": {"original_text": "  Despite advances in image classification methods, detecting the samples not\nbelonging to the training classes is still a challenging problem. There has\nbeen a burst of interest in this subject recently, which is called Open-Set\nRecognition (OSR). In OSR, the goal is to achieve both the classification and\ndetecting out-of-distribution (OOD) samples. Several ideas have been proposed\nto push the empirical result further through complicated techniques. We believe\nthat such complication is indeed not necessary. To this end, we have shown that\nMaximum Softmax Probability (MSP), as the simplest baseline for OSR, applied on\nVision Transformers (ViTs) as the base classifier that is trained with non-OOD\naugmentations can surprisingly outperform many recent methods. Non-OOD\naugmentations are the ones that do not alter the data distribution by much. Our\nresults outperform state-of-the-art in CIFAR-10 datasets, and is also better\nthan most of the current methods in SVHN and MNIST. We show that training\naugmentation has a significant effect on the performance of ViTs in the OSR\ntasks, and while they should produce significant diversity in the augmented\nsamples, the generated sample OOD-ness must remain limited.\n", "rewritten_text": "Open-set recognition (OSR), identifying samples outside the training data, remains a challenge despite advancements in image classification.  While complex methods have been proposed, we demonstrate that a surprisingly simple approach\u2014using Maximum Softmax Probability (MSP) with Vision Transformers (ViTs) trained using minimally-distorting, non-out-of-distribution (OOD) augmentations\u2014achieves state-of-the-art performance on CIFAR-10 and surpasses many existing methods on SVHN and MNIST.  Our findings highlight the significant impact of carefully chosen training augmentations on ViT performance in OSR, emphasizing the importance of augmentation diversity without introducing excessive OOD characteristics.\n"}, "2010.01041": {"original_text": "  Planar homography estimation is foundational to many computer vision\nproblems, such as Simultaneous Localization and Mapping (SLAM) and Augmented\nReality (AR). However, conditions of high variance confound even the\nstate-of-the-art algorithms. In this report, we analyze the performance of two\nrecently published methods using Convolutional Neural Networks (CNNs) that are\nmeant to replace the more traditional feature-matching based approaches to the\nestimation of homography. Our evaluation of the CNN based methods focuses\nparticularly on measuring the performance under conditions of significant\nnoise, illumination shift, and occlusion. We also measure the benefits of\ntraining CNNs to varying degrees of noise. Additionally, we compare the effect\nof using color images instead of grayscale images for inputs to CNNs. Finally,\nwe compare the results against baseline feature-matching based homography\nestimation methods using SIFT, SURF, and ORB. We find that CNNs can be trained\nto be more robust against noise, but at a small cost to accuracy in the\nnoiseless case. Additionally, CNNs perform significantly better in conditions\nof extreme variance than their feature-matching based counterparts. With regard\nto color inputs, we conclude that with no change in the CNN architecture to\ntake advantage of the additional information in the color planes, the\ndifference in performance using color inputs or grayscale inputs is negligible.\nAbout the CNNs trained with noise-corrupted inputs, we show that training a CNN\nto a specific magnitude of noise leads to a \"Goldilocks Zone\" with regard to\nthe noise levels where that CNN performs best.\n", "rewritten_text": "Planar homography estimation, crucial for applications like SLAM and AR, is challenged by high variance, even for advanced algorithms.  This report compares two novel CNN-based homography estimation methods against traditional feature-matching (SIFT, SURF, ORB) techniques.  We evaluate performance under significant noise, illumination changes, and occlusions, examining the impact of training CNNs with varying noise levels and color versus grayscale inputs.  Results show CNNs achieve greater robustness to noise, albeit with slightly reduced accuracy in noiseless scenarios.  Significantly, CNNs outperform feature-matching methods under extreme variance.  Using color images provided negligible performance improvement without architectural modifications to leverage color information.  Finally, training CNNs with specific noise levels reveals an optimal \"Goldilocks Zone\" of noise robustness.\n"}, "2308.10168": {"original_text": "  Since the recent prosperity of Large Language Models (LLMs), there have been\ninterleaved discussions regarding how to reduce hallucinations from LLM\nresponses, how to increase the factuality of LLMs, and whether Knowledge Graphs\n(KGs), which store the world knowledge in a symbolic form, will be replaced\nwith LLMs. In this paper, we try to answer these questions from a new angle:\nHow knowledgeable are LLMs?\n  To answer this question, we constructed Head-to-Tail, a benchmark that\nconsists of 18K question-answer (QA) pairs regarding head, torso, and tail\nfacts in terms of popularity. We designed an automated evaluation method and a\nset of metrics that closely approximate the knowledge an LLM confidently\ninternalizes. Through a comprehensive evaluation of 16 publicly available LLMs,\nwe show that existing LLMs are still far from being perfect in terms of their\ngrasp of factual knowledge, especially for facts of torso-to-tail entities.\n", "rewritten_text": "Recent advancements in Large Language Models (LLMs) have sparked debate about mitigating their inaccuracies (\"hallucinations\"), improving factual accuracy, and whether they will supplant Knowledge Graphs (KGs).  This paper addresses these issues by investigating the extent of LLM knowledge.  We introduce Head-to-Tail, a benchmark of 18,000 question-answer pairs graded by fact popularity (head, torso, and tail).  Using automated evaluation and novel metrics, we assess the factual knowledge confidently retained by 16 publicly available LLMs.  Our findings reveal that these models, while advanced, still exhibit significant limitations in factual knowledge, particularly regarding less popular facts.\n"}, "1912.06683": {"original_text": "  Semantic image segmentation plays a pivotal role in many vision applications\nincluding autonomous driving and medical image analysis. Most of the former\napproaches move towards enhancing the performance in terms of accuracy with a\nlittle awareness of computational efficiency. In this paper, we introduce\nLiteSeg, a lightweight architecture for semantic image segmentation. In this\nwork, we explore a new deeper version of Atrous Spatial Pyramid Pooling module\n(ASPP) and apply short and long residual connections, and depthwise separable\nconvolution, resulting in a faster and efficient model. LiteSeg architecture is\nintroduced and tested with multiple backbone networks as Darknet19, MobileNet,\nand ShuffleNet to provide multiple trade-offs between accuracy and\ncomputational cost. The proposed model LiteSeg, with MobileNetV2 as a backbone\nnetwork, achieves an accuracy of 67.81% mean intersection over union at 161\nframes per second with $640 \\times 360$ resolution on the Cityscapes dataset.\n", "rewritten_text": "Semantic image segmentation is crucial for applications like autonomous driving and medical imaging, but existing high-accuracy methods often neglect computational efficiency.  This paper presents LiteSeg, a lightweight semantic segmentation architecture.  We achieve this by incorporating a novel, deeper Atrous Spatial Pyramid Pooling (ASPP) module, short and long residual connections, and depthwise separable convolutions.  LiteSeg's flexibility is demonstrated through its integration with various backbone networks (Darknet19, MobileNet, ShuffleNet), offering a range of accuracy-efficiency trade-offs.  Using MobileNetV2, LiteSeg achieves 67.81% mean Intersection over Union (mIoU) at 161 frames per second on the Cityscapes dataset ($640 \\times 360$ resolution).\n"}, "2402.14281": {"original_text": "  Map representation learned by expert demonstrations has shown promising\nresearch value. However, recent advancements in the visual navigation field\nface challenges due to the lack of human datasets in the real world for\nefficient supervised representation learning of the environments. We present a\nLandmark-Aware Visual Navigation (LAVN) dataset to allow for supervised\nlearning of human-centric exploration policies and map building. We collect RGB\nobservation and human point-click pairs as a human annotator explores virtual\nand real-world environments with the goal of full coverage exploration of the\nspace. The human annotators also provide distinct landmark examples along each\ntrajectory, which we intuit will simplify the task of map or graph building and\nlocalization. These human point-clicks serve as direct supervision for waypoint\nprediction when learning to explore in environments. Our dataset covers a wide\nspectrum of scenes, including rooms in indoor environments, as well as walkways\noutdoors. Dataset is available at DOI: 10.5281/zenodo.10608067.\n", "rewritten_text": "Expert-demonstrated map representations show promise, but visual navigation research is hampered by a scarcity of real-world human-generated datasets for supervised learning.  To address this, we introduce the Landmark-Aware Visual Navigation (LAVN) dataset, enabling supervised learning of human-like exploration and map creation.  Collected from virtual and real-world environments, the dataset comprises RGB images and human-provided point-click pairs aiming for complete environment coverage.  Annotators also identify landmarks along each trajectory, facilitating map/graph building and localization.  These point-clicks directly supervise waypoint prediction during exploration training.  The dataset encompasses diverse indoor (rooms) and outdoor (walkways) scenes and is available at DOI: 10.5281/zenodo.10608067.\n"}, "2408.05184": {"original_text": "  This paper describes our solution of the first subtask from the AXOLOTL-24\nshared task on Semantic Change Modeling. The goal of this subtask is to\ndistribute a given set of usages of a polysemous word from a newer time period\nbetween senses of this word from an older time period and clusters representing\ngained senses of this word. We propose and experiment with three new methods\nsolving this task. Our methods achieve SOTA results according to both official\nmetrics of the first substask. Additionally, we develop a model that can tell\nif a given word usage is not described by any of the provided sense\ndefinitions. This model serves as a component in one of our methods, but can\npotentially be useful on its own.\n", "rewritten_text": "This paper presents three novel methods for addressing the first subtask of the AXOLOTL-24 Semantic Change Modeling shared task.  This subtask involves assigning modern usages of a polysemous word to either older senses of that word or newly emerged senses.  Our methods achieve state-of-the-art (SOTA) performance according to the subtask's official metrics.  Furthermore, we introduce a supplementary model capable of identifying word usages not covered by existing sense definitions; this model is integrated into one of our primary methods but also offers independent utility.\n"}, "2312.00335": {"original_text": "  Self-supervised learning (SSL) approaches have recently shown substantial\nsuccess in learning visual representations from unannotated images. Compared\nwith photographic images, medical images acquired with the same imaging\nprotocol exhibit high consistency in anatomy. To exploit this anatomical\nconsistency, this paper introduces a novel SSL approach, called PEAC (patch\nembedding of anatomical consistency), for medical image analysis. Specifically,\nin this paper, we propose to learn global and local consistencies via stable\ngrid-based matching, transfer pre-trained PEAC models to diverse downstream\ntasks, and extensively demonstrate that (1) PEAC achieves significantly better\nperformance than the existing state-of-the-art fully/self-supervised methods,\nand (2) PEAC captures the anatomical structure consistency across views of the\nsame patient and across patients of different genders, weights, and healthy\nstatuses, which enhances the interpretability of our method for medical image\nanalysis.\n", "rewritten_text": "This paper introduces PEAC (Patch Embedding of Anatomical Consistency), a novel self-supervised learning (SSL) method for medical image analysis.  Leveraging the inherent anatomical consistency within medical images\u2014greater than that found in general photographic images\u2014PEAC learns global and local consistencies using stable grid-based matching.  We demonstrate that pre-trained PEAC models significantly outperform existing fully and self-supervised methods on diverse downstream tasks.  Furthermore, PEAC's ability to capture anatomical consistency across different patient views and demographics (gender, weight, health status) enhances its interpretability for medical applications.\n"}, "2408.15159": {"original_text": "  Translating written sentences from oral languages to a sequence of manual and\nnon-manual gestures plays a crucial role in building a more inclusive society\nfor deaf and hard-of-hearing people. Facial expressions (non-manual), in\nparticular, are responsible for encoding the grammar of the sentence to be\nspoken, applying punctuation, pronouns, or emphasizing signs. These non-manual\ngestures are closely related to the semantics of the sentence being spoken and\nalso to the utterance of the speaker's emotions. However, most Sign Language\nProduction (SLP) approaches are centered on synthesizing manual gestures and do\nnot focus on modeling the speakers expression. This paper introduces a new\nmethod focused in synthesizing facial expressions for sign language. Our goal\nis to improve sign language production by integrating sentiment information in\nfacial expression generation. The approach leverages a sentence sentiment and\nsemantic features to sample from a meaningful representation space, integrating\nthe bias of the non-manual components into the sign language production\nprocess. To evaluate our method, we extend the Frechet Gesture Distance (FGD)\nand propose a new metric called Frechet Expression Distance (FED) and apply an\nextensive set of metrics to assess the quality of specific regions of the face.\nThe experimental results showed that our method achieved state of the art,\nbeing superior to the competitors on How2Sign and PHOENIX14T datasets.\nMoreover, our architecture is based on a carefully designed graph pyramid that\nmakes it simpler, easier to train, and capable of leveraging emotions to\nproduce facial expressions.\n", "rewritten_text": "Accurate sign language translation, including both manual and non-manual gestures (especially crucial facial expressions conveying grammar and emotion), is vital for inclusivity within the Deaf and hard-of-hearing community.  Current sign language production (SLP) systems largely neglect facial expressions. This paper presents a novel method for synthesizing realistic facial expressions in sign language, incorporating sentiment analysis to enhance expressiveness.  Our approach uses sentence sentiment and semantic features to generate expressions, integrating non-manual components into the sign production process.  We introduce a new evaluation metric, Frechet Expression Distance (FED), alongside an extended Frechet Gesture Distance (FGD), and utilize various metrics to assess facial region quality.  Results on How2Sign and PHOENIX14T datasets demonstrate state-of-the-art performance, surpassing existing methods.  Our efficient graph pyramid architecture simplifies training and effectively leverages emotional information for realistic facial expression generation.\n"}, "2011.08894": {"original_text": "  Medical image segmentation is a relevant task as it serves as the first step\nfor several diagnosis processes, thus it is indispensable in clinical usage.\nWhilst major success has been reported using supervised techniques, they assume\na large and well-representative labelled set. This is a strong assumption in\nthe medical domain where annotations are expensive, time-consuming, and\ninherent to human bias. To address this problem, unsupervised techniques have\nbeen proposed in the literature yet it is still an open problem due to the\ndifficulty of learning any transformation pattern. In this work, we present a\nnovel optimisation model framed into a new CNN-based contrastive registration\narchitecture for unsupervised medical image segmentation. The core of our\napproach is to exploit image-level registration and feature-level from a\ncontrastive learning mechanism, to perform registration-based segmentation.\nFirstly, we propose an architecture to capture the image-to-image\ntransformation pattern via registration for unsupervised medical image\nsegmentation. Secondly, we embed a contrastive learning mechanism into the\nregistration architecture to enhance the discriminating capacity of the network\nin the feature-level. We show that our proposed technique mitigates the major\ndrawbacks of existing unsupervised techniques. We demonstrate, through\nnumerical and visual experiments, that our technique substantially outperforms\nthe current state-of-the-art unsupervised segmentation methods on two major\nmedical image datasets.\n", "rewritten_text": "Medical image segmentation is crucial for clinical diagnosis, but supervised methods, while highly successful, rely on extensive, accurately labeled data\u2014a significant challenge in medicine due to annotation costs and biases.  Unsupervised techniques offer a solution, but their performance has been limited by the difficulty of learning image transformations.  This paper introduces a novel unsupervised medical image segmentation method.  Our approach uses a CNN-based contrastive registration architecture that leverages both image-level registration and feature-level contrastive learning to achieve registration-based segmentation.  This architecture effectively captures image transformations and enhances feature discrimination.  Experimental results on two major medical image datasets demonstrate that our method significantly outperforms existing state-of-the-art unsupervised segmentation techniques.\n"}, "2301.02307": {"original_text": "  Narrated ''how-to'' videos have emerged as a promising data source for a wide\nrange of learning problems, from learning visual representations to training\nrobot policies. However, this data is extremely noisy, as the narrations do not\nalways describe the actions demonstrated in the video. To address this problem\nwe introduce the novel task of visual narration detection, which entails\ndetermining whether a narration is visually depicted by the actions in the\nvideo. We propose What You Say is What You Show (WYS^2), a method that\nleverages multi-modal cues and pseudo-labeling to learn to detect visual\nnarrations with only weakly labeled data. Our model successfully detects visual\nnarrations in in-the-wild videos, outperforming strong baselines, and we\ndemonstrate its impact for state-of-the-art summarization and temporal\nalignment of instructional videos.\n", "rewritten_text": "Instructional how-to videos are a valuable, albeit noisy, resource for machine learning.  The narration often doesn't accurately reflect the on-screen actions.  To address this, we introduce the task of visual narration detection \u2013 verifying if a video's narration matches its visual content.  Our proposed method, What You Say is What You Show (WYS\u00b2), uses multimodal cues and pseudo-labeling to achieve this with weakly labeled data.  WYS\u00b2 surpasses existing methods on real-world videos, improving state-of-the-art results in instructional video summarization and temporal alignment.\n"}, "2403.05170": {"original_text": "  This paper proposes a new pipeline for long-tail (LT) recognition. Instead of\nre-weighting or re-sampling, we utilize the long-tailed dataset itself to\ngenerate a balanced proxy that can be optimized through cross-entropy (CE).\nSpecifically, a randomly initialized diffusion model, trained exclusively on\nthe long-tailed dataset, is employed to synthesize new samples for\nunderrepresented classes. Then, we utilize the inherent information in the\noriginal dataset to filter out harmful samples and keep the useful ones. Our\nstrategy, Diffusion model for Long-Tail recognition (DiffuLT), represents a\npioneering utilization of generative models in long-tail recognition. DiffuLT\nachieves state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT,\nsurpassing the best competitors with non-trivial margins. Abundant ablations\nmake our pipeline interpretable, too. The whole generation pipeline is done\nwithout any external data or pre-trained model weights, making it highly\ngeneralizable to real-world long-tailed settings.\n", "rewritten_text": "This paper introduces DiffuLT, a novel pipeline for long-tail recognition that leverages a diffusion model trained solely on the imbalanced dataset itself.  Unlike existing re-weighting or re-sampling methods, DiffuLT generates a balanced proxy dataset by synthesizing samples for underrepresented classes.  A filtering process then selects high-quality synthetic samples, resulting in a significantly improved training set for standard cross-entropy optimization.  This approach achieves state-of-the-art performance on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT benchmarks, outperforming existing methods by a substantial margin.  Extensive ablation studies enhance the interpretability of our method, which is further distinguished by its data independence and lack of reliance on pre-trained models, making it highly adaptable to real-world scenarios.\n"}, "2404.14779": {"original_text": "  This study presents a comprehensive analysis and comparison of two\npredominant fine-tuning methodologies - full-parameter fine-tuning and\nparameter-efficient tuning - within the context of medical Large Language\nModels (LLMs). We developed and refined a series of LLMs, based on the Llama-2\narchitecture, specifically designed to enhance medical knowledge retrieval,\nreasoning, and question-answering capabilities. Our experiments systematically\nevaluate the effectiveness of these tuning strategies across various well-known\nmedical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of\n72% on the US Medical Licensing Examination (USMLE) datasets, setting a new\nstandard in performance for openly available medical LLMs. Through this\ncomparative analysis, we aim to identify the most effective and efficient\nmethod for fine-tuning LLMs in the medical domain, thereby contributing\nsignificantly to the advancement of AI-driven healthcare applications.\n", "rewritten_text": "This research compares full-parameter and parameter-efficient fine-tuning methods for medical Large Language Models (LLMs), using a series of Llama-2-based models optimized for medical knowledge retrieval, reasoning, and question answering.  Our experiments, conducted on established medical benchmarks, demonstrate the superior performance of our Med42 LLM, achieving 72% accuracy on the USMLE\u2014a new benchmark for publicly available medical LLMs.  This comparative analysis identifies the most effective and efficient fine-tuning approach for medical LLMs, advancing the potential of AI in healthcare.\n"}, "2411.09694": {"original_text": "  Reranking a list of candidates from a machine translation system with an\nexternal scoring model and returning the highest-scoring candidate remains a\nsimple and effective method for improving the overall output quality.\nTranslation scoring models continue to grow in size, with the best models being\ncomparable to generation models. Thus, reranking can add substantial\ncomputational cost to the translation pipeline. In this work, we pose reranking\nas a Bayesian optimization (BayesOpt) problem. By strategically selecting\ncandidates to score based on a balance of exploration and exploitation, we show\nthat it is possible to find top-scoring candidates when scoring only a fraction\nof the candidate list. For instance, our method achieves the same CometKiwi\nscore using only 70 scoring evaluations compared a baseline system using 180.\nWe present a multi-fidelity setting for BayesOpt, where the candidates are\nfirst scored with a cheaper but noisier proxy scoring model, which further\nimproves the cost-performance tradeoff when using smaller but well-trained\ndistilled proxy scorers.\n", "rewritten_text": "Reranking machine translation outputs using an external scoring model is a highly effective but computationally expensive technique.  This work addresses this cost by framing reranking as a Bayesian optimization problem.  By strategically selecting which candidates to score, we achieve comparable translation quality (measured by CometKiwi score) while significantly reducing the number of scoring evaluations\u201470 versus a baseline of 180.  Furthermore, we introduce a multi-fidelity approach using a cheaper, noisier proxy scoring model, further enhancing efficiency, especially when using smaller, distilled proxy models.\n"}, "2407.07080": {"original_text": "  Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.\n", "rewritten_text": "This paper presents DictaLM2.0 and DictaLM2.0-Instruct, two Hebrew-English large language models (LLMs) derived from the Mistral model and trained on a 200-billion-token corpus.  Addressing the challenges of training LLMs for low-resource languages like Hebrew, we detail novel training methodologies optimized for effective language adaptation.  DictaLM2.0-Instruct is further fine-tuned on an instruction dataset for improved task performance.  A new Hebrew LLM benchmark suite, encompassing tasks like question answering, sentiment analysis, and translation, rigorously evaluates these models.  Our framework offers a transferable approach for adapting LLMs to other low-resource languages, advancing multilingual natural language processing.\n"}, "2211.08544": {"original_text": "  Quantization-aware training (QAT) receives extensive popularity as it well\nretains the performance of quantized networks. In QAT, the contemporary\nexperience is that all quantized weights are updated for an entire training\nprocess. In this paper, this experience is challenged based on an interesting\nphenomenon we observed. Specifically, a large portion of quantized weights\nreaches the optimal quantization level after a few training epochs, which we\nrefer to as the partly scratch-off lottery ticket. This\nstraightforward-yet-valuable observation naturally inspires us to zero out\ngradient calculations of these weights in the remaining training period to\navoid meaningless updating. To effectively find the ticket, we develop a\nheuristic method, dubbed lottery ticket scratcher (LTS), which freezes a weight\nonce the distance between the full-precision one and its quantization level is\nsmaller than a controllable threshold. Surprisingly, the proposed LTS typically\neliminates 50%-70% weight updating and 25%-35% FLOPs of the backward pass,\nwhile still resulting on par with or even better performance than the compared\nbaseline. For example, compared with the baseline, LTS improves 2-bit\nMobileNetV2 by 5.05%, eliminating 46% weight updating and 23% FLOPs of the\nbackward pass. Code is at url{https://github.com/zysxmu/LTS}.\n", "rewritten_text": "Quantization-aware training (QAT) is widely used because it preserves the accuracy of quantized neural networks.  However, current QAT methods update all quantized weights throughout training.  This paper challenges this practice, observing that many weights reach their optimal quantization level early in training.  We call this phenomenon the \"partly scratch-off lottery ticket.\"  This led us to develop Lottery Ticket Scratcher (LTS), a heuristic method that freezes weights once they are sufficiently close to their optimal quantization level.  LTS significantly reduces computation:  typically eliminating 50-70% of weight updates and 25-35% of backward pass FLOPs, while maintaining or even improving performance.  For instance, LTS improved 2-bit MobileNetV2 by 5.05%, reducing weight updates by 46% and backward pass FLOPs by 23%.  Code is available at [https://github.com/zysxmu/LTS](https://github.com/zysxmu/LTS).\n"}, "2001.04642": {"original_text": "  We address the dual problems of novel view synthesis and environment\nreconstruction from hand-held RGBD sensors. Our contributions include 1)\nmodeling highly specular objects, 2) modeling inter-reflections and Fresnel\neffects, and 3) enabling surface light field reconstruction with the same input\nneeded to reconstruct shape alone. In cases where scene surface has a strong\nmirror-like material component, we generate highly detailed environment images,\nrevealing room composition, objects, people, buildings, and trees visible\nthrough windows. Our approach yields state of the art view synthesis\ntechniques, operates on low dynamic range imagery, and is robust to geometric\nand calibration errors.\n", "rewritten_text": "This work tackles the simultaneous challenges of novel view synthesis and environment reconstruction using only handheld RGB-D sensors.  We achieve this by: 1) accurately modeling highly specular surfaces, 2) incorporating inter-reflection and Fresnel effects, and 3) reconstructing the surface light field using the same input data required for shape reconstruction.  Our method excels in scenes with strong specular reflections, producing highly detailed environment images revealing objects and scenes reflected in mirror-like surfaces, including room interiors, people, buildings, and even distant trees.  The resulting view synthesis is state-of-the-art, works with low dynamic range imagery, and is robust to both geometric and calibration inaccuracies.\n"}, "1601.00199": {"original_text": "  Active Appearance Models (AAMs) are one of the most popular and\nwell-established techniques for modeling deformable objects in computer vision.\nIn this paper, we study the problem of fitting AAMs using Compositional\nGradient Descent (CGD) algorithms. We present a unified and complete view of\nthese algorithms and classify them with respect to three main characteristics:\ni) cost function; ii) type of composition; and iii) optimization method.\nFurthermore, we extend the previous view by: a) proposing a novel Bayesian cost\nfunction that can be interpreted as a general probabilistic formulation of the\nwell-known project-out loss; b) introducing two new types of composition,\nasymmetric and bidirectional, that combine the gradients of both image and\nappearance model to derive better conver- gent and more robust CGD algorithms;\nand c) providing new valuable insights into existent CGD algorithms by\nreinterpreting them as direct applications of the Schur complement and the\nWiberg method. Finally, in order to encourage open research and facilitate\nfuture comparisons with our work, we make the implementa- tion of the\nalgorithms studied in this paper publicly available as part of the Menpo\nProject.\n", "rewritten_text": "This paper investigates fitting Active Appearance Models (AAMs) to deformable objects using Compositional Gradient Descent (CGD) algorithms.  We provide a comprehensive classification of existing CGD algorithms based on their cost function, composition type, and optimization method.  Our contributions include: a novel Bayesian cost function generalizing the project-out loss; two new composition types (asymmetric and bidirectional) leading to improved convergence and robustness; and a reinterpretation of existing algorithms using the Schur complement and Wiberg method.  To foster further research, our implementation is publicly available via the Menpo Project.\n"}, "1709.01695": {"original_text": "  3D action recognition was shown to benefit from a covariance representation\nof the input data (joint 3D positions). A kernel machine feed with such feature\nis an effective paradigm for 3D action recognition, yielding state-of-the-art\nresults. Yet, the whole framework is affected by the well-known scalability\nissue. In fact, in general, the kernel function has to be evaluated for all\npairs of instances inducing a Gram matrix whose complexity is quadratic in the\nnumber of samples. In this work we reduce such complexity to be linear by\nproposing a novel and explicit feature map to approximate the kernel function.\nThis allows to train a linear classifier with an explicit feature encoding,\nwhich implicitly implements a Log-Euclidean machine in a scalable fashion. Not\nonly we prove that the proposed approximation is unbiased, but also we work out\nan explicit strong bound for its variance, attesting a theoretical superiority\nof our approach with respect to existing ones. Experimentally, we verify that\nour representation provides a compact encoding and outperforms other\napproximation schemes on a number of publicly available benchmark datasets for\n3D action recognition.\n", "rewritten_text": "This paper addresses the scalability limitations of kernel machines for 3D action recognition, which typically use a covariance representation of 3D joint positions.  While this approach achieves state-of-the-art results, its quadratic complexity hinders its application to large datasets.  We introduce a novel, explicit feature map that linearly approximates the kernel function, enabling the use of a linear classifier.  This implicitly implements a scalable Log-Euclidean machine.  We theoretically justify our approach by proving the unbiasedness of the approximation and deriving a strong bound on its variance, demonstrating its superiority over existing methods.  Empirical results on several benchmark datasets confirm that our compact representation significantly outperforms other approximation techniques in 3D action recognition.\n"}, "2212.0583": {"original_text": "  Directly training a document-to-document (Doc2Doc) neural machine translation\n(NMT) via Transformer from scratch, especially on small datasets usually fails\nto converge. Our dedicated probing tasks show that 1) both the absolute\nposition and relative position information gets gradually weakened or even\nvanished once it reaches the upper encoder layers, and 2) the vanishing of\nabsolute position information in encoder output causes the training failure of\nDoc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer\n(P-Transformer) to enhance both the absolute and relative position information\nin both self-attention and cross-attention. Specifically, we integrate absolute\npositional information, i.e., position embeddings, into the query-key pairs\nboth in self-attention and cross-attention through a simple yet effective\naddition operation. Moreover, we also integrate relative position encoding in\nself-attention. The proposed P-Transformer utilizes sinusoidal position\nencoding and does not require any task-specified position embedding, segment\nembedding, or attention mechanism. Through the above methods, we build a\nDoc2Doc NMT model with P-Transformer, which ingests the source document and\ncompletely generates the target document in a sequence-to-sequence (seq2seq)\nway. In addition, P-Transformer can be applied to seq2seq-based\ndocument-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)\ntranslation. Extensive experimental results of Doc2Doc NMT show that\nP-Transformer significantly outperforms strong baselines on widely-used 9\ndocument-level datasets in 7 language pairs, covering small-, middle-, and\nlarge-scales, and achieves a new state-of-the-art. Experimentation on discourse\nphenomena shows that our Doc2Doc NMT models improve the translation quality in\nboth BLEU and discourse coherence. We make our code available on Github.\n", "rewritten_text": "Training document-to-document (Doc2Doc) neural machine translation (NMT) models from scratch using Transformers often fails to converge, especially with limited data.  Our analysis reveals that positional information (both absolute and relative) degrades in higher encoder layers, with the loss of absolute position information being the primary cause of training failure.  To address this, we introduce the Position-Aware Transformer (P-Transformer), which enhances positional information in both self-attention and cross-attention mechanisms by simply adding absolute positional embeddings to query-key pairs.  We also incorporate relative positional encoding into self-attention, using sinusoidal encoding and avoiding task-specific embeddings.  Our P-Transformer-based Doc2Doc NMT model achieves state-of-the-art results on nine document-level datasets across seven language pairs, encompassing various data sizes.  Experiments demonstrate improved translation quality (BLEU score and discourse coherence).  Furthermore, P-Transformer is applicable to document-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent) translation.  Our code is publicly available on GitHub.\n"}, "2308.05430": {"original_text": "  In this work, we propose an ensemble modeling approach for multimodal action\nrecognition. We independently train individual modality models using a variant\nof focal loss tailored to handle the long-tailed distribution of the MECCANO\n[21] dataset. Based on the underlying principle of focal loss, which captures\nthe relationship between tail (scarce) classes and their prediction\ndifficulties, we propose an exponentially decaying variant of focal loss for\nour current task. It initially emphasizes learning from the hard misclassified\nexamples and gradually adapts to the entire range of examples in the dataset.\nThis annealing process encourages the model to strike a balance between\nfocusing on the sparse set of hard samples, while still leveraging the\ninformation provided by the easier ones. Additionally, we opt for the late\nfusion strategy to combine the resultant probability distributions from RGB and\nDepth modalities for final action prediction. Experimental evaluations on the\nMECCANO dataset demonstrate the effectiveness of our approach.\n", "rewritten_text": "This paper introduces a novel ensemble method for multimodal action recognition.  We train separate models for RGB and depth data using a modified focal loss function designed to address the class imbalance in the MECCANO dataset [21].  This modified loss function prioritizes initially learning from difficult, misclassified examples, then gradually incorporates easier examples, balancing performance on both common and rare actions.  Finally, we combine the models' predictions using late fusion to achieve improved action recognition accuracy, as demonstrated by experiments on the MECCANO dataset.\n"}, "1810.1161": {"original_text": "  Despite remarkable advances in image synthesis research, existing works often\nfail in manipulating images under the context of large geometric\ntransformations. Synthesizing person images conditioned on arbitrary poses is\none of the most representative examples where the generation quality largely\nrelies on the capability of identifying and modeling arbitrary transformations\non different body parts. Current generative models are often built on local\nconvolutions and overlook the key challenges (e.g. heavy occlusions, different\nviews or dramatic appearance changes) when distinct geometric changes happen\nfor each part, caused by arbitrary pose manipulations. This paper aims to\nresolve these challenges induced by geometric variability and spatial\ndisplacements via a new Soft-Gated Warping Generative Adversarial Network\n(Warping-GAN), which is composed of two stages: 1) it first synthesizes a\ntarget part segmentation map given a target pose, which depicts the\nregion-level spatial layouts for guiding image synthesis with higher-level\nstructure constraints; 2) the Warping-GAN equipped with a soft-gated\nwarping-block learns feature-level mapping to render textures from the original\nimage into the generated segmentation map. Warping-GAN is capable of\ncontrolling different transformation degrees given distinct target poses.\nMoreover, the proposed warping-block is light-weight and flexible enough to be\ninjected into any networks. Human perceptual studies and quantitative\nevaluations demonstrate the superiority of our Warping-GAN that significantly\noutperforms all existing methods on two large datasets.\n", "rewritten_text": "Current image synthesis struggles with large geometric transformations, particularly when generating images of people in arbitrary poses.  Existing generative models, based on local convolutions, often fail to handle the occlusions, viewpoint changes, and appearance shifts caused by such transformations.  This paper introduces Warping-GAN, a two-stage generative adversarial network designed to address this limitation.  First, it generates a target part segmentation map based on the desired pose, providing high-level structural guidance.  Second, a novel soft-gated warping block maps features from the source image onto this segmentation map, rendering textures according to the pose.  Warping-GAN allows for controlled transformation degrees and its lightweight, flexible warping block can be integrated into other networks.  Extensive evaluation on two large datasets demonstrates Warping-GAN's significant superiority over existing methods.\n"}, "2410.10407": {"original_text": "  The widespread dissemination of false information through manipulative\ntactics that combine deceptive text and images threatens the integrity of\nreliable sources of information. While there has been research on detecting\nfake news in high resource languages using multimodal approaches, methods for\nlow resource Indic languages primarily rely on textual analysis. This\ndifference highlights the need for robust methods that specifically address\nmultimodal fake news in Indic languages, where the lack of extensive datasets\nand tools presents a significant obstacle to progress. To this end, we\nintroduce the Multimodal Multilingual dataset for Indic Fake News Detection\n(MMIFND). This meticulously curated dataset consists of 28,085 instances\ndistributed across Hindi, Bengali, Marathi, Malayalam, Tamil, Gujarati and\nPunjabi. We further propose the Multimodal Multilingual Caption-aware framework\nfor Fake News Detection (MMCFND). MMCFND utilizes pre-trained unimodal encoders\nand pairwise encoders from a foundational model that aligns vision and\nlanguage, allowing for extracting deep representations from visual and textual\ncomponents of news articles. The multimodal fusion encoder in the foundational\nmodel integrates text and image representations derived from its pairwise\nencoders to generate a comprehensive cross modal representation. Furthermore,\nwe generate descriptive image captions that provide additional context to\ndetect inconsistencies and manipulations. The retrieved features are then fused\nand fed into a classifier to determine the authenticity of news articles. The\ncurated dataset can potentially accelerate research and development in low\nresource environments significantly. Thorough experimentation on MMIFND\ndemonstrates that our proposed framework outperforms established methods for\nextracting relevant fake news detection features.\n", "rewritten_text": "The spread of manipulated text and images is undermining trust in reliable information sources.  While high-resource languages leverage multimodal approaches to detect fake news, low-resource Indic languages largely rely on text alone.  This gap necessitates robust multimodal fake news detection methods for Indic languages, hampered by limited datasets and tools.  To address this, we introduce MMIFND, a new multimodal multilingual dataset (28,085 instances across seven Indic languages: Hindi, Bengali, Marathi, Malayalam, Tamil, Gujarati, and Punjabi).  We also propose MMCFND, a novel caption-aware framework that uses pre-trained models to extract deep representations from text and images, integrating them via a multimodal fusion encoder.  Generated image captions provide crucial contextual information for detecting inconsistencies.  Experiments on MMIFND show that MMCFND surpasses existing methods, and the dataset itself promises to significantly advance research in this crucial area.\n"}, "2401.17207": {"original_text": "  A comprehensive understanding of the organizational principles in the human\nbrain requires, among other factors, well-quantifiable descriptors of nerve\nfiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a\nmicroscopic imaging technique that enables insights into the fine-grained\norganization of myelinated nerve fibers with high resolution. Descriptors\ncharacterizing the fiber architecture observed in 3D-PLI would enable\ndownstream analysis tasks such as multimodal correlation studies, clustering,\nand mapping. However, best practices for observer-independent characterization\nof fiber architecture in 3D-PLI are not yet available. To this end, we propose\nthe application of a fully data-driven approach to characterize nerve fiber\narchitecture in 3D-PLI images using self-supervised representation learning. We\nintroduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the\nspatial neighborhood of texture examples across histological brain sections of\na 3D reconstructed volume to sample positive pairs for contrastive learning. We\ncombine this sampling strategy with specifically designed image augmentations\nto gain robustness to typical variations in 3D-PLI parameter maps. The approach\nis demonstrated for the 3D reconstructed occipital lobe of a vervet monkey\nbrain. We show that extracted features are highly sensitive to different\nconfigurations of nerve fibers, yet robust to variations between consecutive\nbrain sections arising from histological processing. We demonstrate their\npractical applicability for retrieving clusters of homogeneous fiber\narchitecture and performing data mining for interactively selected templates of\nspecific components of fiber architecture such as U-fibers.\n", "rewritten_text": "Understanding brain organization requires precise quantification of nerve fiber architecture.  Three-dimensional polarized light imaging (3D-PLI) offers high-resolution visualization of myelinated fibers, but lacks standardized, objective analysis methods.  This study introduces a novel, data-driven approach using self-supervised 3D-Context Contrastive Learning (CL-3D) to characterize nerve fiber architecture in 3D-PLI images.  CL-3D leverages spatial context within 3D-reconstructed brain volumes and incorporates robust image augmentations to handle variations in 3D-PLI data.  Applied to a vervet monkey occipital lobe, the method extracts features sensitive to fiber configurations while being robust to histological variations.  The resulting features effectively cluster homogeneous fiber architectures and facilitate data mining, enabling the identification of specific structures like U-fibers.\n"}, "1703.04454": {"original_text": "  We address the problem of estimating human pose and body shape from 3D scans\nover time. Reliable estimation of 3D body shape is necessary for many\napplications including virtual try-on, health monitoring, and avatar creation\nfor virtual reality. Scanning bodies in minimal clothing, however, presents a\npractical barrier to these applications. We address this problem by estimating\nbody shape under clothing from a sequence of 3D scans. Previous methods that\nhave exploited body models produce smooth shapes lacking personalized details.\nWe contribute a new approach to recover a personalized shape of the person. The\nestimated shape deviates from a parametric model to fit the 3D scans. We\ndemonstrate the method using high quality 4D data as well as sequences of\nvisual hulls extracted from multi-view images. We also make available BUFF, a\nnew 4D dataset that enables quantitative evaluation\n(http://buff.is.tue.mpg.de). Our method outperforms the state of the art in\nboth pose estimation and shape estimation, qualitatively and quantitatively.\n", "rewritten_text": "This paper presents a novel method for accurately estimating human body shape and pose from time-series 3D scans, overcoming the limitations of existing techniques.  Accurate 3D body shape estimation is crucial for applications like virtual try-on, health monitoring, and virtual reality avatar creation, but is hindered by the impracticality of scanning minimally-clothed individuals.  Our approach addresses this by estimating body shape *through* clothing. Unlike previous model-based methods that produce overly smoothed results, our technique recovers personalized body shapes by allowing deviations from a parametric model to better fit the scan data.  We validate our method using high-quality 4D scans and visual hulls derived from multi-view images, and introduce BUFF, a new publicly available 4D dataset (http://buff.is.tue.mpg.de) for quantitative evaluation.  Results demonstrate superior performance over state-of-the-art methods in both pose and shape estimation, both qualitatively and quantitatively.\n"}, "2002.01359": {"original_text": "  This paper gives an overview of the Schema-Guided Dialogue State Tracking\ntask of the 8th Dialogue System Technology Challenge. The goal of this task is\nto develop dialogue state tracking models suitable for large-scale virtual\nassistants, with a focus on data-efficient joint modeling across domains and\nzero-shot generalization to new APIs. This task provided a new dataset\nconsisting of over 16000 dialogues in the training set spanning 16 domains to\nhighlight these challenges, and a baseline model capable of zero-shot\ngeneralization to new APIs. Twenty-five teams participated, developing a range\nof neural network models, exceeding the performance of the baseline model by a\nvery high margin. The submissions incorporated a variety of pre-trained\nencoders and data augmentation techniques. This paper describes the task\ndefinition, dataset and evaluation methodology. We also summarize the approach\nand results of the submitted systems to highlight the overall trends in the\nstate-of-the-art.\n", "rewritten_text": "The 8th Dialogue System Technology Challenge's Schema-Guided Dialogue State Tracking task focused on developing data-efficient, joint-domain dialogue state tracking models capable of zero-shot generalization to new APIs for large-scale virtual assistants.  A new dataset of over 16,000 dialogues across 16 domains, along with a zero-shot baseline model, was provided to 25 participating teams.  These teams, using diverse neural network architectures, pre-trained encoders, and data augmentation techniques, significantly outperformed the baseline. This paper details the challenge's definition, dataset, evaluation, and summarizes the participating systems' approaches and results, revealing key trends in the field.\n"}, "2403.10574": {"original_text": "  The rich spatio-temporal information is crucial to capture the complicated\ntarget appearance variations in visual tracking. However, most top-performing\ntracking algorithms rely on many hand-crafted components for spatio-temporal\ninformation aggregation. Consequently, the spatio-temporal information is far\naway from being fully explored. To alleviate this issue, we propose an adaptive\ntracker with spatio-temporal transformers (named AQATrack), which adopts simple\nautoregressive queries to effectively learn spatio-temporal information without\nmany hand-designed components. Firstly, we introduce a set of learnable and\nautoregressive queries to capture the instantaneous target appearance changes\nin a sliding window fashion. Then, we design a novel attention mechanism for\nthe interaction of existing queries to generate a new query in current frame.\nFinally, based on the initial target template and learnt autoregressive\nqueries, a spatio-temporal information fusion module (STM) is designed for\nspatiotemporal formation aggregation to locate a target object. Benefiting from\nthe STM, we can effectively combine the static appearance and instantaneous\nchanges to guide robust tracking. Extensive experiments show that our method\nsignificantly improves the tracker's performance on six popular tracking\nbenchmarks: LaSOT, LaSOText, TrackingNet, GOT-10k, TNL2K, and UAV123.\n", "rewritten_text": "Visual tracking requires capturing rich spatio-temporal information to handle complex target appearance changes.  However, current state-of-the-art trackers heavily rely on hand-engineered components for this, limiting the full exploitation of this information.  To address this, we introduce AQATrack, an adaptive tracker using spatio-temporal transformers.  AQATrack employs simple, learnable, autoregressive queries to efficiently learn spatio-temporal information without manual design.  These queries capture instantaneous appearance changes using a sliding window approach, and a novel attention mechanism generates new queries by interacting with existing ones.  Finally, a spatio-temporal information fusion module (STM) combines the initial target template and learned queries to robustly locate the target by integrating static appearance and dynamic changes.  Extensive experiments on six benchmark datasets (LaSOT, LaSOText, TrackingNet, GOT-10k, TNL2K, and UAV123) demonstrate significant performance improvements.\n"}, "2206.1503": {"original_text": "  Question Answering (QA) is one of the most important natural language\nprocessing (NLP) tasks. It aims using NLP technologies to generate a\ncorresponding answer to a given question based on the massive unstructured\ncorpus. With the development of deep learning, more and more challenging QA\ndatasets are being proposed, and lots of new methods for solving them are also\nemerging. In this paper, we investigate influential QA datasets that have been\nreleased in the era of deep learning. Specifically, we begin with introducing\ntwo of the most common QA tasks - textual question answer and visual question\nanswering - separately, covering the most representative datasets, and then\ngive some current challenges of QA research.\n", "rewritten_text": "This paper surveys influential Question Answering (QA) datasets developed since the rise of deep learning.  We examine two key QA tasks\u2014textual and visual question answering\u2014highlighting representative datasets for each.  Finally, we discuss current challenges in QA research.  Question Answering is a crucial Natural Language Processing (NLP) task, aiming to automatically generate answers to questions from large unstructured text corpora using NLP techniques.  The recent surge in deep learning has spurred the creation of increasingly complex QA datasets and innovative solution methods.\n"}, "1604.04333": {"original_text": "  Deep Convolutional Neural Networks (CNN) have exhibited superior performance\nin many visual recognition tasks including image classification, object\ndetection, and scene label- ing, due to their large learning capacity and\nresistance to overfit. For the image classification task, most of the current\ndeep CNN- based approaches take the whole size-normalized image as input and\nhave achieved quite promising results. Compared with the previously dominating\napproaches based on feature extraction, pooling, and classification, the deep\nCNN-based approaches mainly rely on the learning capability of deep CNN to\nachieve superior results: the burden of minimizing intra-class variation while\nmaximizing inter-class difference is entirely dependent on the implicit feature\nlearning component of deep CNN; we rely upon the implicitly learned filters and\npooling component to select the discriminative regions, which correspond to the\nactivated neurons. However, if the irrelevant regions constitute a large\nportion of the image of interest, the classification performance of the deep\nCNN, which takes the whole image as input, can be heavily affected. To solve\nthis issue, we propose a novel latent CNN framework, which treats the most\ndiscriminate region as a latent variable. We can jointly learn the global CNN\nwith the latent CNN to avoid the aforementioned big irrelevant region issue,\nand our experimental results show the evident advantage of the proposed latent\nCNN over traditional deep CNN: latent CNN outperforms the state-of-the-art\nperformance of deep CNN on standard benchmark datasets including the CIFAR-10,\nCIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset.\n", "rewritten_text": "Deep Convolutional Neural Networks (CNNs) excel at visual recognition tasks like image classification, object detection, and scene labeling due to their high capacity and resistance to overfitting.  While current methods using full-size images as input achieve promising results, their performance suffers when irrelevant image regions are significant.  Unlike traditional feature extraction methods, deep CNNs implicitly learn features and select discriminative regions.  To address the problem of irrelevant regions impacting performance, we introduce a novel latent CNN framework. This framework treats the most discriminative region as a latent variable, jointly learning a global CNN with this latent CNN.  Our results on CIFAR-10, CIFAR-100, MNIST, and PASCAL VOC 2007 Classification datasets demonstrate that this latent CNN significantly outperforms traditional deep CNN approaches, achieving state-of-the-art results.\n"}, "2301.0734": {"original_text": "  Semi-Supervised Semantic Segmentation aims at training the segmentation model\nwith limited labeled data and a large amount of unlabeled data. To effectively\nleverage the unlabeled data, pseudo labeling, along with the teacher-student\nframework, is widely adopted in semi-supervised semantic segmentation. Though\nproved to be effective, this paradigm suffers from incorrect pseudo labels\nwhich inevitably exist and are taken as auxiliary training data. To alleviate\nthe negative impact of incorrect pseudo labels, we delve into the current\nSemi-Supervised Semantic Segmentation frameworks. We argue that the unlabeled\ndata with pseudo labels can facilitate the learning of representative features\nin the feature extractor, but it is unreliable to supervise the mask predictor.\nMotivated by this consideration, we propose a novel framework, Gentle Teaching\nAssistant (GTA-Seg) to disentangle the effects of pseudo labels on feature\nextractor and mask predictor of the student model. Specifically, in addition to\nthe original teacher-student framework, our method introduces a teaching\nassistant network which directly learns from pseudo labels generated by the\nteacher network. The gentle teaching assistant (GTA) is coined gentle since it\nonly transfers the beneficial feature representation knowledge in the feature\nextractor to the student model in an Exponential Moving Average (EMA) manner,\nprotecting the student model from the negative influences caused by unreliable\npseudo labels in the mask predictor. The student model is also supervised by\nreliable labeled data to train an accurate mask predictor, further facilitating\nfeature representation. Extensive experiment results on benchmark datasets\nvalidate that our method shows competitive performance against previous\nmethods. Code is available at https://github.com/Jin-Ying/GTA-Seg.\n", "rewritten_text": "Semi-supervised semantic segmentation typically uses pseudo-labeling within a teacher-student framework to leverage unlabeled data.  However, inaccurate pseudo-labels can hinder performance.  This work addresses this issue by proposing Gentle Teaching Assistant (GTA-Seg), a novel framework that separates the impact of pseudo-labels on the feature extractor and mask predictor.  GTA-Seg introduces a teaching assistant network trained directly on pseudo-labels, transferring only beneficial feature representations to the student model via exponential moving average (EMA).  This protects the student from unreliable pseudo-label-based mask predictions, while the student is also trained on reliable labeled data for accurate mask prediction.  Experiments on benchmark datasets demonstrate GTA-Seg's competitive performance.  Code is available at https://github.com/Jin-Ying/GTA-Seg.\n"}, "2108.03886": {"original_text": "  A meme is an part of media created to share an opinion or emotion across the\ninternet. Due to its popularity, memes have become the new forms of\ncommunication on social media. However, due to its nature, they are being used\nin harmful ways such as trolling and cyberbullying progressively. Various data\nmodelling methods create different possibilities in feature extraction and\nturning them into beneficial information. The variety of modalities included in\ndata plays a significant part in predicting the results. We try to explore the\nsignificance of visual features of images in classifying memes. Memes are a\nblend of both image and text, where the text is embedded into the image. We try\nto incorporate the memes as troll and non-trolling memes based on the images\nand the text on them. However, the images are to be analysed and combined with\nthe text to increase performance. Our work illustrates different textual\nanalysis methods and contrasting multimodal methods ranging from simple merging\nto cross attention to utilising both worlds' - best visual and textual\nfeatures. The fine-tuned cross-lingual language model, XLM, performed the best\nin textual analysis, and the multimodal transformer performs the best in\nmultimodal analysis.\n", "rewritten_text": "Memes, while popular forms of online communication, are increasingly used for harmful activities like trolling and cyberbullying.  This study investigates the potential of data modeling techniques to identify and classify these harmful memes.  We focus on the significance of visual features in conjunction with textual content.  Our approach explores various textual analysis methods (with XLM showing superior performance) and multimodal approaches (with the multimodal transformer achieving best results), ranging from simple fusion to sophisticated cross-attention mechanisms, to leverage the strengths of both image and text data for improved meme classification (troll vs. non-troll).\n"}, "2305.03973": {"original_text": "  Implicit Discourse Relation Recognition (IDRR) is a sophisticated and\nchallenging task to recognize the discourse relations between the arguments\nwith the absence of discourse connectives. The sense labels for each discourse\nrelation follow a hierarchical classification scheme in the annotation process\n(Prasad et al., 2008), forming a hierarchy structure. Most existing works do\nnot well incorporate the hierarchy structure but focus on the syntax features\nand the prior knowledge of connectives in the manner of pure text\nclassification. We argue that it is more effective to predict the paths inside\nthe hierarchical tree (e.g., \"Comparison -> Contrast -> however\") rather than\nflat labels (e.g., Contrast) or connectives (e.g., however). We propose a\nprompt-based path prediction method to utilize the interactive information and\nintrinsic senses among the hierarchy in IDRR. This is the first work that\ninjects such structure information into pre-trained language models via prompt\ntuning, and the performance of our solution shows significant and consistent\nimprovement against competitive baselines.\n", "rewritten_text": "Implicit Discourse Relation Recognition (IDRR) is a difficult task: identifying the relationships between sentences without explicit connecting words.  Existing methods typically rely on syntax and connective knowledge, treating the problem as simple text classification, ignoring the hierarchical nature of discourse relation labels (Prasad et al., 2008).  We propose a novel approach: prompt-based path prediction within this hierarchy.  Instead of predicting flat labels or relying on connectives, our method predicts paths through the hierarchy (e.g., \"Comparison -> Contrast -> however\"). This is the first application of prompt tuning to leverage this hierarchical structure in pre-trained language models, resulting in significantly improved performance compared to existing methods.\n"}, "1908.06665": {"original_text": "  Recently, significant progresses have been made in object detection on common\nbenchmarks (i.e., Pascal VOC). However, object detection in real world is still\nchallenging due to the serious data imbalance. Images in real world are\ndominated by easy samples like the wide range of background and some easily\nrecognizable objects, for example. Although two-stage detectors like Faster\nR-CNN achieved big successes in object detection due to the strategy of\nextracting region proposals by region proposal network, they show their poor\nadaption in real-world object detection as a result of without considering\nmining hard samples during extracting region proposals. To address this issue,\nwe propose a Cascade framework of Region Proposal Networks, referred to as\nC-RPNs. The essence of C-RPNs is adopting multiple stages to mine hard samples\nwhile extracting region proposals and learn stronger classifiers. Meanwhile, a\nfeature chain and a score chain are proposed to help learning more\ndiscriminative representations for proposals. Moreover, a loss function of\ncascade stages is designed to train cascade classifiers through\nbackpropagation. Our proposed method has been evaluated on Pascal VOC and\nseveral challenging datasets like BSBDV 2017, CityPersons, etc. Our method\nachieves competitive results compared with the current state-of-the-arts and\nall-sided improvements in error analysis, validating its efficacy for detection\nin real world.\n", "rewritten_text": "While object detection has advanced significantly on standard benchmarks like Pascal VOC, real-world applications remain challenging due to severe data imbalance.  Two-stage detectors, such as Faster R-CNN, struggle because their region proposal networks don't effectively mine hard samples.  To address this, we introduce C-RPNs, a cascaded region proposal network framework.  C-RPNs leverage multiple stages to progressively mine hard samples and learn more robust classifiers, aided by a feature chain and a score chain for improved proposal representation.  A specialized loss function enables backpropagation training across the cascade.  Evaluated on Pascal VOC, BSBDV 2017, CityPersons, and other challenging datasets, C-RPNs achieve state-of-the-art results and demonstrate significant improvements in error analysis, proving its effectiveness in real-world object detection.\n"}, "2001.04388": {"original_text": "  The efficient fusion of depth maps is a key part of most state-of-the-art 3D\nreconstruction methods. Besides requiring high accuracy, these depth fusion\nmethods need to be scalable and real-time capable. To this end, we present a\nnovel real-time capable machine learning-based method for depth map fusion.\nSimilar to the seminal depth map fusion approach by Curless and Levoy, we only\nupdate a local group of voxels to ensure real-time capability. Instead of a\nsimple linear fusion of depth information, we propose a neural network that\npredicts non-linear updates to better account for typical fusion errors. Our\nnetwork is composed of a 2D depth routing network and a 3D depth fusion network\nwhich efficiently handle sensor-specific noise and outliers. This is especially\nuseful for surface edges and thin objects for which the original approach\nsuffers from thickening artifacts. Our method outperforms the traditional\nfusion approach and related learned approaches on both synthetic and real data.\nWe demonstrate the performance of our method in reconstructing fine geometric\ndetails from noise and outlier contaminated data on various scenes.\n", "rewritten_text": "Real-time 3D reconstruction relies heavily on efficient depth map fusion.  Current methods, while accurate, often lack scalability and speed.  This paper introduces a novel, real-time capable, machine learning approach to depth map fusion.  Inspired by Curless and Levoy's work, we update only local voxel groups for efficiency. However, unlike their linear fusion, we employ a neural network (comprising 2D routing and 3D fusion components) to predict non-linear updates, mitigating common fusion errors, particularly those causing thickening artifacts at surface edges and on thin objects.  Our method surpasses both traditional and other learning-based techniques on synthetic and real datasets, demonstrating superior reconstruction of fine geometric details even from noisy and outlier-ridden data.\n"}, "1810.08768": {"original_text": "  Motion estimation (ME) and motion compensation (MC) have been widely used for\nclassical video frame interpolation systems over the past decades. Recently, a\nnumber of data-driven frame interpolation methods based on convolutional neural\nnetworks have been proposed. However, existing learning based methods typically\nestimate either flow or compensation kernels, thereby limiting performance on\nboth computational efficiency and interpolation accuracy. In this work, we\npropose a motion estimation and compensation driven neural network for video\nframe interpolation. A novel adaptive warping layer is developed to integrate\nboth optical flow and interpolation kernels to synthesize target frame pixels.\nThis layer is fully differentiable such that both the flow and kernel\nestimation networks can be optimized jointly. The proposed model benefits from\nthe advantages of motion estimation and compensation methods without using\nhand-crafted features. Compared to existing methods, our approach is\ncomputationally efficient and able to generate more visually appealing results.\nFurthermore, the proposed MEMC-Net can be seamlessly adapted to several video\nenhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive\nquantitative and qualitative evaluations demonstrate that the proposed method\nperforms favorably against the state-of-the-art video frame interpolation and\nenhancement algorithms on a wide range of datasets.\n", "rewritten_text": "Traditional video frame interpolation relied heavily on motion estimation (ME) and motion compensation (MC).  While recent deep learning approaches offer improvements, they typically focus on either flow or compensation kernels, hindering both speed and accuracy.  This paper introduces MEMC-Net, a neural network integrating ME and MC.  A novel, fully differentiable adaptive warping layer combines optical flow and interpolation kernels for superior target frame synthesis.  This joint optimization eliminates the need for handcrafted features, resulting in a computationally efficient method that produces visually superior results compared to existing state-of-the-art techniques.  Moreover, MEMC-Net readily adapts to other video enhancement tasks like super-resolution, denoising, and deblocking, as demonstrated by extensive quantitative and qualitative evaluations across diverse datasets.\n"}, "2303.12793": {"original_text": "  This work focuses on sign language retrieval-a recently proposed task for\nsign language understanding. Sign language retrieval consists of two sub-tasks:\ntext-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval.\nDifferent from traditional video-text retrieval, sign language videos, not only\ncontain visual signals but also carry abundant semantic meanings by themselves\ndue to the fact that sign languages are also natural languages. Considering\nthis character, we formulate sign language retrieval as a cross-lingual\nretrieval problem as well as a video-text retrieval task. Concretely, we take\ninto account the linguistic properties of both sign languages and natural\nlanguages, and simultaneously identify the fine-grained cross-lingual (i.e.,\nsign-to-word) mappings while contrasting the texts and the sign videos in a\njoint embedding space. This process is termed as cross-lingual contrastive\nlearning. Another challenge is raised by the data scarcity issue-sign language\ndatasets are orders of magnitude smaller in scale than that of speech\nrecognition. We alleviate this issue by adopting a domain-agnostic sign encoder\npre-trained on large-scale sign videos into the target domain via\npseudo-labeling. Our framework, termed as domain-aware sign language retrieval\nvia Cross-lingual Contrastive learning or CiCo for short, outperforms the\npioneering method by large margins on various datasets, e.g., +22.4 T2V and\n+28.0 V2T R@1 improvements on How2Sign dataset, and +13.7 T2V and +17.1 V2T R@1\nimprovements on PHOENIX-2014T dataset. Code and models are available at:\nhttps://github.com/FangyunWei/SLRT.\n", "rewritten_text": "This paper addresses sign language retrieval, a novel task in sign language understanding encompassing text-to-sign-video (T2V) and sign-video-to-text (V2T) retrieval.  Unlike traditional video-text retrieval, sign language videos inherently possess rich semantic meaning, akin to spoken language.  We thus frame this as a cross-lingual retrieval problem, leveraging linguistic properties of both sign and natural languages.  Our approach, Cross-lingual Contrastive learning (CiCo), jointly embeds texts and sign videos, identifying fine-grained sign-to-word mappings through contrastive learning.  To mitigate the challenge of limited sign language data, we employ a domain-agnostic sign encoder pre-trained with pseudo-labeling.  CiCo significantly outperforms existing methods, achieving substantial improvements (e.g., +22.4% T2V and +28.0% V2T R@1 on How2Sign, +13.7% T2V and +17.1% V2T R@1 on PHOENIX-2014T).  Code and models are available at https://github.com/FangyunWei/SLRT.\n"}, "2410.01928": {"original_text": "  In the domain of battery research, the processing of high-resolution\nmicroscopy images is a challenging task, as it involves dealing with complex\nimages and requires a prior understanding of the components involved. The\nutilization of deep learning methodologies for image analysis has attracted\nconsiderable interest in recent years, with multiple investigations employing\nsuch techniques for image segmentation and analysis within the realm of battery\nresearch. However, the automated analysis of high-resolution microscopy images\nfor detecting phases and components in composite materials is still an\nunderexplored area. This work proposes a novel workflow for detecting\ncomponents and phase segmentation from raw high resolution transmission\nelectron microscopy (TEM) images using a trained U-Net segmentation model. The\ndeveloped model can expedite the detection of components and phase\nsegmentation, diminishing the temporal and cognitive demands associated with\nscrutinizing an extensive array of TEM images, thereby mitigating the potential\nfor human errors. This approach presents a novel and efficient image analysis\napproach with broad applicability beyond the battery field and holds potential\nfor application in other related domains characterized by phase and composition\ndistribution, such as alloy production.\n", "rewritten_text": "Analyzing high-resolution microscopy images of batteries is difficult due to image complexity and the need for material expertise.  While deep learning offers promising image analysis solutions, automated phase and component detection in composite materials remains under-explored.  This work introduces a new workflow using a trained U-Net model to segment and identify components directly from raw high-resolution transmission electron microscopy (TEM) images. This automated approach significantly speeds up analysis, reduces human error, and lowers the time and cognitive burden of manually reviewing numerous images.  Its efficiency and broad applicability extend beyond battery research to other fields involving phase and composition analysis, such as alloy production.\n"}, "2103.15467": {"original_text": "  Unsupervised domain adaptation (UDA) becomes more and more popular in\ntackling real-world problems without ground truth of the target domain. Though\ntedious annotation work is not required, UDA unavoidably faces two problems: 1)\nhow to narrow the domain discrepancy to boost the transferring performance; 2)\nhow to improve pseudo annotation producing mechanism for self-supervised\nlearning (SSL). In this paper, we focus on UDA for semantic segmentation task.\nFirstly, we introduce adversarial learning into style gap bridging mechanism to\nkeep the style information from two domains in the similar space. Secondly, to\nkeep the balance of pseudo labels on each category, we propose a\ncategory-adaptive threshold mechanism to choose category-wise pseudo labels for\nSSL. The experiments are conducted using GTA5 as the source domain, Cityscapes\nas the target domain. The results show that our model outperforms the\nstate-of-the-arts with a noticeable gain on cross-domain adaptation tasks.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) is increasingly used to solve real-world problems lacking target domain ground truth.  While eliminating tedious annotation, UDA struggles with two key challenges: bridging the domain gap to improve transfer performance, and enhancing the pseudo-labeling mechanism for self-supervised learning (SSL). This paper addresses these challenges in the context of semantic segmentation, using GTA5 as the source and Cityscapes as the target domain.  We introduce adversarial learning to align the style information between domains and a novel category-adaptive threshold mechanism to balance pseudo-label generation for SSL.  Our experimental results demonstrate state-of-the-art performance, significantly improving cross-domain adaptation.\n"}, "2203.17013": {"original_text": "  Video streams are utilised to guide minimally-invasive surgery and diagnostic\nprocedures in a wide range of procedures, and many computer assisted techniques\nhave been developed to automatically analyse them. These approaches can provide\nadditional information to the surgeon such as lesion detection, instrument\nnavigation, or anatomy 3D shape modeling. However, the necessary image features\nto recognise these patterns are not always reliably detected due to the\npresence of irregular light patterns such as specular highlight reflections. In\nthis paper, we aim at removing specular highlights from endoscopic videos using\nmachine learning. We propose using a temporal generative adversarial network\n(GAN) to inpaint the hidden anatomy under specularities, inferring its\nappearance spatially and from neighbouring frames where they are not present in\nthe same location. This is achieved using in-vivo data of gastric endoscopy\n(Hyper-Kvasir) in a fully unsupervised manner that relies on automatic\ndetection of specular highlights. System evaluations show significant\nimprovements to traditional methods through direct comparison as well as other\nmachine learning techniques through an ablation study that depicts the\nimportance of the network's temporal and transfer learning components. The\ngeneralizability of our system to different surgical setups and procedures was\nalso evaluated qualitatively on in-vivo data of gastric endoscopy and ex-vivo\nporcine data (SERV-CT, SCARED). We also assess the effect of our method in\ncomputer vision tasks that underpin 3D reconstruction and camera motion\nestimation, namely stereo disparity, optical flow, and sparse point feature\nmatching. These are evaluated quantitatively and qualitatively and results show\na positive effect of specular highlight inpainting on these tasks in a novel\ncomprehensive analysis.\n", "rewritten_text": "Minimally invasive surgery and diagnostics rely heavily on video streams, often analyzed by computer-assisted techniques for tasks like lesion detection and 3D modeling.  However, specular reflections hinder accurate analysis. This paper presents a novel unsupervised machine learning approach using a temporal generative adversarial network (GAN) to remove these reflections from endoscopic videos.  The GAN inpaints hidden anatomy by leveraging spatial and temporal information from neighboring frames, trained on in-vivo gastric endoscopy data (Hyper-Kvasir).  Our method significantly outperforms traditional and other machine learning techniques, as demonstrated by direct comparison and ablation studies highlighting the importance of temporal and transfer learning.  Furthermore, we qualitatively and quantitatively evaluate its generalizability to different surgical settings (gastric endoscopy and ex-vivo porcine data: SERV-CT, SCARED) and its impact on downstream computer vision tasks such as 3D reconstruction (stereo disparity, optical flow, sparse point feature matching), showing improved performance.\n"}, "2010.02808": {"original_text": "  We propose a method to learn image representations from uncurated videos. We\ncombine a supervised loss from off-the-shelf object detectors and\nself-supervised losses which naturally arise from the video-shot-frame-object\nhierarchy present in each video. We report competitive results on 19 transfer\nlearning tasks of the Visual Task Adaptation Benchmark (VTAB), and on 8\nout-of-distribution-generalization tasks, and discuss the benefits and\nshortcomings of the proposed approach. In particular, it improves over the\nbaseline on all 18/19 few-shot learning tasks and 8/8 out-of-distribution\ngeneralization tasks. Finally, we perform several ablation studies and analyze\nthe impact of the pretrained object detector on the performance across this\nsuite of tasks.\n", "rewritten_text": "This paper introduces a novel method for learning image representations directly from uncurated video data.  Leveraging a supervised loss from existing object detectors and self-supervised losses derived from the inherent video structure (shot-frame-object hierarchy), our approach achieves state-of-the-art results on 19 Visual Task Adaptation Benchmark (VTAB) transfer learning tasks and 8 out-of-distribution generalization tasks.  Significantly, it surpasses the baseline on all but one few-shot learning task and all out-of-distribution generalization tasks.  Furthermore, we conduct ablation studies to analyze the contribution of the pretrained object detector to overall performance.\n"}, "1903.12529": {"original_text": "  While deep neural networks (DNN) based single image super-resolution (SISR)\nmethods are rapidly gaining popularity, they are mainly designed for the\nwidely-used bicubic degradation, and there still remains the fundamental\nchallenge for them to super-resolve low-resolution (LR) image with arbitrary\nblur kernels. In the meanwhile, plug-and-play image restoration has been\nrecognized with high flexibility due to its modular structure for easy plug-in\nof denoiser priors. In this paper, we propose a principled formulation and\nframework by extending bicubic degradation based deep SISR with the help of\nplug-and-play framework to handle LR images with arbitrary blur kernels.\nSpecifically, we design a new SISR degradation model so as to take advantage of\nexisting blind deblurring methods for blur kernel estimation. To optimize the\nnew degradation induced energy function, we then derive a plug-and-play\nalgorithm via variable splitting technique, which allows us to plug any\nsuper-resolver prior rather than the denoiser prior as a modular part.\nQuantitative and qualitative evaluations on synthetic and real LR images\ndemonstrate that the proposed deep plug-and-play super-resolution framework is\nflexible and effective to deal with blurry LR images.\n", "rewritten_text": "Deep neural networks (DNNs) excel at single image super-resolution (SISR) but primarily address bicubic downsampling.  Handling arbitrary blur kernels remains a significant challenge.  This paper introduces a novel framework that combines the flexibility of plug-and-play image restoration with DNN-based SISR to address this limitation.  We propose a new SISR degradation model incorporating blind deblurring for kernel estimation and derive a plug-and-play algorithm using variable splitting. This allows the integration of any super-resolution prior, not just denoisers.  Experiments on synthetic and real blurry low-resolution (LR) images demonstrate the framework's effectiveness and flexibility.\n"}, "2407.05092": {"original_text": "  Computational and human perception are often considered separate approaches\nfor studying sound changes over time; few works have touched on the\nintersection of both. To fill this research gap, we provide a pioneering review\ncontrasting computational with human perception from the perspectives of\nmethods and tasks. Overall, computational approaches rely on computer-driven\nmodels to perceive historical sound changes on etymological datasets, while\nhuman approaches use listener-driven models to perceive ongoing sound changes\non recording corpora. Despite their differences, both approaches complement\neach other on phonetic and acoustic levels, showing the potential to achieve a\nmore comprehensive perception of sound change. Moreover, we call for a\ncomparative study on the datasets used by both approaches to investigate the\ninfluence of historical sound changes on ongoing changes. Lastly, we discuss\nthe applications of sound change in computational linguistics, and point out\nthat perceiving sound change alone is insufficient, as many processes of\nlanguage change are complex, with entangled changes at syntactic, semantic, and\nphonetic levels.\n", "rewritten_text": "This review bridges a research gap by comparing computational and human approaches to studying sound change.  Computational methods utilize computer models to analyze historical sound changes in etymological data, while human-based methods employ listener judgments on recorded speech to study ongoing changes.  Despite their differing methodologies and datasets (historical vs. contemporary), these approaches are complementary, offering a more holistic understanding of phonetic and acoustic aspects of sound change.  Future research should compare the datasets used to explore the interplay between historical and ongoing changes.  Finally, while computational linguistics benefits from understanding sound change, it's crucial to acknowledge the complexity of language evolution, encompassing syntactic, semantic, and phonetic levels beyond just phonetic shifts.\n"}, "1812.03621": {"original_text": "  The majority of Multi-Object Tracking (MOT) algorithms based on the\ntracking-by-detection scheme do not use higher order dependencies among objects\nor tracklets, which makes them less effective in handling complex scenarios. In\nthis work, we present a new near-online MOT algorithm based on non-uniform\nhypergraph, which can model different degrees of dependencies among tracklets\nin a unified objective. The nodes in the hypergraph correspond to the tracklets\nand the hyperedges with different degrees encode various kinds of dependencies\namong them. Specifically, instead of setting the weights of hyperedges with\ndifferent degrees empirically, they are learned automatically using the\nstructural support vector machine algorithm (SSVM). Several experiments are\ncarried out on various challenging datasets (i.e., PETS09, ParkingLot sequence,\nSubwayFace, and MOT16 benchmark), to demonstrate that our method achieves\nfavorable performance against the state-of-the-art MOT methods.\n", "rewritten_text": "Most tracking-by-detection Multi-Object Tracking (MOT) algorithms ignore higher-order object dependencies, limiting their performance in complex situations.  This paper introduces a novel near-online MOT algorithm using a non-uniform hypergraph to model varying degrees of tracklet dependencies within a unified framework.  Tracklets are represented as hypergraph nodes, and hyperedges of varying sizes encode different dependency types.  Crucially, hyperedge weights are learned automatically using a Structural Support Vector Machine (SSVM), rather than being empirically set.  Experiments on challenging datasets (PETS09, ParkingLot, SubwayFace, and MOT16) demonstrate state-of-the-art performance.\n"}, "2211.08358": {"original_text": "  Few-shot classification has made great strides due to foundation models that,\nthrough priming and prompting, are highly effective few-shot learners. However,\nthis approach has high variance both across different sets of few shots (data\nselection) and across different finetuning runs (run variability). This is\nproblematic not only because it impedes the fair comparison of different\napproaches, but especially because it makes few-shot learning too unreliable\nfor many real-world applications. To alleviate these issues, we make two\ncontributions for more stable and effective few-shot learning: First, we\npropose novel ensembling methods and show that they substantially reduce run\nvariability. Second, we introduce a new active learning (AL) criterion for data\nselection and present the first AL-based approach specifically tailored towards\nprompt-based learning. In our experiments, we show that our combined method,\nMEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),\nimproves overall performance of prompt-based finetuning by 2.3 points on five\ndiverse tasks. We publicly share our code and data splits in\nhttps://github.com/akoksal/MEAL.\n", "rewritten_text": "Foundation models have significantly advanced few-shot classification through prompting and priming.  However, this approach suffers from high variability, both in data selection and model training runs, hindering fair comparisons and limiting real-world applicability.  To address this, we introduce MEAL (Multiprompt finetuning and prediction Ensembling with Active Learning), a novel approach combining ensemble methods to reduce run variability and a new active learning criterion for improved data selection specifically designed for prompt-based learning.  Our experiments demonstrate a 2.3-point performance improvement across five diverse tasks.  Code and data are publicly available at https://github.com/akoksal/MEAL.\n"}, "1909.09934": {"original_text": "  We propose methods to train convolutional neural networks (CNNs) with both\nbinarized weights and activations, leading to quantized models that are\nspecifically friendly to mobile devices with limited power capacity and\ncomputation resources. Previous works on quantizing CNNs often seek to\napproximate the floating-point information using a set of discrete values,\nwhich we call value approximation, typically assuming the same architecture as\nthe full-precision networks. Here we take a novel \"structure approximation\"\nview of quantization -- it is very likely that different architectures designed\nfor low-bit networks may be better for achieving good performance. In\nparticular, we propose a \"network decomposition\" strategy, termed Group-Net, in\nwhich we divide the network into groups. Thus, each full-precision group can be\neffectively reconstructed by aggregating a set of homogeneous binary branches.\nIn addition, we learn effective connections among groups to improve the\nrepresentation capability. Moreover, the proposed Group-Net shows strong\ngeneralization to other tasks. For instance, we extend Group-Net for accurate\nsemantic segmentation by embedding rich context into the binary structure.\nFurthermore, for the first time, we apply binary neural networks to object\ndetection. Experiments on both classification, semantic segmentation and object\ndetection tasks demonstrate the superior performance of the proposed methods\nover various quantized networks in the literature. Our methods outperform the\nprevious best binary neural networks in terms of accuracy and computation\nefficiency.\n", "rewritten_text": "This paper introduces novel methods for training highly efficient binary convolutional neural networks (CNNs) suitable for resource-constrained mobile devices.  Unlike previous work focusing on approximating floating-point CNNs with low-bit representations (value approximation), we propose a \"structure approximation\" approach.  Our Group-Net architecture decomposes the network into groups of binary branches, reconstructing full-precision equivalents through aggregation.  Inter-group connections further enhance representational power.  This approach demonstrates strong generalization, enabling successful application to semantic segmentation (by incorporating contextual information) and, for the first time, object detection.  Extensive experiments across classification, segmentation, and detection tasks show Group-Net surpasses existing quantized networks in both accuracy and computational efficiency.\n"}, "2407.04024": {"original_text": "  Deep unfolding methods and transformer architecture have recently shown\npromising results in hyperspectral image (HSI) reconstruction. However, there\nstill exist two issues: (1) in the data subproblem, most methods represents the\nstepsize utilizing a learnable parameter. Nevertheless, for different spectral\nchannel, error between features and ground truth is unequal. (2) Transformer\nstruggles to balance receptive field size with pixel-wise detail information.\nTo overcome the aforementioned drawbacks, We proposed an adaptive step-size\nperception unfolding network (ASPUN), a deep unfolding network based on FISTA\nalgorithm, which uses an adaptive step-size perception module to estimate the\nupdate step-size of each spectral channel. In addition, we design a Non-local\nHybrid Attention Transformer(NHAT) module for fully leveraging the receptive\nfield advantage of transformer. By plugging the NLHA into the Non-local\nInformation Aggregation (NLIA) module, the unfolding network can achieve better\nreconstruction results. Experimental results show that our ASPUN is superior to\nthe existing SOTA algorithms and achieves the best performance.\n", "rewritten_text": "Recent advances in hyperspectral image (HSI) reconstruction using deep unfolding methods and transformer architectures have yielded encouraging results.  However, challenges remain:  existing methods often use a single, learned step size for all spectral channels, despite varying reconstruction errors, and transformers struggle to simultaneously capture large receptive fields and fine-grained detail.  To address these limitations, we introduce the Adaptive Step-size Perception Unfolding Network (ASPUN).  ASPUN, a deep unfolding network based on the FISTA algorithm, incorporates an adaptive step-size perception module to individually adjust the update step size for each spectral channel.  Furthermore, a novel Non-local Hybrid Attention Transformer (NHAT) module enhances the network's ability to leverage the benefits of large receptive fields.  Integrating NHAT into a Non-local Information Aggregation (NLIA) module significantly improves reconstruction performance.  Experimental results demonstrate that ASPUN surpasses state-of-the-art methods, achieving the best performance.\n"}, "1810.10519": {"original_text": "  With the increasing use of social networks and mobile devices, the number of\nvideos posted on the Internet is growing exponentially. Among the inappropriate\ncontents published on the Internet, pornography is one of the most worrying as\nit can be accessed by teens and children. Two spatiotemporal CNNs, VGG-C3D CNN\nand ResNet R(2+1)D CNN, were assessed for pornography detection in videos in\nthe present study. Experimental results using the Pornography-800 dataset\nshowed that these spatiotemporal CNNs performed better than some\nstate-of-the-art methods based on bag of visual words and are competitive with\nother CNN-based approaches, reaching accuracy of 95.1%.\n", "rewritten_text": "The proliferation of online video content, fueled by social media and mobile devices, has led to a concerning rise in easily accessible pornography, particularly for children and teens.  This study evaluated two spatiotemporal Convolutional Neural Networks (CNNs), VGG-C3D and ResNet R(2+1)D, for their effectiveness in detecting pornography in videos.  Using the Pornography-800 dataset, results demonstrated that these CNNs outperformed existing bag-of-visual-words methods and achieved competitive accuracy (95.1%) compared to other CNN-based approaches.\n"}, "2406.10209": {"original_text": "  Large language models can memorize and repeat their training data, causing\nprivacy and copyright risks. To mitigate memorization, we introduce a subtle\nmodification to the next-token training objective that we call the goldfish\nloss. During training, randomly sampled subsets of tokens are excluded from the\nloss computation. These dropped tokens are not memorized by the model, which\nprevents verbatim reproduction of a complete chain of tokens from the training\nset. We run extensive experiments training billion-scale Llama-2 models, both\npre-trained and trained from scratch, and demonstrate significant reductions in\nextractable memorization with little to no impact on downstream benchmarks.\n", "rewritten_text": "Large language models risk memorizing and reproducing their training data, raising privacy and copyright concerns.  To address this, we propose the \"goldfish loss,\" a modified training objective that randomly omits tokens from the loss calculation. This prevents the model from memorizing and verbatimly replicating training data sequences.  Experiments with billion-parameter Llama-2 models, both pre-trained and trained from scratch, show substantial reductions in memorization with negligible impact on performance.\n"}, "2110.14398": {"original_text": "  To consider Hawrami and Zaza (Zazaki) standalone languages or dialects of a\nlanguage have been discussed and debated for a while among linguists active in\nstudying Iranian languages. The question of whether those languages/dialects\nbelong to the Kurdish language or if they are independent descendants of\nIranian languages was answered by MacKenzie (1961). However, a majority of\npeople who speak the dialects are against that answer. Their disapproval mainly\nseems to be based on the sociological, cultural, and historical relationship\namong the speakers of the dialects. While the case of Hawrami and Zaza has\nremained unexplored and under-examined, an almost unanimous agreement exists\nabout the classification of Kurmanji and Sorani as Kurdish dialects. The\nrelated studies to address the mentioned cases are primarily qualitative.\nHowever, computational linguistics could approach the question from a\nquantitative perspective. In this research, we look into three questions from a\nlinguistic distance point of view. First, how similar or dissimilar Hawrami and\nZaza are, considering no common geographical coexistence between the two.\nSecond, what about Kurmanji and Sorani that have geographical overlap. Finally,\nwhat is the distance among all these dialects, pair by pair? We base our\ncomputation on phonetic presentations of these dialects (languages), and we\ncalculate various linguistic distances among the pairs. We analyze the data and\ndiscuss the results to conclude.\n", "rewritten_text": "The classification of Hawrami and Zaza (Zazaki) as independent languages or Kurdish dialects has long been debated among Iranian linguistics scholars.  While MacKenzie (1961) offered a definitive classification, it's largely rejected by the speakers themselves, primarily due to socio-cultural and historical factors.  Conversely, Kurmanji and Sorani are widely accepted as Kurdish dialects.  Existing research on this topic is predominantly qualitative.  This study employs computational linguistics to offer a quantitative perspective, investigating the linguistic distances between Hawrami and Zaza (despite their geographical separation), between Kurmanji and Sorani (which share a geographical area), and between all four dialects pairwise.  Using phonetic data, we calculate various linguistic distances and analyze the results.\n"}, "2401.14718": {"original_text": "  Future Frame Synthesis (FFS) aims to enable models to generate sequences of\nfuture frames based on existing content. This survey comprehensively reviews\nhistorical and contemporary works in FFS, including widely used datasets and\nalgorithms. It scrutinizes the challenges and the evolving landscape of FFS\nwithin computer vision, with a focus on the transition from deterministic to\ngenerative synthesis methodologies. Our taxonomy highlights the significant\nadvancements and shifts in approach, underscoring the growing importance of\ngenerative models in achieving realistic and diverse future frame predictions.\n", "rewritten_text": "This survey provides a comprehensive overview of Future Frame Synthesis (FFS), examining its evolution from deterministic to generative methods.  We analyze existing FFS techniques, datasets, and challenges within computer vision, highlighting the increasing dominance of generative models in producing realistic and varied predictions of future frames from existing content.\n"}, "1808.0841": {"original_text": "  Training a good deep learning model often requires a lot of annotated data.\nAs a large amount of labeled data is typically difficult to collect and even\nmore difficult to annotate, data augmentation and data generation are widely\nused in the process of training deep neural networks. However, there is no\nclear common understanding on how much labeled data is needed to get\nsatisfactory performance. In this paper, we try to address such a question\nusing vehicle license plate character recognition as an example application. We\napply computer graphic scripts and Generative Adversarial Networks to generate\nand augment a large number of annotated, synthesized license plate images with\nrealistic colors, fonts, and character composition from a small number of real,\nmanually labeled license plate images. Generated and augmented data are mixed\nand used as training data for the license plate recognition network modified\nfrom DenseNet. The experimental results show that the model trained from the\ngenerated mixed training data has good generalization ability, and the proposed\napproach achieves a new state-of-the-art accuracy on Dataset-1 and AOLP, even\nwith a very limited number of original real license plates. In addition, the\naccuracy improvement caused by data generation becomes more significant when\nthe number of labeled images is reduced. Data augmentation also plays a more\nsignificant role when the number of labeled images is increased.\n", "rewritten_text": "Deep learning models typically require extensive annotated data for effective training.  Acquiring and annotating such data is challenging, prompting the widespread use of data augmentation and generation techniques.  This paper investigates the optimal amount of labeled data needed for satisfactory performance, focusing on vehicle license plate character recognition.  We leverage computer graphics and Generative Adversarial Networks (GANs) to synthesize a large, realistically annotated dataset from a small set of real, manually labeled images.  This augmented dataset, combined with the original data, trains a modified DenseNet model.  Experiments demonstrate that this approach achieves state-of-the-art accuracy on Dataset-1 and AOLP, even with limited real data.  Furthermore, data generation's impact is amplified with smaller datasets, while data augmentation becomes more crucial with larger ones.\n"}, "2401.03183": {"original_text": "  Defeasibility in causal reasoning implies that the causal relationship\nbetween cause and effect can be strengthened or weakened. Namely, the causal\nstrength between cause and effect should increase or decrease with the\nincorporation of strengthening arguments (supporters) or weakening arguments\n(defeaters), respectively. However, existing works ignore defeasibility in\ncausal reasoning and fail to evaluate existing causal strength metrics in\ndefeasible settings. In this work, we present $\\delta$-CAUSAL, the first\nbenchmark dataset for studying defeasibility in causal reasoning.\n$\\delta$-CAUSAL includes around 11K events spanning ten domains, featuring\ndefeasible causality pairs, i.e., cause-effect pairs accompanied by supporters\nand defeaters. We further show current causal strength metrics fail to reflect\nthe change of causal strength with the incorporation of supporters or defeaters\nin $\\delta$-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation\nwith Attention Rating), a metric that measures causal strength based on\ntoken-level causal relationships. CESAR achieves a significant 69.7% relative\nimprovement over existing metrics, increasing from 47.2% to 80.1% in capturing\nthe causal strength change brought by supporters and defeaters. We further\ndemonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and\n10.7 points behind humans in generating supporters and defeaters, emphasizing\nthe challenge posed by $\\delta$-CAUSAL.\n", "rewritten_text": "Causal reasoning often involves defeasibility\u2014the ability for causal relationships to be strengthened or weakened by supporting or opposing evidence.  Existing research overlooks this, failing to assess causal strength metrics in such contexts.  This paper introduces $\\delta$-CAUSAL, a new benchmark dataset (\u224811,000 events across ten domains) containing cause-effect pairs with accompanying supporting and opposing arguments (supporters and defeaters).  We demonstrate that current causal strength metrics inadequately capture the impact of these arguments on causal strength.  To address this, we propose CESAR (Causal Embedding Association with Attention Rating), a novel token-level metric that significantly outperforms existing methods, achieving a 69.7% relative improvement (from 47.2% to 80.1%) in reflecting the influence of supporters and defeaters.  Furthermore, even advanced LLMs like GPT-3.5 struggle to generate these arguments, highlighting the difficulty and importance of the $\\delta$-CAUSAL benchmark.\n"}, "1310.5767": {"original_text": "  Salient object detection aims to locate objects that capture human attention\nwithin images. Previous approaches often pose this as a problem of image\ncontrast analysis. In this work, we model an image as a hypergraph that\nutilizes a set of hyperedges to capture the contextual properties of image\npixels or regions. As a result, the problem of salient object detection becomes\none of finding salient vertices and hyperedges in the hypergraph. The main\nadvantage of hypergraph modeling is that it takes into account each pixel's (or\nregion's) affinity with its neighborhood as well as its separation from image\nbackground. Furthermore, we propose an alternative approach based on\ncenter-versus-surround contextual contrast analysis, which performs salient\nobject detection by optimizing a cost-sensitive support vector machine (SVM)\nobjective function. Experimental results on four challenging datasets\ndemonstrate the effectiveness of the proposed approaches against the\nstate-of-the-art approaches to salient object detection.\n", "rewritten_text": "This paper addresses salient object detection, the task of identifying visually striking objects in images.  Unlike previous methods relying solely on image contrast, we propose two novel approaches.  The first represents the image as a hypergraph, enabling the identification of salient vertices and hyperedges by considering both local pixel/region affinities and background separation.  The second approach uses center-surround contextual contrast analysis optimized via a cost-sensitive SVM.  Experiments on four benchmark datasets show that both methods outperform existing state-of-the-art techniques.\n"}, "2111.02394": {"original_text": "  We propose an accurate and efficient scene text detection framework, termed\nFAST (i.e., faster arbitrarily-shaped text detector). Different from recent\nadvanced text detectors that used complicated post-processing and hand-crafted\nnetwork architectures, resulting in low inference speed, FAST has two new\ndesigns. (1) We design a minimalist kernel representation (only has 1-channel\noutput) to model text with arbitrary shape, as well as a GPU-parallel\npost-processing to efficiently assemble text lines with a negligible time\noverhead. (2) We search the network architecture tailored for text detection,\nleading to more powerful features than most networks that are searched for\nimage classification. Benefiting from these two designs, FAST achieves an\nexcellent trade-off between accuracy and efficiency on several challenging\ndatasets, including Total Text, CTW1500, ICDAR 2015, and MSRA-TD500. For\nexample, FAST-T yields 81.6% F-measure at 152 FPS on Total-Text, outperforming\nthe previous fastest method by 1.7 points and 70 FPS in terms of accuracy and\nspeed. With TensorRT optimization, the inference speed can be further\naccelerated to over 600 FPS. Code and models will be released at\nhttps://github.com/czczup/FAST.\n", "rewritten_text": "FAST (Faster Arbitrarily-Shaped Text Detector) is a new scene text detection framework prioritizing both accuracy and speed.  Unlike existing methods that rely on complex post-processing and handcrafted architectures, leading to slow inference, FAST employs two key innovations: a minimalist, single-channel kernel representation for arbitrary-shaped text and a GPU-parallel post-processing step with minimal overhead.  Furthermore, its architecture is specifically optimized for text detection, resulting in more powerful features than those found in typical image classification networks.  This approach achieves a superior balance of accuracy and efficiency on benchmark datasets (Total Text, CTW1500, ICDAR 2015, MSRA-TD500).  For instance, FAST-T achieves an 81.6% F-measure at 152 FPS on Total-Text, surpassing the previous state-of-the-art by 1.7 points in accuracy and 70 FPS in speed.  TensorRT optimization further boosts speed to over 600 FPS.  The code and models are available at https://github.com/czczup/FAST.\n"}, "2403.17856": {"original_text": "  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.\n", "rewritten_text": "English morphology is characterized by its flexible use of words across different grammatical categories, a process called conversion (or zero-derivation).  While conversion significantly impacts the English lexicon, its representation in language models remains largely unexplored. This paper presents the first study investigating how large language models handle word conversion.  We developed a natural language inference task designed to assess lexical-syntactic flexibility\u2014the models' ability to generalize word usage across non-prototypical grammatical contexts.  Five models were evaluated: GPT-3.5 and GPT-4 (proprietary), and Mistral 7B, Falcon 40B, and Llama 2 70B (open-source).  GPT-4 achieved the best performance, followed by GPT-3.5.  However, the open-source models also demonstrated proficiency, with Mistral 7B showing surprisingly little performance difference between standard natural language inference and the conversion task, comparable to the much larger GPT-4.\n"}, "1712.05277": {"original_text": "  Depth cameras allow to set up reliable solutions for people monitoring and\nbehavior understanding, especially when unstable or poor illumination\nconditions make unusable common RGB sensors. Therefore, we propose a complete\nframework for the estimation of the head and shoulder pose based on depth\nimages only. A head detection and localization module is also included, in\norder to develop a complete end-to-end system. The core element of the\nframework is a Convolutional Neural Network, called POSEidon+, that receives as\ninput three types of images and provides the 3D angles of the pose as output.\nMoreover, a Face-from-Depth component based on a Deterministic Conditional GAN\nmodel is able to hallucinate a face from the corresponding depth image. We\nempirically demonstrate that this positively impacts the system performances.\nWe test the proposed framework on two public datasets, namely Biwi Kinect Head\nPose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by\nthe automotive setup. Experimental results show that our method overcomes\nseveral recent state-of-art works based on both intensity and depth input data,\nrunning in real-time at more than 30 frames per second.\n", "rewritten_text": "This paper presents POSEidon+, a real-time (30+ fps) framework for accurate head and shoulder pose estimation using only depth images.  This approach is particularly valuable in low-light or unstable lighting conditions where standard RGB cameras fail.  The framework incorporates head detection and localization, and utilizes a convolutional neural network (CNN) that processes three types of depth image inputs to output 3D pose angles.  Furthermore, a Face-from-Depth component, leveraging a Deterministic Conditional GAN, synthesizes facial features from depth data, improving overall performance.  Evaluated on Biwi Kinect Head Pose, ICT-3DHP, and the challenging new Pandora dataset (inspired by automotive applications), our method outperforms existing state-of-the-art techniques, even those using both intensity and depth data.\n"}, "2404.19287": {"original_text": "  Pretrained vision-language models (VLMs) like CLIP exhibit exceptional\ngeneralization across diverse downstream tasks. While recent studies reveal\ntheir vulnerability to adversarial attacks, research to date has primarily\nfocused on enhancing the robustness of image encoders against image-based\nattacks, with defenses against text-based and multimodal attacks remaining\nlargely unexplored. To this end, this work presents the first comprehensive\nstudy on improving the adversarial robustness of VLMs against attacks targeting\nimage, text, and multimodal inputs. This is achieved by proposing multimodal\ncontrastive adversarial training (MMCoA). Such an approach strengthens the\nrobustness of both image and text encoders by aligning the clean text\nembeddings with adversarial image embeddings, and adversarial text embeddings\nwith clean image embeddings. The robustness of the proposed MMCoA is examined\nagainst existing defense methods over image, text, and multimodal attacks on\nthe CLIP model. Extensive experiments on 15 datasets across two tasks reveal\nthe characteristics of different adversarial defense methods under distinct\ndistribution shifts and dataset complexities across the three attack types.\nThis paves the way for a unified framework of adversarial robustness against\ndifferent modality attacks, opening up new possibilities for securing VLMs\nagainst multimodal attacks. The code is available at\nhttps://github.com/ElleZWQ/MMCoA.git.\n", "rewritten_text": "Vision-language models (VLMs) like CLIP generalize well but are vulnerable to adversarial attacks.  Existing research primarily focuses on image-based defenses, neglecting text-based and multimodal attacks.  This paper introduces Multimodal Contrastive Adversarial Training (MMCoA), the first comprehensive study improving VLM robustness against all three attack types.  MMCoA enhances both image and text encoders by aligning clean and adversarial embeddings across modalities.  Experiments on 15 datasets across two tasks compare MMCoA to existing defenses, revealing its effectiveness under various distribution shifts and dataset complexities.  This unified framework significantly advances VLM security against multimodal attacks.  Code is available at https://github.com/ElleZWQ/MMCoA.git.\n"}, "2211.09469": {"original_text": "  Generating consecutive descriptions for videos, i.e., Video Captioning,\nrequires taking full advantage of visual representation along with the\ngeneration process. Existing video captioning methods focus on making an\nexploration of spatial-temporal representations and their relationships to\nproduce inferences. However, such methods only exploit the superficial\nassociation contained in the video itself without considering the intrinsic\nvisual commonsense knowledge that existed in a video dataset, which may hinder\ntheir capabilities of knowledge cognitive to reason accurate descriptions. To\naddress this problem, we propose a simple yet effective method, called Visual\nCommonsense-aware Representation Network (VCRN), for video captioning.\nSpecifically, we construct a Video Dictionary, a plug-and-play component,\nobtained by clustering all video features from the total dataset into multiple\nclustered centers without additional annotation. Each center implicitly\nrepresents a visual commonsense concept in the video domain, which is utilized\nin our proposed Visual Concept Selection (VCS) to obtain a video-related\nconcept feature. Next, a Conceptual Integration Generation (CIG) is proposed to\nenhance the caption generation. Extensive experiments on three publicly video\ncaptioning benchmarks: MSVD, MSR-VTT, and VATEX, demonstrate that our method\nreaches state-of-the-art performance, indicating the effectiveness of our\nmethod. In addition, our approach is integrated into the existing method of\nvideo question answering and improves this performance, further showing the\ngeneralization of our method. Source code has been released at\nhttps://github.com/zchoi/VCRN.\n", "rewritten_text": "Video captioning, generating consecutive descriptions of videos, benefits from leveraging both visual representation and the generation process.  Current methods primarily focus on spatial-temporal relationships, neglecting the inherent visual commonsense knowledge within the data.  To address this, we introduce Visual Commonsense-aware Representation Network (VCRN), a novel method that incorporates a \"Video Dictionary.\" This dictionary, created by clustering video features without extra annotation, represents visual commonsense concepts.  These concepts are used in our Visual Concept Selection (VCS) and Conceptual Integration Generation (CIG) modules to improve caption generation.  Experiments on MSVD, MSR-VTT, and VATEX benchmarks show state-of-the-art results, and integration with a video question answering system further demonstrates VCRN's effectiveness and generalizability.  Source code is available at https://github.com/zchoi/VCRN.\n"}, "2203.12788": {"original_text": "  A fundamental characteristic of natural language is the high rate at which\nspeakers produce novel expressions. Because of this novelty, a heavy-tail of\nrare events accounts for a significant amount of the total probability mass of\ndistributions in language (Baayen, 2001). Standard language modeling metrics\nsuch as perplexity quantify the performance of language models (LM) in\naggregate. As a result, we have relatively little understanding of whether\nneural LMs accurately estimate the probability of sequences in this heavy-tail\nof rare events. To address this gap, we develop a controlled evaluation scheme\nwhich uses generative models trained on natural data as artificial languages\nfrom which we can exactly compute sequence probabilities. Training LMs on\ngenerations from these artificial languages, we compare the sequence-level\nprobability estimates given by LMs to the true probabilities in the target\nlanguage. Our experiments reveal that LSTM and Transformer language models (i)\nsystematically underestimate the probability of sequences drawn from the target\nlanguage, and (ii) do so more severely for less-probable sequences.\nInvestigating where this probability mass went, (iii) we find that LMs tend to\noverestimate the probability of ill formed (perturbed) sequences. In addition,\nwe find that this underestimation behaviour (iv) is weakened, but not\neliminated by greater amounts of training data, and (v) is exacerbated for\ntarget distributions with lower entropy.\n", "rewritten_text": "Natural language's inherent novelty, characterized by a high frequency of novel expressions and a heavy-tailed distribution of rare events (Baayen, 2001), poses a challenge for evaluating language models (LMs).  Standard metrics like perplexity offer an aggregate view, obscuring LM performance on these rare events.  To address this, we created a controlled evaluation using generative models of natural language as artificial languages with known sequence probabilities.  Training LMs on these artificial languages, we found that LSTM and Transformer LMs systematically underestimate the probability of sequences, particularly less probable ones.  This underestimation stems from overestimating the probability of ill-formed sequences.  Furthermore, increasing training data only partially mitigates this issue, which is worsened by lower-entropy target distributions.\n"}, "2207.09884": {"original_text": "  We present a Momentum Re-identification (MoReID) framework that can leverage\na very large number of negative samples in training for general\nre-identification task. The design of this framework is inspired by Momentum\nContrast (MoCo), which uses a dictionary to store current and past batches to\nbuild a large set of encoded samples. As we find it less effective to use past\npositive samples which may be highly inconsistent to the encoded feature\nproperty formed with the current positive samples, MoReID is designed to use\nonly a large number of negative samples stored in the dictionary. However, if\nwe train the model using the widely used Triplet loss that uses only one sample\nto represent a set of positive/negative samples, it is hard to effectively\nleverage the enlarged set of negative samples acquired by the MoReID framework.\nTo maximize the advantage of using the scaled-up negative sample set, we newly\nintroduce Hard-distance Elastic loss (HE loss), which is capable of using more\nthan one hard sample to represent a large number of samples. Our experiments\ndemonstrate that a large number of negative samples provided by MoReID\nframework can be utilized at full capacity only with the HE loss, achieving the\nstate-of-the-art accuracy on three re-ID benchmarks, VeRi-776, Market-1501, and\nVeRi-Wild.\n", "rewritten_text": "This paper introduces MoReID, a novel momentum-based re-identification framework capable of effectively utilizing massive negative training samples.  Inspired by MoCo, MoReID employs a dictionary to store encoded features, but unlike MoCo, it excludes past positive samples due to potential inconsistencies with current features.  To fully leverage this expanded negative sample set, we propose a new loss function, Hard-distance Elastic (HE) loss, which can handle multiple hard samples, unlike the traditional triplet loss.  Experiments on VeRi-776, Market-1501, and VeRi-Wild benchmarks demonstrate that MoReID with HE loss achieves state-of-the-art performance by fully exploiting the large-scale negative samples.\n"}, "2410.12158": {"original_text": "  Foundation models have significantly enhanced 2D task performance, and recent\nworks like Bridge3D have successfully applied these models to improve 3D scene\nunderstanding through knowledge distillation, marking considerable\nadvancements. Nonetheless, challenges such as the misalignment between 2D and\n3D representations and the persistent long-tail distribution in 3D datasets\nstill restrict the effectiveness of knowledge distillation from 2D to 3D using\nfoundation models. To tackle these issues, we introduce a novel SAM-guided\ntokenization method that seamlessly aligns 3D transformer structures with\nregion-level knowledge distillation, replacing the traditional KNN-based\ntokenization techniques. Additionally, we implement a group-balanced\nre-weighting strategy to effectively address the long-tail problem in knowledge\ndistillation. Furthermore, inspired by the recent success of masked feature\nprediction, our framework incorporates a two-stage masked token prediction\nprocess in which the student model predicts both the global embeddings and the\ntoken-wise local embeddings derived from the teacher models trained in the\nfirst stage. Our methodology has been validated across multiple datasets,\nincluding SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and\nsemantic segmentation. The results demonstrate significant improvements over\ncurrent State-of-the-art self-supervised methods, establishing new benchmarks\nin this field.\n", "rewritten_text": "Foundation models have boosted 2D task performance, and this success has been extended to 3D scene understanding using techniques like knowledge distillation (e.g., Bridge3D).  However, discrepancies between 2D and 3D representations and the inherent imbalance in 3D datasets hinder effective 2D-to-3D knowledge distillation.  To overcome these limitations, we propose a novel framework.  This framework uses a SAM-guided tokenization method for improved alignment between 3D transformer structures and region-level knowledge distillation, replacing KNN-based methods.  A group-balanced re-weighting strategy addresses the long-tail distribution problem.  Furthermore, a two-stage masked token prediction process, inspired by masked feature prediction, allows the student model to predict both global and local embeddings from a pre-trained teacher model.  Evaluated on SUN RGB-D, ScanNet, and S3DIS for 3D object detection and semantic segmentation, our approach significantly outperforms existing self-supervised methods, setting new state-of-the-art benchmarks.\n"}, "2307.09892": {"original_text": "  We propose 3Deformer, a general-purpose framework for interactive 3D shape\nediting. Given a source 3D mesh with semantic materials, and a user-specified\nsemantic image, 3Deformer can accurately edit the source mesh following the\nshape guidance of the semantic image, while preserving the source topology as\nrigid as possible. Recent studies of 3D shape editing mostly focus on learning\nneural networks to predict 3D shapes, which requires high-cost 3D training\ndatasets and is limited to handling objects involved in the datasets. Unlike\nthese studies, our 3Deformer is a non-training and common framework, which only\nrequires supervision of readily-available semantic images, and is compatible\nwith editing various objects unlimited by datasets. In 3Deformer, the source\nmesh is deformed utilizing the differentiable renderer technique, according to\nthe correspondences between semantic images and mesh materials. However,\nguiding complex 3D shapes with a simple 2D image incurs extra challenges, that\nis, the deform accuracy, surface smoothness, geometric rigidity, and global\nsynchronization of the edited mesh should be guaranteed. To address these\nchallenges, we propose a hierarchical optimization architecture to balance the\nglobal and local shape features, and propose further various strategies and\nlosses to improve properties of accuracy, smoothness, rigidity, and so on.\nExtensive experiments show that our 3Deformer is able to produce impressive\nresults and reaches the state-of-the-art level.\n", "rewritten_text": "3Deformer is a novel, general-purpose framework for interactive 3D shape editing.  Unlike existing methods reliant on expensive, data-limited neural network training, 3Deformer is a training-free approach that uses readily available semantic images to guide the deformation of a source 3D mesh.  It leverages differentiable rendering to deform the mesh based on correspondences between image semantics and mesh materials, while preserving the original topology as much as possible.  To overcome the challenges of accurately and smoothly deforming complex 3D shapes based on 2D image guidance, 3Deformer employs a hierarchical optimization architecture and various strategies to ensure accuracy, smoothness, rigidity, and global consistency.  Extensive experiments demonstrate that 3Deformer achieves state-of-the-art results.\n"}, "2105.03571": {"original_text": "  Dialogue state tracking (DST) plays a key role in task-oriented dialogue\nsystems to monitor the user's goal. In general, there are two strategies to\ntrack a dialogue state: predicting it from scratch and updating it from\nprevious state. The scratch-based strategy obtains each slot value by inquiring\nall the dialogue history, and the previous-based strategy relies on the current\nturn dialogue to update the previous dialogue state. However, it is hard for\nthe scratch-based strategy to correctly track short-dependency dialogue state\nbecause of noise; meanwhile, the previous-based strategy is not very useful for\nlong-dependency dialogue state tracking. Obviously, it plays different roles\nfor the context information of different granularity to track different kinds\nof dialogue states. Thus, in this paper, we will study and discuss how the\ncontext information of different granularity affects dialogue state tracking.\nFirst, we explore how greatly different granularities affect dialogue state\ntracking. Then, we further discuss how to combine multiple granularities for\ndialogue state tracking. Finally, we apply the findings about context\ngranularity to few-shot learning scenario. Besides, we have publicly released\nall codes.\n", "rewritten_text": "Task-oriented dialogue systems rely heavily on dialogue state tracking (DST) to understand user goals.  Two main DST approaches exist: predicting the state from scratch (using the entire dialogue history) and updating the previous state based on the current turn.  Scratch-based methods struggle with short-term dependencies due to noise, while update-based methods falter with long-term dependencies.  This paper investigates how context granularity impacts DST performance. We analyze the effects of varying context granularities, explore methods for combining different granularities, and apply our findings to few-shot learning.  Our code is publicly available.\n"}, "2405.10474": {"original_text": "  Over the last decade, a wide range of training and deployment strategies for\nLarge Language Models (LLMs) have emerged. Among these, the prompting paradigms\nof Auto-regressive LLMs (AR-LLMs) have catalyzed a significant surge in\nArtificial Intelligence (AI). This paper aims to emphasize the significance of\nutilizing free-form modalities (forms of input and output) and verbal free-form\ncontexts as user-directed channels (methods for transforming modalities) for\ndownstream deployment. Specifically, we analyze the structure of modalities\nwithin both two types of LLMs and six task-specific channels during deployment.\nFrom the perspective of users, our analysis introduces and applies the\nanalytical metrics of task customizability, transparency, and complexity to\ngauge their usability, highlighting the superior nature of AR-LLMs' prompting\nparadigms. Moreover, we examine the stimulation of diverse cognitive behaviors\nin LLMs through the adoption of free-form text and verbal contexts, mirroring\nhuman linguistic expressions of such behaviors. We then detail four common\ncognitive behaviors to underscore how AR-LLMs' prompting successfully imitate\nhuman-like behaviors using this free-form modality and channel. Lastly, the\npotential for improving LLM deployment, both as autonomous agents and within\nmulti-agent systems, is identified via cognitive behavior concepts and\nprinciples.\n", "rewritten_text": "This paper examines the impact of free-form input and output modalities on the deployment of Large Language Models (LLMs), focusing on the advantages of Auto-regressive LLMs (AR-LLMs) and their prompting paradigms.  We analyze the structure of modalities in two LLM types and six task-specific deployment channels, evaluating usability through the metrics of task customizability, transparency, and complexity.  Our analysis reveals the superior usability of AR-LLMs, demonstrating their ability to stimulate diverse cognitive behaviors mirroring human language through free-form text and verbal contexts.  We illustrate this by detailing four common cognitive behaviors successfully emulated by AR-LLMs using this approach. Finally, we explore the potential for enhancing LLM deployment, both as individual agents and within multi-agent systems, leveraging insights from cognitive behavior.\n"}, "2212.04214": {"original_text": "  In a citation graph, adjacent paper nodes share related scientific terms and\ntopics. The graph thus conveys unique structure information of document-level\nrelatedness that can be utilized in the paper summarization task, for exploring\nbeyond the intra-document information. In this work, we focus on leveraging\ncitation graphs to improve scientific paper extractive summarization under\ndifferent settings. We first propose a Multi-granularity Unsupervised\nSummarization model (MUS) as a simple and low-cost solution to the task. MUS\nfinetunes a pre-trained encoder model on the citation graph by link prediction\ntasks. Then, the abstract sentences are extracted from the corresponding paper\nconsidering multi-granularity information. Preliminary results demonstrate that\ncitation graph is helpful even in a simple unsupervised framework. Motivated by\nthis, we next propose a Graph-based Supervised Summarization model (GSS) to\nachieve more accurate results on the task when large-scale labeled data are\navailable. Apart from employing the link prediction as an auxiliary task, GSS\nintroduces a gated sentence encoder and a graph information fusion module to\ntake advantage of the graph information to polish the sentence representation.\nExperiments on a public benchmark dataset show that MUS and GSS bring\nsubstantial improvements over the prior state-of-the-art model.\n", "rewritten_text": "This paper explores using citation graphs to enhance scientific paper summarization.  A citation graph's structure, reflecting shared scientific terms and topics between papers, provides valuable inter-document context for summarization.  We propose two models:  MUS (Multi-granularity Unsupervised Summarization), a simple, low-cost model that leverages link prediction on a pre-trained encoder to extract abstract sentences using multi-granularity information; and GSS (Graph-based Supervised Summarization), a more sophisticated model using a gated sentence encoder and graph information fusion module for improved accuracy when labeled data is available.  Both MUS and GSS, evaluated on a public benchmark dataset, significantly outperform existing state-of-the-art methods, demonstrating the effectiveness of incorporating citation graph information into scientific paper summarization.\n"}, "2310.14561": {"original_text": "  Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by\nwell-designed perturbations. This could lead to disastrous results on critical\napplications such as self-driving cars, surveillance security, and medical\ndiagnosis. At present, adversarial training is one of the most effective\ndefenses against adversarial examples. However, traditional adversarial\ntraining makes it difficult to achieve a good trade-off between clean accuracy\nand robustness since spurious features are still learned by DNNs. The intrinsic\nreason is that traditional adversarial training makes it difficult to fully\nlearn core features from adversarial examples when adversarial noise and clean\nexamples cannot be disentangled. In this paper, we disentangle the adversarial\nexamples into natural and perturbed patterns by bit-plane slicing. We assume\nthe higher bit-planes represent natural patterns and the lower bit-planes\nrepresent perturbed patterns, respectively. We propose a Feature-Focusing\nAdversarial Training (F$^2$AT), which differs from previous work in that it\nenforces the model to focus on the core features from natural patterns and\nreduce the impact of spurious features from perturbed patterns. The\nexperimental results demonstrated that F$^2$AT outperforms state-of-the-art\nmethods in clean accuracy and adversarial robustness.\n", "rewritten_text": "Deep neural networks (DNNs) are susceptible to adversarial attacks, posing significant risks in safety-critical applications like autonomous vehicles and medical diagnosis.  While adversarial training is a leading defense, it struggles to balance accuracy on clean data with robustness against attacks, often learning spurious features. This limitation stems from the difficulty in distinguishing between genuine and adversarial noise during training.  This paper introduces Feature-Focusing Adversarial Training (F\u00b2AT), a novel approach that uses bit-plane slicing to separate natural and perturbed patterns in adversarial examples.  By focusing the model on higher-bit-plane (natural) features and suppressing lower-bit-plane (perturbed) features, F\u00b2AT achieves superior clean accuracy and adversarial robustness compared to existing methods.\n"}, "2308.04782": {"original_text": "  Point cloud registration is a task to estimate the rigid transformation\nbetween two unaligned scans, which plays an important role in many computer\nvision applications. Previous learning-based works commonly focus on supervised\nregistration, which have limitations in practice. Recently, with the advance of\ninexpensive RGB-D sensors, several learning-based works utilize RGB-D data to\nachieve unsupervised registration. However, most of existing unsupervised\nmethods follow a cascaded design or fuse RGB-D data in a unidirectional manner,\nwhich do not fully exploit the complementary information in the RGB-D data. To\nleverage the complementary information more effectively, we propose a network\nimplementing multi-scale bidirectional fusion between RGB images and point\nclouds generated from depth images. By bidirectionally fusing visual and\ngeometric features in multi-scales, more distinctive deep features for\ncorrespondence estimation can be obtained, making our registration more\naccurate. Extensive experiments on ScanNet and 3DMatch demonstrate that our\nmethod achieves new state-of-the-art performance. Code will be released at\nhttps://github.com/phdymz/PointMBF\n", "rewritten_text": "Point cloud registration, crucial for many computer vision applications, involves finding the optimal alignment between two unaligned 3D scans. While supervised learning approaches have limitations, recent work leverages inexpensive RGB-D sensors for unsupervised registration.  However, existing unsupervised methods often use cascaded architectures or unidirectional RGB-D fusion, failing to fully exploit the complementary information.  This paper introduces a novel network employing multi-scale, bidirectional fusion of RGB images and depth-derived point clouds. This approach generates more robust deep features for correspondence estimation, leading to improved registration accuracy.  Our method achieves state-of-the-art results on ScanNet and 3DMatch benchmarks. Code is available at https://github.com/phdymz/PointMBF.\n"}, "2404.02573": {"original_text": "  Knowledge distillation (KD) is a promising yet challenging model compression\ntechnique that transfers rich learning representations from a well-performing\nbut cumbersome teacher model to a compact student model. Previous methods for\nimage super-resolution (SR) mostly compare the feature maps directly or after\nstandardizing the dimensions with basic algebraic operations (e.g. average,\ndot-product). However, the intrinsic semantic differences among feature maps\nare overlooked, which are caused by the disparate expressive capacity between\nthe networks. This work presents MiPKD, a multi-granularity mixture of prior KD\nframework, to facilitate efficient SR model through the feature mixture in a\nunified latent space and stochastic network block mixture. Extensive\nexperiments demonstrate the effectiveness of the proposed MiPKD method.\n", "rewritten_text": "Knowledge distillation (KD) offers a powerful but difficult approach to compressing models by transferring knowledge from a large, high-performing teacher model to a smaller, more efficient student model.  Existing image super-resolution (SR) KD methods typically compare teacher and student feature maps directly or after simple alignment, neglecting the significant semantic differences arising from the models' differing capacities.  This paper introduces MiPKD, a multi-granularity mixture-of-priors KD framework that leverages feature mixing in a unified latent space and stochastic network block mixing for improved SR model efficiency.  Extensive experiments validate MiPKD's effectiveness.\n"}, "2307.08198": {"original_text": "  We introduce the notion of point affiliation into feature upsampling. By\nabstracting a feature map into non-overlapped semantic clusters formed by\npoints of identical semantic meaning, feature upsampling can be viewed as point\naffiliation -- designating a semantic cluster for each upsampled point. In the\nframework of kernel-based dynamic upsampling, we show that an upsampled point\ncan resort to its low-res decoder neighbors and high-res encoder point to\nreason the affiliation, conditioned on the mutual similarity between them. We\ntherefore present a generic formulation for generating similarity-aware\nupsampling kernels and prove that such kernels encourage not only semantic\nsmoothness but also boundary sharpness. This formulation constitutes a novel,\nlightweight, and universal upsampling solution, Similarity-Aware Point\nAffiliation (SAPA). We show its working mechanism via our preliminary designs\nwith window-shape kernel. After probing the limitations of the designs on\nobject detection, we reveal additional insights for upsampling, leading to SAPA\nwith the dynamic kernel shape. Extensive experiments demonstrate that SAPA\noutperforms prior upsamplers and invites consistent performance improvements on\na number of dense prediction tasks, including semantic segmentation, object\ndetection, instance segmentation, panoptic segmentation, image matting, and\ndepth estimation. Code is made available at: https://github.com/tiny-smart/sapa\n", "rewritten_text": "This paper introduces Similarity-Aware Point Affiliation (SAPA), a novel upsampling method that leverages semantic clustering.  By treating feature upsampling as assigning each upsampled point to a semantic cluster (point affiliation), SAPA uses a kernel-based approach.  This approach allows upsampled points to consider both low-resolution decoder neighbors and high-resolution encoder points to determine their cluster affiliation based on mutual similarity.  A generic formulation for generating similarity-aware upsampling kernels is presented, proven to enhance both semantic smoothness and boundary sharpness.  Initially explored with window-shaped kernels,  analysis of limitations in object detection led to the development of a dynamic kernel shape.  Extensive experiments across diverse dense prediction tasks (semantic segmentation, object detection, instance segmentation, panoptic segmentation, image matting, and depth estimation) demonstrate SAPA's superior performance over existing methods.  Code is available at: https://github.com/tiny-smart/sapa\n"}, "2410.16646": {"original_text": "  Diffusion models excel at creating visually impressive images but often\nstruggle to generate images with a specified topology. The Betti number, which\nrepresents the number of structures in an image, is a fundamental measure in\ntopology. Yet, diffusion models fail to satisfy even this basic constraint.\nThis limitation restricts their utility in applications requiring exact\ncontrol, like robotics and environmental modeling. To address this, we propose\nTopoDiffusionNet (TDN), a novel approach that enforces diffusion models to\nmaintain the desired topology. We leverage tools from topological data\nanalysis, particularly persistent homology, to extract the topological\nstructures within an image. We then design a topology-based objective function\nto guide the denoising process, preserving intended structures while\nsuppressing noisy ones. Our experiments across four datasets demonstrate\nsignificant improvements in topological accuracy. TDN is the first to integrate\ntopology with diffusion models, opening new avenues of research in this area.\n", "rewritten_text": "Diffusion models generate high-quality images but lack control over their underlying topology.  This limitation, highlighted by their inability to consistently match even basic topological measures like Betti numbers, restricts their use in applications demanding precise control, such as robotics and environmental modeling.  To overcome this, we introduce TopoDiffusionNet (TDN), a novel framework that integrates topological data analysis, specifically persistent homology, into the diffusion model's denoising process.  TDN uses a topology-based objective function to guide image generation, ensuring the preservation of desired topological structures while mitigating noise.  Our experiments across four datasets show significant improvements in topological accuracy, establishing TDN as the first successful integration of topology and diffusion models, paving the way for future research.\n"}, "1603.01684": {"original_text": "  In this paper, we propose an improved mechanism for saliency detection.\nFirstly,based on a neoteric background prior selecting four corners of an image\nas background,we use color and spatial contrast with each superpixel to obtain\na salinecy map(CBP). Inspired by reverse-measurement methods to improve the\naccuracy of measurement in Engineering,we employ the Objectness labels as\nforeground prior based on part of information of CBP to construct a\nmap(OFP).Further,an original energy function is applied to optimize both of\nthem respectively and a single-layer saliency map(SLP)is formed by merging the\nabove twos.Finally,to deal with the scale problem,we obtain our multi-layer\nmap(MLP) by presenting an integration algorithm to take advantage of multiple\nsaliency maps. Quantitative and qualitative experiments on three datasets\ndemonstrate that our method performs favorably against the state-of-the-art\nalgorithm.\n", "rewritten_text": "This paper introduces a novel saliency detection mechanism.  It leverages a background prior, selecting image corners to compute a color and spatial contrast-based saliency map (CBP).  Objectness labels, derived from the CBP, are then used as a foreground prior to create an objectness foreground prior map (OFP).  These maps are individually optimized using a new energy function and merged into a single-layer saliency map (SLP).  Finally, a multi-layer map (MLP) is generated by integrating multiple saliency maps at different scales.  Quantitative and qualitative evaluations on three datasets show that our method outperforms existing state-of-the-art techniques.\n"}, "2403.16998": {"original_text": "  Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly.\n", "rewritten_text": "Recent advancements in Large Language Models (LLMs) have enabled impressive performance on long-video understanding benchmarks.  This study investigates the contribution of LLMs' world knowledge and reasoning abilities to this success, revealing a surprising robustness:  LLM-based approaches achieve high accuracy even with limited or no video-specific input.  Building on this, we propose a Multimodal Video Understanding (MVU) framework that integrates object-centric visual information (extracted using off-the-shelf tools) with an LLM via natural language fusion.  Our MVU framework achieves state-of-the-art results across multiple video understanding benchmarks, demonstrating strong generalization capabilities, including in robotics.  The code will be publicly available.\n"}, "2006.01372": {"original_text": "  In general, the labels used in sequence labeling consist of different types\nof elements. For example, IOB-format entity labels, such as B-Person and\nI-Person, can be decomposed into span (B and I) and type information (Person).\nHowever, while most sequence labeling models do not consider such label\ncomponents, the shared components across labels, such as Person, can be\nbeneficial for label prediction. In this work, we propose to integrate label\ncomponent information as embeddings into models. Through experiments on English\nand Japanese fine-grained named entity recognition, we demonstrate that the\nproposed method improves performance, especially for instances with\nlow-frequency labels.\n", "rewritten_text": "Sequence labeling often uses complex labels (e.g., B-Person, I-Person in IOB format), which can be broken down into constituent parts (e.g., span and entity type).  While most models ignore this structure, we show that leveraging shared label components (like \"Person\") as embeddings improves performance.  Experiments on English and Japanese fine-grained named entity recognition demonstrate this improvement, particularly for infrequent labels.\n"}, "2410.09140": {"original_text": "  The remarkable development of text-to-image generation models has raised\nnotable security concerns, such as the infringement of portrait rights and the\ngeneration of inappropriate content. Concept erasure has been proposed to\nremove the model's knowledge about protected and inappropriate concepts.\nAlthough many methods have tried to balance the efficacy (erasing target\nconcepts) and specificity (retaining irrelevant concepts), they can still\ngenerate abundant erasure concepts under the steering of semantically related\ninputs. In this work, we propose RealEra to address this \"concept residue\"\nissue. Specifically, we first introduce the mechanism of neighbor-concept\nmining, digging out the associated concepts by adding random perturbation into\nthe embedding of erasure concept, thus expanding the erasing range and\neliminating the generations even through associated concept inputs.\nFurthermore, to mitigate the negative impact on the generation of irrelevant\nconcepts caused by the expansion of erasure scope, RealEra preserves the\nspecificity through the beyond-concept regularization. This makes irrelevant\nconcepts maintain their corresponding spatial position, thereby preserving\ntheir normal generation performance. We also employ the closed-form solution to\noptimize weights of U-Net for the cross-attention alignment, as well as the\nprediction noise alignment with the LoRA module. Extensive experiments on\nmultiple benchmarks demonstrate that RealEra outperforms previous concept\nerasing methods in terms of superior erasing efficacy, specificity, and\ngenerality. More details are available on our project page\nhttps://realerasing.github.io/RealEra/ .\n", "rewritten_text": "Text-to-image models, while impressive, pose significant security risks like copyright infringement and the creation of harmful content.  Concept erasure aims to mitigate this by removing the model's knowledge of problematic concepts. However, existing methods struggle with \"concept residue\"\u2014generating unwanted content even when prompted with semantically related terms.  Our new method, RealEra, tackles this by employing neighbor-concept mining to broaden the erasure scope, effectively eliminating generation even from related prompts.  Simultaneously, beyond-concept regularization maintains the model's ability to generate unrelated content.  We further optimize the model's architecture using a closed-form solution for U-Net weight optimization and LoRA module alignment.  Extensive benchmarking shows RealEra surpasses existing methods in efficacy, specificity, and generalization.  For more details, visit our project page: https://realerasing.github.io/RealEra/\n"}, "2210.02843": {"original_text": "  Focusing on the issue of how to effectively capture and utilize\ncross-modality information in RGB-D salient object detection (SOD) task, we\npresent a convolutional neural network (CNN) model, named CIR-Net, based on the\nnovel cross-modality interaction and refinement. For the cross-modality\ninteraction, 1) a progressive attention guided integration unit is proposed to\nsufficiently integrate RGB-D feature representations in the encoder stage, and\n2) a convergence aggregation structure is proposed, which flows the RGB and\ndepth decoding features into the corresponding RGB-D decoding streams via an\nimportance gated fusion unit in the decoder stage. For the cross-modality\nrefinement, we insert a refinement middleware structure between the encoder and\nthe decoder, in which the RGB, depth, and RGB-D encoder features are further\nrefined by successively using a self-modality attention refinement unit and a\ncross-modality weighting refinement unit. At last, with the gradually refined\nfeatures, we predict the saliency map in the decoder stage. Extensive\nexperiments on six popular RGB-D SOD benchmarks demonstrate that our network\noutperforms the state-of-the-art saliency detectors both qualitatively and\nquantitatively.\n", "rewritten_text": "This paper introduces CIR-Net, a novel convolutional neural network for RGB-D salient object detection.  CIR-Net leverages a two-pronged approach: cross-modality interaction and refinement.  Interaction is achieved through a progressive attention-guided integration unit in the encoder and a convergence aggregation structure with importance-gated fusion in the decoder, effectively combining RGB and depth features.  Refinement is performed via a middleware structure employing self- and cross-modality attention refinement units.  Extensive experiments on six benchmark datasets demonstrate that CIR-Net surpasses existing state-of-the-art methods in both qualitative and quantitative evaluations.\n"}, "1703.01515": {"original_text": "  Temporal action localization is an important yet challenging problem. Given a\nlong, untrimmed video consisting of multiple action instances and complex\nbackground contents, we need not only to recognize their action categories, but\nalso to localize the start time and end time of each instance. Many\nstate-of-the-art systems use segment-level classifiers to select and rank\nproposal segments of pre-determined boundaries. However, a desirable model\nshould move beyond segment-level and make dense predictions at a fine\ngranularity in time to determine precise temporal boundaries. To this end, we\ndesign a novel Convolutional-De-Convolutional (CDC) network that places CDC\nfilters on top of 3D ConvNets, which have been shown to be effective for\nabstracting action semantics but reduce the temporal length of the input data.\nThe proposed CDC filter performs the required temporal upsampling and spatial\ndownsampling operations simultaneously to predict actions at the frame-level\ngranularity. It is unique in jointly modeling action semantics in space-time\nand fine-grained temporal dynamics. We train the CDC network in an end-to-end\nmanner efficiently. Our model not only achieves superior performance in\ndetecting actions in every frame, but also significantly boosts the precision\nof localizing temporal boundaries. Finally, the CDC network demonstrates a very\nhigh efficiency with the ability to process 500 frames per second on a single\nGPU server. We will update the camera-ready version and publish the source\ncodes online soon.\n", "rewritten_text": "Precisely locating actions within untrimmed videos is a difficult but crucial task.  Existing methods often rely on segment-level classification of pre-defined video segments, limiting temporal accuracy.  To address this, we introduce a novel Convolutional-De-Convolutional (CDC) network.  This network builds upon the strengths of 3D convolutional networks for action recognition, but overcomes their limitations in temporal resolution by incorporating CDC filters. These filters simultaneously upsample temporally and downsample spatially, enabling frame-level action prediction.  Our end-to-end trained model achieves superior action detection and temporal boundary localization accuracy, processing videos at 500 frames per second on a single GPU.  Source code will be released soon.\n"}, "2305.0541": {"original_text": "  The medical conversational question answering (CQA) system aims at providing\na series of professional medical services to improve the efficiency of medical\ncare. Despite the success of large language models (LLMs) in complex reasoning\ntasks in various fields, such as mathematics, logic, and commonsense QA, they\nstill need to improve with the increased complexity and specialization of the\nmedical field. This is because medical CQA tasks require not only strong\nmedical reasoning, but also the ability to think broadly and deeply. In this\npaper, to address these challenges in medical CQA tasks that need to be\nconsidered and understood in many aspects, we propose the Holistically Thought\n(HoT) method, which is designed to guide the LLMs to perform the diffused and\nfocused thinking for generating high-quality medical responses. The proposed\nHoT method has been evaluated through automated and manual assessments in three\ndifferent medical CQA datasets containing the English and Chinese languages.\nThe extensive experimental results show that our method can produce more\ncorrectness, professional, and considerate answers than several\nstate-of-the-art (SOTA) methods, manifesting its effectiveness. Our code in\nhttps://github.com/WENGSYX/HoT.\n", "rewritten_text": "This paper introduces the Holistically Thought (HoT) method, a novel approach to improve the performance of large language models (LLMs) in medical conversational question answering (CQA).  While LLMs excel in various complex reasoning tasks, their application in the highly specialized and nuanced field of medicine remains challenging.  HoT addresses this by guiding LLMs to employ both broad and focused thinking, leading to higher-quality medical responses.  Evaluated on three diverse English and Chinese medical CQA datasets using automated and manual assessments, HoT outperforms several state-of-the-art methods, demonstrating its effectiveness in generating more accurate, professional, and empathetic answers.  The code is available at https://github.com/WENGSYX/HoT.\n"}, "2110.02204": {"original_text": "  Contextualised word embeddings generated from Neural Language Models (NLMs),\nsuch as BERT, represent a word with a vector that considers the semantics of\nthe target word as well its context. On the other hand, static word embeddings\nsuch as GloVe represent words by relatively low-dimensional, memory- and\ncompute-efficient vectors but are not sensitive to the different senses of the\nword. We propose Context Derived Embeddings of Senses (CDES), a method that\nextracts sense related information from contextualised embeddings and injects\nit into static embeddings to create sense-specific static embeddings.\nExperimental results on multiple benchmarks for word sense disambiguation and\nsense discrimination tasks show that CDES can accurately learn sense-specific\nstatic embeddings reporting comparable performance to the current\nstate-of-the-art sense embeddings.\n", "rewritten_text": "Neural language models (NLMs) like BERT produce contextualized word embeddings, capturing word meaning within its context.  In contrast, static embeddings like GloVe are computationally efficient but lack context sensitivity.  We introduce Context Derived Embeddings of Senses (CDES), a method that leverages contextualized embeddings to enhance static embeddings with sense-specific information.  Evaluated on word sense disambiguation and discrimination benchmarks, CDES achieves state-of-the-art performance, generating accurate sense-specific static embeddings.\n"}, "1804.00247": {"original_text": "  This article describes our experiments in neural machine translation using\nthe recent Tensor2Tensor framework and the Transformer sequence-to-sequence\nmodel (Vaswani et al., 2017). We examine some of the critical parameters that\naffect the final translation quality, memory usage, training stability and\ntraining time, concluding each experiment with a set of recommendations for\nfellow researchers. In addition to confirming the general mantra \"more data and\nlarger models\", we address scaling to multiple GPUs and provide practical tips\nfor improved training regarding batch size, learning rate, warmup steps,\nmaximum sentence length and checkpoint averaging. We hope that our observations\nwill allow others to get better results given their particular hardware and\ndata constraints.\n", "rewritten_text": "This paper details our experiments with neural machine translation using Tensor2Tensor and the Transformer model.  We investigated key parameters impacting translation quality, memory, training stability, and speed, offering practical recommendations for researchers.  Beyond confirming the benefits of larger models and datasets, we explore multi-GPU scaling and provide practical advice on optimizing batch size, learning rate, warmup steps, maximum sentence length, and checkpoint averaging. Our findings aim to help researchers achieve better results, regardless of their hardware and data limitations.\n"}, "2402.08874": {"original_text": "  While large language models (LLMs) excel at understanding and generating\nplain text, they are not tailored to handle hierarchical text structures or\ndirectly predict task-specific properties such as text rating. In fact,\nselectively and repeatedly grasping the hierarchical structure of large-scale\ntext is pivotal for deciphering its essence. To this end, we propose a novel\nframework for hierarchical text rating utilizing LLMs, which incorporates\nRecurrent Alignment with Hard Attention (RAHA). Particularly, hard attention\nmechanism prompts a frozen LLM to selectively focus on pertinent leaf texts\nassociated with the root text and generate symbolic representations of their\nrelationships. Inspired by the gradual stabilization of the Markov Chain,\nrecurrent alignment strategy involves feeding predicted ratings iteratively\nback into the prompts of another trainable LLM, aligning it to progressively\napproximate the desired target. Experimental results demonstrate that RAHA\noutperforms existing state-of-the-art methods on three hierarchical text rating\ndatasets. Theoretical and empirical analysis confirms RAHA's ability to\ngradually converge towards the underlying target through multiple inferences.\nAdditional experiments on plain text rating datasets verify the effectiveness\nof this Markov-like alignment. Our data and code can be available in\nhttps://github.com/ECNU-Text-Computing/Markov-LLM.\n", "rewritten_text": "Large language models (LLMs) struggle with hierarchical text structures and directly predicting properties like text ratings.  Effectively understanding hierarchical text is crucial.  We introduce RAHA, a novel framework using LLMs for hierarchical text rating.  RAHA employs hard attention to focus the LLM on relevant text segments, generating symbolic relationship representations.  A recurrent alignment strategy, inspired by Markov chains, iteratively refines ratings by feeding predictions back into a trainable LLM.  RAHA surpasses existing methods on three hierarchical text rating datasets, demonstrating convergence towards target ratings through iterative inference.  Its effectiveness extends to plain text rating, as confirmed by additional experiments.  Code and data are available at https://github.com/ECNU-Text-Computing/Markov-LLM.\n"}, "2410.10227": {"original_text": "  Few-Shot Learning (FSL) aims to recognize new classes with limited labeled\ndata. Recent studies have attempted to address the challenge of rare samples\nwith textual prompts to modulate visual features. However, they usually\nstruggle to capture complex semantic relationships between textual and visual\nfeatures. Moreover, vanilla self-attention is heavily affected by useless\ninformation in images, severely constraining the potential of semantic priors\nin FSL due to the confusion of numerous irrelevant tokens during interaction.\nTo address these aforementioned issues, a K-NN Transformer with Pyramid Prompts\n(KTPP) is proposed to select discriminative information with K-NN Context\nAttention (KCA) and adaptively modulate visual features with Pyramid\nCross-modal Prompts (PCP). First, for each token, the KCA only selects the K\nmost relevant tokens to compute the self-attention matrix and incorporates the\nmean of all tokens as the context prompt to provide the global context in three\ncascaded stages. As a result, irrelevant tokens can be progressively\nsuppressed. Secondly, pyramid prompts are introduced in the PCP to emphasize\nvisual features via interactions between text-based class-aware prompts and\nmulti-scale visual features. This allows the ViT to dynamically adjust the\nimportance weights of visual features based on rich semantic information at\ndifferent scales, making models robust to spatial variations. Finally,\naugmented visual features and class-aware prompts are interacted via the KCA to\nextract class-specific features. Consequently, our model further enhances\nnoise-free visual representations via deep cross-modal interactions, extracting\ngeneralized visual representation in scenarios with few labeled samples.\nExtensive experiments on four benchmark datasets demonstrate the effectiveness\nof our method.\n", "rewritten_text": "Few-shot learning (FSL) struggles with limited labeled data.  While recent methods use textual prompts to improve visual feature learning, they often fail to capture complex text-image relationships and are hampered by irrelevant image information overwhelming self-attention mechanisms.  To overcome these limitations, we propose K-NN Transformer with Pyramid Prompts (KTPP).  KTPP employs K-NN Context Attention (KCA) to selectively focus on the most relevant image tokens, progressively suppressing irrelevant ones using a three-stage cascaded approach with a global context prompt.  Furthermore, Pyramid Cross-modal Prompts (PCP) leverage multi-scale visual features and class-aware text prompts to dynamically adjust feature importance, improving robustness to spatial variations.  This refined interaction between augmented visual features and class-aware prompts via KCA yields enhanced, noise-reduced visual representations.  Extensive experiments on four benchmark datasets validate KTPP's effectiveness in FSL.\n"}, "2005.00692": {"original_text": "  Cross-lingual Entity Linking (XEL), the problem of grounding mentions of\nentities in a foreign language text into an English knowledge base such as\nWikipedia, has seen a lot of research in recent years, with a range of\npromising techniques. However, current techniques do not rise to the challenges\nintroduced by text in low-resource languages (LRL) and, surprisingly, fail to\ngeneralize to text not taken from Wikipedia, on which they are usually trained.\n  This paper provides a thorough analysis of low-resource XEL techniques,\nfocusing on the key step of identifying candidate English Wikipedia titles that\ncorrespond to a given foreign language mention. Our analysis indicates that\ncurrent methods are limited by their reliance on Wikipedia's interlanguage\nlinks and thus suffer when the foreign language's Wikipedia is small. We\nconclude that the LRL setting requires the use of outside-Wikipedia\ncross-lingual resources and present a simple yet effective zero-shot XEL\nsystem, QuEL, that utilizes search engines query logs. With experiments on 25\nlanguages, QuEL~shows an average increase of 25\\% in gold candidate recall and\nof 13\\% in end-to-end linking accuracy over state-of-the-art baselines.\n", "rewritten_text": "Recent years have witnessed significant advancements in Cross-lingual Entity Linking (XEL), which aims to connect mentions of entities in foreign-language text to an English knowledge base like Wikipedia.  Despite promising techniques, current XEL methods struggle with low-resource languages (LRLs) and surprisingly, fail to generalize beyond Wikipedia, their typical training data.  This paper analyzes existing low-resource XEL techniques, focusing on candidate English Wikipedia title selection.  We find that over-reliance on Wikipedia's interlanguage links limits performance, especially for languages with small Wikipedia editions.  Consequently, we propose QuEL, a novel zero-shot XEL system leveraging search engine query logs.  Evaluated across 25 languages, QuEL achieves a 25% average improvement in gold candidate recall and a 13% improvement in overall accuracy compared to state-of-the-art methods.\n"}, "2103.12462": {"original_text": "  Person ReID methods always learn through a stationary domain that is fixed by\nthe choice of a given dataset. In many contexts (e.g., lifelong learning),\nthose methods are ineffective because the domain is continually changing in\nwhich case incremental learning over multiple domains is required potentially.\nIn this work we explore a new and challenging ReID task, namely lifelong person\nre-identification (LReID), which enables to learn continuously across multiple\ndomains and even generalise on new and unseen domains. Following the cognitive\nprocesses in the human brain, we design an Adaptive Knowledge Accumulation\n(AKA) framework that is endowed with two crucial abilities: knowledge\nrepresentation and knowledge operation. Our method alleviates catastrophic\nforgetting on seen domains and demonstrates the ability to generalize to unseen\ndomains. Correspondingly, we also provide a new and large-scale benchmark for\nLReID. Extensive experiments demonstrate our method outperforms other\ncompetitors by a margin of 5.8% mAP in generalising evaluation.\n", "rewritten_text": "Existing person re-identification (ReID) methods are limited by their reliance on fixed datasets, hindering their effectiveness in dynamic environments.  This paper introduces lifelong person re-identification (LReID), a new task requiring continuous learning across multiple, evolving domains.  We propose an Adaptive Knowledge Accumulation (AKA) framework, inspired by human cognitive processes, that addresses catastrophic forgetting and enables generalization to unseen domains.  This framework leverages novel knowledge representation and operation mechanisms.  Furthermore, we introduce a large-scale benchmark dataset for LReID.  Our extensive experiments show AKA outperforms existing methods by 5.8% mAP in generalization performance.\n"}, "1705.07426": {"original_text": "  While the research community appears to have developed a consensus on the\nmethods of acquiring annotated data, design and training of CNNs, many\nquestions still remain to be answered. In this paper, we explore the following\nquestions that are critical to face recognition research: (i) Can we train on\nstill images and expect the systems to work on videos? (ii) Are deeper datasets\nbetter than wider datasets? (iii) Does adding label noise lead to improvement\nin performance of deep networks? (iv) Is alignment needed for face recognition?\nWe address these questions by training CNNs using CASIA-WebFace, UMDFaces, and\na new video dataset and testing on YouTube- Faces, IJB-A and a disjoint portion\nof UMDFaces datasets. Our new data set, which will be made publicly available,\nhas 22,075 videos and 3,735,476 human annotated frames extracted from them.\n", "rewritten_text": "Despite a general agreement on CNN training and annotated data acquisition in face recognition research, several key questions remain.  This paper investigates the impact of: (i) still image training on video performance; (ii) dataset depth versus breadth; (iii) label noise on deep network performance; and (iv) the necessity of facial alignment.  We address these questions through CNN training on CASIA-WebFace, UMDFaces, and a novel, publicly available video dataset (22,075 videos, 3,735,476 annotated frames), with testing on YouTube-Faces, IJB-A, and a held-out portion of UMDFaces.\n"}, "2010.06363": {"original_text": "  Lip motion reflects behavior characteristics of speakers, and thus can be\nused as a new kind of biometrics in speaker recognition. In the literature,\nlots of works used two-dimensional (2D) lip images to recognize speaker in a\ntextdependent context. However, 2D lip easily suffers from various face\norientations. To this end, in this work, we present a novel end-to-end 3D lip\nmotion Network (3LMNet) by utilizing the sentence-level 3D lip motion (S3DLM)\nto recognize speakers in both the text-independent and text-dependent contexts.\nA new regional feedback module (RFM) is proposed to obtain attentions in\ndifferent lip regions. Besides, prior knowledge of lip motion is investigated\nto complement RFM, where landmark-level and frame-level features are merged to\nform a better feature representation. Moreover, we present two methods, i.e.,\ncoordinate transformation and face posture correction to pre-process the LSD-AV\ndataset, which contains 68 speakers and 146 sentences per speaker. The\nevaluation results on this dataset demonstrate that our proposed 3LMNet is\nsuperior to the baseline models, i.e., LSTM, VGG-16 and ResNet-34, and\noutperforms the state-of-the-art using 2D lip image as well as the 3D face. The\ncode of this work is released at\nhttps://github.com/wutong18/Three-Dimensional-Lip-\nMotion-Network-for-Text-Independent-Speaker-Recognition.\n", "rewritten_text": "This paper introduces 3LMNet, a novel end-to-end 3D lip motion network for speaker recognition, addressing limitations of existing 2D-based systems vulnerable to varying face orientations.  Utilizing sentence-level 3D lip motion data (S3DLM), 3LMNet achieves text-independent and text-dependent speaker recognition.  A regional feedback module (RFM) focuses on relevant lip regions, enhanced by integrating landmark and frame-level lip motion features.  The model is evaluated on the LSD-AV dataset (68 speakers, 146 sentences each), pre-processed using coordinate transformation and face posture correction.  Experimental results show 3LMNet outperforms baseline models (LSTM, VGG-16, ResNet-34) and state-of-the-art methods using 2D lip images or 3D face data.  The code is publicly available at [link].\n"}, "2210.09345": {"original_text": "  Relation Extraction (RE) has attracted increasing attention, but current RE\nevaluation is limited to in-domain evaluation setups. Little is known on how\nwell a RE system fares in challenging, but realistic out-of-distribution\nevaluation setups. To address this gap, we propose CrossRE, a new,\nfreely-available cross-domain benchmark for RE, which comprises six distinct\ntext domains and includes multi-label annotations. An additional innovation is\nthat we release meta-data collected during annotation, to include explanations\nand flags of difficult instances. We provide an empirical evaluation with a\nstate-of-the-art model for relation classification. As the meta-data enables us\nto shed new light on the state-of-the-art model, we provide a comprehensive\nanalysis on the impact of difficult cases and find correlations between model\nand human annotations. Overall, our empirical investigation highlights the\ndifficulty of cross-domain RE. We release our dataset, to spur more research in\nthis direction.\n", "rewritten_text": "While Relation Extraction (RE) is a growing field, its evaluation is currently confined to within-domain scenarios.  To address this limitation, we introduce CrossRE, a new, publicly available cross-domain benchmark for RE.  CrossRE features six diverse text domains, multi-label annotations, and accompanying metadata\u2014including annotator explanations and flags for challenging instances.  We evaluate a state-of-the-art RE model on CrossRE, analyzing its performance and revealing correlations between model and human annotations, particularly regarding difficult cases.  Our findings underscore the challenges of cross-domain RE, and we release our dataset to encourage further research in this area.\n"}, "1006.2734": {"original_text": "  A difficult problem in clustering is how to handle data with a manifold\nstructure, i.e. data that is not shaped in the form of compact clouds of\npoints, forming arbitrary shapes or paths embedded in a high-dimensional space.\nIn this work we introduce the Penalized k-Nearest-Neighbor-Graph (PKNNG) based\nmetric, a new tool for evaluating distances in such cases. The new metric can\nbe used in combination with most clustering algorithms. The PKNNG metric is\nbased on a two-step procedure: first it constructs the k-Nearest-Neighbor-Graph\nof the dataset of interest using a low k-value and then it adds edges with an\nexponentially penalized weight for connecting the sub-graphs produced by the\nfirst step. We discuss several possible schemes for connecting the different\nsub-graphs. We use three artificial datasets in four different embedding\nsituations to evaluate the behavior of the new metric, including a comparison\namong different clustering methods. We also evaluate the new metric in a real\nworld application, clustering the MNIST digits dataset. In all cases the PKNNG\nmetric shows promising clustering results.\n", "rewritten_text": "Clustering data with complex, non-spherical structures (manifolds) embedded in high-dimensional space is challenging.  This paper introduces the Penalized k-Nearest-Neighbor-Graph (PKNNG) metric, a novel distance measure designed to address this challenge.  PKNNG, compatible with most clustering algorithms, operates in two steps:  it first constructs a k-Nearest-Neighbor graph with a small k, then adds penalized edges to connect the resulting sub-graphs.  Several connection schemes are explored.  The metric's performance is evaluated using three artificial datasets in various embeddings and a real-world application (MNIST digit clustering), demonstrating promising results across different clustering methods.\n"}, "1804.01422": {"original_text": "  In this paper, we propose a simple but effective semantic-based aggregation\n(SBA) method. The proposed SBA utilizes the discriminative filters of deep\nconvolutional layers as semantic detectors. Moreover, we propose the effective\nunsupervised strategy to select some semantic detectors to generate the\n\"probabilistic proposals\", which highlight certain discriminative pattern of\nobjects and suppress the noise of background. The final global SBA\nrepresentation could then be acquired by aggregating the regional\nrepresentations weighted by the selected \"probabilistic proposals\"\ncorresponding to various semantic content. Our unsupervised SBA is easy to\ngeneralize and achieves excellent performance on various tasks. We conduct\ncomprehensive experiments and show that our unsupervised SBA outperforms the\nstate-of-the-art unsupervised and supervised aggregation methods on image\nretrieval, place recognition and cloud classification.\n", "rewritten_text": "This paper introduces a novel, easily generalizable semantic-based aggregation (SBA) method for unsupervised feature aggregation.  Leveraging deep convolutional network filters as semantic detectors, SBA selects discriminative filters to generate probabilistic proposals that highlight object patterns while suppressing background noise.  These proposals then weight regional representations to create a final global SBA representation.  Extensive experiments on image retrieval, place recognition, and cloud classification demonstrate that our unsupervised SBA surpasses both supervised and unsupervised state-of-the-art aggregation methods.\n"}, "2109.00881": {"original_text": "  There is a growing interest in product aesthetics analytics and design.\nHowever, the lack of available large-scale data that covers various variables\nand information is one of the biggest challenges faced by analysts and\nresearchers. In this paper, we present our multidisciplinary initiative of\ndeveloping a comprehensive automotive dataset from different online sources and\nformats. Specifically, the created dataset contains 1.4 million images from 899\ncar models and their corresponding model specifications and sales information\nover more than ten years in the UK market. Our work makes significant\ncontributions to: (i) research and applications in the automotive industry;\n(ii) big data creation and sharing; (iii) database design; and (iv) data\nfusion. Apart from our motivation, technical details and data structure, we\nfurther present three simple examples to demonstrate how our data can be used\nin business research and applications.\n", "rewritten_text": "This paper introduces a large-scale automotive dataset compiled from diverse online sources, addressing the critical need for comprehensive data in product aesthetics analytics.  The dataset comprises 1.4 million images of 899 car models, coupled with their specifications and UK sales data spanning over a decade.  This multidisciplinary initiative contributes significantly to automotive research and applications, big data creation and sharing, database design, and data fusion.  We detail the project's motivation, technical aspects, and data structure, further illustrating its potential with three practical business applications.\n"}, "1611.09007": {"original_text": "  Hyperspectral imaging sensors are becoming increasingly popular in robotics\napplications such as agriculture and mining, and allow per-pixel thematic\nclassification of materials in a scene based on their unique spectral\nsignatures. Recently, convolutional neural networks have shown remarkable\nperformance for classification tasks, but require substantial amounts of\nlabelled training data. This data must sufficiently cover the variability\nexpected to be encountered in the environment. For hyperspectral data, one of\nthe main variations encountered outdoors is due to incident illumination, which\ncan change in spectral shape and intensity depending on the scene geometry. For\nexample, regions occluded from the sun have a lower intensity and their\nincident irradiance skewed towards shorter wavelengths.\n  In this work, a data augmentation strategy based on relighting is used during\ntraining of a hyperspectral convolutional neural network. It allows training to\noccur in the outdoor environment given only a small labelled region, which does\nnot need to sufficiently represent the geometric variability of the entire\nscene. This is important for applications where obtaining large amounts of\ntraining data is labourious, hazardous or difficult, such as labelling pixels\nwithin shadows. Radiometric normalisation approaches for pre-processing the\nhyperspectral data are analysed and it is shown that methods based on the raw\npixel data are sufficient to be used as input for the classifier. This removes\nthe need for external hardware such as calibration boards, which can restrict\nthe application of hyperspectral sensors in robotics applications. Experiments\nto evaluate the classification system are carried out on two datasets captured\nfrom a field-based platform.\n", "rewritten_text": "Hyperspectral imaging, increasingly used in robotics (e.g., agriculture, mining), enables per-pixel material classification via unique spectral signatures.  While convolutional neural networks (CNNs) excel at this, they demand extensive labeled training data representing environmental variability, particularly the effects of changing illumination.  Outdoor scenes exhibit significant illumination variations; for instance, shaded areas show lower intensity and a blue shift.  This work introduces a data augmentation technique using relighting to train a hyperspectral CNN. This allows effective training with limited labeled data, even in outdoor settings, overcoming the challenges of acquiring large datasets in difficult or hazardous environments (e.g., labeling shadowed pixels).  We analyze radiometric normalization methods, demonstrating that raw pixel data suffices as CNN input, eliminating the need for calibration hardware and enhancing the practicality of hyperspectral robotics.  The system's performance is evaluated on two field-acquired datasets.\n"}, "1905.00996": {"original_text": "  In this work, we propose a novel framework named Region-Aware Network\n(RANet), which learns the ability of anti-confusing in case of heavy occlusion,\nnearby person and symmetric appearance, for human pose estimation.\nSpecifically, the proposed method addresses three key aspects, i.e., data\naugmentation, feature learning and prediction fusion, respectively. First, we\npropose Parsing-based Data Augmentation (PDA) to generate abundant data that\nsynthesizes confusing textures. Second, we not only propose a Feature Pyramid\nStem (FPS) to learn stronger low-level features in lower stage; but also\nincorporate an Effective Region Extraction (ERE) module to excavate better\ntarget-specific features. Third, we introduce Cascade Voting Fusion (CVF) to\nexplicitly exclude the inferior predictions and fuse the rest effective\npredictions for the final pose estimation. Extensive experimental results on\ntwo popular benchmarks, i.e. MPII and LSP, demonstrate the effectiveness of our\nmethod against the state-of-the-art competitors. Especially on\neasily-confusable joints, our method makes significant improvement.\n", "rewritten_text": "This paper introduces RANet, a novel framework for robust human pose estimation, particularly effective in handling heavy occlusion, nearby individuals, and symmetrical appearances.  RANet tackles this challenge through three key innovations:  Parsing-based Data Augmentation (PDA) to generate synthetically challenging training data; a Feature Pyramid Stem (FPS) and Effective Region Extraction (ERE) module to learn more robust and target-specific features; and Cascade Voting Fusion (CVF) to refine predictions by discarding unreliable estimations.  Extensive experiments on the MPII and LSP datasets demonstrate state-of-the-art performance, with significant improvements observed on challenging, easily-confused joints.\n"}, "2304.00025": {"original_text": "  After the pandemic, artificial intelligence (AI) powered support for mental\nhealth care has become increasingly important. The breadth and complexity of\nsignificant challenges required to provide adequate care involve: (a)\nPersonalized patient understanding, (b) Safety-constrained and medically\nvalidated chatbot patient interactions, and (c) Support for continued\nfeedback-based refinements in design using chatbot-patient interactions. We\npropose Alleviate, a chatbot designed to assist patients suffering from mental\nhealth challenges with personalized care and assist clinicians with\nunderstanding their patients better. Alleviate draws from an array of publicly\navailable clinically valid mental-health texts and databases, allowing\nAlleviate to make medically sound and informed decisions. In addition,\nAlleviate's modular design and explainable decision-making lends itself to\nrobust and continued feedback-based refinements to its design. In this paper,\nwe explain the different modules of Alleviate and submit a short video\ndemonstrating Alleviate's capabilities to help patients and clinicians\nunderstand each other better to facilitate optimal care strategies.\n", "rewritten_text": "The post-pandemic surge in demand for mental healthcare has highlighted the critical need for AI-powered support.  Addressing this need requires overcoming significant challenges in personalized patient understanding, safe and medically validated chatbot interactions, and iterative design refinement based on user feedback.  This paper introduces Alleviate, a chatbot designed to provide personalized mental healthcare support to patients and enhance clinician understanding.  Leveraging publicly available, clinically validated resources, Alleviate makes informed, medically sound decisions. Its modular design and explainable AI facilitate continuous improvement through feedback analysis.  We detail Alleviate's modules and include a video demonstrating its capabilities in improving patient-clinician communication and care.\n"}, "1204.2294": {"original_text": "  This paper present our new intensity chromaticity space-based feature\ndetection and matching algorithm. This approach utilizes hybridization of\nwireless local area network and camera internal sensor which to receive signal\nstrength from a access point and the same time retrieve interest point\ninformation from hallways. This information is combined by model fitting\napproach in order to find the absolute of user target position. No conventional\nsearching algorithm is required, thus it is expected reducing the computational\ncomplexity. Finally we present pre-experimental results to illustrate the\nperformance of the localization system for an indoor environment set-up.\n", "rewritten_text": "This paper introduces a novel indoor localization algorithm that uses intensity chromaticity space for feature detection and matching.  The system combines signal strength data from a wireless access point with interest point information from a camera, employing model fitting to determine the user's precise location.  This approach eliminates the need for conventional search algorithms, thereby reducing computational complexity.  Preliminary experimental results in an indoor environment demonstrate the system's performance.\n"}, "2410.09595": {"original_text": "  Deformable image registration remains a fundamental task in clinical\npractice, yet solving registration problems involving complex deformations\nremains challenging. Current deep learning-based registration methods employ\ncontinuous deformation to model large deformations, which often suffer from\naccumulated registration errors and interpolation inaccuracies. Moreover,\nachieving satisfactory results with these frameworks typically requires a large\nnumber of cascade stages, demanding substantial computational resources.\nTherefore, we propose a novel approach, the field refinement framework\n(FiRework), tailored for unsupervised deformable registration, aiming to\naddress these challenges. In FiRework, we redesign the continuous deformation\nframework to mitigate the aforementioned errors. Notably, our FiRework requires\nonly one level of recursion during training and supports continuous inference,\noffering improved efficacy compared to continuous deformation frameworks. We\nconducted experiments on two brain MRI datasets, enhancing two existing\ndeformable registration networks with FiRework. The experimental results\ndemonstrate the superior performance of our proposed framework in deformable\nregistration. The code is publicly available at\nhttps://github.com/ZAX130/FiRework.\n", "rewritten_text": "Deformable image registration, crucial in clinical practice, faces challenges with complex deformations.  Existing deep learning methods using continuous deformation often accumulate errors and inaccuracies, requiring computationally expensive cascaded stages.  To address this, we introduce FiRework, a novel unsupervised deformable registration framework.  FiRework re-engineers continuous deformation to reduce errors, requiring only a single training recursion and enabling continuous inference for improved efficiency.  Experiments on two brain MRI datasets, augmenting existing networks with FiRework, demonstrate superior performance.  The code is publicly available at https://github.com/ZAX130/FiRework.\n"}, "2401.07770": {"original_text": "  Computer vision tasks typically involve describing what is present in an\nimage (e.g. classification, detection, segmentation, and captioning). We study\na visual common sense task that requires understanding what is not present.\nSpecifically, given an image (e.g. of a living room) and name of an object\n(\"cushion\"), a vision system is asked to predict semantically-meaningful\nregions (masks or bounding boxes) in the image where that object could be\nplaced or is likely be placed by humans (e.g. on the sofa). We call this task:\nSemantic Placement (SP) and believe that such common-sense visual understanding\nis critical for assitive robots (tidying a house), and AR devices\n(automatically rendering an object in the user's space). Studying the invisible\nis hard. Datasets for image description are typically constructed by curating\nrelevant images and asking humans to annotate the contents of the image;\nneither of those two steps are straightforward for objects not present in the\nimage. We overcome this challenge by operating in the opposite direction: we\nstart with an image of an object in context from web, and then remove that\nobject from the image via inpainting. This automated pipeline converts\nunstructured web data into a dataset comprising pairs of images with/without\nthe object. Using this, we collect a novel dataset, with ${\\sim}1.3$M images\nacross $9$ object categories, and train a SP prediction model called CLIP-UNet.\nCLIP-UNet outperforms existing VLMs and baselines that combine semantic priors\nwith object detectors on real-world and simulated images. In our user studies,\nwe find that the SP masks predicted by CLIP-UNet are favored $43.7\\%$ and\n$31.3\\%$ times when comparing against the $4$ SP baselines on real and\nsimulated images. In addition, we demonstrate leveraging SP mask predictions\nfrom CLIP-UNet enables downstream applications like building tidying robots in\nindoor environments.\n", "rewritten_text": "This paper introduces Semantic Placement (SP), a novel computer vision task focusing on predicting the likely locations of absent objects in an image.  Unlike typical image understanding tasks (classification, detection, etc.), SP aims to identify semantically meaningful regions where an object *could* be placed, for example, a cushion on a sofa.  This common-sense reasoning is crucial for applications like assistive robots and augmented reality.  Creating a dataset for this task presented a unique challenge.  Instead of annotating existing images, we developed an automated pipeline that uses inpainting to remove objects from web images, generating a dataset of 1.3 million images across nine object categories.  We trained a model, CLIP-UNet, which outperforms existing methods on both real and simulated images, as demonstrated by user studies showing a significant preference for CLIP-UNet's predictions.  Finally, we showcase the application of SP in building a prototype tidying robot.\n"}, "2112.11454": {"original_text": "  Generating digital humans that move realistically has many applications and\nis widely studied, but existing methods focus on the major limbs of the body,\nignoring the hands and head. Hands have been separately studied, but the focus\nhas been on generating realistic static grasps of objects. To synthesize\nvirtual characters that interact with the world, we need to generate full-body\nmotions and realistic hand grasps simultaneously. Both sub-problems are\nchallenging on their own and, together, the state-space of poses is\nsignificantly larger, the scales of hand and body motions differ, and the\nwhole-body posture and the hand grasp must agree, satisfy physical constraints,\nand be plausible. Additionally, the head is involved because the avatar must\nlook at the object to interact with it. For the first time, we address the\nproblem of generating full-body, hand and head motions of an avatar grasping an\nunknown object. As input, our method, called GOAL, takes a 3D object, its\nposition, and a starting 3D body pose and shape. GOAL outputs a sequence of\nwhole-body poses using two novel networks. First, GNet generates a goal\nwhole-body grasp with a realistic body, head, arm, and hand pose, as well as\nhand-object contact. Second, MNet generates the motion between the starting and\ngoal pose. This is challenging, as it requires the avatar to walk towards the\nobject with foot-ground contact, orient the head towards it, reach out, and\ngrasp it with a realistic hand pose and hand-object contact. To achieve this,\nthe networks exploit a representation that combines SMPL-X body parameters and\n3D vertex offsets. We train and evaluate GOAL, both qualitatively and\nquantitatively, on the GRAB dataset. Results show that GOAL generalizes well to\nunseen objects, outperforming baselines. GOAL takes a step towards synthesizing\nrealistic full-body object grasping.\n", "rewritten_text": "Realistic digital human motion generation, while extensively researched, currently neglects nuanced hand and head movements, focusing primarily on major limbs.  Existing hand modeling emphasizes static grasps.  To create believable virtual character interactions, simultaneous generation of full-body motion and dynamic hand grasps is crucial. This is exceptionally challenging due to the vastly expanded pose space, differing scales of motion, and the need for physically plausible, coordinated body posture, hand grasp, and head orientation.  This paper introduces GOAL, a novel method addressing this challenge by generating full-body, hand, and head motions for an avatar grasping an *unknown* 3D object.  Given an object's 3D model, position, and an initial body pose, GOAL uses two networks: GNet, which generates a realistic goal pose including hand-object contact; and MNet, which generates the intervening motion, ensuring realistic walking, head orientation, reaching, and grasping.  Leveraging an SMPL-X based representation, GOAL is trained and evaluated on the GRAB dataset, demonstrating superior generalization to unseen objects compared to existing methods, representing significant progress towards realistic full-body object grasping synthesis.\n"}, "2103.05861": {"original_text": "  Neural network pruning is an essential approach for reducing the\ncomputational complexity of deep models so that they can be well deployed on\nresource-limited devices. Compared with conventional methods, the recently\ndeveloped dynamic pruning methods determine redundant filters variant to each\ninput instance which achieves higher acceleration. Most of the existing methods\ndiscover effective sub-networks for each instance independently and do not\nutilize the relationship between different inputs. To maximally excavate\nredundancy in the given network architecture, this paper proposes a new\nparadigm that dynamically removes redundant filters by embedding the manifold\ninformation of all instances into the space of pruned networks (dubbed as\nManiDP). We first investigate the recognition complexity and feature similarity\nbetween images in the training set. Then, the manifold relationship between\ninstances and the pruned sub-networks will be aligned in the training\nprocedure. The effectiveness of the proposed method is verified on several\nbenchmarks, which shows better performance in terms of both accuracy and\ncomputational cost compared to the state-of-the-art methods. For example, our\nmethod can reduce 55.3% FLOPs of ResNet-34 with only 0.57% top-1 accuracy\ndegradation on ImageNet.\n", "rewritten_text": "Deep neural networks are computationally expensive, hindering their deployment on resource-constrained devices.  While dynamic pruning techniques offer significant speed improvements over static methods by adapting to each input, they typically operate independently on each instance, ignoring inter-instance relationships.  This paper introduces ManiDP, a novel dynamic pruning paradigm that leverages the manifold structure of the entire input dataset to identify and remove redundant filters more effectively.  By aligning the manifold relationships between inputs and pruned sub-networks during training, ManiDP achieves superior accuracy and computational efficiency.  Experiments on benchmark datasets demonstrate significant improvements; for instance, ResNet-34's FLOPs were reduced by 55.3% with only a 0.57% drop in ImageNet top-1 accuracy.\n"}, "2002.01127": {"original_text": "  How to generate descriptions from structured data organized in tables?\nExisting approaches using neural encoder-decoder models often suffer from\nlacking diversity. We claim that an open set of templates is crucial for\nenriching the phrase constructions and realizing varied generations. Learning\nsuch templates is prohibitive since it often requires a large paired <table,\ndescription> corpus, which is seldom available. This paper explores the problem\nof automatically learning reusable \"templates\" from paired and non-paired data.\nWe propose the variational template machine (VTM), a novel method to generate\ntext descriptions from data tables. Our contributions include: a) we carefully\ndevise a specific model architecture and losses to explicitly disentangle text\ntemplate and semantic content information, in the latent spaces, and b)we\nutilize both small parallel data and large raw text without aligned tables to\nenrich the template learning. Experiments on datasets from a variety of\ndifferent domains show that VTM is able to generate more diversely while\nkeeping a good fluency and quality.\n", "rewritten_text": "Generating diverse and high-quality text descriptions from tabular data is challenging.  Current neural methods often lack variety.  This paper introduces the Variational Template Machine (VTM), a novel approach that addresses this limitation by learning reusable text templates.  Unlike existing methods requiring large paired table-description datasets, VTM leverages both limited paired data and large unpaired text corpora.  VTM's architecture and loss function are designed to disentangle template and semantic information in the latent space, enabling more diverse generation while maintaining fluency and quality.  Experiments across diverse domains demonstrate VTM's superior performance.\n"}, "2010.03725": {"original_text": "  We present a new benchmark dataset called PARADE for paraphrase\nidentification that requires specialized domain knowledge. PARADE contains\nparaphrases that overlap very little at the lexical and syntactic level but are\nsemantically equivalent based on computer science domain knowledge, as well as\nnon-paraphrases that overlap greatly at the lexical and syntactic level but are\nnot semantically equivalent based on this domain knowledge. Experiments show\nthat both state-of-the-art neural models and non-expert human annotators have\npoor performance on PARADE. For example, BERT after fine-tuning achieves an F1\nscore of 0.709, which is much lower than its performance on other paraphrase\nidentification datasets. PARADE can serve as a resource for researchers\ninterested in testing models that incorporate domain knowledge. We make our\ndata and code freely available.\n", "rewritten_text": "The PARADE benchmark dataset challenges paraphrase identification by focusing on computer science examples.  Unlike other datasets, PARADE's paraphrases exhibit minimal lexical and syntactic overlap, relying instead on semantic equivalence within the domain. Conversely, its non-paraphrases show significant overlap but lack semantic equivalence.  State-of-the-art models, including fine-tuned BERT (achieving only 0.709 F1 score), and even human non-experts struggle with PARADE's nuanced distinctions.  This dataset, freely available with its code, offers a valuable resource for evaluating models that leverage domain-specific knowledge.\n"}, "2307.12067": {"original_text": "  We introduce Replay, a collection of multi-view, multi-modal videos of humans\ninteracting socially. Each scene is filmed in high production quality, from\ndifferent viewpoints with several static cameras, as well as wearable action\ncameras, and recorded with a large array of microphones at different positions\nin the room. Overall, the dataset contains over 4000 minutes of footage and\nover 7 million timestamped high-resolution frames annotated with camera poses\nand partially with foreground masks. The Replay dataset has many potential\napplications, such as novel-view synthesis, 3D reconstruction, novel-view\nacoustic synthesis, human body and face analysis, and training generative\nmodels. We provide a benchmark for training and evaluating novel-view\nsynthesis, with two scenarios of different difficulty. Finally, we evaluate\nseveral baseline state-of-the-art methods on the new benchmark.\n", "rewritten_text": "Replay is a new, high-quality multi-view, multi-modal video dataset capturing over 4000 minutes of human social interaction.  Filmed with multiple static and wearable cameras, and a distributed microphone array, it comprises over 7 million high-resolution frames annotated with camera poses and partial foreground masks.  This rich dataset enables research in novel-view synthesis, 3D reconstruction, novel-view acoustic synthesis, human body and face analysis, and generative model training.  We introduce a benchmark, featuring two scenarios of varying complexity, for evaluating novel-view synthesis methods and present baseline results using state-of-the-art techniques.\n"}, "1703.10645": {"original_text": "  We propose a novel method called the Relevance Subject Machine (RSM) to solve\nthe person re-identification (re-id) problem. RSM falls under the category of\nBayesian sparse recovery algorithms and uses the sparse representation of the\ninput video under a pre-defined dictionary to identify the subject in the\nvideo. Our approach focuses on the multi-shot re-id problem, which is the\nprevalent problem in many video analytics applications. RSM captures the\nessence of the multi-shot re-id problem by constraining the support of the\nsparse codes for each input video frame to be the same. Our proposed approach\nis also robust enough to deal with time varying outliers and occlusions by\nintroducing a sparse, non-stationary noise term in the model error. We provide\na novel Variational Bayesian based inference procedure along with an intuitive\ninterpretation of the proposed update rules. We evaluate our approach over\nseveral commonly used re-id datasets and show superior performance over current\nstate-of-the-art algorithms. Specifically, for ILIDS-VID, a recent large scale\nre-id dataset, RSM shows significant improvement over all published approaches,\nachieving an 11.5% (absolute) improvement in rank 1 accuracy over the closest\ncompeting algorithm considered.\n", "rewritten_text": "This paper introduces the Relevance Subject Machine (RSM), a novel Bayesian sparse recovery algorithm for multi-shot person re-identification (re-ID).  RSM leverages sparse video representations to identify individuals, addressing the prevalent challenge of multi-shot re-ID in video analytics.  By enforcing consistent sparse code support across video frames, RSM effectively handles the inherent complexities of multi-shot re-ID.  Furthermore, a sparse, non-stationary noise term enhances robustness against temporal outliers and occlusions.  We present a Variational Bayesian inference procedure with clear update rule interpretations.  Evaluations on standard re-ID datasets demonstrate superior performance compared to existing state-of-the-art methods, achieving a significant 11.5% absolute improvement in rank-1 accuracy on the large-scale ILIDS-VID dataset.\n"}, "1908.08498": {"original_text": "  We focus on multi-modal fusion for egocentric action recognition, and propose\na novel architecture for multi-modal temporal-binding, i.e. the combination of\nmodalities within a range of temporal offsets. We train the architecture with\nthree modalities -- RGB, Flow and Audio -- and combine them with mid-level\nfusion alongside sparse temporal sampling of fused representations. In contrast\nwith previous works, modalities are fused before temporal aggregation, with\nshared modality and fusion weights over time. Our proposed architecture is\ntrained end-to-end, outperforming individual modalities as well as late-fusion\nof modalities.\n  We demonstrate the importance of audio in egocentric vision, on per-class\nbasis, for identifying actions as well as interacting objects. Our method\nachieves state of the art results on both the seen and unseen test sets of the\nlargest egocentric dataset: EPIC-Kitchens, on all metrics using the public\nleaderboard.\n", "rewritten_text": "This paper introduces a novel architecture for egocentric action recognition that leverages multi-modal temporal binding.  By fusing RGB, optical flow, and audio data at a mid-level before temporal aggregation, and sharing modality and fusion weights across time, our approach surpasses both individual modality performance and late-fusion methods.  This end-to-end trained architecture, employing sparse temporal sampling of fused representations, achieves state-of-the-art results on the EPIC-Kitchens dataset, demonstrating the significant contribution of audio to action and object identification, and outperforming all other methods on both seen and unseen test sets according to the public leaderboard.\n"}, "1611.09813": {"original_text": "  We propose a CNN-based approach for 3D human body pose estimation from single\nRGB images that addresses the issue of limited generalizability of models\ntrained solely on the starkly limited publicly available 3D pose data. Using\nonly the existing 3D pose data and 2D pose data, we show state-of-the-art\nperformance on established benchmarks through transfer of learned features,\nwhile also generalizing to in-the-wild scenes. We further introduce a new\ntraining set for human body pose estimation from monocular images of real\nhumans that has the ground truth captured with a multi-camera marker-less\nmotion capture system. It complements existing corpora with greater diversity\nin pose, human appearance, clothing, occlusion, and viewpoints, and enables an\nincreased scope of augmentation. We also contribute a new benchmark that covers\noutdoor and indoor scenes, and demonstrate that our 3D pose dataset shows\nbetter in-the-wild performance than existing annotated data, which is further\nimproved in conjunction with transfer learning from 2D pose data. All in all,\nwe argue that the use of transfer learning of representations in tandem with\nalgorithmic and data contributions is crucial for general 3D body pose\nestimation.\n", "rewritten_text": "This paper presents a novel CNN-based approach for 3D human pose estimation from single RGB images.  Addressing the limited generalizability of existing models due to scarce 3D pose data, we achieve state-of-the-art results on established benchmarks by leveraging transfer learning from 2D pose data.  Furthermore, we introduce a new, diverse 3D human pose dataset captured using marker-less motion capture, featuring varied poses, appearances, clothing, occlusions, and viewpoints.  This dataset, along with a new benchmark encompassing indoor and outdoor scenes, significantly improves in-the-wild performance.  Our results demonstrate the critical role of combined algorithmic and data advancements, specifically transfer learning, in achieving robust and generalizable 3D human pose estimation.\n"}, "2109.12338": {"original_text": "  Model binarization is an effective method of compressing neural networks and\naccelerating their inference process. However, a significant performance gap\nstill exists between the 1-bit model and the 32-bit one. The empirical study\nshows that binarization causes a great loss of information in the forward and\nbackward propagation. We present a novel Distribution-sensitive Information\nRetention Network (DIR-Net) that retains the information in the forward and\nbackward propagation by improving internal propagation and introducing external\nrepresentations. The DIR-Net mainly relies on three technical contributions:\n(1) Information Maximized Binarization (IMB): minimizing the information loss\nand the binarization error of weights/activations simultaneously by weight\nbalance and standardization; (2) Distribution-sensitive Two-stage Estimator\n(DTE): retaining the information of gradients by distribution-sensitive soft\napproximation by jointly considering the updating capability and accurate\ngradient; (3) Representation-align Binarization-aware Distillation (RBD):\nretaining the representation information by distilling the representations\nbetween full-precision and binarized networks. The DIR-Net investigates both\nforward and backward processes of BNNs from the unified information\nperspective, thereby providing new insight into the mechanism of network\nbinarization. The three techniques in our DIR-Net are versatile and effective\nand can be applied in various structures to improve BNNs. Comprehensive\nexperiments on the image classification and objective detection tasks show that\nour DIR-Net consistently outperforms the state-of-the-art binarization\napproaches under mainstream and compact architectures, such as ResNet, VGG,\nEfficientNet, DARTS, and MobileNet. Additionally, we conduct our DIR-Net on\nreal-world resource-limited devices which achieves 11.1x storage saving and\n5.4x speedup.\n", "rewritten_text": "Binarizing neural networks significantly reduces their size and speeds up inference, but typically sacrifices accuracy.  To address this, we introduce DIR-Net, a novel architecture that preserves information during both forward and backward propagation.  DIR-Net achieves this through three key innovations:  Information Maximized Binarization (IMB) for minimizing weight/activation binarization errors; a Distribution-sensitive Two-stage Estimator (DTE) for accurate gradient approximation; and Representation-align Binarization-aware Distillation (RBD) to maintain representational fidelity.  Our unified information-theoretic approach improves binary neural networks (BNNs) across various architectures (ResNet, VGG, EfficientNet, DARTS, MobileNet) on image classification and object detection tasks, outperforming state-of-the-art methods.  Real-world deployment demonstrates an 11.1x storage reduction and a 5.4x speed increase.\n"}, "2310.07343": {"original_text": "  Although large language models (LLMs) are impressive in solving various\ntasks, they can quickly be outdated after deployment. Maintaining their\nup-to-date status is a pressing concern in the current era. This paper provides\na comprehensive review of recent advances in aligning LLMs with the\never-changing world knowledge without re-training from scratch. We categorize\nresearch works systemically and provide in-depth comparisons and discussion. We\nalso discuss existing challenges and highlight future directions to facilitate\nresearch in this field. We release the paper list at\nhttps://github.com/hyintell/awesome-refreshing-llms\n", "rewritten_text": "Large language models (LLMs), while powerful, rapidly become outdated.  This paper surveys recent methods for keeping LLMs current without complete retraining, systematically categorizing and comparing existing research.  We identify key challenges and suggest future research directions.  A curated list of relevant papers is available at https://github.com/hyintell/awesome-refreshing-llms.\n"}, "2301.05792": {"original_text": "  Class-Incremental Learning (CIL) [40] trains classifiers under a strict\nmemory budget: in each incremental phase, learning is done for new data, most\nof which is abandoned to free space for the next phase. The preserved data are\nexemplars used for replaying. However, existing methods use a static and ad hoc\nstrategy for memory allocation, which is often sub-optimal. In this work, we\npropose a dynamic memory management strategy that is optimized for the\nincremental phases and different object classes. We call our method reinforced\nmemory management (RMM), leveraging reinforcement learning. RMM training is not\nnaturally compatible with CIL as the past, and future data are strictly\nnon-accessible during the incremental phases. We solve this by training the\npolicy function of RMM on pseudo CIL tasks, e.g., the tasks built on the data\nof the 0-th phase, and then applying it to target tasks. RMM propagates two\nlevels of actions: Level-1 determines how to split the memory between old and\nnew classes, and Level-2 allocates memory for each specific class. In essence,\nit is an optimizable and general method for memory management that can be used\nin any replaying-based CIL method. For evaluation, we plug RMM into two\ntop-performing baselines (LUCIR+AANets and POD+AANets [30]) and conduct\nexperiments on three benchmarks (CIFAR-100, ImageNet-Subset, and\nImageNet-Full). Our results show clear improvements, e.g., boosting POD+AANets\nby 3.6%, 4.4%, and 1.9% in the 25-Phase settings of the above benchmarks,\nrespectively.\n", "rewritten_text": "Class-Incremental Learning (CIL) typically uses a fixed, suboptimal memory allocation strategy.  This paper introduces Reinforced Memory Management (RMM), a dynamic memory management system for CIL that leverages reinforcement learning to optimize memory allocation across incremental phases and classes.  Because RMM's training requires access to past and future data, which is unavailable in standard CIL, it's pre-trained on simulated CIL tasks.  RMM operates on two levels:  Level-1 splits memory between old and new classes, while Level-2 allocates memory within each class.  Integrated into two state-of-the-art CIL methods (LUCIR+AANets and POD+AANets), RMM demonstrates significant performance gains across CIFAR-100, ImageNet-Subset, and ImageNet-Full benchmarks (e.g., a 3.6% improvement for POD+AANets on CIFAR-100 in a 25-phase setting).\n"}, "1711.01362": {"original_text": "  An Unreliable news is any piece of information which is false or misleading,\ndeliberately spread to promote political, ideological and financial agendas.\nRecently the problem of unreliable news has got a lot of attention as the\nnumber instances of using news and social media outlets for propaganda have\nincreased rapidly. This poses a serious threat to society, which calls for\ntechnology to automatically and reliably identify unreliable news sources. This\npaper is an effort made in this direction to build systems for detecting\nunreliable news articles. In this paper, various NLP algorithms were built and\nevaluated on Unreliable News Data 2017 dataset. Variants of hierarchical\nattention networks (HAN) are presented for encoding and classifying news\narticles which achieve the best results of 0.944 ROC-AUC. Finally, Attention\nlayer weights are visualized to understand and give insight into the decisions\nmade by HANs. The results obtained are very promising and encouraging to deploy\nand use these systems in the real world to mitigate the problem of unreliable\nnews.\n", "rewritten_text": "The proliferation of false and misleading information spread to advance political, ideological, or financial agendas has fueled a critical need for automated unreliable news detection.  This paper addresses this challenge by developing and evaluating Natural Language Processing (NLP) algorithms, specifically hierarchical attention networks (HANs), on the Unreliable News Data 2017 dataset.  Our best-performing HAN variants achieved a 0.944 ROC-AUC score.  Furthermore, visualization of attention layer weights provides insights into the models' decision-making processes.  These promising results suggest the potential for real-world deployment of these systems to combat the spread of unreliable news.\n"}, "1803.10464": {"original_text": "  The deficiency of segmentation labels is one of the main obstacles to\nsemantic segmentation in the wild. To alleviate this issue, we present a novel\nframework that generates segmentation labels of images given their image-level\nclass labels. In this weakly supervised setting, trained models have been known\nto segment local discriminative parts rather than the entire object area. Our\nsolution is to propagate such local responses to nearby areas which belong to\nthe same semantic entity. To this end, we propose a Deep Neural Network (DNN)\ncalled AffinityNet that predicts semantic affinity between a pair of adjacent\nimage coordinates. The semantic propagation is then realized by random walk\nwith the affinities predicted by AffinityNet. More importantly, the supervision\nemployed to train AffinityNet is given by the initial discriminative part\nsegmentation, which is incomplete as a segmentation annotation but sufficient\nfor learning semantic affinities within small image areas. Thus the entire\nframework relies only on image-level class labels and does not require any\nextra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with\nsegmentation labels generated by our method outperforms previous models trained\nwith the same level of supervision, and is even as competitive as those relying\non stronger supervision.\n", "rewritten_text": "Lack of segmentation labels hinders effective semantic image segmentation.  This paper introduces a novel framework generating segmentation labels from image-level class labels.  Addressing the common problem of weakly supervised models only segmenting object parts, our approach propagates these local segmentations to complete object areas.  This is achieved using AffinityNet, a deep neural network predicting semantic affinity between pixels.  A random walk, guided by AffinityNet's predictions, performs this propagation.  Crucially, AffinityNet's training leverages the incomplete initial part segmentations, requiring only image-level labels, not additional data or annotations.  Experiments on PASCAL VOC 2012 demonstrate that models trained with our generated labels outperform previous weakly supervised methods and achieve performance comparable to strongly supervised models.\n"}, "1902.07938": {"original_text": "  Named entity recognition (NER) is an important task in NLP, which is all the\nmore challenging in conversational domain with their noisy facets. Moreover,\nconversational texts are often available in limited amount, making supervised\ntasks infeasible. To learn from small data, strong inductive biases are\nrequired. Previous work relied on hand-crafted features to encode these biases\nuntil transfer learning emerges. Here, we explore a transfer learning method,\nnamely language model pretraining, on NER task in Indonesian conversational\ntexts. We utilize large unlabeled data (generic domain) to be transferred to\nconversational texts, enabling supervised training on limited in-domain data.\nWe report two transfer learning variants, namely supervised model fine-tuning\nand unsupervised pretrained LM fine-tuning. Our experiments show that both\nvariants outperform baseline neural models when trained on small data (100\nsentences), yielding an absolute improvement of 32 points of test F1 score.\nFurthermore, we find that the pretrained LM encodes part-of-speech information\nwhich is a strong predictor for NER.\n", "rewritten_text": "Named entity recognition (NER) in conversational Indonesian text presents unique challenges due to noisy data and limited availability of labeled examples.  To overcome data scarcity, we leverage transfer learning via language model pre-training.  Using a large, unlabeled Indonesian corpus, we fine-tune a pre-trained language model in two ways: supervised fine-tuning and unsupervised pre-trained language model fine-tuning.  Both approaches significantly outperform baseline neural NER models trained on a small (100-sentence) dataset, achieving a 32-point absolute improvement in F1 score.  Our findings suggest that the pre-trained language model effectively captures part-of-speech information, a crucial feature for NER.\n"}, "1504.0234": {"original_text": "  In this paper, we focus on the two key aspects of multiple target tracking\nproblem: 1) designing an accurate affinity measure to associate detections and\n2) implementing an efficient and accurate (near) online multiple target\ntracking algorithm. As the first contribution, we introduce a novel Aggregated\nLocal Flow Descriptor (ALFD) that encodes the relative motion pattern between a\npair of temporally distant detections using long term interest point\ntrajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust\naffinity measure for estimating the likelihood of matching detections\nregardless of the application scenarios. As another contribution, we present a\nNear-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is\nformulated as a data-association between targets and detections in a temporal\nwindow, that is performed repeatedly at every frame. While being efficient,\nNOMT achieves robustness via integrating multiple cues including ALFD metric,\ntarget dynamics, appearance similarity, and long term trajectory regularization\ninto the model. Our ablative analysis verifies the superiority of the ALFD\nmetric over the other conventional affinity metrics. We run a comprehensive\nexperimental evaluation on two challenging tracking datasets, KITTI and MOT\ndatasets. The NOMT method combined with ALFD metric achieves the best accuracy\nin both datasets with significant margins (about 10% higher MOTA) over the\nstate-of-the-arts.\n", "rewritten_text": "This paper addresses two core challenges in multiple target tracking: accurate data association and efficient, near real-time tracking.  We propose a novel Aggregated Local Flow Descriptor (ALFD) that uses long-term interest point trajectories to robustly measure the affinity between temporally distant detections, regardless of the application.  Furthermore, we introduce a Near-Online Multi-target Tracking (NOMT) algorithm that efficiently and robustly associates targets and detections within a temporal window by integrating ALFD, target dynamics, appearance similarity, and long-term trajectory regularization.  Our ablation study demonstrates ALFD's superiority over existing methods, and comprehensive experiments on KITTI and MOT datasets show that NOMT with ALFD achieves state-of-the-art accuracy, exceeding existing methods by approximately 10% in MOTA.\n"}, "2405.07399": {"original_text": "  Weeds present a significant challenge in agriculture, causing yield loss and\nrequiring expensive control measures. Automatic weed detection using computer\nvision and deep learning offers a promising solution. However, conventional\ndeep learning methods often require large amounts of labelled training data,\nwhich can be costly and time-consuming to acquire. This paper introduces a\nnovel method for semi-supervised weed detection, comprising two main\ncomponents. Firstly, a multi-scale feature representation technique is employed\nto capture distinctive weed features across different scales. Secondly, we\npropose an adaptive pseudo-label assignment strategy, leveraging a small set of\nlabelled images during training. This strategy dynamically assigns confidence\nscores to pseudo-labels generated from unlabeled data. Additionally, our\napproach integrates epoch-corresponding and mixed pseudo-labels to further\nenhance the learning process. Experimental results on the COCO dataset and five\nprominent weed datasets -- CottonWeedDet12, CropAndWeed, Palmer amaranth,\nRadishWheat, and RoboWeedMap -- illustrate that our method achieves\nstate-of-the-art performance in weed detection, even with significantly less\nlabelled data compared to existing techniques. This approach holds the\npotential to alleviate the labelling burden and enhance the feasibility and\ndeployment speed of deep learning for weed detection in real-world agricultural\nscenarios.\n", "rewritten_text": "Weed control is crucial in agriculture, but current methods are costly and inefficient.  This paper presents a novel semi-supervised deep learning approach for automatic weed detection that addresses the high cost of data labeling.  Our method uses a multi-scale feature representation to capture weed characteristics and an adaptive pseudo-labeling strategy that leverages a small labeled dataset and dynamically assigns confidence scores to pseudo-labels from unlabeled data.  This includes integrating epoch-corresponding and mixed pseudo-labels.  Experiments on standard and weed-specific datasets (COCO, CottonWeedDet12, CropAndWeed, Palmer amaranth, RadishWheat, and RoboWeedMap) demonstrate state-of-the-art weed detection performance with significantly reduced labeling requirements, making deep learning-based weed detection more practical for real-world agricultural applications.\n"}, "2204.12785": {"original_text": "  Language models (LMs) have shown great potential as implicit knowledge bases\n(KBs). And for their practical use, knowledge in LMs need to be updated\nperiodically. However, existing tasks to assess LMs' efficacy as KBs do not\nadequately consider multiple large-scale updates. To this end, we first propose\na novel task--Continuously-updated QA (CuQA)--in which multiple large-scale\nupdates are made to LMs, and the performance is measured with respect to the\nsuccess in adding and updating knowledge while retaining existing knowledge. We\nthen present LMs with plug-in modules that effectively handle the updates.\nExperiments conducted on zsRE QA and NQ datasets show that our method\noutperforms existing approaches. We find that our method is 4x more effective\nin terms of updates/forgets ratio, compared to a fine-tuning baseline.\n", "rewritten_text": "Large language models (LLMs) are promising implicit knowledge bases, but require regular updates.  Existing evaluation methods for LLMs as knowledge bases inadequately address the impact of multiple large-scale knowledge updates.  We introduce Continuously-updated Question Answering (CuQA), a novel task evaluating the ability of LLMs to incorporate and retain knowledge across numerous substantial updates.  We further propose LLM plug-in modules designed to efficiently manage these updates.  Experiments on zsRE QA and NQ datasets demonstrate our approach significantly outperforms existing methods, achieving a fourfold improvement in the update/forget ratio compared to fine-tuning.\n"}, "2406.08215": {"original_text": "  Extractive summarization is a task of highlighting the most important parts\nof the text. We introduce a new approach to extractive summarization task using\nhidden clustering structure of the text. Experimental results on CNN/DailyMail\ndemonstrate that our approach generates more accurate summaries than both\nextractive and abstractive methods, achieving state-of-the-art results in terms\nof ROUGE-2 metric exceeding the previous approaches by 10%. Additionally, we\nshow that hidden structure of the text could be interpreted as aspects.\n", "rewritten_text": "This paper presents a novel extractive summarization method leveraging the text's hidden clustering structure.  Experiments on the CNN/DailyMail dataset show our approach outperforms both extractive and abstractive state-of-the-art methods, achieving a 10% improvement in ROUGE-2 score.  Furthermore, we demonstrate that this hidden structure can be interpreted as representing different aspects of the text.\n"}, "1205.3183": {"original_text": "  Existing probabilistic scanners and parsers impose hard constraints on the\nway lexical and syntactic ambiguities can be resolved. Furthermore, traditional\ngrammar-based parsing tools are limited in the mechanisms they allow for taking\ncontext into account. In this paper, we propose a model-driven tool that allows\nfor statistical language models with arbitrary probability estimators. Our work\non model-driven probabilistic parsing is built on top of ModelCC, a model-based\nparser generator, and enables the probabilistic interpretation and resolution\nof anaphoric, cataphoric, and recursive references in the disambiguation of\nabstract syntax graphs. In order to prove the expression power of ModelCC, we\ndescribe the design of a general-purpose natural language parser.\n", "rewritten_text": "Current probabilistic parsing tools restrict how lexical and syntactic ambiguities are handled.  To address this, we present a model-driven tool supporting statistical language models with flexible probability estimation.  Built upon the ModelCC parser generator, our approach enables probabilistic resolution of anaphora, cataphora, and recursion within abstract syntax graphs.  We demonstrate ModelCC's capabilities by detailing a general-purpose natural language parser.\n"}, "1607.06408": {"original_text": "  Action recognition has received increasing attention from the computer vision\nand machine learning communities in the last decade. To enable the study of\nthis problem, there exist a vast number of action datasets, which are recorded\nunder controlled laboratory settings, real-world surveillance environments, or\ncrawled from the Internet. Apart from the \"in-the-wild\" datasets, the training\nand test split of conventional datasets often possess similar environments\nconditions, which leads to close to perfect performance on constrained\ndatasets. In this paper, we introduce a new dataset, namely Multi-Camera Action\nDataset (MCAD), which is designed to evaluate the open view classification\nproblem under the surveillance environment. In total, MCAD contains 14,298\naction samples from 18 action categories, which are performed by 20 subjects\nand independently recorded with 5 cameras. Inspired by the well received\nevaluation approach on the LFW dataset, we designed a standard evaluation\nprotocol and benchmarked MCAD under several scenarios. The benchmark shows that\nwhile an average of 85% accuracy is achieved under the closed-view scenario,\nthe performance suffers from a significant drop under the cross-view scenario.\nIn the worst case scenario, the performance of 10-fold cross validation drops\nfrom 87.0% to 47.4%.\n", "rewritten_text": "The past decade has seen a surge in research on action recognition within computer vision and machine learning.  Numerous datasets exist, ranging from controlled lab settings to real-world surveillance footage and internet-sourced videos. However, many datasets exhibit similar training and testing environments, leading to artificially high accuracy.  This paper introduces the Multi-Camera Action Dataset (MCAD), a new benchmark designed to evaluate open-view action classification in surveillance settings.  MCAD comprises 14,298 action samples from 18 categories, performed by 20 subjects and recorded by 5 cameras.  Following the LFW dataset's evaluation methodology, we establish a standardized protocol and benchmark several scenarios. Results reveal an average 85% accuracy under closed-view conditions, but performance significantly degrades under cross-view conditions, dropping from 87.0% to 47.4% in the worst-case 10-fold cross-validation scenario.\n"}, "2007.06233": {"original_text": "  In the majority of object detection frameworks, the confidence of instance\nclassification is used as the quality criterion of predicted bounding boxes,\nlike the confidence-based ranking in non-maximum suppression (NMS). However,\nthe quality of bounding boxes, indicating the spatial relations, is not only\ncorrelated with the classification scores. Compared with the region proposal\nnetwork (RPN) based detectors, single-shot object detectors suffer the box\nquality as there is a lack of pre-selection of box proposals. In this paper, we\naim at single-shot object detectors and propose a location-aware anchor-based\nreasoning (LAAR) for the bounding boxes. LAAR takes both the location and\nclassification confidences into consideration for the quality evaluation of\nbounding boxes. We introduce a novel network block to learn the relative\nlocation between the anchors and the ground truths, denoted as a localization\nscore, which acts as a location reference during the inference stage. The\nproposed localization score leads to an independent regression branch and\ncalibrates the bounding box quality by scoring the predicted localization score\nso that the best-qualified bounding boxes can be picked up in NMS. Experiments\non MS COCO and PASCAL VOC benchmarks demonstrate that the proposed\nlocation-aware framework enhances the performances of current anchor-based\nsingle-shot object detection frameworks and yields consistent and robust\ndetection results.\n", "rewritten_text": "Most object detectors rely on classification confidence to rank bounding boxes, for example, in non-maximum suppression (NMS).  However, bounding box quality, reflecting spatial accuracy, isn't solely determined by classification scores. Single-shot detectors, unlike RPN-based methods, often suffer from poorer box quality due to the lack of proposal pre-selection.  This paper introduces Location-Aware Anchor-based Reasoning (LAAR) for single-shot detectors. LAAR improves bounding box quality assessment by incorporating both location and classification confidence.  A novel network block learns a \"localization score\" representing the relative position between anchors and ground truth boxes. This score, generated by an independent regression branch, calibrates bounding box quality, enabling NMS to select higher-quality boxes.  Experiments on MS COCO and PASCAL VOC show that LAAR significantly improves the performance and robustness of anchor-based single-shot object detectors.\n"}, "2308.09475": {"original_text": "  Robot-assisted surgery has made significant progress, with instrument\nsegmentation being a critical factor in surgical intervention quality. It\nserves as the building block to facilitate surgical robot navigation and\nsurgical education for the next generation of operating intelligence. Although\nexisting methods have achieved accurate instrument segmentation results, they\nsimultaneously generate segmentation masks for all instruments, without the\ncapability to specify a target object and allow an interactive experience. This\nwork explores a new task of Referring Surgical Video Instrument Segmentation\n(RSVIS), which aims to automatically identify and segment the corresponding\nsurgical instruments based on the given language expression. To achieve this,\nwe devise a novel Video-Instrument Synergistic Network (VIS-Net) to learn both\nvideo-level and instrument-level knowledge to boost performance, while previous\nwork only used video-level information. Meanwhile, we design a Graph-based\nRelation-aware Module (GRM) to model the correlation between multi-modal\ninformation (i.e., textual description and video frame) to facilitate the\nextraction of instrument-level information. We are also the first to produce\ntwo RSVIS datasets to promote related research. Our method is verified on these\ndatasets, and experimental results exhibit that the VIS-Net can significantly\noutperform existing state-of-the-art referring segmentation methods. Our code\nand our datasets will be released upon the publication of this work.\n", "rewritten_text": "Robot-assisted surgery relies heavily on accurate instrument segmentation for optimal outcomes.  While existing methods achieve accurate segmentation, they lack the ability to target specific instruments interactively.  This paper introduces Referring Surgical Video Instrument Segmentation (RSVIS), a new task aiming to segment surgical instruments based on textual descriptions.  We propose a novel Video-Instrument Synergistic Network (VIS-Net), incorporating both video and instrument-level information (unlike previous methods which only used video-level data), and a Graph-based Relation-aware Module (GRM) to integrate textual and visual information.  Furthermore, we introduce two new RSVIS datasets to advance research in this area.  Our experiments demonstrate that VIS-Net significantly outperforms existing methods.  Our code and datasets will be publicly available upon publication.\n"}, "1401.119": {"original_text": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n", "rewritten_text": "Extracting and recognizing Bangla text from video frames is difficult due to factors like complex backgrounds and low resolution.  This paper presents a two-step algorithm to address this challenge.  First, text lines are segmented into words using line contour information and first-order gradient values to identify word gaps.  A local binarization technique reconstructs the text line from these words. Second, this binarized text is fed into an Optical Character Recognition (OCR) system for recognition.\n"}, "1502.06807": {"original_text": "  We introduce and evaluate several architectures for Convolutional Neural\nNetworks to predict the 3D joint locations of a hand given a depth map. We\nfirst show that a prior on the 3D pose can be easily introduced and\nsignificantly improves the accuracy and reliability of the predictions. We also\nshow how to use context efficiently to deal with ambiguities between fingers.\nThese two contributions allow us to significantly outperform the\nstate-of-the-art on several challenging benchmarks, both in terms of accuracy\nand computation times.\n", "rewritten_text": "This paper presents and assesses multiple convolutional neural network (CNN) architectures for predicting 3D hand joint locations from depth maps.  We demonstrate that incorporating a 3D pose prior substantially improves prediction accuracy and reliability, and we introduce a method for leveraging contextual information to resolve finger ambiguities.  These advancements enable us to achieve state-of-the-art performance on several challenging benchmarks, surpassing existing methods in both accuracy and speed.\n"}, "1812.01855": {"original_text": "  We aim to dismantle the prevalent black-box neural architectures used in\ncomplex visual reasoning tasks, into the proposed eXplainable and eXplicit\nNeural Modules (XNMs), which advance beyond existing neural module networks\ntowards using scene graphs --- objects as nodes and the pairwise relationships\nas edges --- for explainable and explicit reasoning with structured knowledge.\nXNMs allow us to pay more attention to teach machines how to \"think\",\nregardless of what they \"look\". As we will show in the paper, by using scene\ngraphs as an inductive bias, 1) we can design XNMs in a concise and flexible\nfashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce\nthe number of parameters by 10 to 100 times, and 2) we can explicitly trace the\nreasoning-flow in terms of graph attentions. XNMs are so generic that they\nsupport a wide range of scene graph implementations with various qualities. For\nexample, when the graphs are detected perfectly, XNMs achieve 100% accuracy on\nboth CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound\nfor visual reasoning; when the graphs are noisily detected from real-world\nimages, XNMs are still robust to achieve a competitive 67.5% accuracy on\nVQAv2.0, surpassing the popular bag-of-objects attention models without graph\nstructures.\n", "rewritten_text": "This paper introduces eXplainable and eXplicit Neural Modules (XNMs), a novel approach to visual reasoning that replaces opaque neural networks with transparent, modular architectures based on scene graphs.  XNMs represent objects as nodes and relationships as edges, enabling explicit, explainable reasoning using structured knowledge.  This approach offers two key advantages:  first, XNMs are concise and flexible, requiring only four meta-types and reducing parameters by a factor of 10 to 100; second, reasoning flow is explicitly traceable through graph attentions.  The generality of XNMs allows for diverse scene graph implementations.  Results demonstrate perfect accuracy (100%) on CLEVR and CLEVR CoGenT with perfect scene graphs, establishing a performance upper bound.  Even with noisy scene graphs from real-world images (VQAv2.0), XNMs achieve a competitive 67.5% accuracy, outperforming standard bag-of-objects models.  This work emphasizes teaching machines *how* to reason, rather than solely focusing on visual perception.\n"}, "2401.03340": {"original_text": "  This paper introduces the CowStallNumbers dataset, a collection of images\nextracted from videos focusing on cow teats, designed to advance the field of\ncow stall number detection. The dataset comprises 1042 training images and 261\ntest images, featuring stall numbers ranging from 0 to 60. To enhance the\ndataset, we performed fine-tuning on a YOLO model and applied data augmentation\ntechniques, including random crop, center crop, and random rotation. The\nexperimental outcomes demonstrate a notable 95.4\\% accuracy in recognizing\nstall numbers.\n", "rewritten_text": "We present CowStallNumbers, a new dataset of 1203 cow teat images (1042 training, 261 testing) extracted from video footage, for the task of automated cow stall number detection (numbers 0-60).  The dataset was improved using YOLO fine-tuning and data augmentation (random cropping, center cropping, and rotation).  Our experiments achieved 95.4% accuracy in stall number recognition.\n"}, "2307.05158": {"original_text": "  Predicting where a person is looking is a complex task, requiring to\nunderstand not only the person's gaze and scene content, but also the 3D scene\nstructure and the person's situation (are they manipulating? interacting or\nobserving others? attentive?) to detect obstructions in the line of sight or\napply attention priors that humans typically have when observing others. In\nthis paper, we hypothesize that identifying and leveraging such priors can be\nbetter achieved through the exploitation of explicitly derived multimodal cues\nsuch as depth and pose. We thus propose a modular multimodal architecture\nallowing to combine these cues using an attention mechanism. The architecture\ncan naturally be exploited in privacy-sensitive situations such as surveillance\nand health, where personally identifiable information cannot be released. We\nperform extensive experiments on the GazeFollow and VideoAttentionTarget public\ndatasets, obtaining state-of-the-art performance and demonstrating very\ncompetitive results in the privacy setting case.\n", "rewritten_text": "Accurately predicting gaze direction is challenging, demanding an understanding of gaze, scene content, 3D structure, and the observer's context (e.g., interaction, attention).  This paper proposes a novel modular, multimodal architecture leveraging depth and pose cues, combined via an attention mechanism, to better utilize inherent human attention priors. This approach is particularly suitable for privacy-sensitive applications like surveillance and healthcare, where personal data disclosure is restricted.  Our experiments on GazeFollow and VideoAttentionTarget datasets achieve state-of-the-art performance, demonstrating strong results even with privacy constraints.\n"}, "2302.07027": {"original_text": "  Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.\n", "rewritten_text": "Large pretrained language models (PLMs), while powerful, often require domain-specific adaptation.  While training separate adapters for each domain achieves strong in-domain performance, this is inefficient.  This paper introduces AdapterSoup, a parameter-efficient method that averages the weights of pre-trained adapters from related domains to adapt to new, unseen domains *without* further training.  This embarrassingly parallel approach first trains a set of domain-specific adapters, then, at test time, intelligently selects and averages the most relevant adapters.  Experiments demonstrate consistent performance improvements on novel domains, even when averaging adapters trained with different hyperparameters on the same domain.  We investigate several adapter selection methods, including text clustering and semantic similarity, finding that clustering yields the best results for adapting to unseen domains.\n"}, "1706.09147": {"original_text": "  We address the task of Named Entity Disambiguation (NED) for noisy text. We\npresent WikilinksNED, a large-scale NED dataset of text fragments from the web,\nwhich is significantly noisier and more challenging than existing news-based\ndatasets. To capture the limited and noisy local context surrounding each\nmention, we design a neural model and train it with a novel method for sampling\ninformative negative examples. We also describe a new way of initializing word\nand entity embeddings that significantly improves performance. Our model\nsignificantly outperforms existing state-of-the-art methods on WikilinksNED\nwhile achieving comparable performance on a smaller newswire dataset.\n", "rewritten_text": "This paper tackles Named Entity Disambiguation (NED) in noisy web text.  We introduce WikilinksNED, a new, large-scale NED dataset significantly more challenging than existing news datasets due to its inherent noise.  To address this, we propose a novel neural model trained with a new negative sampling method to effectively utilize limited, noisy contextual information.  Furthermore, we introduce a novel word and entity embedding initialization technique.  Our model surpasses state-of-the-art performance on WikilinksNED and achieves comparable results on a smaller, cleaner newswire dataset.\n"}, "2312.16274": {"original_text": "  Recent progress in multi-modal conditioned face synthesis has enabled the\ncreation of visually striking and accurately aligned facial images. Yet,\ncurrent methods still face issues with scalability, limited flexibility, and a\none-size-fits-all approach to control strength, not accounting for the\ndiffering levels of conditional entropy, a measure of unpredictability in data\ngiven some condition, across modalities. To address these challenges, we\nintroduce a novel uni-modal training approach with modal surrogates, coupled\nwith an entropy-aware modal-adaptive modulation, to support flexible, scalable,\nand scalable multi-modal conditioned face synthesis network. Our uni-modal\ntraining with modal surrogate that only leverage uni-modal data, use modal\nsurrogate to decorate condition with modal-specific characteristic and serve as\nlinker for inter-modal collaboration , fully learns each modality control in\nface synthesis process as well as inter-modal collaboration. The entropy-aware\nmodal-adaptive modulation finely adjust diffusion noise according to\nmodal-specific characteristics and given conditions, enabling well-informed\nstep along denoising trajectory and ultimately leading to synthesis results of\nhigh fidelity and quality. Our framework improves multi-modal face synthesis\nunder various conditions, surpassing current methods in image quality and\nfidelity, as demonstrated by our thorough experimental results.\n", "rewritten_text": "State-of-the-art multi-modal face synthesis generates impressive results, but suffers from scalability limitations, inflexible control, and a uniform approach to condition strength.  This paper introduces a novel uni-modal training method using modal surrogates and entropy-aware modulation to address these shortcomings.  Our approach leverages uni-modal data, employing surrogates to encode modality-specific characteristics and facilitate inter-modal interaction. This allows for independent learning of each modality's influence and their combined effect on synthesis.  Entropy-aware modulation dynamically adjusts the denoising process based on both modality and condition, resulting in higher-fidelity outputs.  Extensive experiments demonstrate superior image quality and fidelity compared to existing methods.\n"}, "2407.06938": {"original_text": "  We present RodinHD, which can generate high-fidelity 3D avatars from a\nportrait image. Existing methods fail to capture intricate details such as\nhairstyles which we tackle in this paper. We first identify an overlooked\nproblem of catastrophic forgetting that arises when fitting triplanes\nsequentially on many avatars, caused by the MLP decoder sharing scheme. To\novercome this issue, we raise a novel data scheduling strategy and a weight\nconsolidation regularization term, which improves the decoder's capability of\nrendering sharper details. Additionally, we optimize the guiding effect of the\nportrait image by computing a finer-grained hierarchical representation that\ncaptures rich 2D texture cues, and injecting them to the 3D diffusion model at\nmultiple layers via cross-attention. When trained on 46K avatars with a noise\nschedule optimized for triplanes, the resulting model can generate 3D avatars\nwith notably better details than previous methods and can generalize to\nin-the-wild portrait input.\n", "rewritten_text": "RodinHD generates high-fidelity 3D avatars from single portrait images, overcoming limitations of existing methods, particularly in capturing fine details like hairstyles.  We address the novel problem of catastrophic forgetting in sequential triplane fitting, caused by shared MLP decoder weights, through a new data scheduling strategy and weight consolidation regularization.  Furthermore, we enhance the portrait's guiding influence by incorporating a hierarchical representation of 2D texture cues into the 3D diffusion model via cross-attention.  Trained on 46,000 avatars with an optimized noise schedule, RodinHD produces significantly more detailed 3D avatars and generalizes well to diverse portrait inputs.\n"}, "2311.08107": {"original_text": "  Large Language Models (LLMs) can justify or critique their predictions\nthrough discussions with other models or humans, thereby enriching their\nintrinsic understanding of instances. While proactive discussions in the\ninference phase have been shown to boost performance, such interactions have\nnot been extensively explored during the training phase. We hypothesize that\nincorporating interactive discussions into the training process can enhance the\nmodels' understanding and improve their reasoning and verbal expression\nabilities during inference. This work introduces the SAIE framework, which\nfacilitates supportive and adversarial discussions between learner and partner\nmodels. The learner model receives responses from the partner, and its\nparameters are then updated based on this discussion. This dynamic adjustment\nprocess continues throughout the training phase, responding to the evolving\noutputs of the learner model. Our empirical evaluation across various tasks,\nincluding math problems, commonsense reasoning, and multi-domain knowledge,\ndemonstrates that models fine-tuned with the SAIE framework outperform those\ntrained with conventional fine-tuning approaches. Furthermore, our method\nenhances the models' reasoning capabilities, improving both individual and\nmulti-agent inference performance.\n", "rewritten_text": "This paper introduces SAIE, a novel framework for training Large Language Models (LLMs) that leverages interactive discussions.  Unlike existing methods focusing solely on post-training interactions, SAIE integrates supportive and adversarial dialogues between a learner LLM and a partner model during training.  The learner's parameters are dynamically updated based on these discussions, leading to improved understanding and reasoning abilities.  Empirical results across diverse tasks show that SAIE-trained models significantly outperform conventionally trained models, exhibiting enhanced reasoning and improved performance in both single and multi-agent inference scenarios.\n"}, "1704.05831": {"original_text": "  We propose a hierarchical approach for making long-term predictions of future\nframes. To avoid inherent compounding errors in recursive pixel-level\nprediction, we propose to first estimate high-level structure in the input\nframes, then predict how that structure evolves in the future, and finally by\nobserving a single frame from the past and the predicted high-level structure,\nwe construct the future frames without having to observe any of the pixel-level\npredictions. Long-term video prediction is difficult to perform by recurrently\nobserving the predicted frames because the small errors in pixel space\nexponentially amplify as predictions are made deeper into the future. Our\napproach prevents pixel-level error propagation from happening by removing the\nneed to observe the predicted frames. Our model is built with a combination of\nLSTM and analogy based encoder-decoder convolutional neural networks, which\nindependently predict the video structure and generate the future frames,\nrespectively. In experiments, our model is evaluated on the Human3.6M and Penn\nAction datasets on the task of long-term pixel-level video prediction of humans\nperforming actions and demonstrate significantly better results than the\nstate-of-the-art.\n", "rewritten_text": "This paper introduces a hierarchical method for long-term video prediction that mitigates the compounding error problem inherent in recursive pixel-level prediction.  Instead of iteratively predicting frames, our approach first predicts the high-level structural evolution of the input video.  Then, using a single past frame and this predicted structure, it generates future frames directly, bypassing the need for intermediate pixel-level predictions. This avoids the exponential error amplification common in recurrent methods.  Our model combines LSTMs for structure prediction and analogy-based convolutional encoder-decoder networks for frame generation.  Experiments on Human3.6M and Penn Action datasets demonstrate state-of-the-art performance in long-term human action video prediction.\n"}, "1606.05706": {"original_text": "  We study the problem of agreement and disagreement detection in online\ndiscussions. An isotonic Conditional Random Fields (isotonic CRF) based\nsequential model is proposed to make predictions on sentence- or segment-level.\nWe automatically construct a socially-tuned lexicon that is bootstrapped from\nexisting general-purpose sentiment lexicons to further improve the performance.\nWe evaluate our agreement and disagreement tagging model on two disparate\nonline discussion corpora -- Wikipedia Talk pages and online debates. Our model\nis shown to outperform the state-of-the-art approaches in both datasets. For\nexample, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for\nagreement and disagreement detection, when a linear chain CRF obtains 0.58 and\n0.56 for the discussions on Wikipedia Talk pages.\n", "rewritten_text": "This paper addresses online discussion agreement and disagreement detection.  We introduce a novel sequential model using isotonic Conditional Random Fields (CRFs) for sentence/segment-level prediction.  Performance is enhanced by a socially-tuned lexicon bootstrapped from existing sentiment lexicons.  Evaluated on Wikipedia Talk pages and online debate corpora, our model surpasses state-of-the-art methods.  Specifically, on Wikipedia Talk pages, our isotonic CRF achieved F1 scores of 0.74 and 0.67 for agreement and disagreement detection, respectively, compared to 0.58 and 0.56 for a linear-chain CRF.\n"}, "2401.13011": {"original_text": "  This paper presents a novel generative model, Collaborative Competitive\nAgents (CCA), which leverages the capabilities of multiple Large Language\nModels (LLMs) based agents to execute complex tasks. Drawing inspiration from\nGenerative Adversarial Networks (GANs), the CCA system employs two equal-status\ngenerator agents and a discriminator agent. The generators independently\nprocess user instructions and generate results, while the discriminator\nevaluates the outputs, and provides feedback for the generator agents to\nfurther reflect and improve the generation results. Unlike the previous\ngenerative model, our system can obtain the intermediate steps of generation.\nThis allows each generator agent to learn from other successful executions due\nto its transparency, enabling a collaborative competition that enhances the\nquality and robustness of the system's results. The primary focus of this study\nis image editing, demonstrating the CCA's ability to handle intricate\ninstructions robustly. The paper's main contributions include the introduction\nof a multi-agent-based generative model with controllable intermediate steps\nand iterative optimization, a detailed examination of agent relationships, and\ncomprehensive experiments on image editing. Code is available at\n\\href{https://github.com/TiankaiHang/CCA}{https://github.com/TiankaiHang/CCA}.\n", "rewritten_text": "This paper introduces Collaborative Competitive Agents (CCA), a novel generative model for complex task execution.  Inspired by Generative Adversarial Networks (GANs), CCA uses two equally weighted generator agents and a discriminator agent.  Generators independently process user instructions, producing results that are evaluated by the discriminator, which then provides feedback for iterative improvement.  Unlike previous models, CCA's transparent process reveals intermediate steps, enabling collaborative learning and competition between generators. This leads to higher quality and more robust results.  We demonstrate CCA's capabilities through extensive image editing experiments, focusing on handling intricate instructions.  Key contributions include a multi-agent generative model with controllable intermediate steps and iterative optimization, an analysis of agent interactions, and comprehensive experimental results.  Code is available at [https://github.com/TiankaiHang/CCA](https://github.com/TiankaiHang/CCA).\n"}, "2309.08644": {"original_text": "  3D pose estimation is an invaluable task in computer vision with various\npractical applications. Especially, 3D pose estimation for multi-person from a\nmonocular video (3DMPPE) is particularly challenging and is still largely\nuncharted, far from applying to in-the-wild scenarios yet. We pose three\nunresolved issues with the existing methods: lack of robustness on unseen views\nduring training, vulnerability to occlusion, and severe jittering in the\noutput. As a remedy, we propose POTR-3D, the first realization of a\nsequence-to-sequence 2D-to-3D lifting model for 3DMPPE, powered by a novel\ngeometry-aware data augmentation strategy, capable of generating unbounded data\nwith a variety of views while caring about the ground plane and occlusions.\nThrough extensive experiments, we verify that the proposed model and data\naugmentation robustly generalizes to diverse unseen views, robustly recovers\nthe poses against heavy occlusions, and reliably generates more natural and\nsmoother outputs. The effectiveness of our approach is verified not only by\nachieving the state-of-the-art performance on public benchmarks, but also by\nqualitative results on more challenging in-the-wild videos. Demo videos are\navailable at https://www.youtube.com/@potr3d.\n", "rewritten_text": "Accurate 3D pose estimation from monocular video, especially for multiple people (3DMPPE), remains a significant challenge in computer vision.  Current methods suffer from limitations including poor generalization to unseen viewpoints, vulnerability to occlusion, and jerky output.  To address these issues, we introduce POTR-3D, a novel sequence-to-sequence 2D-to-3D lifting model.  POTR-3D leverages a geometry-aware data augmentation technique to generate diverse, realistic training data, including variations in viewpoint, ground plane interaction, and occlusions.  Our extensive experiments demonstrate improved robustness to unseen views and occlusions, smoother pose estimations, and state-of-the-art performance on public benchmarks and challenging real-world videos.  See our demo videos at https://www.youtube.com/@potr3d.\n"}, "2008.08735": {"original_text": "  Computer vision (CV) has achieved great success in interpreting semantic\nmeanings from images, yet CV algorithms can be brittle for tasks with adverse\nvision conditions and the ones suffering from data/label pair limitation. One\nof this tasks is in-bed human pose estimation, which has significant values in\nmany healthcare applications. In-bed pose monitoring in natural settings could\ninvolve complete darkness or full occlusion. Furthermore, the lack of publicly\navailable in-bed pose datasets hinders the use of many successful pose\nestimation algorithms for this task. In this paper, we introduce our\nSimultaneously-collected multimodal Lying Pose (SLP) dataset, which includes\nin-bed pose images from 109 participants captured using multiple imaging\nmodalities including RGB, long wave infrared, depth, and pressure map. We also\npresent a physical hyper parameter tuning strategy for ground truth pose label\ngeneration under extreme conditions such as lights off and being fully covered\nby a sheet/blanket. SLP design is compatible with the mainstream human pose\ndatasets, therefore, the state-of-the-art 2D pose estimation models can be\ntrained effectively with SLP data with promising performance as high as 95% at\nPCKh@0.5 on a single modality. The pose estimation performance can be further\nimproved by including additional modalities through collaboration.\n", "rewritten_text": "While computer vision excels at image interpretation, its performance degrades significantly under challenging conditions like poor lighting or limited data, particularly in applications such as in-bed human pose estimation.  This task is crucial for healthcare, but is hampered by the lack of suitable datasets and the difficulties of capturing reliable data in real-world scenarios (e.g., complete darkness or occlusion).  To address this, we introduce the Simultaneously-collected Multimodal Lying Pose (SLP) dataset, comprising in-bed pose images from 109 participants using RGB, long-wave infrared, depth, and pressure map modalities.  We detail a novel physical hyperparameter tuning strategy for accurate ground truth labeling, even under extreme conditions.  SLP's design is compatible with existing datasets, enabling effective training of state-of-the-art 2D pose estimation models.  Results show promising performance, achieving up to 95% PCKh@0.5 accuracy using a single modality, with further improvements possible through multimodal fusion.\n"}, "2206.04846": {"original_text": "  Deep neural networks are capable of learning powerful representations to\ntackle complex vision tasks but expose undesirable properties like the\nover-fitting issue. To this end, regularization techniques like image\naugmentation are necessary for deep neural networks to generalize well.\nNevertheless, most prevalent image augmentation recipes confine themselves to\noff-the-shelf linear transformations like scale, flip, and colorjitter. Due to\ntheir hand-crafted property, these augmentations are insufficient to generate\ntruly hard augmented examples. In this paper, we propose a novel perspective of\naugmentation to regularize the training process. Inspired by the recent success\nof applying masked image modeling to self-supervised learning, we adopt the\nself-supervised masked autoencoder to generate the distorted view of the input\nimages. We show that utilizing such model-based nonlinear transformation as\ndata augmentation can improve high-level recognition tasks. We term the\nproposed method as \\textbf{M}ask-\\textbf{R}econstruct \\textbf{A}ugmentation\n(MRA). The extensive experiments on various image classification benchmarks\nverify the effectiveness of the proposed augmentation. Specifically, MRA\nconsistently enhances the performance on supervised, semi-supervised as well as\nfew-shot classification. The code will be available at\n\\url{https://github.com/haohang96/MRA}.\n", "rewritten_text": "Deep neural networks excel at complex vision tasks but are prone to overfitting.  While image augmentation helps mitigate this, standard techniques (scaling, flipping, color adjustments) are limited in their ability to generate challenging training examples.  This paper introduces Mask-Reconstruct Augmentation (MRA), a novel augmentation method inspired by masked image modeling in self-supervised learning.  MRA uses a masked autoencoder to create non-linearly distorted image views for training.  Extensive experiments on various image classification benchmarks demonstrate MRA's effectiveness in improving performance across supervised, semi-supervised, and few-shot learning scenarios.  The code is available at [https://github.com/haohang96/MRA](https://github.com/haohang96/MRA).\n"}, "1803.10547": {"original_text": "  Text articles with false claims, especially news, have recently become\naggravating for the Internet users. These articles are in wide circulation and\nreaders face difficulty discerning fact from fiction. Previous work on\ncredibility assessment has focused on factual analysis and linguistic features.\nThe task's main challenge is the distinction between the features of true and\nfalse articles. In this paper, we propose a novel approach called Credibility\nOutcome (CREDO) which aims at scoring the credibility of an article in an open\ndomain setting.\n  CREDO consists of different modules for capturing various features\nresponsible for the credibility of an article. These features includes\ncredibility of the article's source and author, semantic similarity between the\narticle and related credible articles retrieved from a knowledge base, and\nsentiments conveyed by the article. A neural network architecture learns the\ncontribution of each of these modules to the overall credibility of an article.\nExperiments on Snopes dataset reveals that CREDO outperforms the\nstate-of-the-art approaches based on linguistic features.\n", "rewritten_text": "The proliferation of false news articles online has made it difficult for internet users to distinguish truth from fiction.  Existing credibility assessment methods primarily rely on factual analysis and linguistic features.  This paper introduces CREDO, a novel approach for scoring article credibility in an open domain.  CREDO integrates multiple modules analyzing source and author credibility, semantic similarity to trusted knowledge base articles, and article sentiment.  A neural network combines these features to generate a credibility score.  Experiments on the Snopes dataset demonstrate CREDO's superior performance compared to state-of-the-art linguistic-based methods.\n"}, "1704.01926": {"original_text": "  This paper tackles the problem of semi-supervised video object segmentation,\nthat is, segmenting an object in a sequence given its mask in the first frame.\nOne of the main challenges in this scenario is the change of appearance of the\nobjects of interest. Their semantics, on the other hand, do not vary. This\npaper investigates how to take advantage of such invariance via the\nintroduction of a semantic prior that guides the appearance model.\nSpecifically, given the segmentation mask of the first frame of a sequence, we\nestimate the semantics of the object of interest, and propagate that knowledge\nthroughout the sequence to improve the results based on an appearance model. We\npresent Semantically-Guided Video Object Segmentation (SGV), which improves\nresults over previous state of the art on two different datasets using a\nvariety of evaluation metrics, while running in half a second per frame.\n", "rewritten_text": "This paper addresses semi-supervised video object segmentation, where object segmentation in a video sequence is performed using only the object's mask in the first frame.  Leveraging the inherent semantic consistency of objects despite appearance changes, we propose Semantically-Guided Video Object Segmentation (SGV).  SGV estimates the object's semantics from the initial frame and propagates this information throughout the video sequence, guiding an appearance model for improved segmentation.  Our method outperforms state-of-the-art techniques on two datasets, across multiple evaluation metrics, achieving a processing speed of 0.5 seconds per frame.\n"}, "1902.06557": {"original_text": "  We propose a novel biophysical and dichromatic reflectance model that\nefficiently characterises spectral skin reflectance. We show how to fit the\nmodel to multispectral face images enabling high quality estimation of diffuse\nand specular shading as well as biophysical parameter maps (melanin and\nhaemoglobin). Our method works from a single image without requiring complex\ncontrolled lighting setups yet provides quantitatively accurate reconstructions\nand qualitatively convincing decomposition and editing.\n", "rewritten_text": "This paper introduces a new biophysical model for accurately estimating skin reflectance from a single multispectral face image.  The model efficiently separates diffuse and specular components, and simultaneously recovers maps of key biophysical parameters like melanin and hemoglobin concentration.  This approach avoids the need for complex lighting conditions while delivering both quantitatively precise results and visually realistic decompositions, enabling applications such as image editing.\n"}, "2211.09379": {"original_text": "  In dialogue state tracking (DST), labeling the dataset involves considerable\nhuman labor. We propose a new self-training framework for few-shot generative\nDST that utilize unlabeled data. Our self-training method iteratively improves\nthe model by pseudo labeling and employs Purpose Preserving Augmentation\n(PPAug) to prevent overfitting. We increaese the few-shot 10% performance by\napproximately 4% on MultiWOZ 2.1 and enhances the slot-recall 8.34% for unseen\nvalues compared to baseline.\n", "rewritten_text": "Dialogue state tracking (DST) datasets are expensive to label.  To address this, we introduce a novel self-training framework for few-shot generative DST leveraging unlabeled data.  This iterative approach uses pseudo-labeling and Purpose Preserving Augmentation (PPAug) to mitigate overfitting.  Our method improves performance on MultiWOZ 2.1, achieving a ~4% increase in few-shot (10%) accuracy and an 8.34% improvement in slot recall for unseen values compared to a baseline model.\n"}, "1902.11268": {"original_text": "  Deep neural networks (DNNs), especially deep convolutional neural networks\n(CNNs), have emerged as the powerful technique in various machine learning\napplications. However, the large model sizes of DNNs yield high demands on\ncomputation resource and weight storage, thereby limiting the practical\ndeployment of DNNs. To overcome these limitations, this paper proposes to\nimpose the circulant structure to the construction of convolutional layers, and\nhence leads to circulant convolutional layers (CircConvs) and circulant CNNs.\nThe circulant structure and models can be either trained from scratch or\nre-trained from a pre-trained non-circulant model, thereby making it very\nflexible for different training environments. Through extensive experiments,\nsuch strong structure-imposing approach is proved to be able to substantially\nreduce the number of parameters of convolutional layers and enable significant\nsaving of computational cost by using fast multiplication of the circulant\ntensor.\n", "rewritten_text": "Deep convolutional neural networks (CNNs) are highly effective in many machine learning applications, but their large size hinders practical deployment due to high computational and storage demands.  This paper introduces circulant convolutional layers (CircConvs) and circulant CNNs, leveraging the circulant structure to significantly reduce model size.  This structure can be trained from scratch or fine-tuned from existing models, offering flexibility.  Extensive experiments demonstrate that this approach drastically reduces the number of parameters and computational cost through fast circulant tensor multiplication.\n"}, "1704.00447": {"original_text": "  Purpose: To allow fast and high-quality reconstruction of clinical\naccelerated multi-coil MR data by learning a variational network that combines\nthe mathematical structure of variational models with deep learning.\n  Theory and Methods: Generalized compressed sensing reconstruction formulated\nas a variational model is embedded in an unrolled gradient descent scheme. All\nparameters of this formulation, including the prior model defined by filter\nkernels and activation functions as well as the data term weights, are learned\nduring an offline training procedure. The learned model can then be applied\nonline to previously unseen data.\n  Results: The variational network approach is evaluated on a clinical knee\nimaging protocol. The variational network reconstructions outperform standard\nreconstruction algorithms in terms of image quality and residual artifacts for\nall tested acceleration factors and sampling patterns.\n  Conclusion: Variational network reconstructions preserve the natural\nappearance of MR images as well as pathologies that were not included in the\ntraining data set. Due to its high computational performance, i.e.,\nreconstruction time of 193 ms on a single graphics card, and the omission of\nparameter tuning once the network is trained, this new approach to image\nreconstruction can easily be integrated into clinical workflow.\n", "rewritten_text": "This study presents a novel variational network for fast, high-quality reconstruction of accelerated multi-coil MRI data.  By integrating a generalized compressed sensing variational model into an unrolled gradient descent framework, the network learns optimal parameters (including filter kernels, activation functions, and data term weights) during offline training.  Evaluated on clinical knee imaging data, this learned model significantly outperforms standard reconstruction methods across various acceleration factors and sampling patterns, producing images with superior quality and fewer artifacts.  Crucially, the network preserves natural image appearance and pathology, even for unseen data.  Its rapid reconstruction time (193 ms on a single GPU) and elimination of parameter tuning make it readily adaptable to clinical practice.\n"}, "2104.1269": {"original_text": "  Data is the engine of modern computer vision, which necessitates collecting\nlarge-scale datasets. This is expensive, and guaranteeing the quality of the\nlabels is a major challenge. In this paper, we investigate efficient annotation\nstrategies for collecting multi-class classification labels for a large\ncollection of images. While methods that exploit learnt models for labeling\nexist, a surprisingly prevalent approach is to query humans for a fixed number\nof labels per datum and aggregate them, which is expensive. Building on prior\nwork on online joint probabilistic modeling of human annotations and\nmachine-generated beliefs, we propose modifications and best practices aimed at\nminimizing human labeling effort. Specifically, we make use of advances in\nself-supervised learning, view annotation as a semi-supervised learning\nproblem, identify and mitigate pitfalls and ablate several key design choices\nto propose effective guidelines for labeling. Our analysis is done in a more\nrealistic simulation that involves querying human labelers, which uncovers\nissues with evaluation using existing worker simulation methods. Simulated\nexperiments on a 125k image subset of the ImageNet100 show that it can be\nannotated to 80% top-1 accuracy with 0.35 annotations per image on average, a\n2.7x and 6.7x improvement over prior work and manual annotation, respectively.\nProject page: https://fidler-lab.github.io/efficient-annotation-cookbook\n", "rewritten_text": "Large-scale datasets are crucial for training effective computer vision models, but creating high-quality labels is costly and challenging.  This paper presents improved strategies for efficiently annotating large image datasets for multi-class classification.  Instead of relying solely on expensive human labeling or existing model-based approaches, we leverage self-supervised learning and a semi-supervised framework to minimize human effort.  By refining existing joint probabilistic models of human and machine annotations and addressing common pitfalls, we develop practical guidelines for efficient labeling.  Our human-in-the-loop simulation reveals limitations of existing worker simulation methods and demonstrates significant improvements: achieving 80% top-1 accuracy on a 125k ImageNet100 subset with only 0.35 annotations per image on average\u2014a 2.7x and 6.7x improvement over previous methods and manual annotation, respectively.  (Project page: https://fidler-lab.github.io/efficient-annotation-cookbook)\n"}, "2206.00923": {"original_text": "  We present a method that achieves state-of-the-art results on challenging\n(few-shot) layout-to-image generation tasks by accurately modeling textures,\nstructures and relationships contained in a complex scene. After compressing\nRGB images into patch tokens, we propose the Transformer with Focal Attention\n(TwFA) for exploring dependencies of object-to-object, object-to-patch and\npatch-to-patch. Compared to existing CNN-based and Transformer-based generation\nmodels that entangled modeling on pixel-level&patch-level and\nobject-level&patch-level respectively, the proposed focal attention predicts\nthe current patch token by only focusing on its highly-related tokens that\nspecified by the spatial layout, thereby achieving disambiguation during\ntraining. Furthermore, the proposed TwFA largely increases the data efficiency\nduring training, therefore we propose the first few-shot complex scene\ngeneration strategy based on the well-trained TwFA. Comprehensive experiments\nshow the superiority of our method, which significantly increases both\nquantitative metrics and qualitative visual realism with respect to\nstate-of-the-art CNN-based and transformer-based methods. Code is available at\nhttps://github.com/JohnDreamer/TwFA.\n", "rewritten_text": "This paper introduces TwFA, a novel Transformer with Focal Attention, achieving state-of-the-art results in challenging few-shot layout-to-image generation.  TwFA efficiently models complex scene textures, structures, and relationships by compressing RGB images into patch tokens and leveraging focal attention to selectively focus on highly relevant object-to-object, object-to-patch, and patch-to-patch dependencies as specified by the spatial layout.  Unlike existing CNN and Transformer methods that struggle with entangled pixel/patch and object/patch level modeling, TwFA's focused approach improves disambiguation and data efficiency, enabling the first few-shot complex scene generation strategy.  Extensive experiments demonstrate significant improvements in both quantitative metrics and qualitative visual realism over existing methods.  Code is available at https://github.com/JohnDreamer/TwFA.\n"}, "2204.09841": {"original_text": "  Information from an image occurs over multiple and distinct spatial scales.\nImage pyramid multiresolution representations are a useful data structure for\nimage analysis and manipulation over a spectrum of spatial scales. This paper\nemploys the Gaussian-Laplacian pyramid to treat different spatial frequency\nbands of a texture separately. First, we generate three images corresponding to\nthree levels of the Gaussian-Laplacian pyramid for an input image to capture\nintrinsic details. Then we aggregate features extracted from gray and color\ntexture images using bio-inspired texture descriptors, information-theoretic\nmeasures, gray-level co-occurrence matrix features, and Haralick statistical\nfeatures into a single feature vector. Such an aggregation aims at producing\nfeatures that characterize textures to their maximum extent, unlike employing\neach descriptor separately, which may lose some relevant textural information\nand reduce the classification performance. The experimental results on texture\nand histopathologic image datasets have shown the advantages of the proposed\nmethod compared to state-of-the-art approaches. Such findings emphasize the\nimportance of multiscale image analysis and corroborate that the descriptors\nmentioned above are complementary.\n", "rewritten_text": "Multiscale image analysis is crucial for effective texture classification.  This paper leverages a Gaussian-Laplacian pyramid to decompose images into distinct spatial frequency bands.  Three pyramid levels are used to capture detailed texture information, which is then analyzed using a diverse set of bio-inspired, information-theoretic, and statistical features (including gray-level co-occurrence matrices and Haralick features).  Combining these features into a single vector improves classification performance compared to using individual descriptors, demonstrating the complementary nature of these approaches and the benefits of multiscale analysis.  Experiments on texture and histopathological images confirm the superiority of this method over existing state-of-the-art techniques.\n"}, "2207.00744": {"original_text": "  Spatial-Temporal Video Grounding (STVG) is a challenging task which aims to\nlocalize the spatio-temporal tube of the interested object semantically\naccording to a natural language query. Most previous works not only severely\nrely on the anchor boxes extracted by Faster R-CNN, but also simply regard the\nvideo as a series of individual frames, thus lacking their temporal modeling.\nInstead, in this paper, we are the first to propose an anchor-free framework\nfor STVG, called Gaussian Kernel-based Cross Modal Network (GKCMN).\nSpecifically, we utilize the learned Gaussian Kernel-based heatmaps of each\nvideo frame to locate the query-related object. A mixed serial and parallel\nconnection network is further developed to leverage both spatial and temporal\nrelations among frames for better grounding. Experimental results on VidSTG\ndataset demonstrate the effectiveness of our proposed GKCMN.\n", "rewritten_text": "This paper introduces Gaussian Kernel-based Cross Modal Network (GKCMN), the first anchor-free framework for Spatial-Temporal Video Grounding (STVG).  Unlike previous methods that heavily rely on Faster R-CNN anchor boxes and treat videos as independent frames, GKCMN leverages learned Gaussian kernel-based heatmaps to locate objects described in natural language queries.  A novel mixed serial-parallel network architecture effectively integrates spatial and temporal relationships between video frames to improve grounding accuracy.  Experiments on the VidSTG dataset validate GKCMN's superior performance.\n"}, "2209.00232": {"original_text": "  Capsule networks (CapsNets) aim to parse images into a hierarchy of objects,\nparts, and their relations using a two-step process involving part-whole\ntransformation and hierarchical component routing. However, this hierarchical\nrelationship modeling is computationally expensive, which has limited the wider\nuse of CapsNet despite its potential advantages. The current state of CapsNet\nmodels primarily focuses on comparing their performance with capsule baselines,\nfalling short of achieving the same level of proficiency as deep CNN variants\nin intricate tasks. To address this limitation, we present an efficient\napproach for learning capsules that surpasses canonical baseline models and\neven demonstrates superior performance compared to high-performing convolution\nmodels. Our contribution can be outlined in two aspects: firstly, we introduce\na group of subcapsules onto which an input vector is projected. Subsequently,\nwe present the Hybrid Gromov-Wasserstein framework, which initially quantifies\nthe dissimilarity between the input and the components modeled by the\nsubcapsules, followed by determining their alignment degree through optimal\ntransport. This innovative mechanism capitalizes on new insights into defining\nalignment between the input and subcapsules, based on the similarity of their\nrespective component distributions. This approach enhances CapsNets' capacity\nto learn from intricate, high-dimensional data while retaining their\ninterpretability and hierarchical structure. Our proposed model offers two\ndistinct advantages: (i) its lightweight nature facilitates the application of\ncapsules to more intricate vision tasks, including object detection; (ii) it\noutperforms baseline approaches in these demanding tasks.\n", "rewritten_text": "Capsule networks (CapsNets), while promising for hierarchical image understanding, suffer from high computational cost, hindering their widespread adoption.  Existing CapsNets primarily focus on benchmarking against other capsule models, lagging behind Convolutional Neural Networks (CNNs) on complex tasks.  This paper introduces an efficient CapsNet architecture that outperforms both baseline capsule and high-performing CNN models.  This is achieved through two key innovations:  first, a novel subcapsule projection mechanism; and second, a Hybrid Gromov-Wasserstein framework that efficiently measures the similarity between input data and subcapsule representations using optimal transport. This framework leverages the distribution similarity of input and subcapsule components for improved alignment.  The resulting model is lightweight, enabling its application to complex vision tasks like object detection, while simultaneously achieving superior performance compared to state-of-the-art methods.\n"}, "2311.12076": {"original_text": "  Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof open-world intelligent systems. Despite the notable advancements in existing\nOOD detection methodologies, our study identifies a significant performance\ndrop under the scarcity of training samples. In this context, we introduce a\nnovel few-shot OOD detection benchmark, carefully constructed to address this\ngap. Our empirical analysis reveals the superiority of ParameterEfficient\nFine-Tuning (PEFT) strategies, such as visual prompt tuning and visual adapter\ntuning, over conventional techniques, including fully fine-tuning and linear\nprobing tuning in the few-shot OOD detection task. Recognizing some crucial\ninformation from the pre-trained model, which is pivotal for OOD detection, may\nbe lost during the fine-tuning process, we propose a method termed\nDomainSpecific and General Knowledge Fusion (DSGF). This approach is designed\nto be compatible with diverse fine-tuning frameworks. Our experiments show that\nthe integration of DSGF significantly enhances the few-shot OOD detection\ncapabilities across various methods and fine-tuning methodologies, including\nfully fine-tuning, visual adapter tuning, and visual prompt tuning. The code\nwill be released.\n", "rewritten_text": "Reliable open-world AI systems require robust out-of-distribution (OOD) detection.  However, current methods suffer from significant performance degradation with limited training data.  To address this, we introduce a new few-shot OOD detection benchmark and demonstrate that Parameter-Efficient Fine-Tuning (PEFT) methods, such as visual prompt and adapter tuning, outperform full fine-tuning and linear probing.  We hypothesize that standard fine-tuning loses crucial pre-trained information vital for OOD detection.  Therefore, we propose Domain-Specific and General Knowledge Fusion (DSGF), a compatible framework for various fine-tuning approaches, which significantly improves few-shot OOD detection performance across the board.  Our code will be publicly available.\n"}, "2104.10117": {"original_text": "  We present a novel deep learning-based framework to generate embedding\nrepresentations of fine-grained emotions that can be used to computationally\ndescribe psychological models of emotions. Our framework integrates a\ncontextualized embedding encoder with a multi-head probing model that enables\nto interpret dynamically learned representations optimized for an emotion\nclassification task. Our model is evaluated on the Empathetic Dialogue dataset\nand shows the state-of-the-art result for classifying 32 emotions. Our layer\nanalysis can derive an emotion graph to depict hierarchical relations among the\nemotions. Our emotion representations can be used to generate an emotion wheel\ndirectly comparable to the one from Plutchik's\\LN model, and also augment the\nvalues of missing emotions in the PAD emotional state model.\n", "rewritten_text": "This paper introduces a novel deep learning framework for generating fine-grained emotion embeddings.  These embeddings, derived from a contextualized encoder and a multi-head probing model, facilitate computational modeling of psychological emotion theories.  Achieving state-of-the-art performance on the Empathetic Dialogues dataset (32 emotion classification), our model allows for layer analysis to create an emotion hierarchy graph.  Furthermore, the generated embeddings enable the creation of an emotion wheel comparable to Plutchik's model and the imputation of missing emotions within the PAD emotional state model.\n"}, "1912.06842": {"original_text": "  The main requisite for fine-grained recognition task is to focus on subtle\ndiscriminative details that make the subordinate classes different from each\nother. We note that existing methods implicitly address this requirement and\nleave it to a data-driven pipeline to figure out what makes a subordinate class\ndifferent from the others. This results in two major limitations: First, the\nnetwork focuses on the most obvious distinctions between classes and overlooks\nmore subtle inter-class variations. Second, the chance of misclassifying a\ngiven sample in any of the negative classes is considered equal, while in fact,\nconfusions generally occur among only the most similar classes. Here, we\npropose to explicitly force the network to find the subtle differences among\nclosely related classes. In this pursuit, we introduce two key novelties that\ncan be easily plugged into existing end-to-end deep learning pipelines. On one\nhand, we introduce diversification block which masks the most salient features\nfor an input to force the network to use more subtle cues for its correct\nclassification. Concurrently, we introduce a gradient-boosting loss function\nthat focuses only on the confusing classes for each sample and therefore moves\nswiftly along the direction on the loss surface that seeks to resolve these\nambiguities. The synergy between these two blocks helps the network to learn\nmore effective feature representations. Comprehensive experiments are performed\non five challenging datasets. Our approach outperforms existing methods using\nsimilar experimental setting on all five datasets.\n", "rewritten_text": "Fine-grained recognition demands identifying subtle differences between closely related subclasses.  Current methods implicitly address this, relying on data-driven learning which suffers from two key limitations:  overemphasis on obvious distinctions and equal weighting of all misclassification possibilities, ignoring the fact that confusion primarily arises between similar classes.  To overcome these limitations, we propose a novel approach incorporating two key components easily integrated into existing deep learning pipelines: a diversification block that masks salient features, forcing the network to rely on subtler cues; and a gradient-boosting loss function that prioritizes resolving ambiguities between confusing classes.  This synergistic approach leads to more effective feature representations.  Extensive experiments across five challenging datasets demonstrate superior performance compared to existing methods under identical conditions.\n"}, "1310.7447": {"original_text": "  A new method for removing impulse noise from speech in the wavelet transform\ndomain is proposed. The method utilizes the multiresolution property of the\nwavelet transform, which provides finer time resolution at the higher\nfrequencies than the short-time Fourier transform (STFT), to effectively\nidentify and remove impulse noise. It uses two features of speech to\ndiscriminate speech from impulse noise: one is the slow time-varying nature of\nspeech and the other is the Lipschitz regularity of the speech components. On\nthe basis of these features, an algorithm has been developed to identify and\nsuppress wavelet coefficients that correspond to impulse noise. Experiment\nresults show that the new method is able to significantly reduce impulse noise\nwithout degrading the quality of the speech signal or introducing any audible\nartifacts.\n", "rewritten_text": "This paper introduces a novel wavelet-based method for impulse noise reduction in speech signals.  Leveraging the wavelet transform's superior time resolution at high frequencies compared to the short-time Fourier transform, the method effectively detects and removes impulse noise.  This is achieved by exploiting two key speech characteristics: its slow time-varying nature and the Lipschitz regularity of its components.  An algorithm based on these features identifies and suppresses noisy wavelet coefficients.  Experimental results demonstrate significant impulse noise reduction without compromising speech quality or introducing artifacts.\n"}, "2108.06771": {"original_text": "  Quantification of uncertainty in deep-neural-networks (DNN) based image\nregistration algorithms plays a critical role in the deployment of image\nregistration algorithms for clinical applications such as surgical planning,\nintraoperative guidance, and longitudinal monitoring of disease progression or\ntreatment efficacy as well as in research-oriented processing pipelines.\nCurrently available approaches for uncertainty estimation in DNN-based image\nregistration algorithms may result in sub-optimal clinical decision making due\nto potentially inaccurate estimation of the uncertainty of the registration\nstems for the assumed parametric distribution of the registration latent space.\nWe introduce NPBDREG, a fully non-parametric Bayesian framework for uncertainty\nestimation in DNN-based deformable image registration by combining an Adam\noptimizer with stochastic gradient Langevin dynamics (SGLD) to characterize the\nunderlying posterior distribution through posterior sampling. Thus, it has the\npotential to provide uncertainty estimates that are highly correlated with the\npresence of out of distribution data. We demonstrated the added-value of\nNPBDREG, compared to the baseline probabilistic VoxelMorph model (PrVXM), on\nbrain MRI image registration using $390$ image pairs from four publicly\navailable databases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a\nbetter correlation of the predicted uncertainty with out-of-distribution data\n($r>0.95$ vs. $r<0.5$) as well as a 7.3%improvement in the registration\naccuracy (Dice score, $0.74$ vs. $0.69$, $p \\ll 0.01$), and 18% improvement in\nregistration smoothness (percentage of folds in the deformation field, 0.014\nvs. 0.017, $p \\ll 0.01$). Finally, NPBDREG demonstrated a better generalization\ncapability for data corrupted by a mixed structure noise (Dice score of $0.73$\nvs. $0.69$, $p \\ll 0.01$) compared to the baseline PrVXM approach.\n", "rewritten_text": "Accurate uncertainty quantification is crucial for using deep-neural-network (DNN)-based image registration in clinical applications (surgical planning, intraoperative guidance, disease monitoring) and research.  Existing methods often provide inaccurate uncertainty estimates, potentially leading to poor clinical decisions.  We present NPBDREG, a novel non-parametric Bayesian framework that uses Adam optimization and stochastic gradient Langevin dynamics (SGLD) for robust posterior sampling and uncertainty estimation in DNN-based deformable image registration.  Evaluated on 390 brain MRI image pairs from four public databases (MGH10, CMUC12, ISBR18, LPBA40), NPBDREG significantly outperforms the probabilistic VoxelMorph (PrVXM) baseline.  It exhibits a much stronger correlation between predicted uncertainty and out-of-distribution data (r > 0.95 vs. r < 0.5), improved registration accuracy (Dice score: 0.74 vs. 0.69, p << 0.01, a 7.3% improvement), increased smoothness (18% reduction in deformation field folds, p << 0.01), and better generalization to noise-corrupted data (Dice score: 0.73 vs. 0.69, p << 0.01).\n"}, "2308.13678": {"original_text": "  Reconstructing and tracking deformable surface with little or no texture has\nposed long-standing challenges. Fundamentally, the challenges stem from\ntextureless surfaces lacking features for establishing cross-image\ncorrespondences. In this work, we present a novel type of markers to\nproactively enrich the object's surface features, and thereby ease the 3D\nsurface reconstruction and correspondence tracking. Our markers are made of\nfluorescent dyes, visible only under the ultraviolet (UV) light and invisible\nunder regular lighting condition. Leveraging the markers, we design a\nmulti-camera system that captures surface deformation under the UV light and\nthe visible light in a time multiplexing fashion. Under the UV light, markers\non the object emerge to enrich its surface texture, allowing high-quality 3D\nshape reconstruction and tracking. Under the visible light, markers become\ninvisible, allowing us to capture the object's original untouched appearance.\nWe perform experiments on various challenging scenes, including hand gestures,\nfacial expressions, waving cloth, and hand-object interaction. In all these\ncases, we demonstrate that our system is able to produce robust, high-quality\n3D reconstruction and tracking.\n", "rewritten_text": "Reconstructing and tracking deformable, textureless surfaces has been a persistent problem due to the lack of features for matching across images.  This work introduces novel fluorescent markers, invisible under normal light but visible under UV light, to enhance surface features.  A multi-camera system, using time-multiplexed UV and visible light capture, leverages these markers for high-quality 3D reconstruction and tracking.  The UV illumination reveals the markers, improving surface texture for accurate 3D modeling, while visible light captures the object's natural appearance.  Experiments on diverse scenarios, including hand gestures, facial expressions, and cloth movement, demonstrate the system's robustness and high-quality results.\n"}, "2408.12100": {"original_text": "  In recent years Plug-and-Play (PnP) methods have achieved state-of-the-art\nperformance in inverse imaging problems by replacing proximal operators with\ndenoisers. Based on the proximal gradient method, some theoretical results of\nPnP have appeared, where appropriate step size is crucial for convergence\nanalysis. However, in practical applications, applying PnP methods with\ntheoretically guaranteed step sizes is difficult, and these algorithms are\nlimited to Gaussian noise. In this paper,from a perspective of split convex\nfeasibility problems (SCFP), an adaptive PnP algorithm with Projected Landweber\nOperator (PnP-PLO) is proposed to address these issues. Numerical experiments\non image deblurring, super-resolution, and compressed sensing MRI experiments\nillustrate that PnP-PLO with theoretical guarantees outperforms\nstate-of-the-art methods such as RED and RED-PRO.\n", "rewritten_text": "Plug-and-Play (PnP) methods, which substitute denoisers for proximal operators, have recently achieved leading performance in inverse imaging.  While existing PnP methods based on the proximal gradient method offer theoretical convergence guarantees dependent on a carefully chosen step size, practical application is hampered by the difficulty of determining this step size and their limitation to Gaussian noise.  This paper introduces PnP-PLO, an adaptive PnP algorithm based on the split convex feasibility problem framework and incorporating a Projected Landweber Operator.  Experiments in image deblurring, super-resolution, and compressed sensing MRI demonstrate that PnP-PLO, with its theoretical convergence guarantees, surpasses existing state-of-the-art methods like RED and RED-PRO.\n"}, "2210.13077": {"original_text": "  Novel-view synthesis (NVS) can be tackled through different approaches,\ndepending on the general setting: a single source image to a short video\nsequence, exact or noisy camera pose information, 3D-based information such as\npoint clouds etc. The most challenging scenario, the one where we stand in this\nwork, only considers a unique source image to generate a novel one from another\nviewpoint. However, in such a tricky situation, the latest learning-based\nsolutions often struggle to integrate the camera viewpoint transformation.\nIndeed, the extrinsic information is often passed as-is, through a\nlow-dimensional vector. It might even occur that such a camera pose, when\nparametrized as Euler angles, is quantized through a one-hot representation.\nThis vanilla encoding choice prevents the learnt architecture from inferring\nnovel views on a continuous basis (from a camera pose perspective). We claim it\nexists an elegant way to better encode relative camera pose, by leveraging\n3D-related concepts such as the epipolar constraint. We, therefore, introduce\nan innovative method that encodes the viewpoint transformation as a 2D feature\nimage. Such a camera encoding strategy gives meaningful insights to the network\nregarding how the camera has moved in space between the two views. By encoding\nthe camera pose information as a finite number of coloured epipolar lines, we\ndemonstrate through our experiments that our strategy outperforms vanilla\nencoding.\n", "rewritten_text": "Novel-view synthesis (NVS) faces varying challenges depending on input data (single image, short video), camera pose accuracy, and the availability of 3D information.  This work focuses on the most challenging scenario: generating a novel view from a single source image.  Existing learning-based methods often struggle with viewpoint transformation, typically encoding camera pose as a low-dimensional vector, sometimes even quantizing Euler angles using a one-hot representation. This crude encoding limits the network's ability to generate continuous novel views.  We propose a novel approach that leverages 3D geometry, specifically the epipolar constraint, to encode the relative camera pose as a 2D feature image. This representation provides the network with richer, more meaningful information about camera movement.  Our experiments demonstrate that encoding camera pose as a set of colored epipolar lines significantly outperforms traditional low-dimensional encoding methods.\n"}, "2106.00184": {"original_text": "  Encouraging progress in few-shot semantic segmentation has been made by\nleveraging features learned upon base classes with sufficient training data to\nrepresent novel classes with few-shot examples. However, this feature sharing\nmechanism inevitably causes semantic aliasing between novel classes when they\nhave similar compositions of semantic concepts. In this paper, we reformulate\nfew-shot segmentation as a semantic reconstruction problem, and convert base\nclass features into a series of basis vectors which span a class-level semantic\nspace for novel class reconstruction. By introducing contrastive loss, we\nmaximize the orthogonality of basis vectors while minimizing semantic aliasing\nbetween classes. Within the reconstructed representation space, we further\nsuppress interference from other classes by projecting query features to the\nsupport vector for precise semantic activation. Our proposed approach, referred\nto as anti-aliasing semantic reconstruction (ASR), provides a systematic yet\ninterpretable solution for few-shot learning problems. Extensive experiments on\nPASCAL VOC and MS COCO datasets show that ASR achieves strong results compared\nwith the prior works.\n", "rewritten_text": "Few-shot semantic segmentation methods often leverage features learned from base classes to represent novel classes with limited data.  However, this approach suffers from semantic aliasing when novel classes share similar semantic components.  This paper addresses this by reformulating few-shot segmentation as a semantic reconstruction problem.  We represent base class features as orthogonal basis vectors spanning a semantic space, enabling the reconstruction of novel classes.  A contrastive loss maximizes the orthogonality of these basis vectors, minimizing aliasing.  Finally, query features are projected onto support vectors to suppress interference from other classes and achieve precise semantic activation.  Our method, Anti-Aliasing Semantic Reconstruction (ASR), offers a systematic and interpretable solution, demonstrating strong performance on PASCAL VOC and MS COCO datasets compared to existing methods.\n"}, "2310.00274": {"original_text": "  Africa has a very low doctor-to-patient ratio. At very busy clinics, doctors\ncould see 30+ patients per day -- a heavy patient burden compared with\ndeveloped countries -- but productivity tools such as clinical automatic speech\nrecognition (ASR) are lacking for these overworked clinicians. However,\nclinical ASR is mature, even ubiquitous, in developed nations, and\nclinician-reported performance of commercial clinical ASR systems is generally\nsatisfactory. Furthermore, the recent performance of general domain ASR is\napproaching human accuracy. However, several gaps exist. Several publications\nhave highlighted racial bias with speech-to-text algorithms and performance on\nminority accents lags significantly. To our knowledge, there is no publicly\navailable research or benchmark on accented African clinical ASR, and speech\ndata is non-existent for the majority of African accents. We release\nAfriSpeech, 200hrs of Pan-African English speech, 67,577 clips from 2,463\nunique speakers across 120 indigenous accents from 13 countries for clinical\nand general domain ASR, a benchmark test set, with publicly available\npre-trained models with SOTA performance on the AfriSpeech benchmark.\n", "rewritten_text": "Africa suffers from a severe shortage of doctors, leading to extremely high patient loads for clinicians.  While clinical automatic speech recognition (ASR) is commonplace and effective in developed countries, it's largely absent in Africa.  Although general ASR technology is rapidly improving, existing systems exhibit racial bias and perform poorly on minority accents, a critical issue given the lack of data representing the diverse African linguistic landscape.  To address this, we introduce AfriSpeech, a publicly available dataset of 200 hours of Pan-African English speech from 2,463 speakers across 13 countries and 120 accents.  This dataset, along with a benchmark test set and pre-trained state-of-the-art models, aims to facilitate the development of more inclusive and effective clinical ASR systems for the African continent.\n"}, "2306.02351": {"original_text": "  We present the RSSOD-Bench dataset for salient object detection (SOD) in\noptical remote sensing imagery. While SOD has achieved success in natural scene\nimages with deep learning, research in SOD for remote sensing imagery (RSSOD)\nis still in its early stages. Existing RSSOD datasets have limitations in terms\nof scale, and scene categories, which make them misaligned with real-world\napplications. To address these shortcomings, we construct the RSSOD-Bench\ndataset, which contains images from four different cities in the USA. The\ndataset provides annotations for various salient object categories, such as\nbuildings, lakes, rivers, highways, bridges, aircraft, ships, athletic fields,\nand more. The salient objects in RSSOD-Bench exhibit large-scale variations,\ncluttered backgrounds, and different seasons. Unlike existing datasets,\nRSSOD-Bench offers uniform distribution across scene categories. We benchmark\n23 different state-of-the-art approaches from both the computer vision and\nremote sensing communities. Experimental results demonstrate that more research\nefforts are required for the RSSOD task.\n", "rewritten_text": "This paper introduces RSSOD-Bench, a new benchmark dataset for salient object detection in optical remote sensing imagery.  Current deep learning-based salient object detection methods, while successful in natural images, lag behind in remote sensing due to limitations in existing datasets' scale and diversity.  RSSOD-Bench addresses this by providing a large, diverse dataset of images from four US cities, annotated with various salient object categories (e.g., buildings, water bodies, infrastructure, vehicles).  It features large-scale variations, complex backgrounds, and seasonal diversity, unlike existing datasets, offering a balanced representation of scene categories.  We benchmark 23 state-of-the-art methods, revealing a significant need for further research in remote sensing salient object detection.\n"}, "2303.12946": {"original_text": "  We have made a dataset of camouflage object detection mainly for complex\nseabed scenes, and named it UnderWater RGB&Sonar,or UW-RS for short. The UW-RS\ndataset contains a total of 1972 image data. The dataset mainly consists of two\nparts, namely underwater optical data part (UW-R dataset) and underwater sonar\ndata part (UW-S dataset).\n", "rewritten_text": "We introduce UW-RS, a new dataset for complex seabed camouflage object detection.  UW-RS comprises 1972 images, divided into two parts: an underwater optical image dataset (UW-R) and an underwater sonar image dataset (UW-S).\n"}, "2308.02738": {"original_text": "  Recently, visual-language learning has shown great potential in enhancing\nvisual-based person re-identification (ReID). Existing visual-language\nlearning-based ReID methods often focus on whole-body scale image-text feature\nalignment, while neglecting supervisions on fine-grained part features. This\nchoice simplifies the learning process but cannot guarantee within-part feature\nsemantic consistency thus hindering the final performance. Therefore, we\npropose to enhance fine-grained visual features with part-informed language\nsupervision for ReID tasks. The proposed method, named Part-Informed\nVisual-language Learning ($\\pi$-VL), suggests that (i) a human parsing-guided\nprompt tuning strategy and (ii) a hierarchical fusion-based visual-language\nalignment paradigm play essential roles in ensuring within-part feature\nsemantic consistency. Specifically, we combine both identity labels and parsing\nmaps to constitute pixel-level text prompts and fuse multi-stage visual\nfeatures with a light-weight auxiliary head to perform fine-grained image-text\nalignment. As a plug-and-play and inference-free solution, our $\\pi$-VL\nachieves substantial improvements over previous state-of-the-arts on four\ncommon-used ReID benchmarks, especially reporting 90.3% Rank-1 and 76.5% mAP\nfor the most challenging MSMT17 database without bells and whistles.\n", "rewritten_text": "Visual-language learning significantly improves person re-identification (ReID), but current methods primarily align whole-body image-text features, ignoring fine-grained part-level details. This simplification hinders performance by neglecting within-part semantic consistency.  To address this, we introduce Part-Informed Visual-language Learning ($\\pi$-VL).  $\\pi$-VL leverages human parsing and a hierarchical fusion approach for improved fine-grained image-text alignment.  Specifically, it uses pixel-level text prompts generated from identity labels and parsing maps, fusing multi-stage visual features with a lightweight auxiliary head.  This plug-and-play method, requiring no inference-time overhead, achieves state-of-the-art results on four benchmark datasets, notably reaching 90.3% Rank-1 accuracy and 76.5% mAP on the challenging MSMT17 dataset.\n"}, "1812.05785": {"original_text": "  It is prohibitively expensive to annotate a large-scale video-based person\nre-identification (re-ID) dataset, which makes fully supervised methods\ninapplicable to real-world deployment. How to maximally reduce the annotation\ncost while retaining the re-ID performance becomes an interesting problem. In\nthis paper, we address this problem by integrating an active learning scheme\ninto a deep learning framework. Noticing that the truly matched tracklet-pairs,\nalso denoted as true positives (TP), are the most informative samples for our\nre-ID model, we propose a sampling criterion to choose the most TP-likely\ntracklet-pairs for annotation. A view-aware sampling strategy considering\nview-specific biases is designed to facilitate candidate selection, followed by\nan adaptive resampling step to leave out the selected candidates that are\nunnecessary to annotate. Our method learns the re-ID model and updates the\nannotation set iteratively. The re-ID model is supervised by the tracklets'\npesudo labels that are initialized by treating each tracklet as a distinct\nclass. With the gained annotations of the actively selected candidates, the\ntracklets' pesudo labels are updated by label merging and further used to\nre-train our re-ID model. While being simple, the proposed method demonstrates\nits effectiveness on three video-based person re-ID datasets. Experimental\nresults show that less than 3\\% pairwise annotations are needed for our method\nto reach comparable performance with the fully-supervised setting.\n", "rewritten_text": "Large-scale video-based person re-identification (re-ID) datasets are prohibitively expensive to fully annotate, hindering the real-world applicability of supervised methods.  This paper proposes a cost-effective solution: an active learning approach integrated into a deep learning framework.  Focusing on the most informative samples \u2013 true positive (TP) tracklet pairs \u2013 a novel sampling criterion selects TP-likely candidates for annotation.  A view-aware strategy mitigates view-specific biases, and an adaptive resampling step removes redundant candidates.  The system iteratively refines a re-ID model using initially pseudo-labeled tracklets (each treated as a distinct class), updating these labels via merging as annotations are acquired.  This simple method achieves performance comparable to fully supervised methods using less than 3% pairwise annotations, as demonstrated on three benchmark datasets.\n"}, "2202.05457": {"original_text": "  Sentiment Analysis typically refers to using natural language processing,\ntext analysis and computational linguistics to extract affect and emotion based\ninformation from text data. Our work explores how we can effectively use deep\nneural networks in transfer learning and joint dual input learning settings to\neffectively classify sentiments and detect hate speech in Hindi and Bengali\ndata. We start by training Word2Vec word embeddings for Hindi \\textbf{HASOC\ndataset} and Bengali hate speech and then train LSTM and subsequently, employ\nparameter sharing based transfer learning to Bengali sentiment classifiers by\nreusing and fine-tuning the trained weights of Hindi classifiers with both\nclassifier being used as baseline in our study. Finally, we use BiLSTM with\nself attention in joint dual input learning setting where we train a single\nneural network on Hindi and Bengali dataset simultaneously using their\nrespective embeddings.\n", "rewritten_text": "This research investigates the application of deep neural networks to sentiment analysis and hate speech detection in Hindi and Bengali.  We leverage transfer learning and joint dual input learning, employing pre-trained Word2Vec embeddings on the HASOC dataset (Hindi) and Bengali hate speech data.  Our approach involves training LSTMs and BiLSTMs with self-attention.  Transfer learning is implemented by fine-tuning Hindi LSTM weights for Bengali sentiment classification.  A single BiLSTM model is also trained jointly on both Hindi and Bengali data using their respective embeddings, providing a comparative baseline against the transfer learning approach.\n"}, "1210.0115": {"original_text": "  A framework of demosaicing and superresolution for color filter array (CFA)\nvia residual image reconstruction and sparse representation is presented.Given\nthe intermediate image produced by certain demosaicing and interpolation\ntechnique, a residual image between the final reconstruction image and the\nintermediate image is reconstructed using sparse representation.The final\nreconstruction image has richer edges and details than that of the intermediate\nimage. Specifically, a generic dictionary is learned from a large set of\ncomposite training data composed of intermediate data and residual data. The\nlearned dictionary implies a mapping between the two data. A specific\ndictionary adaptive to the input CFA is learned thereafter. Using the adaptive\ndictionary, the sparse coefficients of intermediate data are computed and\ntransformed to predict residual image. The residual image is added back into\nthe intermediate image to obtain the final reconstruction image. Experimental\nresults demonstrate the state-of-the-art performance in terms of PSNR and\nsubjective visual perception.\n", "rewritten_text": "This paper proposes a novel demosaicing and super-resolution framework for color filter array (CFA) images.  It leverages sparse representation to reconstruct a residual image from an intermediate image generated by a standard demosaicing/interpolation method. This residual image, representing finer details and sharper edges, is then added to the intermediate image to produce a high-quality final reconstruction.  A generic dictionary is learned from a large dataset of intermediate and residual image pairs, establishing a mapping between them.  This generic dictionary is then adapted to the specific input CFA.  Using this adaptive dictionary, sparse coefficients are computed for the intermediate image and used to predict the residual, ultimately enhancing the final image quality.  Experimental results show state-of-the-art performance in both PSNR and visual quality.\n"}, "2406.01300": {"original_text": "  Text-guided image generation enables the creation of visual content from\ntextual descriptions. However, certain visual concepts cannot be effectively\nconveyed through language alone. This has sparked a renewed interest in\nutilizing the CLIP image embedding space for more visually-oriented tasks\nthrough methods such as IP-Adapter. Interestingly, the CLIP image embedding\nspace has been shown to be semantically meaningful, where linear operations\nwithin this space yield semantically meaningful results. Yet, the specific\nmeaning of these operations can vary unpredictably across different images. To\nharness this potential, we introduce pOps, a framework that trains specific\nsemantic operators directly on CLIP image embeddings. Each pOps operator is\nbuilt upon a pretrained Diffusion Prior model. While the Diffusion Prior model\nwas originally trained to map between text embeddings and image embeddings, we\ndemonstrate that it can be tuned to accommodate new input conditions, resulting\nin a diffusion operator. Working directly over image embeddings not only\nimproves our ability to learn semantic operations but also allows us to\ndirectly use a textual CLIP loss as an additional supervision when needed. We\nshow that pOps can be used to learn a variety of photo-inspired operators with\ndistinct semantic meanings, highlighting the semantic diversity and potential\nof our proposed approach.\n", "rewritten_text": "Text-to-image generation, while powerful, struggles with concepts difficult to describe in words.  This limitation has led to exploring CLIP's image embedding space for visual manipulation, using techniques like IP-Adapter.  While CLIP embeddings exhibit semantic meaning, where linear operations produce interpretable results, this meaning is inconsistently applied across images.  To address this, we introduce pOps, a framework training specific semantic operators directly on CLIP embeddings.  Each pOps operator leverages a fine-tuned Diffusion Prior model\u2014initially trained for text-to-image generation\u2014to create a diffusion operator capable of handling new input conditions.  Operating directly on image embeddings enhances semantic operation learning and allows for direct textual CLIP loss supervision.  Our results demonstrate pOps' ability to learn diverse, photo-realistic operators with clear semantic interpretations, showcasing the framework's potential.\n"}, "1805.07477": {"original_text": "  Augmenting neural networks with skip connections, as introduced in the\nso-called ResNet architecture, surprised the community by enabling the training\nof networks of more than 1,000 layers with significant performance gains. This\npaper deciphers ResNet by analyzing the effect of skip connections, and puts\nforward new theoretical results on the advantages of identity skip connections\nin neural networks. We prove that the skip connections in the residual blocks\nfacilitate preserving the norm of the gradient, and lead to stable\nback-propagation, which is desirable from optimization perspective. We also\nshow that, perhaps surprisingly, as more residual blocks are stacked, the\nnorm-preservation of the network is enhanced. Our theoretical arguments are\nsupported by extensive empirical evidence. Can we push for extra\nnorm-preservation? We answer this question by proposing an efficient method to\nregularize the singular values of the convolution operator and making the\nResNet's transition layers extra norm-preserving. Our numerical investigations\ndemonstrate that the learning dynamics and the classification performance of\nResNet can be improved by making it even more norm preserving. Our results and\nthe introduced modification for ResNet, referred to as Procrustes ResNets, can\nbe used as a guide for training deeper networks and can also inspire new deeper\narchitectures.\n", "rewritten_text": "This paper investigates the success of ResNet's skip connections in training exceptionally deep neural networks.  We present novel theoretical analysis demonstrating that these connections maintain gradient norms, leading to stable backpropagation and improved optimization.  Surprisingly, this norm-preservation effect strengthens with increasing network depth.  Furthermore, we introduce Procrustes ResNets, a modification that enhances norm-preservation by regularizing convolutional singular values.  Our theoretical findings and empirical results show that this improves both training dynamics and classification accuracy, offering a pathway to designing even deeper and more effective architectures.\n"}, "2104.08668": {"original_text": "  Communicating new research ideas involves highlighting similarities and\ndifferences with past work. Authors write fluent, often long sections to survey\nthe distinction of a new paper with related work. In this work we model\ngenerating related work sections while being cognisant of the motivation behind\nciting papers. Our content planning model generates a tree of cited papers\nbefore a surface realization model lexicalizes this skeleton. Our model\noutperforms several strong state-of-the-art summarization and multi-document\nsummarization models on generating related work on an ACL Anthology (AA) based\ndataset which we contribute.\n", "rewritten_text": "This paper presents a novel model for generating \"related work\" sections in research papers.  Unlike existing summarization models, our approach considers the underlying reasons for citing specific papers.  We employ a two-stage process: first, a content planning model creates a citation tree, then a surface realization model generates the text.  Evaluated on a new ACL Anthology-based dataset we introduce, our model surpasses several leading summarization and multi-document summarization models.\n"}, "1608.08021": {"original_text": "  This paper presents how we can achieve the state-of-the-art accuracy in\nmulti-category object detection task while minimizing the computational cost by\nadapting and combining recent technical innovations. Following the common\npipeline of \"CNN feature extraction + region proposal + RoI classification\", we\nmainly redesign the feature extraction part, since region proposal part is not\ncomputationally expensive and classification part can be efficiently compressed\nwith common techniques like truncated SVD. Our design principle is \"less\nchannels with more layers\" and adoption of some building blocks including\nconcatenated ReLU, Inception, and HyperNet. The designed network is deep and\nthin and trained with the help of batch normalization, residual connections,\nand learning rate scheduling based on plateau detection. We obtained solid\nresults on well-known object detection benchmarks: 83.8% mAP (mean average\nprecision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only\n750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA\nTitan X GPU. Theoretically, our network requires only 12.3% of the\ncomputational cost compared to ResNet-101, the winner on VOC2012.\n", "rewritten_text": "This paper introduces a novel multi-category object detection network achieving state-of-the-art accuracy with significantly reduced computational cost.  By focusing on a redesigned, deep and narrow feature extraction network (employing concatenated ReLU, Inception, and HyperNet building blocks and adhering to a \"less channels, more layers\" principle), while leveraging established techniques for efficient region proposal and classification (e.g., truncated SVD), we surpass existing methods.  Training utilized batch normalization, residual connections, and plateau-based learning rate scheduling.  The resulting network achieved 83.8% mAP on VOC2007 and 82.5% mAP on VOC2012 (placing second), processing images at 750ms/image on a single-core Intel i7-6700K CPU and 46ms/image on an NVIDIA Titan X GPU \u2013 approximately 12.3% of the computational cost of the ResNet-101 VOC2012 winner.\n"}, "2210.05738": {"original_text": "  In this work, we propose to explicitly use the landmarks of prostate to guide\nthe MR-TRUS image registration. We first train a deep neural network to\nautomatically localize a set of meaningful landmarks, and then directly\ngenerate the affine registration matrix from the location of these landmarks.\nFor landmark localization, instead of directly training a network to predict\nthe landmark coordinates, we propose to regress a full-resolution distance map\nof the landmark, which is demonstrated effective in avoiding statistical bias\nto unsatisfactory performance and thus improving performance. We then use the\npredicted landmarks to generate the affine transformation matrix, which\noutperforms the clinicians' manual rigid registration by a significant margin\nin terms of TRE.\n", "rewritten_text": "This study introduces a novel prostate MR-TRUS image registration method guided by automatically localized landmarks.  A deep neural network predicts full-resolution distance maps for key landmarks, mitigating biases and improving accuracy compared to direct coordinate prediction.  These predicted landmarks then directly generate an affine transformation matrix, significantly outperforming manual clinician registration as measured by target registration error (TRE).\n"}, "2006.08844": {"original_text": "  We tackle the problem of establishing dense pixel-wise correspondences\nbetween a pair of images. In this work, we introduce Dual-Resolution\nCorrespondence Networks (DualRC-Net), to obtain pixel-wise correspondences in a\ncoarse-to-fine manner. DualRC-Net extracts both coarse- and fine- resolution\nfeature maps. The coarse maps are used to produce a full but coarse 4D\ncorrelation tensor, which is then refined by a learnable neighbourhood\nconsensus module. The fine-resolution feature maps are used to obtain the final\ndense correspondences guided by the refined coarse 4D correlation tensor. The\nselected coarse-resolution matching scores allow the fine-resolution features\nto focus only on a limited number of possible matches with high confidence. In\nthis way, DualRC-Net dramatically increases matching reliability and\nlocalisation accuracy, while avoiding to apply the expensive 4D convolution\nkernels on fine-resolution feature maps. We comprehensively evaluate our method\non large-scale public benchmarks including HPatches, InLoc, and Aachen\nDay-Night. It achieves the state-of-the-art results on all of them.\n", "rewritten_text": "This paper presents DualRC-Net, a novel approach to establishing dense pixel-wise correspondences between image pairs.  Employing a coarse-to-fine strategy, DualRC-Net extracts features at both coarse and fine resolutions.  A coarse 4D correlation tensor is initially generated from the coarse features and refined using a learnable neighborhood consensus module.  This refined tensor then guides the generation of final dense correspondences from the fine features, focusing computation on high-confidence matches. This approach significantly improves matching reliability and localization accuracy while avoiding computationally expensive 4D convolutions on high-resolution features.  Extensive evaluation on HPatches, InLoc, and Aachen Day-Night benchmarks demonstrates state-of-the-art performance.\n"}, "1704.00405": {"original_text": "  As for semantic role labeling (SRL) task, when it comes to utilizing parsing\ninformation, both traditional methods and recent recurrent neural network (RNN)\nbased methods use the feature engineering way. In this paper, we propose Syntax\nAware Long Short Time Memory(SA-LSTM). The structure of SA-LSTM modifies\naccording to dependency parsing information in order to model parsing\ninformation directly in an architecture engineering way instead of feature\nengineering way. We experimentally demonstrate that SA-LSTM gains more\nimprovement from the model architecture. Furthermore, SA-LSTM outperforms the\nstate-of-the-art on CPB 1.0 significantly according to Student t-test\n($p<0.05$).\n", "rewritten_text": "This paper introduces Syntax-Aware Long Short-Term Memory (SA-LSTM), a novel architecture for semantic role labeling (SRL).  Unlike existing traditional and RNN-based methods that rely on feature engineering to incorporate parsing information, SA-LSTM directly integrates dependency parsing information into its architecture.  Experimental results on the CPB 1.0 dataset demonstrate that SA-LSTM significantly outperforms the state-of-the-art (p<0.05, Student's t-test), achieving substantial improvements through its architectural modifications.\n"}, "2304.08481": {"original_text": "  High-definition (HD) semantic maps are crucial in enabling autonomous\nvehicles to navigate urban environments. The traditional method of creating\noffline HD maps involves labor-intensive manual annotation processes, which are\nnot only costly but also insufficient for timely updates. Recent studies have\nproposed an alternative approach that generates local maps using online sensor\nobservations. However, this approach is limited by the sensor's perception\nrange and its susceptibility to occlusions. In this study, we propose Neural\nMap Prior (NMP), a neural representation of global maps. This representation\nautomatically updates itself and improves the performance of local map\ninference. Specifically, we utilize two approaches to achieve this. Firstly, to\nintegrate a strong map prior into local map inference, we apply\ncross-attention, a mechanism that dynamically identifies correlations between\ncurrent and prior features. Secondly, to update the global neural map prior, we\nutilize a learning-based fusion module that guides the network in fusing\nfeatures from previous traversals. Our experimental results, based on the\nnuScenes dataset, demonstrate that our framework is highly compatible with\nvarious map segmentation and detection architectures. It significantly improves\nmap prediction performance, even in challenging weather conditions and\nsituations with a longer perception range. To the best of our knowledge, this\nis the first learning-based system for creating a global map prior.\n", "rewritten_text": "Autonomous vehicle navigation in urban areas relies heavily on high-definition (HD) semantic maps.  Traditional offline map creation is expensive and slow due to manual annotation. While online sensor-based mapping offers an alternative, it's limited by sensor range and occlusion.  This paper introduces Neural Map Prior (NMP), a novel learning-based system for creating and maintaining a global HD map representation. NMP uses cross-attention to integrate prior map knowledge into local map inference and a learning-based fusion module to update the global map from past sensor data.  Experiments on the nuScenes dataset show significant improvements in map prediction accuracy across various architectures, even under challenging conditions and extended perception ranges.  This is the first known learning-based approach to generating a global map prior.\n"}, "2202.10108": {"original_text": "  Vision transformers have shown great potential in various computer vision\ntasks owing to their strong capability to model long-range dependency using the\nself-attention mechanism. Nevertheless, they treat an image as a 1D sequence of\nvisual tokens, lacking an intrinsic inductive bias (IB) in modeling local\nvisual structures and dealing with scale variance, which is instead learned\nimplicitly from large-scale training data with longer training schedules. In\nthis paper, we propose a Vision Transformer Advanced by Exploring intrinsic IB\nfrom convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid\nreduction modules to downsample and embed the input image into tokens with rich\nmulti-scale context using multiple convolutions with different dilation rates.\nIn this way, it acquires an intrinsic scale invariance IB and can learn robust\nfeature representation for objects at various scales. Moreover, in each\ntransformer layer, ViTAE has a convolution block parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. The proposed two kinds\nof cells are stacked in both isotropic and multi-stage manners to formulate two\nfamilies of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on\nthe ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and\nAP10K datasets validate the superiority of our models over the baseline\ntransformer models and concurrent works. Besides, we scale up our ViTAE model\nto 644M parameters and obtain the state-of-the-art classification performance,\ni.e., 88.5% Top-1 classification accuracy on ImageNet validation set and the\nbest 91.2% Top-1 accuracy on ImageNet real validation set, without using extra\nprivate data.\n", "rewritten_text": "Vision Transformers (ViTs) excel at modeling long-range dependencies but lack inherent understanding of local visual structures and scale variations.  To address this, we introduce ViTAE (Vision Transformer Advanced by Exploring intrinsic IB from convolutions), which incorporates convolutional spatial pyramid reduction modules to generate multi-scale tokens. This provides inherent scale invariance.  Furthermore, each transformer layer includes a parallel convolutional block, fused with the self-attention output, enabling collaborative learning of local and global features.  ViTAE's architecture is flexible, offering both isotropic and multi-stage variants (ViTAE and ViTAEv2).  Extensive experiments on ImageNet, MS COCO, ADE20K, and AP10K demonstrate ViTAE's superiority over existing ViTs, achieving state-of-the-art results, including 88.5% Top-1 accuracy on ImageNet validation and 91.2% on the real ImageNet validation set, without using external data.  A scaled-up 644M parameter ViTAE model was used to achieve these results.\n"}, "1809.08371": {"original_text": "  Human pose estimation is an essential yet challenging task in computer\nvision. One of the reasons for this difficulty is that there are many redundant\nregions in the images. In this work, we proposed a convolutional network\narchitecture combined with the novel attention model. We named it attention\nconvolutional neural network (ACNN). ACNN learns to focus on specific regions\nof different input features. It's a multi-stage architecture. Early stages\nfiltrate the \"nothing-regions\", such as background and redundant body parts.\nAnd then, they submit the important regions which contain the joints of the\nhuman body to the following stages to get a more accurate result. What's more,\nit does not require extra manual annotations and self-learning is one of our\nintentions. We separately trained the network because the attention learning\ntask and the pose estimation task are not independent. State-of-the-art\nperformance is obtained on the MPII benchmarks.\n", "rewritten_text": "This paper introduces Attention Convolutional Neural Network (ACNN), a novel multi-stage convolutional architecture for human pose estimation.  Addressing the challenge of redundant image regions, ACNN incorporates a novel attention model to selectively focus on key areas containing human joints.  Early stages filter out irrelevant background and redundant body parts, refining the input for subsequent stages and improving accuracy.  Importantly, ACNN requires no additional manual annotations and is designed for self-supervised learning, with the attention and pose estimation tasks trained separately due to their interdependence.  The resulting model achieves state-of-the-art performance on the MPII benchmark.\n"}, "1310.0302": {"original_text": "  Surface registration is a technique that is used in various areas such as\nobject recognition and 3D model reconstruction. Problem of surface registration\ncan be analyzed as an optimization problem of seeking a rigid motion between\ntwo different views. Genetic algorithms can be used for solving this\noptimization problem, both for obtaining the robust parameter estimation and\nfor its fine-tuning. The main drawback of genetic algorithms is that they are\ntime consuming which makes them unsuitable for online applications. Modern\nacquisition systems enable the implementation of the solutions that would\nimmediately give the information on the rotational angles between the different\nviews, thus reducing the dimension of the optimization problem. The paper gives\nan analysis of the genetic algorithm implemented in the conditions when the\nrotation matrix is known and a comparison of these results with results when\nthis information is not available.\n", "rewritten_text": "Surface registration, crucial for object recognition and 3D model reconstruction, involves finding the optimal rigid transformation between two views.  While genetic algorithms offer robust parameter estimation, their computational cost limits their use in real-time applications.  This paper analyzes a genetic algorithm for surface registration, comparing its performance when the rotation matrix is known (reducing the optimization problem's dimensionality) against its performance when this information is unavailable, leveraging the capabilities of modern acquisition systems that can directly provide rotational angles.\n"}, "2008.0194": {"original_text": "  While the progress of machine translation of written text has come far in the\npast several years thanks to the increasing availability of parallel corpora\nand corpora-based training technologies, automatic translation of spoken text\nand dialogues remains challenging even for modern systems. In this paper, we\naim to boost the machine translation quality of conversational texts by\nintroducing a newly constructed Japanese-English business conversation parallel\ncorpus. A detailed analysis of the corpus is provided along with challenging\nexamples for automatic translation. We also experiment with adding the corpus\nin a machine translation training scenario and show how the resulting system\nbenefits from its use.\n", "rewritten_text": "Recent advancements in machine translation (MT) of written text, fueled by readily available parallel corpora and training methods, haven't translated to similar success in spoken language and dialogue.  This paper addresses this gap by introducing a new Japanese-English parallel corpus of business conversations. We analyze this corpus, highlighting translation challenges, and demonstrate improved MT performance by incorporating it into a training model.\n"}, "2204.08308": {"original_text": "  With the rapid development of multimedia technology, Augmented Reality (AR)\nhas become a promising next-generation mobile platform. The primary theory\nunderlying AR is human visual confusion, which allows users to perceive the\nreal-world scenes and augmented contents (virtual-world scenes) simultaneously\nby superimposing them together. To achieve good Quality of Experience (QoE), it\nis important to understand the interaction between two scenarios, and\nharmoniously display AR contents. However, studies on how this superimposition\nwill influence the human visual attention are lacking. Therefore, in this\npaper, we mainly analyze the interaction effect between background (BG) scenes\nand AR contents, and study the saliency prediction problem in AR. Specifically,\nwe first construct a Saliency in AR Dataset (SARD), which contains 450 BG\nimages, 450 AR images, as well as 1350 superimposed images generated by\nsuperimposing BG and AR images in pair with three mixing levels. A large-scale\neye-tracking experiment among 60 subjects is conducted to collect eye movement\ndata. To better predict the saliency in AR, we propose a vector quantized\nsaliency prediction method and generalize it for AR saliency prediction. For\ncomparison, three benchmark methods are proposed and evaluated together with\nour proposed method on our SARD. Experimental results demonstrate the\nsuperiority of our proposed method on both of the common saliency prediction\nproblem and the AR saliency prediction problem over benchmark methods. Our\ndataset and code are available at: https://github.com/DuanHuiyu/ARSaliency.\n", "rewritten_text": "Augmented Reality (AR), fueled by rapid multimedia advancements, is emerging as a leading mobile platform.  Its foundation lies in exploiting human visual perception to seamlessly blend real-world and virtual scenes.  Optimizing user experience (QoE) requires understanding and harmonizing the interaction between these scenes.  However, research on how this superposition affects visual attention is limited.  This paper addresses this gap by analyzing the interplay between background (BG) scenes and AR content, focusing on AR saliency prediction.  We introduce a new dataset, SARD, comprising 450 BG images, 450 AR images, and 1350 superimposed images at three mixing levels.  Eye-tracking data from 60 participants informs a novel vector-quantized saliency prediction method, specifically adapted for AR.  Comparative analysis against three benchmark methods on SARD demonstrates its superior performance in both general and AR saliency prediction.  The dataset and code are publicly available at: https://github.com/DuanHuiyu/ARSaliency.\n"}, "2210.16865": {"original_text": "  Explicit decomposition modeling, which involves breaking down complex tasks\ninto more straightforward and often more interpretable sub-tasks, has long been\na central theme in developing robust and interpretable NLU systems. However,\ndespite the many datasets and resources built as part of this effort, the\nmajority have small-scale annotations and limited scope, which is insufficient\nto solve general decomposition tasks. In this paper, we look at large-scale\nintermediate pre-training of decomposition-based transformers using distant\nsupervision from comparable texts, particularly large-scale parallel news. We\nshow that with such intermediate pre-training, developing robust\ndecomposition-based models for a diverse range of tasks becomes more feasible.\nFor example, on semantic parsing, our model, DecompT5, improves 20% to 30% on\ntwo datasets, Overnight and TORQUE, over the baseline language model. We\nfurther use DecompT5 to build a novel decomposition-based QA system named\nDecompEntail, improving over state-of-the-art models, including GPT-3, on both\nHotpotQA and StrategyQA by 8% and 4%, respectively.\n", "rewritten_text": "Robust and interpretable natural language understanding (NLU) systems often rely on explicit task decomposition.  While this approach has been pursued extensively, existing datasets are too small and limited to enable truly general solutions.  This paper introduces a novel approach: large-scale intermediate pre-training of decomposition-based transformers using distant supervision from parallel news corpora.  This method significantly improves the feasibility of building robust decomposition models.  Our resulting model, DecompT5, achieves 20-30% improvement on semantic parsing benchmarks (Overnight and TORQUE) over baseline language models.  Furthermore, a DecompT5-based question answering system, DecompEntail, surpasses state-of-the-art models like GPT-3, showing 8% and 4% gains on HotpotQA and StrategyQA, respectively.\n"}, "2103.16364": {"original_text": "  Unsupervised person re-identification (ReID) aims at learning discriminative\nidentity features without annotations. Recently, self-supervised contrastive\nlearning has gained increasing attention for its effectiveness in unsupervised\nrepresentation learning. The main idea of instance contrastive learning is to\nmatch a same instance in different augmented views. However, the relationship\nbetween different instances has not been fully explored in previous contrastive\nmethods, especially for instance-level contrastive loss. To address this issue,\nwe propose Inter-instance Contrastive Encoding (ICE) that leverages\ninter-instance pairwise similarity scores to boost previous class-level\ncontrastive ReID methods. We first use pairwise similarity ranking as one-hot\nhard pseudo labels for hard instance contrast, which aims at reducing\nintra-class variance. Then, we use similarity scores as soft pseudo labels to\nenhance the consistency between augmented and original views, which makes our\nmodel more robust to augmentation perturbations. Experiments on several\nlarge-scale person ReID datasets validate the effectiveness of our proposed\nunsupervised method ICE, which is competitive with even supervised methods.\nCode is made available at https://github.com/chenhao2345/ICE.\n", "rewritten_text": "Unsupervised person re-identification (ReID) typically uses self-supervised contrastive learning to learn discriminative features without labels.  Existing methods primarily focus on matching augmented views of the same person, neglecting inter-instance relationships.  Our proposed Inter-instance Contrastive Encoding (ICE) addresses this by incorporating pairwise similarity scores.  These scores generate hard pseudo-labels for improved intra-class variance reduction and soft pseudo-labels to enhance robustness to data augmentation.  Experiments on large-scale datasets demonstrate ICE's effectiveness, achieving performance comparable to supervised methods.  Code is available at https://github.com/chenhao2345/ICE.\n"}, "1607.07604": {"original_text": "  The interpretation and analysis of the wireless capsule endoscopy recording\nis a complex task which requires sophisticated computer aided decision (CAD)\nsystems in order to help physicians with the video screening and, finally, with\nthe diagnosis. Most of the CAD systems in the capsule endoscopy share a common\nsystem design, but use very different image and video representations. As a\nresult, each time a new clinical application of WCE appears, new CAD system has\nto be designed from scratch. This characteristic makes the design of new CAD\nsystems a very time consuming. Therefore, in this paper we introduce a system\nfor small intestine motility characterization, based on Deep Convolutional\nNeural Networks, which avoids the laborious step of designing specific features\nfor individual motility events. Experimental results show the superiority of\nthe learned features over alternative classifiers constructed by using state of\nthe art hand-crafted features. In particular, it reaches a mean classification\naccuracy of 96% for six intestinal motility events, outperforming the other\nclassifiers by a large margin (a 14% relative performance increase).\n", "rewritten_text": "Analyzing wireless capsule endoscopy (WCE) videos is complex, demanding sophisticated computer-aided diagnosis (CAD) systems to assist physicians.  While most existing WCE CAD systems share a similar design, their differing image/video representations necessitate rebuilding each system for new clinical applications, a time-consuming process.  This paper presents a novel system for small intestine motility characterization using deep convolutional neural networks (DCNNs).  This approach avoids the need for manually designing features for each motility event.  Experimental results demonstrate the superiority of learned DCNN features over state-of-the-art handcrafted features, achieving a 96% mean classification accuracy across six motility events\u2014a 14% relative improvement over other classifiers.\n"}, "2206.12262": {"original_text": "  Microblogs have become a social platform for people to express their emotions\nin real-time, and it is a trend to analyze user emotional tendencies from the\ninformation on Microblogs. The dynamic features of emojis can affect the\nsentiment polarity of microblog texts. Since existing models seldom consider\nthe diversity of emoji sentiment polarity,the paper propose a microblog\nsentiment classification model based on ALBERT-FAET. We obtain text embedding\nvia ALBERT pretraining model and learn the inter-emoji embedding with an\nattention-based LSTM network. In addition, a fine-grained attention mechanism\nis proposed to capture the word-level interactions between plain text and\nemoji. Finally, we concatenate these features and feed them into a CNN\nclassifier to predict the sentiment labels of the microblogs. To verify the\neffectiveness of the model and the fine-grained attention network, we conduct\ncomparison experiments and ablation experiments. The comparison experiments\nshow that the model outperforms previous methods in three evaluation indicators\n(accuracy, precision, and recall) and the model can significantly improve\nsentiment classification. The ablation experiments show that compared with\nALBERT-AET, the proposed model ALBERT-FAET is better in the metrics, indicating\nthat the fine-grained attention network can understand the diversified\ninformation of emoticons.\n", "rewritten_text": "This paper introduces ALBERT-FAET, a novel microblog sentiment classification model that addresses the limitations of existing methods by explicitly considering the diverse emotional expressions conveyed by emojis.  The model leverages pre-trained ALBERT embeddings for text, incorporates an attention-based LSTM network to learn emoji relationships, and employs a fine-grained attention mechanism to capture text-emoji interactions.  These features are then fed into a CNN classifier for sentiment prediction.  Experimental results, including comparative and ablation studies, demonstrate that ALBERT-FAET significantly outperforms existing models in accuracy, precision, and recall, highlighting the effectiveness of the proposed fine-grained attention network in capturing nuanced emoji sentiment.\n"}, "1910.07481": {"original_text": "  In Machine Translation, considering the document as a whole can help to\nresolve ambiguities and inconsistencies. In this paper, we propose a simple yet\npromising approach to add contextual information in Neural Machine Translation.\nWe present a method to add source context that capture the whole document with\naccurate boundaries, taking every word into account. We provide this additional\ninformation to a Transformer model and study the impact of our method on three\nlanguage pairs. The proposed approach obtains promising results in the\nEnglish-German, English-French and French-English document-level translation\ntasks. We observe interesting cross-sentential behaviors where the model learns\nto use document-level information to improve translation coherence.\n", "rewritten_text": "This paper introduces a novel method for improving neural machine translation (NMT) by incorporating full document context.  Our approach efficiently integrates source document information, including precise word boundaries, into a Transformer model.  Experiments on English-German, English-French, and French-English document-level translation tasks demonstrate promising results, with evidence suggesting the model effectively leverages this contextual information to enhance translation coherence across sentences.\n"}, "2007.02424": {"original_text": "  Recent advances in unsupervised domain adaptation for semantic segmentation\nhave shown great potentials to relieve the demand of expensive per-pixel\nannotations. However, most existing works address the domain discrepancy by\naligning the data distributions of two domains at a global image level whereas\nthe local consistencies are largely neglected. This paper presents an\ninnovative local contextual-relation consistent domain adaptation (CrCDA)\ntechnique that aims to achieve local-level consistencies during the\nglobal-level alignment. The idea is to take a closer look at region-wise\nfeature representations and align them for local-level consistencies.\nSpecifically, CrCDA learns and enforces the prototypical local\ncontextual-relations explicitly in the feature space of a labelled source\ndomain while transferring them to an unlabelled target domain via\nbackpropagation-based adversarial learning. An adaptive entropy max-min\nadversarial learning scheme is designed to optimally align these hundreds of\nlocal contextual-relations across domain without requiring discriminator or\nextra computation overhead. The proposed CrCDA has been evaluated extensively\nover two challenging domain adaptive segmentation tasks (e.g., GTA5 to\nCityscapes and SYNTHIA to Cityscapes), and experiments demonstrate its superior\nsegmentation performance as compared with state-of-the-art methods.\n", "rewritten_text": "Unsupervised domain adaptation for semantic segmentation offers a promising solution to the high cost of pixel-level annotation.  However, current methods primarily focus on aligning global image distributions, ignoring crucial local consistencies.  This paper introduces CrCDA, a novel technique that achieves both global and local alignment.  CrCDA leverages region-wise feature representations, explicitly learning and enforcing prototypical local contextual relations in a labeled source domain and transferring them to an unlabeled target domain using adversarial learning.  A novel adaptive entropy max-min adversarial learning scheme efficiently aligns these relations without needing a discriminator or extra computation.  Extensive experiments on challenging domain adaptation tasks (GTA5 to Cityscapes and SYNTHIA to Cityscapes) demonstrate CrCDA's superior performance compared to existing state-of-the-art methods.\n"}, "1805.10047": {"original_text": "  Neural machine translation (NMT) has a drawback in that can generate only\nhigh-frequency words owing to the computational costs of the softmax function\nin the output layer.\n  In Japanese-English NMT, Japanese predicate conjugation causes an increase in\nvocabulary size. For example, one verb can have as many as 19 surface\nvarieties. In this research, we focus on predicate conjugation for compressing\nthe vocabulary size in Japanese. The vocabulary list is filled with the various\nforms of verbs. We propose methods using predicate conjugation information\nwithout discarding linguistic information. The proposed methods can generate\nlow-frequency words and deal with unknown words. Two methods were considered to\nintroduce conjugation information: the first considers it as a token\n(conjugation token) and the second considers it as an embedded vector\n(conjugation feature).\n  The results using these methods demonstrate that the vocabulary size can be\ncompressed by approximately 86.1% (Tanaka corpus) and the NMT models can output\nthe words not in the training data set. Furthermore, BLEU scores improved by\n0.91 points in Japanese-to-English translation, and 0.32 points in\nEnglish-to-Japanese translation with ASPEC.\n", "rewritten_text": "Neural Machine Translation (NMT) systems struggle to generate low-frequency words due to the computational burden of the softmax function.  This is exacerbated in Japanese-English translation by the extensive conjugation of verbs, which significantly inflates the vocabulary size (e.g., a single verb can have 19 forms).  This research addresses this by proposing two methods to leverage predicate conjugation information without losing linguistic detail: representing conjugation as a token or as an embedded vector.  Experiments using these methods achieved an approximately 86.1% vocabulary reduction (Tanaka corpus) and enabled the generation of out-of-vocabulary words.  Furthermore, BLEU scores improved by 0.91 points for Japanese-to-English and 0.32 points for English-to-Japanese translation (using ASPEC).\n"}, "1703.01028": {"original_text": "  Outlier detection and cluster number estimation is an important issue for\nclustering real data. This paper focuses on spectral clustering, a time-tested\nclustering method, and reveals its important properties related to outliers.\nThe highlights of this paper are the following two mathematical observations:\nfirst, spectral clustering's intrinsic property of an outlier cluster\nformation, and second, the singularity of an outlier cluster with a valid\ncluster number. Based on these observations, we designed a function that\nevaluates clustering and outlier detection results. In experiments, we prepared\ntwo scenarios, face clustering in photo album and person re-identification in a\ncamera network. We confirmed that the proposed method detects outliers and\nestimates the number of clusters properly in both problems. Our method\noutperforms state-of-the-art methods in both the 128-dimensional sparse space\nfor face clustering and the 4,096-dimensional non-sparse space for person\nre-identification.\n", "rewritten_text": "This paper addresses the challenge of outlier detection and optimal cluster number estimation in spectral clustering, a widely used clustering method.  We present two key mathematical findings: spectral clustering's inherent tendency to form outlier clusters, and the singularity of these outlier clusters when the correct cluster number is identified.  These findings underpin a novel evaluation function for clustering and outlier detection.  Experiments on face clustering (128-dimensional sparse data) and person re-identification (4096-dimensional non-sparse data) demonstrate superior performance compared to existing methods in accurately detecting outliers and estimating the number of clusters in both scenarios.\n"}, "2402.19122": {"original_text": "  Gait recognition stands as one of the most pivotal remote identification\ntechnologies and progressively expands across research and industry\ncommunities. However, existing gait recognition methods heavily rely on\ntask-specific upstream driven by supervised learning to provide explicit gait\nrepresentations like silhouette sequences, which inevitably introduce expensive\nannotation costs and potential error accumulation. Escaping from this trend,\nthis work explores effective gait representations based on the all-purpose\nknowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a\nsimple yet efficient gait framework, termed BigGait. Specifically, the Gait\nRepresentation Extractor (GRE) within BigGait draws upon design principles from\nestablished gait representations, effectively transforming all-purpose\nknowledge into implicit gait representations without requiring third-party\nsupervision signals. Experiments on CCPG, CAISA-B* and SUSTech1K indicate that\nBigGait significantly outperforms the previous methods in both within-domain\nand cross-domain tasks in most cases, and provides a more practical paradigm\nfor learning the next-generation gait representation. Finally, we delve into\nprospective challenges and promising directions in LVMs-based gait recognition,\naiming to inspire future work in this emerging topic. The source code is\navailable at https://github.com/ShiqiYu/OpenGait.\n", "rewritten_text": "Gait recognition is a rapidly growing remote identification technology.  Current methods, however, rely heavily on expensive, supervised learning techniques to create explicit gait representations (e.g., silhouette sequences).  This paper introduces BigGait, a novel gait recognition framework that leverages the implicit gait representations learned by task-agnostic Large Vision Models (LVMs).  BigGait's Gait Representation Extractor (GRE) efficiently transforms LVM knowledge into gait representations without requiring manual annotations.  Experiments on CCPG, CAISA-B*, and SUSTech1K datasets demonstrate BigGait's superior performance over existing methods in both within-domain and cross-domain scenarios.  The paper concludes by discussing future research directions for LVM-based gait recognition.  Source code is available at https://github.com/ShiqiYu/OpenGait.\n"}, "1808.03959": {"original_text": "  Deep Learning based stereo matching methods have shown great successes and\nachieved top scores across different benchmarks. However, like most data-driven\nmethods, existing deep stereo matching networks suffer from some well-known\ndrawbacks such as requiring large amount of labeled training data, and that\ntheir performances are fundamentally limited by the generalization ability. In\nthis paper, we propose a novel Recurrent Neural Network (RNN) that takes a\ncontinuous (possibly previously unseen) stereo video as input, and directly\npredicts a depth-map at each frame without a pre-training process, and without\nthe need of ground-truth depth-maps as supervision. Thanks to the recurrent\nnature (provided by two convolutional-LSTM blocks), our network is able to\nmemorize and learn from its past experiences, and modify its inner parameters\n(network weights) to adapt to previously unseen or unfamiliar environments.\nThis suggests a remarkable generalization ability of the net, making it\napplicable in an {\\em open world} setting. Our method works robustly with\nchanges in scene content, image statistics, and lighting and season conditions\n{\\em etc}. By extensive experiments, we demonstrate that the proposed method\nseamlessly adapts between different scenarios. Equally important, in terms of\nthe stereo matching accuracy, it outperforms state-of-the-art deep stereo\napproaches on standard benchmark datasets such as KITTI and Middlebury stereo.\n", "rewritten_text": "While deep learning has yielded highly accurate stereo matching, existing methods rely heavily on large labeled datasets and suffer from limited generalization.  This paper introduces a novel recurrent convolutional neural network (RNN) that directly predicts depth maps from continuous stereo video streams without pre-training or ground truth depth supervision.  Leveraging two convolutional LSTM blocks, the network learns and adapts to new environments by memorizing past experiences, exhibiting strong generalization capabilities suitable for open-world scenarios.  Robust performance is demonstrated across varying scene content, image statistics, lighting, and seasonal conditions.  Extensive experiments show seamless adaptation between diverse scenarios and superior accuracy compared to state-of-the-art deep stereo methods on benchmark datasets like KITTI and Middlebury.\n"}, "2206.12571": {"original_text": "  This competition focus on Urban-Sense Segmentation based on the vehicle\ncamera view. Class highly unbalanced Urban-Sense images dataset challenge the\nexisting solutions and further studies. Deep Conventional neural network-based\nsemantic segmentation methods such as encoder-decoder architecture and\nmulti-scale and pyramid-based approaches become flexible solutions applicable\nto real-world applications. In this competition, we mainly review the\nliterature and conduct experiments on transformer-driven methods especially\nSegFormer, to achieve an optimal trade-off between performance and efficiency.\nFor example, SegFormer-B0 achieved 74.6% mIoU with the smallest FLOPS, 15.6G,\nand the largest model, SegFormer- B5 archived 80.2% mIoU. According to multiple\nfactors, including individual case failure analysis, individual class\nperformance, training pressure and efficiency estimation, the final candidate\nmodel for the competition is SegFormer- B2 with 50.6 GFLOPS and 78.5% mIoU\nevaluated on the testing set. Checkout our code implementation at\nhttps://vmv.re/cv3315.\n", "rewritten_text": "This competition addresses urban scene segmentation using vehicle camera images.  The highly imbalanced Urban-Sense dataset poses a significant challenge. While encoder-decoder and multi-scale deep learning methods offer viable solutions, this work focuses on transformer-based approaches, specifically SegFormer.  Experiments revealed a performance-efficiency trade-off: SegFormer-B0 achieved 74.6% mIoU with low computational cost (15.6 GFLOPS), while SegFormer-B5 reached 80.2% mIoU at higher cost.  Considering factors like individual class performance and computational efficiency, SegFormer-B2 (78.5% mIoU, 50.6 GFLOPS) was selected as the optimal model.  Our code is available at https://vmv.re/cv3315.\n"}, "2204.11994": {"original_text": "  Digital pathological analysis is run as the main examination used for cancer\ndiagnosis. Recently, deep learning-driven feature extraction from pathology\nimages is able to detect genetic variations and tumor environment, but few\nstudies focus on differential gene expression in tumor cells. In this paper, we\npropose a self-supervised contrastive learning framework, HistCode, to infer\ndifferential gene expressions from whole slide images (WSIs). We leveraged\ncontrastive learning on large-scale unannotated WSIs to derive slide-level\nhistopathological feature in latent space, and then transfer it to tumor\ndiagnosis and prediction of differentially expressed cancer driver genes. Our\nextensive experiments showed that our method outperformed other\nstate-of-the-art models in tumor diagnosis tasks, and also effectively\npredicted differential gene expressions. Interestingly, we found the higher\nfold-changed genes can be more precisely predicted. To intuitively illustrate\nthe ability to extract informative features from pathological images, we\nspatially visualized the WSIs colored by the attentive scores of image tiles.\nWe found that the tumor and necrosis areas were highly consistent with the\nannotations of experienced pathologists. Moreover, the spatial heatmap\ngenerated by lymphocyte-specific gene expression patterns was also consistent\nwith the manually labeled WSI.\n", "rewritten_text": "Digital pathology is the primary method for cancer diagnosis. While deep learning excels at extracting features from pathology images to identify genetic variations and tumor microenvironment,  its application to predicting differential gene expression in tumor cells remains limited.  This paper introduces HistCode, a self-supervised contrastive learning framework that infers differential gene expression from whole slide images (WSIs).  By leveraging unannotated WSIs, HistCode learns slide-level histopathological features in a latent space, which are then used for tumor diagnosis and prediction of differentially expressed cancer driver genes.  Our results demonstrate superior performance compared to existing methods in both tumor diagnosis and differential gene expression prediction, particularly for genes with high fold changes.  Spatial visualization of attentive scores on WSIs reveals that the model focuses on tumor and necrosis areas, aligning with pathologist annotations.  Similarly, the spatial heatmap for lymphocyte-specific gene expression matches manual labeling.\n"}, "2309.07509": {"original_text": "  Generating realistic talking faces is a complex and widely discussed task\nwith numerous applications. In this paper, we present DiffTalker, a novel model\ndesigned to generate lifelike talking faces through audio and landmark\nco-driving. DiffTalker addresses the challenges associated with directly\napplying diffusion models to audio control, which are traditionally trained on\ntext-image pairs. DiffTalker consists of two agent networks: a\ntransformer-based landmarks completion network for geometric accuracy and a\ndiffusion-based face generation network for texture details. Landmarks play a\npivotal role in establishing a seamless connection between the audio and image\ndomains, facilitating the incorporation of knowledge from pre-trained diffusion\nmodels. This innovative approach efficiently produces articulate-speaking\nfaces. Experimental results showcase DiffTalker's superior performance in\nproducing clear and geometrically accurate talking faces, all without the need\nfor additional alignment between audio and image features.\n", "rewritten_text": "This paper introduces DiffTalker, a novel model for generating realistic talking faces from audio input.  Unlike traditional methods relying on text-image pairs, DiffTalker uses a two-agent architecture: a transformer network for precise landmark prediction and a diffusion model for detailed texture generation.  This co-driving approach, leveraging landmarks to bridge audio and visual domains, effectively generates highly articulate and geometrically accurate talking faces without requiring pre-alignment of audio and image features.  Our results demonstrate DiffTalker's superior performance in this complex and widely studied area.\n"}, "2210.09071": {"original_text": "  Monocular Depth Estimation (MDE) aims to predict pixel-wise depth given a\nsingle RGB image. For both, the convolutional as well as the recent\nattention-based models, encoder-decoder-based architectures have been found to\nbe useful due to the simultaneous requirement of global context and pixel-level\nresolution. Typically, a skip connection module is used to fuse the encoder and\ndecoder features, which comprises of feature map concatenation followed by a\nconvolution operation. Inspired by the demonstrated benefits of attention in a\nmultitude of computer vision problems, we propose an attention-based fusion of\nencoder and decoder features. We pose MDE as a pixel query refinement problem,\nwhere coarsest-level encoder features are used to initialize pixel-level\nqueries, which are then refined to higher resolutions by the proposed Skip\nAttention Module (SAM). We formulate the prediction problem as ordinal\nregression over the bin centers that discretize the continuous depth range and\nintroduce a Bin Center Predictor (BCP) module that predicts bins at the\ncoarsest level using pixel queries. Apart from the benefit of image adaptive\ndepth binning, the proposed design helps learn improved depth embedding in\ninitial pixel queries via direct supervision from the ground truth. Extensive\nexperiments on the two canonical datasets, NYUV2 and KITTI, show that our\narchitecture outperforms the state-of-the-art by 5.3% and 3.9%, respectively,\nalong with an improved generalization performance by 9.4% on the SUNRGBD\ndataset. Code is available at https://github.com/ashutosh1807/PixelFormer.git.\n", "rewritten_text": "This paper introduces PixelFormer, a novel monocular depth estimation (MDE) model that leverages attention mechanisms for improved encoder-decoder feature fusion.  Unlike traditional skip connections, PixelFormer employs a Skip Attention Module (SAM) to refine pixel-level queries initialized by coarse encoder features.  Furthermore, a Bin Center Predictor (BCP) module performs ordinal regression on discretized depth bins, enabling image-adaptive binning and direct supervision of initial depth embeddings.  Experiments on NYUV2, KITTI, and SUNRGBD datasets demonstrate state-of-the-art performance, achieving improvements of 5.3%, 3.9%, and 9.4% respectively.  Code is available at https://github.com/ashutosh1807/PixelFormer.git.\n"}, "2312.11716": {"original_text": "  Demand for efficient onboard object detection is increasing due to its key\nrole in autonomous navigation. However, deploying object detection models such\nas YOLO on resource constrained edge devices is challenging due to the high\ncomputational requirements of such models. In this paper, an compressed object\ndetection model named Squeezed Edge YOLO is examined. This model is compressed\nand optimized to kilobytes of parameters in order to fit onboard such edge\ndevices. To evaluate Squeezed Edge YOLO, two use cases - human and shape\ndetection - are used to show the model accuracy and performance. Moreover, the\nmodel is deployed onboard a GAP8 processor with 8 RISC-V cores and an NVIDIA\nJetson Nano with 4GB of memory. Experimental results show Squeezed Edge YOLO\nmodel size is optimized by a factor of 8x which leads to 76% improvements in\nenergy efficiency and 3.3x faster throughout.\n", "rewritten_text": "The growing demand for autonomous navigation fuels the need for efficient onboard object detection.  However, deploying computationally intensive models like YOLO on resource-constrained edge devices is difficult.  This paper introduces Squeezed Edge YOLO, a compressed object detection model optimized to a kilobyte-scale parameter size for deployment on such devices.  Evaluated on human and shape detection tasks, and deployed on a GAP8 processor (8 RISC-V cores) and an NVIDIA Jetson Nano (4GB RAM), Squeezed Edge YOLO achieves an 8x model size reduction, resulting in a 76% energy efficiency improvement and a 3.3x speedup.\n"}, "2409.15939": {"original_text": "  3D shape completion is traditionally solved using supervised training or by\ndistribution learning on complete shape examples. Recently self-supervised\nlearning approaches that do not require any complete 3D shape examples have\ngained more interests. In this paper, we propose a non-adversarial\nself-supervised approach for the shape completion task. Our first finding is\nthat completion problems can be formulated as an involutory function trivially,\nwhich implies a special constraint on the completion function G, such that\nG(G(X)) = X. Our second constraint on self-supervised shape completion relies\non the fact that shape completion becomes easier to solve with correspondences\nand similarly, completion can simplify the correspondences problem. We\nformulate a consistency measure in the canonical space in order to supervise\nthe completion function. We efficiently optimize the completion and\ncorrespondence modules using \"freeze and alternate\" strategy. The overall\napproach performs well for rigid shapes in a category as well as dynamic\nnon-rigid shapes. We ablate our design choices and compare our solution against\nstate-of-the-art methods, showing remarkable accuracy approaching supervised\naccuracy in some cases.\n", "rewritten_text": "This paper introduces a novel self-supervised approach to 3D shape completion, avoiding the need for complete shape examples in training.  Unlike traditional supervised or distribution-based methods, our non-adversarial technique leverages two key constraints:  first, the inherent involutory nature of the completion function (G(G(X))=X); and second, the reciprocal relationship between shape completion and correspondence finding.  We define a consistency measure in canonical space to guide the self-supervised learning process, optimizing a completion and correspondence module using a \"freeze and alternate\" strategy.  Our method demonstrates strong performance on both rigid and non-rigid shapes, achieving state-of-the-art results and in some cases approaching the accuracy of supervised methods.  Ablation studies validate our design choices.\n"}, "1807.10552": {"original_text": "  Histology imaging is an essential diagnosis method to finalize the grade and\nstage of cancer of different tissues, especially for breast cancer diagnosis.\nSpecialists often disagree on the final diagnosis on biopsy tissue due to the\ncomplex morphological variety. Although convolutional neural networks (CNN)\nhave advantages in extracting discriminative features in image classification,\ndirectly training a CNN on high resolution histology images is computationally\ninfeasible currently. Besides, inconsistent discriminative features often\ndistribute over the whole histology image, which incurs challenges in\npatch-based CNN classification method. In this paper, we propose a novel\narchitecture for automatic classification of high resolution histology images.\nFirst, an adapted residual network is employed to explore hierarchical features\nwithout attenuation. Second, we develop a robust deep fusion network to utilize\nthe spatial relationship between patches and learn to correct the prediction\nbias generated from inconsistent discriminative feature distribution. The\nproposed method is evaluated using 10-fold cross-validation on 400 high\nresolution breast histology images with balanced labels and reports 95%\naccuracy on 4-class classification and 98.5% accuracy, 99.6% AUC on 2-class\nclassification (carcinoma and non-carcinoma), which substantially outperforms\nprevious methods and close to pathologist performance.\n", "rewritten_text": "Accurate cancer grading and staging, particularly for breast cancer, relies heavily on histology imaging.  However, the complex morphology of biopsy tissue often leads to inter-specialist diagnostic disagreements.  While Convolutional Neural Networks (CNNs) excel at feature extraction, applying them directly to high-resolution histology images is computationally prohibitive.  Furthermore, inconsistent feature distribution across images poses challenges for patch-based CNN approaches.  This paper introduces a novel CNN architecture for automated high-resolution histology image classification.  It uses an adapted residual network to capture hierarchical features effectively and a deep fusion network to leverage spatial patch relationships and mitigate prediction bias from inconsistent features.  Evaluated on 400 balanced high-resolution breast histology images using 10-fold cross-validation, our method achieves 95% accuracy in 4-class classification and 98.5% accuracy with 99.6% AUC in 2-class (carcinoma vs. non-carcinoma) classification, significantly surpassing existing methods and approaching pathologist performance.\n"}, "2205.11395": {"original_text": "  Hyperspectral anomalous change detection has been a challenging task for its\nemphasis on the dynamics of small and rare objects against the prevalent\nchanges. In this paper, we have proposed a Multi-Temporal spatial-spectral\nComparison Network for hyperspectral anomalous change detection (MTC-NET). The\nwhole model is a deep siamese network, aiming at learning the prevalent\nspectral difference resulting from the complex imaging conditions from the\nhyperspectral images by contrastive learning. A three-dimensional spatial\nspectral attention module is designed to effectively extract the spatial\nsemantic information and the key spectral differences. Then the gaps between\nthe multi-temporal features are minimized, boosting the alignment of the\nsemantic and spectral features and the suppression of the multi-temporal\nbackground spectral difference. The experiments on the \"Viareggio 2013\"\ndatasets demonstrate the effectiveness of proposed MTC-NET.\n", "rewritten_text": "Detecting subtle, infrequent changes in hyperspectral imagery is difficult due to the dominance of common variations.  This paper introduces MTC-NET, a multi-temporal spatial-spectral comparison network based on a deep Siamese architecture.  MTC-NET uses contrastive learning to identify prevalent spectral differences caused by complex imaging conditions.  A 3D spatial-spectral attention module enhances extraction of spatial context and key spectral variations.  By minimizing feature discrepancies across multiple time points, the network aligns semantic and spectral features while suppressing background variations.  Experiments on the Viareggio 2013 dataset validate MTC-NET's effectiveness.\n"}, "2109.01211": {"original_text": "  Reproducibility has become an intensely debated topic in NLP and ML over\nrecent years, but no commonly accepted way of assessing reproducibility, let\nalone quantifying it, has so far emerged. The assumption has been that wider\nscientific reproducibility terminology and definitions are not applicable to\nNLP/ML, with the result that many different terms and definitions have been\nproposed, some diametrically opposed. In this paper, we test this assumption,\nby taking the standard terminology and definitions from metrology and applying\nthem directly to NLP/ML. We find that we are able to straightforwardly derive a\npractical framework for assessing reproducibility which has the desirable\nproperty of yielding a quantified degree of reproducibility that is comparable\nacross different reproduction studies.\n", "rewritten_text": "Reproducibility in NLP and ML lacks a standardized assessment method.  While existing scientific definitions have been deemed inapplicable, this paper challenges that assumption. By directly applying metrological terminology and definitions, we develop a practical framework for quantifying and comparing reproducibility across different NLP/ML reproduction studies.\n"}, "2101.1065": {"original_text": "  In machine translation field, in both academia and industry, there is a\ngrowing interest in increasingly powerful systems, using corpora of several\nhundred million to several billion examples. These systems represent the\nstate-of-the-art. Here we defend the idea of developing in parallel <<frugal>>\nbilingual translation systems, trained with relatively small corpora. Based on\nthe observation of a standard human professional translator, we estimate that\nthe corpora should be composed at maximum of a monolingual sub-corpus of 75\nmillion examples for the source language, a second monolingual sub-corpus of 6\nmillion examples for the target language, and an aligned bilingual sub-corpus\nof 6 million bi-examples. A less desirable alternative would be an aligned\nbilingual corpus of 47.5 million bi-examples.\n", "rewritten_text": "While the machine translation field increasingly focuses on massive datasets (hundreds of millions to billions of examples), this paper advocates for the parallel development of \"frugal\" systems trained on significantly smaller corpora.  Based on observations of professional human translators, we propose optimal corpus sizes of 75 million source language monolingual examples, 6 million target language monolingual examples, and 6 million aligned bilingual examples.  A less optimal alternative would be a single 47.5 million example aligned bilingual corpus.\n"}, "2211.10104": {"original_text": "  Stereo images, containing left and right view images with disparity, are\nutilized in solving low-vision tasks recently, e.g., rain removal and\nsuper-resolution. Stereo image restoration methods usually obtain better\nperformance than monocular methods by learning the disparity between dual views\neither implicitly or explicitly. However, existing stereo rain removal methods\nstill cannot make full use of the complementary information between two views,\nand we find it is because: 1) the rain streaks have more complex distributions\nin directions and densities, which severely damage the complementary\ninformation and pose greater challenges; 2) the disparity estimation is not\naccurate enough due to the imperfect fusion mechanism for the features between\ntwo views. To overcome such limitations, we propose a new \\underline{Stereo}\n\\underline{I}mage \\underline{R}ain \\underline{R}emoval method (StereoIRR) via\nsufficient interaction between two views, which incorporates: 1) a new\nDual-view Mutual Attention (DMA) mechanism which generates mutual attention\nmaps by taking left and right views as key information for each other to\nfacilitate cross-view feature fusion; 2) a long-range and cross-view\ninteraction, which is constructed with basic blocks and dual-view mutual\nattention, can alleviate the adverse effect of rain on complementary\ninformation to help the features of stereo images to get long-range and\ncross-view interaction and fusion. Notably, StereoIRR outperforms other related\nmonocular and stereo image rain removal methods on several datasets. Our codes\nand datasets will be released.\n", "rewritten_text": "Recent low-vision tasks, such as rain removal and super-resolution, leverage stereo images (containing left and right views with disparity) to achieve superior performance compared to monocular methods.  This improvement stems from exploiting the disparity between the views, either implicitly or explicitly. However, current stereo rain removal techniques underutilize the complementary information in these views due to: 1) the complex and varied distribution of rain streaks, which significantly degrades this information; and 2) inaccurate disparity estimation resulting from imperfect inter-view feature fusion.  To address these limitations, we introduce StereoIRR, a novel stereo image rain removal method.  StereoIRR facilitates sufficient inter-view interaction through: 1) a Dual-view Mutual Attention (DMA) mechanism that generates mutual attention maps, enabling effective cross-view feature fusion; and 2) a long-range, cross-view interaction module built using basic blocks and DMA, mitigating rain's adverse effects and promoting comprehensive feature fusion.  Experiments on multiple datasets demonstrate StereoIRR's superior performance over existing monocular and stereo methods.  Our code and datasets will be publicly available.\n"}, "1704.07293": {"original_text": "  The accuracy of object detectors and trackers is most commonly evaluated by\nthe Intersection over Union (IoU) criterion. To date, most approaches are\nrestricted to axis-aligned or oriented boxes and, as a consequence, many\ndatasets are only labeled with boxes. Nevertheless, axis-aligned or oriented\nboxes cannot accurately capture an object's shape. To address this, a number of\ndensely segmented datasets has started to emerge in both the object detection\nand the object tracking communities. However, evaluating the accuracy of object\ndetectors and trackers that are restricted to boxes on densely segmented data\nis not straightforward. To close this gap, we introduce the relative\nIntersection over Union (rIoU) accuracy measure. The measure normalizes the IoU\nwith the optimal box for the segmentation to generate an accuracy measure that\nranges between 0 and 1 and allows a more precise measurement of accuracies.\nFurthermore, it enables an efficient and easy way to understand scenes and the\nstrengths and weaknesses of an object detection or tracking approach. We\ndisplay how the new measure can be efficiently calculated and present an\neasy-to-use evaluation framework. The framework is tested on the DAVIS and the\nVOT2016 segmentations and has been made available to the community.\n", "rewritten_text": "Object detection and tracking accuracy is typically assessed using Intersection over Union (IoU).  However, the prevalent use of axis-aligned or oriented bounding boxes, mirroring common dataset labeling practices, limits accuracy as these boxes poorly represent object shapes.  While densely segmented datasets are increasingly available, evaluating box-based detectors/trackers on this data is challenging.  To address this, we introduce relative IoU (rIoU), a normalized IoU score (0-1) that uses the optimal bounding box for a given segmentation.  rIoU provides a more precise and easily interpretable accuracy measure, revealing strengths and weaknesses of detection/tracking methods. We present an efficient calculation method and an open-source evaluation framework, demonstrated on the DAVIS and VOT2016 datasets.\n"}, "2306.02819": {"original_text": "  Natural language understanding (NLU) is an essential branch of natural\nlanguage processing, which relies on representations generated by pre-trained\nlanguage models (PLMs). However, PLMs primarily focus on acquiring\nlexico-semantic information, while they may be unable to adequately handle the\nmeaning of constructions. To address this issue, we introduce construction\ngrammar (CxG), which highlights the pairings of form and meaning, to enrich\nlanguage representation. We adopt usage-based construction grammar as the basis\nof our work, which is highly compatible with statistical models such as PLMs.\nThen a HyCxG framework is proposed to enhance language representation through a\nthree-stage solution. First, all constructions are extracted from sentences via\na slot-constraints approach. As constructions can overlap with each other,\nbringing redundancy and imbalance, we formulate the conditional max coverage\nproblem for selecting the discriminative constructions. Finally, we propose a\nrelational hypergraph attention network to acquire representation from\nconstructional information by capturing high-order word interactions among\nconstructions. Extensive experiments demonstrate the superiority of the\nproposed model on a variety of NLU tasks.\n", "rewritten_text": "This paper addresses the limitations of pre-trained language models (PLMs) in natural language understanding (NLU), specifically their insufficient handling of constructional meaning.  We leverage usage-based Construction Grammar (CxG), which explicitly links form and meaning, to enrich PLM representations.  Our proposed HyCxG framework employs a three-stage process:  first, extracting constructions using a slot-constraints approach; second, resolving constructional overlap and redundancy via a conditional max coverage problem; and third, employing a relational hypergraph attention network to capture high-order word interactions within these constructions.  Experiments across various NLU tasks demonstrate the effectiveness of our approach.\n"}, "1309.4573": {"original_text": "  This paper is based on an application of smoothing of 3D face images followed\nby feature detection i.e. detecting the nose tip. The present method uses a\nweighted mesh median filtering technique for smoothing. In this present\nsmoothing technique we have built the neighborhood surrounding a particular\npoint in 3D face and replaced that with the weighted value of the surrounding\npoints in 3D face image. After applying the smoothing technique to the 3D face\nimages our experimental results show that we have obtained considerable\nimprovement as compared to the algorithm without smoothing. We have used here\nthe maximum intensity algorithm for detecting the nose-tip and this method\ncorrectly detects the nose-tip in case of any pose i.e. along X, Y, and Z axes.\nThe present technique gave us worked successfully on 535 out of 542 3D face\nimages as compared to the method without smoothing which worked only on 521 3D\nface images out of 542 face images. Thus we have obtained a 98.70% performance\nrate over 96.12% performance rate of the algorithm without smoothing. All the\nexperiments have been performed on the FRAV3D database.\n", "rewritten_text": "This paper presents a 3D face nose-tip detection method that incorporates weighted mesh median filtering for image smoothing prior to feature detection.  This smoothing technique improves the accuracy of nose-tip detection, which is performed using a maximum intensity algorithm robust to variations in pose (X, Y, and Z axes).  Experiments on the FRAV3D database showed a significant improvement: the proposed method successfully detected the nose-tip in 535 out of 542 images (98.70% success rate), compared to 521 out of 542 (96.12%) for the method without smoothing.\n"}, "2306.16682": {"original_text": "  Egocentric action anticipation aims to predict the future actions the camera\nwearer will perform from the observation of the past. While predictions about\nthe future should be available before the predicted events take place, most\napproaches do not pay attention to the computational time required to make such\npredictions. As a result, current evaluation schemes assume that predictions\nare available right after the input video is observed, i.e., presuming a\nnegligible runtime, which may lead to overly optimistic evaluations. We propose\na streaming egocentric action evaluation scheme which assumes that predictions\nare performed online and made available only after the model has processed the\ncurrent input segment, which depends on its runtime. To evaluate all models\nconsidering the same prediction horizon, we hence propose that slower models\nshould base their predictions on temporal segments sampled ahead of time. Based\non the observation that model runtime can affect performance in the considered\nstreaming evaluation scenario, we further propose a lightweight action\nanticipation model based on feed-forward 3D CNNs which is optimized using\nknowledge distillation techniques with a novel past-to-future distillation\nloss. Experiments on the three popular datasets EPIC-KITCHENS-55,\nEPIC-KITCHENS-100 and EGTEA Gaze+ show that (i) the proposed evaluation scheme\ninduces a different ranking on state-of-the-art methods as compared to classic\nevaluations, (ii) lightweight approaches tend to outmatch more computationally\nexpensive ones, and (iii) the proposed model based on feed-forward 3D CNNs and\nknowledge distillation outperforms current art in the streaming egocentric\naction anticipation scenario.\n", "rewritten_text": "Predicting a camera wearer's future actions (egocentric action anticipation) typically ignores the prediction's computation time.  Existing evaluation methods unrealistically assume instantaneous predictions, leading to inflated performance scores.  We introduce a new streaming evaluation scheme that accounts for model runtime.  Slower models predict using pre-sampled data to ensure a fair comparison across all models within the same prediction horizon.  Recognizing the impact of runtime, we also propose a lightweight, feed-forward 3D CNN model optimized via knowledge distillation with a novel past-to-future loss.  Experiments on EPIC-KITCHENS-55, EPIC-KITCHENS-100, and EGTEA Gaze+ datasets demonstrate that our evaluation scheme alters the ranking of existing methods, favors lightweight models, and that our proposed model achieves state-of-the-art performance in streaming egocentric action anticipation.\n"}, "1508.04458": {"original_text": "  Three-dimensional x-ray CT image reconstruction in baggage scanning in\nsecurity applications is an important research field. The variety of materials\nto be reconstructed is broader than medical x-ray imaging. Presence of high\nattenuating materials such as metal may cause artifacts if analytical\nreconstruction methods are used. Statistical modeling and the resultant\niterative algorithms are known to reduce these artifacts and present good\nquantitative accuracy in estimates of linear attenuation coefficients. However,\niterative algorithms may require computations in order to achieve\nquantitatively accurate results. For the case of baggage scanning, in order to\nprovide fast accurate inspection throughput, they must be accelerated\ndrastically. There are many approaches proposed in the literature to increase\nspeed of convergence. This paper presents a new method that estimates the\nwavelet coefficients of the images in the discrete wavelet transform domain\ninstead of the image space itself. Initially, surrogate functions are created\naround approximation coefficients only. As the iterations proceed, the wavelet\ntree on which the updates are made is expanded based on a criterion and detail\ncoefficients at each level are updated and the tree is expanded this way. For\nexample, in the smooth regions of the image the detail coefficients are not\nupdated while the coefficients that represent the high-frequency component\naround edges are being updated, thus saving time by focusing computations where\nthey are needed. This approach is implemented on real data from a SureScan (TM)\nx1000 Explosive Detection System and compared to straightforward implementation\nof the unregularized alternating minimization of O'Sullivan and Benac [1].\n", "rewritten_text": "Baggage scanning security relies on accurate 3D X-ray CT image reconstruction, a challenge amplified by the diverse materials (including high-attenuating metals) compared to medical imaging.  While iterative statistical methods offer superior artifact reduction and quantitative accuracy, their computational demands hinder the speed needed for efficient baggage inspection.  This paper introduces a novel accelerated iterative algorithm.  Instead of directly reconstructing the image, it operates in the discrete wavelet transform domain, initially focusing on approximation coefficients and progressively expanding the wavelet tree to update detail coefficients only where necessary (e.g., near edges). This targeted approach, demonstrated on real data from a SureScan x1000 system, is compared to a standard unregularized alternating minimization method to show its improved speed.\n"}, "2301.00184": {"original_text": "  Most existing text-video retrieval methods focus on cross-modal matching\nbetween the visual content of videos and textual query sentences. However, in\nreal-world scenarios, online videos are often accompanied by relevant text\ninformation such as titles, tags, and even subtitles, which can be utilized to\nmatch textual queries. This insight has motivated us to propose a novel\napproach to text-video retrieval, where we directly generate associated\ncaptions from videos using zero-shot video captioning with knowledge from\nweb-scale pre-trained models (e.g., CLIP and GPT-2). Given the generated\ncaptions, a natural question arises: what benefits do they bring to text-video\nretrieval? To answer this, we introduce Cap4Video, a new framework that\nleverages captions in three ways: i) Input data: video-caption pairs can\naugment the training data. ii) Intermediate feature interaction: we perform\ncross-modal feature interaction between the video and caption to produce\nenhanced video representations. iii) Output score: the Query-Caption matching\nbranch can complement the original Query-Video matching branch for text-video\nretrieval. We conduct comprehensive ablation studies to demonstrate the\neffectiveness of our approach. Without any post-processing, Cap4Video achieves\nstate-of-the-art performance on four standard text-video retrieval benchmarks:\nMSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%). The code is\navailable at https://github.com/whwu95/Cap4Video .\n", "rewritten_text": "Current text-video retrieval methods primarily rely on matching video content with text queries.  However, online videos often include valuable textual metadata (titles, tags, subtitles).  This led us to develop Cap4Video, a novel text-video retrieval framework that leverages automatically generated video captions.  Using zero-shot video captioning with large language models (like CLIP and GPT-2), Cap4Video enhances retrieval in three ways:  augmenting training data with video-caption pairs, enriching video representations through cross-modal interaction with captions, and incorporating a Query-Caption matching branch alongside the standard Query-Video branch.  Our comprehensive evaluation on four benchmarks (MSR-VTT, VATEX, MSVD, and DiDeMo) demonstrates state-of-the-art performance (51.4%, 66.6%, 51.8%, and 52.0% respectively) without post-processing.  The code is available at https://github.com/whwu95/Cap4Video.\n"}, "1907.08854": {"original_text": "  Document Grounded Conversations is a task to generate dialogue responses when\nchatting about the content of a given document. Obviously, document knowledge\nplays a critical role in Document Grounded Conversations, while existing\ndialogue models do not exploit this kind of knowledge effectively enough. In\nthis paper, we propose a novel Transformer-based architecture for multi-turn\ndocument grounded conversations. In particular, we devise an Incremental\nTransformer to encode multi-turn utterances along with knowledge in related\ndocuments. Motivated by the human cognitive process, we design a two-pass\ndecoder (Deliberation Decoder) to improve context coherence and knowledge\ncorrectness. Our empirical study on a real-world Document Grounded Dataset\nproves that responses generated by our model significantly outperform\ncompetitive baselines on both context coherence and knowledge relevance.\n", "rewritten_text": "This paper introduces a novel Transformer-based architecture for generating coherent and factually accurate multi-turn dialogues grounded in a given document.  Unlike existing dialogue models, our approach leverages document knowledge effectively through an Incremental Transformer that encodes both multi-turn utterances and relevant document information.  Furthermore, a novel two-pass \"Deliberation Decoder,\" inspired by human cognitive processes, enhances context coherence and knowledge accuracy.  Experiments on a real-world dataset demonstrate that our model significantly outperforms existing methods in both context coherence and knowledge relevance.\n"}, "2406.05023": {"original_text": "  Generative adversarial networks (GANs) are machine learning models that are\nused to estimate the underlying statistical structure of a given dataset and as\na result can be used for a variety of tasks such as image generation or anomaly\ndetection. Despite their initial simplicity, designing an effective loss\nfunction for training GANs remains challenging, and various loss functions have\nbeen proposed aiming to improve the performance and stability of the generative\nmodels. In this study, loss function design for GANs is presented as an\noptimization problem solved using the genetic programming (GP) approach.\nInitial experiments were carried out using small Deep Convolutional GAN (DCGAN)\nmodel and the MNIST dataset, in order to search experimentally for an improved\nloss function. The functions found were evaluated on CIFAR10, with the best\nfunction, named GANetic loss, showing exceptionally better performance and\nstability compared to the losses commonly used for GAN training. To further\nevalute its general applicability on more challenging problems, GANetic loss\nwas applied for two medical applications: image generation and anomaly\ndetection. Experiments were performed with histopathological, gastrointestinal\nor glaucoma images to evaluate the GANetic loss in medical image generation,\nresulting in improved image quality compared to the baseline models. The\nGANetic Loss used for polyp and glaucoma images showed a strong improvement in\nthe detection of anomalies. In summary, the GANetic loss function was evaluated\non multiple datasets and applications where it consistently outperforms\nalternative loss functions. Moreover, GANetic loss leads to stable training and\nreproducible results, a known weak spot of GANs.\n", "rewritten_text": "This study addresses the challenge of designing effective loss functions for Generative Adversarial Networks (GANs).  Using genetic programming, we optimized the loss function for a Deep Convolutional GAN (DCGAN) trained on the MNIST dataset.  The resulting GANetic loss function significantly improved performance and stability compared to standard GAN loss functions, as demonstrated on the CIFAR-10 dataset.  Further evaluation on medical image generation (histopathological, gastrointestinal, and glaucoma images) and anomaly detection (polyp and glaucoma images) confirmed GANetic loss's superior performance and robustness, yielding higher-quality images and improved anomaly detection compared to baseline models.  These results highlight GANetic loss as a promising, stable, and reproducible alternative for various GAN applications.\n"}, "2208.01633": {"original_text": "  We present UnrealEgo, i.e., a new large-scale naturalistic dataset for\negocentric 3D human pose estimation. UnrealEgo is based on an advanced concept\nof eyeglasses equipped with two fisheye cameras that can be used in\nunconstrained environments. We design their virtual prototype and attach them\nto 3D human models for stereo view capture. We next generate a large corpus of\nhuman motions. As a consequence, UnrealEgo is the first dataset to provide\nin-the-wild stereo images with the largest variety of motions among existing\negocentric datasets. Furthermore, we propose a new benchmark method with a\nsimple but effective idea of devising a 2D keypoint estimation module for\nstereo inputs to improve 3D human pose estimation. The extensive experiments\nshow that our approach outperforms the previous state-of-the-art methods\nqualitatively and quantitatively. UnrealEgo and our source codes are available\non our project web page.\n", "rewritten_text": "UnrealEgo, a novel large-scale dataset for egocentric 3D human pose estimation, is introduced.  Generated using a virtual prototype of glasses with two fisheye cameras, it captures diverse human motions in unconstrained environments via stereo vision from 3D models.  UnrealEgo surpasses existing egocentric datasets in both scale and motion variety, offering in-the-wild stereo images.  We also present a new benchmark method\u2014a simple yet effective 2D keypoint estimation module for stereo inputs\u2014which significantly improves 3D pose estimation accuracy compared to state-of-the-art techniques.  The dataset and source code are publicly available on our project website.\n"}, "2011.131": {"original_text": "  To assess the knowledge proficiency of a learner, multiple choice question is\nan efficient and widespread form in standard tests. However, the composition of\nthe multiple choice question, especially the construction of distractors is\nquite challenging. The distractors are required to both incorrect and plausible\nenough to confuse the learners who did not master the knowledge. Currently, the\ndistractors are generated by domain experts which are both expensive and\ntime-consuming. This urges the emergence of automatic distractor generation,\nwhich can benefit various standard tests in a wide range of domains. In this\npaper, we propose a question and answer guided distractor generation (EDGE)\nframework to automate distractor generation. EDGE consists of three major\nmodules: (1) the Reforming Question Module and the Reforming Passage Module\napply gate layers to guarantee the inherent incorrectness of the generated\ndistractors; (2) the Distractor Generator Module applies attention mechanism to\ncontrol the level of plausibility. Experimental results on a large-scale public\ndataset demonstrate that our model significantly outperforms existing models\nand achieves a new state-of-the-art.\n", "rewritten_text": "Multiple-choice questions are a common and efficient way to assess learning, but creating effective distractors (incorrect but plausible answer choices) is challenging and time-consuming for human experts.  This paper introduces EDGE, a novel framework for automated distractor generation.  EDGE uses three modules:  question and passage reforming modules (employing gate layers to ensure incorrectness) and a distractor generator module (using an attention mechanism to control plausibility).  Experiments on a large public dataset show that EDGE significantly surpasses existing methods, achieving state-of-the-art performance.\n"}, "2308.14082": {"original_text": "  Reconstructing interacting hands from monocular images is indispensable in\nAR/VR applications. Most existing solutions rely on the accurate localization\nof each skeleton joint. However, these methods tend to be unreliable due to the\nsevere occlusion and confusing similarity among adjacent hand parts. This also\ndefies human perception because humans can quickly imitate an interaction\npattern without localizing all joints. Our key idea is to first construct a\ntwo-hand interaction prior and recast the interaction reconstruction task as\nthe conditional sampling from the prior. To expand more interaction states, a\nlarge-scale multimodal dataset with physical plausibility is proposed. Then a\nVAE is trained to further condense these interaction patterns as latent codes\nin a prior distribution. When looking for image cues that contribute to\ninteraction prior sampling, we propose the interaction adjacency heatmap (IAH).\nCompared with a joint-wise heatmap for localization, IAH assigns denser visible\nfeatures to those invisible joints. Compared with an all-in-one visible\nheatmap, it provides more fine-grained local interaction information in each\ninteraction region. Finally, the correlations between the extracted features\nand corresponding interaction codes are linked by the ViT module. Comprehensive\nevaluations on benchmark datasets have verified the effectiveness of this\nframework. The code and dataset are publicly available at\nhttps://github.com/binghui-z/InterPrior_pytorch\n", "rewritten_text": "Accurate hand interaction reconstruction from single images is crucial for augmented and virtual reality.  Existing methods, reliant on precise joint localization, struggle with occlusion and similar hand features.  Unlike these methods, our approach mimics human perception by focusing on interaction patterns rather than individual joints. We introduce a two-hand interaction prior, framing reconstruction as conditional sampling from this prior.  To enrich this prior, we created a large, physically realistic, multimodal dataset and trained a Variational Autoencoder (VAE) to represent interaction patterns as latent codes.  We also propose an Interaction Adjacency Heatmap (IAH) to guide sampling, focusing on visible features even for occluded joints, providing more detailed interaction information than joint-wise or global heatmaps.  Finally, a Vision Transformer (ViT) module links extracted features to interaction codes.  Extensive benchmark testing confirms our method's effectiveness.  The code and dataset are publicly available at https://github.com/binghui-z/InterPrior_pytorch.\n"}, "1910.10223": {"original_text": "  Deep generative models come with the promise to learn an explainable\nrepresentation for visual objects that allows image sampling, synthesis, and\nselective modification. The main challenge is to learn to properly model the\nindependent latent characteristics of an object, especially its appearance and\npose. We present a novel approach that learns disentangled representations of\nthese characteristics and explains them individually. Training requires only\npairs of images depicting the same object appearance, but no pose annotations.\nWe propose an additional classifier that estimates the minimal amount of\nregularization required to enforce disentanglement. Thus both representations\ntogether can completely explain an image while being independent of each other.\nPrevious methods based on adversarial approaches fail to enforce this\nindependence, while methods based on variational approaches lead to\nuninformative representations. In experiments on diverse object categories, the\napproach successfully recombines pose and appearance to reconstruct and\nretarget novel synthesized images. We achieve significant improvements over\nstate-of-the-art methods which utilize the same level of supervision, and reach\nperformances comparable to those of pose-supervised approaches. However, we can\nhandle the vast body of articulated object classes for which no pose\nmodels/annotations are available.\n", "rewritten_text": "This paper introduces a novel deep generative model that learns disentangled representations of object appearance and pose from image pairs showing the same object, requiring no pose annotations.  Unlike previous adversarial or variational methods, which either fail to ensure independence between representations or produce uninformative results, our approach uses a classifier to dynamically adjust regularization, ensuring both appearance and pose are independently and completely explained.  This allows for the successful recombination and retargeting of synthesized images, significantly outperforming unsupervised state-of-the-art methods and achieving performance comparable to supervised approaches, while overcoming the limitations of requiring pre-existing pose models.  Our method is particularly valuable for the many articulated object classes lacking pose annotations.\n"}, "2405.17765": {"original_text": "  Video quality assessment (VQA) is a challenging problem due to the numerous\nfactors that can affect the perceptual quality of a video, \\eg, content\nattractiveness, distortion type, motion pattern, and level. However, annotating\nthe Mean opinion score (MOS) for videos is expensive and time-consuming, which\nlimits the scale of VQA datasets, and poses a significant obstacle for deep\nlearning-based methods. In this paper, we propose a VQA method named PTM-VQA,\nwhich leverages PreTrained Models to transfer knowledge from models pretrained\non various pre-tasks, enabling benefits for VQA from different aspects.\n  Specifically, we extract features of videos from different pretrained models\nwith frozen weights and integrate them to generate representation. Since these\nmodels possess various fields of knowledge and are often trained with labels\nirrelevant to quality, we propose an Intra-Consistency and Inter-Divisibility\n(ICID) loss to impose constraints on features extracted by multiple pretrained\nmodels. The intra-consistency constraint ensures that features extracted by\ndifferent pretrained models are in the same unified quality-aware latent space,\nwhile the inter-divisibility introduces pseudo clusters based on the annotation\nof samples and tries to separate features of samples from different clusters.\nFurthermore, with a constantly growing number of pretrained models, it is\ncrucial to determine which models to use and how to use them. To address this\nproblem, we propose an efficient scheme to select suitable candidates. Models\nwith better clustering performance on VQA datasets are chosen to be our\ncandidates. Extensive experiments demonstrate the effectiveness of the proposed\nmethod.\n", "rewritten_text": "Video quality assessment (VQA) is hampered by the subjective nature of perceived quality and the high cost of obtaining human-rated Mean Opinion Scores (MOS).  This limits the size of training datasets, hindering deep learning approaches.  This paper introduces PTM-VQA, a novel VQA method that leverages pre-trained models (PTMs).  PTM-VQA extracts features from multiple PTMs (with frozen weights) and integrates them into a unified representation.  To harmonize these diverse features, often trained on unrelated tasks, we introduce an Intra-Consistency and Inter-Divisibility (ICID) loss.  This loss ensures feature consistency across PTMs within a quality-aware latent space and promotes separation of features based on MOS annotations.  Finally, to address the growing number of available PTMs, we propose a selection scheme prioritizing models demonstrating strong clustering performance on VQA datasets.  Extensive experiments validate the effectiveness of our approach.\n"}, "2306.00246": {"original_text": "  The monetary value of a given piece of real estate, a parcel, is often\nreadily available from a geographic information system. However, for many\napplications, such as insurance and urban planning, it is useful to have\nestimates of property value at much higher spatial resolutions. We propose a\nmethod to estimate the distribution over property value at the pixel level from\nremote sensing imagery. We evaluate on a real-world dataset of a major urban\narea. Our results show that the proposed approaches are capable of generating\nfine-level estimates of property values, significantly improving upon a diverse\ncollection of baseline approaches.\n", "rewritten_text": "Real estate parcel values are often accessible through GIS, but many applications (e.g., insurance, urban planning) require finer-grained estimates.  We present a novel method using remote sensing imagery to predict property value distributions at the pixel level.  Evaluated on a large urban dataset, our method significantly outperforms existing approaches, generating highly detailed property value maps.\n"}, "1606.02147": {"original_text": "  The ability to perform pixel-wise semantic segmentation in real-time is of\nparamount importance in mobile applications. Recent deep neural networks aimed\nat this task have the disadvantage of requiring a large number of floating\npoint operations and have long run-times that hinder their usability. In this\npaper, we propose a novel deep neural network architecture named ENet\n(efficient neural network), created specifically for tasks requiring low\nlatency operation. ENet is up to 18$\\times$ faster, requires 75$\\times$ less\nFLOPs, has 79$\\times$ less parameters, and provides similar or better accuracy\nto existing models. We have tested it on CamVid, Cityscapes and SUN datasets\nand report on comparisons with existing state-of-the-art methods, and the\ntrade-offs between accuracy and processing time of a network. We present\nperformance measurements of the proposed architecture on embedded systems and\nsuggest possible software improvements that could make ENet even faster.\n", "rewritten_text": "Real-time pixel-wise semantic segmentation is crucial for mobile applications, but existing deep neural networks are too computationally expensive.  This paper introduces ENet, a novel architecture designed for low-latency operation.  ENet achieves up to 18x faster processing speed, 75x fewer floating-point operations, and 79x fewer parameters than existing models, while maintaining or improving accuracy.  We evaluate ENet on CamVid, Cityscapes, and SUN datasets, comparing it to state-of-the-art methods and analyzing the accuracy-speed trade-off.  Furthermore, we present embedded system performance results and propose potential software optimizations for further speed improvements.\n"}, "2201.11374": {"original_text": "  In this work, we focus on low-resource dependency parsing for multiple\nlanguages. Several strategies are tailored to enhance performance in\nlow-resource scenarios. While these are well-known to the community, it is not\ntrivial to select the best-performing combination of these strategies for a\nlow-resource language that we are interested in, and not much attention has\nbeen given to measuring the efficacy of these strategies. We experiment with 5\nlow-resource strategies for our ensembled approach on 7 Universal Dependency\n(UD) low-resource languages. Our exhaustive experimentation on these languages\nsupports the effective improvements for languages not covered in pretrained\nmodels. We show a successful application of the ensembled system on a truly\nlow-resource language Sanskrit. The code and data are available at:\nhttps://github.com/Jivnesh/SanDP\n", "rewritten_text": "This paper investigates effective dependency parsing strategies for low-resource languages.  While several techniques exist, optimally combining them for a specific low-resource language remains challenging.  We empirically evaluate five such strategies within an ensemble approach across seven Universal Dependencies (UD) low-resource languages.  Our results demonstrate significant performance improvements, particularly for languages lacking pre-trained models, and showcase successful application to the extremely low-resource language Sanskrit.  Code and data are available at: https://github.com/Jivnesh/SanDP\n"}, "2407.03841": {"original_text": "  Large Language Models (LLMs) have showcased remarkable capabilities in\nvarious Natural Language Processing tasks. For automatic open-domain dialogue\nevaluation in particular, LLMs have been seamlessly integrated into evaluation\nframeworks, and together with human evaluation, compose the backbone of most\nevaluations. However, existing evaluation benchmarks often rely on outdated\ndatasets and evaluate aspects like Fluency and Relevance, which fail to\nadequately capture the capabilities and limitations of state-of-the-art chatbot\nmodels.\n  This paper critically examines current evaluation benchmarks, highlighting\nthat the use of older response generators and quality aspects fail to\naccurately reflect modern chatbot capabilities. A small annotation experiment\non a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as\nGPT-4 struggle to detect actual deficiencies in dialogues generated by current\nLLM chatbots.\n", "rewritten_text": "Large Language Models (LLMs) are integral to evaluating open-domain dialogue systems, often alongside human assessment.  However, current evaluation benchmarks suffer from outdated datasets and metrics (like fluency and relevance) that inadequately assess the strengths and weaknesses of advanced chatbot models. This paper critiques these benchmarks, arguing that reliance on older response generators and simplistic quality measures misrepresents modern chatbot capabilities.  An annotation experiment using the recent SODA dataset demonstrates that even powerful LLMs like GPT-4 struggle to identify flaws in dialogues generated by state-of-the-art chatbot LLMs.\n"}, "1502.00377": {"original_text": "  In order to track the moving objects in long range against occlusion,\ninterruption, and background clutter, this paper proposes a unified approach\nfor global trajectory analysis. Instead of the traditional frame-by-frame\ntracking, our method recovers target trajectories based on a short sequence of\nvideo frames, e.g. $15$ frames. We initially calculate a foreground map at each\nframe, as obtained from a state-of-the-art background model. An attribute graph\nis then extracted from the foreground map, where the graph vertices are image\nprimitives represented by the composite features. With this graph\nrepresentation, we pose trajectory analysis as a joint task of spatial graph\npartitioning and temporal graph matching. The task can be formulated by\nmaximizing a posteriori under the Bayesian framework, in which we integrate the\nspatio-temporal contexts and the appearance models. The probabilistic inference\nis achieved by a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given a\nperoid of observed frames, the algorithm simulates a ergodic and aperiodic\nMarkov Chain, and it visits a sequence of solution states in the joint space of\nspatial graph partitioning and temporal graph matching. In the experiments, our\nmethod is tested on several challenging videos from the public datasets of\nvisual surveillance, and it outperforms the state-of-the-art methods.\n", "rewritten_text": "This paper presents a novel global trajectory analysis approach for robust long-range tracking of moving objects, even with occlusion, interruptions, and background clutter.  Unlike traditional frame-by-frame methods, our approach uses short video sequences (e.g., 15 frames) to recover trajectories.  We begin by generating foreground maps from a state-of-the-art background model.  These maps are then converted into attribute graphs, where nodes represent image primitives with composite features.  Trajectory analysis is formulated as a joint spatial graph partitioning and temporal graph matching problem, solved by maximizing a posteriori probability within a Bayesian framework that incorporates spatio-temporal context and appearance models.  A data-driven Markov Chain Monte Carlo (MCMC) algorithm performs probabilistic inference, exploring the solution space through an ergodic and aperiodic Markov chain.  Experiments on challenging public surveillance videos demonstrate superior performance compared to existing state-of-the-art methods.\n"}, "1607.02204": {"original_text": "  In this paper we introduce a method to overcome one of the main challenges of\nperson re-identification in multi-camera networks, namely cross-view appearance\nchanges. The proposed solution addresses the extreme variability of person\nappearance in different camera views by exploiting multiple feature\nrepresentations. For each feature, Kernel Canonical Correlation Analysis (KCCA)\nwith different kernels is exploited to learn several projection spaces in which\nthe appearance correlation between samples of the same person observed from\ndifferent cameras is maximized. An iterative logistic regression is finally\nused to select and weigh the contributions of each feature projections and\nperform the matching between the two views. Experimental evaluation shows that\nthe proposed solution obtains comparable performance on VIPeR and PRID 450s\ndatasets and improves on PRID and CUHK01 datasets with respect to the state of\nthe art.\n", "rewritten_text": "This paper presents a novel method to address the significant challenge of cross-view appearance variations in multi-camera person re-identification.  Our approach leverages multiple feature representations, employing Kernel Canonical Correlation Analysis (KCCA) with diverse kernels to learn optimal projection spaces that maximize inter-camera appearance correlation for the same individual.  An iterative logistic regression then integrates these projections, weighting their contributions to achieve accurate cross-view matching.  Experimental results on VIPeR, PRID 450s, PRID, and CUHK01 datasets demonstrate performance comparable to, and in some cases exceeding, the current state-of-the-art.\n"}, "2208.03207": {"original_text": "  Learning with noisy labels (LNL) aims at designing strategies to improve\nmodel performance and generalization by mitigating the effects of model\noverfitting to noisy labels. The key success of LNL lies in identifying as many\nclean samples as possible from massive noisy data, while rectifying the wrongly\nassigned noisy labels. Recent advances employ the predicted label distributions\nof individual samples to perform noise verification and noisy label correction,\neasily giving rise to confirmation bias. To mitigate this issue, we propose\nNeighborhood Collective Estimation, in which the predictive reliability of a\ncandidate sample is re-estimated by contrasting it against its feature-space\nnearest neighbors. Specifically, our method is divided into two steps: 1)\nNeighborhood Collective Noise Verification to separate all training samples\ninto a clean or noisy subset, 2) Neighborhood Collective Label Correction to\nrelabel noisy samples, and then auxiliary techniques are used to assist further\nmodel optimization. Extensive experiments on four commonly used benchmark\ndatasets, i.e., CIFAR-10, CIFAR-100, Clothing-1M and Webvision-1.0, demonstrate\nthat our proposed method considerably outperforms state-of-the-art methods.\n", "rewritten_text": "Learning with noisy labels (LNL) seeks to enhance model performance and generalization by addressing overfitting caused by inaccurate labels.  Success hinges on identifying clean data within large, noisy datasets and correcting mislabeled samples.  Existing methods, which often rely on individual sample label distributions, are susceptible to confirmation bias.  To overcome this, we introduce Neighborhood Collective Estimation, a two-step approach:  first, it verifies the reliability of each sample by comparing it to its nearest neighbors in feature space, classifying samples as clean or noisy; second, it corrects noisy labels.  Auxiliary techniques further optimize the model.  Experiments on CIFAR-10, CIFAR-100, Clothing-1M, and Webvision-1.0 datasets show our method significantly surpasses current state-of-the-art techniques.\n"}, "2305.03112": {"original_text": "  A surge of interest has emerged in weakly supervised semantic segmentation\ndue to its remarkable efficiency in recent years. Existing approaches based on\ntransformers mainly focus on exploring the affinity matrix to boost CAMs with\nglobal relationships. While in this work, we first perform a scrupulous\nexamination towards the impact of successive affinity matrices and discover\nthat they possess an inclination toward sparsification as the network\napproaches convergence, hence disclosing a manifestation of over-smoothing.\nBesides, it has been observed that enhanced attention maps tend to evince a\nsubstantial amount of extraneous background noise in deeper layers. Drawing\nupon this, we posit a daring conjecture that the undisciplined over-smoothing\nphenomenon introduces a noteworthy quantity of semantically irrelevant\nbackground noise, causing performance degradation. To alleviate this issue, we\npropose a novel perspective that highlights the objects of interest by\ninvestigating the regions of the trait, thereby fostering an extensive\ncomprehension of the successive affinity matrix. Consequently, we suggest an\nadaptive re-activation mechanism (AReAM) that alleviates the issue of\nincomplete attention within the object and the unbounded background noise.\nAReAM accomplishes this by supervising high-level attention with shallow\naffinity matrices, yielding promising results. Exhaustive experiments conducted\non the commonly used dataset manifest that segmentation results can be greatly\nimproved through our proposed AReAM, which imposes restrictions on each\naffinity matrix in deep layers to make it attentive to semantic regions.\n", "rewritten_text": "Weakly supervised semantic segmentation has seen a surge in popularity due to its efficiency.  Existing transformer-based methods primarily leverage affinity matrices to improve class activation maps (CAMs) by capturing global relationships.  However, we find that successive affinity matrices become increasingly sparse during training, indicating over-smoothing.  Furthermore, deeper layers exhibit increased background noise in attention maps.  We hypothesize that this over-smoothing introduces semantically irrelevant background noise, hindering performance.  To address this, we propose Adaptive Re-activation Mechanism (AReAM), which uses shallow affinity matrices to guide high-level attention.  AReAM mitigates incomplete object attention and excessive background noise by selectively focusing on semantically relevant regions in deeper layers.  Extensive experiments demonstrate that AReAM significantly improves segmentation results on standard datasets.\n"}, "2010.10894": {"original_text": "  This paper aims to enhance the few-shot relation classification especially\nfor sentences that jointly describe multiple relations. Due to the fact that\nsome relations usually keep high co-occurrence in the same context, previous\nfew-shot relation classifiers struggle to distinguish them with few annotated\ninstances. To alleviate the above relation confusion problem, we propose CTEG,\na model equipped with two mechanisms to learn to decouple these easily-confused\nrelations. On the one hand, an Entity-Guided Attention (EGA) mechanism, which\nleverages the syntactic relations and relative positions between each word and\nthe specified entity pair, is introduced to guide the attention to filter out\ninformation causing confusion. On the other hand, a Confusion-Aware Training\n(CAT) method is proposed to explicitly learn to distinguish relations by\nplaying a pushing-away game between classifying a sentence into a true relation\nand its confusing relation. Extensive experiments are conducted on the FewRel\ndataset, and the results show that our proposed model achieves comparable and\neven much better results to strong baselines in terms of accuracy. Furthermore,\nthe ablation test and case study verify the effectiveness of our proposed EGA\nand CAT, especially in addressing the relation confusion problem.\n", "rewritten_text": "This paper addresses the challenge of few-shot relation classification, particularly for sentences expressing multiple, often co-occurring, relations.  Existing methods struggle to distinguish these relations with limited training data.  To overcome this, we introduce CTEG, a model incorporating two novel mechanisms: Entity-Guided Attention (EGA), which uses syntactic relationships and word positions relative to entities to focus attention and reduce confusion; and Confusion-Aware Training (CAT), which explicitly learns to differentiate relations through an adversarial training approach.  Experiments on the FewRel dataset demonstrate that CTEG achieves state-of-the-art or comparable performance to strong baselines.  Ablation studies and case analyses confirm the effectiveness of EGA and CAT in resolving relation ambiguity.\n"}, "1903.05396": {"original_text": "  This paper introduces improved methods for sub-event detection in social\nmedia streams, by applying neural sequence models not only on the level of\nindividual posts, but also directly on the stream level. Current approaches to\nidentify sub-events within a given event, such as a goal during a soccer match,\nessentially do not exploit the sequential nature of social media streams. We\naddress this shortcoming by framing the sub-event detection problem in social\nmedia streams as a sequence labeling task and adopt a neural sequence\narchitecture that explicitly accounts for the chronological order of posts.\nSpecifically, we (i) establish a neural baseline that outperforms a graph-based\nstate-of-the-art method for binary sub-event detection (2.7% micro-F1\nimprovement), as well as (ii) demonstrate superiority of a recurrent neural\nnetwork model on the posts sequence level for labeled sub-events (2.4%\nbin-level F1 improvement over non-sequential models).\n", "rewritten_text": "This paper presents novel methods for detecting sub-events within social media streams.  Unlike existing approaches that ignore the inherent sequential nature of these streams, our methods leverage neural sequence models to analyze both individual posts and the entire stream.  We frame sub-event detection as a sequence labeling task, using a neural architecture that explicitly considers the chronological order of posts.  Our results show that this approach significantly outperforms a state-of-the-art graph-based method for binary sub-event detection (2.7% micro-F1 improvement) and a non-sequential recurrent neural network (2.4% bin-level F1 improvement).\n"}, "2212.03496": {"original_text": "  Script event prediction aims to predict the subsequent event given the\ncontext. This requires the capability to infer the correlations between events.\nRecent works have attempted to improve event correlation reasoning by using\npretrained language models and incorporating external knowledge~(e.g.,\ndiscourse relations). Though promising results have been achieved, some\nchallenges still remain. First, the pretrained language models adopted by\ncurrent works ignore event-level knowledge, resulting in an inability to\ncapture the correlations between events well. Second, modeling correlations\nbetween events with discourse relations is limited because it can only capture\nexplicit correlations between events with discourse markers, and cannot capture\nmany implicit correlations. To this end, we propose a novel generative approach\nfor this task, in which a pretrained language model is fine-tuned with an\nevent-centric pretraining objective and predicts the next event within a\ngenerative paradigm. Specifically, we first introduce a novel event-level blank\ninfilling strategy as the learning objective to inject event-level knowledge\ninto the pretrained language model, and then design a likelihood-based\ncontrastive loss for fine-tuning the generative model. Instead of using an\nadditional prediction layer, we perform prediction by using sequence\nlikelihoods generated by the generative model. Our approach models correlations\nbetween events in a soft way without any external knowledge. The\nlikelihood-based prediction eliminates the need to use additional networks to\nmake predictions and is somewhat interpretable since it scores each word in the\nevent. Experimental results on the multi-choice narrative cloze~(MCNC) task\ndemonstrate that our approach achieves better results than other\nstate-of-the-art baselines. Our code will be available at\nhttps://github.com/zhufq00/mcnc.\n", "rewritten_text": "Predicting the next event in a script requires understanding event correlations. While recent methods using pretrained language models and external knowledge show promise, they struggle to fully capture event relationships.  Current approaches neglect event-level knowledge and rely heavily on explicitly marked correlations, missing many implicit ones.  This paper introduces a novel generative approach that addresses these limitations.  We fine-tune a pretrained language model using a new event-centric pretraining objective based on blank infilling, and employ a likelihood-based contrastive loss.  This allows for soft modeling of event correlations without external knowledge, and prediction is directly derived from the model's sequence likelihoods, eliminating the need for extra prediction layers.  Our method achieves state-of-the-art results on the MCNC benchmark, as demonstrated in our experiments (code available at https://github.com/zhufq00/mcnc).\n"}, "1911.12982": {"original_text": "  Recently, Chinese word segmentation (CWS) methods using neural networks have\nmade impressive progress. Most of them regard the CWS as a sequence labeling\nproblem which construct models based on local features rather than considering\nglobal information of input sequence. In this paper, we cast the CWS as a\nsequence translation problem and propose a novel sequence-to-sequence CWS model\nwith an attention-based encoder-decoder framework. The model captures the\nglobal information from the input and directly outputs the segmented sequence.\nIt can also tackle other NLP tasks with CWS jointly in an end-to-end mode.\nExperiments on Weibo, PKU and MSRA benchmark datasets show that our approach\nhas achieved competitive performances compared with state-of-the-art methods.\nMeanwhile, we successfully applied our proposed model to jointly learning CWS\nand Chinese spelling correction, which demonstrates its applicability of\nmulti-task fusion.\n", "rewritten_text": "Neural network-based Chinese Word Segmentation (CWS) has advanced significantly, but most methods treat it as a local sequence labeling problem, neglecting global context.  This paper introduces a novel sequence-to-sequence CWS model using an attention-based encoder-decoder architecture.  This approach treats CWS as a translation task, capturing global input information to directly generate segmented output.  Furthermore, it enables end-to-end multi-task learning, as demonstrated by its successful joint application with Chinese spelling correction.  Experiments on standard benchmarks (Weibo, PKU, MSRA) show our method achieves state-of-the-art performance.\n"}, "2308.14018": {"original_text": "  Few-shot font generation is challenging, as it needs to capture the\nfine-grained stroke styles from a limited set of reference glyphs, and then\ntransfer to other characters, which are expected to have similar styles.\nHowever, due to the diversity and complexity of Chinese font styles, the\nsynthesized glyphs of existing methods usually exhibit visible artifacts, such\nas missing details and distorted strokes. In this paper, we propose a\nVQGAN-based framework (i.e., VQ-Font) to enhance glyph fidelity through token\nprior refinement and structure-aware enhancement. Specifically, we pre-train a\nVQGAN to encapsulate font token prior within a codebook. Subsequently, VQ-Font\nrefines the synthesized glyphs with the codebook to eliminate the domain gap\nbetween synthesized and real-world strokes. Furthermore, our VQ-Font leverages\nthe inherent design of Chinese characters, where structure components such as\nradicals and character components are combined in specific arrangements, to\nrecalibrate fine-grained styles based on references. This process improves the\nmatching and fusion of styles at the structure level. Both modules collaborate\nto enhance the fidelity of the generated fonts. Experiments on a collected font\ndataset show that our VQ-Font outperforms the competing methods both\nquantitatively and qualitatively, especially in generating challenging styles.\n", "rewritten_text": "Generating fonts from few examples is difficult because it requires accurately capturing subtle stylistic details from limited reference characters and applying them consistently to others.  Existing methods struggle with the complexity of Chinese fonts, often producing artifacts like missing details and distorted strokes.  This paper introduces VQ-Font, a VQGAN-based framework that improves font fidelity.  VQ-Font uses a pre-trained VQGAN to learn a codebook of font style tokens, which it then uses to refine generated glyphs, bridging the gap between synthetic and real strokes.  Furthermore, it leverages the structural components of Chinese characters (radicals, etc.) to refine styles at a structural level, improving style consistency.  Experiments demonstrate VQ-Font's superior quantitative and qualitative performance, particularly with challenging font styles.\n"}, "2403.19836": {"original_text": "  Identifying the targets of hate speech is a crucial step in grasping the\nnature of such speech and, ultimately, in improving the detection of offensive\nposts on online forums. Much harmful content on online platforms uses implicit\nlanguage especially when targeting vulnerable and protected groups such as\nusing stereotypical characteristics instead of explicit target names, making it\nharder to detect and mitigate the language. In this study, we focus on\nidentifying implied targets of hate speech, essential for recognizing subtler\nhate speech and enhancing the detection of harmful content on digital\nplatforms. We define a new task aimed at identifying the targets even when they\nare not explicitly stated. To address that task, we collect and annotate target\nspans in three prominent implicit hate speech datasets: SBIC, DynaHate, and\nIHC. We call the resulting merged collection Implicit-Target-Span. The\ncollection is achieved using an innovative pooling method with matching scores\nbased on human annotations and Large Language Models (LLMs). Our experiments\nindicate that Implicit-Target-Span provides a challenging test bed for target\nspan detection methods.\n", "rewritten_text": "This study addresses the challenge of detecting implicit hate speech targets\u2014a crucial step in improving online hate speech detection.  Harmful online content often uses implicit language, particularly when targeting vulnerable groups, relying on stereotypes rather than explicit naming.  To tackle this, we introduce a new task focused on identifying implied targets. We created a new dataset, Implicit-Target-Span, by merging and annotating three existing datasets (SBIC, DynaHate, and IHC) using a novel pooling method combining human annotations and Large Language Model (LLM) scores.  Our results demonstrate that Implicit-Target-Span provides a robust and challenging benchmark for evaluating hate speech target detection methods.\n"}, "2303.05955": {"original_text": "  Remote photoplethysmography (rPPG) technology has drawn increasing attention\nin recent years. It can extract Blood Volume Pulse (BVP) from facial videos,\nmaking many applications like health monitoring and emotional analysis more\naccessible. However, as the BVP signal is easily affected by environmental\nchanges, existing methods struggle to generalize well for unseen domains. In\nthis paper, we systematically address the domain shift problem in the rPPG\nmeasurement task. We show that most domain generalization methods do not work\nwell in this problem, as domain labels are ambiguous in complicated\nenvironmental changes. In light of this, we propose a domain-label-free\napproach called NEuron STructure modeling (NEST). NEST improves the\ngeneralization capacity by maximizing the coverage of feature space during\ntraining, which reduces the chance for under-optimized feature activation\nduring inference. Besides, NEST can also enrich and enhance domain invariant\nfeatures across multi-domain. We create and benchmark a large-scale domain\ngeneralization protocol for the rPPG measurement task. Extensive experiments\nshow that our approach outperforms the state-of-the-art methods on both\ncross-dataset and intra-dataset settings.\n", "rewritten_text": "Remote photoplethysmography (rPPG) offers promising applications in health monitoring and emotion analysis by extracting blood volume pulse (BVP) from facial videos.  However, its susceptibility to environmental variations hinders its generalizability.  This paper tackles this domain shift problem in rPPG, demonstrating the limitations of existing domain generalization methods due to ambiguous domain labels in complex environments.  We introduce NEST (NEuron STructure modeling), a novel domain-label-free approach that enhances generalization by maximizing feature space coverage during training, thereby preventing under-optimized activations during inference and promoting domain-invariant feature learning.  Using a new large-scale benchmark dataset, we show that NEST significantly outperforms state-of-the-art methods in both cross-dataset and intra-dataset evaluations.\n"}, "2401.06957": {"original_text": "  As virtual environments continue to advance, the demand for immersive and\nemotionally engaging experiences has grown. Addressing this demand, we\nintroduce Emotion enabled Virtual avatar mapping using Optimized KnowledgE\ndistillation (EVOKE), a lightweight emotion recognition framework designed for\nthe seamless integration of emotion recognition into 3D avatars within virtual\nenvironments. Our approach leverages knowledge distillation involving\nmulti-label classification on the publicly available DEAP dataset, which covers\nvalence, arousal, and dominance as primary emotional classes. Remarkably, our\ndistilled model, a CNN with only two convolutional layers and 18 times fewer\nparameters than the teacher model, achieves competitive results, boasting an\naccuracy of 87% while demanding far less computational resources. This\nequilibrium between performance and deployability positions our framework as an\nideal choice for virtual environment systems. Furthermore, the multi-label\nclassification outcomes are utilized to map emotions onto custom-designed 3D\navatars.\n", "rewritten_text": "The increasing demand for emotionally immersive virtual experiences has driven the development of EVOKE, a lightweight emotion recognition framework.  EVOKE seamlessly integrates emotion recognition into 3D virtual avatars.  Using knowledge distillation and multi-label classification on the DEAP dataset (valence, arousal, and dominance), EVOKE's compact CNN model (two convolutional layers, 18x fewer parameters than its teacher model) achieves 87% accuracy, offering a compelling balance between performance and resource efficiency.  This makes it ideal for virtual environments, where its emotion recognition outputs are mapped to customized 3D avatar expressions.\n"}, "2108.07638": {"original_text": "  Affective Computing is the study of how computers can recognize, interpret\nand simulate human affects. Sentiment Analysis is a common task inNLP related\nto this topic, but it focuses only on emotion valence (positive, negative,\nneutral). An emerging approach in NLP is Emotion Recognition, which relies on\nfined-grained classification. This research describes an approach to create a\nlexical-based weakly supervised corpus for fine-grained emotion in Portuguese.\nWe evaluated our dataset by fine-tuning a transformer-based language model\n(BERT) and validating it on a Gold Standard annotated validation set. Our\nresults (F1-score=.64) suggest lexical-based weak supervision as an appropriate\nstrategy for initial work in low resourced environment.\n", "rewritten_text": "This research presents a novel lexical-based weakly supervised method for creating a fine-grained Portuguese emotion corpus.  Addressing the limitations of sentiment analysis, which only considers positive, negative, and neutral emotions, this approach leverages emotion recognition techniques within Natural Language Processing (NLP).  We evaluated the corpus by fine-tuning a BERT model on a gold-standard validation set, achieving an F1-score of 0.64.  These results demonstrate the effectiveness of this weakly supervised strategy for building emotion resources in low-resource languages like Portuguese.\n"}, "2408.01653": {"original_text": "  We introduce Multi-Cylindrical Panoramic Depth Estimation (MCPDepth), a\ntwo-stage framework for omnidirectional depth estimation via stereo matching\nbetween multiple cylindrical panoramas. MCPDepth uses cylindrical panoramas for\ninitial stereo matching and then fuses the resulting depth maps across views. A\ncircular attention module is employed to overcome the distortion along the\nvertical axis. MCPDepth exclusively utilizes standard network components,\nsimplifying deployment to embedded devices and outperforming previous methods\nthat require custom kernels. We theoretically and experimentally compare\nspherical and cylindrical projections for stereo matching, highlighting the\nadvantages of the cylindrical projection. MCPDepth achieves state-of-the-art\nperformance with an 18.8% reduction in mean absolute error (MAE) for depth on\nthe outdoor synthetic dataset Deep360 and a 19.9% reduction on the indoor\nreal-scene dataset 3D60.\n", "rewritten_text": "This paper presents MCPDepth, a novel two-stage omnidirectional depth estimation framework.  It leverages stereo matching between multiple cylindrical panoramas, fusing the resulting depth maps using a circular attention module to correct vertical distortion.  Unlike previous methods relying on custom kernels, MCPDepth uses standard network components, enabling easy deployment on embedded devices.  A theoretical and experimental comparison favors cylindrical over spherical projection for stereo matching.  MCPDepth achieves state-of-the-art results, reducing mean absolute error (MAE) by 18.8% on the Deep360 outdoor synthetic dataset and 19.9% on the 3D60 indoor real-scene dataset.\n"}, "2111.09091": {"original_text": "  Monitoring behaviour in smart homes using sensors can offer insights into\nchanges in the independent ability and long-term health of residents. Passive\nInfrared motion sensors (PIRs) are standard, however may not accurately track\nthe full duration of movement. They also require line-of-sight to detect motion\nwhich can restrict performance and ensures they must be visible to residents.\nChannel State Information (CSI) is a low cost, unintrusive form of radio\nsensing which can monitor movement but also offers opportunities to generate\nrich data. We have developed a novel, self-calibrating motion detection system\nwhich uses CSI data collected and processed on a stock Raspberry Pi 4. This\nsystem exploits the correlation between CSI frames, on which we perform\nvariance analysis using our algorithm to accurately measure the full period of\na resident's movement. We demonstrate the effectiveness of this approach in\nseveral real-world environments. Experiments conducted demonstrate that\nactivity start and end time can be accurately detected for motion examples of\ndifferent intensities at different locations.\n", "rewritten_text": "Smart home sensor technology offers valuable insights into residents' independence and long-term health. While common Passive Infrared (PIR) motion sensors have limitations in accuracy and require line-of-sight, our novel system leverages the low-cost, unobtrusive Channel State Information (CSI) from Wi-Fi signals.  This self-calibrating system, implemented on a standard Raspberry Pi 4, analyzes CSI data to accurately measure the duration of movement, regardless of intensity or location.  Real-world experiments demonstrate its effectiveness in precisely detecting the start and end times of various movements.\n"}, "2405.02296": {"original_text": "  Perspective distortion (PD) causes unprecedented changes in shape, size,\norientation, angles, and other spatial relationships of visual concepts in\nimages. Precisely estimating camera intrinsic and extrinsic parameters is a\nchallenging task that prevents synthesizing perspective distortion.\nNon-availability of dedicated training data poses a critical barrier to\ndeveloping robust computer vision methods. Additionally, distortion correction\nmethods make other computer vision tasks a multi-step approach and lack\nperformance. In this work, we propose mitigating perspective distortion (MPD)\nby employing a fine-grained parameter control on a specific family of M\\\"obius\ntransform to model real-world distortion without estimating camera intrinsic\nand extrinsic parameters and without the need for actual distorted data. Also,\nwe present a dedicated perspectively distorted benchmark dataset, ImageNet-PD,\nto benchmark the robustness of deep learning models against this new dataset.\nThe proposed method outperforms existing benchmarks, ImageNet-E and ImageNet-X.\nAdditionally, it significantly improves performance on ImageNet-PD while\nconsistently performing on standard data distribution. Notably, our method\nshows improved performance on three PD-affected real-world applications crowd\ncounting, fisheye image recognition, and person re-identification and one\nPD-affected challenging CV task: object detection. The source code, dataset,\nand models are available on the project webpage at\nhttps://prakashchhipa.github.io/projects/mpd.\n", "rewritten_text": "Perspective distortion (PD) drastically alters the geometry of images.  Accurately synthesizing PD is difficult due to challenges in estimating camera parameters and the lack of suitable training data. Existing distortion correction methods are inefficient and complicate other computer vision tasks.  This work introduces a novel method, MPD, to mitigate PD by using a fine-grained M\u00f6bius transform, eliminating the need for camera parameter estimation and real distorted data.  We also present ImageNet-PD, a new benchmark dataset of perspectively distorted images.  MPD outperforms existing benchmarks (ImageNet-E and ImageNet-X) and significantly improves performance on ImageNet-PD while maintaining performance on standard datasets.  Furthermore, it enhances performance on real-world applications like crowd counting, fisheye image recognition, and person re-identification, as well as object detection.  The code, dataset, and models are available at https://prakashchhipa.github.io/projects/mpd.\n"}, "1705.08479": {"original_text": "  This paper introduces a new architectural framework, known as input\nfast-forwarding, that can enhance the performance of deep networks. The main\nidea is to incorporate a parallel path that sends representations of input\nvalues forward to deeper network layers. This scheme is substantially different\nfrom \"deep supervision\" in which the loss layer is re-introduced to earlier\nlayers. The parallel path provided by fast-forwarding enhances the training\nprocess in two ways. First, it enables the individual layers to combine\nhigher-level information (from the standard processing path) with lower-level\ninformation (from the fast-forward path). Second, this new architecture reduces\nthe problem of vanishing gradients substantially because the fast-forwarding\npath provides a shorter route for gradient backpropagation. In order to\nevaluate the utility of the proposed technique, a Fast-Forward Network (FFNet),\nwith 20 convolutional layers along with parallel fast-forward paths, has been\ncreated and tested. The paper presents empirical results that demonstrate\nimproved learning capacity of FFNet due to fast-forwarding, as compared to\nGoogLeNet (with deep supervision) and CaffeNet, which are 4x and 18x larger in\nsize, respectively. All of the source code and deep learning models described\nin this paper will be made available to the entire research community\n", "rewritten_text": "This paper proposes a novel deep network architecture, \"input fast-forwarding,\" which improves performance by adding a parallel path directly forwarding input representations to deeper layers.  Unlike deep supervision, which adds loss layers to earlier layers, fast-forwarding enhances training by allowing layers to integrate both high-level (standard path) and low-level (fast-forward path) information, and by mitigating vanishing gradients through a shorter backpropagation route.  Experiments with a 20-layer convolutional Fast-Forward Network (FFNet) demonstrate superior learning capacity compared to significantly larger networks like GoogLeNet (4x larger) and CaffeNet (18x larger), which utilize deep supervision.  All code and models are publicly available.\n"}, "2002.06515": {"original_text": "  Automatic analysis of highly crowded people has attracted extensive attention\nfrom computer vision research. Previous approaches for crowd counting have\nalready achieved promising performance across various benchmarks. However, to\ndeal with the real situation, we hope the model run as fast as possible while\nkeeping accuracy. In this paper, we propose a compact convolutional neural\nnetwork for crowd counting which learns a more efficient model with a small\nnumber of parameters. With three parallel filters executing the convolutional\noperation on the input image simultaneously at the front of the network, our\nmodel could achieve nearly real-time speed and save more computing resources.\nExperiments on two benchmarks show that our proposed method not only takes a\nbalance between performance and efficiency which is more suitable for actual\nscenes but also is superior to existing light-weight models in speed.\n", "rewritten_text": "Crowd counting in dense scenes is a significant area of computer vision research, with existing methods showing promising results.  However, real-world applications demand both high accuracy and speed.  This paper presents a compact convolutional neural network for crowd counting.  Employing three parallel convolutional filters, our model achieves near real-time performance while minimizing computational resources and parameters.  Benchmark evaluations demonstrate superior speed compared to existing lightweight models, achieving a strong balance between accuracy and efficiency ideal for practical deployment.\n"}, "1711.03278": {"original_text": "  Convolution Neural Networks (CNN), known as ConvNets are widely used in many\nvisual imagery application, object classification, speech recognition. After\nthe implementation and demonstration of the deep convolution neural network in\nImagenet classification in 2012 by krizhevsky, the architecture of deep\nConvolution Neural Network is attracted many researchers. This has led to the\nmajor development in Deep learning frameworks such as Tensorflow, caffe, keras,\ntheno. Though the implementation of deep learning is quite possible by\nemploying deep learning frameworks, mathematical theory and concepts are harder\nto understand for new learners and practitioners. This article is intended to\nprovide an overview of ConvNets architecture and to explain the mathematical\ntheory behind it including activation function, loss function, feedforward and\nbackward propagation. In this article, grey scale image is taken as input\ninformation image, ReLU and Sigmoid activation function are considered for\ndeveloping the architecture and cross-entropy loss function are used for\ncomputing the difference between predicted value and actual value. The\narchitecture is developed in such a way that it can contain one convolution\nlayer, one pooling layer, and multiple dense layers\n", "rewritten_text": "Convolutional Neural Networks (CNNs or ConvNets) are prevalent in various visual applications, including image recognition and speech processing.  The 2012 ImageNet success of a deep CNN by Krizhevsky spurred significant research and development of deep learning frameworks like TensorFlow, Caffe, Keras, and Theano. While these frameworks simplify implementation, the underlying mathematical concepts can be challenging for newcomers. This article provides a concise overview of ConvNet architecture and its mathematical foundations, covering activation functions (ReLU and Sigmoid), loss functions (cross-entropy), feedforward, and backpropagation.  Using a grayscale image as input, the described architecture incorporates one convolutional layer, one pooling layer, and multiple dense layers.\n"}, "1610.07995": {"original_text": "  In this paper, the task of recognizing signs made by hearing impaired people\nat sentence level has been addressed. A novel method of extracting spatial\nfeatures to capture hand movements of a signer has been proposed. Frames of a\ngiven video of a sign are preprocessed to extract face and hand components of a\nsigner. The local centroids of the extracted components along with the global\ncentroid are exploited to extract spatial features. The concept of interval\nvalued type symbolic data has been explored to capture variations in the same\nsign made by the different signers at different instances of time. A suitable\nsymbolic similarity measure is studied to establish matching between test and\nreference signs and a simple nearest neighbour classifier is used to recognize\nan unknown sign as one among the known signs by specifying a desired level of\nthreshold. An extensive experimentation is conducted on a considerably large\ndatabase of signs created by us during the course of research work in order to\nevaluate the performance of the proposed system\n", "rewritten_text": "This paper presents a novel method for sentence-level recognition of sign language.  The approach uses spatial features extracted from preprocessed video frames to capture hand movements.  Face and hand components are identified, and their local and global centroids are used to define these features.  Interval-valued symbolic data accounts for variations in signs across different signers and instances.  A symbolic similarity measure and nearest-neighbor classifier, with a tunable threshold, are employed for sign recognition.  The system's performance is evaluated through extensive experimentation on a large, custom-created sign language database.\n"}, "1908.04501": {"original_text": "  When a deep neural network is trained on data with only image-level labeling,\nthe regions activated in each image tend to identify only a small region of the\ntarget object. We propose a method of using videos automatically harvested from\nthe web to identify a larger region of the target object by using temporal\ninformation, which is not present in the static image. The temporal variations\nin a video allow different regions of the target object to be activated. We\nobtain an activated region in each frame of a video, and then aggregate the\nregions from successive frames into a single image, using a warping technique\nbased on optical flow. The resulting localization maps cover more of the target\nobject, and can then be used as proxy ground-truth to train a segmentation\nnetwork. This simple approach outperforms existing methods under the same level\nof supervision, and even approaches relying on extra annotations. Based on\nVGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4,\nrespectively, on PASCAL VOC 2012 test images, which represents a new\nstate-of-the-art.\n", "rewritten_text": "Training deep neural networks on image-level labeled data yields object localizations limited to small regions.  We leverage readily available web videos to improve localization by exploiting temporal information absent in static images.  Analyzing video frame-by-frame, we track object regions using optical flow warping to aggregate them into a comprehensive localization map. This map, serving as pseudo-ground truth, trains a segmentation network, surpassing existing weakly-supervised methods and even some fully-supervised approaches.  Using VGG-16 and ResNet-101 backbones, we achieve state-of-the-art mean Intersection over Union (mIoU) scores of 65.0 and 67.4, respectively, on PASCAL VOC 2012.\n"}, "2203.01532": {"original_text": "  Recently, contrastive learning-based image translation methods have been\nproposed, which contrasts different spatial locations to enhance the spatial\ncorrespondence. However, the methods often ignore the diverse semantic relation\nwithin the images. To address this, here we propose a novel semantic relation\nconsistency (SRC) regularization along with the decoupled contrastive learning,\nwhich utilize the diverse semantics by focusing on the heterogeneous semantics\nbetween the image patches of a single image. To further improve the\nperformance, we present a hard negative mining by exploiting the semantic\nrelation. We verified our method for three tasks: single-modal and multi-modal\nimage translations, and GAN compression task for image translation.\nExperimental results confirmed the state-of-art performance of our method in\nall the three tasks.\n", "rewritten_text": "Recent contrastive learning methods for image translation improve spatial correspondence by comparing different image locations.  However, they neglect the rich semantic relationships within images.  This paper introduces Semantic Relation Consistency (SRC) regularization, coupled with decoupled contrastive learning, to leverage these diverse semantics by focusing on heterogeneous relationships between image patches.  Further performance gains are achieved through hard negative mining based on semantic relationships.  We demonstrate state-of-the-art results on three tasks: single-modal and multi-modal image translation, and GAN compression for image translation.\n"}, "2310.03414": {"original_text": "  Multi-document summarization is a challenging task due to its inherent\nsubjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4\namong DUC-2004 reference summaries. In this work, we aim to enhance the\nobjectivity of news summarization by focusing on the main event of a group of\nrelated news documents and presenting it coherently with sufficient context.\nOur primary objective is to succinctly report the main event, ensuring that the\nsummary remains objective and informative. To achieve this, we employ an\nextract-rewrite approach that incorporates a main-event biased\nmonotone-submodular function for content selection. This enables us to extract\nthe most crucial information related to the main event from the document\ncluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for\nrewriting the extracted content into a coherent text. The evaluation using\nobjective metrics and human evaluators confirms the effectiveness of our\napproach, as it surpasses potential baselines, demonstrating excellence in both\ncontent coverage, coherence, and informativeness.\n", "rewritten_text": "This paper addresses the subjectivity inherent in multi-document summarization, a problem evidenced by the low inter-annotator agreement (ROUGE-1 score of 0.4 in DUC-2004).  We propose a novel approach to improve the objectivity of news summarization by focusing on the main event within a set of related news articles.  Our method uses an extract-rewrite approach, employing a main-event-biased monotone-submodular function for content selection and a fine-tuned large language model (LLM) for coherent rewriting.  Evaluation using both automatic metrics and human judgment demonstrates that our approach outperforms baselines, achieving superior content coverage, coherence, and informativeness.\n"}, "1608.04314": {"original_text": "  We present a technique for weakly supervised object localization (WSOL),\nbuilding on the observation that WSOL algorithms usually work better on images\nwith bigger objects. Instead of training the object detector on the entire\ntraining set at the same time, we propose a curriculum learning strategy to\nfeed training images into the WSOL learning loop in an order from images\ncontaining bigger objects down to smaller ones. To automatically determine the\norder, we train a regressor to estimate the size of the object given the whole\nimage as input. Furthermore, we use these size estimates to further improve the\nre-localization step of WSOL by assigning weights to object proposals according\nto how close their size matches the estimated object size. We demonstrate the\neffectiveness of using size order and size weighting on the challenging PASCAL\nVOC 2007 dataset, where we achieve a significant improvement over existing\nstate-of-the-art WSOL techniques.\n", "rewritten_text": "This paper introduces a novel weakly supervised object localization (WSOL) technique leveraging the observation that WSOL performance improves with larger objects.  We employ curriculum learning, training the detector on images progressively, starting with those containing the largest objects and proceeding to smaller ones.  An image-based object size regressor automatically determines this training order.  Moreover, this size information refines the re-localization step by weighting object proposals based on their size similarity to the regressor's prediction.  Experiments on PASCAL VOC 2007 demonstrate significant performance gains over existing WSOL methods, highlighting the effectiveness of our size-ordered training and size-weighted re-localization.\n"}, "2402.00160": {"original_text": "  In this work, we introduce the Multiple Embedding Model for EHR (MEME), an\napproach that serializes multimodal EHR tabular data into text using\npseudo-notes, mimicking clinical text generation. This conversion not only\npreserves better representations of categorical data and learns contexts but\nalso enables the effective employment of pretrained foundation models for rich\nfeature representation. To address potential issues with context length, our\nframework encodes embeddings for each EHR modality separately. We demonstrate\nthe effectiveness of MEME by applying it to several decision support tasks\nwithin the Emergency Department across multiple hospital systems. Our findings\nindicate that MEME outperforms traditional machine learning, EHR-specific\nfoundation models, and general LLMs, highlighting its potential as a general\nand extendible EHR representation strategy.\n", "rewritten_text": "This paper presents MEME (Multiple Embedding Model for EHRs), a novel approach that transforms multimodal EHR tabular data into textual \"pseudo-notes\" resembling clinical narratives.  This text-based representation improves the handling of categorical data, captures contextual information, and allows leveraging powerful pretrained language models for richer feature extraction.  To mitigate context length limitations, MEME encodes each EHR modality independently.  Evaluated on diverse emergency department decision support tasks across multiple hospitals, MEME surpasses traditional machine learning methods, dedicated EHR models, and general-purpose large language models, establishing its potential as a versatile and scalable EHR representation technique.\n"}, "2305.11853": {"original_text": "  Large language models (LLMs) with in-context learning have demonstrated\nremarkable capability in the text-to-SQL task. Previous research has prompted\nLLMs with various demonstration-retrieval strategies and intermediate reasoning\nsteps to enhance the performance of LLMs. However, those works often employ\nvaried strategies when constructing the prompt text for text-to-SQL inputs,\nsuch as databases and demonstration examples. This leads to a lack of\ncomparability in both the prompt constructions and their primary contributions.\nFurthermore, selecting an effective prompt construction has emerged as a\npersistent problem for future research. To address this limitation, we\ncomprehensively investigate the impact of prompt constructions across various\nsettings and provide insights into prompt constructions for future text-to-SQL\nstudies.\n", "rewritten_text": "Large language models (LLMs) excel at text-to-SQL tasks, particularly with in-context learning.  While prior research has improved LLM performance using different prompt engineering techniques (e.g., varied demonstration-retrieval strategies and intermediate reasoning steps), inconsistent prompt construction methods hinder direct comparison of results.  This study addresses this limitation by comprehensively analyzing the impact of various prompt constructions on text-to-SQL performance, offering valuable guidelines for future research.\n"}, "1007.0621": {"original_text": "  In this paper fusion of visual and thermal images in wavelet transformed\ndomain has been presented. Here, Daubechies wavelet transform, called as D2,\ncoefficients from visual and corresponding coefficients computed in the same\nmanner from thermal images are combined to get fused coefficients. After\ndecomposition up to fifth level (Level 5) fusion of coefficients is done.\nInverse Daubechies wavelet transform of those coefficients gives us fused face\nimages. The main advantage of using wavelet transform is that it is well-suited\nto manage different image resolution and allows the image decomposition in\ndifferent kinds of coefficients, while preserving the image information. Fused\nimages thus found are passed through Principal Component Analysis (PCA) for\nreduction of dimensions and then those reduced fused images are classified\nusing a multi-layer perceptron. For experiments IRIS Thermal/Visual Face\nDatabase was used. Experimental results show that the performance of the\napproach presented here achieves maximum success rate of 100% in many cases.\n", "rewritten_text": "This paper presents a face recognition method that fuses visual and thermal images using a Daubechies (D2) wavelet transform.  Visual and thermal image coefficients, decomposed to level 5, are combined, and an inverse wavelet transform reconstructs a fused image.  Wavelet transforms effectively handle varying resolutions and preserve image information.  Principal Component Analysis (PCA) reduces the dimensionality of these fused images, which are then classified using a multilayer perceptron.  Experiments on the IRIS Thermal/Visual Face Database achieved 100% success rates in many instances.\n"}, "2210.11725": {"original_text": "  We present AROS, a one-shot learning approach that uses an explicit\nrepresentation of interactions between highly-articulated human poses and 3D\nscenes. The approach is one-shot as the method does not require re-training to\nadd new affordance instances. Furthermore, only one or a small handful of\nexamples of the target pose are needed to describe the interaction. Given a 3D\nmesh of a previously unseen scene, we can predict affordance locations that\nsupport the interactions and generate corresponding articulated 3D human bodies\naround them. We evaluate on three public datasets of scans of real environments\nwith varied degrees of noise. Via rigorous statistical analysis of crowdsourced\nevaluations, results show that our one-shot approach outperforms data-intensive\nbaselines by up to 80\\%.\n", "rewritten_text": "AROS is a novel one-shot learning method for predicting human-object interaction affordances in 3D scenes.  Unlike data-intensive baselines, AROS requires only one or a few examples of a target pose to learn new affordances, eliminating the need for retraining.  Given a new 3D scene, AROS accurately predicts affordance locations and generates corresponding articulated human poses.  Evaluated on three public datasets, AROS demonstrates a significant performance improvement\u2014up to 80%\u2014over existing methods, as confirmed by rigorous statistical analysis of crowdsourced evaluations.\n"}, "1604.06646": {"original_text": "  In this paper we introduce a new method for text detection in natural images.\nThe method comprises two contributions: First, a fast and scalable engine to\ngenerate synthetic images of text in clutter. This engine overlays synthetic\ntext to existing background images in a natural way, accounting for the local\n3D scene geometry. Second, we use the synthetic images to train a\nFully-Convolutional Regression Network (FCRN) which efficiently performs text\ndetection and bounding-box regression at all locations and multiple scales in\nan image. We discuss the relation of FCRN to the recently-introduced YOLO\ndetector, as well as other end-to-end object detection systems based on deep\nlearning. The resulting detection network significantly out performs current\nmethods for text detection in natural images, achieving an F-measure of 84.2%\non the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per\nsecond on a GPU.\n", "rewritten_text": "This paper presents a novel text detection method for natural images.  It features a fast, scalable engine for generating realistic synthetic text images, considering 3D scene geometry, and a Fully-Convolutional Regression Network (FCRN) trained on these images for efficient, multi-scale text detection and bounding box regression.  Related to YOLO and other deep learning-based object detectors, our FCRN achieves state-of-the-art performance, reaching an F-measure of 84.2% on the ICDAR 2013 benchmark and processing 15 images per second on a GPU.\n"}, "2204.06028": {"original_text": "  In this paper, we describe our submission to the Simultaneous Speech\nTranslation at IWSLT 2022. We explore strategies to utilize an offline model in\na simultaneous setting without the need to modify the original model. In our\nexperiments, we show that our onlinization algorithm is almost on par with the\noffline setting while being $3\\times$ faster than offline in terms of latency\non the test set. We also show that the onlinized offline model outperforms the\nbest IWSLT2021 simultaneous system in medium and high latency regimes and is\nalmost on par in the low latency regime. We make our system publicly available.\n", "rewritten_text": "This paper details our IWSLT 2022 simultaneous speech translation submission.  We present a novel method for adapting an offline model to simultaneous translation without architectural changes.  Our approach achieves near offline performance while being three times faster (latency-wise) on the test set.  Furthermore, our onlinized model surpasses the best IWSLT 2021 simultaneous system at medium and high latencies and performs comparably at low latency.  The system's code is publicly available.\n"}, "2304.02715": {"original_text": "  This paper presents an unsupervised approach that leverages raw aerial videos\nto learn to estimate planar homographic transformation between consecutive\nvideo frames. Previous learning-based estimators work on pairs of images to\nestimate their planar homographic transformations but suffer from severe\nover-fitting issues, especially when applying over aerial videos. To address\nthis concern, we develop a sequential estimator that directly processes a\nsequence of video frames and estimates their pairwise planar homographic\ntransformations in batches. We also incorporate a set of spatial-temporal\nknowledge to regularize the learning of such a sequence-to-sequence model. We\ncollect a set of challenging aerial videos and compare the proposed method to\nthe alternative algorithms. Empirical studies suggest that our sequential model\nachieves significant improvement over alternative image-based methods and the\nknowledge-rich regularization further boosts our system performance. Our codes\nand dataset could be found at https://github.com/Paul-LiPu/DeepVideoHomography\n", "rewritten_text": "This paper introduces a novel unsupervised method for estimating planar homographies between consecutive frames in aerial videos.  Unlike existing image-pair methods prone to overfitting, our approach uses a sequential estimator processing video frame sequences in batches.  This is further enhanced by incorporating spatio-temporal constraints to regularize the model.  Evaluated on a challenging new aerial video dataset, our method significantly outperforms existing image-based techniques, with further performance gains achieved through the knowledge-rich regularization.  The code and dataset are available at https://github.com/Paul-LiPu/DeepVideoHomography.\n"}, "2306.16925": {"original_text": "  Pretraining with large-scale 3D volumes has a potential for improving the\nsegmentation performance on a target medical image dataset where the training\nimages and annotations are limited. Due to the high cost of acquiring\npixel-level segmentation annotations on the large-scale pretraining dataset,\npretraining with unannotated images is highly desirable. In this work, we\npropose a novel self-supervised learning strategy named Volume Fusion (VF) for\npretraining 3D segmentation models. It fuses several random patches from a\nforeground sub-volume to a background sub-volume based on a predefined set of\ndiscrete fusion coefficients, and forces the model to predict the fusion\ncoefficient of each voxel, which is formulated as a self-supervised\nsegmentation task without manual annotations. Additionally, we propose a novel\nnetwork architecture based on parallel convolution and transformer blocks that\nis suitable to be transferred to different downstream segmentation tasks with\nvarious scales of organs and lesions. The proposed model was pretrained with\n110k unannotated 3D CT volumes, and experiments with different downstream\nsegmentation targets including head and neck organs, thoracic/abdominal organs\nshowed that our pretrained model largely outperformed training from scratch and\nseveral state-of-the-art self-supervised training methods and segmentation\nmodels. The code and pretrained model are available at\nhttps://github.com/openmedlab/MIS-FM.\n", "rewritten_text": "This paper introduces Volume Fusion (VF), a novel self-supervised learning method for pretraining 3D medical image segmentation models.  Addressing the scarcity of annotated data, VF leverages a large (110k volumes) dataset of unannotated 3D CT scans.  The method fuses patches from foreground and background sub-volumes, training the model to predict the fusion coefficients \u2013 effectively creating a self-supervised segmentation task.  A novel network architecture, combining parallel convolutions and transformer blocks, facilitates transfer learning to diverse downstream segmentation tasks involving organs and lesions of varying scales.  Experiments on head and neck, thoracic, and abdominal organ segmentation demonstrate significant performance improvements over training from scratch and other state-of-the-art self-supervised methods.  The code and pretrained model are publicly available at https://github.com/openmedlab/MIS-FM.\n"}, "2102.0932": {"original_text": "  Event cameras are novel vision sensors that report per-pixel brightness\nchanges as a stream of asynchronous \"events\". They offer significant advantages\ncompared to standard cameras due to their high temporal resolution, high\ndynamic range and lack of motion blur. However, events only measure the varying\ncomponent of the visual signal, which limits their ability to encode scene\ncontext. By contrast, standard cameras measure absolute intensity frames, which\ncapture a much richer representation of the scene. Both sensors are thus\ncomplementary. However, due to the asynchronous nature of events, combining\nthem with synchronous images remains challenging, especially for learning-based\nmethods. This is because traditional recurrent neural networks (RNNs) are not\ndesigned for asynchronous and irregular data from additional sensors. To\naddress this challenge, we introduce Recurrent Asynchronous Multimodal (RAM)\nnetworks, which generalize traditional RNNs to handle asynchronous and\nirregular data from multiple sensors. Inspired by traditional RNNs, RAM\nnetworks maintain a hidden state that is updated asynchronously and can be\nqueried at any time to generate a prediction. We apply this novel architecture\nto monocular depth estimation with events and frames where we show an\nimprovement over state-of-the-art methods by up to 30% in terms of mean\nabsolute depth error. To enable further research on multimodal learning with\nevents, we release EventScape, a new dataset with events, intensity frames,\nsemantic labels, and depth maps recorded in the CARLA simulator.\n", "rewritten_text": "Event cameras, which asynchronously report pixel brightness changes as event streams, offer superior temporal resolution, dynamic range, and lack of motion blur compared to traditional cameras.  However, their event-based nature, focusing only on changes, limits scene context representation compared to the richer absolute intensity data provided by standard cameras.  This complementarity presents a challenge: effectively combining asynchronous event data with synchronous image data, particularly for learning-based methods.  Traditional recurrent neural networks (RNNs) are ill-suited for this asynchronous data.  To overcome this, we introduce Recurrent Asynchronous Multimodal (RAM) networks, a generalization of RNNs capable of handling asynchronous, multi-sensor data.  RAM networks maintain an asynchronously updated hidden state, allowing for on-demand predictions.  Applied to monocular depth estimation using events and frames, RAM networks achieve up to a 30% improvement in mean absolute depth error over state-of-the-art methods.  To facilitate further research, we also release EventScape, a new dataset containing events, intensity frames, semantic labels, and depth maps simulated in CARLA.\n"}, "2410.05963": {"original_text": "  Existing perception models achieve great success by learning from large\namounts of labeled data, but they still struggle with open-world scenarios. To\nalleviate this issue, researchers introduce open-set perception tasks to detect\nor segment unseen objects in the training set. However, these models require\npredefined object categories as inputs during inference, which are not\navailable in real-world scenarios. Recently, researchers pose a new and more\npractical problem, \\textit{i.e.}, open-ended object detection, which discovers\nunseen objects without any object categories as inputs. In this paper, we\npresent VL-SAM, a training-free framework that combines the generalized object\nrecognition model (\\textit{i.e.,} Vision-Language Model) with the generalized\nobject localization model (\\textit{i.e.,} Segment-Anything Model), to address\nthe open-ended object detection and segmentation task. Without additional\ntraining, we connect these two generalized models with attention maps as the\nprompts. Specifically, we design an attention map generation module by\nemploying head aggregation and a regularized attention flow to aggregate and\npropagate attention maps across all heads and layers in VLM, yielding\nhigh-quality attention maps. Then, we iteratively sample positive and negative\npoints from the attention maps with a prompt generation module and send the\nsampled points to SAM to segment corresponding objects. Experimental results on\nthe long-tail instance segmentation dataset (LVIS) show that our method\nsurpasses the previous open-ended method on the object detection task and can\nprovide additional instance segmentation masks. Besides, VL-SAM achieves\nfavorable performance on the corner case object detection dataset (CODA),\ndemonstrating the effectiveness of VL-SAM in real-world applications. Moreover,\nVL-SAM exhibits good model generalization that can incorporate various VLMs and\nSAMs.\n", "rewritten_text": "Current perception models excel with ample labeled data but falter in open-world settings.  While open-set perception methods address this by detecting unseen objects, they require predefined categories\u2014unavailable in real-world applications.  Open-ended object detection, a more practical approach, aims to discover unseen objects without such input.  This paper introduces VL-SAM, a training-free framework combining a vision-language model (VLM) and a Segment-Anything Model (SAM).  VL-SAM uses attention maps as prompts to connect these models, generating high-quality maps via a novel head aggregation and regularized attention flow mechanism.  A prompt generation module iteratively samples points from these maps, feeding them to SAM for object segmentation.  Experiments on LVIS and CODA datasets demonstrate VL-SAM's superior performance in open-ended object detection and segmentation, surpassing existing methods and exhibiting strong generalization across different VLMs and SAMs.\n"}, "1805.05421": {"original_text": "  Deep learning has made significant improvements at many image processing\ntasks in recent years, such as image classification, object recognition and\nobject detection. Convolutional neural networks (CNN), which is a popular deep\nlearning architecture designed to process data in multiple array form, show\ngreat success to almost all detection \\& recognition problems and computer\nvision tasks. However, the number of parameters in a CNN is too high such that\nthe computers require more energy and larger memory size. In order to solve\nthis problem, we propose a novel energy efficient model Binary Weight and\nHadamard-transformed Image Network (BWHIN), which is a combination of Binary\nWeight Network (BWN) and Hadamard-transformed Image Network (HIN). It is\nobserved that energy efficiency is achieved with a slight sacrifice at\nclassification accuracy. Among all energy efficient networks, our novel\nensemble model outperforms other energy efficient models.\n", "rewritten_text": "Recent advancements in deep learning, particularly convolutional neural networks (CNNs), have significantly improved image processing tasks like classification, recognition, and detection.  However, CNNs' high parameter count demands substantial computational resources.  To address this, we introduce BWHIN, a novel energy-efficient model combining Binary Weight Networks (BWNs) and Hadamard-transformed Image Networks (HINs).  While achieving significant energy savings, BWHIN maintains competitive classification accuracy, outperforming other energy-efficient networks.\n"}, "2001.11561": {"original_text": "  We consider referring image segmentation. It is a problem at the intersection\nof computer vision and natural language understanding. Given an input image and\na referring expression in the form of a natural language sentence, the goal is\nto segment the object of interest in the image referred by the linguistic\nquery. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to\ntackle this problem. Our model consists of an encoder network and a decoder\nnetwork, where ConvLSTM is used in both encoder and decoder networks to capture\nspatial and sequential information. The encoder network extracts visual and\nlinguistic features for each word in the expression sentence, and adopts an\nattention mechanism to focus on words that are more informative in the\nmultimodal interaction. The decoder network integrates the features generated\nby the encoder network at multiple levels as its input and produces the final\nprecise segmentation mask. Experimental results on four challenging datasets\ndemonstrate that the proposed network achieves superior segmentation\nperformance compared with other state-of-the-art methods.\n", "rewritten_text": "Referring image segmentation, a task bridging computer vision and natural language processing, aims to segment an image object based on a natural language description.  We address this challenge with a novel dual convolutional LSTM (ConvLSTM) network.  This encoder-decoder architecture uses ConvLSTMs in both stages to process spatial and sequential information.  The encoder extracts visual and linguistic features, employing attention to prioritize informative words.  The decoder integrates multi-level encoder features to generate a precise segmentation mask.  Our model outperforms state-of-the-art methods on four benchmark datasets.\n"}, "1903.07224": {"original_text": "  This work develops a novel end-to-end deep unsupervised learning method based\non convolutional neural network (CNN) with pseudo-classes for remote sensing\nscene representation. First, we introduce center points as the centers of the\npseudo classes and the training samples can be allocated with pseudo labels\nbased on the center points. Therefore, the CNN model, which is used to extract\nfeatures from the scenes, can be trained supervised with the pseudo labels.\nMoreover, a pseudo-center loss is developed to decrease the variance between\nthe samples and the corresponding pseudo center point. The pseudo-center loss\nis important since it can update both the center points with the training\nsamples and the CNN model with the center points in the training process\nsimultaneously. Finally, joint learning of the pseudo-center loss and the\npseudo softmax loss which is formulated with the samples and the pseudo labels\nis developed for unsupervised remote sensing scene representation to obtain\ndiscriminative representations from the scenes. Experiments are conducted over\ntwo commonly used remote sensing scene datasets to validate the effectiveness\nof the proposed method and the experimental results show the superiority of the\nproposed method when compared with other state-of-the-art methods.\n", "rewritten_text": "This paper presents a novel unsupervised deep learning method for remote sensing scene representation.  It uses a convolutional neural network (CNN) trained with pseudo-labels generated from automatically determined center points.  These pseudo-labels allow supervised training of the CNN, while a novel pseudo-center loss function minimizes the variance between data points and their assigned centers.  This loss function simultaneously refines both the center points and the CNN weights.  Combining this pseudo-center loss with a pseudo-softmax loss enables the learning of discriminative scene representations.  Experiments on two standard remote sensing datasets demonstrate the superior performance of this method compared to existing state-of-the-art techniques.\n"}, "2203.073": {"original_text": "  Current mobile user authentication systems based on PIN codes, fingerprint,\nand face recognition have several shortcomings. Such limitations have been\naddressed in the literature by exploring the feasibility of passive\nauthentication on mobile devices through behavioral biometrics. In this line of\nresearch, this work carries out a comparative analysis of unimodal and\nmultimodal behavioral biometric traits acquired while the subjects perform\ndifferent activities on the phone such as typing, scrolling, drawing a number,\nand tapping on the screen, considering the touchscreen and the simultaneous\nbackground sensor data (accelerometer, gravity sensor, gyroscope, linear\naccelerometer, and magnetometer). Our experiments are performed over HuMIdb,\none of the largest and most comprehensive freely available mobile user\ninteraction databases to date. A separate Recurrent Neural Network (RNN) with\ntriplet loss is implemented for each single modality. Then, the weighted fusion\nof the different modalities is carried out at score level. In our experiments,\nthe most discriminative background sensor is the magnetometer, whereas among\ntouch tasks the best results are achieved with keystroke in a fixed-text\nscenario. In all cases, the fusion of modalities is very beneficial, leading to\nEqual Error Rates (EER) ranging from 4% to 9% depending on the modality\ncombination in a 3-second interval.\n", "rewritten_text": "This study addresses limitations of current mobile authentication methods (PINs, fingerprints, facial recognition) by evaluating passive behavioral biometrics.  We comparatively analyze unimodal and multimodal behavioral data from a large public dataset (HuMIdb), encompassing touchscreen interactions (typing, scrolling, number drawing, tapping) and simultaneous background sensor readings (accelerometer, gravity, gyroscope, linear accelerometer, magnetometer).  Each modality is modeled using a separate Recurrent Neural Network with triplet loss, and a weighted score-level fusion is applied.  Our results show the magnetometer as the most effective background sensor and fixed-text keystroke input as the best touchscreen task.  Modality fusion significantly improves performance, achieving Equal Error Rates (EERs) between 4% and 9% using 3-second intervals.\n"}, "2305.14722": {"original_text": "  Most contemporary supervised Remote Sensing (RS) image Change Detection (CD)\napproaches are customized for equal-resolution bitemporal images. Real-world\napplications raise the need for cross-resolution change detection, aka, CD\nbased on bitemporal images with different spatial resolutions. Given training\nsamples of a fixed bitemporal resolution difference (ratio) between the\nhigh-resolution (HR) image and the low-resolution (LR) one, current\ncross-resolution methods may fit a certain ratio but lack adaptation to other\nresolution differences. Toward continuous cross-resolution CD, we propose\nscale-invariant learning to enforce the model consistently predicting HR\nresults given synthesized samples of varying resolution differences.\nConcretely, we synthesize blurred versions of the HR image by random\ndownsampled reconstructions to reduce the gap between HR and LR images. We\nintroduce coordinate-based representations to decode per-pixel predictions by\nfeeding the coordinate query and corresponding multi-level embedding features\ninto an MLP that implicitly learns the shape of land cover changes, therefore\nbenefiting recognizing blurred objects in the LR image. Moreover, considering\nthat spatial resolution mainly affects the local textures, we apply\nlocal-window self-attention to align bitemporal features during the early\nstages of the encoder. Extensive experiments on two synthesized and one\nreal-world different-resolution CD datasets verify the effectiveness of the\nproposed method. Our method significantly outperforms several vanilla CD\nmethods and two cross-resolution CD methods on the three datasets both in\nin-distribution and out-of-distribution settings. The empirical results suggest\nthat our method could yield relatively consistent HR change predictions\nregardless of varying bitemporal resolution ratios. Our code is available at\n\\url{https://github.com/justchenhao/SILI_CD}.\n", "rewritten_text": "Current supervised remote sensing change detection methods primarily handle images with matching resolutions.  However, real-world applications demand cross-resolution change detection, where images have differing resolutions.  Existing methods struggle to adapt to varying resolution differences, performing well only for the specific ratio they were trained on.  To address this, we propose a scale-invariant learning approach for continuous cross-resolution change detection.  This involves synthesizing blurred, downsampled versions of high-resolution (HR) images to bridge the gap between HR and low-resolution (LR) images during training.  We employ coordinate-based representations and a multi-layer perceptron (MLP) to decode per-pixel predictions, implicitly learning the shape of land cover changes, even in blurred LR images.  Early encoder stages utilize local-window self-attention to align bitemporal features, mitigating the impact of differing resolutions on local textures.  Experiments on synthetic and real-world datasets demonstrate significant improvements over existing methods, showing consistent HR change predictions across various resolution ratios.  Our code is available at [https://github.com/justchenhao/SILI_CD](https://github.com/justchenhao/SILI_CD).\n"}, "1704.08763": {"original_text": "  We present GazeDirector, a new approach for eye gaze redirection that uses\nmodel-fitting. Our method first tracks the eyes by fitting a multi-part eye\nregion model to video frames using analysis-by-synthesis, thereby recovering\neye region shape, texture, pose, and gaze simultaneously. It then redirects\ngaze by 1) warping the eyelids from the original image using a model-derived\nflow field, and 2) rendering and compositing synthesized 3D eyeballs onto the\noutput image in a photorealistic manner. GazeDirector allows us to change where\npeople are looking without person-specific training data, and with full\narticulation, i.e. we can precisely specify new gaze directions in 3D.\nQuantitatively, we evaluate both model-fitting and gaze synthesis, with\nexperiments for gaze estimation and redirection on the Columbia gaze dataset.\nQualitatively, we compare GazeDirector against recent work on gaze redirection,\nshowing better results especially for large redirection angles. Finally, we\ndemonstrate gaze redirection on YouTube videos by introducing new 3D gaze\ntargets and by manipulating visual behavior.\n", "rewritten_text": "GazeDirector is a novel eye gaze redirection technique employing model-fitting.  It simultaneously tracks eye region shape, texture, pose, and gaze by fitting a multi-part model to video frames using analysis-by-synthesis. Gaze redirection is achieved by warping eyelids via a model-derived flow field and photorealistically compositing synthesized 3D eyeballs.  Unlike other methods, GazeDirector requires no person-specific training and offers precise 3D gaze control.  We quantitatively evaluate its performance on the Columbia gaze dataset, demonstrating superior gaze redirection, particularly at large angles, compared to existing techniques.  Qualitative results, including applications to YouTube videos with manipulated gaze targets and visual behavior, further highlight its capabilities.\n"}, "2110.13385": {"original_text": "  Recently, Transformer-based networks have shown great promise on\nskeleton-based action recognition tasks. The ability to capture global and\nlocal dependencies is the key to success while it also brings quadratic\ncomputation and memory cost. Another problem is that previous studies mainly\nfocus on the relationships among individual joints, which often suffers from\nthe noisy skeleton joints introduced by the noisy inputs of sensors or\ninaccurate estimations. To address the above issues, we propose a novel\nTransformer-based network (IIP-Transformer). Instead of exploiting interactions\namong individual joints, our IIP-Transformer incorporates body joints and parts\ninteractions simultaneously and thus can capture both joint-level (intra-part)\nand part-level (inter-part) dependencies efficiently and effectively. From the\ndata aspect, we introduce a part-level skeleton data encoding that\nsignificantly reduces the computational complexity and is more robust to\njoint-level skeleton noise. Besides, a new part-level data augmentation is\nproposed to improve the performance of the model. On two large-scale datasets,\nNTU-RGB+D 60 and NTU RGB+D 120, the proposed IIP-Transformer achieves\nthe-state-of-art performance with more than 8x less computational complexity\nthan DSTA-Net, which is the SOTA Transformer-based method.\n", "rewritten_text": "Transformer networks have shown significant potential for skeleton-based action recognition, but their quadratic computational cost and reliance on individual joint relationships\u2014vulnerable to noisy sensor data\u2014limit their effectiveness.  To overcome these limitations, we introduce IIP-Transformer, a novel Transformer network that simultaneously models interactions between body joints and parts. This approach efficiently captures both intra-part (joint-level) and inter-part (part-level) dependencies.  Furthermore, we introduce a part-level skeleton data encoding and augmentation strategy to reduce computational complexity and improve robustness to noisy data.  Evaluated on NTU-RGB+D 60 and 120, IIP-Transformer achieves state-of-the-art performance with over eight times less computational complexity than the previous best Transformer-based method, DSTA-Net.\n"}, "2307.01200": {"original_text": "  Learning-based approaches to monocular motion capture have recently shown\npromising results by learning to regress in a data-driven manner. However, due\nto the challenges in data collection and network designs, it remains\nchallenging for existing solutions to achieve real-time full-body capture while\nbeing accurate in world space. In this work, we introduce ProxyCap, a\nhuman-centric proxy-to-motion learning scheme to learn world-space motions from\na proxy dataset of 2D skeleton sequences and 3D rotational motions. Such proxy\ndata enables us to build a learning-based network with accurate world-space\nsupervision while also mitigating the generalization issues. For more accurate\nand physically plausible predictions in world space, our network is designed to\nlearn human motions from a human-centric perspective, which enables the\nunderstanding of the same motion captured with different camera trajectories.\nMoreover, a contact-aware neural motion descent module is proposed in our\nnetwork so that it can be aware of foot-ground contact and motion misalignment\nwith the proxy observations. With the proposed learning-based solution, we\ndemonstrate the first real-time monocular full-body capture system with\nplausible foot-ground contact in world space even using hand-held moving\ncameras. Our project page is https://zhangyux15.github.io/ProxyCapV2.\n", "rewritten_text": "Recent learning-based monocular motion capture methods show promise, but real-time, accurate, full-body, world-space capture remains elusive due to data limitations and network architecture challenges.  This paper introduces ProxyCap, a novel approach using a proxy dataset of 2D skeleton sequences and 3D rotations to train a network for accurate world-space motion capture.  This proxy data provides strong supervision and improves generalization.  A human-centric network design enhances accuracy and physical plausibility by understanding motions regardless of camera viewpoint.  Furthermore, a contact-aware module ensures realistic foot-ground contact.  ProxyCap achieves the first real-time, full-body, world-space monocular capture system with accurate ground contact, even with handheld cameras.  See our project page: https://zhangyux15.github.io/ProxyCapV2.\n"}, "1410.7484": {"original_text": "  Stochastic sampling based trackers have shown good performance for abrupt\nmotion tracking so that they have gained popularity in recent years. However,\nconventional methods tend to use a two-stage sampling paradigm, in which the\nsearch space needs to be uniformly explored with an inefficient preliminary\nsampling phase. In this paper, we propose a novel sampling-based method in the\nBayesian filtering framework to address the problem. Within the framework,\nnearest neighbor field estimation is utilized to compute the importance\nproposal probabilities, which guide the Markov chain search towards promising\nregions and thus enhance the sampling efficiency; given the motion priors, a\nsmoothing stochastic sampling Monte Carlo algorithm is proposed to approximate\nthe posterior distribution through a smoothing weight-updating scheme.\nMoreover, to track the abrupt and the smooth motions simultaneously, we develop\nan abrupt-motion detection scheme which can discover the presence of abrupt\nmotions during online tracking. Extensive experiments on challenging image\nsequences demonstrate the effectiveness and the robustness of our algorithm in\nhandling the abrupt motions.\n", "rewritten_text": "Recent years have seen the rise of stochastic sampling trackers for their effectiveness in handling abrupt motion.  However, traditional two-stage approaches suffer from inefficient uniform exploration of the search space. This paper introduces a novel Bayesian filtering method that addresses this limitation.  Our approach leverages nearest neighbor field estimation to generate importance proposal probabilities, directing the Markov chain Monte Carlo search towards more promising areas.  A smoothing stochastic sampling algorithm, incorporating motion priors and a novel weight-updating scheme, approximates the posterior distribution.  Furthermore, an integrated abrupt-motion detection scheme enables simultaneous tracking of both abrupt and smooth movements.  Extensive experiments confirm the algorithm's robustness and effectiveness in challenging scenarios involving abrupt motion.\n"}, "2010.06944": {"original_text": "  We present a formulation of the relative depth estimation from a single image\nproblem, as a ranking problem. By reformulating the problem this way, we were\nable to utilize literature on the ranking problem, and apply the existing\nknowledge to achieve better results. To this end, we have introduced a listwise\nranking loss borrowed from ranking literature, weighted ListMLE, to the\nrelative depth estimation problem. We have also brought a new metric which\nconsiders pixel depth ranking accuracy, on which our method is stronger.\n", "rewritten_text": "This paper frames single-image relative depth estimation as a ranking problem.  Leveraging existing ranking literature, we improve estimation accuracy by incorporating a weighted ListMLE loss function.  Furthermore, we introduce a novel metric focusing on pixel depth ranking accuracy, demonstrating the superiority of our approach.\n"}, "2012.02951": {"original_text": "  Visual scene understanding is the core task in making any crucial decision in\nany computer vision system. Although popular computer vision datasets like\nCityscapes, MS-COCO, PASCAL provide good benchmarks for several tasks (e.g.\nimage classification, segmentation, object detection), these datasets are\nhardly suitable for post disaster damage assessments. On the other hand,\nexisting natural disaster datasets include mainly satellite imagery which have\nlow spatial resolution and a high revisit period. Therefore, they do not have a\nscope to provide quick and efficient damage assessment tasks. Unmanned Aerial\nVehicle(UAV) can effortlessly access difficult places during any disaster and\ncollect high resolution imagery that is required for aforementioned tasks of\ncomputer vision. To address these issues we present a high resolution UAV\nimagery, FloodNet, captured after the hurricane Harvey. This dataset\ndemonstrates the post flooded damages of the affected areas. The images are\nlabeled pixel-wise for semantic segmentation task and questions are produced\nfor the task of visual question answering. FloodNet poses several challenges\nincluding detection of flooded roads and buildings and distinguishing between\nnatural water and flooded water. With the advancement of deep learning\nalgorithms, we can analyze the impact of any disaster which can make a precise\nunderstanding of the affected areas. In this paper, we compare and contrast the\nperformances of baseline methods for image classification, semantic\nsegmentation, and visual question answering on our dataset.\n", "rewritten_text": "Accurate visual scene understanding is crucial for effective computer vision systems, particularly in disaster response.  While existing datasets like Cityscapes, MS-COCO, and PASCAL benchmark tasks such as image classification and object detection, their limitations hinder post-disaster damage assessment.  Satellite imagery, commonly used in disaster datasets, suffers from low resolution and infrequent updates, preventing rapid assessments.  High-resolution imagery acquired by Unmanned Aerial Vehicles (UAVs) offers a solution, enabling access to disaster-stricken areas and providing the detail needed for timely analysis.  To address this need, we introduce FloodNet, a high-resolution UAV imagery dataset collected after Hurricane Harvey, annotated for semantic segmentation and visual question answering.  FloodNet presents unique challenges, such as differentiating flooded from natural water and identifying damaged infrastructure.  Leveraging advancements in deep learning, we analyze the performance of baseline methods on FloodNet for image classification, semantic segmentation, and visual question answering, providing insights into the potential for automated disaster impact assessment.\n"}, "1905.1062": {"original_text": "  Large-scale face recognition in-the-wild has been recently achieved matured\nperformance in many real work applications. However, such systems are built on\nGPU platforms and mostly deploy heavy deep network architectures. Given a\nhigh-performance heavy network as a teacher, this work presents a simple and\nelegant teacher-student learning paradigm, namely ShrinkTeaNet, to train a\nportable student network that has significantly fewer parameters and\ncompetitive accuracy against the teacher network. Far apart from prior\nteacher-student frameworks mainly focusing on accuracy and compression ratios\nin closed-set problems, our proposed teacher-student network is proved to be\nmore robust against open-set problem, i.e. large-scale face recognition. In\naddition, this work introduces a novel Angular Distillation Loss for distilling\nthe feature direction and the sample distributions of the teacher's hypersphere\nto its student. Then ShrinkTeaNet framework can efficiently guide the student's\nlearning process with the teacher's knowledge presented in both intermediate\nand last stages of the feature embedding. Evaluations on LFW, CFP-FP, AgeDB,\nIJB-B and IJB-C Janus, and MegaFace with one million distractors have\ndemonstrated the efficiency of the proposed approach to learn robust student\nnetworks which have satisfying accuracy and compact sizes. Our ShrinkTeaNet is\nable to support the light-weight architecture achieving high performance with\n99.77% on LFW and 95.64% on large-scale Megaface protocols.\n", "rewritten_text": "Recent advancements in large-scale, real-world face recognition leverage powerful deep networks running on GPUs.  This work introduces ShrinkTeaNet, a novel teacher-student learning paradigm that trains a significantly smaller, more portable student network while maintaining competitive accuracy. Unlike previous methods primarily focused on closed-set accuracy and compression, ShrinkTeaNet excels in open-set, large-scale face recognition.  This is achieved through a new Angular Distillation Loss, which transfers both feature direction and sample distribution information from the teacher network's hypersphere to the student.  ShrinkTeaNet leverages this knowledge at both intermediate and final feature embedding stages.  Extensive evaluations on LFW, CFP-FP, AgeDB, IJB-B, IJB-C Janus, and MegaFace (with one million distractors) demonstrate ShrinkTeaNet's effectiveness, achieving high accuracy (99.77% on LFW, 95.64% on MegaFace) with a significantly reduced model size.\n"}, "1601.0763": {"original_text": "  We propose a method that integrates two widely available data sources,\nbuilding footprints from 2D maps and street level images, to derive valuable\ninformation that is generally difficult to acquire -- building heights and\nbuilding facade masks in images. Building footprints are elevated in world\ncoordinates and projected onto images. Building heights are estimated by\nscoring projected footprints based on their alignment with building features in\nimages. Building footprints with estimated heights can be converted to simple\n3D building models, which are projected back to images to identify buildings.\nIn this procedure, accurate camera projections are critical. However, camera\nposition errors inherited from external sensors commonly exist, which adversely\naffect results. We derive a solution to precisely locate cameras on maps using\ncorrespondence between image features and building footprints. Experiments on\nreal-world datasets show the promise of our method.\n", "rewritten_text": "This paper presents a novel method for estimating building heights and generating facade masks from readily available 2D map footprints and street-level imagery.  By projecting elevated footprints onto images and scoring their alignment with observed building features, we estimate building heights and create simple 3D models.  These models are then reprojected to identify buildings in the images.  Addressing the challenge of camera position inaccuracies inherent in external sensor data, we introduce a camera localization technique leveraging correspondences between image features and building footprints.  Real-world experiments demonstrate the effectiveness of our approach.\n"}, "2112.12141": {"original_text": "  3D human pose estimation (HPE) in autonomous vehicles (AV) differs from other\nuse cases in many factors, including the 3D resolution and range of data,\nabsence of dense depth maps, failure modes for LiDAR, relative location between\nthe camera and LiDAR, and a high bar for estimation accuracy. Data collected\nfor other use cases (such as virtual reality, gaming, and animation) may\ntherefore not be usable for AV applications. This necessitates the collection\nand annotation of a large amount of 3D data for HPE in AV, which is\ntime-consuming and expensive. In this paper, we propose one of the first\napproaches to alleviate this problem in the AV setting. Specifically, we\npropose a multi-modal approach which uses 2D labels on RGB images as weak\nsupervision to perform 3D HPE. The proposed multi-modal architecture\nincorporates LiDAR and camera inputs with an auxiliary segmentation branch. On\nthe Waymo Open Dataset, our approach achieves a 22% relative improvement over\ncamera-only 2D HPE baseline, and 6% improvement over LiDAR-only model. Finally,\ncareful ablation studies and parts based analysis illustrate the advantages of\neach of our contributions.\n", "rewritten_text": "Autonomous vehicle (AV) 3D human pose estimation (HPE) presents unique challenges compared to other applications.  The lower resolution and range of AV sensor data, lack of dense depth maps, LiDAR failure modes, varying camera-LiDAR geometry, and stringent accuracy requirements render datasets from virtual reality, gaming, or animation unsuitable.  This necessitates costly and time-consuming collection and annotation of large-scale 3D AV datasets.  This paper introduces a novel multi-modal approach to address this, leveraging weakly supervised 2D RGB image labels to perform 3D HPE.  Our architecture fuses LiDAR and camera data with an auxiliary segmentation branch, achieving a 22% relative improvement over a camera-only baseline and a 6% improvement over a LiDAR-only model on the Waymo Open Dataset.  Ablation studies validate the effectiveness of our design choices.\n"}, "2306.02182": {"original_text": "  Indian court legal texts and processes are essential towards the integrity of\nthe judicial system and towards maintaining the social and political order of\nthe nation. Due to the increase in number of pending court cases, there is an\nurgent need to develop tools to automate many of the legal processes with the\nknowledge of artificial intelligence. In this paper, we employ knowledge\nextraction techniques, specially the named entity extraction of legal entities\nwithin court case judgements. We evaluate several state of the art\narchitectures in the realm of sequence labeling using models trained on a\ncurated dataset of legal texts. We observe that a Bi-LSTM model trained on\nFlair Embeddings achieves the best results, and we also publish the BIO\nformatted dataset as part of this paper.\n", "rewritten_text": "To address India's burgeoning case backlog and strengthen its judicial system, this paper explores the application of artificial intelligence to automate legal processes.  We focus on named entity recognition (NER) for legal entities within court judgments, evaluating several leading sequence labeling architectures trained on a newly curated dataset of Indian legal texts.  Our results show that a Bi-LSTM model using Flair embeddings performs best.  This dataset, formatted in BIO, is publicly released with this paper.\n"}, "2011.09563": {"original_text": "  Unsupervised domain adaptation (UDA) is widely used to transfer knowledge\nfrom a labeled source domain to an unlabeled target domain with different data\ndistribution. While extensive studies attested that deep learning models are\nvulnerable to adversarial attacks, the adversarial robustness of models in\ndomain adaptation application has largely been overlooked. This paper points\nout that the inevitable domain distribution deviation in UDA is a critical\nbarrier to model robustness on the target domain. To address the problem, we\npropose a novel Class-consistent Unsupervised Robust Domain Adaptation (CURDA)\nframework for training robust UDA models. With the introduced contrastive\nrobust training and source anchored adversarial contrastive losses, our\nproposed CURDA framework can effectively robustify UDA models by simultaneously\nminimizing the data distribution deviation and the distance between target\ndomain clean-adversarial pairs without creating classification confusion.\nExperiments on several public benchmarks show that CURDA can significantly\nimprove model robustness in the target domain with only minor cost of accuracy\non the clean samples.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) leverages labeled source data to train models on unlabeled target data with differing distributions.  However, the robustness of UDA models to adversarial attacks remains largely unexplored, despite deep learning's known vulnerability.  This paper argues that inherent domain distribution discrepancies in UDA hinder robustness.  To address this, we introduce CURDA (Class-consistent Unsupervised Robust Domain Adaptation), a novel framework employing contrastive robust training and source-anchored adversarial contrastive losses.  CURDA simultaneously reduces domain distribution divergence and the distance between clean and adversarial target samples, preventing classification errors.  Experiments on standard benchmarks demonstrate CURDA's significant improvement in target domain robustness with minimal impact on clean sample accuracy.\n"}, "1907.00184": {"original_text": "  Since Bahdanau et al. [1] first introduced attention for neural machine\ntranslation, most sequence-to-sequence models made use of attention mechanisms\n[2, 3, 4]. While they produce soft-alignment matrices that could be interpreted\nas alignment between target and source languages, we lack metrics to quantify\ntheir quality, being unclear which approach produces the best alignments. This\npaper presents an empirical evaluation of 3 main sequence-to-sequence models\n(CNN, RNN and Transformer-based) for word discovery from unsegmented phoneme\nsequences. This task consists in aligning word sequences in a source language\nwith phoneme sequences in a target language, inferring from it word\nsegmentation on the target side [5]. Evaluating word segmentation quality can\nbe seen as an extrinsic evaluation of the soft-alignment matrices produced\nduring training. Our experiments in a low-resource scenario on Mboshi and\nEnglish languages (both aligned to French) show that RNNs surprisingly\noutperform CNNs and Transformer for this task. Our results are confirmed by an\nintrinsic evaluation of alignment quality through the use of Average Normalized\nEntropy (ANE). Lastly, we improve our best word discovery model by using an\nalignment entropy confidence measure that accumulates ANE over all the\noccurrences of a given alignment pair in the collection.\n", "rewritten_text": "Attention mechanisms, introduced by Bahdanau et al. [1], are ubiquitous in sequence-to-sequence models [2, 3, 4].  However,  lacking quantitative metrics for evaluating the quality of their soft-alignment matrices hinders comparison of different approaches. This paper empirically evaluates three sequence-to-sequence architectures (CNN, RNN, and Transformer) for word discovery from unsegmented phonemes, a task requiring alignment of source language words with target language phonemes and subsequent word segmentation [5].  Word segmentation accuracy serves as an extrinsic evaluation of alignment quality.  Experiments on low-resource Mboshi and English (both aligned to French) surprisingly reveal RNNs outperform CNNs and Transformers.  This finding is corroborated by an intrinsic evaluation using Average Normalized Entropy (ANE).  Finally, we enhance our best-performing model by incorporating an alignment entropy confidence measure based on aggregated ANE across all occurrences of each alignment pair.\n"}, "1812.09877": {"original_text": "  In multimodal unsupervised image-to-image translation tasks, the goal is to\ntranslate an image from the source domain to many images in the target domain.\nWe present a simple method that produces higher quality images than current\nstate-of-the-art while maintaining the same amount of multimodal diversity.\nPrevious methods follow the unconditional approach of trying to map the latent\ncode directly to a full-size image. This leads to complicated network\narchitectures with several introduced hyperparameters to tune. By treating the\nlatent code as a modifier of the convolutional filters, we produce multimodal\noutput while maintaining the traditional Generative Adversarial Network (GAN)\nloss and without additional hyperparameters. The only tuning required by our\nmethod controls the tradeoff between variability and quality of generated\nimages. Furthermore, we achieve disentanglement between source domain content\nand target domain style for free as a by-product of our formulation. We perform\nqualitative and quantitative experiments showing the advantages of our method\ncompared with the state-of-the art on multiple benchmark image-to-image\ntranslation datasets.\n", "rewritten_text": "This paper introduces a novel, simpler method for multimodal unsupervised image-to-image translation that surpasses current state-of-the-art results in image quality while preserving diverse outputs.  Unlike existing methods that directly map latent codes to full images, resulting in complex architectures and numerous hyperparameters, our approach treats the latent code as a filter modifier within a standard Generative Adversarial Network (GAN). This eliminates the need for extra hyperparameters, requiring only tuning of a single trade-off parameter between image variability and quality.  Importantly, our method achieves content and style disentanglement as a natural consequence.  Qualitative and quantitative evaluations on multiple benchmark datasets demonstrate the superiority of our approach.\n"}, "1807.04047": {"original_text": "  Document image quality assessment (DIQA) is an important and challenging\nproblem in real applications. In order to predict the quality scores of\ndocument images, this paper proposes a novel no-reference DIQA method based on\ncharacter gradient, where the OCR accuracy is used as a ground-truth quality\nmetric. Character gradient is computed on character patches detected with the\nmaximally stable extremal regions (MSER) based method. Character patches are\nessentially significant to character recognition and therefore suitable for use\nin estimating document image quality. Experiments on a benchmark dataset show\nthat the proposed method outperforms the state-of-the-art methods in estimating\nthe quality score of document images.\n", "rewritten_text": "This paper introduces a new no-reference method for document image quality assessment (DIQA).  Leveraging character gradients calculated from character patches identified using Maximally Stable Extremal Regions (MSER), the method uses Optical Character Recognition (OCR) accuracy as its ground truth.  Because character patches are crucial for character recognition, they provide a robust basis for quality estimation.  Benchmark dataset results demonstrate superior performance compared to existing DIQA methods.\n"}, "2406.01171": {"original_text": "  The concept of persona, originally adopted in dialogue literature, has\nre-surged as a promising framework for tailoring large language models (LLMs)\nto specific context (e.g., personalized search, LLM-as-a-judge). However, the\ngrowing research on leveraging persona in LLMs is relatively disorganized and\nlacks a systematic taxonomy. To close the gap, we present a comprehensive\nsurvey to categorize the current state of the field. We identify two lines of\nresearch, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and\n(2) LLM Personalization, where LLMs take care of user personas. Additionally,\nwe introduce existing methods for LLM personality evaluation. To the best of\nour knowledge, we present the first survey for role-playing and personalization\nin LLMs under the unified view of persona. We continuously maintain a paper\ncollection to foster future endeavors:\nhttps://github.com/MiuLab/PersonaLLM-Survey\n", "rewritten_text": "This survey provides a comprehensive overview of the burgeoning field of persona-based large language model (LLM) adaptation.  While the use of personas\u2014originally from dialogue literature\u2014offers a promising approach to tailoring LLMs for specific applications (like personalized search or LLM-based judgment), current research lacks a unified framework.  We address this by categorizing existing work into two main approaches: LLM role-playing (assigning personas to LLMs) and LLM personalization (LLMs adapting to user personas).  We also review existing LLM personality evaluation methods.  This is the first survey to unify these approaches under the concept of persona, and we maintain a continuously updated paper collection at [https://github.com/MiuLab/PersonaLLM-Survey](https://github.com/MiuLab/PersonaLLM-Survey) to support future research.\n"}, "1804.02745": {"original_text": "  Dynamic contrast-enhanced (DCE) MRI is an evolving imaging technique that\nprovides a quantitative measure of pharmacokinetic (PK) parameters in body\ntissues, in which series of T1-weighted images are collected following the\nadministration of a paramagnetic contrast agent. Unfortunately, in many\napplications, conventional clinical DCE-MRI suffers from low spatiotemporal\nresolution and insufficient volume coverage. In this paper, we propose a novel\ndeep learning based approach to directly estimate the PK parameters from\nundersampled DCE-MRI data. Specifically, we design a custom loss function where\nwe incorporate a forward physical model that relates the PK parameters to\ncorrupted image-time series obtained due to subsampling in k-space. This allows\nthe network to directly exploit the knowledge of true contrast agent kinetics\nin the training phase, and hence provide more accurate restoration of PK\nparameters. Experiments on clinical brain DCE datasets demonstrate the efficacy\nof our approach in terms of fidelity of PK parameter reconstruction and\nsignificantly faster parameter inference compared to a model-based iterative\nreconstruction method.\n", "rewritten_text": "This paper introduces a novel deep learning method for faster and more accurate estimation of pharmacokinetic (PK) parameters from undersampled dynamic contrast-enhanced (DCE) MRI data.  Current clinical DCE-MRI suffers from limitations in spatial and temporal resolution, and volume coverage.  Our approach uses a custom loss function incorporating a forward physical model to directly relate PK parameters to undersampled image data. This model-informed training improves PK parameter reconstruction fidelity, significantly outperforming a conventional model-based iterative reconstruction method in experiments using clinical brain DCE datasets.\n"}, "2207.01164": {"original_text": "  Neural Radiance Field (NeRF) regresses a neural parameterized scene by\ndifferentially rendering multi-view images with ground-truth supervision.\nHowever, when interpolating novel views, NeRF often yields inconsistent and\nvisually non-smooth geometric results, which we consider as a generalization\ngap between seen and unseen views. Recent advances in convolutional neural\nnetworks have demonstrated the promise of advanced robust data augmentations,\neither random or learned, in enhancing both in-distribution and\nout-of-distribution generalization. Inspired by that, we propose Augmented NeRF\n(Aug-NeRF), which for the first time brings the power of robust data\naugmentations into regularizing the NeRF training. Particularly, our proposal\nlearns to seamlessly blend worst-case perturbations into three distinct levels\nof the NeRF pipeline with physical grounds, including (1) the input\ncoordinates, to simulate imprecise camera parameters at image capture; (2)\nintermediate features, to smoothen the intrinsic feature manifold; and (3)\npre-rendering output, to account for the potential degradation factors in the\nmulti-view image supervision. Extensive results demonstrate that Aug-NeRF\neffectively boosts NeRF performance in both novel view synthesis (up to 1.5dB\nPSNR gain) and underlying geometry reconstruction. Furthermore, thanks to the\nimplicit smooth prior injected by the triple-level augmentations, Aug-NeRF can\neven recover scenes from heavily corrupted images, a highly challenging setting\nuntackled before. Our codes are available in\nhttps://github.com/VITA-Group/Aug-NeRF.\n", "rewritten_text": "Neural Radiance Fields (NeRFs) train a neural network to represent a scene by rendering images from multiple viewpoints.  However, NeRFs often struggle to generate consistent and smooth novel views, a limitation we address.  Inspired by successful data augmentation techniques in convolutional neural networks, we introduce Augmented NeRF (Aug-NeRF).  Aug-NeRF incorporates learned data augmentations at three stages of the NeRF pipeline: input coordinates (simulating imprecise camera parameters), intermediate features (smoothing the feature manifold), and pre-rendering outputs (accounting for image degradation).  Our results show Aug-NeRF significantly improves both novel view synthesis (up to 1.5dB PSNR improvement) and geometric reconstruction.  Moreover, Aug-NeRF's implicit smoothing allows it to reconstruct scenes even from heavily corrupted images, a previously unsolved problem.  Code is available at https://github.com/VITA-Group/Aug-NeRF.\n"}, "1511.05296": {"original_text": "  In this paper, we propose a method for ranking fashion images to find the\nones which might be liked by more people. We collect two new datasets from\nimage sharing websites (Pinterest and Polyvore). We represent fashion images\nbased on attributes: semantic attributes and data-driven attributes. To learn\nsemantic attributes from limited training data, we use an algorithm on\nmulti-task convolutional neural networks to share visual knowledge among\ndifferent semantic attribute categories. To discover data-driven attributes\nunsupervisedly, we propose an algorithm to simultaneously discover visual\nclusters and learn fashion-specific feature representations. Given attributes\nas representations, we propose to learn a ranking SPN (sum product networks) to\nrank pairs of fashion images. The proposed ranking SPN can capture the\nhigh-order correlations of the attributes. We show the effectiveness of our\nmethod on our two newly collected datasets.\n", "rewritten_text": "This paper introduces a novel method for ranking fashion images based on predicted popularity.  Using two new datasets sourced from Pinterest and Polyvore, we represent images using semantic and data-driven attributes.  Semantic attributes are learned from limited data via a multi-task convolutional neural network, leveraging knowledge transfer across attribute categories.  Data-driven attributes are discovered unsupervisedly through a novel algorithm that simultaneously identifies visual clusters and learns relevant features.  Finally, a ranking Sum-Product Network (SPN) is trained on these attributes to predict pairwise image preferences, capturing high-order attribute correlations.  The method's effectiveness is demonstrated on our collected datasets.\n"}, "1901.00049": {"original_text": "  We introduce a new silhouette-based representation for modeling clothed human\nbodies using deep generative models. Our method can reconstruct a complete and\ntextured 3D model of a person wearing clothes from a single input picture.\nInspired by the visual hull algorithm, our implicit representation uses 2D\nsilhouettes and 3D joints of a body pose to describe the immense shape\ncomplexity and variations of clothed people. Given a segmented 2D silhouette of\na person and its inferred 3D joints from the input picture, we first synthesize\nconsistent silhouettes from novel view points around the subject. The\nsynthesized silhouettes which are the most consistent with the input\nsegmentation are fed into a deep visual hull algorithm for robust 3D shape\nprediction. We then infer the texture of the subject's back view using the\nfrontal image and segmentation mask as input to a conditional generative\nadversarial network. Our experiments demonstrate that our silhouette-based\nmodel is an effective representation and the appearance of the back view can be\npredicted reliably using an image-to-image translation network. While classic\nmethods based on parametric models often fail for single-view images of\nsubjects with challenging clothing, our approach can still produce successful\nresults, which are comparable to those obtained from multi-view input.\n", "rewritten_text": "This paper presents a novel silhouette-based approach for reconstructing textured 3D models of clothed humans from a single image using deep generative models.  Our method leverages 2D silhouettes and 3D joint positions, inspired by visual hull algorithms, to implicitly represent the complex shapes of clothed individuals.  From a segmented input image, we synthesize consistent silhouettes from multiple viewpoints, feeding the most consistent ones into a deep visual hull algorithm for 3D shape prediction.  The back view texture is then inferred using a conditional GAN trained on frontal images and segmentation masks.  Experiments show our method effectively reconstructs 3D models, reliably predicting back views, and outperforming parametric methods on single-view images with complex clothing, achieving results comparable to multi-view approaches.\n"}, "2308.07837": {"original_text": "  In this paper, we present a novel shape reconstruction method leveraging\ndiffusion model to generate 3D sparse point cloud for the object captured in a\nsingle RGB image. Recent methods typically leverage global embedding or local\nprojection-based features as the condition to guide the diffusion model.\nHowever, such strategies fail to consistently align the denoised point cloud\nwith the given image, leading to unstable conditioning and inferior\nperformance. In this paper, we present CCD-3DR, which exploits a novel centered\ndiffusion probabilistic model for consistent local feature conditioning. We\nconstrain the noise and sampled point cloud from the diffusion model into a\nsubspace where the point cloud center remains unchanged during the forward\ndiffusion process and reverse process. The stable point cloud center further\nserves as an anchor to align each point with its corresponding local\nprojection-based features. Extensive experiments on synthetic benchmark\nShapeNet-R2N2 demonstrate that CCD-3DR outperforms all competitors by a large\nmargin, with over 40% improvement. We also provide results on real-world\ndataset Pix3D to thoroughly demonstrate the potential of CCD-3DR in real-world\napplications. Codes will be released soon\n", "rewritten_text": "This paper introduces CCD-3DR, a novel 3D shape reconstruction method that uses a centered diffusion probabilistic model to generate sparse point clouds from single RGB images.  Unlike existing methods relying on global or local projection features for conditioning, which suffer from inconsistent alignment and instability, CCD-3DR maintains a stable point cloud center throughout the diffusion process. This center acts as an anchor, improving the alignment of points with their corresponding local features.  Extensive experiments on ShapeNet-R2N2 show CCD-3DR significantly outperforms existing methods (over 40% improvement), and results on Pix3D demonstrate its real-world applicability.  Code will be released shortly.\n"}, "2312.10610": {"original_text": "  A number of tasks have been proposed recently to facilitate easy access to\ncharts such as chart QA and summarization. The dominant paradigm to solve these\ntasks has been to fine-tune a pretrained model on the task data. However, this\napproach is not only expensive but also not generalizable to unseen tasks. On\nthe other hand, large language models (LLMs) have shown impressive\ngeneralization capabilities to unseen tasks with zero- or few-shot prompting.\nHowever, their application to chart-related tasks is not trivial as these tasks\ntypically involve considering not only the underlying data but also the visual\nfeatures in the chart image. We propose PromptChart, a multimodal few-shot\nprompting framework with LLMs for chart-related applications. By analyzing the\ntasks carefully, we have come up with a set of prompting guidelines for each\ntask to elicit the best few-shot performance from LLMs. We further propose a\nstrategy to inject visual information into the prompts. Our experiments on\nthree different chart-related information consumption tasks show that with\nproperly designed prompts LLMs can excel on the benchmarks, achieving\nstate-of-the-art.\n", "rewritten_text": "Recent efforts to simplify chart access, such as chart question answering and summarization, have relied heavily on expensive and task-specific fine-tuning of pre-trained models.  Large language models (LLMs), however, offer superior generalization through few-shot prompting.  Their application to charts is challenging due to the need to process both textual and visual information.  This paper introduces PromptChart, a multimodal few-shot prompting framework leveraging LLMs for chart-related tasks.  We develop task-specific prompting guidelines and a method for incorporating visual data, achieving state-of-the-art results on three benchmark datasets.\n"}, "1906.03657": {"original_text": "  Group convolution works well with many deep convolutional neural networks\n(CNNs) that can effectively compress the model by reducing the number of\nparameters and computational cost. Using this operation, feature maps of\ndifferent group cannot communicate, which restricts their representation\ncapability. To address this issue, in this work, we propose a novel operation\nnamed Hierarchical Group Convolution (HGC) for creating computationally\nefficient neural networks. Different from standard group convolution which\nblocks the inter-group information exchange and induces the severe performance\ndegradation, HGC can hierarchically fuse the feature maps from each group and\nleverage the inter-group information effectively. Taking advantage of the\nproposed method, we introduce a family of compact networks called HGCNets.\nCompared to networks using standard group convolution, HGCNets have a huge\nimprovement in accuracy at the same model size and complexity level. Extensive\nexperimental results on the CIFAR dataset demonstrate that HGCNets obtain\nsignificant reduction of parameters and computational cost to achieve\ncomparable performance over the prior CNN architectures designed for mobile\ndevices such as MobileNet and ShuffleNet.\n", "rewritten_text": "Hierarchical Group Convolution (HGC) is a novel operation designed to improve the efficiency of deep convolutional neural networks (CNNs).  While standard group convolution reduces parameters and computation by limiting communication between feature map groups, this often hurts performance.  HGC addresses this by hierarchically fusing feature maps from different groups, effectively leveraging inter-group information.  This leads to HGCNets, a family of compact networks that significantly outperform networks using standard group convolution at the same model size and complexity.  Experiments on CIFAR demonstrate that HGCNets achieve comparable accuracy to MobileNet and ShuffleNet, while significantly reducing parameters and computational cost, making them ideal for mobile devices.\n"}, "2304.01534": {"original_text": "  Bird's eye view (BEV) perception is becoming increasingly important in the\nfield of autonomous driving. It uses multi-view camera data to learn a\ntransformer model that directly projects the perception of the road environment\nonto the BEV perspective. However, training a transformer model often requires\na large amount of data, and as camera data for road traffic are often private,\nthey are typically not shared. Federated learning offers a solution that\nenables clients to collaborate and train models without exchanging data but\nmodel parameters. In this paper, we introduce FedBEVT, a federated transformer\nlearning approach for BEV perception. In order to address two common data\nheterogeneity issues in FedBEVT: (i) diverse sensor poses, and (ii) varying\nsensor numbers in perception systems, we propose two approaches -- Federated\nLearning with Camera-Attentive Personalization (FedCaP) and Adaptive\nMulti-Camera Masking (AMCM), respectively. To evaluate our method in real-world\nsettings, we create a dataset consisting of four typical federated use cases.\nOur findings suggest that FedBEVT outperforms the baseline approaches in all\nfour use cases, demonstrating the potential of our approach for improving BEV\nperception in autonomous driving.\n", "rewritten_text": "Autonomous driving increasingly relies on Bird's Eye View (BEV) perception, achieved by training transformer models on multi-view camera data.  However, the required large datasets are often unavailable due to privacy concerns.  This paper introduces FedBEVT, a federated learning approach addressing this limitation.  To mitigate data heterogeneity (varying sensor poses and numbers), we propose FedCaP (Federated Learning with Camera-Attentive Personalization) and AMCM (Adaptive Multi-Camera Masking).  Evaluated on a new four-scenario dataset representing real-world federated learning use cases, FedBEVT significantly outperforms baselines, showcasing its potential for enhancing BEV perception in autonomous vehicles.\n"}, "1911.10248": {"original_text": "  The rapid development of inexpensive commodity depth sensors has made\nkeypoint detection and matching in the depth image modality an important\nproblem in computer vision. Despite great improvements in recent RGB local\nfeature learning methods, adapting them directly in the depth modality leads to\nunsatisfactory performance. Most of these methods do not explicitly reason\nbeyond the visible pixels in the images. To address the limitations of these\nmethods, we propose a framework ViewSynth, to jointly learn: (1) viewpoint\ninvariant keypoint-descriptor from depth images using a proposed Contrastive\nMatching Loss, and (2) view synthesis of depth images from different viewpoints\nusing the proposed View Synthesis Module and View Synthesis Loss. By learning\nview synthesis, we explicitly encourage the feature extractor to encode\ninformation about not only the visible, but also the occluded parts of the\nscene. We demonstrate that in the depth modality, ViewSynth outperforms the\nstate-of-the-art depth and RGB local feature extraction techniques in the 3D\nkeypoint matching and camera localization tasks on the RGB-D datasets 7-Scenes,\nTUM RGBD and CoRBS in most scenarios. We also show the generalizability of\nViewSynth in 3D keypoint matching across different datasets.\n", "rewritten_text": "Affordable depth sensors have fueled interest in depth-image-based keypoint detection and matching.  While RGB image feature learning has advanced significantly, directly applying these methods to depth images yields poor results, largely due to their inability to consider occluded scene information.  To overcome this, we introduce ViewSynth, a framework that jointly learns viewpoint-invariant keypoint descriptors (using a novel contrastive matching loss) and depth image view synthesis (using a novel view synthesis module and loss).  View synthesis explicitly forces the feature extractor to represent both visible and occluded scene parts.  Experiments on 7-Scenes, TUM RGBD, and CoRBS RGB-D datasets demonstrate ViewSynth's superior performance in 3D keypoint matching and camera localization compared to state-of-the-art depth and RGB methods, and its strong generalizability across datasets.\n"}, "2211.10018": {"original_text": "  Relation extraction has the potential for large-scale knowledge graph\nconstruction, but current methods do not consider the qualifier attributes for\neach relation triplet, such as time, quantity or location. The qualifiers form\nhyper-relational facts which better capture the rich and complex knowledge\ngraph structure. For example, the relation triplet (Leonard Parker, Educated\nAt, Harvard University) can be factually enriched by including the qualifier\n(End Time, 1967). Hence, we propose the task of hyper-relational extraction to\nextract more specific and complete facts from text. To support the task, we\nconstruct HyperRED, a large-scale and general-purpose dataset. Existing models\ncannot perform hyper-relational extraction as it requires a model to consider\nthe interaction between three entities. Hence, we propose CubeRE, a\ncube-filling model inspired by table-filling approaches and explicitly\nconsiders the interaction between relation triplets and qualifiers. To improve\nmodel scalability and reduce negative class imbalance, we further propose a\ncube-pruning method. Our experiments show that CubeRE outperforms strong\nbaselines and reveal possible directions for future research. Our code and data\nare available at github.com/declare-lab/HyperRED.\n", "rewritten_text": "Current relation extraction methods for knowledge graph construction overlook crucial qualifier attributes (e.g., time, quantity, location) that enrich relation triplets, forming hyper-relational facts.  To address this, we introduce the task of hyper-relational extraction and a large-scale dataset, HyperRED.  We propose CubeRE, a novel cube-filling model that explicitly handles the interaction between entities and qualifiers, improving upon existing models which struggle with this three-entity interaction.  To enhance scalability and mitigate class imbalance, we incorporate a cube-pruning method.  Our experiments demonstrate CubeRE's superior performance and suggest avenues for future work.  The code and data are available at github.com/declare-lab/HyperRED.\n"}, "2210.06246": {"original_text": "  Recently, the community has achieved substantial progress on many commonsense\nreasoning benchmarks. However, it is still unclear what is learned from the\ntraining process: the knowledge, inference capability, or both? We argue that\ndue to the large scale of commonsense knowledge, it is infeasible to annotate a\nlarge enough training set for each task to cover all commonsense for learning.\nThus we should separate the commonsense knowledge acquisition and inference\nover commonsense knowledge as two separate tasks. In this work, we focus on\ninvestigating models' commonsense inference capabilities from two perspectives:\n(1) Whether models can know if the knowledge they have is enough to solve the\ntask; (2) Whether models can develop commonsense inference capabilities that\ngeneralize across commonsense tasks. We first align commonsense tasks with\nrelevant knowledge from commonsense knowledge bases and ask humans to annotate\nwhether the knowledge is enough or not. Then, we convert different commonsense\ntasks into a unified question answering format to evaluate models'\ngeneralization capabilities. We name the benchmark as Commonsense Inference\nwith Knowledge-in-the-loop Question Answering (CIKQA).\n", "rewritten_text": "Significant advancements have been made in commonsense reasoning benchmarks, yet the nature of what's learned\u2014knowledge itself, inferential ability, or both\u2014remains unclear.  The sheer scale of commonsense knowledge makes comprehensive annotation for training infeasible.  Therefore, we propose separating knowledge acquisition and inference as distinct tasks. This work investigates models' commonsense inference capabilities by examining: (1) their awareness of sufficient knowledge for task completion; and (2) their ability to generalize across diverse commonsense tasks.  We achieve this by aligning tasks with knowledge from commonsense knowledge bases, obtaining human annotations on knowledge sufficiency, and unifying tasks into a question-answering format for generalization evaluation.  This new benchmark is termed Commonsense Inference with Knowledge-in-the-loop Question Answering (CIKQA).\n"}, "2409.11819": {"original_text": "  6D object pose estimation is the problem of identifying the position and\norientation of an object relative to a chosen coordinate system, which is a\ncore technology for modern XR applications. State-of-the-art 6D object pose\nestimators directly predict an object pose given an object observation. Due to\nthe ill-posed nature of the pose estimation problem, where multiple different\nposes can correspond to a single observation, generating additional plausible\nestimates per observation can be valuable. To address this, we reformulate the\nstate-of-the-art algorithm GDRNPP and introduce EPRO-GDR (End-to-End\nProbabilistic Geometry-Guided Regression). Instead of predicting a single pose\nper detection, we estimate a probability density distribution of the pose.\nUsing the evaluation procedure defined by the BOP (Benchmark for 6D Object Pose\nEstimation) Challenge, we test our approach on four of its core datasets and\ndemonstrate superior quantitative results for EPRO-GDR on LM-O, YCB-V, and\nITODD. Our probabilistic solution shows that predicting a pose distribution\ninstead of a single pose can improve state-of-the-art single-view pose\nestimation while providing the additional benefit of being able to sample\nmultiple meaningful pose candidates.\n", "rewritten_text": "Accurate 6D object pose estimation\u2014pinpointing an object's position and orientation\u2014is crucial for modern extended reality (XR) applications.  Current leading methods directly predict a single pose from an image. However, because multiple poses can look identical in an image,  we improve upon the state-of-the-art GDRNPP algorithm by introducing EPRO-GDR, which predicts a probability distribution of possible poses instead of a single estimate.  Evaluated using the BOP Challenge benchmark on four datasets, EPRO-GDR outperforms existing methods on LM-O, YCB-V, and ITODD.  Our probabilistic approach demonstrates that estimating a pose distribution, rather than a single pose, significantly enhances single-view 6D pose estimation accuracy and provides multiple plausible pose candidates.\n"}, "2405.18111": {"original_text": "  Large language models (LLMs) are proven to benefit a lot from\nretrieval-augmented generation (RAG) in alleviating hallucinations confronted\nwith knowledge-intensive questions. RAG adopts information retrieval techniques\nto inject external knowledge from semantic-relevant documents as input\ncontexts. However, since today's Internet is flooded with numerous noisy and\nfabricating content, it is inevitable that RAG systems are vulnerable to these\nnoises and prone to respond incorrectly. To this end, we propose to optimize\nthe retrieval-augmented Generator with an Adversarial Tuning Multi-agent system\n(ATM). The ATM steers the Generator to have a robust perspective of useful\ndocuments for question answering with the help of an auxiliary Attacker agent\nthrough adversarially tuning the agents for several iterations. After rounds of\nmulti-agent iterative tuning, the Generator can eventually better discriminate\nuseful documents amongst fabrications. The experimental results verify the\neffectiveness of ATM and we also observe that the Generator can achieve better\nperformance compared to the state-of-the-art baselines.\n", "rewritten_text": "Retrieval-augmented generation (RAG) significantly improves large language models (LLMs) by addressing hallucinations on knowledge-intensive questions through external knowledge retrieval.  However, the prevalence of unreliable online information makes RAG vulnerable to inaccuracies.  To mitigate this, we introduce an Adversarial Tuning Multi-agent system (ATM).  ATM uses an adversarial attacker agent to robustly train the RAG generator, iteratively improving its ability to distinguish credible from fabricated information.  Experimental results demonstrate ATM's effectiveness, surpassing state-of-the-art baselines.\n"}, "2309.00468": {"original_text": "  Dietary assessment is essential to maintaining a healthy lifestyle. Automatic\nimage-based dietary assessment is a growing field of research due to the\nincreasing prevalence of image capturing devices (e.g. mobile phones). In this\nwork, we estimate food energy from a single monocular image, a difficult task\ndue to the limited hard-to-extract amount of energy information present in an\nimage. To do so, we employ an improved encoder-decoder framework for energy\nestimation; the encoder transforms the image into a representation embedded\nwith food energy information in an easier-to-extract format, which the decoder\nthen extracts the energy information from. To implement our method, we compile\na high-quality food image dataset verified by registered dietitians containing\neating scene images, food-item segmentation masks, and ground truth calorie\nvalues. Our method improves upon previous caloric estimation methods by over\n10\\% and 30 kCal in terms of MAPE and MAE respectively.\n", "rewritten_text": "Accurate dietary assessment is crucial for health.  Leveraging the ubiquity of image-capturing devices, we developed a novel method for estimating food energy from single images \u2013 a challenging task due to the limited visual cues.  Our approach uses an improved encoder-decoder architecture to transform image data into a format that facilitates energy extraction.  This method was trained on a high-quality dataset of eating scenes, annotated by registered dietitians, including food segmentations and calorie labels.  Our results demonstrate a significant improvement over existing methods, achieving over 10% and 30 kcal reductions in MAPE and MAE, respectively.\n"}, "2411.08196": {"original_text": "  Diffusion Transformers (DiTs) have recently achieved remarkable success in\ntext-guided image generation. In image editing, DiTs project text and image\ninputs to a joint latent space, from which they decode and synthesize new\nimages. However, it remains largely unexplored how multimodal information\ncollectively forms this joint space and how they guide the semantics of the\nsynthesized images. In this paper, we investigate the latent space of DiT\nmodels and uncover two key properties: First, DiT's latent space is inherently\nsemantically disentangled, where different semantic attributes can be\ncontrolled by specific editing directions. Second, consistent semantic editing\nrequires utilizing the entire joint latent space, as neither encoded image nor\ntext alone contains enough semantic information. We show that these editing\ndirections can be obtained directly from text prompts, enabling precise\nsemantic control without additional training or mask annotations. Based on\nthese insights, we propose a simple yet effective Encode-Identify-Manipulate\n(EIM) framework for zero-shot fine-grained image editing. Specifically, we\nfirst encode both the given source image and the text prompt that describes the\nimage, to obtain the joint latent embedding. Then, using our proposed Hessian\nScore Distillation Sampling (HSDS) method, we identify editing directions that\ncontrol specific target attributes while preserving other image features. These\ndirections are guided by text prompts and used to manipulate the latent\nembeddings. Moreover, we propose a new metric to quantify the disentanglement\ndegree of the latent space of diffusion models. Extensive experiment results on\nour new curated benchmark dataset and analysis demonstrate DiT's\ndisentanglement properties and effectiveness of the EIM framework.\n", "rewritten_text": "Diffusion Transformers (DiTs) excel at text-guided image generation and editing, but their latent space's role in multimodal information processing remains unclear.  This paper analyzes DiT's latent space, revealing two key properties: inherent semantic disentanglement, allowing control of specific attributes via distinct editing directions; and the necessity of the full joint (image and text) latent space for consistent editing, as neither modality alone provides sufficient semantic information.  We leverage these findings to develop EIM (Encode-Identify-Manipulate), a zero-shot, fine-grained image editing framework.  EIM encodes both image and text prompt, then uses our novel Hessian Score Distillation Sampling (HSDS) method to identify text-guided editing directions that precisely control target attributes while preserving others.  Furthermore, we introduce a new metric for quantifying latent space disentanglement.  Experiments on a new benchmark dataset validate DiT's disentanglement and EIM's effectiveness.\n"}, "2403.06444": {"original_text": "  Estimating reliable geometric model parameters from the data with severe\noutliers is a fundamental and important task in computer vision. This paper\nattempts to sample high-quality subsets and select model instances to estimate\nparameters in the multi-structural data. To address this, we propose an\neffective method called Latent Semantic Consensus (LSC). The principle of LSC\nis to preserve the latent semantic consensus in both data points and model\nhypotheses. Specifically, LSC formulates the model fitting problem into two\nlatent semantic spaces based on data points and model hypotheses, respectively.\nThen, LSC explores the distributions of points in the two latent semantic\nspaces, to remove outliers, generate high-quality model hypotheses, and\neffectively estimate model instances. Finally, LSC is able to provide\nconsistent and reliable solutions within only a few milliseconds for general\nmulti-structural model fitting, due to its deterministic fitting nature and\nefficiency. Compared with several state-of-the-art model fitting methods, our\nLSC achieves significant superiority for the performance of both accuracy and\nspeed on synthetic data and real images. The code will be available at\nhttps://github.com/guobaoxiao/LSC.\n", "rewritten_text": "Robustly estimating geometric model parameters from outlier-ridden data is crucial in computer vision.  This paper introduces Latent Semantic Consensus (LSC), a novel method for fitting models to multi-structural data. LSC identifies high-quality data subsets and model instances by preserving latent semantic consistency across both data points and model hypotheses within two separate latent semantic spaces. This approach effectively removes outliers, generates superior model hypotheses, and yields accurate parameter estimates.  LSC's deterministic nature ensures fast, consistent results (within milliseconds), significantly outperforming state-of-the-art methods in both accuracy and speed on synthetic and real-image datasets.  The code is available at https://github.com/guobaoxiao/LSC.\n"}, "1708.0923": {"original_text": "  Named Entity Recognition and Disambiguation (NERD) systems have recently been\nwidely researched to deal with the significant growth of the Web. NERD systems\nare crucial for several Natural Language Processing (NLP) tasks such as\nsummarization, understanding, and machine translation. However, there is no\nstandard interface specification, i.e. these systems may vary significantly\neither for exporting their outputs or for processing the inputs. Thus, when a\ngiven company desires to implement more than one NERD system, the process is\nquite exhaustive and prone to failure. In addition, industrial solutions demand\ncritical requirements, e.g., large-scale processing, completeness, versatility,\nand licenses. Commonly, these requirements impose a limitation, making good\nNERD models to be ignored by companies. This paper presents TANKER, a\ndistributed architecture which aims to overcome scalability, reliability and\nfailure tolerance limitations related to industrial needs by combining NERD\nsystems. To this end, TANKER relies on a micro-services oriented architecture,\nwhich enables agile development and delivery of complex enterprise\napplications. In addition, TANKER provides a standardized API which makes\npossible to combine several NERD systems at once.\n", "rewritten_text": "The proliferation of web data has fueled extensive research into Named Entity Recognition and Disambiguation (NERD) systems, crucial for NLP tasks like summarization and machine translation.  However, the lack of a standardized interface hinders the integration of multiple NERD systems, creating significant challenges for companies.  Industrial applications demand scalability, completeness, and versatility, often outweighing the benefits of even the best NERD models due to licensing and implementation complexities.  This paper introduces TANKER, a distributed, microservices-based architecture designed to address these limitations.  TANKER's standardized API allows for the seamless combination of multiple NERD systems, improving scalability, reliability, and fault tolerance to meet industrial needs.\n"}, "1708.01846": {"original_text": "  Low-rank decomposition (LRD) is a state-of-the-art method for visual data\nreconstruction and modelling. However, it is a very challenging problem when\nthe image data contains significant occlusion, noise, illumination variation,\nand misalignment from rotation or viewpoint changes. We leverage the specific\nstructure of data in order to improve the performance of LRD when the data are\nnot ideal. To this end, we propose a new framework that embeds manifold priors\ninto LRD. To implement the framework, we design an alternating direction method\nof multipliers (ADMM) method which efficiently integrates the manifold\nconstraints during the optimization process. The proposed approach is\nsuccessfully used to calculate low-rank models from face images, hand-written\ndigits and planar surface images. The results show a consistent increase of\nperformance when compared to the state-of-the-art over a wide range of\nrealistic image misalignments and corruptions.\n", "rewritten_text": "Low-rank decomposition (LRD) excels at visual data reconstruction and modeling, but struggles with real-world challenges like occlusion, noise, varying illumination, and misalignment.  This paper introduces a novel LRD framework that incorporates manifold priors to improve performance under these non-ideal conditions.  An efficient alternating direction method of multipliers (ADMM) algorithm integrates these manifold constraints.  Experiments on face images, handwritten digits, and planar surfaces demonstrate superior performance compared to existing state-of-the-art methods, particularly in the presence of realistic image corruption and misalignment.\n"}, "2001.0732": {"original_text": "  It's natural these days for people to know the local events from massive\ndocuments. Many texts contain location information, such as city name or road\nname, which is always incomplete or latent. It's significant to extract the\nadministrative area of the text and organize the hierarchy of area, called\nlocation normalization. Existing detecting location systems either exclude\nhierarchical normalization or present only a few specific regions. We propose a\nsystem named ROIBase that normalizes the text by the Chinese hierarchical\nadministrative divisions. ROIBase adopts a co-occurrence constraint as the\nbasic framework to score the hit of the administrative area, achieves the\ninference by special embeddings, and expands the recall by the ROI (region of\ninterest). It has high efficiency and interpretability because it mainly\nestablishes on the definite knowledge and has less complex logic than the\nsupervised models. We demonstrate that ROIBase achieves better performance\nagainst feasible solutions and is useful as a strong support system for\nlocation normalization.\n", "rewritten_text": "Extracting and organizing location information from text is crucial for understanding local events, but existing systems often lack comprehensive hierarchical normalization or are limited geographically.  This paper introduces ROIBase, a novel system for normalizing location information within Chinese administrative divisions.  ROIBase leverages co-occurrence constraints and specialized embeddings to efficiently and interpretably infer administrative areas, enhancing recall through a region-of-interest (ROI) approach.  Unlike complex supervised models, ROIBase relies on established knowledge, resulting in high efficiency and interpretability.  Our results demonstrate ROIBase's superior performance compared to existing methods, establishing it as a valuable tool for location normalization.\n"}, "1908.02505": {"original_text": "  This paper presents a scoring system that has shown the top result on the\ntext subset of CALL v3 shared task. The presented system is based on text\nembeddings, namely NNLM~\\cite{nnlm} and BERT~\\cite{Bert}. The distinguishing\nfeature of the given approach is that it does not rely on the reference grammar\nfile for scoring. The model is compared against approaches that use the grammar\nfile and proves the possibility to achieve similar and even higher results\nwithout a predefined set of correct answers.\n  The paper describes the model itself and the data preparation process that\nplayed a crucial role in the model training.\n", "rewritten_text": "This paper introduces a novel scoring system that achieved state-of-the-art performance on the text subset of the CALL v3 shared task.  Unlike competing systems relying on reference grammars, our approach leverages text embeddings (NNLM and BERT) to achieve comparable or superior results without predefined correct answers.  The paper details both the model architecture and the crucial data preparation process used in its training.\n"}, "1604.0801": {"original_text": "  The purpose of this paper is the detection of salient areas in natural video\nby using the new deep learning techniques. Salient patches in video frames are\npredicted first. Then the predicted visual fixation maps are built upon them.\nWe design the deep architecture on the basis of CaffeNet implemented with Caffe\ntoolkit. We show that changing the way of data selection for optimisation of\nnetwork parameters, we can save computation cost up to 12 times. We extend deep\nlearning approaches for saliency prediction in still images with RGB values to\nspecificity of video using the sensitivity of the human visual system to\nresidual motion. Furthermore, we complete primary colour pixel values by\ncontrast features proposed in classical visual attention prediction models. The\nexperiments are conducted on two publicly available datasets. The first is\nIRCCYN video database containing 31 videos with an overall amount of 7300\nframes and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided\n2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, the\naccuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction of\nsaliency of patches show the improvement up to 2% with regard to RGB use only.\nThe resulting accuracy of 76, 6% is obtained. The AUC metric in comparison of\npredicted saliency maps with visual fixation maps shows the increase up to 16%\non a sample of video clips from this dataset.\n", "rewritten_text": "This paper presents a novel deep learning approach for detecting salient areas in natural videos.  Leveraging a CaffeNet architecture, we predict salient patches in individual frames to construct visual fixation maps.  By optimizing data selection during network training, we achieve a 12x reduction in computational cost.  Our method extends existing still-image saliency prediction techniques by incorporating human visual system sensitivity to residual motion and augmenting RGB data with contrast features from classical visual attention models.  Evaluated on the IRCCYN (31 videos, 7300 frames, 37 subjects) and HOLLYWOOD2 (2517 clips, 19 subjects) datasets, our approach achieves 89.51% accuracy on IRCCYN and a 2% improvement over RGB-only methods on HOLLYWOOD2, reaching 76.6% accuracy.  Furthermore, AUC scores show up to a 16% improvement in predicting saliency maps compared to ground truth fixation maps on a subset of HOLLYWOOD2.\n"}, "2301.03182": {"original_text": "  Existing deep learning-based shadow removal methods still produce images with\nshadow remnants. These shadow remnants typically exist in homogeneous regions\nwith low-intensity values, making them untraceable in the existing\nimage-to-image mapping paradigm. We observe that shadows mainly degrade images\nat the image-structure level (in which humans perceive object shapes and\ncontinuous colors). Hence, in this paper, we propose to remove shadows at the\nimage structure level. Based on this idea, we propose a novel\nstructure-informed shadow removal network (StructNet) to leverage the\nimage-structure information to address the shadow remnant problem.\nSpecifically, StructNet first reconstructs the structure information of the\ninput image without shadows and then uses the restored shadow-free structure\nprior to guiding the image-level shadow removal. StructNet contains two main\nnovel modules: (1) a mask-guided shadow-free extraction (MSFE) module to\nextract image structural features in a non-shadow-to-shadow directional manner,\nand (2) a multi-scale feature & residual aggregation (MFRA) module to leverage\nthe shadow-free structure information to regularize feature consistency. In\naddition, we also propose to extend StructNet to exploit multi-level structure\ninformation (MStructNet), to further boost the shadow removal performance with\nminimum computational overheads. Extensive experiments on three shadow removal\nbenchmarks demonstrate that our method outperforms existing shadow removal\nmethods, and our StructNet can be integrated with existing methods to improve\nthem further.\n", "rewritten_text": "Current deep learning methods for shadow removal leave behind residual shadows, particularly in dark, uniform areas, hindering existing image-to-image translation techniques.  These shadows primarily affect image structure\u2014how we perceive shapes and colors.  This paper introduces StructNet, a novel network that tackles shadow removal by focusing on image structure.  StructNet first reconstructs a shadow-free image structure, then uses this as a guide for shadow removal.  Key components include a mask-guided shadow-free extraction (MSFE) module for directional feature extraction and a multi-scale feature & residual aggregation (MFRA) module for consistent feature regularization.  Furthermore, we extend this to MStructNet, leveraging multi-level structure information for improved performance with minimal added computation.  Extensive benchmarking shows StructNet surpasses existing methods and can enhance their performance when integrated.\n"}, "1709.0586": {"original_text": "  We present a novel method for cell segmentation in microscopy images which is\ninspired by the Generative Adversarial Neural Network (GAN) approach. Our\nframework is built on a pair of two competitive artificial neural networks,\nwith a unique architecture, termed Rib Cage, which are trained simultaneously\nand together define a min-max game resulting in an accurate segmentation of a\ngiven image. Our approach has two main strengths, similar to the GAN, the\nmethod does not require a formulation of a loss function for the optimization\nprocess. This allows training on a limited amount of annotated data in a weakly\nsupervised manner. Promising segmentation results on real fluorescent\nmicroscopy data are presented. The code is freely available at:\nhttps://github.com/arbellea/DeepCellSeg.git\n", "rewritten_text": "This paper introduces Rib Cage, a novel GAN-inspired method for cell segmentation in microscopy images.  Our unique architecture uses two competing neural networks trained simultaneously in a min-max game to achieve accurate segmentation without requiring a manually defined loss function. This allows for weakly supervised training with limited annotated data.  We demonstrate promising results on real fluorescent microscopy data.  The code is publicly available at: https://github.com/arbellea/DeepCellSeg.git\n"}, "2203.14565": {"original_text": "  Domain adaptation (DA) or domain generalization (DG) for face presentation\nattack detection (PAD) has attracted attention recently with its robustness\nagainst unseen attack scenarios. Existing DA/DG-based PAD methods, however,\nhave not yet fully explored the domain-specific style information that can\nprovide knowledge regarding attack styles (e.g., materials, background,\nillumination and resolution). In this paper, we introduce a novel Style-Guided\nDomain Adaptation (SGDA) framework for inference-time adaptive PAD.\nSpecifically, Style-Selective Normalization (SSN) is proposed to explore the\ndomain-specific style information within the high-order feature statistics. The\nproposed SSN enables the adaptation of the model to the target domain by\nreducing the style difference between the target and the source domains.\nMoreover, we carefully design Style-Aware Meta-Learning (SAML) to boost the\nadaptation ability, which simulates the inference-time adaptation with style\nselection process on virtual test domain. In contrast to previous domain\nadaptation approaches, our method does not require either additional auxiliary\nmodels (e.g., domain adaptors) or the unlabeled target domain during training,\nwhich makes our method more practical to PAD task. To verify our experiments,\nwe utilize the public datasets: MSU-MFSD, CASIA-FASD, OULU-NPU and Idiap\nREPLAYATTACK. In most assessments, the result demonstrates a notable gap of\nperformance compared to the conventional DA/DG-based PAD methods.\n", "rewritten_text": "Recent research focuses on domain adaptation (DA) and domain generalization (DG) for robust face presentation attack detection (PAD) against unseen attacks.  However, existing DA/DG methods underutilize domain-specific style information (materials, background, illumination, resolution) crucial for understanding attack styles.  This paper proposes a novel Style-Guided Domain Adaptation (SGDA) framework for inference-time adaptive PAD.  SGDA leverages Style-Selective Normalization (SSN) to exploit high-order feature statistics and reduce style differences between source and target domains.  Furthermore, Style-Aware Meta-Learning (SAML) enhances adaptation by simulating inference-time adaptation with style selection on a virtual test domain.  Unlike previous methods, SGDA requires neither auxiliary models nor unlabeled target data during training, improving practicality.  Experiments on MSU-MFSD, CASIA-FASD, OULU-NPU, and Idiap REPLAYATTACK datasets demonstrate significantly improved performance compared to conventional DA/DG-based PAD methods.\n"}, "1905.08511": {"original_text": "  Question answering (QA) using textual sources for purposes such as reading\ncomprehension (RC) has attracted much attention. This study focuses on the task\nof explainable multi-hop QA, which requires the system to return the answer\nwith evidence sentences by reasoning and gathering disjoint pieces of the\nreference texts. It proposes the Query Focused Extractor (QFE) model for\nevidence extraction and uses multi-task learning with the QA model. QFE is\ninspired by extractive summarization models; compared with the existing method,\nwhich extracts each evidence sentence independently, it sequentially extracts\nevidence sentences by using an RNN with an attention mechanism on the question\nsentence. It enables QFE to consider the dependency among the evidence\nsentences and cover important information in the question sentence.\nExperimental results show that QFE with a simple RC baseline model achieves a\nstate-of-the-art evidence extraction score on HotpotQA. Although designed for\nRC, it also achieves a state-of-the-art evidence extraction score on FEVER,\nwhich is a recognizing textual entailment task on a large textual database.\n", "rewritten_text": "Explainable multi-hop question answering (QA), requiring systems to justify answers with evidence from disparate text sources, is a growing research area.  This study introduces the Query Focused Extractor (QFE), a novel evidence extraction model inspired by extractive summarization. Unlike existing methods that independently extract evidence sentences, QFE uses a recurrent neural network (RNN) with an attention mechanism focused on the question, enabling sequential extraction and consideration of inter-sentence dependencies.  Integrated with a multi-task learning framework and a simple reading comprehension (RC) baseline, QFE achieves state-of-the-art evidence extraction results on both HotpotQA (an RC task) and FEVER (a textual entailment task), demonstrating its broad applicability.\n"}, "2310.18205": {"original_text": "  Claim span identification (CSI) is an important step in fact-checking\npipelines, aiming to identify text segments that contain a checkworthy claim or\nassertion in a social media post. Despite its importance to journalists and\nhuman fact-checkers, it remains a severely understudied problem, and the scarce\nresearch on this topic so far has only focused on English. Here we aim to\nbridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K\nreal-world claims collected from numerous social media platforms in five Indian\nlanguages and English. We report strong baselines with state-of-the-art\nencoder-only language models (e.g., XLM-R) and we demonstrate the benefits of\ntraining on multiple languages over alternative cross-lingual transfer methods\nsuch as zero-shot transfer, or training on translated data, from a\nhigh-resource language such as English. We evaluate generative large language\nmodels from the GPT series using prompting methods on the X-CLAIM dataset and\nwe find that they underperform the smaller encoder-only language models for\nlow-resource languages.\n", "rewritten_text": "Fact-checking relies heavily on claim span identification (CSI), pinpointing the verifiable claims within social media posts.  While crucial for journalists and fact-checkers, CSI research is limited, primarily focusing on English.  This paper introduces X-CLAIM, a new dataset of 7,000 real-world claims from five Indian languages and English, addressing this gap.  We establish strong baselines using cutting-edge encoder-only language models like XLM-R, showing multilingual training surpasses cross-lingual transfer techniques (e.g., zero-shot or English-to-other-language translation).  Furthermore, our evaluation reveals that generative models like GPT, even with prompting, underperform smaller encoder-only models on low-resource languages within this context.\n"}, "2306.08075": {"original_text": "  Current knowledge distillation approaches in semantic segmentation tend to\nadopt a holistic approach that treats all spatial locations equally. However,\nfor dense prediction, students' predictions on edge regions are highly\nuncertain due to contextual information leakage, requiring higher spatial\nsensitivity knowledge than the body regions. To address this challenge, this\npaper proposes a novel approach called boundary-privileged knowledge\ndistillation (BPKD). BPKD distills the knowledge of the teacher model's body\nand edges separately to the compact student model. Specifically, we employ two\ndistinct loss functions: (i) edge loss, which aims to distinguish between\nambiguous classes at the pixel level in edge regions; (ii) body loss, which\nutilizes shape constraints and selectively attends to the inner-semantic\nregions. Our experiments demonstrate that the proposed BPKD method provides\nextensive refinements and aggregation for edge and body regions. Additionally,\nthe method achieves state-of-the-art distillation performance for semantic\nsegmentation on three popular benchmark datasets, highlighting its\neffectiveness and generalization ability. BPKD shows consistent improvements\nacross a diverse array of lightweight segmentation structures, including both\nCNNs and transformers, underscoring its architecture-agnostic adaptability. The\ncode is available at \\url{https://github.com/AkideLiu/BPKD}.\n", "rewritten_text": "Existing knowledge distillation methods for semantic segmentation treat all image regions uniformly.  However,  predicting edges is inherently more challenging due to ambiguity, requiring more precise knowledge transfer than for central regions.  To overcome this, we introduce Boundary-Privileged Knowledge Distillation (BPKD).  BPKD separately distills teacher knowledge for edges and body regions using distinct loss functions: an edge loss focusing on ambiguous class distinctions at the pixel level, and a body loss leveraging shape constraints and focusing on semantically consistent areas.  Experiments on three benchmark datasets demonstrate BPKD's superior performance, achieving state-of-the-art results and consistent improvements across various lightweight architectures (CNNs and transformers).  Code is available at [https://github.com/AkideLiu/BPKD](https://github.com/AkideLiu/BPKD).\n"}, "2010.05762": {"original_text": "  Scene flow represents the 3D motion of every point in the dynamic\nenvironments. Like the optical flow that represents the motion of pixels in 2D\nimages, 3D motion representation of scene flow benefits many applications, such\nas autonomous driving and service robot. This paper studies the problem of\nscene flow estimation from two consecutive 3D point clouds. In this paper, a\nnovel hierarchical neural network with double attention is proposed for\nlearning the correlation of point features in adjacent frames and refining\nscene flow from coarse to fine layer by layer. The proposed network has a new\nmore-for-less hierarchical architecture. The more-for-less means that the\nnumber of input points is greater than the number of output points for scene\nflow estimation, which brings more input information and balances the precision\nand resource consumption. In this hierarchical architecture, scene flow of\ndifferent levels is generated and supervised respectively. A novel attentive\nembedding module is introduced to aggregate the features of adjacent points\nusing a double attention method in a patch-to-patch manner. The proper layers\nfor flow embedding and flow supervision are carefully considered in our network\ndesignment. Experiments show that the proposed network outperforms the\nstate-of-the-art performance of 3D scene flow estimation on the FlyingThings3D\nand KITTI Scene Flow 2015 datasets. We also apply the proposed network to\nrealistic LiDAR odometry task, which is an key problem in autonomous driving.\nThe experiment results demonstrate that our proposed network can outperform the\nICP-based method and shows the good practical application ability.\n", "rewritten_text": "This paper introduces a novel hierarchical neural network for 3D scene flow estimation from consecutive point clouds.  Unlike traditional methods, our \"more-for-less\" architecture processes more input points than output points, improving accuracy while managing computational resources.  This hierarchical network employs a double-attention mechanism to effectively aggregate point features within local patches, refining scene flow estimations layer by layer from coarse to fine.  The network's architecture and supervision strategy are carefully designed to optimize performance.  Experiments on FlyingThings3D and KITTI Scene Flow 2015 datasets demonstrate state-of-the-art results, surpassing existing methods.  Furthermore, application to LiDAR odometry, a crucial task in autonomous driving, shows superior performance compared to ICP-based approaches, highlighting the practical value of our proposed method.\n"}, "2303.14829": {"original_text": "  Generating grammatically and semantically correct captions in video\ncaptioning is a challenging task. The captions generated from the existing\nmethods are either word-by-word that do not align with grammatical structure or\nmiss key information from the input videos. To address these issues, we\nintroduce a novel global-local fusion network, with a Global-Local Fusion Block\n(GLFB) that encodes and fuses features from different parts of speech (POS)\ncomponents with visual-spatial features. We use novel combinations of different\nPOS components - 'determinant + subject', 'auxiliary verb', 'verb', and\n'determinant + object' for supervision of the POS blocks - Det + Subject, Aux\nVerb, Verb, and Det + Object respectively. The novel global-local fusion\nnetwork together with POS blocks helps align the visual features with language\ndescription to generate grammatically and semantically correct captions.\nExtensive qualitative and quantitative experiments on benchmark MSVD and MSRVTT\ndatasets demonstrate that the proposed approach generates more grammatically\nand semantically correct captions compared to the existing methods, achieving\nthe new state-of-the-art. Ablations on the POS blocks and the GLFB demonstrate\nthe impact of the contributions on the proposed method.\n", "rewritten_text": "Video captioning faces the challenge of generating grammatically and semantically accurate captions.  Existing methods often produce word-for-word transcriptions lacking proper grammatical structure or omit crucial video information.  This paper introduces a novel global-local fusion network incorporating a Global-Local Fusion Block (GLFB).  This block fuses visual-spatial features with features derived from different parts-of-speech (POS) components ('determinant + subject', 'auxiliary verb', 'verb', 'determinant + object'),  guiding the generation of grammatically correct captions.  Experiments on MSVD and MSRVTT datasets demonstrate state-of-the-art performance, surpassing existing methods in generating accurate and grammatically sound captions.  Ablation studies confirm the effectiveness of the POS blocks and GLFB.\n"}, "1805.11788": {"original_text": "  Traditional Convolutional Neural Networks (CNNs) typically use the same\nactivation function (usually ReLU) for all neurons with non-linear mapping\noperations. For example, the deep convolutional architecture Inception-v4 uses\nReLU. To improve the classification performance of traditional CNNs, a new\n\"Multi-function Convolutional Neural Network\" (MCNN) is created by using\ndifferent activation functions for different neurons. For $n$ neurons and $m$\ndifferent activation functions, there are a total of $m^n-m$ MCNNs and only $m$\ntraditional CNNs. Therefore, the best model is very likely to be chosen from\nMCNNs because there are $m^n-2m$ more MCNNs than traditional CNNs. For\nperformance analysis, two different datasets for two applications (classifying\nhandwritten digits from the MNIST database and classifying brain MRI images\ninto one of the four stages of Alzheimer's disease (AD)) are used. For both\napplications, an activation function is randomly selected for each layer of a\nMCNN. For the AD diagnosis application, MCNNs using a newly created\nmulti-function Inception-v4 architecture are constructed. Overall, simulations\nshow that MCNNs can outperform traditional CNNs in terms of multi-class\nclassification accuracy for both applications. An important future research\nwork will be to efficiently select the best MCNN from $m^n-m$ candidate MCNNs.\nCurrent CNN software only provides users with partial functionality of MCNNs\nsince different layers can use different activation functions but not\nindividual neurons in the same layer. Thus, modifying current CNN software\nsystems such as ResNets, DenseNets, and Dual Path Networks by using multiple\nactivation functions and developing more effective and faster MCNN software\nsystems and tools would be very useful to solve difficult practical image\nclassification problems.\n", "rewritten_text": "This paper introduces the Multi-function Convolutional Neural Network (MCNN), an architecture that enhances traditional CNNs (e.g., Inception-v4) by employing diverse activation functions across individual neurons, rather than a uniform function like ReLU.  Unlike traditional CNNs with only *m* possible configurations given *m* activation functions, MCNNs offer a significantly larger search space of *m<sup>n</sup> - m* unique models (*n* being the number of neurons).  This expanded search space dramatically increases the likelihood of finding a superior model.  Experiments on MNIST (handwritten digit classification) and Alzheimer's disease staging using brain MRI images demonstrate that randomly assigning activation functions to each layer in MCNNs consistently outperforms traditional CNNs in multi-class classification accuracy.  A key challenge for future research is efficiently selecting the optimal MCNN from this vast space.  Current CNN software frameworks lack the full functionality of MCNNs, only allowing varied activation functions across layers, not individual neurons within a layer.  Therefore, adapting existing frameworks (ResNets, DenseNets, Dual Path Networks) and developing specialized MCNN software are crucial steps towards solving complex image classification problems.\n"}, "2108.024": {"original_text": "  Gait data captured by inertial sensors have demonstrated promising results on\nuser authentication. However, most existing approaches stored the enrolled gait\npattern insecurely for matching with the validating pattern, thus, posed\ncritical security and privacy issues. In this study, we present a gait\ncryptosystem that generates from gait data the random key for user\nauthentication, meanwhile, secures the gait pattern. First, we propose a\nrevocable and random binary string extraction method using a deep neural\nnetwork followed by feature-wise binarization. A novel loss function for\nnetwork optimization is also designed, to tackle not only the intrauser\nstability but also the inter-user randomness. Second, we propose a new\nbiometric key generation scheme, namely Irreversible Error Correct and\nObfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme,\nto securely generate from the binary string the random and irreversible key.\nThe model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. We\nshowed that our model could generate the key of 139 bits from 5-second data\nsequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR)\nsmaller than 5.441%. In addition, the security and user privacy analyses showed\nthat our model was secure against existing attacks on biometric template\nprotection, and fulfilled irreversibility and unlinkability.\n", "rewritten_text": "This study addresses security and privacy vulnerabilities in gait-based authentication systems that store gait patterns insecurely.  We introduce a novel gait cryptosystem that generates a random authentication key directly from gait data, eliminating the need to store sensitive gait information.  This is achieved through a two-stage process:  first, a deep neural network with a custom loss function extracts a revocable, random binary string from gait data; second, a new biometric key generation scheme, IECO (an improvement on ECO), irreversibly transforms this string into a secure, random key.  Evaluated on OU-ISIR and whuGAIT datasets, our system generated a 139-bit key from 5-second gait sequences, achieving zero False Acceptance Rate (FAR) and a False Rejection Rate (FRR) below 5.441%.  Security analysis demonstrates resistance to existing attacks and confirms irreversibility and unlinkability, ensuring user privacy.\n"}, "2404.03443": {"original_text": "  The goal of occluded person re-identification (ReID) is to retrieve specific\npedestrians in occluded situations. However, occluded person ReID still suffers\nfrom background clutter and low-quality local feature representations, which\nlimits model performance. In our research, we introduce a new framework called\nPAB-ReID, which is a novel ReID model incorporating part-attention mechanisms\nto tackle the aforementioned issues effectively. Firstly, we introduce the\nhuman parsing label to guide the generation of more accurate human part\nattention maps. In addition, we propose a fine-grained feature focuser for\ngenerating fine-grained human local feature representations while suppressing\nbackground interference. Moreover, We also design a part triplet loss to\nsupervise the learning of human local features, which optimizes\nintra/inter-class distance. We conducted extensive experiments on specialized\nocclusion and regular ReID datasets, showcasing that our approach outperforms\nthe existing state-of-the-art methods.\n", "rewritten_text": "Occluded person re-identification (ReID) struggles with background clutter and poor local feature representation.  To address this, we propose PAB-ReID, a novel ReID model employing part-attention mechanisms.  PAB-ReID uses human parsing labels to generate accurate part attention maps, a fine-grained feature focuser to improve local feature representation while suppressing background noise, and a part triplet loss to optimize intra- and inter-class distances.  Extensive experiments on occlusion-specific and standard ReID datasets demonstrate that PAB-ReID surpasses existing state-of-the-art methods.\n"}, "2310.05620": {"original_text": "  General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic.\n", "rewritten_text": "Existing benchmarks for evaluating Large Language Models (LLMs) in LegalAI, designed by computer scientists, inadequately reflect the reasoning processes of legal professionals.  To address this, we introduce LAiW, the first Chinese legal LLM benchmark grounded in legal practice.  LAiW's three-tiered evaluation (basic information retrieval, legal foundation inference, and complex legal application) mirrors the progression of legal reasoning.  Our automated evaluation reveals that current LLMs, while seemingly proficient in complex tasks, struggle with fundamental ones, hindering their practical applicability.  Further human evaluation by legal experts confirms this, highlighting the need for improved legal reasoning capabilities in LLMs before widespread adoption.\n"}, "2408.12677": {"original_text": "  Traditional volumetric fusion algorithms preserve the spatial structure of 3D\nscenes, which is beneficial for many tasks in computer vision and robotics.\nHowever, they often lack realism in terms of visualization. Emerging 3D\nGaussian splatting bridges this gap, but existing Gaussian-based reconstruction\nmethods often suffer from artifacts and inconsistencies with the underlying 3D\nstructure, and struggle with real-time optimization, unable to provide users\nwith immediate feedback in high quality. One of the bottlenecks arises from the\nmassive amount of Gaussian parameters that need to be updated during\noptimization. Instead of using 3D Gaussian as a standalone map representation,\nwe incorporate it into a volumetric mapping system to take advantage of\ngeometric information and propose to use a quadtree data structure on images to\ndrastically reduce the number of splats initialized. In this way, we\nsimultaneously generate a compact 3D Gaussian map with fewer artifacts and a\nvolumetric map on the fly. Our method, GSFusion, significantly enhances\ncomputational efficiency without sacrificing rendering quality, as demonstrated\non both synthetic and real datasets. Code will be available at\nhttps://github.com/goldoak/GSFusion.\n", "rewritten_text": "Volumetric fusion excels at preserving 3D scene structure, crucial for computer vision and robotics, but often lacks visual realism.  While Gaussian splatting offers improved realism, existing methods suffer from artifacts, inconsistencies, and slow real-time optimization.  This slow optimization stems from the large number of Gaussian parameters requiring updates.  Our novel approach, GSFusion, addresses these limitations by integrating Gaussian splatting into a volumetric mapping system, leveraging geometric information and a quadtree image structure to drastically reduce the number of splats. This yields a compact, artifact-reduced 3D Gaussian map and a simultaneous volumetric map, significantly improving computational efficiency without compromising rendering quality.  Results on synthetic and real datasets demonstrate this improvement.  Code is available at https://github.com/goldoak/GSFusion.\n"}, "2309.14962": {"original_text": "  All tables can be represented as grids. Based on this observation, we propose\nGridFormer, a novel approach for interpreting unconstrained table structures by\npredicting the vertex and edge of a grid. First, we propose a flexible table\nrepresentation in the form of an MXN grid. In this representation, the vertexes\nand edges of the grid store the localization and adjacency information of the\ntable. Then, we introduce a DETR-style table structure recognizer to\nefficiently predict this multi-objective information of the grid in a single\nshot. Specifically, given a set of learned row and column queries, the\nrecognizer directly outputs the vertexes and edges information of the\ncorresponding rows and columns. Extensive experiments on five challenging\nbenchmarks which include wired, wireless, multi-merge-cell, oriented, and\ndistorted tables demonstrate the competitive performance of our model over\nother methods.\n", "rewritten_text": "GridFormer is a new method for interpreting complex table structures.  It represents tables as grids, predicting grid vertices and edges to capture table layout and cell adjacency.  This approach uses an MXN grid to store localization and adjacency information, and a DETR-style architecture to simultaneously predict vertex and edge information from learned row and column queries.  Evaluated on five diverse benchmarks (including wired, wireless, multi-merge-cell, oriented, and distorted tables), GridFormer demonstrates strong performance compared to existing techniques.\n"}, "2303.17658": {"original_text": "  Machine learning models deployed in the open world may encounter observations\nthat they were not trained to recognize, and they risk misclassifying such\nobservations with high confidence. Therefore, it is essential that these models\nare able to ascertain what is in-distribution (ID) and out-of-distribution\n(OOD), to avoid this misclassification. In recent years, huge strides have been\nmade in creating models that are robust to this distinction. As a result, the\ncurrent state-of-the-art has reached near perfect performance on relatively\ncoarse-grained OOD detection tasks, such as distinguishing horses from trucks,\nwhile struggling with finer-grained classification, like differentiating models\nof commercial aircraft. In this paper, we describe a new theoretical framework\nfor understanding fine- and coarse-grained OOD detection, we re-conceptualize\nfine grained classification into a three part problem, and we propose a new\nbaseline task for OOD models on two fine-grained hierarchical data sets, two\nnew evaluation methods to differentiate fine- and coarse-grained OOD\nperformance, along with a new loss function for models in this task.\n", "rewritten_text": "Real-world machine learning models can confidently misclassify unfamiliar data.  Robustly distinguishing between in-distribution (ID) and out-of-distribution (OOD) data is crucial to prevent this. While recent advancements have yielded near-perfect performance on coarse-grained OOD detection (e.g., horses vs. trucks), finer-grained tasks (e.g., aircraft models) remain challenging. This paper introduces a novel theoretical framework for understanding both fine- and coarse-grained OOD detection.  We propose a three-part problem re-conceptualization of fine-grained classification, a new baseline task using two hierarchical datasets, two novel evaluation metrics to differentiate performance levels, and a new loss function tailored to this task.\n"}, "2109.00698": {"original_text": "  While conventional wisdom suggests that more aggressively filtering data from\nlow-quality sources like Common Crawl always monotonically improves the quality\nof training data, we find that aggressive filtering can in fact lead to a\ndecrease in model quality on a wide array of downstream tasks for a GPT-like\nlanguage model. We speculate that this is because optimizing sufficiently\nstrongly for a proxy metric harms performance on the true objective, suggesting\na need for more robust filtering objectives when attempting to filter more\naggressively. We hope this work leads to detailed analysis of the effects of\ndataset filtering design choices on downstream model performance in future\nwork.\n", "rewritten_text": "Contrary to the common belief that stricter filtering of low-quality data (e.g., from Common Crawl) consistently improves large language model training, our research shows that aggressive filtering can actually harm model performance across various downstream tasks.  This suggests that optimizing too heavily for a proxy metric of data quality can negatively impact the true objective.  We propose that more sophisticated filtering methods are needed, and call for future research to thoroughly investigate the impact of different data filtering strategies on model performance.\n"}, "1501.03952": {"original_text": "  Domain adaptation techniques aim at adapting a classifier learnt on a source\ndomain to work on the target domain. Exploiting the subspaces spanned by\nfeatures of the source and target domains respectively is one approach that has\nbeen investigated towards solving this problem. These techniques normally\nassume the existence of a single subspace for the entire source / target\ndomain. In this work, we consider the hierarchical organization of the data and\nconsider multiple subspaces for the source and target domain based on the\nhierarchy. We evaluate different subspace based domain adaptation techniques\nunder this setting and observe that using different subspaces based on the\nhierarchy yields consistent improvement over a non-hierarchical baseline\n", "rewritten_text": "This work addresses domain adaptation by leveraging the hierarchical structure of data.  Unlike existing subspace-based methods that assume a single source and target subspace, we propose a multi-subspace approach reflecting the data hierarchy.  Our evaluation demonstrates that this hierarchical multi-subspace method consistently outperforms a single-subspace baseline.\n"}, "1707.00569": {"original_text": "  This survey presents a deep analysis of the learning and inference\ncapabilities in nine popular trackers. It is neither intended to study the\nwhole literature nor is it an attempt to review all kinds of neural networks\nproposed for visual tracking. We focus instead on Siamese neural networks which\nare a promising starting point for studying the challenging problem of\ntracking. These networks integrate efficiently feature learning and the\ntemporal matching and have so far shown state-of-the-art performance. In\nparticular, the branches of Siamese networks, their layers connecting these\nbranches, specific aspects of training and the embedding of these networks into\nthe tracker are highlighted. Quantitative results from existing papers are\ncompared with the conclusion that the current evaluation methodology shows\nproblems with the reproducibility and the comparability of results. The paper\nproposes a novel Lisp-like formalism for a better comparison of trackers. This\nassumes a certain functional design and functional decomposition of trackers.\nThe paper tries to give foundation for tracker design by a formulation of the\nproblem based on the theory of machine learning and by the interpretation of a\ntracker as a decision function. The work concludes with promising lines of\nresearch and suggests future work.\n", "rewritten_text": "This paper analyzes the learning and inference capabilities of nine popular Siamese neural network trackers.  While not an exhaustive review of visual tracking literature or all neural network architectures, it focuses on Siamese networks\u2014a high-performing, efficient architecture integrating feature learning and temporal matching.  The analysis highlights key aspects of Siamese network design, including branch structures, inter-branch connections, training methods, and integration within the tracker.  A comparative analysis of existing quantitative results reveals reproducibility and comparability issues in current evaluation methodologies.  To address this, the paper introduces a novel Lisp-like formalism for improved tracker comparison, based on a functional design and decomposition approach.  Furthermore, it provides a machine learning-based theoretical foundation for tracker design, interpreting trackers as decision functions.  The paper concludes by outlining promising avenues for future research.\n"}, "2202.04897": {"original_text": "  Knowledge graph embedding (KGE) models learn the representation of entities\nand relations in knowledge graphs. Distance-based methods show promising\nperformance on link prediction task, which predicts the result by the distance\nbetween two entity representations. However, most of these methods represent\nthe head entity and tail entity separately, which limits the model capacity. We\npropose two novel distance-based methods named InterHT and InterHT+ that allow\nthe head and tail entities to interact better and get better entity\nrepresentation. Experimental results show that our proposed method achieves the\nbest results on ogbl-wikikg2 dataset.\n", "rewritten_text": "Knowledge graph embedding (KGE) models represent entities and relations within knowledge graphs.  While distance-based link prediction methods, which infer relationships based on entity representation proximity, show promise, their separate representation of head and tail entities hinders performance.  This paper introduces InterHT and InterHT+, two novel distance-based methods that improve entity representation by enabling stronger head-tail entity interaction.  Experiments on the ogbl-wikikg2 dataset demonstrate state-of-the-art results.\n"}, "1907.0203": {"original_text": "  Factchecking has always been a part of the journalistic process. However with\nnewsroom budgets shrinking it is coming under increasing pressure just as the\namount of false information circulating is on the rise. We therefore propose a\nmethod to increase the efficiency of the factchecking process, using the latest\ndevelopments in Natural Language Processing (NLP). This method allows us to\ncompare incoming claims to an existing corpus and return similar, factchecked,\nclaims in a live system-allowing factcheckers to work simultaneously without\nduplicating their work.\n", "rewritten_text": "Journalistic fact-checking, while crucial, faces growing challenges due to shrinking newsroom budgets and a surge in misinformation.  To address this, we propose a novel NLP-based method to enhance fact-checking efficiency.  Our system compares incoming claims against a database of previously verified information, instantly identifying similar claims and enabling collaborative, non-redundant fact-checking.\n"}, "2311.06377": {"original_text": "  Heaps' law is an empirical relation in text analysis that predicts vocabulary\ngrowth as a function of corpus size. While this law has been validated in\ndiverse human-authored text corpora, its applicability to large language model\ngenerated text remains unexplored. This study addresses this gap, focusing on\nthe emulation of corpora using the suite of GPT-Neo large language models. To\nconduct our investigation, we emulated corpora of PubMed abstracts using three\ndifferent parameter sizes of the GPT-Neo model. Our emulation strategy involved\nusing the initial five words of each PubMed abstract as a prompt and\ninstructing the model to expand the content up to the original abstract's\nlength. Our findings indicate that the generated corpora adhere to Heaps' law.\nInterestingly, as the GPT-Neo model size grows, its generated vocabulary\nincreasingly adheres to Heaps' law as as observed in human-authored text. To\nfurther improve the richness and authenticity of GPT-Neo outputs, future\niterations could emphasize enhancing model size or refining the model\narchitecture to curtail vocabulary repetition.\n", "rewritten_text": "This study investigates the applicability of Heaps' Law \u2013 which describes vocabulary growth in relation to text corpus size \u2013 to text generated by large language models (LLMs).  Using three GPT-Neo models of varying sizes, we generated corpora by prompting each model with the first five words of PubMed abstracts and instructing it to expand the text to the original abstract's length.  Our results show that the generated text obeys Heaps' Law, with larger GPT-Neo models exhibiting a closer adherence to the law observed in human-written text.  Future work could focus on increasing model size or architectural improvements to reduce vocabulary repetition and enhance the quality of LLM-generated text.\n"}, "1809.07099": {"original_text": "  Deep neural networks have exhibited promising performance in image\nsuper-resolution (SR) due to the power in learning the non-linear mapping from\nlow-resolution (LR) images to high-resolution (HR) images. However, most deep\nlearning methods employ feed-forward architectures, and thus the dependencies\nbetween LR and HR images are not fully exploited, leading to limited learning\nperformance. Moreover, most deep learning based SR methods apply the pixel-wise\nreconstruction error as the loss, which, however, may fail to capture\nhigh-frequency information and produce perceptually unsatisfying results,\nwhilst the recent perceptual loss relies on some pre-trained deep model and\nthey may not generalize well. In this paper, we introduce a mask to separate\nthe image into low- and high-frequency parts based on image gradient magnitude,\nand then devise a gradient sensitive loss to well capture the structures in the\nimage without sacrificing the recovery of low-frequency content. Moreover, by\ninvestigating the duality in SR, we develop a dual reconstruction network (DRN)\nto improve the SR performance. We provide theoretical analysis on the\ngeneralization performance of our method and demonstrate its effectiveness and\nsuperiority with thorough experiments.\n", "rewritten_text": "Deep neural networks show promise in image super-resolution (SR) by learning the complex mapping from low-resolution (LR) to high-resolution (HR) images.  However, standard feed-forward architectures underutilize the LR-HR dependencies, limiting performance.  Furthermore, common pixel-wise loss functions often neglect high-frequency details, resulting in visually unsatisfactory results, while perceptual losses based on pre-trained models may lack generalizability.  This paper addresses these limitations by introducing a novel gradient-sensitive loss function, employing a mask to separate low- and high-frequency image components, and proposing a Dual Reconstruction Network (DRN) leveraging the inherent duality in SR.  We provide theoretical analysis and experimental validation demonstrating the superior performance of our approach.\n"}, "2310.15052": {"original_text": "  Dataset distillation plays a crucial role in creating compact datasets with\nsimilar training performance compared with original large-scale ones. This is\nessential for addressing the challenges of data storage and training costs.\nPrevalent methods facilitate knowledge transfer by matching the gradients,\nembedding distributions, or training trajectories of synthetic images with\nthose of the sampled original images. Although there are various matching\nobjectives, currently the strategy for selecting original images is limited to\nnaive random sampling. We argue that random sampling overlooks the evenness of\nthe selected sample distribution, which may result in noisy or biased matching\ntargets. Besides, the sample diversity is also not constrained by random\nsampling. Additionally, current methods predominantly focus on\nsingle-dimensional matching, where information is not fully utilized. To\naddress these challenges, we propose a novel matching strategy called Dataset\nDistillation by Bidirectional REpresentAtive Matching (DREAM+), which selects\nrepresentative original images for bidirectional matching. DREAM+ is applicable\nto a variety of mainstream dataset distillation frameworks and significantly\nreduces the number of distillation iterations by more than 15 times without\naffecting performance. Given sufficient training time, DREAM+ can further\nimprove the performance and achieve state-of-the-art results. We have released\nthe code at github.com/NUS-HPC-AI-Lab/DREAM+.\n", "rewritten_text": "Dataset distillation creates smaller, efficient training datasets without sacrificing performance.  Current methods, while effective at transferring knowledge via gradient, embedding, or trajectory matching, rely on simple random sampling of original images. This random selection can lead to uneven, noisy, and unrepresentative sample distributions, hindering optimal knowledge transfer.  To address this, we introduce DREAM+, a novel bidirectional representative matching strategy.  DREAM+ selects representative images for more effective matching, significantly improving existing distillation frameworks.  It reduces training iterations by over 15x while maintaining, and even improving upon, performance given sufficient training time.  Our code is available at github.com/NUS-HPC-AI-Lab/DREAM+.\n"}, "2207.09625": {"original_text": "  Given an image and a reference caption, the image caption editing task aims\nto correct the misalignment errors and generate a refined caption. However, all\nexisting caption editing works are implicit models, ie, they directly produce\nthe refined captions without explicit connections to the reference captions. In\nthis paper, we introduce a new task: Explicit Caption Editing (ECE). ECE models\nexplicitly generate a sequence of edit operations, and this edit operation\nsequence can translate the reference caption into a refined one. Compared to\nthe implicit editing, ECE has multiple advantages: 1) Explainable: it can trace\nthe whole editing path. 2) Editing Efficient: it only needs to modify a few\nwords. 3) Human-like: it resembles the way that humans perform caption editing,\nand tries to keep original sentence structures. To solve this new task, we\npropose the first ECE model: TIger. TIger is a non-autoregressive\ntransformer-based model, consisting of three modules: Tagger_del, Tagger_add,\nand Inserter. Specifically, Tagger_del decides whether each word should be\npreserved or not, Tagger_add decides where to add new words, and Inserter\npredicts the specific word for adding. To further facilitate ECE research, we\npropose two new ECE benchmarks by re-organizing two existing datasets, dubbed\nCOCO-EE and Flickr30K-EE, respectively. Extensive ablations on both two\nbenchmarks have demonstrated the effectiveness of TIger.\n", "rewritten_text": "Image caption editing aims to correct inaccurate captions.  Existing methods implicitly generate corrected captions without showing how they differ from the original.  This paper introduces Explicit Caption Editing (ECE), a new task where models explicitly generate a sequence of edits transforming the original caption into a refined one.  This approach offers explainability (showing the edit path), efficiency (modifying only a few words), and a more human-like process (preserving sentence structure).  We propose TIger, the first ECE model\u2014a non-autoregressive transformer with three modules:  Tagger_del (word deletion), Tagger_add (word insertion location), and Inserter (word prediction).  We also introduce two new ECE benchmarks, COCO-EE and Flickr30K-EE, derived from existing datasets.  Extensive experiments demonstrate TIger's effectiveness.\n"}, "1805.05503": {"original_text": "  Human faces are one interesting object class with numerous applications.\nWhile significant progress has been made in the generic deblurring problem,\nexisting methods are less effective for blurry face images. The success of the\nstate-of-the-art image deblurring algorithms stems mainly from implicit or\nexplicit restoration of salient edges for kernel estimation. However, existing\nmethods are less effective as only few edges can be restored from blurry face\nimages for kernel estimation. In this paper, we address the problem of\ndeblurring face images by exploiting facial structures. We propose a deblurring\nalgorithm based on an exemplar dataset without using coarse-to-fine strategies\nor heuristic edge selections. In addition, we develop a convolutional neural\nnetwork to restore sharp edges from blurry images for deblurring. Extensive\nexperiments against the state-of-the-art methods demonstrate the effectiveness\nof the proposed algorithms for deblurring face images. In addition, we show the\nproposed algorithms can be applied to image deblurring for other object\nclasses.\n", "rewritten_text": "Blurring significantly impacts the effectiveness of image deblurring algorithms on faces, despite advancements in general image deblurring.  Current methods rely heavily on edge restoration for kernel estimation, which is hindered by the limited edge information in blurry faces. This paper introduces a novel face image deblurring algorithm leveraging facial structures and an exemplar dataset, avoiding coarse-to-fine approaches and manual edge selection.  Furthermore, a convolutional neural network is developed to enhance edge restoration.  Extensive experiments confirm the superior performance of our approach compared to state-of-the-art methods, and demonstrate its applicability beyond face images.\n"}, "2205.09299": {"original_text": "  Convolutional Neural Networks (CNNs) have achieved promising results in\nmedical image segmentation. However, CNNs require lots of training data and are\nincapable of handling pose and deformation of objects. Furthermore, their\npooling layers tend to discard important information such as positions as well\nas CNNs are sensitive to rotation and affine transformation. Capsule network is\na recent new architecture that has achieved better robustness in part-whole\nrepresentation learning by replacing pooling layers with dynamic routing and\nconvolutional strides, which has shown potential results on popular tasks such\nas digit classification and object segmentation. In this paper, we propose a 3D\nencoder-decoder network with Convolutional Capsule Encoder (called 3DConvCaps)\nto learn lower-level features (short-range attention) with convolutional layers\nwhile modeling the higher-level features (long-range dependence) with capsule\nlayers. Our experiments on multiple datasets including iSeg-2017, Hippocampus,\nand Cardiac demonstrate that our 3D 3DConvCaps network considerably outperforms\nprevious capsule networks and 3D-UNets. We further conduct ablation studies of\nnetwork efficiency and segmentation performance under various configurations of\nconvolution layers and capsule layers at both contracting and expanding paths.\n", "rewritten_text": "While Convolutional Neural Networks (CNNs) show promise in medical image segmentation, their reliance on extensive training data and vulnerability to object pose, deformation, and affine transformations limit their effectiveness.  Capsule networks offer improved robustness in part-whole representation learning, addressing these limitations.  This paper introduces 3DConvCaps, a 3D encoder-decoder network incorporating convolutional layers for short-range attention (lower-level features) and capsule layers for long-range dependencies (higher-level features).  Experiments on iSeg-2017, Hippocampus, and Cardiac datasets demonstrate that 3DConvCaps significantly outperforms existing capsule networks and 3D-UNets.  Ablation studies further analyze the impact of varying convolutional and capsule layer configurations on network efficiency and segmentation performance.\n"}, "2407.08290": {"original_text": "  Recent advances in mobile mapping systems have greatly enhanced the\nefficiency and convenience of acquiring urban 3D data. These systems utilize\nLiDAR sensors mounted on vehicles to capture vast cityscapes. However, a\nsignificant challenge arises due to occlusions caused by roadside parked\nvehicles, leading to the loss of scene information, particularly on the roads,\nsidewalks, curbs, and the lower sections of buildings. In this study, we\npresent a novel approach that leverages deep neural networks to learn a model\ncapable of filling gaps in urban scenes that are obscured by vehicle occlusion.\nWe have developed an innovative technique where we place virtual vehicle models\nalong road boundaries in the gap-free scene and utilize a ray-casting algorithm\nto create a new scene with occluded gaps. This allows us to generate diverse\nand realistic urban point cloud scenes with and without vehicle occlusion,\nsurpassing the limitations of real-world training data collection and\nannotation. Furthermore, we introduce the Scene Gap Completion Network\n(SGC-Net), an end-to-end model that can generate well-defined shape boundaries\nand smooth surfaces within occluded gaps. The experiment results reveal that\n97.66% of the filled points fall within a range of 5 centimeters relative to\nthe high-density ground truth point cloud scene. These findings underscore the\nefficacy of our proposed model in gap completion and reconstructing urban\nscenes affected by vehicle occlusions.\n", "rewritten_text": "Mobile mapping systems, while efficient at acquiring 3D urban data using LiDAR, suffer from occlusions caused by parked vehicles.  This study addresses this by introducing a novel deep learning approach. We generate synthetic training data by virtually placing vehicles in gap-free point cloud scenes and then using ray casting to simulate occlusions. This overcomes the limitations of real-world data acquisition.  Our proposed Scene Gap Completion Network (SGC-Net) accurately fills these gaps, achieving a 97.66% accuracy within 5 centimeters of ground truth.  This demonstrates the effectiveness of our method in reconstructing complete and accurate 3D urban models.\n"}, "2004.04699": {"original_text": "  Deep Neural Networks trained in a fully supervised fashion are the dominant\ntechnology in perception-based autonomous driving systems. While collecting\nlarge amounts of unlabeled data is already a major undertaking, only a subset\nof it can be labeled by humans due to the effort needed for high-quality\nannotation. Therefore, finding the right data to label has become a key\nchallenge. Active learning is a powerful technique to improve data efficiency\nfor supervised learning methods, as it aims at selecting the smallest possible\ntraining set to reach a required performance. We have built a scalable\nproduction system for active learning in the domain of autonomous driving. In\nthis paper, we describe the resulting high-level design, sketch some of the\nchallenges and their solutions, present our current results at scale, and\nbriefly describe the open problems and future directions.\n", "rewritten_text": "Autonomous driving heavily relies on deep neural networks trained with fully supervised learning.  However, labeling the vast amounts of data needed is expensive and time-consuming.  This paper presents a scalable, production-ready active learning system designed to address this data efficiency challenge. We detail the system's architecture, discuss key challenges and their solutions, present large-scale results, and outline future research directions.\n"}, "1808.01491": {"original_text": "  Single image rain streaks removal has recently witnessed substantial progress\ndue to the development of deep convolutional neural networks. However, existing\ndeep learning based methods either focus on the entrance and exit of the\nnetwork by decomposing the input image into high and low frequency information\nand employing residual learning to reduce the mapping range, or focus on the\nintroduction of cascaded learning scheme to decompose the task of rain streaks\nremoval into multi-stages. These methods treat the convolutional neural network\nas an encapsulated end-to-end mapping module without deepening into the\nrationality and superiority of neural network design. In this paper, we delve\ninto an effective end-to-end neural network structure for stronger feature\nexpression and spatial correlation learning. Specifically, we propose a\nnon-locally enhanced encoder-decoder network framework, which consists of a\npooling indices embedded encoder-decoder network to efficiently learn\nincreasingly abstract feature representation for more accurate rain streaks\nmodeling while perfectly preserving the image detail. The proposed\nencoder-decoder framework is composed of a series of non-locally enhanced dense\nblocks that are designed to not only fully exploit hierarchical features from\nall the convolutional layers but also well capture the long-distance\ndependencies and structural information. Extensive experiments on synthetic and\nreal datasets demonstrate that the proposed method can effectively remove\nrain-streaks on rainy image of various densities while well preserving the\nimage details, which achieves significant improvements over the recent\nstate-of-the-art methods.\n", "rewritten_text": "Recent advancements in single image rain streak removal leverage deep convolutional neural networks (CNNs).  Existing deep learning approaches either pre-process images (decomposing into high and low frequencies) or use cascaded networks, treating the CNN as a black box.  This paper introduces a novel end-to-end network architecture designed for superior feature representation and spatial correlation learning.  Our non-locally enhanced encoder-decoder network, incorporating pooling indices, efficiently learns increasingly abstract features for accurate rain streak modeling while preserving image detail.  This architecture utilizes dense blocks to fully exploit hierarchical features and capture long-range dependencies.  Extensive experiments on synthetic and real datasets demonstrate significant improvements over state-of-the-art methods in rain streak removal while maintaining image quality.\n"}, "2303.15698": {"original_text": "  Standard deep learning models such as convolutional neural networks (CNNs)\nlack the ability of generalizing to domains which have not been seen during\ntraining. This problem is mainly due to the common but often wrong assumption\nof such models that the source and target data come from the same i.i.d.\ndistribution. Recently, Vision Transformers (ViTs) have shown outstanding\nperformance for a broad range of computer vision tasks. However, very few\nstudies have investigated their ability to generalize to new domains. This\npaper presents a first Token-level Feature Stylization (TFS-ViT) approach for\ndomain generalization, which improves the performance of ViTs to unseen data by\nsynthesizing new domains. Our approach transforms token features by mixing the\nnormalization statistics of images from different domains. We further improve\nthis approach with a novel strategy for attention-aware stylization, which uses\nthe attention maps of class (CLS) tokens to compute and mix normalization\nstatistics of tokens corresponding to different image regions. The proposed\nmethod is flexible to the choice of backbone model and can be easily applied to\nany ViT-based architecture with a negligible increase in computational\ncomplexity. Comprehensive experiments show that our approach is able to achieve\nstate-of-the-art performance on five challenging benchmarks for domain\ngeneralization, and demonstrate its ability to deal with different types of\ndomain shifts. The implementation is available at:\nhttps://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.\n", "rewritten_text": "Standard deep learning models, including Convolutional Neural Networks (CNNs), struggle to generalize to unseen data due to their assumption of identically and independently distributed (i.i.d.) training and target data. While Vision Transformers (ViTs) excel at various computer vision tasks, their domain generalization capabilities remain largely unexplored.  This paper introduces TFS-ViT, a novel token-level feature stylization approach for improving ViT performance on unseen data by synthesizing new domains.  TFS-ViT mixes normalization statistics from different domains to transform token features, further enhanced by attention-aware stylization using class token attention maps to selectively mix statistics from specific image regions.  This method is computationally efficient and adaptable to various ViT architectures.  Extensive experiments on five challenging benchmarks demonstrate state-of-the-art domain generalization performance across diverse domain shifts.  The code is available at: https://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.\n"}, "1804.09555": {"original_text": "  Outdoor vision-based systems suffer from atmospheric turbulences, and rain is\none of the worst factors for vision degradation. Current rain removal methods\nshow limitations either for complex dynamic scenes, or under torrential rain\nwith opaque occlusions. We propose a novel derain framework which applies\nsuperpixel (SP) segmentation to decompose the scene into depth consistent\nunits. Alignment of scene contents are done at the SP level, which proves to be\nrobust against rain occlusion interferences and fast camera motion. Two\nalignment output tensors, i.e., optimal temporal match tensor and sorted\nspatial-temporal match tensor, provide informative clues for the location of\nrain streaks and the occluded background contents. Different classical and\nnovel methods such as Robust Principle Component Analysis and Convolutional\nNeural Networks are applied and compared for their respective advantages in\nefficiently exploiting the rich spatial-temporal features provided by the two\ntensors. Extensive evaluations show that advantage of up to 5dB is achieved on\nthe scene restoration PSNR over state-of-the-art methods, and the advantage is\nespecially obvious with highly complex and dynamic scenes. Visual evaluations\nshow that the proposed framework is not only able to suppress heavy and opaque\noccluding rain streaks, but also large semi-transparent regional fluctuations\nand distortions.\n", "rewritten_text": "Outdoor vision systems are severely hampered by atmospheric turbulence, especially rain.  Existing rain removal techniques struggle with complex, dynamic scenes and heavy rain obscuring the view.  This paper introduces a new deraining framework that uses superpixel segmentation to divide the scene into depth-consistent units.  Alignment of scene content at the superpixel level is robust to rain occlusion and fast camera movement.  Two alignment tensors\u2014an optimal temporal match and a sorted spatio-temporal match\u2014identify rain streaks and occluded background.  We compare several classical and novel methods, including Robust Principal Component Analysis and Convolutional Neural Networks, to effectively utilize these tensors' rich spatio-temporal information.  Our extensive evaluation demonstrates a PSNR improvement of up to 5dB over state-of-the-art methods, particularly in complex, dynamic scenes.  Visually, our framework effectively removes heavy, opaque rain streaks, as well as large semi-transparent distortions.\n"}, "2005.00975": {"original_text": "  In the deep learning (DL) era, parsing models are extremely simplified with\nlittle hurt on performance, thanks to the remarkable capability of multi-layer\nBiLSTMs in context representation. As the most popular graph-based dependency\nparser due to its high efficiency and performance, the biaffine parser directly\nscores single dependencies under the arc-factorization assumption, and adopts a\nvery simple local token-wise cross-entropy training loss. This paper for the\nfirst time presents a second-order TreeCRF extension to the biaffine parser.\nFor a long time, the complexity and inefficiency of the inside-outside\nalgorithm hinder the popularity of TreeCRF. To address this issue, we propose\nan effective way to batchify the inside and Viterbi algorithms for direct large\nmatrix operation on GPUs, and to avoid the complex outside algorithm via\nefficient back-propagation. Experiments and analysis on 27 datasets from 13\nlanguages clearly show that techniques developed before the DL era, such as\nstructural learning (global TreeCRF loss) and high-order modeling are still\nuseful, and can further boost parsing performance over the state-of-the-art\nbiaffine parser, especially for partially annotated training data. We release\nour code at https://github.com/yzhangcs/crfpar.\n", "rewritten_text": "Deep learning's powerful BiLSTMs have simplified parsing models without sacrificing performance.  While the efficient biaffine parser is currently dominant, this paper introduces a novel second-order TreeCRF extension.  Addressing the historical computational limitations of TreeCRFs, we present a batched implementation of the inside and Viterbi algorithms, leveraging GPUs and avoiding the outside algorithm through efficient backpropagation.  Experiments across 27 datasets in 13 languages demonstrate that pre-deep learning techniques like structural learning (global TreeCRF loss) and higher-order modeling remain valuable, significantly improving the state-of-the-art biaffine parser, particularly with partially annotated data.  Our code is available at https://github.com/yzhangcs/crfpar.\n"}, "1812.00108": {"original_text": "  With vast amounts of video content being uploaded to the Internet every\nminute, video summarization becomes critical for efficient browsing, searching,\nand indexing of visual content. Nonetheless, the spread of social and\negocentric cameras creates an abundance of sparse scenarios captured by several\ndevices, and ultimately required to be jointly summarized. In this paper, we\ndiscuss the problem of summarizing videos recorded independently by several\ndynamic cameras that intermittently share the field of view. We present a\nrobust framework that (a) identifies a diverse set of important events among\nmoving cameras that often are not capturing the same scene, and (b) selects the\nmost representative view(s) at each event to be included in a universal\nsummary. Due to the lack of an applicable alternative, we collected a new\nmulti-view egocentric dataset, Multi-Ego. Our dataset is recorded\nsimultaneously by three cameras, covering a wide variety of real-life\nscenarios. The footage is annotated by multiple individuals under various\nsummarization configurations, with a consensus analysis ensuring a reliable\nground truth. We conduct extensive experiments on the compiled dataset in\naddition to three other standard benchmarks that show the robustness and the\nadvantage of our approach in both supervised and unsupervised settings.\nAdditionally, we show that our approach learns collectively from data of varied\nnumber-of-views and orthogonal to other summarization methods, deeming it\nscalable and generic.\n", "rewritten_text": "The explosion of online video necessitates efficient summarization techniques.  However, the proliferation of social and egocentric cameras generates numerous sparsely overlapping video streams requiring joint summarization. This paper addresses this challenge by proposing a robust framework for summarizing videos from multiple independently moving cameras with intermittent shared viewpoints.  This framework identifies key events across these diverse perspectives and selects the most representative views for a unified summary.  To facilitate research, we introduce Multi-Ego, a novel multi-view egocentric dataset recorded simultaneously from three cameras across various real-world scenarios, with reliably annotated summaries.  Extensive experiments on Multi-Ego and three established benchmarks demonstrate our approach's robustness and superiority in both supervised and unsupervised settings, showcasing its scalability and generalizability across varying numbers of viewpoints and its independence from other summarization methods.\n"}, "2012.07791": {"original_text": "  We propose real-time, six degrees of freedom (6DoF), 3D face pose estimation\nwithout face detection or landmark localization. We observe that estimating the\n6DoF rigid transformation of a face is a simpler problem than facial landmark\ndetection, often used for 3D face alignment. In addition, 6DoF offers more\ninformation than face bounding box labels. We leverage these observations to\nmake multiple contributions: (a) We describe an easily trained, efficient,\nFaster R-CNN--based model which regresses 6DoF pose for all faces in the photo,\nwithout preliminary face detection. (b) We explain how pose is converted and\nkept consistent between the input photo and arbitrary crops created while\ntraining and evaluating our model. (c) Finally, we show how face poses can\nreplace detection bounding box training labels. Tests on AFLW2000-3D and BIWI\nshow that our method runs at real-time and outperforms state of the art (SotA)\nface pose estimators. Remarkably, our method also surpasses SotA models of\ncomparable complexity on the WIDER FACE detection benchmark, despite not been\noptimized on bounding box labels.\n", "rewritten_text": "This paper introduces a novel real-time 6DoF 3D face pose estimation method that bypasses traditional face detection and landmark localization.  We argue that directly estimating 6DoF pose is simpler and provides richer information than landmark detection or bounding box estimation.  Our contributions include: (a) a fast, easily trained Faster R-CNN based model for direct 6DoF pose regression; (b) a method for consistent pose representation across different image crops during training and evaluation; and (c) a demonstration of using pose estimates to replace bounding box labels.  Experiments on AFLW2000-3D and BIWI datasets show real-time performance surpassing state-of-the-art pose estimators.  Furthermore, our method unexpectedly outperforms comparable detection models on the WIDER FACE benchmark, despite not being explicitly trained for bounding box detection.\n"}, "2208.05909": {"original_text": "  Preservation of domain knowledge from the source to target is crucial in any\ntranslation workflow. It is common in the translation industry to receive\nhighly specialized projects, where there is hardly any parallel in-domain data.\nIn such scenarios where there is insufficient in-domain data to fine-tune\nMachine Translation (MT) models, producing translations that are consistent\nwith the relevant context is challenging. In this work, we propose a novel\napproach to domain adaptation leveraging state-of-the-art pretrained language\nmodels (LMs) for domain-specific data augmentation for MT, simulating the\ndomain characteristics of either (a) a small bilingual dataset, or (b) the\nmonolingual source text to be translated. Combining this idea with\nback-translation, we can generate huge amounts of synthetic bilingual in-domain\ndata for both use cases. For our investigation, we use the state-of-the-art\nTransformer architecture. We employ mixed fine-tuning to train models that\nsignificantly improve translation of in-domain texts. More specifically, in\nboth scenarios, our proposed methods achieve improvements of approximately 5-6\nBLEU and 2-3 BLEU, respectively, on the Arabic-to-English and English-to-Arabic\nlanguage pairs. Furthermore, the outcome of human evaluation corroborates the\nautomatic evaluation results.\n", "rewritten_text": "Accurate translation requires preserving specialized knowledge.  Machine translation (MT) often struggles with highly specialized texts lacking parallel training data.  This paper introduces a novel domain adaptation method using state-of-the-art pretrained language models (LMs) to augment data for MT.  This augmentation simulates either a small bilingual dataset or the monolingual source text, generating large amounts of synthetic in-domain data via back-translation.  Using a Transformer architecture and mixed fine-tuning, our approach significantly improves translation quality, achieving approximately 5-6 BLEU improvement for Arabic-to-English and 2-3 BLEU for English-to-Arabic, confirmed by both automatic and human evaluation.\n"}, "1811.09745": {"original_text": "  We propose a new geometric regularization principle for reconstructing vector\nfields based on prior knowledge about their divergence. As one important\nexample of this general idea, we focus on vector fields modelling blood flow\npattern that should be divergent in arteries and convergent in veins. We show\nthat this previously ignored regularization constraint can significantly\nimprove the quality of vessel tree reconstruction particularly around\nbifurcations where non-zero divergence is concentrated. Our divergence prior is\ncritical for resolving (binary) sign ambiguity in flow orientations produced by\nstandard vessel filters, e.g. Frangi. Our vessel tree centerline reconstruction\ncombines divergence constraints with robust curvature regularization. Our\nunsupervised method can reconstruct complete vessel trees with near-capillary\ndetails on synthetic and real 3D volumes.\n", "rewritten_text": "This paper introduces a novel geometric regularization method for reconstructing vector fields, leveraging prior knowledge of their divergence.  Applied to blood flow modeling, where divergence is expected in arteries and convergence in veins, this method significantly enhances vessel tree reconstruction, especially at bifurcations.  By incorporating divergence information, we address the directional ambiguity inherent in standard filters like Frangi, improving the accuracy of binary flow orientation.  Our unsupervised approach, combining divergence constraints with curvature regularization, reconstructs detailed, complete vessel trees, including near-capillary structures, from both synthetic and real 3D datasets.\n"}, "2104.08575": {"original_text": "  Super-resolution (SR) is an ill-posed problem, which means that infinitely\nmany high-resolution (HR) images can be degraded to the same low-resolution\n(LR) image. To study the one-to-many stochastic SR mapping, we implicitly\nrepresent the non-local self-similarity of natural images and develop a\nVariational Sparse framework for Super-Resolution (VSpSR) via neural networks.\nSince every small patch of a HR image can be well approximated by the sparse\nrepresentation of atoms in an over-complete dictionary, we design a two-branch\nmodule, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM\nextracts patch-level basis from the LR input, and the other branch infers\npixel-wise variational distributions with respect to the sparse coefficients.\nBy repeatedly sampling coefficients, we could obtain infinite sparse\nrepresentations, and thus generate diverse HR images. According to the\npreliminary results of NTIRE 2021 challenge on learning SR space, our team\n(FudanZmic21) ranks 7-th in terms of released scores. The implementation of\nVSpSR is released at https://zmiclab.github.io/.\n", "rewritten_text": "Super-resolution (SR) is an inherently ambiguous problem, as numerous high-resolution (HR) images can produce the same low-resolution (LR) image.  To address this, we propose VSpSR, a variational sparse framework for SR using neural networks.  VSpSR leverages the non-local self-similarity of natural images by employing a two-branch module (VSpM) that extracts patch-level basis from the LR input and infers pixel-wise variational distributions for sparse coefficients.  This allows for the generation of diverse HR images through repeated coefficient sampling.  Our method achieved a 7th-place ranking in the NTIRE 2021 SR challenge (team FudanZmic21).  The code is available at https://zmiclab.github.io/.\n"}, "2206.12505": {"original_text": "  We propose a novel semi-supervised learning approach for classification of\nhistopathology images. We employ strong supervision with patch-level\nannotations combined with a novel co-training loss to create a semi-supervised\nlearning framework. Co-training relies on multiple conditionally independent\nand sufficient views of the data. We separate the hematoxylin and eosin\nchannels in pathology images using color deconvolution to create two views of\neach slide that can partially fulfill these requirements. Two separate CNNs are\nused to embed the two views into a joint feature space. We use a contrastive\nloss between the views in this feature space to implement co-training. We\nevaluate our approach in clear cell renal cell and prostate carcinomas, and\ndemonstrate improvement over state-of-the-art semi-supervised learning methods.\n", "rewritten_text": "This paper introduces a new semi-supervised method for classifying histopathology images.  Leveraging patch-level annotations and a novel co-training loss, our framework uses color deconvolution to separate hematoxylin and eosin channels, creating two independent views of each image.  Two convolutional neural networks (CNNs) independently embed these views into a shared feature space, where a contrastive loss enforces consistency between them.  Experiments on clear cell renal cell and prostate carcinomas demonstrate superior performance compared to existing semi-supervised techniques.\n"}, "2303.08714": {"original_text": "  Adapting the Diffusion Probabilistic Model (DPM) for direct image\nsuper-resolution is wasteful, given that a simple Convolutional Neural Network\n(CNN) can recover the main low-frequency content. Therefore, we present\nResDiff, a novel Diffusion Probabilistic Model based on Residual structure for\nSingle Image Super-Resolution (SISR). ResDiff utilizes a combination of a CNN,\nwhich restores primary low-frequency components, and a DPM, which predicts the\nresidual between the ground-truth image and the CNN predicted image. In\ncontrast to the common diffusion-based methods that directly use LR images to\nguide the noise towards HR space, ResDiff utilizes the CNN's initial prediction\nto direct the noise towards the residual space between HR space and\nCNN-predicted space, which not only accelerates the generation process but also\nacquires superior sample quality. Additionally, a frequency-domain-based loss\nfunction for CNN is introduced to facilitate its restoration, and a\nfrequency-domain guided diffusion is designed for DPM on behalf of predicting\nhigh-frequency details. The extensive experiments on multiple benchmark\ndatasets demonstrate that ResDiff outperforms previous diffusion based methods\nin terms of shorter model convergence time, superior generation quality, and\nmore diverse samples.\n", "rewritten_text": "Directly applying Diffusion Probabilistic Models (DPMs) to image super-resolution is inefficient, as a Convolutional Neural Network (CNN) can effectively capture low-frequency information.  We propose ResDiff, a novel residual-based DPM for single image super-resolution (SISR). ResDiff leverages a CNN to predict the low-frequency components, and a DPM to refine this prediction by modeling the residual between the CNN output and the high-resolution (HR) ground truth.  Unlike existing diffusion methods, ResDiff guides the noise towards this residual space, leading to faster generation and improved sample quality.  Furthermore, we introduce a frequency-domain loss for the CNN and a frequency-guided diffusion process for the DPM to enhance high-frequency detail recovery.  Extensive experiments show ResDiff surpasses existing diffusion-based methods in convergence speed, image quality, and sample diversity.\n"}, "1607.06797": {"original_text": "  In this paper we proposed an ordered patch based method using Conditional\nRandom Field (CRF) in order to encode local properties and their spatial\nrelationship in images to address texture classification, face recognition, and\nscene classification problems. Typical image classification approaches work\nwithout considering spatial causality among distinctive properties of an image\nfor image representation in feature space. In this method first, each image is\nencoded as a sequence of ordered patches, including local properties. Second,\nthe sequence of these ordered patches is modeled as a probabilistic feature\nvector by CRF to model spatial relationship of these local properties. And\nfinally, image classification is performed on such probabilistic image\nrepresentation. Experimental results on several standard image datasets\nindicate that proposed method outperforms some of existing image classification\nmethods.\n", "rewritten_text": "This paper introduces a novel ordered patch-based method for image classification that leverages Conditional Random Fields (CRFs) to capture both local image properties and their spatial relationships.  Unlike traditional methods that ignore spatial context, our approach encodes images as sequences of ordered patches, then uses a CRF to model these patches as a probabilistic feature vector reflecting their spatial dependencies.  This probabilistic representation is subsequently used for classification.  Experiments on standard datasets demonstrate superior performance compared to existing methods.\n"}, "2305.15929": {"original_text": "  Current large language models, such as OpenAI's ChatGPT, have captured the\npublic's attention because how remarkable they are in the use of language.\nHere, I demonstrate that ChatGPT displays phonological biases that are a\nhallmark of human language processing. More concretely, just like humans,\nChatGPT has a consonant bias. That is, the chatbot has a tendency to use\nconsonants over vowels to identify words. This is observed across languages\nthat differ in their relative distribution of consonants and vowels such as\nEnglish and Spanish. Despite the differences in how current artificial\nintelligence language models are trained to process linguistic stimuli and how\nhuman infants acquire language, such training seems to be enough for the\nemergence of a phonological bias in ChatGPT\n", "rewritten_text": "Large language models like ChatGPT have impressed the public with their linguistic abilities.  This paper demonstrates that ChatGPT, mirroring human language processing, exhibits a consonant bias: a preference for consonants over vowels in word identification. This bias is consistent across languages with varying consonant-vowel ratios, such as English and Spanish.  Remarkably, despite differences in how these models are trained versus how humans learn language,  ChatGPT's training appears sufficient to induce this human-like phonological bias.\n"}, "2008.13196": {"original_text": "  The task of spatial-temporal action detection has attracted increasing\nattention among researchers. Existing dominant methods solve this problem by\nrelying on short-term information and dense serial-wise detection on each\nindividual frames or clips. Despite their effectiveness, these methods showed\ninadequate use of long-term information and are prone to inefficiency. In this\npaper, we propose for the first time, an efficient framework that generates\naction tube proposals from video streams with a single forward pass in a\nsparse-to-dense manner. There are two key characteristics in this framework:\n(1) Both long-term and short-term sampled information are explicitly utilized\nin our spatiotemporal network, (2) A new dynamic feature sampling module (DTS)\nis designed to effectively approximate the tube output while keeping the system\ntractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 and\nUCFSports benchmark datasets, achieving promising results that are competitive\nto state-of-the-art methods. The proposed sparse-to-dense strategy rendered our\nframework about 7.6 times more efficient than the nearest competitor.\n", "rewritten_text": "Spatial-temporal action detection is a growing research area.  Current leading methods rely on short-term information and frame-by-frame processing, limiting their efficiency and long-term context understanding.  This paper introduces a novel, efficient framework that generates action tube proposals from video streams using a single, sparse-to-dense forward pass.  This framework uniquely integrates both short-term and long-term information via a spatiotemporal network and employs a new dynamic feature sampling module (DTS) to maintain efficiency.  Evaluated on UCF101-24, JHMDB-21, and UCFSports, our method achieves state-of-the-art performance, demonstrating a 7.6x speed improvement over the fastest comparable method.\n"}, "2010.07574": {"original_text": "  Evaluation of grammatical error correction (GEC) systems has primarily\nfocused on essays written by non-native learners of English, which however is\nonly part of the full spectrum of GEC applications. We aim to broaden the\ntarget domain of GEC and release CWEB, a new benchmark for GEC consisting of\nwebsite text generated by English speakers of varying levels of proficiency.\nWebsite data is a common and important domain that contains far fewer\ngrammatical errors than learner essays, which we show presents a challenge to\nstate-of-the-art GEC systems. We demonstrate that a factor behind this is the\ninability of systems to rely on a strong internal language model in low error\ndensity domains. We hope this work shall facilitate the development of\nopen-domain GEC models that generalize to different topics and genres.\n", "rewritten_text": "Current grammatical error correction (GEC) system evaluation primarily uses non-native English learner essays.  To address this limited scope, we introduce CWEB, a new benchmark dataset of website text written by native and non-native English speakers.  Unlike learner corpora, website text has a lower error density, posing a significant challenge to existing GEC systems.  Our analysis reveals that this challenge stems from the systems' over-reliance on strong internal language models in high-error-density scenarios.  CWEB aims to foster the development of more robust, open-domain GEC models capable of handling diverse text types and proficiency levels.\n"}, "2306.0045": {"original_text": "  Semantic segmentation is a crucial task in computer vision that involves\nsegmenting images into semantically meaningful regions at the pixel level.\nHowever, existing approaches often rely on expensive human annotations as\nsupervision for model training, limiting their scalability to large, unlabeled\ndatasets. To address this challenge, we present ZeroSeg, a novel method that\nleverages the existing pretrained vision-language (VL) model (e.g. CLIP) to\ntrain open-vocabulary zero-shot semantic segmentation models. Although acquired\nextensive knowledge of visual concepts, it is non-trivial to exploit knowledge\nfrom these VL models to the task of semantic segmentation, as they are usually\ntrained at an image level. ZeroSeg overcomes this by distilling the visual\nconcepts learned by VL models into a set of segment tokens, each summarizing a\nlocalized region of the target image. We evaluate ZeroSeg on multiple popular\nsegmentation benchmarks, including PASCAL VOC 2012, PASCAL Context, and COCO,\nin a zero-shot manner (i.e., no training or adaption on target segmentation\ndatasets). Our approach achieves state-of-the-art performance when compared to\nother zero-shot segmentation methods under the same training data, while also\nperforming competitively compared to strongly supervised methods. Finally, we\nalso demonstrated the effectiveness of ZeroSeg on open-vocabulary segmentation,\nthrough both human studies and qualitative visualizations.\n", "rewritten_text": "Semantic segmentation, crucial for computer vision, requires pixel-level image segmentation into meaningful regions.  Current methods heavily rely on expensive human-labeled data, hindering scalability.  ZeroSeg addresses this by leveraging pretrained vision-language (VL) models (like CLIP) to create open-vocabulary, zero-shot semantic segmentation models.  While VL models possess extensive visual knowledge, adapting this to pixel-level segmentation is challenging.  ZeroSeg overcomes this by distilling VL model knowledge into segment tokens representing localized image regions.  Zero-shot evaluation on PASCAL VOC 2012, PASCAL Context, and COCO benchmarks demonstrates state-of-the-art performance compared to other zero-shot methods and competitive results against strongly supervised approaches.  Furthermore, human studies and qualitative visualizations confirm ZeroSeg's effectiveness in open-vocabulary segmentation.\n"}, "2401.05166": {"original_text": "  In dyadic interactions, humans communicate their intentions and state of mind\nusing verbal and non-verbal cues, where multiple different facial reactions\nmight be appropriate in response to a specific speaker behaviour. Then, how to\ndevelop a machine learning (ML) model that can automatically generate multiple\nappropriate, diverse, realistic and synchronised human facial reactions from an\npreviously unseen speaker behaviour is a challenging task. Following the\nsuccessful organisation of the first REACT challenge (REACT 2023), this edition\nof the challenge (REACT 2024) employs a subset used by the previous challenge,\nwhich contains segmented 30-secs dyadic interaction clips originally recorded\nas part of the NOXI and RECOLA datasets, encouraging participants to develop\nand benchmark Machine Learning (ML) models that can generate multiple\nappropriate facial reactions (including facial image sequences and their\nattributes) given an input conversational partner's stimulus under various\ndyadic video conference scenarios. This paper presents: (i) the guidelines of\nthe REACT 2024 challenge; (ii) the dataset utilized in the challenge; and (iii)\nthe performance of the baseline systems on the two proposed sub-challenges:\nOffline Multiple Appropriate Facial Reaction Generation and Online Multiple\nAppropriate Facial Reaction Generation, respectively. The challenge baseline\ncode is publicly available at\nhttps://github.com/reactmultimodalchallenge/baseline_react2024.\n", "rewritten_text": "The REACT 2024 challenge focuses on developing machine learning models capable of generating diverse, realistic, and synchronized facial reactions to unseen conversational stimuli.  Building on the success of REACT 2023, this year's challenge uses a subset of the NOXI and RECOLA datasets \u2013 30-second dyadic interaction clips \u2013 to benchmark models generating multiple appropriate facial responses (including image sequences and attributes) in various video conferencing scenarios. This paper details the challenge guidelines, the dataset, and the baseline system performance on two sub-challenges: offline and online facial reaction generation.  The baseline code is publicly available at https://github.com/reactmultimodalchallenge/baseline_react2024.\n"}, "2406.10323": {"original_text": "  Most public instruction finetuning datasets are relatively small compared to\nthe closed source datasets used to train industry models. To study questions\nabout finetuning at scale, such as curricula and learning rate cooldown\nschedules, there is a need for industrial-scale datasets. However, this scale\nnecessitates a data generation process that is almost entirely automated. In\nthis work, we study methods for generating large instruction datasets from a\nsingle prompt. With little human oversight, we get LLMs to write diverse sets\nof instruction examples ranging from simple completion tasks to complex\nmulti-turn dialogs across a variety of subject areas. When finetuning a Llama-3\n8B base model, our dataset meets or exceeds both WizardLM and Ultrachat on both\nknowledge-intensive leaderboard tasks as well as conversational evaluations. We\nrelease our dataset, the \"generator\" prompts that created it, and our finetuned\nmodel checkpoints.\n", "rewritten_text": "Large language model (LLM) finetuning research is hampered by the limited size of publicly available datasets compared to those used in industry.  To address this, we developed a highly automated method for generating large instruction-following datasets from a single prompt.  This method uses LLMs to create diverse examples, from simple tasks to complex multi-turn dialogues, with minimal human intervention.  Finetuning a Llama-3 8B model with our generated dataset achieved performance exceeding or matching WizardLM and Ultrachat on both knowledge-based and conversational benchmarks.  We publicly release our dataset, the generating prompts, and the finetuned model weights.\n"}, "2303.12776": {"original_text": "  One-to-one label assignment in object detection has successfully obviated the\nneed for non-maximum suppression (NMS) as postprocessing and makes the pipeline\nend-to-end. However, it triggers a new dilemma as the widely used sparse\nqueries cannot guarantee a high recall, while dense queries inevitably bring\nmore similar queries and encounter optimization difficulties. As both sparse\nand dense queries are problematic, then what are the expected queries in\nend-to-end object detection? This paper shows that the solution should be Dense\nDistinct Queries (DDQ). Concretely, we first lay dense queries like traditional\ndetectors and then select distinct ones for one-to-one assignments. DDQ blends\nthe advantages of traditional and recent end-to-end detectors and significantly\nimproves the performance of various detectors including FCN, R-CNN, and DETRs.\nMost impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12\nepochs using a ResNet-50 backbone, outperforming all existing detectors in the\nsame setting. DDQ also shares the benefit of end-to-end detectors in crowded\nscenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchers\nto consider the complementarity between traditional methods and end-to-end\ndetectors. The source code can be found at\n\\url{https://github.com/jshilong/DDQ}.\n", "rewritten_text": "End-to-end object detection, enabled by one-to-one label assignment, eliminates the need for non-maximum suppression (NMS).  However, this approach faces a challenge: sparse queries lack recall, while dense queries lead to optimization difficulties due to redundancy.  This paper introduces Dense Distinct Queries (DDQ) as a solution.  DDQ generates dense queries initially, then selects distinct ones for one-to-one assignment, combining the strengths of traditional and end-to-end methods.  This significantly improves performance across various detectors (FCN, R-CNN, DETR), with DDQ-DETR achieving a remarkable 52.1 AP on MS-COCO (ResNet-50 backbone, 12 epochs), surpassing existing methods.  DDQ also excels in crowded scenes (93.8 AP on CrowdHuman).  The code is available at [https://github.com/jshilong/DDQ](https://github.com/jshilong/DDQ).  We believe DDQ highlights the value of integrating traditional and end-to-end detection techniques.\n"}, "2303.1112": {"original_text": "  Positional reasoning is the process of ordering unsorted parts contained in a\nset into a consistent structure. We present Positional Diffusion, a\nplug-and-play graph formulation with Diffusion Probabilistic Models to address\npositional reasoning. We use the forward process to map elements' positions in\na set to random positions in a continuous space. Positional Diffusion learns to\nreverse the noising process and recover the original positions through an\nAttention-based Graph Neural Network. We conduct extensive experiments with\nbenchmark datasets including two puzzle datasets, three sentence ordering\ndatasets, and one visual storytelling dataset, demonstrating that our method\noutperforms long-lasting research on puzzle solving with up to +18% compared to\nthe second-best deep learning method, and performs on par against the\nstate-of-the-art methods on sentence ordering and visual storytelling. Our work\nhighlights the suitability of diffusion models for ordering problems and\nproposes a novel formulation and method for solving various ordering tasks.\nProject website at https://iit-pavis.github.io/Positional_Diffusion/\n", "rewritten_text": "Positional Diffusion is a novel approach to positional reasoning\u2014the task of ordering unordered elements.  This plug-and-play graph-based method leverages Diffusion Probabilistic Models.  It maps element positions to a continuous space, then uses an attention-based Graph Neural Network to reverse this \"noising\" process and recover the original order.  Extensive experiments across diverse datasets (including puzzles, sentence ordering, and visual storytelling) show Positional Diffusion surpasses existing deep learning methods by up to 18% on puzzle solving and achieves state-of-the-art performance on sentence ordering and visual storytelling.  This work demonstrates the effectiveness of diffusion models for ordering problems and introduces a new framework for tackling various ordering tasks.  See the project website for more details: https://iit-pavis.github.io/Positional_Diffusion/\n"}, "2311.13735": {"original_text": "  Recent advances in large language models (LLMs) show potential for clinical\napplications, such as clinical decision support and trial recommendations.\nHowever, the GPT-4 LLM predicts an excessive number of ICD codes for medical\ncoding tasks, leading to high recall but low precision. To tackle this\nchallenge, we introduce LLM-codex, a two-stage approach to predict ICD codes\nthat first generates evidence proposals using an LLM and then employs an\nLSTM-based verification stage. The LSTM learns from both the LLM's high recall\nand human expert's high precision, using a custom loss function. Our model is\nthe only approach that simultaneously achieves state-of-the-art results in\nmedical coding accuracy, accuracy on rare codes, and sentence-level evidence\nidentification to support coding decisions without training on human-annotated\nevidence according to experiments on the MIMIC dataset.\n", "rewritten_text": "Large language models (LLMs) hold promise for clinical applications like decision support, but their high recall and low precision in medical coding (e.g., excessive ICD code predictions by GPT-4) hinder their effectiveness.  To address this, we present LLM-codex, a two-stage system.  First, an LLM generates potential ICD codes; second, an LSTM network verifies these predictions, learning from both the LLM's high recall and expert-provided high-precision annotations via a novel loss function.  Our results on the MIMIC dataset demonstrate that LLM-codex uniquely achieves state-of-the-art performance in medical coding accuracy, including rare code identification, and sentence-level evidence retrieval, all without requiring human-annotated evidence for training.\n"}, "2209.14575": {"original_text": "  In this paper, we investigate the problem of bit allocation in Neural Video\nCompression (NVC). First, we reveal that a recent bit allocation approach\nclaimed to be optimal is, in fact, sub-optimal due to its implementation.\nSpecifically, we find that its sub-optimality lies in the improper application\nof semi-amortized variational inference (SAVI) on latent with non-factorized\nvariational posterior. Then, we show that the corrected version of SAVI on\nnon-factorized latent requires recursively applying back-propagating through\ngradient ascent, based on which we derive the corrected optimal bit allocation\nalgorithm. Due to the computational in-feasibility of the corrected bit\nallocation, we design an efficient approximation to make it practical.\nEmpirical results show that our proposed correction significantly improves the\nincorrect bit allocation in terms of R-D performance and bitrate error, and\noutperforms all other bit allocation methods by a large margin. The source code\nis provided in the supplementary material.\n", "rewritten_text": "This paper addresses suboptimal bit allocation in Neural Video Compression (NVC). We demonstrate that a purportedly optimal method suffers from flawed implementation due to incorrect application of semi-amortized variational inference (SAVI) to non-factorized latent variables.  We correct this by deriving a new algorithm requiring recursive backpropagation through gradient ascent. While computationally expensive, we propose an efficient approximation.  Our corrected method significantly improves rate-distortion performance and bitrate accuracy, outperforming existing techniques.  Source code is available in the supplementary material.\n"}, "2302.03744": {"original_text": "  The ability to perceive and understand 3D scenes is crucial for many\napplications in computer vision and robotics. Inverse graphics is an appealing\napproach to 3D scene understanding that aims to infer the 3D scene structure\nfrom 2D images. In this paper, we introduce probabilistic modeling to the\ninverse graphics framework to quantify uncertainty and achieve robustness in 6D\npose estimation tasks. Specifically, we propose 3D Neural Embedding Likelihood\n(3DNEL) as a unified probabilistic model over RGB-D images, and develop\nefficient inference procedures on 3D scene descriptions. 3DNEL effectively\ncombines learned neural embeddings from RGB with depth information to improve\nrobustness in sim-to-real 6D object pose estimation from RGB-D images.\nPerformance on the YCB-Video dataset is on par with state-of-the-art yet is\nmuch more robust in challenging regimes. In contrast to discriminative\napproaches, 3DNEL's probabilistic generative formulation jointly models\nmultiple objects in a scene, quantifies uncertainty in a principled way, and\nhandles object pose tracking under heavy occlusion. Finally, 3DNEL provides a\nprincipled framework for incorporating prior knowledge about the scene and\nobjects, which allows natural extension to additional tasks like camera pose\ntracking from video.\n", "rewritten_text": "Accurate 3D scene understanding is vital for computer vision and robotics.  This paper presents 3D Neural Embedding Likelihood (3DNEL), a novel probabilistic inverse graphics approach for robust 6D pose estimation.  3DNEL leverages a unified model integrating RGB and depth information from RGB-D images, enabling efficient inference of 3D scene descriptions.  By combining learned neural embeddings from RGB data with depth cues, 3DNEL achieves state-of-the-art performance on the YCB-Video dataset, exhibiting superior robustness in challenging scenarios, particularly with occlusion. Unlike discriminative methods, its generative nature allows for joint modeling of multiple objects, principled uncertainty quantification, and robust object pose tracking.  Furthermore, 3DNEL's framework readily incorporates prior knowledge, facilitating extensions to tasks such as camera pose tracking.\n"}, "2005.04551": {"original_text": "  A common approach to localize 3D human joints in a synchronized and\ncalibrated multi-view setup consists of two-steps: (1) apply a 2D detector\nseparately on each view to localize joints in 2D, and (2) perform robust\ntriangulation on 2D detections from each view to acquire the 3D joint\nlocations. However, in step 1, the 2D detector is limited to solving\nchallenging cases which could potentially be better resolved in 3D, such as\nocclusions and oblique viewing angles, purely in 2D without leveraging any 3D\ninformation. Therefore, we propose the differentiable \"epipolar transformer\",\nwhich enables the 2D detector to leverage 3D-aware features to improve 2D pose\nestimation. The intuition is: given a 2D location p in the current view, we\nwould like to first find its corresponding point p' in a neighboring view, and\nthen combine the features at p' with the features at p, thus leading to a\n3D-aware feature at p. Inspired by stereo matching, the epipolar transformer\nleverages epipolar constraints and feature matching to approximate the features\nat p'. Experiments on InterHand and Human3.6M show that our approach has\nconsistent improvements over the baselines. Specifically, in the condition\nwhere no external data is used, our Human3.6M model trained with ResNet-50\nbackbone and image size 256 x 256 outperforms state-of-the-art by 4.23 mm and\nachieves MPJPE 26.9 mm.\n", "rewritten_text": "Current multi-view 3D human joint localization typically uses a two-step process: 2D joint detection in each view, followed by 3D triangulation.  However, this approach struggles with occlusions and oblique views, which are better addressed in 3D.  To overcome this limitation, we introduce the differentiable epipolar transformer. This allows the 2D detector to incorporate 3D-aware features by matching features across views using epipolar geometry.  This improves 2D pose estimation by combining features from corresponding points in neighboring views.  Experiments on InterHand and Human3.6M datasets demonstrate consistent performance gains. Notably, our ResNet-50 based model (256x256 images, no external data) surpasses the state-of-the-art on Human3.6M by 4.23 mm, achieving a Mean Per Joint Position Error (MPJPE) of 26.9 mm.\n"}, "1912.0643": {"original_text": "  Annotating videos is cumbersome, expensive and not scalable. Yet, many strong\nvideo models still rely on manually annotated data. With the recent\nintroduction of the HowTo100M dataset, narrated videos now offer the\npossibility of learning video representations without manual supervision. In\nthis work we propose a new learning approach, MIL-NCE, capable of addressing\nmisalignments inherent to narrated videos. With this approach we are able to\nlearn strong video representations from scratch, without the need for any\nmanual annotation. We evaluate our representations on a wide range of four\ndownstream tasks over eight datasets: action recognition (HMDB-51, UCF-101,\nKinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization\n(YouTube-8M Segments, CrossTask) and action segmentation (COIN). Our method\noutperforms all published self-supervised approaches for these tasks as well as\nseveral fully supervised baselines.\n", "rewritten_text": "Manually annotating videos is costly, laborious, and limits scalability, despite its prevalence in training high-performing video models.  This paper introduces MIL-NCE, a novel self-supervised learning approach leveraging the narrated HowTo100M dataset to overcome this limitation.  MIL-NCE addresses inherent misalignments in narrated video data, enabling the learning of robust video representations without manual annotation.  Extensive evaluation across eight datasets and four downstream tasks (action recognition, text-to-video retrieval, action localization, and action segmentation) demonstrates superior performance compared to existing self-supervised and several fully supervised methods.\n"}, "2211.09795": {"original_text": "  Diffusion models have become the go-to method for many generative tasks,\nparticularly for image-to-image generation tasks such as super-resolution and\ninpainting. Current diffusion-based methods do not provide statistical\nguarantees regarding the generated results, often preventing their use in\nhigh-stakes situations. To bridge this gap, we construct a confidence interval\naround each generated pixel such that the true value of the pixel is guaranteed\nto fall within the interval with a probability set by the user. Since diffusion\nmodels parametrize the data distribution, a straightforward way of constructing\nsuch intervals is by drawing multiple samples and calculating their bounds.\nHowever, this method has several drawbacks: i) slow sampling speeds ii)\nsuboptimal bounds iii) requires training a diffusion model per task. To\nmitigate these shortcomings we propose Conffusion, wherein we fine-tune a\npre-trained diffusion model to predict interval bounds in a single forward\npass. We show that Conffusion outperforms the baseline method while being three\norders of magnitude faster.\n", "rewritten_text": "Diffusion models dominate generative tasks, especially image-to-image applications like super-resolution and inpainting.  However, their lack of statistical guarantees hinders their use in critical applications.  To address this, we introduce Conffusion, a method that generates confidence intervals around each pixel in a generated image, guaranteeing the true pixel value falls within the interval with user-specified probability. While directly sampling multiple times to estimate these intervals is slow, produces suboptimal bounds, and requires task-specific model training, Conffusion efficiently fine-tunes a pre-trained diffusion model to predict these bounds in a single forward pass.  Our results demonstrate that Conffusion is three orders of magnitude faster than the sampling baseline while achieving superior accuracy.\n"}, "2404.14248": {"original_text": "  This paper reviews the NTIRE 2024 low light image enhancement challenge,\nhighlighting the proposed solutions and results. The aim of this challenge is\nto discover an effective network design or solution capable of generating\nbrighter, clearer, and visually appealing results when dealing with a variety\nof conditions, including ultra-high resolution (4K and beyond), non-uniform\nillumination, backlighting, extreme darkness, and night scenes. A notable total\nof 428 participants registered for the challenge, with 22 teams ultimately\nmaking valid submissions. This paper meticulously evaluates the\nstate-of-the-art advancements in enhancing low-light images, reflecting the\nsignificant progress and creativity in this field.\n", "rewritten_text": "The NTIRE 2024 low-light image enhancement challenge attracted 428 participants, with 22 submitting valid solutions.  This paper analyzes these submissions, showcasing the innovative approaches used to improve the brightness, clarity, and visual appeal of low-light images, particularly those with high resolutions (4K+), uneven lighting, backlighting, and extreme darkness.  The review highlights significant advancements in the field of low-light image enhancement.\n"}, "2311.01907": {"original_text": "  Automatic simplification can help laypeople to comprehend complex scientific\ntext. Language models are frequently applied to this task by translating from\ncomplex to simple language. In this paper, we describe our system based on\nLlama 2, which ranked first in the PLABA shared task addressing the\nsimplification of biomedical text. We find that the large portion of shared\ntokens between input and output leads to weak training signals and\nconservatively editing models. To mitigate these issues, we propose\nsentence-level and token-level loss weights. They give higher weight to\nmodified tokens, indicated by edit distance and edit operations, respectively.\nWe conduct an empirical evaluation on the PLABA dataset and find that both\napproaches lead to simplifications closer to those created by human annotators\n(+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x /\n1.8x edit distance) compared to the same model fine-tuned with standard cross\nentropy. We furthermore show that the hyperparameter $\\lambda$ in token-level\nloss weights can be used to control the edit distance and the simplicity level\n(FKGL).\n", "rewritten_text": "This paper presents a Llama 2-based system for simplifying biomedical text, achieving top performance in the PLABA shared task.  We address the challenge of weak training signals caused by high input-output token overlap in existing simplification models.  Our solution uses novel sentence- and token-level loss weighting schemes that prioritize modified tokens, identified via edit distance and operations.  Evaluation on the PLABA dataset demonstrates that these methods yield significantly improved simplification, producing outputs closer to human-generated simplifications (+1.8%/+3.5% SARI), simpler language (-1/-1.1 FKGL), and more edits (1.6x/1.8x edit distance) compared to a standard cross-entropy approach.  Furthermore, we show that a hyperparameter ($\\lambda$) controls the trade-off between edit distance and simplification level (FKGL).\n"}, "2402.12923": {"original_text": "  In recent years, 3D point clouds (PCs) have gained significant attention due\nto their diverse applications across various fields, such as computer vision\n(CV), condition monitoring (CM), virtual reality, robotics, autonomous driving,\netc. Deep learning (DL) has proven effective in leveraging 3D PCs to address\nvarious challenges encountered in 2D vision. However, applying deep neural\nnetworks (DNNs) to process 3D PCs presents unique challenges. This paper\nprovides an in-depth review of recent advancements in DL-based industrial CM\nusing 3D PCs, with a specific focus on defect shape classification and\nsegmentation within industrial applications. Recognizing the crucial role of\nthese aspects in industrial maintenance, the paper offers insightful\nobservations on the strengths and limitations of the reviewed DL-based PC\nprocessing methods. This knowledge synthesis aims to contribute to\nunderstanding and enhancing CM processes, particularly within the framework of\nremaining useful life (RUL), in industrial systems.\n", "rewritten_text": "This paper reviews recent advancements in deep learning (DL) for industrial condition monitoring (CM) using 3D point clouds (PCs).  Focusing on defect shape classification and segmentation, it examines the strengths and limitations of DL-based PC processing methods relevant to industrial applications.  The increasing use of 3D PCs across diverse fields, including computer vision and robotics, motivates this study, which addresses the unique challenges of applying deep neural networks (DNNs) to 3D data.  Ultimately, this review aims to improve understanding and enhance CM processes, particularly in predicting remaining useful life (RUL) of industrial systems.\n"}, "2402.02082": {"original_text": "  Speculative decoding is a relatively new decoding framework that leverages\nsmall and efficient draft models to reduce the latency of LLMs. In this study,\nwe introduce GliDe and CaPE, two low-hassle modifications to vanilla\nspeculative decoding to further improve the decoding speed of a frozen LLM.\nSpecifically, GliDe is a modified draft model architecture that reuses the\ncached keys and values from the target LLM, while CaPE is a proposal expansion\nmethod that uses the draft model's confidence scores to help select additional\ncandidate tokens for verification. Extensive experiments on different\nbenchmarks demonstrate that our proposed GliDe draft model significantly\nreduces the expected decoding latency. Additional evaluation using walltime\nreveals that GliDe can accelerate Vicuna models up to 2.17x and further extend\nthe improvement to 2.61x with CaPE. We will release our code, data, and the\ntrained draft models.\n", "rewritten_text": "This paper presents GliDe and CaPE, two novel enhancements to speculative decoding, a recent framework for accelerating large language model (LLM) inference.  GliDe, a redesigned draft model architecture, leverages cached key-value pairs from the target LLM to improve efficiency.  CaPE, a complementary proposal expansion method, utilizes the draft model's confidence scores to refine token selection.  Our experiments show GliDe significantly reduces decoding latency, achieving up to a 2.17x speedup for Vicuna models, further boosted to 2.61x with the addition of CaPE.  Code, data, and trained models will be publicly released.\n"}, "2302.14354": {"original_text": "  The cultural heritage buildings (CHB), which are part of mankind's history\nand identity, are in constant danger of damage or in extreme situations total\ndestruction. That being said, it's of utmost importance to preserve them by\nidentifying the existent, or presumptive, defects using novel methods so that\nrenovation processes can be done in a timely manner and with higher accuracy.\nThe main goal of this research is to use new deep learning (DL) methods in the\nprocess of preserving CHBs (situated in Iran); a goal that has been neglected\nespecially in developing countries such as Iran, as these countries still\npreserve their CHBs using manual, and even archaic, methods that need direct\nhuman supervision. Having proven their effectiveness and performance when it\ncomes to processing images, the convolutional neural networks (CNN) are a\nstaple in computer vision (CV) literacy and this paper is not exempt. When\nlacking enough CHB images, training a CNN from scratch would be very difficult\nand prone to overfitting; that's why we opted to use a technique called\ntransfer learning (TL) in which we used pre-trained ResNet, MobileNet, and\nInception networks, for classification. Even more, the Grad-CAM was utilized to\nlocalize the defects to some extent. The final results were very favorable\nbased on those of similar research. The final proposed model can pave the way\nfor moving from manual to unmanned CHB conservation, hence an increase in\naccuracy and a decrease in human-induced errors.\n", "rewritten_text": "Cultural heritage buildings (CHBs) worldwide face constant threats of damage and destruction.  Their preservation is crucial, demanding timely and accurate identification of defects. This research addresses this need, particularly in developing countries like Iran, where traditional methods are often insufficient.  We leverage the power of deep learning, specifically convolutional neural networks (CNNs), to automate CHB defect detection.  Due to limited available CHB imagery, we employed transfer learning with pre-trained ResNet, MobileNet, and Inception networks for classification, further enhancing localization with Grad-CAM.  Our results compare favorably to existing research, demonstrating a promising path towards automated CHB conservation, reducing human error and improving accuracy.\n"}, "2303.0149": {"original_text": "  Language identification is an important first step in many IR and NLP\napplications. Most publicly available language identification datasets,\nhowever, are compiled under the assumption that the gold label of each instance\nis determined by where texts are retrieved from. Research has shown that this\nis a problematic assumption, particularly in the case of very similar languages\n(e.g., Croatian and Serbian) and national language varieties (e.g., Brazilian\nand European Portuguese), where texts may contain no distinctive marker of the\nparticular language or variety. To overcome this important limitation, this\npaper presents DSL True Labels (DSL-TL), the first human-annotated multilingual\ndataset for language variety identification. DSL-TL contains a total of 12,900\ninstances in Portuguese, split between European Portuguese and Brazilian\nPortuguese; Spanish, split between Argentine Spanish and Castilian Spanish; and\nEnglish, split between American English and British English. We trained\nmultiple models to discriminate between these language varieties, and we\npresent the results in detail. The data and models presented in this paper\nprovide a reliable benchmark toward the development of robust and fairer\nlanguage variety identification systems. We make DSL-TL freely available to the\nresearch community.\n", "rewritten_text": "Accurate language identification is crucial for many information retrieval and natural language processing applications.  Existing public datasets often rely on the unreliable assumption that a text's origin accurately reflects its language. This is especially flawed for closely related languages (e.g., Croatian/Serbian, Brazilian/European Portuguese), which may lack clear distinguishing features.  To address this, we introduce DSL True Labels (DSL-TL), the first human-annotated multilingual dataset for language *variety* identification.  DSL-TL comprises 12,900 instances of Portuguese (European and Brazilian), Spanish (Argentine and Castilian), and English (American and British). We trained and evaluated several models on this dataset, providing a robust benchmark for fairer and more accurate language variety identification systems.  DSL-TL is publicly available for research use.\n"}, "2004.0827": {"original_text": "  In this paper, we tackle the task of automatically analyzing 3D volumetric\nscans obtained from computed tomography (CT) devices. In particular, we address\na particular task for which data is very limited: the segmentation of ancient\nEgyptian mummies CT scans. We aim at digitally unwrapping the mummy and\nidentify different segments such as body, bandages and jewelry. The problem is\ncomplex because of the lack of annotated data for the different semantic\nregions to segment, thus discouraging the use of strongly supervised\napproaches. We, therefore, propose a weakly supervised and efficient\ninteractive segmentation method to solve this challenging problem. After\nsegmenting the wrapped mummy from its exterior region using histogram analysis\nand template matching, we first design a voxel distance measure to find an\napproximate solution for the body and bandage segments. Here, we use geodesic\ndistances since voxel features as well as spatial relationship among voxels is\nincorporated in this measure. Next, we refine the solution using a GrabCut\nbased segmentation together with a tracking method on the slices of the scan\nthat assigns labels to different regions in the volume, using limited\nsupervision in the form of scribbles drawn by the user. The efficiency of the\nproposed method is demonstrated using visualizations and validated through\nquantitative measures and qualitative unwrapping of the mummy.\n", "rewritten_text": "This paper presents a novel weakly supervised method for segmenting 3D CT scans of ancient Egyptian mummies, a task hampered by limited annotated data.  Our approach addresses the challenge of digitally unwrapping mummies to identify distinct segments (body, bandages, jewelry).  We begin by separating the mummy from its surroundings using histogram analysis and template matching.  A voxel distance measure, leveraging geodesic distances to incorporate voxel features and spatial relationships, provides an initial segmentation of the body and bandages.  This is refined using a GrabCut-based segmentation with a slice-wise tracking method guided by minimal user-provided scribbles.  The method's efficiency and accuracy are demonstrated through visualizations, quantitative metrics, and qualitative mummy unwrapping results.\n"}, "1607.06871": {"original_text": "  The \"interpretation through synthesis\" approach to analyze face images,\nparticularly Active Appearance Models (AAMs) method, has become one of the most\nsuccessful face modeling approaches over the last two decades. AAM models have\nability to represent face images through synthesis using a controllable\nparameterized Principal Component Analysis (PCA) model. However, the accuracy\nand robustness of the synthesized faces of AAM are highly depended on the\ntraining sets and inherently on the generalizability of PCA subspaces. This\npaper presents a novel Deep Appearance Models (DAMs) approach, an efficient\nreplacement for AAMs, to accurately capture both shape and texture of face\nimages under large variations. In this approach, three crucial components\nrepresented in hierarchical layers are modeled using the Deep Boltzmann\nMachines (DBM) to robustly capture the variations of facial shapes and\nappearances. DAMs are therefore superior to AAMs in inferencing a\nrepresentation for new face images under various challenging conditions. The\nproposed approach is evaluated in various applications to demonstrate its\nrobustness and capabilities, i.e. facial super-resolution reconstruction,\nfacial off-angle reconstruction or face frontalization, facial occlusion\nremoval and age estimation using challenging face databases, i.e. Labeled Face\nParts in the Wild (LFPW), Helen and FG-NET. Comparing to AAMs and other deep\nlearning based approaches, the proposed DAMs achieve competitive results in\nthose applications, thus this showed their advantages in handling occlusions,\nfacial representation, and reconstruction.\n", "rewritten_text": "Active Appearance Models (AAMs), a leading face modeling technique based on synthesis via Principal Component Analysis (PCA), have limitations in accuracy and robustness due to their dependence on training data and the generalizability of PCA subspaces.  This paper introduces Deep Appearance Models (DAMs), a novel alternative leveraging Deep Boltzmann Machines (DBMs) to efficiently and robustly model facial shape and texture variations across diverse conditions.  DAMs achieve this through a hierarchical representation of three crucial components.  Evaluated on challenging datasets (LFPW, Helen, FG-NET) for applications such as super-resolution, off-angle reconstruction, occlusion removal, and age estimation, DAMs demonstrate competitive performance against AAMs and other deep learning methods, showcasing superior handling of occlusions and improved facial representation and reconstruction.\n"}, "2105.11872": {"original_text": "  Despite recent advances, standard sequence labeling systems often fail when\nprocessing noisy user-generated text or consuming the output of an Optical\nCharacter Recognition (OCR) process. In this paper, we improve the noise-aware\ntraining method by proposing an empirical error generation approach that\nemploys a sequence-to-sequence model trained to perform translation from\nerror-free to erroneous text. Using an OCR engine, we generated a large\nparallel text corpus for training and produced several real-world noisy\nsequence labeling benchmarks for evaluation. Moreover, to overcome the data\nsparsity problem that exacerbates in the case of imperfect textual input, we\nlearned noisy language model-based embeddings. Our approach outperformed the\nbaseline noise generation and error correction techniques on the erroneous\nsequence labeling data sets. To facilitate future research on robustness, we\nmake our code, embeddings, and data conversion scripts publicly available.\n", "rewritten_text": "Standard sequence labeling models struggle with noisy text, such as user-generated content or OCR output.  This paper introduces an improved noise-aware training method using a sequence-to-sequence model to generate synthetically noisy text from clean text.  We created a large parallel corpus using an OCR engine and several benchmark datasets reflecting real-world noise.  To address data sparsity, we developed noisy language model embeddings.  Our method surpasses existing noise generation and correction techniques on these benchmark datasets.  For reproducibility, our code, embeddings, and data scripts are publicly available.\n"}, "2102.00062": {"original_text": "  In this paper, we present a method of clothes retargeting; generating the\npotential poses and deformations of a given 3D clothing template model to fit\nonto a person in a single RGB image. The problem is fundamentally ill-posed as\nattaining the ground truth data is impossible, i.e., images of people wearing\nthe different 3D clothing template model at exact same pose. We address this\nchallenge by utilizing large-scale synthetic data generated from physical\nsimulation, allowing us to map 2D dense body pose to 3D clothing deformation.\nWith the simulated data, we propose a semi-supervised learning framework that\nvalidates the physical plausibility of the 3D deformation by matching with the\nprescribed body-to-cloth contact points and clothing silhouette to fit onto the\nunlabeled real images. A new neural clothes retargeting network (CRNet) is\ndesigned to integrate the semi-supervised retargeting task in an end-to-end\nfashion. In our evaluation, we show that our method can predict the realistic\n3D pose and deformation field needed for retargeting clothes models in\nreal-world examples.\n", "rewritten_text": "This paper introduces a novel method for clothing retargeting, adapting 3D clothing models to fit individuals in single RGB images.  This is a challenging, ill-posed problem due to the impossibility of obtaining ground truth data \u2013 images of the same person wearing the same 3D model in identical poses.  We overcome this limitation by leveraging a large synthetic dataset generated through physics simulation, enabling us to map 2D body pose to 3D clothing deformation.  A semi-supervised learning framework, implemented in a new neural network (CRNet), then refines these deformations, ensuring physical plausibility by enforcing body-cloth contact constraints and silhouette matching against unlabeled real images.  Our results demonstrate the ability of CRNet to accurately predict realistic 3D clothing poses and deformations for real-world retargeting.\n"}, "1810.0578": {"original_text": "  Pose estimation is a widely explored problem, enabling many robotic tasks\nsuch as grasping and manipulation. In this paper, we tackle the problem of pose\nestimation for objects that exhibit rotational symmetry, which are common in\nman-made and industrial environments. In particular, our aim is to infer poses\nfor objects not seen at training time, but for which their 3D CAD models are\navailable at test time. Previous work has tackled this problem by learning to\ncompare captured views of real objects with the rendered views of their 3D CAD\nmodels, by embedding them in a joint latent space using neural networks. We\nshow that sidestepping the issue of symmetry in this scenario during training\nleads to poor performance at test time. We propose a model that reasons about\nrotational symmetry during training by having access to only a small set of\nsymmetry-labeled objects, whereby exploiting a large collection of unlabeled\nCAD models. We demonstrate that our approach significantly outperforms a\nnaively trained neural network on a new pose dataset containing images of tools\nand hardware.\n", "rewritten_text": "Pose estimation, crucial for robotic tasks like grasping, is challenging for rotationally symmetric objects prevalent in industrial settings.  This paper addresses pose estimation of unseen objects (i.e., objects not present during training) for which 3D CAD models are available. While existing methods compare real and rendered views in a shared latent space, we show that ignoring rotational symmetry during training hinders performance.  Our novel approach incorporates symmetry reasoning during training using a small set of labeled symmetric objects and a large unlabeled CAD model dataset.  Experiments on a new dataset of tools and hardware demonstrate significant performance improvements over a standard neural network approach.\n"}, "2012.15832": {"original_text": "  Increasing the input length has been a driver of progress in language\nmodeling with transformers. We identify conditions where shorter inputs are not\nharmful, and achieve perplexity and efficiency improvements through two new\nmethods that decrease input length. First, we show that initially training a\nmodel on short subsequences before moving on to longer ones both reduces\noverall training time and, surprisingly, substantially improves perplexity.\nSecond, we show how to improve the efficiency of recurrence methods in\ntransformers, which let models condition on previously processed tokens when\ngenerating sequences that exceed the maximal length the transformer can handle\nat once. Existing methods require computationally expensive relative position\nembeddings; we introduce a simple alternative of adding absolute position\nembeddings to queries and keys instead of to word embeddings, which efficiently\nproduces superior results. We show that these recurrent models also benefit\nfrom short input lengths. Combining these techniques speeds up training by a\nfactor of 1.65, reduces memory usage, and substantially improves perplexity on\nWikiText-103, without adding any parameters.\n", "rewritten_text": "Longer input sequences have fueled advancements in transformer-based language models.  This work demonstrates that shorter inputs can be beneficial, achieving both faster training and improved perplexity through two novel methods.  First, a staged training approach\u2014initially using short subsequences before transitioning to longer ones\u2014significantly reduces training time and unexpectedly boosts perplexity. Second, we enhance the efficiency of recurrent transformer methods, which handle sequences exceeding the model's maximum input length.  Instead of computationally expensive relative position embeddings, we propose a simpler, more efficient approach using absolute position embeddings added to queries and keys.  This method, also benefiting from shorter inputs, improves results.  Combining these techniques yields a 1.65x training speedup, reduced memory consumption, and substantial perplexity improvements on WikiText-103, without increasing model size.\n"}, "cs/0006021": {"original_text": "  Systems now exist which are able to compile unification grammars into\nlanguage models that can be included in a speech recognizer, but it is so far\nunclear whether non-trivial linguistically principled grammars can be used for\nthis purpose. We describe a series of experiments which investigate the\nquestion empirically, by incrementally constructing a grammar and discovering\nwhat problems emerge when successively larger versions are compiled into finite\nstate graph representations and used as language models for a medium-vocabulary\nrecognition task.\n", "rewritten_text": "This paper presents experiments evaluating the feasibility of using linguistically principled unification grammars in speech recognition.  We incrementally build a unification grammar, compiling successively larger versions into finite-state automata for use as language models in a medium-vocabulary speech recognition system.  This approach allows us to empirically investigate the challenges of integrating complex, theoretically sound grammars into practical speech recognition applications.\n"}, "2209.13351": {"original_text": "  Accurately and timely detecting multiscale small objects that contain tens of\npixels from remote sensing images (RSI) remains challenging. Most of the\nexisting solutions primarily design complex deep neural networks to learn\nstrong feature representations for objects separated from the background, which\noften results in a heavy computation burden. In this article, we propose an\naccurate yet fast object detection method for RSI, named SuperYOLO, which fuses\nmultimodal data and performs high-resolution (HR) object detection on\nmultiscale objects by utilizing the assisted super resolution (SR) learning and\nconsidering both the detection accuracy and computation cost. First, we utilize\na symmetric compact multimodal fusion (MF) to extract supplementary information\nfrom various data for improving small object detection in RSI. Furthermore, we\ndesign a simple and flexible SR branch to learn HR feature representations that\ncan discriminate small objects from vast backgrounds with low-resolution (LR)\ninput, thus further improving the detection accuracy. Moreover, to avoid\nintroducing additional computation, the SR branch is discarded in the inference\nstage, and the computation of the network model is reduced due to the LR input.\nExperimental results show that, on the widely used VEDAI RS dataset, SuperYOLO\nachieves an accuracy of 75.09% (in terms of mAP50 ), which is more than 10%\nhigher than the SOTA large models, such as YOLOv5l, YOLOv5x, and RS designed\nYOLOrs. Meanwhile, the parameter size and GFLOPs of SuperYOLO are about 18\ntimes and 3.8 times less than YOLOv5x. Our proposed model shows a favorable\naccuracy and speed tradeoff compared to the state-of-the-art models. The code\nwill be open-sourced at https://github.com/icey-zhang/SuperYOLO.\n", "rewritten_text": "Detecting small objects (tens of pixels) in remote sensing images (RSI) remains a significant challenge.  Existing deep learning solutions often rely on computationally expensive, complex networks.  This paper introduces SuperYOLO, a fast and accurate object detection method for RSI.  SuperYOLO leverages multimodal data fusion and assisted super-resolution (SR) learning to improve the detection of multiscale objects.  A compact multimodal fusion module extracts supplementary information, while a lightweight SR branch enhances feature representation for small objects, improving accuracy without impacting inference speed (the SR branch is discarded during inference).  On the VEDAI RS dataset, SuperYOLO achieves 75.09% mAP50, outperforming state-of-the-art models like YOLOv5l, YOLOv5x, and YOLOrs by over 10%, while using significantly fewer parameters (18x less) and computations (3.8x less GFLOPs).  The code is available at https://github.com/icey-zhang/SuperYOLO.\n"}, "2109.01958": {"original_text": "  Transformer-based pre-trained language models boost the performance of\nopen-domain dialogue systems. Prior works leverage Transformer-based\npre-trained language models to generate texts with desired attributes in two\ngeneral approaches: (1) gradient-based methods: updating all latent\nrepresentations of pre-trained models with gradients from attribute models; (2)\nweighted-decoding methods: re-ranking beam candidates from pre-trained models\nwith attribute functions. However, gradient-based methods lead to high\ncomputation cost and can easily get overfitted on small training sets, while\nweighted-decoding methods are inherently constrained by the low-variance\nhigh-bias pre-trained model. In this work, we propose a novel approach to\ncontrol the generation of Transformer-based pre-trained language models: the\nSideControl framework, which leverages a novel control attributes loss to\nincorporate useful control signals, and is shown to perform well with very\nlimited training samples. We evaluate our proposed method on two benchmark\nopen-domain dialogue datasets, and results show that the SideControl framework\nhas better controllability, higher generation quality and better\nsample-efficiency than existing gradient-based and weighted-decoding baselines.\n", "rewritten_text": "Pre-trained Transformer language models improve open-domain dialogue systems.  Existing methods for controlling text generation using these models \u2013 gradient-based fine-tuning and weighted decoding \u2013 suffer from drawbacks: high computational cost and overfitting (gradient-based) or limited expressiveness due to the pre-trained model's biases (weighted decoding).  This paper introduces SideControl, a novel framework that uses a specialized loss function to effectively incorporate control signals.  Evaluated on two benchmark datasets, SideControl demonstrates superior controllability, generation quality, and sample efficiency compared to existing approaches.\n"}, "2205.02022": {"original_text": "  Recent advances in the pre-training of language models leverage large-scale\ndatasets to create multilingual models. However, low-resource languages are\nmostly left out in these datasets. This is primarily because many widely spoken\nlanguages are not well represented on the web and therefore excluded from the\nlarge-scale crawls used to create datasets. Furthermore, downstream users of\nthese models are restricted to the selection of languages originally chosen for\npre-training. This work investigates how to optimally leverage existing\npre-trained models to create low-resource translation systems for 16 African\nlanguages. We focus on two questions: 1) How can pre-trained models be used for\nlanguages not included in the initial pre-training? and 2) How can the\nresulting translation models effectively transfer to new domains? To answer\nthese questions, we create a new African news corpus covering 16 languages, of\nwhich eight languages are not part of any existing evaluation dataset. We\ndemonstrate that the most effective strategy for transferring both to\nadditional languages and to additional domains is to fine-tune large\npre-trained models on small quantities of high-quality translation data.\n", "rewritten_text": "Multilingual language models, trained on massive datasets, often neglect low-resource languages due to their underrepresentation online.  This limits downstream applications to only the languages initially included.  This research addresses this gap by exploring optimal methods for leveraging existing pre-trained models to build machine translation systems for 16 African languages, eight of which lack existing evaluation datasets.  We investigate how to adapt pre-trained models to unseen languages and how to ensure effective domain transfer.  Our approach involves creating a new African news corpus and demonstrates that fine-tuning large pre-trained models with small amounts of high-quality translation data is the most effective strategy for both cross-lingual and cross-domain adaptation.\n"}, "2109.12028": {"original_text": "  Human knowledge is collectively encoded in the roughly 6500 languages spoken\naround the world, but it is not distributed equally across languages. Hence,\nfor information-seeking question answering (QA) systems to adequately serve\nspeakers of all languages, they need to operate cross-lingually. In this work\nwe investigate the capabilities of multilingually pre-trained language models\non cross-lingual QA. We find that explicitly aligning the representations\nacross languages with a post-hoc fine-tuning step generally leads to improved\nperformance. We additionally investigate the effect of data size as well as the\nlanguage choice in this fine-tuning step, also releasing a dataset for\nevaluating cross-lingual QA systems. Code and dataset are publicly available\nhere: https://github.com/ffaisal93/aligned_qa\n", "rewritten_text": "Global knowledge is encoded in approximately 6,500 languages, but this knowledge is unevenly distributed.  To ensure equitable access to information, question-answering (QA) systems must be cross-lingual. This research evaluates the cross-lingual QA capabilities of multilingual language models, finding that post-hoc alignment of language representations through fine-tuning significantly improves performance.  We also analyze the impact of training data size and language selection during fine-tuning.  A new dataset for evaluating cross-lingual QA systems is released alongside the code (available at: https://github.com/ffaisal93/aligned_qa).\n"}, "2010.00363": {"original_text": "  Long Short-Term Memory recurrent neural network (LSTM) is widely used and\nknown to capture informative long-term syntactic dependencies. However, how\nsuch information are reflected in its internal vectors for natural text has not\nyet been sufficiently investigated. We analyze them by learning a language\nmodel where syntactic structures are implicitly given. We empirically show that\nthe context update vectors, i.e. outputs of internal gates, are approximately\nquantized to binary or ternary values to help the language model to count the\ndepth of nesting accurately, as Suzgun et al. (2019) recently show for\nsynthetic Dyck languages. For some dimensions in the context vector, we show\nthat their activations are highly correlated with the depth of phrase\nstructures, such as VP and NP. Moreover, with an $L_1$ regularization, we also\nfound that it can accurately predict whether a word is inside a phrase\nstructure or not from a small number of components of the context vector. Even\nfor the case of learning from raw text, context vectors are shown to still\ncorrelate well with the phrase structures. Finally, we show that natural\nclusters of the functional words and the part of speeches that trigger phrases\nare represented in a small but principal subspace of the context-update vector\nof LSTM.\n", "rewritten_text": "This study investigates how Long Short-Term Memory (LSTM) networks capture long-term syntactic dependencies in natural language.  We analyze LSTM language models trained on implicitly structured data, revealing that internal gate outputs (context update vectors) exhibit approximate binary or ternary quantization, enabling accurate nesting depth counting, mirroring findings in synthetic languages.  Specific vector dimensions strongly correlate with phrase structure depth (e.g., VP, NP).  Furthermore, L1 regularization allows accurate phrase membership prediction from a few vector components.  Even with raw text training, context vectors maintain strong phrase structure correlations. Finally, we demonstrate that functional words and parts-of-speech triggering phrases cluster within a low-dimensional subspace of the context update vector.\n"}, "2005.02877": {"original_text": "  Task-oriented dialog systems rely on dialog state tracking (DST) to monitor\nthe user's goal during the course of an interaction. Multi-domain and\nopen-vocabulary settings complicate the task considerably and demand scalable\nsolutions. In this paper we present a new approach to DST which makes use of\nvarious copy mechanisms to fill slots with values. Our model has no need to\nmaintain a list of candidate values. Instead, all values are extracted from the\ndialog context on-the-fly. A slot is filled by one of three copy mechanisms:\n(1) Span prediction may extract values directly from the user input; (2) a\nvalue may be copied from a system inform memory that keeps track of the\nsystem's inform operations; (3) a value may be copied over from a different\nslot that is already contained in the dialog state to resolve coreferences\nwithin and across domains. Our approach combines the advantages of span-based\nslot filling methods with memory methods to avoid the use of value picklists\naltogether. We argue that our strategy simplifies the DST task while at the\nsame time achieving state of the art performance on various popular evaluation\nsets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55%.\n", "rewritten_text": "Dialog state tracking (DST) is crucial for task-oriented dialog systems to understand user goals.  However, multi-domain and open-vocabulary interactions pose significant challenges.  This paper introduces a novel DST approach employing various copy mechanisms to populate slots without needing a predefined value list.  Values are dynamically extracted from the dialog context using three methods:  direct extraction from user input (span prediction), copying from the system's inform memory, and copying from other already-filled slots to handle coreferences.  This approach, combining span-based and memory-based methods, eliminates the need for value lists, simplifying DST while achieving state-of-the-art results, exceeding 55% joint goal accuracy on MultiWOZ 2.1.\n"}, "2408.06899": {"original_text": "  We present a novel method for measuring the rate of periodic phenomena (e.g.,\nrotation, flicker, and vibration), by an event camera, a device asynchronously\nreporting brightness changes at independently operating pixels with high\ntemporal resolution. The approach assumes that for a periodic phenomenon, a\nhighly similar set of events is generated within a spatio-temporal window at a\ntime difference corresponding to its period. The sets of similar events are\ndetected by a correlation in the spatio-temporal event stream space. The\nproposed method, EEPPR, is evaluated on a dataset of 12 sequences of periodic\nphenomena, i.e. flashing light and vibration, and periodic motion, e.g.,\nrotation, ranging from 3.2 Hz to 2 kHz (equivalent to 192 - 120 000 RPM). EEPPR\nsignificantly outperforms published methods on this dataset, achieving a mean\nrelative error of 0.1%, setting new state-of-the-art. The dataset and codes are\npublicly available on GitHub.\n", "rewritten_text": "This paper introduces EEPPR, a novel method for measuring the frequency of periodic events (e.g., rotation, flicker, vibration) using event cameras.  Leveraging the high temporal resolution and asynchronous nature of event cameras, EEPPR identifies periodic phenomena by detecting highly similar spatio-temporal event patterns separated by the event's period.  Evaluated on a publicly available dataset (GitHub) of 12 sequences (3.2 Hz to 2 kHz), EEPPR significantly outperforms existing techniques, achieving a mean relative error of only 0.1% and establishing a new state-of-the-art.\n"}, "1703.08448": {"original_text": "  We investigate a principle way to progressively mine discriminative object\nregions using classification networks to address the weakly-supervised semantic\nsegmentation problems. Classification networks are only responsive to small and\nsparse discriminative regions from the object of interest, which deviates from\nthe requirement of the segmentation task that needs to localize dense, interior\nand integral regions for pixel-wise inference. To mitigate this gap, we propose\na new adversarial erasing approach for localizing and expanding object regions\nprogressively. Starting with a single small object region, our proposed\napproach drives the classification network to sequentially discover new and\ncomplement object regions by erasing the current mined regions in an\nadversarial manner. These localized regions eventually constitute a dense and\ncomplete object region for learning semantic segmentation. To further enhance\nthe quality of the discovered regions by adversarial erasing, an online\nprohibitive segmentation learning approach is developed to collaborate with\nadversarial erasing by providing auxiliary segmentation supervision modulated\nby the more reliable classification scores. Despite its apparent simplicity,\nthe proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union\n(mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new\nstate-of-the-arts.\n", "rewritten_text": "This paper addresses weakly-supervised semantic segmentation by progressively mining discriminative object regions using classification networks.  Because classification networks only focus on small, sparse regions, unlike segmentation which requires dense, complete object localization, we introduce a novel adversarial erasing approach.  This method iteratively expands object regions: starting with a single region, it adversarially erases already-identified areas, forcing the network to discover new, complementary regions.  This process culminates in a dense, complete object representation for semantic segmentation.  Further improvements are achieved through an online prohibitive segmentation learning approach, which uses reliable classification scores to guide the adversarial erasing.  This simple yet effective method achieves state-of-the-art results, reaching 55.0% and 55.7% mean Intersection-over-Union (mIoU) on the PASCAL VOC 2012 validation and test sets, respectively.\n"}, "1708.00284": {"original_text": "  Future frame prediction in videos is a promising avenue for unsupervised\nvideo representation learning. Video frames are naturally generated by the\ninherent pixel flows from preceding frames based on the appearance and motion\ndynamics in the video. However, existing methods focus on directly\nhallucinating pixel values, resulting in blurry predictions. In this paper, we\ndevelop a dual motion Generative Adversarial Net (GAN) architecture, which\nlearns to explicitly enforce future-frame predictions to be consistent with the\npixel-wise flows in the video through a dual-learning mechanism. The primal\nfuture-frame prediction and dual future-flow prediction form a closed loop,\ngenerating informative feedback signals to each other for better video\nprediction. To make both synthesized future frames and flows indistinguishable\nfrom reality, a dual adversarial training method is proposed to ensure that the\nfuture-flow prediction is able to help infer realistic future-frames, while the\nfuture-frame prediction in turn leads to realistic optical flows. Our dual\nmotion GAN also handles natural motion uncertainty in different pixel locations\nwith a new probabilistic motion encoder, which is based on variational\nautoencoders. Extensive experiments demonstrate that the proposed dual motion\nGAN significantly outperforms state-of-the-art approaches on synthesizing new\nvideo frames and predicting future flows. Our model generalizes well across\ndiverse visual scenes and shows superiority in unsupervised video\nrepresentation learning.\n", "rewritten_text": "Unsupervised video representation learning can be significantly advanced through accurate future frame prediction.  Current methods, which directly predict pixel values, often produce blurry results.  This paper introduces a novel dual motion Generative Adversarial Network (GAN) that addresses this limitation.  Our architecture uses a dual-learning mechanism: a primal network predicts future frames, and a dual network predicts the corresponding pixel flows.  These predictions form a closed loop, providing mutual feedback for improved accuracy.  A dual adversarial training process ensures both predicted frames and flows are realistic.  Furthermore, a variational autoencoder-based probabilistic motion encoder handles motion uncertainty.  Extensive experiments demonstrate superior performance compared to existing methods in both future frame synthesis and flow prediction, showcasing strong generalization across diverse scenes and improved unsupervised video representation learning.\n"}, "1405.6103": {"original_text": "  The Swiss avalanche bulletin is produced twice a day in four languages. Due\nto the lack of time available for manual translation, a fully automated\ntranslation system is employed, based on a catalogue of predefined phrases and\npredetermined rules of how these phrases can be combined to produce sentences.\nThe system is able to automatically translate such sentences from German into\nthe target languages French, Italian and English without subsequent\nproofreading or correction. Our catalogue of phrases is limited to a small\nsublanguage. The reduction of daily translation costs is expected to offset the\ninitial development costs within a few years. After being operational for two\nwinter seasons, we assess here the quality of the produced texts based on an\nevaluation where participants rate real danger descriptions from both origins,\nthe catalogue of phrases versus the manually written and translated texts. With\na mean recognition rate of 55%, users can hardly distinguish between the two\ntypes of texts, and give similar ratings with respect to their language\nquality. Overall, the output from the catalogue system can be considered\nvirtually equivalent to a text written by avalanche forecasters and then\nmanually translated by professional translators. Furthermore, forecasters\ndeclared that all relevant situations were captured by the system with\nsufficient accuracy and within the limited time available.\n", "rewritten_text": "The Swiss avalanche bulletin, issued twice daily in four languages, utilizes a fully automated translation system due to time constraints.  This system, based on a predefined phrase catalogue and combination rules, translates German bulletins into French, Italian, and English without human intervention.  While limited to a specific sublanguage, the system's cost savings are projected to recoup development costs within a few years.  After two winter seasons, a user evaluation comparing automated and manually translated bulletins showed a 55% indistinguishability rate, with similar quality ratings.  Forecasters confirmed the system's accuracy in conveying all critical information within the allotted timeframe, demonstrating the automated system's virtual equivalence to professionally translated bulletins.\n"}, "2408.15063": {"original_text": "  Although most existing multi-modal salient object detection (SOD) methods\ndemonstrate effectiveness through training models from scratch, the limited\nmulti-modal data hinders these methods from reaching optimality. In this paper,\nwe propose a novel framework to explore and exploit the powerful feature\nrepresentation and zero-shot generalization ability of the pre-trained Segment\nAnything Model (SAM) for multi-modal SOD. Despite serving as a recent vision\nfundamental model, driving the class-agnostic SAM to comprehend and detect\nsalient objects accurately is non-trivial, especially in challenging scenes. To\nthis end, we develop \\underline{SAM} with se\\underline{m}antic\nf\\underline{e}ature fu\\underline{s}ion guidanc\\underline{e} (Sammese), which\nincorporates multi-modal saliency-specific knowledge into SAM to adapt SAM to\nmulti-modal SOD tasks. However, it is difficult for SAM trained on single-modal\ndata to directly mine the complementary benefits of multi-modal inputs and\ncomprehensively utilize them to achieve accurate saliency prediction. To\naddress these issues, we first design a multi-modal complementary fusion module\nto extract robust multi-modal semantic features by integrating information from\nvisible and thermal or depth image pairs. Then, we feed the extracted\nmulti-modal semantic features into both the SAM image encoder and mask decoder\nfor fine-tuning and prompting, respectively. Specifically, in the image\nencoder, a multi-modal adapter is proposed to adapt the single-modal SAM to\nmulti-modal information. In the mask decoder, a semantic-geometric prompt\ngeneration strategy is proposed to produce corresponding embeddings with\nvarious saliency cues. Extensive experiments on both RGB-D and RGB-T SOD\nbenchmarks show the effectiveness of the proposed framework. The code will be\navailable at \\url{https://github.com/Angknpng/Sammese}.\n", "rewritten_text": "Current multi-modal salient object detection (SOD) methods, while effective, are limited by the scarcity of training data.  This paper introduces Sammese, a novel framework leveraging the pre-trained Segment Anything Model (SAM) for improved multi-modal SOD.  Adapting the class-agnostic SAM to accurately detect salient objects in challenging scenes requires overcoming its inherent limitations in handling multi-modal data.  Sammese addresses this by incorporating multi-modal saliency knowledge into SAM through a multi-modal complementary fusion module that extracts robust features from visible and thermal/depth image pairs.  These features are then used to fine-tune SAM's image encoder (via a multi-modal adapter) and prompt its mask decoder (using a semantic-geometric prompt generation strategy).  Extensive experiments on RGB-D and RGB-T SOD benchmarks demonstrate Sammese's effectiveness.  Code is available at [https://github.com/Angknpng/Sammese](https://github.com/Angknpng/Sammese).\n"}, "2210.09563": {"original_text": "  With the continuous development of deep learning in the field of image\ngeneration models, a large number of vivid forged faces have been generated and\nspread on the Internet. These high-authenticity artifacts could grow into a\nthreat to society security. Existing face forgery detection methods directly\nutilize the obtained public shared or centralized data for training but ignore\nthe personal privacy and security issues when personal data couldn't be\ncentralizedly shared in real-world scenarios. Additionally, different\ndistributions caused by diverse artifact types would further bring adverse\ninfluences on the forgery detection task. To solve the mentioned problems, the\npaper proposes a novel generalized residual Federated learning for face Forgery\ndetection (FedForgery). The designed variational autoencoder aims to learn\nrobust discriminative residual feature maps to detect forgery faces (with\ndiverse or even unknown artifact types). Furthermore, the general federated\nlearning strategy is introduced to construct distributed detection model\ntrained collaboratively with multiple local decentralized devices, which could\nfurther boost the representation generalization. Experiments conducted on\npublicly available face forgery detection datasets prove the superior\nperformance of the proposed FedForgery. The designed novel generalized face\nforgery detection protocols and source code would be publicly available.\n", "rewritten_text": "The proliferation of realistic deepfake faces online poses a significant societal security risk.  Current face forgery detection methods, trained on centralized datasets, compromise privacy and struggle with the diverse nature of forgery artifacts.  To address these issues, this paper introduces FedForgery, a novel generalized residual federated learning framework for face forgery detection.  FedForgery employs a variational autoencoder to learn robust, discriminative residual features, effectively detecting forgeries regardless of artifact type.  Its federated learning approach enables collaborative training across decentralized devices, improving generalization.  Experiments on publicly available datasets demonstrate FedForgery's superior performance, and its protocols and source code will be publicly released.\n"}, "2304.05995": {"original_text": "  In recent years, the success of large-scale vision-language models (VLMs)\nsuch as CLIP has led to their increased usage in various computer vision tasks.\nThese models enable zero-shot inference through carefully crafted instructional\ntext prompts without task-specific supervision. However, the potential of VLMs\nfor generalization tasks in remote sensing (RS) has not been fully realized. To\naddress this research gap, we propose a novel image-conditioned prompt learning\nstrategy called the Visual Attention Parameterized Prompts Learning Network\n(APPLeNet). APPLeNet emphasizes the importance of multi-scale feature learning\nin RS scene classification and disentangles visual style and content primitives\nfor domain generalization tasks. To achieve this, APPLeNet combines visual\ncontent features obtained from different layers of the vision encoder and style\nproperties obtained from feature statistics of domain-specific batches. An\nattention-driven injection module is further introduced to generate visual\ntokens from this information. We also introduce an anti-correlation regularizer\nto ensure discrimination among the token embeddings, as this visual information\nis combined with the textual tokens. To validate APPLeNet, we curated four\navailable RS benchmarks and introduced experimental protocols and datasets for\nthree domain generalization tasks. Our results consistently outperform the\nrelevant literature and code is available at\nhttps://github.com/mainaksingha01/APPLeNet\n", "rewritten_text": "Large-scale vision-language models (VLMs) like CLIP have shown promise in computer vision, enabling zero-shot inference via text prompts.  However, their potential in remote sensing (RS) remains untapped.  This paper introduces APPLeNet, a novel image-conditioned prompt learning network for RS scene classification and domain generalization.  APPLeNet leverages multi-scale features, disentangling visual style and content, by combining features from different encoder layers with domain-specific style statistics.  An attention module generates visual tokens from this combined information, and an anti-correlation regularizer enhances token discrimination.  Evaluated on four RS benchmarks with newly introduced domain generalization tasks and protocols, APPLeNet significantly outperforms existing methods.  Code is available at https://github.com/mainaksingha01/APPLeNet.\n"}, "1804.03287": {"original_text": "  Despite the noticeable progress in perceptual tasks like detection, instance\nsegmentation and human parsing, computers still perform unsatisfactorily on\nvisually understanding humans in crowded scenes, such as group behavior\nanalysis, person re-identification and autonomous driving, etc. To this end,\nmodels need to comprehensively perceive the semantic information and the\ndifferences between instances in a multi-human image, which is recently defined\nas the multi-human parsing task. In this paper, we present a new large-scale\ndatabase \"Multi-Human Parsing (MHP)\" for algorithm development and evaluation,\nand advances the state-of-the-art in understanding humans in crowded scenes.\nMHP contains 25,403 elaborately annotated images with 58 fine-grained semantic\ncategory labels, involving 2-26 persons per image and captured in real-world\nscenes from various viewpoints, poses, occlusion, interactions and background.\nWe further propose a novel deep Nested Adversarial Network (NAN) model for\nmulti-human parsing. NAN consists of three Generative Adversarial Network\n(GAN)-like sub-nets, respectively performing semantic saliency prediction,\ninstance-agnostic parsing and instance-aware clustering. These sub-nets form a\nnested structure and are carefully designed to learn jointly in an end-to-end\nway. NAN consistently outperforms existing state-of-the-art solutions on our\nMHP and several other datasets, and serves as a strong baseline to drive the\nfuture research for multi-human parsing.\n", "rewritten_text": "While computer vision has advanced significantly in tasks like object detection and segmentation, accurately understanding humans in complex scenes (e.g., group behavior analysis, re-identification, autonomous driving) remains challenging.  This requires comprehensive semantic understanding and individual distinction within crowded images \u2013 a task defined as multi-human parsing.  This paper introduces MHP, a large-scale dataset (25,403 images with 58 fine-grained labels, 2-26 people per image, diverse real-world scenarios) for advancing this field.  We also propose a novel Nested Adversarial Network (NAN) for multi-human parsing.  NAN, a three-subnet GAN architecture performing semantic saliency prediction, instance-agnostic parsing, and instance-aware clustering, achieves state-of-the-art performance on MHP and other benchmarks, establishing a strong baseline for future research.\n"}, "2206.10779": {"original_text": "  We propose a large-scale dataset of real-world rainy and clean image pairs\nand a method to remove degradations, induced by rain streaks and rain\naccumulation, from the image. As there exists no real-world dataset for\nderaining, current state-of-the-art methods rely on synthetic data and thus are\nlimited by the sim2real domain gap; moreover, rigorous evaluation remains a\nchallenge due to the absence of a real paired dataset. We fill this gap by\ncollecting a real paired deraining dataset through meticulous control of\nnon-rain variations. Our dataset enables paired training and quantitative\nevaluation for diverse real-world rain phenomena (e.g. rain streaks and rain\naccumulation). To learn a representation robust to rain phenomena, we propose a\ndeep neural network that reconstructs the underlying scene by minimizing a\nrain-robust loss between rainy and clean images. Extensive experiments\ndemonstrate that our model outperforms the state-of-the-art deraining methods\non real rainy images under various conditions. Project website:\nhttps://visual.ee.ucla.edu/gt_rain.htm/.\n", "rewritten_text": "This paper introduces a novel large-scale dataset of real-world rainy and clean image pairs, addressing the current lack of such data in rain removal research.  Existing methods rely on synthetic data, limiting their performance due to the simulation-to-reality gap.  Our meticulously collected dataset, featuring diverse rain phenomena (streaks and accumulation), enables paired training and rigorous quantitative evaluation.  We also present a deep neural network trained on this dataset, using a rain-robust loss function to reconstruct clean images from rainy ones.  Extensive experiments show our method surpasses state-of-the-art deraining techniques on real-world images.  See our project website for details: https://visual.ee.ucla.edu/gt_rain.htm/\n"}, "1812.08115": {"original_text": "  We introduce a model-based deep learning architecture termed MoDL-MUSSELS for\nthe correction of phase errors in multishot diffusion-weighted echo-planar MRI\nimages. The proposed algorithm is a generalization of existing MUSSELS\nalgorithm with similar performance but with significantly reduced computational\ncomplexity. In this work, we show that an iterative re-weighted least-squares\nimplementation of MUSSELS alternates between a multichannel filter bank and the\nenforcement of data consistency. The multichannel filter bank projects the data\nto the signal subspace thus exploiting the phase relations between shots. Due\nto the high computational complexity of self-learned filter bank, we propose to\nreplace it with a convolutional neural network (CNN) whose parameters are\nlearned from exemplary data. The proposed CNN is a hybrid model involving a\nmultichannel CNN in the k-space and another CNN in the image space. The k-space\nCNN exploits the phase relations between the shot images, while the image\ndomain network is used to project the data to an image manifold. The\nexperiments show that the proposed scheme can yield reconstructions that are\ncomparable to state of the art methods while offering several orders of\nmagnitude reduction in run-time.\n", "rewritten_text": "This paper presents MoDL-MUSSELS, a model-based deep learning architecture for correcting phase errors in multishot diffusion-weighted echo-planar MRI.  Building upon the MUSSELS algorithm, MoDL-MUSSELS achieves comparable performance with drastically reduced computational cost.  We demonstrate that MUSSELS iteratively applies a multichannel filter bank and data consistency constraints.  To address the high computational complexity of the self-learned filter bank, we replace it with a hybrid convolutional neural network (CNN). This CNN comprises a k-space CNN leveraging inter-shot phase relationships and an image-space CNN projecting data onto an image manifold.  Our results show that MoDL-MUSSELS achieves state-of-the-art reconstruction quality with significantly faster processing times.\n"}, "1912.10644": {"original_text": "  In spite of the recent progresses on classifying 3D point cloud with deep\nCNNs, large geometric transformations like rotation and translation remain\nchallenging problem and harm the final classification performance. To address\nthis challenge, we propose Geometry Sharing Network (GS-Net) which effectively\nlearns point descriptors with holistic context to enhance the robustness to\ngeometric transformations. Compared with previous 3D point CNNs which perform\nconvolution on nearby points, GS-Net can aggregate point features in a more\nglobal way. Specially, GS-Net consists of Geometry Similarity Connection (GSC)\nmodules which exploit Eigen-Graph to group distant points with similar and\nrelevant geometric information, and aggregate features from nearest neighbors\nin both Euclidean space and Eigenvalue space. This design allows GS-Net to\nefficiently capture both local and holistic geometric features such as\nsymmetry, curvature, convexity and connectivity. Theoretically, we show the\nnearest neighbors of each point in Eigenvalue space are invariant to rotation\nand translation. We conduct extensive experiments on public datasets,\nModelNet40, ShapeNet Part. Experiments demonstrate that GS-Net achieves the\nstate-of-the-art performances on major datasets, 93.3% on ModelNet40, and are\nmore robust to geometric transformations.\n", "rewritten_text": "Deep CNNs have made strides in 3D point cloud classification, but remain vulnerable to geometric transformations like rotation and translation.  To overcome this, we introduce Geometry Sharing Network (GS-Net), which learns robust point descriptors by incorporating holistic context. Unlike existing methods that focus on local convolutions, GS-Net globally aggregates features using Geometry Similarity Connection (GSC) modules.  These modules leverage Eigen-graphs to group distant points with similar geometric properties, aggregating features from neighbors in both Euclidean and Eigenvalue space. This approach efficiently captures local and global geometric features (symmetry, curvature, convexity, connectivity), with the Eigenvalue space providing inherent invariance to rotation and translation.  Extensive experiments on ModelNet40 and ShapeNet Part demonstrate state-of-the-art performance (93.3% on ModelNet40) and significantly improved robustness to geometric transformations.\n"}, "2111.01515": {"original_text": "  The enormous amount of data being generated on the web and social media has\nincreased the demand for detecting online hate speech. Detecting hate speech\nwill reduce their negative impact and influence on others. A lot of effort in\nthe Natural Language Processing (NLP) domain aimed to detect hate speech in\ngeneral or detect specific hate speech such as religion, race, gender, or\nsexual orientation. Hate communities tend to use abbreviations, intentional\nspelling mistakes, and coded words in their communication to evade detection,\nadding more challenges to hate speech detection tasks. Thus, word\nrepresentation will play an increasingly pivotal role in detecting hate speech.\nThis paper investigates the feasibility of leveraging domain-specific word\nembedding in Bidirectional LSTM based deep model to automatically\ndetect/classify hate speech. Furthermore, we investigate the use of the\ntransfer learning language model (BERT) on hate speech problem as a binary\nclassification task. The experiments showed that domainspecific word embedding\nwith the Bidirectional LSTM based deep model achieved a 93% f1-score while BERT\nachieved up to 96% f1-score on a combined balanced dataset from available hate\nspeech datasets.\n", "rewritten_text": "The proliferation of online data necessitates improved hate speech detection.  While Natural Language Processing (NLP) has made strides in identifying hate speech targeting various groups (religion, race, gender, sexual orientation), the use of obfuscation techniques like abbreviations and coded language presents a significant challenge.  This paper explores two approaches to enhance hate speech detection: a Bidirectional LSTM model using domain-specific word embeddings, and a transfer learning approach utilizing BERT.  Experiments on a balanced dataset yielded an F1-score of 93% for the LSTM model and 96% for the BERT model, demonstrating the effectiveness of these methods in binary hate speech classification.\n"}, "2409.17432": {"original_text": "  Reducing the atmospheric haze and enhancing image clarity is crucial for\ncomputer vision applications. The lack of real-life hazy ground truth images\nnecessitates synthetic datasets, which often lack diverse haze types, impeding\neffective haze type classification and dehazing algorithm selection. This\nresearch introduces the HazeSpace2M dataset, a collection of over 2 million\nimages designed to enhance dehazing through haze type classification.\nHazeSpace2M includes diverse scenes with 10 haze intensity levels, featuring\nFog, Cloud, and Environmental Haze (EH). Using the dataset, we introduce a\ntechnique of haze type classification followed by specialized dehazers to clear\nhazy images. Unlike conventional methods, our approach classifies haze types\nbefore applying type-specific dehazing, improving clarity in real-life hazy\nimages. Benchmarking with state-of-the-art (SOTA) models, ResNet50 and AlexNet\nachieve 92.75\\% and 92.50\\% accuracy, respectively, against existing synthetic\ndatasets. However, these models achieve only 80% and 70% accuracy,\nrespectively, against our Real Hazy Testset (RHT), highlighting the challenging\nnature of our HazeSpace2M dataset. Additional experiments show that haze type\nclassification followed by specialized dehazing improves results by 2.41% in\nPSNR, 17.14% in SSIM, and 10.2\\% in MSE over general dehazers. Moreover, when\ntesting with SOTA dehazing models, we found that applying our proposed\nframework significantly improves their performance. These results underscore\nthe significance of HazeSpace2M and our proposed framework in addressing\natmospheric haze in multimedia processing. Complete code and dataset is\navailable on \\href{https://github.com/tanvirnwu/HazeSpace2M}\n{\\textcolor{blue}{\\textbf{GitHub}}}.\n", "rewritten_text": "Clear images are vital for computer vision, but a lack of real-world hazy image data hinders development of effective dehazing algorithms.  To address this, we introduce HazeSpace2M, a massive (2M+ images) synthetic dataset featuring diverse scenes, ten haze intensity levels, and three haze types (fog, cloud, and environmental haze).  We leverage this dataset to develop a novel two-stage dehazing approach: first classifying the haze type, then applying a specialized dehazing algorithm.  While achieving high accuracy (92.75% and 92.50% for ResNet50 and AlexNet, respectively) on existing synthetic datasets, our approach demonstrates significant improvement (2.41% PSNR, 17.14% SSIM, and 10.2% MSE) over general dehazing methods when evaluated on our challenging Real Hazy Testset (RHT).  Furthermore, our framework substantially boosts the performance of state-of-the-art dehazing models.  The HazeSpace2M dataset and code are publicly available at [GitHub link].\n"}, "1808.045": {"original_text": "  Medical images with specific pathologies are scarce, but a large amount of\ndata is usually required for a deep convolutional neural network (DCNN) to\nachieve good accuracy. We consider the problem of segmenting the left\nventricular (LV) myocardium on late gadolinium enhancement (LGE) cardiovascular\nmagnetic resonance (CMR) scans of which only some of the scans have scar\ntissue. We propose ScarGAN to simulate scar tissue on healthy myocardium using\nchained generative adversarial networks (GAN). Our novel approach factorizes\nthe simulation process into 3 steps: 1) a mask generator to simulate the shape\nof the scar tissue; 2) a domain-specific heuristic to produce the initial\nsimulated scar tissue from the simulated shape; 3) a refining generator to add\ndetails to the simulated scar tissue. Unlike other approaches that generate\nsamples from scratch, we simulate scar tissue on normal scans resulting in\nhighly realistic samples. We show that experienced radiologists are unable to\ndistinguish between real and simulated scar tissue. Training a U-Net with\nadditional scans with scar tissue simulated by ScarGAN increases the percentage\nof scar pixels correctly included in LV myocardium prediction from 75.9% to\n80.5%.\n", "rewritten_text": "Deep learning models for medical image segmentation require extensive data, but datasets of images showing specific pathologies like cardiac scar tissue are limited.  To address this, we introduce ScarGAN, a three-stage generative adversarial network (GAN) that realistically simulates scar tissue on healthy late gadolinium enhancement (LGE) cardiovascular magnetic resonance (CMR) images.  ScarGAN first generates a scar tissue mask, then uses a heuristic to create an initial scar simulation, and finally refines the simulation with a second GAN.  Unlike methods generating synthetic images from scratch, ScarGAN modifies existing healthy images, producing highly realistic results indistinguishable from real scar tissue by expert radiologists.  Augmenting a U-Net model with ScarGAN-generated data improved the accuracy of left ventricular (LV) myocardium segmentation, increasing the correctly identified scar pixel percentage from 75.9% to 80.5%.\n"}, "2409.00045": {"original_text": "  Colonoscopy is the primary method for examination, detection, and removal of\npolyps. Regular screening helps detect and prevent colorectal cancer at an\nearly curable stage. However, challenges such as variation among the\nendoscopists' skills, bowel quality preparation, and complex nature of the\nlarge intestine which cause large number of polyp miss-rate. These missed\npolyps can develop into cancer later on, which underscores the importance of\nimproving the detection methods. A computer-aided diagnosis system can support\nphysicians by assisting in detecting overlooked polyps. However, one of the\nimportant challenges for developing novel deep learning models for automatic\npolyp detection and segmentation is the lack of publicly available,\nmulti-center large and diverse datasets. To address this gap, we introduce\nPolypDB, a large scale publicly available dataset that contains 3934 still\npolyp images and their corresponding ground truth from real colonoscopy videos\nto design efficient polyp detection and segmentation architectures. The dataset\nhas been developed and verified by a team of 10 gastroenterologists. PolypDB\ncomprises of images from five modalities: Blue Light Imaging (BLI), Flexible\nImaging Color Enhancement (FICE), Linked Color Imaging (LCI), Narrow Band\nImaging (NBI), and White Light Imaging (WLI) and three medical centers from\nNorway, Sweden and Vietnam. Thus, we split the dataset based on modality and\nmedical center for modality-wise and center-wise analysis. We provide a\nbenchmark on each modality using eight popular segmentation methods and six\nstandard benchmark polyp detection methods. Furthermore, we also provide\nbenchmark on center-wise under federated learning settings. Our dataset is\npublic and can be downloaded at \\url{https://osf.io/pr7ms/}.\n", "rewritten_text": "Colonoscopy is the gold standard for colorectal polyp detection and removal, crucial for early cancer prevention.  However, limitations like inter-endoscopist variability, bowel preparation quality, and the colon's complexity lead to significant polyp miss rates, increasing cancer risk.  To improve detection, computer-aided diagnosis systems are needed, but their development is hampered by a lack of large, diverse, publicly available datasets.  We address this by introducing PolypDB, a publicly accessible dataset (available at [https://osf.io/pr7ms/]) containing 3934 polyp images with ground truth annotations from real colonoscopy videos across five imaging modalities (BLI, FICE, LCI, NBI, WLI) and three medical centers (Norway, Sweden, Vietnam).  Validated by 10 gastroenterologists, PolypDB enables benchmarking of polyp detection and segmentation algorithms, with provided results for eight segmentation and six detection methods, including a federated learning analysis across centers and modalities.\n"}, "2201.09724": {"original_text": "  The task of hot-refresh model upgrades of image retrieval systems plays an\nessential role in the industry but has never been investigated in academia\nbefore. Conventional cold-refresh model upgrades can only deploy new models\nafter the gallery is overall backfilled, taking weeks or even months for\nmassive data. In contrast, hot-refresh model upgrades deploy the new model\nimmediately and then gradually improve the retrieval accuracy by backfilling\nthe gallery on-the-fly. Compatible training has made it possible, however, the\nproblem of model regression with negative flips poses a great challenge to the\nstable improvement of user experience. We argue that it is mainly due to the\nfact that new-to-old positive query-gallery pairs may show less similarity than\nnew-to-new negative pairs. To solve the problem, we introduce a\nRegression-Alleviating Compatible Training (RACT) method to properly constrain\nthe feature compatibility while reducing negative flips. The core is to\nencourage the new-to-old positive pairs to be more similar than both the\nnew-to-old negative pairs and the new-to-new negative pairs. An efficient\nuncertainty-based backfilling strategy is further introduced to fasten accuracy\nimprovements. Extensive experiments on large-scale retrieval benchmarks (e.g.,\nGoogle Landmark) demonstrate that our RACT effectively alleviates the model\nregression for one more step towards seamless model upgrades. The code will be\navailable at https://github.com/binjiezhang/RACT_ICLR2022.\n", "rewritten_text": "Hot-refresh model upgrades are crucial for industrial image retrieval systems, yet remain unexplored academically.  Unlike slow, complete gallery rebuilds (cold-refresh) taking weeks or months, hot-refresh immediately deploys new models, incrementally improving accuracy via on-the-fly gallery updates. While compatible training enables this, model regression due to \"negative flips\" \u2013 new-to-old positive pairs appearing less similar than new-to-new negative pairs \u2013 hinders user experience.  We address this by introducing Regression-Alleviating Compatible Training (RACT), which constrains feature compatibility to ensure new-to-old positive pairs are more similar than both new-to-old and new-to-new negative pairs.  An efficient uncertainty-based backfilling strategy further accelerates accuracy gains.  Extensive experiments on large-scale datasets (e.g., Google Landmark) demonstrate RACT's effectiveness in mitigating model regression, paving the way for seamless model upgrades.  Code is available at https://github.com/binjiezhang/RACT_ICLR2022.\n"}, "2012.14345": {"original_text": "  Deep Learning (DL) based methods for object detection achieve remarkable\nperformance at the cost of computationally expensive training and extensive\ndata labeling. Robots embodiment can be exploited to mitigate this burden by\nacquiring automatically annotated training data via a natural interaction with\na human showing the object of interest, handheld. However, learning solely from\nthis data may introduce biases (the so-called domain shift), and prevents\nadaptation to novel tasks. While Weakly-supervised Learning (WSL) offers a\nwell-established set of techniques to cope with these problems in\ngeneral-purpose Computer Vision, its adoption in challenging robotic domains is\nstill at a preliminary stage. In this work, we target the scenario of a robot\ntrained in a teacher-learner setting to detect handheld objects. The aim is to\nimprove detection performance in different settings by letting the robot\nexplore the environment with a limited human labeling budget. We compare\nseveral techniques for WSL in detection pipelines to reduce model re-training\ncosts without compromising accuracy, proposing solutions which target the\nconsidered robotic scenario. We show that the robot can improve adaptation to\nnovel domains, either by interacting with a human teacher (Active Learning) or\nwith an autonomous supervision (Semi-supervised Learning). We integrate our\nstrategies into an on-line detection method, achieving efficient model update\ncapabilities with few labels. We experimentally benchmark our method on\nchallenging robotic object detection tasks under domain shift.\n", "rewritten_text": "Deep learning excels at object detection but requires computationally expensive training and extensive data labeling.  Robots offer a solution:  human-robot interaction, where a human shows the robot objects, provides automatically annotated data. However, this approach can lead to bias and hinder adaptation to new tasks.  While weakly supervised learning (WSL) addresses these issues in general computer vision, its application in robotics remains limited.  This work focuses on a teacher-learner robot system for handheld object detection, aiming to improve performance across diverse settings with minimal human labeling. We compare WSL techniques to reduce retraining costs without sacrificing accuracy, proposing solutions tailored to robotic scenarios.  Our results demonstrate improved adaptation to new domains through both active learning (human interaction) and semi-supervised learning (autonomous supervision), enabling efficient online model updates with limited labels.  We evaluate our method on challenging robotic object detection tasks involving domain shifts.\n"}, "cs/0310014": {"original_text": "  Spoken Language can be used to provide insights into organisational\nprocesses, unfortunately transcription and coding stages are very time\nconsuming and expensive. The concept of partial transcription and coding is\nproposed in which spoken language is indexed prior to any subsequent\nprocessing. The functional linguistic theory of texture is used to describe the\neffects of partial transcription on observational records. The standard used to\nencode transcript context and metadata is called CHAT, but a previous XML\nschema developed to implement it contains design assumptions that make it\ndifficult to support partial transcription for example. This paper describes a\nmore effective XML schema that overcomes many of these problems and is intended\nfor use in applications that support the rapid development of spoken language\ndeliverables.\n", "rewritten_text": "Analyzing spoken language offers valuable insights into organizational processes, but transcription and coding are costly and time-intensive.  This paper proposes partial transcription and coding, indexing spoken language before full processing.  We leverage the functional linguistic theory of texture to analyze the impact of partial transcription on observational data. While the CHAT standard encodes transcript context and metadata, its existing XML schema hinders partial transcription.  Therefore, we present a new, more efficient XML schema designed to facilitate rapid development of spoken language analysis tools and overcome limitations of the previous schema.\n"}, "2007.12099": {"original_text": "  Object detection is one of the most important areas in computer vision, which\nplays a key role in various practical scenarios. Due to limitation of hardware,\nit is often necessary to sacrifice accuracy to ensure the infer speed of the\ndetector in practice. Therefore, the balance between effectiveness and\nefficiency of object detector must be considered. The goal of this paper is to\nimplement an object detector with relatively balanced effectiveness and\nefficiency that can be directly applied in actual application scenarios, rather\nthan propose a novel detection model. Considering that YOLOv3 has been widely\nused in practice, we develop a new object detector based on YOLOv3. We mainly\ntry to combine various existing tricks that almost not increase the number of\nmodel parameters and FLOPs, to achieve the goal of improving the accuracy of\ndetector as much as possible while ensuring that the speed is almost unchanged.\nSince all experiments in this paper are conducted based on PaddlePaddle, we\ncall it PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a better\nbalance between effectiveness (45.2% mAP) and efficiency (72.9 FPS), surpassing\nthe existing state-of-the-art detectors such as EfficientDet and YOLOv4.Source\ncode is at https://github.com/PaddlePaddle/PaddleDetection.\n", "rewritten_text": "This paper presents PP-YOLO, a practical object detector built upon the widely used YOLOv3 architecture.  Addressing the common trade-off between accuracy and inference speed, PP-YOLO focuses on improving accuracy without significantly increasing computational cost (model parameters or FLOPs) by incorporating various existing optimization techniques.  Evaluated using PaddlePaddle, PP-YOLO achieves a strong balance of effectiveness (45.2% mAP) and efficiency (72.9 FPS), outperforming state-of-the-art detectors like EfficientDet and YOLOv4.  The source code is available at https://github.com/PaddlePaddle/PaddleDetection.\n"}, "2406.15823": {"original_text": "  Understanding the abilities of LLMs to reason about natural language plans,\nsuch as instructional text and recipes, is critical to reliably using them in\ndecision-making systems. A fundamental aspect of plans is the temporal order in\nwhich their steps needs to be executed, which reflects the underlying causal\ndependencies between them. We introduce CaT-Bench, a benchmark of Step Order\nPrediction questions, which test whether a step must necessarily occur before\nor after another in cooking recipe plans. We use this to evaluate how well\nfrontier LLMs understand causal and temporal dependencies. We find that SOTA\nLLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased\ntowards predicting dependence more often, perhaps relying on temporal order of\nsteps as a heuristic. While prompting for explanations and using few-shot\nexamples improve performance, the best F1 result is only 0.73. Further, human\nevaluation of explanations along with answer correctness show that, on average,\nhumans do not agree with model reasoning. Surprisingly, we also find that\nexplaining after answering leads to better performance than normal\nchain-of-thought prompting, and LLM answers are not consistent across questions\nabout the same step pairs. Overall, results show that LLMs' ability to detect\ndependence between steps has significant room for improvement.\n", "rewritten_text": "To reliably integrate large language models (LLMs) into decision-making systems, we must understand their capacity to reason about sequential tasks, like following recipes.  We introduce CaT-Bench, a benchmark evaluating LLMs' ability to predict the necessary temporal order of steps in cooking recipes, reflecting underlying causal relationships.  Our evaluation reveals that even state-of-the-art LLMs perform poorly (best zero-shot F1 score: 0.59), exhibiting a bias towards predicting dependencies and potentially relying on superficial temporal cues.  While prompting techniques like providing explanations and few-shot examples improve performance (best F1: 0.73), human evaluation reveals significant disagreement with the models' reasoning.  Interestingly, explaining *after* answering improves accuracy, highlighting inconsistencies in LLM responses.  These findings demonstrate a substantial need for improved LLM understanding of causal and temporal dependencies in sequential tasks.\n"}, "1907.06007": {"original_text": "  With the development of deep neural networks, the demand for a significant\namount of annotated training data becomes the performance bottlenecks in many\nfields of research and applications. Image synthesis can generate annotated\nimages automatically and freely, which gains increasing attention recently. In\nthis paper, we propose to synthesize scene text images from the 3D virtual\nworlds, where the precise descriptions of scenes, editable\nillumination/visibility, and realistic physics are provided. Different from the\nprevious methods which paste the rendered text on static 2D images, our method\ncan render the 3D virtual scene and text instances as an entirety. In this way,\nreal-world variations, including complex perspective transformations, various\nilluminations, and occlusions, can be realized in our synthesized scene text\nimages. Moreover, the same text instances with various viewpoints can be\nproduced by randomly moving and rotating the virtual camera, which acts as\nhuman eyes. The experiments on the standard scene text detection benchmarks\nusing the generated synthetic data demonstrate the effectiveness and\nsuperiority of the proposed method. The code and synthetic data is available\nat: https://github.com/MhLiao/SynthText3D\n", "rewritten_text": "Deep learning's reliance on vast annotated datasets creates a major bottleneck.  Synthetic image generation offers a solution, and this paper introduces a novel method for creating realistic scene text images from 3D virtual worlds. Unlike previous approaches that overlay text onto existing 2D images, our method renders text and scene as a single entity, enabling realistic variations in perspective, lighting, and occlusion.  By manipulating a virtual camera, we generate multiple viewpoints of the same text instance, mimicking human observation.  Experiments on standard scene text detection benchmarks demonstrate the effectiveness of our approach, which significantly improves performance.  The code and dataset are publicly available at: https://github.com/MhLiao/SynthText3D\n"}, "2006.13017": {"original_text": "  Recently, 3D convolutional networks (3D ConvNets) yield good performance in\naction recognition. However, optical flow stream is still needed to ensure\nbetter performance, the cost of which is very high. In this paper, we propose a\nfast but effective way to extract motion features from videos utilizing\nresidual frames as the input data in 3D ConvNets. By replacing traditional\nstacked RGB frames with residual ones, 35.6% and 26.6% points improvements over\ntop-1 accuracy can be obtained on the UCF101 and HMDB51 datasets when ResNet-18\nmodels are trained from scratch. And we achieved the state-of-the-art results\nin this training mode. Analysis shows that better motion features can be\nextracted using residual frames compared to RGB counterpart. By combining with\na simple appearance path, our proposal can be even better than some methods\nusing optical flow streams.\n", "rewritten_text": "While 3D convolutional networks (3D ConvNets) show promise in action recognition, they often require computationally expensive optical flow for optimal performance.  This paper introduces a fast and effective method for extracting motion features from videos by feeding residual frames directly into a 3D ConvNet.  Replacing standard RGB frame stacks with residual frames resulted in significant accuracy improvements \u2013 35.6% and 26.6% on UCF101 and HMDB51 datasets, respectively, using a ResNet-18 model trained from scratch, achieving state-of-the-art results under these conditions.  Analysis confirms that residual frames yield superior motion features compared to RGB frames.  Furthermore, combining this approach with a simple appearance stream surpasses some methods relying on optical flow.\n"}, "2403.15615": {"original_text": "  Conversation is the subject of increasing interest in the social, cognitive,\nand computational sciences. And yet, as conversational datasets continue to\nincrease in size and complexity, researchers lack scalable methods to segment\nspeech-to-text transcripts into conversational turns--the basic building blocks\nof social interaction. We introduce \"NaturalTurn,\" a turn segmentation\nalgorithm designed to accurately capture the dynamics of naturalistic exchange.\nNaturalTurn operates by distinguishing speakers' primary conversational turns\nfrom listeners' secondary utterances, such as backchannels, brief\ninterjections, and other forms of parallel speech that characterize\nconversation. Using data from a large conversation corpus, we show how\nNaturalTurn-derived transcripts demonstrate favorable statistical and\ninferential characteristics compared to transcripts derived from existing\nmethods. The NaturalTurn algorithm represents an improvement in\nmachine-generated transcript processing methods, or \"turn models\" that will\nenable researchers to associate turn-taking dynamics with the broader outcomes\nthat result from social interaction, a central goal of conversation science.\n", "rewritten_text": "The study of conversation is booming across social, cognitive, and computational sciences.  However, analyzing increasingly large and complex conversational datasets is hampered by a lack of scalable methods for segmenting speech transcripts into conversational turns.  This paper introduces NaturalTurn, a novel algorithm that accurately segments speech into turns by distinguishing primary speaker turns from secondary utterances (e.g., backchannels, interjections).  Evaluated on a large corpus, NaturalTurn produces statistically superior transcripts compared to existing methods.  This improved turn-taking model allows researchers to better link conversational dynamics to broader social interaction outcomes, a key objective in conversation science.\n"}, "1807.00502": {"original_text": "  The use of deep learning for medical imaging has seen tremendous growth in\nthe research community. One reason for the slow uptake of these systems in the\nclinical setting is that they are complex, opaque and tend to fail silently.\nOutside of the medical imaging domain, the machine learning community has\nrecently proposed several techniques for quantifying model uncertainty (i.e.~a\nmodel knowing when it has failed). This is important in practical settings, as\nwe can refer such cases to manual inspection or correction by humans. In this\npaper, we aim to bring these recent results on estimating uncertainty to bear\non two important outputs in deep learning-based segmentation. The first is\nproducing spatial uncertainty maps, from which a clinician can observe where\nand why a system thinks it is failing. The second is quantifying an image-level\nprediction of failure, which is useful for isolating specific cases and\nremoving them from automated pipelines. We also show that reasoning about\nspatial uncertainty, the first output, is a useful intermediate representation\nfor generating segmentation quality predictions, the second output. We propose\na two-stage architecture for producing these measures of uncertainty, which can\naccommodate any deep learning-based medical segmentation pipeline.\n", "rewritten_text": "Deep learning in medical imaging research is booming, but clinical adoption lags due to the complexity, opacity, and silent failure modes of these systems.  To address this, we adapt recent machine learning techniques for quantifying model uncertainty to deep learning-based medical image segmentation.  Our approach generates two key outputs: spatial uncertainty maps, visually highlighting areas of potential system failure for clinicians, and image-level failure predictions, enabling automated pipeline quality control.  We demonstrate that spatial uncertainty serves as a valuable intermediate step in predicting segmentation quality.  Our proposed two-stage architecture is adaptable to any existing deep learning segmentation pipeline.\n"}, "2112.15439": {"original_text": "  This paper aims to conduct a comprehensive study on facial-sketch synthesis\n(FSS). However, due to the high costs of obtaining hand-drawn sketch datasets,\nthere lacks a complete benchmark for assessing the development of FSS\nalgorithms over the last decade. We first introduce a high-quality dataset for\nFSS, named FS2K, which consists of 2,104 image-sketch pairs spanning three\ntypes of sketch styles, image backgrounds, lighting conditions, skin colors,\nand facial attributes. FS2K differs from previous FSS datasets in difficulty,\ndiversity, and scalability and should thus facilitate the progress of FSS\nresearch. Second, we present the largest-scale FSS investigation by reviewing\n89 classical methods, including 25 handcrafted feature-based facial-sketch\nsynthesis approaches, 29 general translation methods, and 35 image-to-sketch\napproaches. Besides, we elaborate comprehensive experiments on the existing 19\ncutting-edge models. Third, we present a simple baseline for FSS, named FSGAN.\nWith only two straightforward components, i.e., facial-aware masking and\nstyle-vector expansion, FSGAN surpasses the performance of all previous\nstate-of-the-art models on the proposed FS2K dataset by a large margin.\nFinally, we conclude with lessons learned over the past years and point out\nseveral unsolved challenges. Our code is available at\nhttps://github.com/DengPingFan/FSGAN.\n", "rewritten_text": "This paper addresses the lack of a comprehensive benchmark for facial-sketch synthesis (FSS) algorithms due to the scarcity of high-quality datasets.  We introduce FS2K, a new dataset of 2,104 image-sketch pairs encompassing diverse styles, backgrounds, lighting, skin tones, and facial attributes, significantly improving upon existing datasets in scale and complexity.  We then conduct a large-scale analysis of 89 FSS methods, encompassing handcrafted, general translation, and image-to-sketch approaches, and evaluate 19 state-of-the-art models.  Furthermore, we propose FSGAN, a simple yet highly effective FSS baseline that outperforms existing methods on FS2K.  Finally, we discuss key insights and future challenges in the field.  Our code is available at https://github.com/DengPingFan/FSGAN.\n"}, "1804.09803": {"original_text": "  The inference structures and computational complexity of existing deep neural\nnetworks, once trained, are fixed and remain the same for all test images.\nHowever, in practice, it is highly desirable to establish a progressive\nstructure for deep neural networks which is able to adapt its inference process\nand complexity for images with different visual recognition complexity. In this\nwork, we develop a multi-stage progressive structure with integrated confidence\nanalysis and decision policy learning for deep neural networks. This new\nframework consists of a set of network units to be activated in a sequential\nmanner with progressively increased complexity and visual recognition power.\nOur extensive experimental results on the CIFAR-10 and ImageNet datasets\ndemonstrate that the proposed progressive deep neural network is able to obtain\nmore than 10 fold complexity scalability while achieving the state-of-the-art\nperformance using a single network model satisfying different\ncomplexity-accuracy requirements.\n", "rewritten_text": "Current deep neural networks have fixed inference structures and computational costs regardless of input image complexity.  This paper introduces a novel multi-stage deep neural network architecture that adapts its inference process and complexity dynamically.  This architecture sequentially activates network units of increasing complexity and recognition power, incorporating confidence analysis and learned decision policies.  Experiments on CIFAR-10 and ImageNet demonstrate a more than tenfold improvement in complexity scalability while achieving state-of-the-art performance across varying accuracy-complexity trade-offs, all within a single model.\n"}, "2312.14988": {"original_text": "  Autoregressive and diffusion models drive the recent breakthroughs on\ntext-to-image generation. Despite their huge success of generating\nhigh-realistic images, a common shortcoming of these models is their high\ninference latency - autoregressive models run more than a thousand times\nsuccessively to produce image tokens and diffusion models convert Gaussian\nnoise into images with many hundreds of denoising steps. In this work, we\nexplore non-autoregressive text-to-image models that efficiently generate\nhundreds of image tokens in parallel. We develop many model variations with\ndifferent learning and inference strategies, initialized text encoders, etc.\nCompared with autoregressive baselines that needs to run one thousand times,\nour model only runs 16 times to generate images of competitive quality with an\norder of magnitude lower inference latency. Our non-autoregressive model with\n346M parameters generates an image of 256$\\times$256 with about one second on\none V100 GPU.\n", "rewritten_text": "Recent advancements in text-to-image generation rely heavily on autoregressive and diffusion models.  However, these models suffer from slow inference times due to their iterative nature: autoregressive models generate images token by token (thousands of iterations), while diffusion models require hundreds of denoising steps.  This work introduces a novel non-autoregressive approach that generates hundreds of image tokens concurrently, significantly improving efficiency.  Through experimentation with various model architectures, training methods, and text encoders, we achieved a speedup of over an order of magnitude.  Our best-performing model (346M parameters) generates 256x256 images in approximately one second on a single V100 GPU, requiring only 16 iterations compared to the thousands needed by autoregressive baselines, while maintaining competitive image quality.\n"}, "2106.16038": {"original_text": "  Recent pretraining models in Chinese neglect two important aspects specific\nto the Chinese language: glyph and pinyin, which carry significant syntax and\nsemantic information for language understanding. In this work, we propose\nChineseBERT, which incorporates both the {\\it glyph} and {\\it pinyin}\ninformation of Chinese characters into language model pretraining. The glyph\nembedding is obtained based on different fonts of a Chinese character, being\nable to capture character semantics from the visual features, and the pinyin\nembedding characterizes the pronunciation of Chinese characters, which handles\nthe highly prevalent heteronym phenomenon in Chinese (the same character has\ndifferent pronunciations with different meanings). Pretrained on large-scale\nunlabeled Chinese corpus, the proposed ChineseBERT model yields significant\nperformance boost over baseline models with fewer training steps. The porpsoed\nmodel achieves new SOTA performances on a wide range of Chinese NLP tasks,\nincluding machine reading comprehension, natural language inference, text\nclassification, sentence pair matching, and competitive performances in named\nentity recognition. Code and pretrained models are publicly available at\nhttps://github.com/ShannonAI/ChineseBert.\n", "rewritten_text": "Existing Chinese pre-trained language models overlook crucial linguistic features: glyphs and pinyin.  To address this, we introduce ChineseBERT, a model that integrates both visual glyph embeddings (derived from variations in character fonts) and phonetic pinyin embeddings. This dual approach captures semantic nuances from visual features and resolves ambiguities arising from Chinese heteronyms (characters with multiple pronunciations and meanings).  Trained on a large unlabeled corpus, ChineseBERT significantly outperforms baseline models, achieving state-of-the-art results on various Chinese NLP tasks, including machine reading comprehension, natural language inference, text classification, and sentence pair matching, and competitive results in named entity recognition.  The code and pre-trained models are publicly available at https://github.com/ShannonAI/ChineseBert.\n"}, "2103.05342": {"original_text": "  In this paper, we propose a novel data augmentation strategy named\nCut-Thumbnail, that aims to improve the shape bias of the network. We reduce an\nimage to a certain size and replace the random region of the original image\nwith the reduced image. The generated image not only retains most of the\noriginal image information but also has global information in the reduced\nimage. We call the reduced image as thumbnail. Furthermore, we find that the\nidea of thumbnail can be perfectly integrated with Mixed Sample Data\nAugmentation, so we put one image's thumbnail on another image while the ground\ntruth labels are also mixed, making great achievements on various computer\nvision tasks. Extensive experiments show that Cut-Thumbnail works better than\nstate-of-the-art augmentation strategies across classification, fine-grained\nimage classification, and object detection. On ImageNet classification,\nResNet-50 architecture with our method achieves 79.21\\% accuracy, which is more\nthan 2.8\\% improvement on the baseline.\n", "rewritten_text": "This paper introduces Cut-Thumbnail, a novel data augmentation technique designed to mitigate shape bias in neural networks.  Cut-Thumbnail downsizes an image to create a \"thumbnail,\" then replaces a random region of the original image with this thumbnail. This preserves most original image information while incorporating global context from the thumbnail.  We further integrate this with Mixed Sample Data Augmentation, overlaying one image's thumbnail onto another, along with corresponding label mixing.  Extensive experiments across classification, fine-grained classification, and object detection demonstrate Cut-Thumbnail's superiority over existing state-of-the-art augmentation methods.  For instance, on ImageNet, ResNet-50 achieves a 79.21% accuracy, a 2.8% improvement over the baseline.\n"}, "2307.15257": {"original_text": "  The complexity of learning problems, such as Generative Adversarial Network\n(GAN) and its variants, multi-task and meta-learning, hyper-parameter learning,\nand a variety of real-world vision applications, demands a deeper understanding\nof their underlying coupling mechanisms. Existing approaches often address\nthese problems in isolation, lacking a unified perspective that can reveal\ncommonalities and enable effective solutions. Therefore, in this work, we\nproposed a new framework, named Learning with Constraint Learning (LwCL), that\ncan holistically examine challenges and provide a unified methodology to tackle\nall the above-mentioned complex learning and vision problems. Specifically,\nLwCL is designed as a general hierarchical optimization model that captures the\nessence of these diverse learning and vision problems. Furthermore, we develop\na gradient-response based fast solution strategy to overcome optimization\nchallenges of the LwCL framework. Our proposed framework efficiently addresses\na wide range of applications in learning and vision, encompassing three\ncategories and nine different problem types. Extensive experiments on synthetic\ntasks and real-world applications verify the effectiveness of our approach. The\nLwCL framework offers a comprehensive solution for tackling complex machine\nlearning and computer vision problems, bridging the gap between theory and\npractice.\n", "rewritten_text": "This work introduces Learning with Constraint Learning (LwCL), a novel hierarchical optimization framework addressing the complex interplay of various machine learning and computer vision problems.  Unlike existing isolated approaches, LwCL provides a unified perspective on challenges such as GANs, multi-task and meta-learning, hyperparameter optimization, and real-world vision applications.  A fast, gradient-response based solution strategy overcomes the framework's optimization difficulties.  Evaluated across nine diverse problem types within three application categories using both synthetic and real-world data, LwCL demonstrates its effectiveness and bridges the gap between theoretical understanding and practical application in complex machine learning and computer vision.\n"}, "2309.11119": {"original_text": "  A recent sensor fusion in a Bird's Eye View (BEV) space has shown its utility\nin various tasks such as 3D detection, map segmentation, etc. However, the\napproach struggles with inaccurate camera BEV estimation, and a perception of\ndistant areas due to the sparsity of LiDAR points. In this paper, we propose a\nbroad BEV fusion (BroadBEV) that addresses the problems with a spatial\nsynchronization approach of cross-modality. Our strategy aims to enhance camera\nBEV estimation for a broad-sighted perception while simultaneously improving\nthe completion of LiDAR's sparsity in the entire BEV space. Toward that end, we\ndevise Point-scattering that scatters LiDAR BEV distribution to camera depth\ndistribution. The method boosts the learning of depth estimation of the camera\nbranch and induces accurate location of dense camera features in BEV space. For\nan effective BEV fusion between the spatially synchronized features, we suggest\nColFusion that applies self-attention weights of LiDAR and camera BEV features\nto each other. Our extensive experiments demonstrate that BroadBEV provides a\nbroad-sighted BEV perception with remarkable performance gains.\n", "rewritten_text": "Existing Bird's Eye View (BEV) sensor fusion methods, while useful for tasks like 3D object detection and map segmentation, suffer from inaccuracies in camera-based BEV estimation and limited perception of distant areas due to sparse LiDAR data.  This paper introduces BroadBEV, a novel approach that addresses these limitations through cross-modality spatial synchronization.  BroadBEV enhances camera BEV estimation for wider field-of-view perception and compensates for LiDAR sparsity using a novel Point-scattering technique. This technique distributes LiDAR BEV data to match camera depth distribution, improving camera depth estimation and precisely locating dense camera features in BEV space.  Furthermore, a novel ColFusion module uses self-attention to effectively fuse the spatially synchronized LiDAR and camera BEV features.  Extensive experiments demonstrate BroadBEV's superior performance and broad-sighted BEV perception.\n"}, "2403.17920": {"original_text": "  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.\n", "rewritten_text": "Current text-to-4D methods, while leveraging pre-trained text-to-video models, struggle to generate realistic, extensive motion due to limitations in their motion representation models.  These models often fail to produce motion beyond the initial object boundaries.  To address this, we introduce TC4D, a trajectory-conditioned text-to-4D generation approach.  TC4D separates motion into global (rigid transformations along spline-parameterized trajectories) and local (deformations guided by a text-to-video model) components. This allows for the generation of scenes animated along arbitrary paths, compositional scene creation, and significantly improved realism and motion extent.  Qualitative and user study evaluations support these claims.  See our results at https://sherwinbahmani.github.io/tc4d.\n"}, "2407.11144": {"original_text": "  Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25.\n", "rewritten_text": "Machine learning research on sign language is hampered by a lack of data, especially for languages beyond American Sign Language (ASL).  To address this, we introduce YouTube-SL-25, a massive, multilingual corpus of over 3000 hours of YouTube sign language videos with aligned captions, encompassing more than 25 languages.  This dataset is over three times larger than existing YouTube-ASL datasets, making it the largest parallel sign language dataset to date and the first or largest for many included languages.  We establish baseline performance on sign-to-text tasks using a multilingual T5-based model, demonstrating cross-lingual transfer learning benefits for both high- and low-resource sign languages within the corpus.\n"}, "1702.01776": {"original_text": "  In aspect-based sentiment analysis, most existing methods either focus on\naspect/opinion terms extraction or aspect terms categorization. However, each\ntask by itself only provides partial information to end users. To generate more\ndetailed and structured opinion analysis, we propose a finer-grained problem,\nwhich we call category-specific aspect and opinion terms extraction. This\nproblem involves the identification of aspect and opinion terms within each\nsentence, as well as the categorization of the identified terms. To this end,\nwe propose an end-to-end multi-task attention model, where each task\ncorresponds to aspect/opinion terms extraction for a specific category. Our\nmodel benefits from exploring the commonalities and relationships among\ndifferent tasks to address the data sparsity issue. We demonstrate its\nstate-of-the-art performance on three benchmark datasets.\n", "rewritten_text": "Current aspect-based sentiment analysis methods typically focus solely on extracting aspect/opinion terms or categorizing them, yielding incomplete analyses.  To achieve more comprehensive understanding, we introduce category-specific aspect and opinion term extraction, a finer-grained task integrating term identification and categorization.  We address this with a novel end-to-end multi-task attention model, where each task extracts aspect/opinion terms for a specific category.  This approach leverages inter-task relationships to mitigate data sparsity, achieving state-of-the-art results on three benchmark datasets.\n"}, "2301.00399": {"original_text": "  In the present paper, semantic parsing challenges are briefly introduced and\nQDMR formalism in semantic parsing is implemented using sequence to sequence\nmodel with attention but uses only part of speech(POS) as a representation of\nwords of a sentence to make the training as simple and as fast as possible and\nalso avoiding curse of dimensionality as well as overfitting. It is shown how\nsemantic operator prediction could be augmented with other models like the\nCopyNet model or the recursive neural net model.\n", "rewritten_text": "This paper explores semantic parsing challenges and presents a simplified sequence-to-sequence model with attention for QDMR formalism.  To expedite training and mitigate the curse of dimensionality and overfitting, the model uses only part-of-speech (POS) tags as word representations.  The paper also suggests augmenting semantic operator prediction with models like CopyNet or recursive neural networks.\n"}, "2411.07075": {"original_text": "  To predict upcoming text, language models must in some cases retrieve\nin-context information verbatim. In this report, we investigated how the\nability of language models to retrieve arbitrary in-context nouns developed\nduring training (across time) and as language models trained on the same\ndataset increase in size (across scale). We then asked whether learning of\nin-context retrieval correlates with learning of more challenging zero-shot\nbenchmarks. Furthermore, inspired by semantic effects in human short-term\nmemory, we evaluated the retrieval with respect to a major semantic component\nof target nouns, namely whether they denote a concrete or abstract entity, as\nrated by humans. We show that verbatim in-context retrieval developed in a\nsudden transition early in the training process, after about 1% of the training\ntokens. This was observed across model sizes (from 14M and up to 12B\nparameters), and the transition occurred slightly later for the two smallest\nmodels. We further found that the development of verbatim in-context retrieval\nis positively correlated with the learning of zero-shot benchmarks. Around the\ntransition point, all models showed the advantage of retrieving concrete nouns\nas opposed to abstract nouns. In all but two smallest models, the advantage\ndissipated away toward the end of training.\n", "rewritten_text": "This study examines the development of verbatim in-context information retrieval in language models during training.  We analyzed this ability across both training time and model size, investigating its correlation with performance on zero-shot benchmarks.  Inspired by human memory, we also considered the impact of noun concreteness (abstract vs. concrete) on retrieval.  Results reveal a sharp emergence of verbatim retrieval early in training (after roughly 1% of tokens processed), consistent across model sizes (14M to 12B parameters), though slightly delayed in the smallest models.  This improved retrieval significantly correlated with better zero-shot benchmark performance.  Finally, all models initially showed a preference for retrieving concrete nouns, a bias that diminished in larger models by the end of training.\n"}, "1911.11351": {"original_text": "  Recently, Human Attribute Recognition (HAR) has become a hot topic due to its\nscientific challenges and application potentials, where localizing attributes\nis a crucial stage but not well handled. In this paper, we propose a novel deep\nlearning approach to HAR, namely Distraction-aware HAR (Da-HAR). It enhances\ndeep CNN feature learning by improving attribute localization through a\ncoarse-to-fine attention mechanism. At the coarse step, a self-mask block is\nbuilt to roughly discriminate and reduce distractions, while at the fine step,\na masked attention branch is applied to further eliminate irrelevant regions.\nThanks to this mechanism, feature learning is more accurate, especially when\nheavy occlusions and complex backgrounds exist. Extensive experiments are\nconducted on the WIDER-Attribute and RAP databases, and state-of-the-art\nresults are achieved, demonstrating the effectiveness of the proposed approach.\n", "rewritten_text": "Human Attribute Recognition (HAR) is a rapidly growing field, hampered by the difficulty of accurately localizing attributes.  This paper introduces Distraction-aware HAR (Da-HAR), a novel deep learning approach that addresses this limitation. Da-HAR employs a coarse-to-fine attention mechanism to improve deep CNN feature learning.  A self-mask block initially reduces distractions, followed by a masked attention branch for precise irrelevant region removal. This two-stage process significantly improves accuracy, particularly in challenging scenarios with occlusions and complex backgrounds.  Experiments on WIDER-Attribute and RAP datasets demonstrate state-of-the-art performance, validating the effectiveness of Da-HAR.\n"}, "2310.08487": {"original_text": "  While multi-modal models have successfully integrated information from image,\nvideo, and audio modalities, integrating graph modality into large language\nmodels (LLMs) remains unexplored. This discrepancy largely stems from the\ninherent divergence between structured graph data and unstructured text data.\nIncorporating graph knowledge provides a reliable source of information,\nenabling potential solutions to address issues in text generation, e.g.,\nhallucination, and lack of domain knowledge. To evaluate the integration of\ngraph knowledge into language models, a dedicated dataset is needed. However,\nthere is currently no benchmark dataset specifically designed for multimodal\ngraph-language models. To address this gap, we propose GraphextQA, a question\nanswering dataset with paired subgraphs, retrieved from Wikidata, to facilitate\nthe evaluation and future development of graph-language models. Additionally,\nwe introduce a baseline model called CrossGNN, which conditions answer\ngeneration on the paired graphs by cross-attending question-aware graph\nfeatures at decoding. The proposed dataset is designed to evaluate\ngraph-language models' ability to understand graphs and make use of it for\nanswer generation. We perform experiments with language-only models and the\nproposed graph-language model to validate the usefulness of the paired graphs\nand to demonstrate the difficulty of the task.\n", "rewritten_text": "Large language models (LLMs) effectively integrate image, video, and audio data, but integrating graph data remains largely uncharted territory due to the fundamental differences between structured graphs and unstructured text.  This integration offers significant potential for improving text generation, particularly by mitigating issues like hallucinations and knowledge gaps.  To enable research in this area, we introduce GraphextQA, a novel question-answering benchmark dataset featuring Wikidata-derived paired subgraphs.  We also present CrossGNN, a baseline model that leverages these paired graphs via cross-attention during answer generation.  Experiments comparing language-only models with CrossGNN highlight the value of graph integration and the challenge of this task.\n"}, "cs/0010012": {"original_text": "  We describe a new framework for distilling information from word lattices to\nimprove the accuracy of speech recognition and obtain a more perspicuous\nrepresentation of a set of alternative hypotheses. In the standard MAP decoding\napproach the recognizer outputs the string of words corresponding to the path\nwith the highest posterior probability given the acoustics and a language\nmodel. However, even given optimal models, the MAP decoder does not necessarily\nminimize the commonly used performance metric, word error rate (WER). We\ndescribe a method for explicitly minimizing WER by extracting word hypotheses\nwith the highest posterior probabilities from word lattices. We change the\nstandard problem formulation by replacing global search over a large set of\nsentence hypotheses with local search over a small set of word candidates. In\naddition to improving the accuracy of the recognizer, our method produces a new\nrepresentation of the set of candidate hypotheses that specifies the sequence\nof word-level confusions in a compact lattice format. We study the properties\nof confusion networks and examine their use for other tasks, such as lattice\ncompression, word spotting, confidence annotation, and reevaluation of\nrecognition hypotheses using higher-level knowledge sources.\n", "rewritten_text": "This paper introduces a novel framework for enhancing speech recognition accuracy and creating clearer representations of alternative hypotheses by distilling information from word lattices.  Unlike standard maximum a posteriori (MAP) decoding, which selects the single highest-probability word sequence, our method directly minimizes word error rate (WER) by selecting the most probable word candidates from the lattice. This involves a shift from global sentence-level search to a local word-level search, resulting in improved accuracy and a compact lattice representation\u2014a \"confusion network\"\u2014highlighting word-level ambiguities.  We explore the applications of confusion networks for various tasks, including lattice compression, word spotting, confidence scoring, and integrating higher-level knowledge sources for improved hypothesis reevaluation.\n"}, "2111.03993": {"original_text": "  Skeleton data is of low dimension. However, there is a trend of using very\ndeep and complicated feedforward neural networks to model the skeleton sequence\nwithout considering the complexity in recent year. In this paper, a simple yet\neffective multi-scale semantics-guided neural network (MS-SGN) is proposed for\nskeleton-based action recognition. We explicitly introduce the high level\nsemantics of joints (joint type and frame index) into the network to enhance\nthe feature representation capability of joints. Moreover, a multi-scale\nstrategy is proposed to be robust to the temporal scale variations. In\naddition, we exploit the relationship of joints hierarchically through two\nmodules, i.e., a joint-level module for modeling the correlations of joints in\nthe same frame and a frame-level module for modeling the temporal dependencies\nof frames. With an order of magnitude smaller model size than most previous\nmethods, MSSGN achieves the state-of-the-art performance on the NTU60, NTU120,\nand SYSU datasets.\n", "rewritten_text": "While skeleton data is low-dimensional, recent approaches to skeleton-based action recognition have employed overly complex deep neural networks.  This paper introduces MS-SGN, a simpler yet highly effective multi-scale semantics-guided neural network.  MS-SGN leverages high-level joint semantics (type and frame index) to improve feature representation and employs a multi-scale strategy for robustness to temporal variations.  Furthermore, it hierarchically models joint relationships using joint-level and frame-level modules.  Despite its significantly smaller size, MS-SGN achieves state-of-the-art performance on NTU60, NTU120, and SYSU datasets.\n"}, "1808.06428": {"original_text": "  This paper presents an approach for automatic detection of Munro's\nMicroabscess in stratum corneum (SC) of human skin biopsy in order to realize a\nmachine assisted diagnosis of Psoriasis. The challenge of detecting neutrophils\nin presence of nucleated cells is solved using the recent advances of deep\nlearning algorithms. Separation of SC layer, extraction of patches from the\nlayer followed by classification of patches with respect to presence or absence\nof neutrophils form the basis of the overall approach which is effected through\nan integration of a U-Net based segmentation network and a capsule network for\nclassification. The novel design of the present capsule net leads to a drastic\nreduction in the number of parameters without any noticeable compromise in the\noverall performance. The research further addresses the challenge of dealing\nwith Mega-pixel images (in 10X) vis-a-vis Giga-pixel ones (in 40X). The\npromising result coming out of an experiment on a dataset consisting of 273\nreal-life images shows that a practical system is possible based on the present\nresearch. The implementation of our system is available at\nhttps://github.com/Anabik/CapsDeMM.\n", "rewritten_text": "This paper introduces a deep learning-based system for automated detection of Munro's microabscesses in human skin biopsy images, aiding in psoriasis diagnosis.  Addressing the difficulty of identifying neutrophils amidst nucleated cells, the system leverages a U-Net for stratum corneum (SC) layer segmentation and a novel, parameter-efficient capsule network for classifying patches as containing neutrophils or not.  This approach effectively handles both megapixel (10X) and gigapixel (40X) images.  Experiments on a dataset of 273 real-world images demonstrate the system's feasibility, with the code available at https://github.com/Anabik/CapsDeMM.\n"}, "1302.1422": {"original_text": "  The variation of word meaning according to the context leads us to enrich the\ntype system of our syntactical and semantic analyser of French based on\ncategorial grammars and Montague semantics (or lambda-DRT). The main advantage\nof a deep semantic analyse is too represent meaning by logical formulae that\ncan be easily used e.g. for inferences. Determiners and quantifiers play a\nfundamental role in the construction of those formulae. But in our rich type\nsystem the usual semantic terms do not work. We propose a solution ins- pired\nby the tau and epsilon operators of Hilbert, kinds of generic elements and\nchoice functions. This approach unifies the treatment of the different determi-\nners and quantifiers as well as the dynamic binding of pronouns. Above all,\nthis fully computational view fits in well within the wide coverage parser\nGrail, both from a theoretical and a practical viewpoint.\n", "rewritten_text": "Contextual word-meaning variation necessitates a richer type system for our French syntactic and semantic analyzer, which is based on categorial grammars and Montague semantics (or lambda-DRT).  Deep semantic analysis, representing meaning via easily inferable logical formulas, is key.  However, standard semantic terms are insufficient within our enhanced type system.  We propose a solution using Hilbert's tau and epsilon operators\u2014generic elements and choice functions\u2014to unify the treatment of determiners, quantifiers, and dynamic pronoun binding. This computationally efficient approach integrates seamlessly with the Grail wide-coverage parser, both theoretically and practically.\n"}, "2012.07315": {"original_text": "  The categorical distribution is a natural representation of uncertainty in\nmulti-class segmentations. In the two-class case the categorical distribution\nreduces to the Bernoulli distribution, for which grayscale morphology provides\na range of useful operations. In the general case, applying morphological\noperations on uncertain multi-class segmentations is not straightforward as an\nimage of categorical distributions is not a complete lattice. Although\nmorphology on color images has received wide attention, this is not so for\ncolor-coded or categorical images and even less so for images of categorical\ndistributions. In this work, we establish a set of requirements for morphology\non categorical distributions by combining classic morphology with a\nprobabilistic view. We then define operators respecting these requirements,\nintroduce protected operations on categorical distributions and illustrate the\nutility of these operators on two example tasks: modeling annotator bias in\nbrain tumor segmentations and segmenting vesicle instances from the predictions\nof a multi-class U-Net.\n", "rewritten_text": "Multi-class segmentations are naturally represented by categorical distributions, simplifying to Bernoulli distributions (and thus amenable to grayscale morphology) in the two-class case.  However, extending morphological operations to general multi-class segmentations\u2014represented as images of categorical distributions\u2014is challenging due to the lack of complete lattice structure. While color image morphology is well-studied, this is not true for categorical or color-coded images, especially those representing categorical distributions.  This paper addresses this gap by developing a probabilistic framework for morphological operations on categorical distributions.  We define operators satisfying key requirements, introduce \"protected\" operations, and demonstrate their effectiveness in two applications: modeling annotator bias in brain tumor segmentation and segmenting vesicles using multi-class U-Net predictions.\n"}, "2410.24219": {"original_text": "  Despite advancements in Text-to-Video (T2V) generation, producing videos with\nrealistic motion remains challenging. Current models often yield static or\nminimally dynamic outputs, failing to capture complex motions described by\ntext. This issue stems from the internal biases in text encoding, which\noverlooks motions, and inadequate conditioning mechanisms in T2V generation\nmodels. To address this, we propose a novel framework called DEcomposed MOtion\n(DEMO), which enhances motion synthesis in T2V generation by decomposing both\ntext encoding and conditioning into content and motion components. Our method\nincludes a content encoder for static elements and a motion encoder for\ntemporal dynamics, alongside separate content and motion conditioning\nmechanisms. Crucially, we introduce text-motion and video-motion supervision to\nimprove the model's understanding and generation of motion. Evaluations on\nbenchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench\ndemonstrate DEMO's superior ability to produce videos with enhanced motion\ndynamics while maintaining high visual quality. Our approach significantly\nadvances T2V generation by integrating comprehensive motion understanding\ndirectly from textual descriptions. Project page:\nhttps://PR-Ryan.github.io/DEMO-project/\n", "rewritten_text": "Generating realistic motion in text-to-video (T2V) remains a significant hurdle.  Existing models struggle with dynamic content, producing static or limited movement due to biases in text encoding and insufficient conditioning.  To overcome this, we introduce DEMO (DEcomposed MOtion), a novel framework that separates text and conditioning into content and motion components.  This involves distinct content and motion encoders, and dedicated conditioning mechanisms.  Crucially, we incorporate text-motion and video-motion supervision to improve motion understanding and generation.  Benchmark results (MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, VBench) show DEMO significantly improves motion realism in generated videos while preserving visual quality.  Our approach advances the state-of-the-art in T2V by directly integrating comprehensive motion understanding from text.  See our project page: https://PR-Ryan.github.io/DEMO-project/\n"}, "2406.09386": {"original_text": "  Controllable synthetic data generation can substantially lower the annotation\ncost of training data. Prior works use diffusion models to generate driving\nimages conditioned on the 3D object layout. However, those models are trained\non small-scale datasets like nuScenes, which lack appearance and layout\ndiversity. Moreover, overfitting often happens, where the trained models can\nonly generate images based on the layout data from the validation set of the\nsame dataset. In this work, we introduce a simulator-conditioned scene\ngeneration framework called SimGen that can learn to generate diverse driving\nscenes by mixing data from the simulator and the real world. It uses a novel\ncascade diffusion pipeline to address challenging sim-to-real gaps and\nmulti-condition conflicts. A driving video dataset DIVA is collected to enhance\nthe generative diversity of SimGen, which contains over 147.5 hours of\nreal-world driving videos from 73 locations worldwide and simulated driving\ndata from the MetaDrive simulator. SimGen achieves superior generation quality\nand diversity while preserving controllability based on the text prompt and the\nlayout pulled from a simulator. We further demonstrate the improvements brought\nby SimGen for synthetic data augmentation on the BEV detection and segmentation\ntask and showcase its capability in safety-critical data generation.\n", "rewritten_text": "Generating synthetic training data offers significant cost savings in data annotation.  While diffusion models have been used to create driving images from 3D layouts, their performance is limited by the small, homogenous datasets (like nuScenes) typically used for training, leading to overfitting.  Our novel framework, SimGen, overcomes these limitations by leveraging a cascade diffusion pipeline to integrate real-world and simulated driving data, bridging the sim-to-real gap.  Trained on the new DIVA dataset (147.5+ hours of real-world driving videos from 73 global locations and MetaDrive simulated data), SimGen generates high-quality, diverse driving scenes controllable via text prompts and simulator-provided layouts.  We demonstrate SimGen's effectiveness in augmenting training data for improved performance on BEV detection and segmentation tasks, and highlight its potential for generating safety-critical scenarios.\n"}, "2311.02313": {"original_text": "  Large-scale semantic mapping is crucial for outdoor autonomous agents to\nfulfill high-level tasks such as planning and navigation. This paper proposes a\nnovel method for large-scale 3D semantic reconstruction through implicit\nrepresentations from posed LiDAR measurements alone. We first leverage an\noctree-based and hierarchical structure to store implicit features, then these\nimplicit features are decoded to semantic information and signed distance value\nthrough shallow Multilayer Perceptrons (MLPs). We adopt off-the-shelf\nalgorithms to predict the semantic labels and instance IDs of point clouds. We\nthen jointly optimize the feature embeddings and MLPs parameters with a\nself-supervision paradigm for point cloud geometry and a pseudo-supervision\nparadigm for semantic and panoptic labels. Subsequently, categories and\ngeometric structures for novel points are regressed, and marching cubes are\nexploited to subdivide and visualize the scenes in the inferring stage. For\nscenarios with memory constraints, a map stitching strategy is also developed\nto merge sub-maps into a complete map. Experiments on two real-world datasets,\nSemanticKITTI and SemanticPOSS, demonstrate the superior segmentation\nefficiency and mapping effectiveness of our framework compared to current\nstate-of-the-art 3D LiDAR mapping methods.\n", "rewritten_text": "This paper presents a novel method for creating large-scale 3D semantic maps using only LiDAR data.  Our approach leverages an octree structure to efficiently store implicit features, which are then decoded into semantic information and signed distance values via shallow MLPs.  Semantic and instance segmentation are performed using existing algorithms, and the entire system is trained using a self-supervised approach for geometry and a pseudo-supervised approach for semantic labels.  The method generates a 3D mesh representation using marching cubes, and a map stitching strategy addresses memory limitations.  Experiments on SemanticKITTI and SemanticPOSS datasets demonstrate that our method outperforms current state-of-the-art 3D LiDAR mapping techniques in both segmentation efficiency and overall mapping quality.\n"}, "2203.09333": {"original_text": "  Perceiving the similarity between images has been a long-standing and\nfundamental problem underlying various visual generation tasks. Predominant\napproaches measure the inter-image distance by computing pointwise absolute\ndeviations, which tends to estimate the median of instance distributions and\nleads to blurs and artifacts in the generated images. This paper presents\nMoNCE, a versatile metric that introduces image contrast to learn a calibrated\nmetric for the perception of multifaceted inter-image distances. Unlike vanilla\ncontrast which indiscriminately pushes negative samples from the anchor\nregardless of their similarity, we propose to re-weight the pushing force of\nnegative samples adaptively according to their similarity to the anchor, which\nfacilitates the contrastive learning from informative negative samples. Since\nmultiple patch-level contrastive objectives are involved in image distance\nmeasurement, we introduce optimal transport in MoNCE to modulate the pushing\nforce of negative samples collaboratively across multiple contrastive\nobjectives. Extensive experiments over multiple image translation tasks show\nthat the proposed MoNCE outperforms various prevailing metrics substantially.\nThe code is available at https://github.com/fnzhan/MoNCE.\n", "rewritten_text": "Accurately assessing image similarity is crucial for many visual generation tasks.  Existing methods often rely on simple distance metrics, leading to blurry or flawed outputs.  This paper introduces MoNCE, a novel metric that leverages image contrast to learn a more nuanced understanding of inter-image differences.  Unlike standard contrastive learning, MoNCE adaptively weights negative samples based on their similarity to the target image, focusing learning on informative examples.  Furthermore, MoNCE employs optimal transport to coordinate multiple patch-level comparisons.  Extensive experiments on various image translation tasks demonstrate MoNCE's significant superiority over existing metrics.  The code is available at https://github.com/fnzhan/MoNCE.\n"}, "2402.17292": {"original_text": "  Text-to-Avatar generation has recently made significant strides due to\nadvancements in diffusion models. However, most existing work remains\nconstrained by limited diversity, producing avatars with subtle differences in\nappearance for a given text prompt. We design DivAvatar, a novel framework that\ngenerates diverse avatars, empowering 3D creatives with a multitude of distinct\nand richly varied 3D avatars from a single text prompt. Different from most\nexisting work that exploits scene-specific 3D representations such as NeRF,\nDivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse\navatar generation from simply noise sampling in inference time. DivAvatar has\ntwo key designs that help achieve generation diversity and visual quality. The\nfirst is a noise sampling technique during training phase which is critical in\ngenerating diverse appearances. The second is a semantic-aware zoom mechanism\nand a novel depth loss, the former producing appearances of high textual\nfidelity by separate fine-tuning of specific body parts and the latter\nimproving geometry quality greatly by smoothing the generated mesh in the\nfeatures space. Extensive experiments show that DivAvatar is highly versatile\nin generating avatars of diverse appearances.\n", "rewritten_text": "Recent advancements in diffusion models have significantly improved text-to-avatar generation.  However, current methods often lack diversity, producing subtly different avatars from the same text prompt.  DivAvatar addresses this limitation by introducing a novel framework that generates diverse and richly varied 3D avatars from a single prompt. Unlike approaches relying on scene-specific 3D representations like NeRF, DivAvatar fine-tunes a pre-trained 3D generative model (EVA3D), enabling diverse avatar generation through simple noise sampling during inference.  This diversity is achieved through two key innovations: a novel noise sampling technique during training, and a semantic-aware zoom mechanism coupled with a novel depth loss. The zoom mechanism enhances textual fidelity by fine-tuning specific body parts, while the depth loss improves mesh quality through feature-space smoothing.  Extensive experiments demonstrate DivAvatar's exceptional versatility in generating diverse and high-quality avatars.\n"}, "2208.13078": {"original_text": "  Owing to the lack of corpora for low-resource languages, current works on\ndialogue generation have mainly focused on English. In this paper, we present\nmDIA, the first large-scale multilingual benchmark for dialogue generation\nacross low- to high-resource languages. It covers real-life conversations in 46\nlanguages across 19 language families. We present baseline results obtained by\nfine-tuning the multilingual, non-dialogue-focused pre-trained model mT5 as\nwell as English-centric, dialogue-focused pre-trained chatbot DialoGPT. The\nresults show that mT5-based models perform better on sacreBLEU and BertScore\nbut worse on diversity. Even though promising results are found in few-shot and\nzero-shot scenarios, there is a large gap between the generation quality in\nEnglish and other languages. We hope that the release of mDIA could encourage\nmore works on multilingual dialogue generation to promote language diversity.\n", "rewritten_text": "Existing dialogue generation research heavily favors English due to a scarcity of datasets for low-resource languages.  This paper introduces mDIA, the first large-scale multilingual benchmark for dialogue generation encompassing 46 languages from 19 families.  We evaluate both multilingual (mT5) and English-centric (DialoGPT) pre-trained models.  While mT5 achieves higher sacreBLEU and BertScore scores, it lacks the diversity of DialoGPT.  Although promising few-shot and zero-shot results were obtained, a significant performance gap persists between English and other languages.  We anticipate that mDIA will stimulate further research into multilingual dialogue generation, fostering greater linguistic diversity.\n"}, "2310.08908": {"original_text": "  The large language model (LLM) has garnered significant attention due to its\nin-context learning mechanisms and emergent capabilities. The research\ncommunity has conducted several pilot studies to apply LLMs to machine\ntranslation tasks and evaluate their performance from diverse perspectives.\nHowever, previous research has primarily focused on the LLM itself and has not\nexplored human intervention in the inference process of LLM. The\ncharacteristics of LLM, such as in-context learning and prompt engineering,\nclosely mirror human cognitive abilities in language tasks, offering an\nintuitive solution for human-in-the-loop generation. In this study, we propose\na human-in-the-loop pipeline that guides LLMs to produce customized outputs\nwith revision instructions. The pipeline initiates by prompting the LLM to\nproduce a draft translation, followed by the utilization of automatic retrieval\nor human feedback as supervision signals to enhance the LLM's translation\nthrough in-context learning. The human-machine interactions generated in this\npipeline are also stored in an external database to expand the in-context\nretrieval database, enabling us to leverage human supervision in an offline\nsetting. We evaluate the proposed pipeline using GPT-3.5-turbo API on five\ndomain-specific benchmarks for German-English translation. The results\ndemonstrate the effectiveness of the pipeline in tailoring in-domain\ntranslations and improving translation performance compared to direct\ntranslation. Additionally, we discuss the results from the following\nperspectives: 1) the effectiveness of different in-context retrieval methods;\n2) the construction of a retrieval database under low-resource scenarios; 3)\nthe observed domains differences; 4) the quantitative analysis of linguistic\nstatistics; and 5) the qualitative analysis of translation cases. The code and\ndata are available at https://github.com/NLP2CT/HIL-MT/.\n", "rewritten_text": "Large language models (LLMs) like GPT-3.5-turbo show promise in machine translation due to their in-context learning.  However, existing research overlooks the potential of human-in-the-loop (HIL) approaches.  This study introduces a novel HIL pipeline for improved machine translation, using human feedback or automatic retrieval to refine LLM-generated drafts via iterative in-context learning.  This feedback is stored to augment the LLM's knowledge base for future offline use.  We evaluate this pipeline on five German-English translation benchmarks, demonstrating improved performance over direct LLM translation.  Our analysis considers the effectiveness of different retrieval methods, database construction under resource constraints, domain-specific variations, linguistic statistics, and qualitative case studies.  Code and data are available at https://github.com/NLP2CT/HIL-MT/.\n"}, "1011.0519": {"original_text": "  It is usual to consider that standards generate mixed feelings among\nscientists. They are often seen as not really reflecting the state of the art\nin a given domain and a hindrance to scientific creativity. Still, scientists\nshould theoretically be at the best place to bring their expertise into\nstandard developments, being even more neutral on issues that may typically be\nrelated to competing industrial interests. Even if it could be thought of as\neven more complex to think about developping standards in the humanities, we\nwill show how this can be made feasible through the experience gained both\nwithin the Text Encoding Initiative consortium and the International\nOrganisation for Standardisation. By taking the specific case of lexical\nresources, we will try to show how this brings about new ideas for designing\nfuture research infrastructures in the human and social sciences.\n", "rewritten_text": "Scientists often view standards with ambivalence, perceiving them as lagging behind current research and stifling creativity.  However, scientists are ideally positioned to contribute their expertise to standard development, offering a neutrality lacking in industry-driven initiatives. While humanities standard-setting might seem more challenging, this paper demonstrates its feasibility using examples from the Text Encoding Initiative and the International Organization for Standardization.  Focusing on lexical resources, we illustrate how this approach fosters innovative designs for future humanities and social sciences research infrastructures.\n"}, "2305.04451": {"original_text": "  Virtual try-on attracts increasing research attention as a promising way for\nenhancing the user experience for online cloth shopping. Though existing\nmethods can generate impressive results, users need to provide a well-designed\nreference image containing the target fashion clothes that often do not exist.\nTo support user-friendly fashion customization in full-body portraits, we\npropose a multi-modal interactive setting by combining the advantages of both\ntext and texture for multi-level fashion manipulation. With the carefully\ndesigned fashion editing module and loss functions, FashionTex framework can\nsemantically control cloth types and local texture patterns without annotated\npairwise training data. We further introduce an ID recovery module to maintain\nthe identity of input portrait. Extensive experiments have demonstrated the\neffectiveness of our proposed pipeline.\n", "rewritten_text": "Online virtual try-on is gaining traction as a way to improve online clothing shopping.  Current methods, while impressive, require users to supply high-quality images of the desired clothing, which are often unavailable.  To address this, we introduce FashionTex, a multi-modal interactive system that uses text and texture inputs for flexible full-body clothing customization.  Our framework employs a novel editing module and loss functions to enable semantic control over clothing type and texture without needing paired training data, while also preserving the user's identity.  Extensive experiments validate the effectiveness of our approach.\n"}, "2310.05989": {"original_text": "  3D object detection plays a pivotal role in autonomous driving and robotics,\ndemanding precise interpretation of Bird's Eye View (BEV) images. The dynamic\nnature of real-world environments necessitates the use of dynamic query\nmechanisms in 3D object detection to adaptively capture and process the complex\nspatio-temporal relationships present in these scenes. However, prior\nimplementations of dynamic queries have often faced difficulties in effectively\nleveraging these relationships, particularly when it comes to integrating\ntemporal information in a computationally efficient manner. Addressing this\nlimitation, we introduce a framework utilizing dynamic query evolution\nstrategy, harnesses K-means clustering and Top-K attention mechanisms for\nrefined spatio-temporal data processing. By dynamically segmenting the BEV\nspace and prioritizing key features through Top-K attention, our model achieves\na real-time, focused analysis of pertinent scene elements. Our extensive\nevaluation on the nuScenes and Waymo dataset showcases a marked improvement in\ndetection accuracy, setting a new benchmark in the domain of query-based BEV\nobject detection. Our dynamic query evolution strategy has the potential to\npush the boundaries of current BEV methods with enhanced adaptability and\ncomputational efficiency. Project page:\nhttps://github.com/Jiawei-Yao0812/QE-BEV\n", "rewritten_text": "Accurate 3D object detection from Bird's Eye View (BEV) images is crucial for autonomous driving and robotics.  Existing dynamic query methods struggle to efficiently integrate temporal information for processing complex, real-world scenes.  We introduce a novel framework that uses a dynamic query evolution strategy, combining K-means clustering and Top-K attention to achieve real-time, focused analysis of BEV data.  This approach dynamically segments the BEV space and prioritizes key features, resulting in significantly improved detection accuracy on nuScenes and Waymo datasets.  Our method sets a new benchmark for query-based BEV object detection, offering enhanced adaptability and computational efficiency.  See our project page for details: https://github.com/Jiawei-Yao0812/QE-BEV\n"}, "1907.03513": {"original_text": "  Keeping up to date on emerging entities that appear every day is\nindispensable for various applications, such as social-trend analysis and\nmarketing research. Previous studies have attempted to detect unseen entities\nthat are not registered in a particular knowledge base as emerging entities and\nconsequently find non-emerging entities since the absence of entities in\nknowledge bases does not guarantee their emergence. We therefore introduce a\nnovel task of discovering truly emerging entities when they have just been\nintroduced to the public through microblogs and propose an effective method\nbased on time-sensitive distant supervision, which exploits distinctive\nearly-stage contexts of emerging entities. Experimental results with a\nlarge-scale Twitter archive show that the proposed method achieves 83.2%\nprecision of the top 500 discovered emerging entities, which outperforms\nbaselines based on unseen entity recognition with burst detection. Besides\nnotable emerging entities, our method can discover massive long-tail and\nhomographic emerging entities. An evaluation of relative recall shows that the\nmethod detects 80.4% emerging entities newly registered in Wikipedia; 92.4% of\nthem are discovered earlier than their registration in Wikipedia, and the\naverage lead-time is more than one year (571 days).\n", "rewritten_text": "Identifying newly emerging entities is crucial for applications like social trend analysis and marketing.  Existing methods struggle to distinguish truly emerging entities from those simply absent from existing knowledge bases.  This paper introduces a novel task and method for discovering truly emerging entities as they appear on microblogs, using a time-sensitive distant supervision approach that leverages their unique early-stage contexts.  Evaluated on a large Twitter dataset, our method achieves 83.2% precision in the top 500 discovered entities, surpassing baselines.  Furthermore, it effectively identifies a wide range of emerging entities, including those in the long tail and homographic entities.  Our method demonstrates strong recall, detecting 80.4% of entities newly added to Wikipedia, with 92.4% discovered over a year (average 571 days) before their Wikipedia entry.\n"}, "2312.04567": {"original_text": "  Recent significant advances in text-to-image models unlock the possibility of\ntraining vision systems using synthetic images, potentially overcoming the\ndifficulty of collecting curated data at scale. It is unclear, however, how\nthese models behave at scale, as more synthetic data is added to the training\nset. In this paper we study the scaling laws of synthetic images generated by\nstate of the art text-to-image models, for the training of supervised models:\nimage classifiers with label supervision, and CLIP with language supervision.\nWe identify several factors, including text prompts, classifier-free guidance\nscale, and types of text-to-image models, that significantly affect scaling\nbehavior. After tuning these factors, we observe that synthetic images\ndemonstrate a scaling trend similar to, but slightly less effective than, real\nimages in CLIP training, while they significantly underperform in scaling when\ntraining supervised image classifiers. Our analysis indicates that the main\nreason for this underperformance is the inability of off-the-shelf\ntext-to-image models to generate certain concepts, a limitation that\nsignificantly impairs the training of image classifiers. Our findings also\nsuggest that scaling synthetic data can be particularly effective in scenarios\nsuch as: (1) when there is a limited supply of real images for a supervised\nproblem (e.g., fewer than 0.5 million images in ImageNet), (2) when the\nevaluation dataset diverges significantly from the training data, indicating\nthe out-of-distribution scenario, or (3) when synthetic data is used in\nconjunction with real images, as demonstrated in the training of CLIP models.\n", "rewritten_text": "Advances in text-to-image models enable training vision systems with synthetic data, circumventing the challenges of large-scale real-world data collection.  This paper investigates how the performance of supervised image classifiers and CLIP models scales with increasing amounts of synthetic training data generated by state-of-the-art text-to-image models.  We analyze the impact of various factors, including text prompts, classifier-free guidance, and model type, on scaling behavior.  Results show that while synthetic data scales similarly to real data for CLIP training (though slightly less effectively), it significantly underperforms for supervised image classifiers. This underperformance stems from the inability of current text-to-image models to generate certain concepts, hindering classifier training.  However, synthetic data proves valuable in scenarios with limited real data (e.g., less than 0.5 million ImageNet images), significant dataset divergence (out-of-distribution settings), or when used to augment real data, as evidenced by our CLIP training results.\n"}, "1909.1244": {"original_text": "  Recently, pre-trained language models have achieved remarkable success in a\nbroad range of natural language processing tasks. However, in multilingual\nsetting, it is extremely resource-consuming to pre-train a deep language model\nover large-scale corpora for each language. Instead of exhaustively\npre-training monolingual language models independently, an alternative solution\nis to pre-train a powerful multilingual deep language model over large-scale\ncorpora in hundreds of languages. However, the vocabulary size for each\nlanguage in such a model is relatively small, especially for low-resource\nlanguages. This limitation inevitably hinders the performance of these\nmultilingual models on tasks such as sequence labeling, wherein in-depth\ntoken-level or sentence-level understanding is essential.\n  In this paper, inspired by previous methods designed for monolingual\nsettings, we investigate two approaches (i.e., joint mapping and mixture\nmapping) based on a pre-trained multilingual model BERT for addressing the\nout-of-vocabulary (OOV) problem on a variety of tasks, including part-of-speech\ntagging, named entity recognition, machine translation quality estimation, and\nmachine reading comprehension. Experimental results show that using mixture\nmapping is more promising. To the best of our knowledge, this is the first work\nthat attempts to address and discuss the OOV issue in multilingual settings.\n", "rewritten_text": "Pre-trained multilingual language models, while achieving impressive results across numerous NLP tasks, suffer from limited vocabulary sizes, particularly for low-resource languages. This constraint hampers performance on tasks requiring fine-grained understanding, such as sequence labeling.  This paper addresses this out-of-vocabulary (OOV) problem by adapting monolingual OOV mitigation techniques to a multilingual BERT model. We explore two approaches: joint mapping and mixture mapping, evaluating them on part-of-speech tagging, named entity recognition, machine translation quality estimation, and machine reading comprehension.  Our findings demonstrate the superiority of mixture mapping and represent, to our knowledge, the first comprehensive investigation of OOV issues in multilingual model settings.\n"}, "2311.05821": {"original_text": "  While recent advances have boosted LM proficiency in linguistic benchmarks,\nLMs consistently struggle to reason correctly on complex tasks like\nmathematics. We turn to Reinforcement Learning from Human Feedback (RLHF) as a\nmethod with which to shape model reasoning processes. In particular, we explore\ntwo reward schemes, outcome-supervised reward models (ORMs) and\nprocess-supervised reward models (PRMs), to optimize for logical reasoning. Our\nresults show that the fine-grained reward provided by PRM-based methods\nenhances accuracy on simple mathematical reasoning (GSM8K) while, unexpectedly,\nreducing performance in complex tasks (MATH). Furthermore, we show the critical\nrole reward aggregation functions play in model performance. Providing\npromising avenues for future research, our study underscores the need for\nfurther exploration into fine-grained reward modeling for more reliable\nlanguage models.\n", "rewritten_text": "Large language models (LLMs), despite recent improvements in linguistic tasks, remain deficient in complex reasoning, particularly mathematics.  We investigated Reinforcement Learning from Human Feedback (RLHF), using outcome-supervised (ORM) and process-supervised (PRM) reward models, to improve their logical reasoning abilities.  While PRMs improved accuracy on simpler math problems (GSM8K), they surprisingly hindered performance on more complex ones (MATH).  Our findings highlight the crucial influence of reward aggregation methods and suggest that further research into fine-grained reward modeling is essential for developing more reliable LLMs.\n"}, "1309.6379": {"original_text": "  We propose a large deformation diffeomorphic metric mapping algorithm to\nalign multiple b-value diffusion weighted imaging (mDWI) data, specifically\nacquired via hybrid diffusion imaging (HYDI), denoted as LDDMM-HYDI. We then\npropose a Bayesian model for estimating the white matter atlas from HYDIs. We\nadopt the work given in Hosseinbor et al. (2012) and represent the q-space\ndiffusion signal with the Bessel Fourier orientation reconstruction (BFOR)\nsignal basis. The BFOR framework provides the representation of mDWI in the\nq-space and thus reduces memory requirement. In addition, since the BFOR signal\nbasis is orthonormal, the L2 norm that quantifies the differences in the\nq-space signals of any two mDWI datasets can be easily computed as the sum of\nthe squared differences in the BFOR expansion coefficients. In this work, we\nshow that the reorientation of the $q$-space signal due to spatial\ntransformation can be easily defined on the BFOR signal basis. We incorporate\nthe BFOR signal basis into the LDDMM framework and derive the gradient descent\nalgorithm for LDDMM-HYDI with explicit orientation optimization. Additionally,\nwe extend the previous Bayesian atlas estimation framework for scalar-valued\nimages to HYDIs and derive the expectation-maximization algorithm for solving\nthe HYDI atlas estimation problem. Using real HYDI datasets, we show the\nBayesian model generates the white matter atlas with anatomical details.\nMoreover, we show that it is important to consider the variation of mDWI\nreorientation due to a small change in diffeomorphic transformation in the\nLDDMM-HYDI optimization and to incorporate the full information of HYDI for\naligning mDWI.\n", "rewritten_text": "This paper presents LDDMM-HYDI, a large deformation diffeomorphic metric mapping algorithm for aligning multiple b-value diffusion weighted imaging (mDWI) data acquired using hybrid diffusion imaging (HYDI).  We leverage the Bessel Fourier orientation reconstruction (BFOR) signal basis, as described in Hosseinbor et al. (2012), to efficiently represent the q-space diffusion signal, reducing memory demands and simplifying the computation of L2 norm differences.  This representation allows for straightforward definition of q-space signal reorientation under spatial transformations within the LDDMM framework.  We derive a gradient descent algorithm for LDDMM-HYDI with explicit orientation optimization.  Furthermore, we extend a Bayesian atlas estimation framework to HYDI data, deriving an expectation-maximization algorithm for white matter atlas generation.  Results on real HYDI datasets demonstrate the efficacy of our Bayesian model in producing anatomically detailed white matter atlases, highlighting the importance of accurately modeling mDWI reorientation during diffeomorphic transformation and utilizing the full HYDI information for optimal mDWI alignment.\n"}, "2101.03848": {"original_text": "  Convolutional neural networks (CNNs) have been widely used in various vision\ntasks, e.g. image classification, semantic segmentation, etc. Unfortunately,\nstandard 2D CNNs are not well suited for spherical signals such as panorama\nimages or spherical projections, as the sphere is an unstructured grid. In this\npaper, we present Spherical Transformer which can transform spherical signals\ninto vectors that can be directly processed by standard CNNs such that many\nwell-designed CNNs architectures can be reused across tasks and datasets by\npretraining. To this end, the proposed method first uses local structured\nsampling methods such as HEALPix to construct a transformer grid by using the\ninformation of spherical points and its adjacent points, and then transforms\nthe spherical signals to the vectors through the grid. By building the\nSpherical Transformer module, we can use multiple CNN architectures directly.\nWe evaluate our approach on the tasks of spherical MNIST recognition, 3D object\nclassification and omnidirectional image semantic segmentation. For 3D object\nclassification, we further propose a rendering-based projection method to\nimprove the performance and a rotational-equivariant model to improve the\nanti-rotation ability. Experimental results on three tasks show that our\napproach achieves superior performance over state-of-the-art methods.\n", "rewritten_text": "This paper introduces Spherical Transformer, a novel method for processing spherical signals like panorama images.  Unlike standard 2D CNNs, which struggle with the unstructured nature of spherical data, Spherical Transformer converts spherical signals into vector representations suitable for standard CNNs. This leverages the extensive existing CNN architectures and allows for pre-training across diverse tasks and datasets.  The method employs local structured sampling (e.g., HEALPix) to create a transformer grid, mapping spherical data to vectors.  Evaluated on spherical MNIST recognition, 3D object classification (enhanced with a novel rendering-based projection and rotation-equivariant model), and omnidirectional image semantic segmentation, Spherical Transformer demonstrates state-of-the-art performance.\n"}, "2209.02686": {"original_text": "  Image-to-image translation has played an important role in enabling synthetic\ndata for computer vision. However, if the source and target domains have a\nlarge semantic mismatch, existing techniques often suffer from source content\ncorruption aka semantic flipping. To address this problem, we propose a new\nparadigm for image-to-image translation using Vector Symbolic Architectures\n(VSA), a theoretical framework which defines algebraic operations in a\nhigh-dimensional vector (hypervector) space. We introduce VSA-based constraints\non adversarial learning for source-to-target translations by learning a\nhypervector mapping that inverts the translation to ensure consistency with\nsource content. We show both qualitatively and quantitatively that our method\nimproves over other state-of-the-art techniques.\n", "rewritten_text": "Synthetic data generation for computer vision relies heavily on image-to-image translation.  However, significant semantic differences between source and target domains can lead to \"semantic flipping,\" distorting the source content.  This paper introduces a novel image-to-image translation paradigm using Vector Symbolic Architectures (VSAs).  By incorporating VSA-based constraints into adversarial learning, we learn a hypervector mapping that inverts the translation, preserving source content integrity.  Qualitative and quantitative results demonstrate superior performance compared to existing state-of-the-art methods.\n"}, "2305.14576": {"original_text": "  Pre-trained language models (PLMs) have ignited a surge in demand for\neffective fine-tuning techniques, particularly in low-resource domains and\nlanguages. Active learning (AL), a set of algorithms designed to decrease\nlabeling costs by minimizing label complexity, has shown promise in confronting\nthe labeling bottleneck. In parallel, adapter modules designed for\nparameter-efficient fine-tuning (PEFT) have demonstrated notable potential in\nlow-resource settings. However, the interplay between AL and adapter-based PEFT\nremains unexplored. We present an empirical study of PEFT behavior with AL in\nlow-resource settings for text classification tasks. Our findings affirm the\nsuperiority of PEFT over full-fine tuning (FFT) in low-resource settings and\ndemonstrate that this advantage persists in AL setups. We further examine the\nproperties of PEFT and FFT through the lens of forgetting dynamics and\ninstance-level representations, where we find that PEFT yields more stable\nrepresentations of early and middle layers compared to FFT. Our research\nunderscores the synergistic potential of AL and PEFT in low-resource settings,\npaving the way for advancements in efficient and effective fine-tuning.\n", "rewritten_text": "The rise of pre-trained language models (PLMs) has increased the need for efficient fine-tuning, especially for low-resource languages.  Active learning (AL), which reduces labeling costs, and parameter-efficient fine-tuning (PEFT) using adapter modules, both offer potential solutions.  This study investigates their combined effectiveness.  We empirically demonstrate that PEFT outperforms full fine-tuning (FFT) in low-resource text classification, even with AL.  Analyzing forgetting dynamics and instance-level representations reveals PEFT's superior stability in early and middle layers.  Our results highlight the synergistic benefits of combining AL and PEFT for efficient and effective low-resource fine-tuning.\n"}, "2308.15448": {"original_text": "  The negative effects of online bullying and harassment are increasing with\nInternet popularity, especially in social media. One solution is using natural\nlanguage processing (NLP) and machine learning (ML) methods for the automatic\ndetection of harmful remarks, but these methods are limited in low-resource\nlanguages like the Chittagonian dialect of Bangla.This study focuses on\ndetecting vulgar remarks in social media using supervised ML and deep learning\nalgorithms.Logistic Regression achieved promising accuracy (0.91) while simple\nRNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the\nissue that NN algorithms require more data.\n", "rewritten_text": "Online bullying and harassment are growing problems exacerbated by the increasing popularity of social media.  While natural language processing (NLP) and machine learning (ML) offer a potential solution for automatically detecting harmful content, their effectiveness is hampered by a lack of resources for less-common languages such as the Chittagonian dialect of Bangla. This study investigates the use of supervised machine learning and deep learning algorithms to detect vulgar remarks in social media posts.  Results show that Logistic Regression achieved high accuracy (0.91), while recurrent neural networks (RNNs) using Word2vec and fastText showed lower accuracy (0.84-0.90), suggesting that neural network approaches require significantly more training data.\n"}, "2305.06052": {"original_text": "  Quantization is a widely adopted technique for deep neural networks to reduce\nthe memory and computational resources required. However, when quantized, most\nmodels would need a suitable calibration process to keep their performance\nintact, which requires data from the target domain, such as a fraction of the\ndataset used in model training and model validation (i.e. calibration dataset).\n  In this study, we investigate the use of synthetic data as a substitute for\nthe calibration with real data for the quantization method. We propose a data\ngeneration method based on Generative Adversarial Networks that are trained\nprior to the model quantization step. We compare the performance of models\nquantized using data generated by StyleGAN2-ADA and our pre-trained DiStyleGAN,\nwith quantization using real data and an alternative data generation method\nbased on fractal images. Overall, the results of our experiments demonstrate\nthe potential of leveraging synthetic data for calibration during the\nquantization process. In our experiments, the percentage of accuracy\ndegradation of the selected models was less than 0.6%, with our best\nperformance achieved on MobileNetV2 (0.05%). The code is available at:\nhttps://github.com/ThanosM97/gsoc2022-openvino\n", "rewritten_text": "This study explores using synthetic data generated by Generative Adversarial Networks (GANs) \u2013 specifically StyleGAN2-ADA and a novel DiStyleGAN \u2013 to calibrate quantized deep neural networks.  Calibration, crucial for maintaining accuracy after quantization, typically requires real-world data.  We compare GAN-generated calibration data to real data and fractal image-based alternatives, evaluating performance on quantized models.  Results show minimal accuracy degradation (less than 0.6%, with MobileNetV2 achieving only 0.05% loss), demonstrating the viability of synthetic data for efficient model quantization.  Our code is publicly available at https://github.com/ThanosM97/gsoc2022-openvino.\n"}, "2406.15719": {"original_text": "  Convolutional Neural Networks (CNNs) and vision transformers (ViTs) have\nshown excellent capability in complex hyperspectral image (HSI) classification.\nHowever, these models require a significant number of training data and are\ncomputational resources. On the other hand, modern Multi-Layer Perceptrons\n(MLPs) have demonstrated great classification capability. These modern\nMLP-based models require significantly less training data compared to CNNs and\nViTs, achieving the state-of-the-art classification accuracy. Recently,\nKolmogorov-Arnold Networks (KANs) were proposed as viable alternatives for\nMLPs. Because of their internal similarity to splines and their external\nsimilarity to MLPs, KANs are able to optimize learned features with remarkable\naccuracy in addition to being able to learn new features. Thus, in this study,\nwe assess the effectiveness of KANs for complex HSI data classification.\nMoreover, to enhance the HSI classification accuracy obtained by the KANs, we\ndevelop and propose a Hybrid architecture utilizing 1D, 2D, and 3D KANs. To\ndemonstrate the effectiveness of the proposed KAN architecture, we conducted\nextensive experiments on three newly created HSI benchmark datasets:\nQUH-Pingan, QUH-Tangdaowan, and QUH-Qingyun. The results underscored the\ncompetitive or better capability of the developed hybrid KAN-based model across\nthese benchmark datasets over several other CNN- and ViT-based algorithms,\nincluding 1D-CNN, 2DCNN, 3D CNN, VGG-16, ResNet-50, EfficientNet, RNN, and ViT.\nThe code are publicly available at (https://github.com/aj1365/HSIConvKAN)\n", "rewritten_text": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) excel at hyperspectral image (HSI) classification but demand substantial training data and computational resources.  In contrast, modern Multi-Layer Perceptrons (MLPs) achieve state-of-the-art accuracy with significantly less data.  Building on MLPs, Kolmogorov-Arnold Networks (KANs), with their spline-like internal structure and MLP-like external behavior, offer efficient feature learning and optimization.  This study evaluates KANs for HSI classification, proposing a novel hybrid architecture combining 1D, 2D, and 3D KANs to improve accuracy.  Experiments on three new HSI benchmark datasets (QUH-Pingan, QUH-Tangdaowan, and QUH-Qingyun) demonstrate the competitive performance of this hybrid KAN model against various CNN and ViT-based methods (including 1D-CNN, 2D-CNN, 3D-CNN, VGG-16, ResNet-50, EfficientNet, RNN, and ViT).  The code is publicly available at [https://github.com/aj1365/HSIConvKAN](https://github.com/aj1365/HSIConvKAN).\n"}, "1905.00413": {"original_text": "  We explore the problem of view synthesis from a narrow baseline pair of\nimages, and focus on generating high-quality view extrapolations with plausible\ndisocclusions. Our method builds upon prior work in predicting a multiplane\nimage (MPI), which represents scene content as a set of RGB$\\alpha$ planes\nwithin a reference view frustum and renders novel views by projecting this\ncontent into the target viewpoints. We present a theoretical analysis showing\nhow the range of views that can be rendered from an MPI increases linearly with\nthe MPI disparity sampling frequency, as well as a novel MPI prediction\nprocedure that theoretically enables view extrapolations of up to $4\\times$ the\nlateral viewpoint movement allowed by prior work. Our method ameliorates two\nspecific issues that limit the range of views renderable by prior methods: 1)\nWe expand the range of novel views that can be rendered without depth\ndiscretization artifacts by using a 3D convolutional network architecture along\nwith a randomized-resolution training procedure to allow our model to predict\nMPIs with increased disparity sampling frequency. 2) We reduce the repeated\ntexture artifacts seen in disocclusions by enforcing a constraint that the\nappearance of hidden content at any depth must be drawn from visible content at\nor behind that depth. Please see our results video at:\nhttps://www.youtube.com/watch?v=aJqAaMNL2m4.\n", "rewritten_text": "This paper addresses the challenge of synthesizing novel views from a closely spaced image pair, specifically focusing on high-quality view extrapolation and realistic handling of occlusions.  We improve upon existing multiplane image (MPI) methods, which represent scenes as layered RGB\u03b1 planes.  Our theoretical analysis demonstrates a linear relationship between renderable view range and MPI disparity sampling frequency.  We introduce a novel MPI prediction method enabling up to four times greater lateral view extrapolation than previous approaches.  This improvement addresses two key limitations:  first, we mitigate depth discretization artifacts by employing a 3D convolutional network and a randomized-resolution training strategy to increase disparity sampling; second, we reduce repeated texture artifacts in occluded regions by constraining hidden content appearance to visible content at equal or greater depth.  See our results video: https://www.youtube.com/watch?v=aJqAaMNL2m4.\n"}, "1909.10579": {"original_text": "  Neural language models (LMs) perform well on tasks that require sensitivity\nto syntactic structure. Drawing on the syntactic priming paradigm from\npsycholinguistics, we propose a novel technique to analyze the representations\nthat enable such success. By establishing a gradient similarity metric between\nstructures, this technique allows us to reconstruct the organization of the\nLMs' syntactic representational space. We use this technique to demonstrate\nthat LSTM LMs' representations of different types of sentences with relative\nclauses are organized hierarchically in a linguistically interpretable manner,\nsuggesting that the LMs track abstract properties of the sentence.\n", "rewritten_text": "This study investigates how neural language models (LMs) handle syntax.  Adapting the psycholinguistic concept of syntactic priming, we introduce a new method to analyze the internal syntactic representations of these models.  This method, based on a gradient similarity metric, reveals the organization of the LM's syntactic space.  Applying this method to LSTM models processing relative clauses, we find a linguistically meaningful hierarchical organization, indicating that these models capture abstract syntactic properties.\n"}, "1804.04213": {"original_text": "  We study how to synthesize novel views of human body from a single image.\nThough recent deep learning based methods work well for rigid objects, they\noften fail on objects with large articulation, like human bodies. The core step\nof existing methods is to fit a map from the observable views to novel views by\nCNNs; however, the rich articulation modes of human body make it rather\nchallenging for CNNs to memorize and interpolate the data well. To address the\nproblem, we propose a novel deep learning based pipeline that explicitly\nestimates and leverages the geometry of the underlying human body. Our new\npipeline is a composition of a shape estimation network and an image generation\nnetwork, and at the interface a perspective transformation is applied to\ngenerate a forward flow for pixel value transportation. Our design is able to\nfactor out the space of data variation and makes learning at each step much\neasier. Empirically, we show that the performance for pose-varying objects can\nbe improved dramatically. Our method can also be applied on real data captured\nby 3D sensors, and the flow generated by our methods can be used for generating\nhigh quality results in higher resolution.\n", "rewritten_text": "This research focuses on generating novel views of human bodies from a single image.  Existing deep learning methods, while effective for rigid objects, struggle with the significant articulation of the human body.  These methods typically use CNNs to map observable views to novel ones, but this approach is hampered by the complexity of human pose variations.  To overcome this, we propose a novel deep learning pipeline that explicitly incorporates human body geometry.  This pipeline comprises a shape estimation network, an image generation network, and a perspective transformation to accurately transfer pixel values. This decomposition simplifies the learning process by separating sources of data variation.  Our results demonstrate significantly improved performance on pose-varying objects, and our method extends to real-world 3D sensor data, enabling high-resolution image generation.\n"}, "2401.04354": {"original_text": "  With the explosive growth of video data in real-world applications, a\ncomprehensive representation of videos becomes increasingly important. In this\npaper, we address the problem of video scene recognition, whose goal is to\nlearn a high-level video representation to classify scenes in videos. Due to\nthe diversity and complexity of video contents in realistic scenarios, this\ntask remains a challenge. Most existing works identify scenes for videos only\nfrom visual or textual information in a temporal perspective, ignoring the\nvaluable information hidden in single frames, while several earlier studies\nonly recognize scenes for separate images in a non-temporal perspective. We\nargue that these two perspectives are both meaningful for this task and\ncomplementary to each other, meanwhile, externally introduced knowledge can\nalso promote the comprehension of videos. We propose a novel two-stream\nframework to model video representations from multiple perspectives, i.e.\ntemporal and non-temporal perspectives, and integrate the two perspectives in\nan end-to-end manner by self-distillation. Besides, we design a\nknowledge-enhanced feature fusion and label prediction method that contributes\nto naturally introducing knowledge into the task of video scene recognition.\nExperiments conducted on a real-world dataset demonstrate the effectiveness of\nour proposed method.\n", "rewritten_text": "The rapid increase in video data necessitates robust video representations for tasks like scene recognition.  Current approaches often focus solely on temporal (video-level) or non-temporal (image-level) information, neglecting the complementary value of both.  This paper introduces a novel two-stream framework that integrates these perspectives via self-distillation, further enhanced by a knowledge-enhanced feature fusion and prediction method.  Experiments on a real-world dataset validate the effectiveness of our approach, which leverages both temporal and non-temporal information, along with external knowledge, for improved video scene recognition.\n"}, "2211.08112": {"original_text": "  Active Learning (AL) is a powerful tool for learning with less labeled data,\nin particular, for specialized domains, like legal documents, where unlabeled\ndata is abundant, but the annotation requires domain expertise and is thus\nexpensive. Recent works have shown the effectiveness of AL strategies for\npre-trained language models. However, most AL strategies require a set of\nlabeled samples to start with, which is expensive to acquire. In addition,\npre-trained language models have been shown unstable during fine-tuning with\nsmall datasets, and their embeddings are not semantically meaningful. In this\nwork, we propose a pipeline for effectively using active learning with\npre-trained language models in the legal domain. To this end, we leverage the\navailable unlabeled data in three phases. First, we continue pre-training the\nmodel to adapt it to the downstream task. Second, we use knowledge distillation\nto guide the model's embeddings to a semantically meaningful space. Finally, we\npropose a simple, yet effective, strategy to find the initial set of labeled\nsamples with fewer actions compared to existing methods. Our experiments on\nContract-NLI, adapted to the classification task, and LEDGAR benchmarks show\nthat our approach outperforms standard AL strategies, and is more efficient.\nFurthermore, our pipeline reaches comparable results to the fully-supervised\napproach with a small performance gap, and dramatically reduced annotation\ncost. Code and the adapted data will be made available.\n", "rewritten_text": "Active learning (AL) offers significant potential for training models on limited labeled data, especially in specialized fields like law where unlabeled data is plentiful but annotation is costly.  While AL has shown promise with pre-trained language models (PLMs), challenges remain:  most AL methods need initial labeled data, PLMs can be unstable with small datasets, and their embeddings may lack semantic meaning.  This work introduces a three-phase pipeline for effective AL with PLMs in legal text classification.  We first continue pre-training the PLM on unlabeled data, then use knowledge distillation to improve embedding quality, and finally, employ a novel, efficient method for selecting initial labeled samples.  Experiments on Contract-NLI and LEDGAR demonstrate superior performance and efficiency compared to standard AL, achieving near fully-supervised results with drastically reduced annotation costs.  Code and data are publicly available.\n"}, "2102.03115": {"original_text": "  Object detection in natural scenes can be a challenging task. In many\nreal-life situations, the visible spectrum is not suitable for traditional\ncomputer vision tasks. Moving outside the visible spectrum range, such as the\nthermal spectrum or the near-infrared (NIR) images, is much more beneficial in\nlow visibility conditions, NIR images are very helpful for understanding the\nobject's material quality. In this work, we have taken images with both the\nThermal and NIR spectrum for the object detection task. As multi-spectral data\nwith both Thermal and NIR is not available for the detection task, we needed to\ncollect data ourselves. Data collection is a time-consuming process, and we\nfaced many obstacles that we had to overcome. We train the YOLO v3 network from\nscratch to detect an object from multi-spectral images. Also, to avoid\noverfitting, we have done data augmentation and tune hyperparameters.\n", "rewritten_text": "Object detection in natural scenes is challenging, especially in low-visibility conditions where the visible spectrum is insufficient.  Thermal and near-infrared (NIR) imaging offer significant advantages in such scenarios, with NIR additionally providing insights into material properties.  This work addresses object detection using both thermal and NIR imagery.  Due to the unavailability of existing multi-spectral datasets (combining thermal and NIR), we collected our own data, a time-consuming process fraught with challenges.  We trained a YOLOv3 network from scratch on this dataset, employing data augmentation and hyperparameter tuning to mitigate overfitting.\n"}, "2207.09157": {"original_text": "  Supervised deep learning-based approaches have been applied to task-oriented\ndialog and have proven to be effective for limited domain and language\napplications when a sufficient number of training examples are available. In\npractice, these approaches suffer from the drawbacks of domain-driven design\nand under-resourced languages. Domain and language models are supposed to grow\nand change as the problem space evolves. On one hand, research on transfer\nlearning has demonstrated the cross-lingual ability of multilingual\nTransformers-based models to learn semantically rich representations. On the\nother, in addition to the above approaches, meta-learning have enabled the\ndevelopment of task and language learning algorithms capable of far\ngeneralization. Through this context, this article proposes to investigate the\ncross-lingual transferability of using synergistically few-shot learning with\nprototypical neural networks and multilingual Transformers-based models.\nExperiments in natural language understanding tasks on MultiATIS++ corpus shows\nthat our approach substantially improves the observed transfer learning\nperformances between the low and the high resource languages. More generally\nour approach confirms that the meaningful latent space learned in a given\nlanguage can be can be generalized to unseen and under-resourced ones using\nmeta-learning.\n", "rewritten_text": "Supervised deep learning excels in task-oriented dialogue within limited domains and languages, provided ample training data.  However, its reliance on domain-specific designs hinders adaptability and struggles with low-resource languages.  To address this, we leverage the cross-lingual capabilities of multilingual Transformer models and the generalizability of meta-learning, specifically few-shot learning with prototypical neural networks.  Experiments on the MultiATIS++ corpus demonstrate that this synergistic approach significantly improves cross-lingual transfer learning performance between high and low-resource languages, confirming the transferability of learned semantic representations to unseen languages.\n"}, "2203.1521": {"original_text": "  To learn camera-view invariant features for person Re-IDentification (Re-ID),\nthe cross-camera image pairs of each person play an important role. However,\nsuch cross-view training samples could be unavailable under the ISolated Camera\nSupervised (ISCS) setting, e.g., a surveillance system deployed across distant\nscenes. To handle this challenging problem, a new pipeline is introduced by\nsynthesizing the cross-camera samples in the feature space for model training.\nSpecifically, the feature encoder and generator are end-to-end optimized under\na novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint\nlearning procedure raises concern on the stability of generative model\ntraining. Therefore, a new feature generator, $\\sigma$-Regularized Conditional\nVariational Autoencoder ($\\sigma$-Reg.~CVAE), is proposed with theoretical and\nexperimental analysis on its robustness. Extensive experiments on two ISCS\nperson Re-ID datasets demonstrate the superiority of our CCSFG to the\ncompetitors.\n", "rewritten_text": "Person re-identification (Re-ID) models benefit greatly from cross-camera image pairs.  However, these pairs are often unavailable in Isolated Camera Supervised (ISCS) scenarios, such as widely dispersed surveillance systems.  To address this, we propose a novel pipeline that synthesizes cross-camera image features for training.  This pipeline uses a Camera-Conditioned Stable Feature Generation (CCSFG) method to jointly optimize a feature encoder and generator.  To improve the stability of the generative model, we introduce a novel $\\sigma$-Regularized Conditional Variational Autoencoder ($\\sigma$-Reg. CVAE) with theoretical and empirical justification for its robustness.  Experiments on two ISCS Re-ID datasets demonstrate the superior performance of our CCSFG approach.\n"}, "1812.0957": {"original_text": "  In recent years, we have seen the performance of video-based person\nRe-Identification (ReID) methods have improved considerably. However, most of\nthe work in this area has dealt with videos acquired by fixed cameras with\nwider field of view. Recently, widespread use of wearable cameras and recording\ndevices such as cellphones have opened the door to interesting research in\nfirst-person Point-of-view (POV) videos (egocentric videos). Nonetheless,\nanalysis of such videos is challenging due to factors such as poor video\nquality due to ego-motion, blurriness, severe changes in lighting conditions\nand perspective distortions. To facilitate the research towards conquering\nthese challenges, this paper contributes a new dataset called EgoReID. The\ndataset is captured using 3 mobile cellphones with non-overlapping\nfield-of-view. It contains 900 IDs and around 10,200 tracks with a total of\n176,000 detections. The dataset also contains 12-sensor meta data e.g. camera\norientation pitch and rotation for each video.\n  In addition, we propose a new framework which takes advantage of both visual\nand sensor meta data to successfully perform Person ReID. We extend image-based\nre-ID method employing human body parsing trained on ten datasets to\nvideo-based re-ID. In our method, first frame level local features are\nextracted for each semantic region, then 3D convolutions are applied to encode\nthe temporal information in each sequence of semantic regions. Additionally, we\nemploy sensor meta data to predict targets' next camera and their estimated\ntime of arrival, which considerably improves our ReID performance as it\nsignificantly reduces our search space.\n", "rewritten_text": "Recent advancements in video-based person re-identification (ReID) have primarily focused on fixed, wide-field-of-view cameras.  The proliferation of wearable cameras and smartphones, however, has spurred interest in egocentric (POV) video ReID, which presents unique challenges: poor quality due to ego-motion, blur, variable lighting, and perspective distortions.  To address this, we introduce EgoReID, a new dataset captured using three non-overlapping mobile phone cameras.  EgoReID comprises 900 identities, approximately 10,200 tracks (176,000 detections), and 12-sensor metadata (e.g., camera orientation).  Furthermore, we propose a novel ReID framework that leverages both visual and sensor metadata.  Extending an image-based ReID method with human body parsing (trained on ten datasets), our framework extracts frame-level local features from semantic regions, employs 3D convolutions for temporal encoding, and utilizes sensor data to predict target locations and arrival times, significantly improving ReID accuracy by reducing the search space.\n"}, "2112.07928": {"original_text": "  Real-world data often follows a long-tailed distribution, which makes the\nperformance of existing classification algorithms degrade heavily. A key issue\nis that samples in tail categories fail to depict their intra-class diversity.\nHumans can imagine a sample in new poses, scenes, and view angles with their\nprior knowledge even if it is the first time to see this category. Inspired by\nthis, we propose a novel reasoning-based implicit semantic data augmentation\nmethod to borrow transformation directions from other classes. Since the\ncovariance matrix of each category represents the feature transformation\ndirections, we can sample new directions from similar categories to generate\ndefinitely different instances. Specifically, the long-tailed distributed data\nis first adopted to train a backbone and a classifier. Then, a covariance\nmatrix for each category is estimated, and a knowledge graph is constructed to\nstore the relations of any two categories. Finally, tail samples are adaptively\nenhanced via propagating information from all the similar categories in the\nknowledge graph. Experimental results on CIFAR-100-LT, ImageNet-LT, and\niNaturalist 2018 have demonstrated the effectiveness of our proposed method\ncompared with the state-of-the-art methods.\n", "rewritten_text": "Existing classification algorithms struggle with real-world data's long-tailed distribution, as under-represented tail categories lack sufficient intra-class diversity.  To address this, we introduce a novel implicit data augmentation method.  Mimicking human ability to infer variations in unseen samples, our approach leverages a knowledge graph representing relationships between classes.  We use covariance matrices to capture feature transformation directions within each class, borrowing directions from similar classes to generate diverse synthetic samples for tail categories.  This method, first training a backbone and classifier on the long-tailed data, then constructing the knowledge graph and augmenting tail samples, significantly improves performance on CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018, outperforming state-of-the-art techniques.\n"}, "2208.07291": {"original_text": "  One of the possible dangers that older people face in their daily lives is\nfalling. Occlusion is one of the biggest challenges of vision-based fall\ndetection systems and degrades their detection performance considerably. To\ntackle this problem, we synthesize specifically-designed occluded videos for\ntraining fall detection systems using existing datasets. Then, by defining a\nnew cost function, we introduce a framework for weighted training of fall\ndetection models using occluded and un-occluded videos, which can be applied to\nany learnable fall detection system. Finally, we use both a non-deep and deep\nmodel to evaluate the effect of the proposed weighted training method.\nExperiments show that the proposed method can improve the classification\naccuracy by 36% for a non-deep model and 55% for a deep model in occlusion\nconditions. Moreover, it is shown that the proposed training framework can also\nsignificantly improve the detection performance of a deep network on normal\nun-occluded samples.\n", "rewritten_text": "Falls pose a significant risk to older adults.  Existing vision-based fall detection systems struggle with occlusions, which severely impact their accuracy.  To address this, we developed a novel training framework.  This framework uses synthetically generated occluded videos and a new weighted cost function to train fall detection models (both deep and non-deep) on both occluded and un-occluded data.  Our results demonstrate a substantial improvement in fall detection accuracy: a 36% increase for a non-deep model and a 55% increase for a deep model under occlusion.  Furthermore, the framework also improved performance on un-occluded data for the deep learning model.\n"}, "2101.0219": {"original_text": "  Object detection and semantic segmentation are two of the most widely adopted\ndeep learning algorithms in agricultural applications. One of the major sources\nof variability in image quality acquired in the outdoors for such tasks is\nchanging lighting condition that can alter the appearance of the objects or the\ncontents of the entire image. While transfer learning and data augmentation to\nsome extent reduce the need for large amount of data to train deep neural\nnetworks, the large variety of cultivars and the lack of shared datasets in\nagriculture makes wide-scale field deployments difficult. In this paper, we\npresent a high throughput robust active lighting-based camera system that\ngenerates consistent images in all lighting conditions. We detail experiments\nthat show the consistency in images quality leading to relatively fewer images\nto train deep neural networks for the task of object detection. We further\npresent results from field experiment under extreme lighting conditions where\nimages without active lighting significantly lack to provide consistent\nresults. The experimental results show that on average, deep nets for object\ndetection trained on consistent data required nearly four times less data to\nachieve similar level of accuracy. This proposed work could potentially provide\npragmatic solutions to computer vision needs in agriculture.\n", "rewritten_text": "Agricultural applications increasingly rely on deep learning for object detection and semantic segmentation.  However, variable outdoor lighting significantly impacts image quality. While transfer learning and data augmentation help, the diversity of crops and scarcity of shared datasets hinder widespread deployment. This paper introduces a high-throughput, robust active lighting camera system that produces consistent images regardless of lighting conditions.  Experiments demonstrate this system's effectiveness, requiring approximately four times less training data for object detection compared to systems using passively acquired images, even under extreme lighting.  This approach offers a practical solution to the challenges of computer vision in agriculture.\n"}, "1911.03828": {"original_text": "  Wasserstein autoencoders are effective for text generation. They do not\nhowever provide any control over the style and topic of the generated sentences\nif the dataset has multiple classes and includes different topics. In this\nwork, we present a semi-supervised approach for generating stylized sentences.\nOur model is trained on a multi-class dataset and learns the latent\nrepresentation of the sentences using a mixture of Gaussian prior without any\nadversarial losses. This allows us to generate sentences in the style of a\nspecified class or multiple classes by sampling from their corresponding prior\ndistributions. Moreover, we can train our model on relatively small datasets\nand learn the latent representation of a specified class by adding external\ndata with other styles/classes to our dataset. While a simple WAE or VAE cannot\ngenerate diverse sentences in this case, generated sentences with our approach\nare diverse, fluent, and preserve the style and the content of the desired\nclasses.\n", "rewritten_text": "This paper introduces a semi-supervised method for generating stylized text using Wasserstein autoencoders (WAEs).  Unlike standard WAEs, which struggle to control style and topic in multi-class datasets, our approach leverages a Gaussian mixture prior without adversarial training. This enables generation of sentences in specific styles or combinations thereof by sampling from corresponding prior distributions.  Furthermore, our model effectively handles smaller datasets by incorporating external data with diverse styles, achieving fluent, diverse sentence generation that faithfully reflects the desired style and content\u2014a capability lacking in standard WAE or VAE models.\n"}, "2106.01656": {"original_text": "  Many variants of unsupervised domain adaptation (UDA) problems have been\nproposed and solved individually. Its side effect is that a method that works\nfor one variant is often ineffective for or not even applicable to another,\nwhich has prevented practical applications. In this paper, we give a general\nrepresentation of UDA problems, named Generalized Domain Adaptation (GDA). GDA\ncovers the major variants as special cases, which allows us to organize them in\na comprehensive framework. Moreover, this generalization leads to a new\nchallenging setting where existing methods fail, such as when domain labels are\nunknown, and class labels are only partially given to each domain. We propose a\nnovel approach to the new setting. The key to our approach is self-supervised\nclass-destructive learning, which enables the learning of class-invariant\nrepresentations and domain-adversarial classifiers without using any domain\nlabels. Extensive experiments using three benchmark datasets demonstrate that\nour method outperforms the state-of-the-art UDA methods in the new setting and\nthat it is competitive in existing UDA variations as well.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) has seen numerous specialized solutions, each often ineffective or inapplicable to other UDA variations, hindering practical deployment.  This paper introduces Generalized Domain Adaptation (GDA), a unifying framework encompassing major UDA variants as special cases.  This generalization reveals a challenging new setting where domain labels are unknown and class labels are partially available.  We propose a novel solution leveraging self-supervised class-destructive learning to learn class-invariant representations and domain-adversarial classifiers without domain labels.  Experiments on three benchmark datasets show our method surpasses state-of-the-art UDA methods in this new setting and performs competitively in existing variations.\n"}, "2305.07152": {"original_text": "  The ability to automatically detect and track surgical instruments in\nendoscopic videos can enable transformational interventions. Assessing surgical\nperformance and efficiency, identifying skilled tool use and choreography, and\nplanning operational and logistical aspects of OR resources are just a few of\nthe applications that could benefit. Unfortunately, obtaining the annotations\nneeded to train machine learning models to identify and localize surgical tools\nis a difficult task. Annotating bounding boxes frame-by-frame is tedious and\ntime-consuming, yet large amounts of data with a wide variety of surgical tools\nand surgeries must be captured for robust training. Moreover, ongoing annotator\ntraining is needed to stay up to date with surgical instrument innovation. In\nrobotic-assisted surgery, however, potentially informative data like timestamps\nof instrument installation and removal can be programmatically harvested. The\nability to rely on tool installation data alone would significantly reduce the\nworkload to train robust tool-tracking models. With this motivation in mind we\ninvited the surgical data science community to participate in the challenge,\nSurgToolLoc 2022. The goal was to leverage tool presence data as weak labels\nfor machine learning models trained to detect tools and localize them in video\nframes with bounding boxes. We present the results of this challenge along with\nmany of the team's efforts. We conclude by discussing these results in the\nbroader context of machine learning and surgical data science. The training\ndata used for this challenge consisting of 24,695 video clips with tool\npresence labels is also being released publicly and can be accessed at\nhttps://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022.\n", "rewritten_text": "Automatic surgical instrument detection and tracking in endoscopic videos offers transformative potential for assessing surgical performance, optimizing resource allocation, and improving surgical training.  However, training the necessary machine learning models requires extensive, painstakingly annotated data \u2013 a significant bottleneck.  While annotating bounding boxes frame-by-frame is laborious and requires continuous updates to account for new instruments, robotic surgery offers a potential solution: leveraging readily available instrument installation/removal timestamps as \"weak labels.\"  To address this challenge, we launched SurgToolLoc 2022, a competition focused on using these weak labels to train robust tool detection and localization models.  This paper presents the challenge results, participant approaches, and broader implications for surgical data science and machine learning.  The challenge dataset (24,695 video clips with tool presence labels) is publicly available at [https://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022](https://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022).\n"}, "2403.01858": {"original_text": "  We present TMMLU+, a new benchmark designed for Traditional Chinese language\nunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66\nsubjects from elementary to professional level. It is six times larger and\nboasts a more balanced subject distribution than its predecessor, Taiwan\nMassive Multitask Language Understanding (TMMLU). We also benchmark\nclosed-source models and 26 open-weight Chinese large language models (LLMs) of\nparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings reveal\nthat (1.) Traditional Chinese models still trail behind their Simplified\nChinese counterparts, highlighting a need for more focused advancements in LLMs\ncatering to Traditional Chinese. (2.) Current LLMs still fall short of human\nperformance in average scores, indicating a potential need for future research\nto delve deeper into social science and humanities subjects. (3.) Among all the\ntokenization compression metrics examined, we identify that only the fertility\nscore uniquely demonstrates strong correlations with our benchmark results. We\nforesee that TMMLU+ will pinpoint areas for future model improvement, thereby\nnarrowing the gap between machine and human linguistic capabilities and\nsupporting researchers in developing Traditional Chinese LLMs. Our dataset,\nalong with the benchmark source code, is accessible at\nhuggingface.co/datasets/ikala/tmmluplus.\n", "rewritten_text": "We introduce TMMLU+, a significantly expanded and improved multi-choice question-answering benchmark for Traditional Chinese language understanding.  Six times larger than its predecessor, TMMLU, TMMLU+ features a more balanced distribution across 66 subjects ranging from elementary to professional levels.  We evaluated 26 open-source and several closed-source Chinese LLMs (1.8B to 72B parameters) on TMMLU+, revealing that Traditional Chinese models lag behind their Simplified Chinese counterparts, and that even the best LLMs underperform humans, particularly on social science and humanities questions.  Our analysis of tokenization compression metrics showed only the fertility score consistently correlated with benchmark performance.  TMMLU+, available at huggingface.co/datasets/ikala/tmmluplus (including benchmark code), will help guide future research in developing more robust Traditional Chinese LLMs and bridging the gap between machine and human language comprehension.\n"}, "2005.04621": {"original_text": "  Deep convolutional neural networks generally perform well in underwater\nobject recognition tasks on both optical and sonar images. Many such methods\nrequire hundreds, if not thousands, of images per class to generalize well to\nunseen examples. However, obtaining and labeling sufficiently large volumes of\ndata can be relatively costly and time-consuming, especially when observing\nrare objects or performing real-time operations. Few-Shot Learning (FSL)\nefforts have produced many promising methods to deal with low data\navailability. However, little attention has been given in the underwater\ndomain, where the style of images poses additional challenges for object\nrecognition algorithms. To the best of our knowledge, this is the first paper\nto evaluate and compare several supervised and semi-supervised Few-Shot\nLearning (FSL) methods using underwater optical and side-scan sonar imagery.\nOur results show that FSL methods offer a significant advantage over the\ntraditional transfer learning methods that fine-tune pre-trained models. We\nhope that our work will help apply FSL to autonomous underwater systems and\nexpand their learning capabilities.\n", "rewritten_text": "Deep convolutional neural networks excel at underwater object recognition using both optical and sonar imagery, but typically require extensive training data.  Acquiring and labeling this data is expensive and time-consuming, particularly for rare objects or real-time applications. While Few-Shot Learning (FSL) offers a promising solution to this data scarcity problem, its application to the challenging underwater imaging domain remains largely unexplored.  This paper presents the first comprehensive evaluation and comparison of supervised and semi-supervised FSL methods on underwater optical and side-scan sonar datasets.  Our findings demonstrate that FSL significantly outperforms traditional transfer learning approaches, paving the way for enhanced learning capabilities in autonomous underwater systems.\n"}, "1712.01821": {"original_text": "  Factored neural machine translation (FNMT) is founded on the idea of using\nthe morphological and grammatical decomposition of the words (factors) at the\noutput side of the neural network. This architecture addresses two well-known\nproblems occurring in MT, namely the size of target language vocabulary and the\nnumber of unknown tokens produced in the translation. FNMT system is designed\nto manage larger vocabulary and reduce the training time (for systems with\nequivalent target language vocabulary size). Moreover, we can produce\ngrammatically correct words that are not part of the vocabulary. FNMT model is\nevaluated on IWSLT'15 English to French task and compared to the baseline\nword-based and BPE-based NMT systems. Promising qualitative and quantitative\nresults (in terms of BLEU and METEOR) are reported.\n", "rewritten_text": "Factored Neural Machine Translation (FNMT) improves upon traditional neural machine translation (NMT) by decomposing target-language words into morphological and grammatical factors. This approach mitigates the challenges of large vocabularies and unknown words, enabling FNMT to handle larger vocabularies and faster training times compared to equivalent word-based or Byte Pair Encoding (BPE)-based NMT systems.  Furthermore, FNMT can generate grammatically correct words even if they are not in its training vocabulary.  Experiments on the IWSLT'15 English-to-French translation task demonstrate promising improvements in BLEU and METEOR scores compared to word-based and BPE-based baselines.\n"}, "2206.01988": {"original_text": "  Automatic generation of ophthalmic reports using data-driven neural networks\nhas great potential in clinical practice. When writing a report,\nophthalmologists make inferences with prior clinical knowledge. This knowledge\nhas been neglected in prior medical report generation methods. To endow models\nwith the capability of incorporating expert knowledge, we propose a Cross-modal\nclinical Graph Transformer (CGT) for ophthalmic report generation (ORG), in\nwhich clinical relation triples are injected into the visual features as prior\nknowledge to drive the decoding procedure. However, two major common Knowledge\nNoise (KN) issues may affect models' effectiveness. 1) Existing general\nbiomedical knowledge bases such as the UMLS may not align meaningfully to the\nspecific context and language of the report, limiting their utility for\nknowledge injection. 2) Incorporating too much knowledge may divert the visual\nfeatures from their correct meaning. To overcome these limitations, we design\nan automatic information extraction scheme based on natural language processing\nto obtain clinical entities and relations directly from in-domain training\nreports. Given a set of ophthalmic images, our CGT first restores a sub-graph\nfrom the clinical graph and injects the restored triples into visual features.\nThen visible matrix is employed during the encoding procedure to limit the\nimpact of knowledge. Finally, reports are predicted by the encoded cross-modal\nfeatures via a Transformer decoder. Extensive experiments on the large-scale\nFFA-IR benchmark demonstrate that the proposed CGT is able to outperform\nprevious benchmark methods and achieve state-of-the-art performances.\n", "rewritten_text": "Automating ophthalmic report generation using neural networks offers significant clinical potential.  Existing methods overlook the crucial role of ophthalmologists' clinical expertise in report writing.  To address this, we introduce the Cross-modal clinical Graph Transformer (CGT) for ophthalmic report generation (ORG).  CGT incorporates clinical knowledge as relation triples, injected into image features to guide report generation.  However, we address two key knowledge noise (KN) challenges:  inaccurate general biomedical knowledge bases and the risk of knowledge overload.  We mitigate these by extracting clinical entities and relations directly from ophthalmic reports using NLP.  Our CGT then selectively injects this knowledge into image features using a visibility matrix, before generating reports via a Transformer decoder.  Extensive evaluation on the FFA-IR benchmark shows CGT achieves state-of-the-art performance.\n"}, "1602.01125": {"original_text": "  We propose a fully automatic method for fitting a 3D morphable model to\nsingle face images in arbitrary pose and lighting. Our approach relies on\ngeometric features (edges and landmarks) and, inspired by the iterated closest\npoint algorithm, is based on computing hard correspondences between model\nvertices and edge pixels. We demonstrate that this is superior to previous work\nthat uses soft correspondences to form an edge-derived cost surface that is\nminimised by nonlinear optimisation.\n", "rewritten_text": "This paper presents a fully automated method for fitting a 3D morphable face model to single images, regardless of pose or lighting.  Unlike previous methods using soft correspondences and nonlinear optimization of an edge-derived cost surface, our approach uses hard correspondences between model vertices and image edge pixels, inspired by the Iterative Closest Point algorithm.  We show that this approach outperforms existing techniques.\n"}, "2405.11862": {"original_text": "  Table structure recognition (TSR) aims to parse the inherent structure of a\ntable from its input image. The `\"split-and-merge\" paradigm is a pivotal\napproach to parse table structure, where the table separation line detection is\ncrucial. However, challenges such as wireless and deformed tables make it\ndemanding. In this paper, we adhere to the \"split-and-merge\" paradigm and\npropose SEMv3 (SEM: Split, Embed and Merge), a method that is both fast and\nrobust for detecting table separation lines. During the split stage, we\nintroduce a Keypoint Offset Regression (KOR) module, which effectively detects\ntable separation lines by directly regressing the offset of each line relative\nto its keypoint proposals. Moreover, in the merge stage, we define a series of\nmerge actions to efficiently describe the table structure based on table grids.\nExtensive ablation studies demonstrate that our proposed KOR module can detect\ntable separation lines quickly and accurately. Furthermore, on public datasets\n(e.g. WTW, ICDAR-2019 cTDaR Historical and iFLYTAB), SEMv3 achieves\nstate-of-the-art (SOTA) performance. The code is available at\nhttps://github.com/Chunchunwumu/SEMv3.\n", "rewritten_text": "This paper presents SEMv3, a fast and robust method for table structure recognition (TSR) using the split-and-merge paradigm.  Addressing the challenges posed by complex table layouts, SEMv3 introduces a novel Keypoint Offset Regression (KOR) module to accurately and efficiently detect table separation lines by directly regressing their offsets from keypoint proposals.  A refined merge stage, utilizing table grid information, efficiently constructs the final table structure.  Extensive experiments on benchmark datasets (WTW, ICDAR-2019 cTDaR Historical, iFLYTAB) demonstrate state-of-the-art performance.  The code is publicly available at https://github.com/Chunchunwumu/SEMv3.\n"}, "1505.04424": {"original_text": "  In this work, we propose a novel microaneurysm (MA) detection for early\ndiabetic retinopathy screening using color fundus images. Since MA usually the\nfirst lesions to appear as an indicator of diabetic retinopathy, accurate\ndetection of MA is necessary for treatment. Each pixel of the image is\nclassified as either MA or non-MA using a deep neural network with dropout\ntraining procedure using maxout activation function. No preprocessing step or\nmanual feature extraction is required. Substantial improvements over standard\nMA detection method based on the pipeline of preprocessing, feature extraction,\nclassification followed by post processing is achieved. The presented method is\nevaluated in publicly available Retinopathy Online Challenge (ROC) and\nDiaretdb1v2 database and achieved state-of-the-art accuracy.\n", "rewritten_text": "This paper introduces a novel deep learning method for automated microaneurysm (MA) detection in color fundus images, enabling early diabetic retinopathy screening.  Our approach directly classifies each pixel as MA or non-MA using a deep neural network with dropout and maxout activation, eliminating the need for preprocessing or manual feature extraction.  Evaluated on the publicly available ROC and Diaretdb1v2 datasets, our method significantly outperforms traditional MA detection pipelines and achieves state-of-the-art accuracy.  Early detection of MAs is crucial because they are often the first sign of diabetic retinopathy.\n"}, "2206.13156": {"original_text": "  Transformer has been widely used in histopathology whole slide image (WSI)\nclassification for the purpose of tumor grading, prognosis analysis, etc.\nHowever, the design of token-wise self-attention and positional embedding\nstrategy in the common Transformer limits the effectiveness and efficiency in\nthe application to gigapixel histopathology images. In this paper, we propose a\nkernel attention Transformer (KAT) for histopathology WSI classification. The\ninformation transmission of the tokens is achieved by cross-attention between\nthe tokens and a set of kernels related to a set of positional anchors on the\nWSI. Compared to the common Transformer structure, the proposed KAT can better\ndescribe the hierarchical context information of the local regions of the WSI\nand meanwhile maintains a lower computational complexity. The proposed method\nwas evaluated on a gastric dataset with 2040 WSIs and an endometrial dataset\nwith 2560 WSIs, and was compared with 6 state-of-the-art methods. The\nexperimental results have demonstrated the proposed KAT is effective and\nefficient in the task of histopathology WSI classification and is superior to\nthe state-of-the-art methods. The code is available at\nhttps://github.com/zhengyushan/kat.\n", "rewritten_text": "Gigapixel histopathology whole slide image (WSI) classification using transformers is hampered by the computational cost of standard token-wise self-attention.  This paper introduces Kernel Attention Transformer (KAT), a novel architecture that addresses this limitation.  KAT uses cross-attention between image tokens and kernels anchored at specific positions, efficiently capturing hierarchical contextual information within local WSI regions.  Evaluated on large gastric (2040 WSIs) and endometrial (2560 WSIs) datasets, KAT outperforms six state-of-the-art methods in WSI classification, demonstrating both effectiveness and efficiency.  The code is publicly available at https://github.com/zhengyushan/kat.\n"}, "1610.00291": {"original_text": "  We present a novel method for constructing Variational Autoencoder (VAE).\nInstead of using pixel-by-pixel loss, we enforce deep feature consistency\nbetween the input and the output of a VAE, which ensures the VAE's output to\npreserve the spatial correlation characteristics of the input, thus leading the\noutput to have a more natural visual appearance and better perceptual quality.\nBased on recent deep learning works such as style transfer, we employ a\npre-trained deep convolutional neural network (CNN) and use its hidden features\nto define a feature perceptual loss for VAE training. Evaluated on the CelebA\nface dataset, we show that our model produces better results than other methods\nin the literature. We also show that our method can produce latent vectors that\ncan capture the semantic information of face expressions and can be used to\nachieve state-of-the-art performance in facial attribute prediction.\n", "rewritten_text": "This paper introduces a novel Variational Autoencoder (VAE) architecture that prioritizes deep feature consistency over pixel-wise reconstruction.  By leveraging a pre-trained convolutional neural network (CNN) and its feature maps to define a perceptual loss function, our method ensures the VAE generates outputs with improved spatial correlation and visual fidelity.  Experiments on the CelebA dataset demonstrate superior performance compared to existing approaches, with generated latent vectors effectively capturing semantic information about facial expressions and enabling state-of-the-art facial attribute prediction.\n"}, "2206.00227": {"original_text": "  A data augmentation module is utilized in contrastive learning to transform\nthe given data example into two views, which is considered essential and\nirreplaceable. However, the predetermined composition of multiple data\naugmentations brings two drawbacks. First, the artificial choice of\naugmentation types brings specific representational invariances to the model,\nwhich have different degrees of positive and negative effects on different\ndownstream tasks. Treating each type of augmentation equally during training\nmakes the model learn non-optimal representations for various downstream tasks\nand limits the flexibility to choose augmentation types beforehand. Second, the\nstrong data augmentations used in classic contrastive learning methods may\nbring too much invariance in some cases, and fine-grained information that is\nessential to some downstream tasks may be lost. This paper proposes a general\nmethod to alleviate these two problems by considering where and what to\ncontrast in a general contrastive learning framework. We first propose to learn\ndifferent augmentation invariances at different depths of the model according\nto the importance of each data augmentation instead of learning\nrepresentational invariances evenly in the backbone. We then propose to expand\nthe contrast content with augmentation embeddings to reduce the misleading\neffects of strong data augmentations. Experiments based on several baseline\nmethods demonstrate that we learn better representations for various benchmarks\non classification, detection, and segmentation downstream tasks.\n", "rewritten_text": "Contrastive learning relies on data augmentation to create two views of each data example.  However, the fixed combination of augmentations used in existing methods has limitations.  First, the pre-selected augmentations impose specific invariances on the learned representations, which may be beneficial for some downstream tasks but detrimental to others.  This fixed approach prevents optimal representation learning for diverse tasks. Second, strong augmentations can lead to excessive invariance, potentially losing crucial fine-grained information needed for certain tasks.  This paper introduces a novel method to address these issues by dynamically controlling *where* and *what* to contrast.  We propose learning different augmentation invariances at varying network depths, prioritizing important augmentations, and enriching the contrastive information with augmentation embeddings to mitigate the negative effects of strong augmentations.  Experiments across classification, detection, and segmentation tasks demonstrate improved representation learning compared to baseline methods.\n"}, "1711.0719": {"original_text": "  We present a stochastic first-order optimization algorithm, named BCSC, that\nadds a cyclic constraint to stochastic block-coordinate descent. It uses\ndifferent subsets of the data to update different subsets of the parameters,\nthus limiting the detrimental effect of outliers in the training set. Empirical\ntests in benchmark datasets show that our algorithm outperforms\nstate-of-the-art optimization methods in both accuracy as well as convergence\nspeed. The improvements are consistent across different architectures, and can\nbe combined with other training techniques and regularization methods.\n", "rewritten_text": "Our novel stochastic first-order optimization algorithm, BCSC, enhances stochastic block-coordinate descent by incorporating a cyclic constraint.  This approach leverages diverse data subsets to update different parameter subsets, mitigating the negative impact of outliers.  Benchmark tests demonstrate BCSC's superior accuracy and convergence speed compared to existing methods across various architectures.  Furthermore, its benefits are compatible with other training techniques and regularization strategies.\n"}, "2008.11872": {"original_text": "  In Viticulture, visual inspection of the plant is a necessary task for\nmeasuring relevant variables. In many cases, these visual inspections are\nsusceptible to automation through computer vision methods. Bud detection is one\nsuch visual task, central for the measurement of important variables such as:\nmeasurement of bud sunlight exposure, autonomous pruning, bud counting,\ntype-of-bud classification, bud geometric characterization, internode length,\nbud area, and bud development stage, among others. This paper presents a\ncomputer method for grapevine bud detection based on a Fully Convolutional\nNetworks MobileNet architecture (FCN-MN). To validate its performance, this\narchitecture was compared in the detection task with a strong method for bud\ndetection, Scanning Windows (SW) based on a patch classifier, showing\nimprovements over three aspects of detection: segmentation, correspondence\nidentification and localization. The best version of FCN-MN showed a detection\nF1-measure of $88.6\\%$ (for true positives defined as detected components whose\nintersection-over-union with the true bud is above $0.5$), and false positives\nthat are small and near the true bud. Splits -- false positives overlapping the\ntrue bud -- showed a mean segmentation precision of $89.3\\% (21.7)$, while\nfalse alarms -- false positives not overlapping the true bud -- showed a mean\npixel area of only $8\\%$ the area of a true bud, and a distance (between mass\ncenters) of $1.1$ true bud diameters. The paper concludes by discussing how\nthese results for FCN-MN would produce sufficiently accurate measurements of\nbud variables such as bud number, bud area, and internode length, suggesting a\ngood performance in a practical setup.\n", "rewritten_text": "This paper introduces a novel computer vision method for automated grapevine bud detection, crucial for various viticultural measurements.  Using a Fully Convolutional Network based on MobileNet (FCN-MN), the system significantly outperforms a state-of-the-art Scanning Windows (SW) method in segmentation, correspondence identification, and localization.  The FCN-MN achieved an F1-score of 88.6% (IoU > 0.5), with false positives being small and proximate to true buds.  Specifically, false positives overlapping true buds (splits) exhibited 89.3% segmentation precision, while those not overlapping (false alarms) were only 8% the size of true buds and located within 1.1 bud diameters.  These results suggest the FCN-MN approach enables accurate measurement of key bud variables like number, area, and internode length, promising practical applicability in viticulture.\n"}, "1703.10798": {"original_text": "  We present a system for converting a fully panoramic ($360^\\circ$) video into\na normal field-of-view (NFOV) hyperlapse for an optimal viewing experience. Our\nsystem exploits visual saliency and semantics to non-uniformly sample in space\nand time for generating hyperlapses. In addition, users can optionally choose\nobjects of interest for customizing the hyperlapses. We first stabilize an\ninput $360^\\circ$ video by smoothing the rotation between adjacent frames and\nthen compute regions of interest and saliency scores. An initial hyperlapse is\ngenerated by optimizing the saliency and motion smoothness followed by the\nsaliency-aware frame selection. We further smooth the result using an efficient\n2D video stabilization approach that adaptively selects the motion model to\ngenerate the final hyperlapse. We validate the design of our system by showing\nresults for a variety of scenes and comparing against the state-of-the-art\nmethod through a user study.\n", "rewritten_text": "This paper introduces a system that transforms 360\u00b0 videos into engaging, standard-field-of-view hyperlapses.  Leveraging visual saliency and semantic understanding, our system intelligently samples both space and time to create dynamic hyperlapses.  Users can further personalize these hyperlapses by selecting specific objects of interest.  The process involves initial 360\u00b0 video stabilization, followed by identifying regions of interest and calculating saliency scores.  An optimized hyperlapse is then generated, prioritizing saliency and smooth motion, and subsequently refined with adaptive 2D video stabilization.  Finally, we demonstrate the system's effectiveness through a user study comparing our results to existing methods across diverse scenes.\n"}, "2001.03182": {"original_text": "  Unsupervised domain adaptation algorithms aim to transfer the knowledge\nlearned from one domain to another (e.g., synthetic to real images). The\nadapted representations often do not capture pixel-level domain shifts that are\ncrucial for dense prediction tasks (e.g., semantic segmentation). In this\npaper, we present a novel pixel-wise adversarial domain adaptation algorithm.\nBy leveraging image-to-image translation methods for data augmentation, our key\ninsight is that while the translated images between domains may differ in\nstyles, their predictions for the task should be consistent. We exploit this\nproperty and introduce a cross-domain consistency loss that enforces our\nadapted model to produce consistent predictions. Through extensive experimental\nresults, we show that our method compares favorably against the\nstate-of-the-art on a wide variety of unsupervised domain adaptation tasks.\n", "rewritten_text": "This paper introduces a new pixel-wise adversarial domain adaptation algorithm for improving dense prediction tasks like semantic segmentation.  Addressing the limitation of existing methods that often fail to account for crucial pixel-level domain shifts (e.g., between synthetic and real images), our approach uses image-to-image translation for data augmentation.  We leverage the insight that while translated images may differ stylistically, their task predictions should remain consistent.  This consistency is enforced via a novel cross-domain loss function.  Extensive experiments demonstrate state-of-the-art performance across diverse unsupervised domain adaptation benchmarks.\n"}, "1802.08636": {"original_text": "  Single document summarization is the task of producing a shorter version of a\ndocument while preserving its principal information content. In this paper we\nconceptualize extractive summarization as a sentence ranking task and propose a\nnovel training algorithm which globally optimizes the ROUGE evaluation metric\nthrough a reinforcement learning objective. We use our algorithm to train a\nneural summarization model on the CNN and DailyMail datasets and demonstrate\nexperimentally that it outperforms state-of-the-art extractive and abstractive\nsystems when evaluated automatically and by humans.\n", "rewritten_text": "This paper introduces a novel reinforcement learning algorithm for extractive text summarization.  By framing the task as sentence ranking and directly optimizing the ROUGE metric, our neural model achieves state-of-the-art performance on the CNN/DailyMail datasets, surpassing both extractive and abstractive methods in automated and human evaluations.\n"}, "2406.11192": {"original_text": "  Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets faces issues due to inconsistent entity definitions and\nredundant data, limiting LLMs to dataset-specific learning and hindering\nout-of-domain generalization. To address this, we present B2NERD, a cohesive\nand efficient dataset for Open NER, normalized from 54 existing English or\nChinese datasets using a two-step approach. First, we detect inconsistent\nentity definitions across datasets and clarify them by distinguishable label\nnames to construct a universal taxonomy of 400+ entity types. Second, we\naddress redundancy using a data pruning strategy that selects fewer samples\nwith greater category and semantic diversity. Comprehensive evaluation shows\nthat B2NERD significantly improves LLMs' generalization on Open NER. Our B2NER\nmodels, trained on B2NERD, outperform GPT-4 by 6.8-12.0 F1 points and surpass\nprevious methods in 3 out-of-domain benchmarks across 15 datasets and 6\nlanguages.\n", "rewritten_text": "Large language models (LLMs) struggle with open named entity recognition (NER), which involves identifying diverse entities from various domains. While fine-tuning on large NER datasets improves performance, inconsistencies in entity definitions and data redundancy hinder generalization.  To overcome this, we introduce B2NERD, a unified, efficient open NER dataset created by normalizing 54 English and Chinese datasets.  Our two-step process first harmonizes inconsistent entity definitions into a universal taxonomy of over 400 types, then prunes redundant data to maximize diversity.  Results show B2NERD significantly enhances LLM generalization in open NER.  Models trained on B2NERD (B2NER models) outperform GPT-4 by 6.8-12.0 F1 points and achieve state-of-the-art results on three out-of-domain benchmarks spanning 15 datasets and 6 languages.\n"}, "2411.01465": {"original_text": "  Despite the outstanding performance in many individual tasks, deep neural\nnetworks suffer from catastrophic forgetting when learning from continuous data\nstreams in real-world scenarios. Current Non-Exemplar Class-Incremental\nLearning (NECIL) methods mitigate forgetting by storing a single prototype per\nclass, which serves to inject previous information when sequentially learning\nnew classes. However, these stored prototypes or their augmented variants often\nfail to simultaneously capture spatial distribution diversity and precision\nneeded for representing old classes. Moreover, as the model acquires new\nknowledge, these prototypes gradually become outdated, making them less\neffective. To overcome these limitations, we propose a more efficient NECIL\nmethod that replaces prototypes with synthesized retrospective features for old\nclasses. Specifically, we model each old class's feature space using a\nmultivariate Gaussian distribution and generate deep representations by\nsampling from high-likelihood regions. Additionally, we introduce a\nsimilarity-based feature compensation mechanism that integrates generated old\nclass features with similar new class features to synthesize robust\nretrospective representations. These retrospective features are then\nincorporated into our incremental learning framework to preserve the decision\nboundaries of previous classes while learning new ones. Extensive experiments\non CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate that our method\nsignificantly improves the efficiency of non-exemplar class-incremental\nlearning and achieves state-of-the-art performance.\n", "rewritten_text": "Deep neural networks, while individually strong, struggle with catastrophic forgetting when handling continuous data streams.  Existing Non-Exemplar Class-Incremental Learning (NECIL) methods address this by using single class prototypes, but these prototypes are insufficient to accurately represent the full complexity of previously learned classes and become outdated over time.  We introduce a novel NECIL method that replaces prototypes with synthesized retrospective features.  This approach models each old class using a multivariate Gaussian distribution, generating representative features by sampling from high-probability regions.  A similarity-based compensation mechanism further enhances these features by integrating them with similar features from newly learned classes.  This improved representation of old classes, integrated into our incremental learning framework, preserves decision boundaries and significantly improves performance.  Experiments on CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate state-of-the-art results.\n"}, "2404.03654": {"original_text": "  NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel\nview synthesis and 3D reconstruction, but its performance is sensitive to input\nimage quality, which struggles to achieve high-fidelity rendering when provided\nwith low-quality sparse input viewpoints. Previous methods for NeRF restoration\nare tailored for specific degradation type, ignoring the generality of\nrestoration. To overcome this limitation, we propose a generic radiance fields\nrestoration pipeline, named RaFE, which applies to various types of\ndegradations, such as low resolution, blurriness, noise, compression artifacts,\nor their combinations. Our approach leverages the success of off-the-shelf 2D\nrestoration methods to recover the multi-view images individually. Instead of\nreconstructing a blurred NeRF by averaging inconsistencies, we introduce a\nnovel approach using Generative Adversarial Networks (GANs) for NeRF generation\nto better accommodate the geometric and appearance inconsistencies present in\nthe multi-view images. Specifically, we adopt a two-level tri-plane\narchitecture, where the coarse level remains fixed to represent the low-quality\nNeRF, and a fine-level residual tri-plane to be added to the coarse level is\nmodeled as a distribution with GAN to capture potential variations in\nrestoration. We validate RaFE on both synthetic and real cases for various\nrestoration tasks, demonstrating superior performance in both quantitative and\nqualitative evaluations, surpassing other 3D restoration methods specific to\nsingle task. Please see our project website\nhttps://zkaiwu.github.io/RaFE-Project/.\n", "rewritten_text": "Neural Radiance Fields (NeRFs) excel at novel view synthesis and 3D reconstruction, but their performance suffers with low-quality, sparse input images.  Existing NeRF restoration methods address only specific image degradations.  To overcome this limitation, we introduce RaFE, a general-purpose radiance field restoration pipeline handling various degradations (low resolution, blur, noise, compression artifacts, and combinations thereof).  RaFE pre-processes multi-view images using existing 2D restoration techniques.  Instead of averaging inconsistencies to reconstruct a blurred NeRF, it employs a novel GAN-based approach for NeRF generation, better handling geometric and appearance inconsistencies.  This uses a two-level tri-plane architecture: a fixed coarse level representing the low-quality NeRF, and a fine-level residual tri-plane modeled as a GAN-generated distribution to capture restoration variations.  Extensive synthetic and real-world evaluations demonstrate RaFE's superior quantitative and qualitative performance over single-task 3D restoration methods.  See our project website: https://zkaiwu.github.io/RaFE-Project/.\n"}, "2107.04217": {"original_text": "  This paper studies joint models for selecting correct answer sentences among\nthe top $k$ provided by answer sentence selection (AS2) modules, which are core\ncomponents of retrieval-based Question Answering (QA) systems. Our work shows\nthat a critical step to effectively exploit an answer set regards modeling the\ninterrelated information between pair of answers. For this purpose, we build a\nthree-way multi-classifier, which decides if an answer supports, refutes, or is\nneutral with respect to another one. More specifically, our neural architecture\nintegrates a state-of-the-art AS2 model with the multi-classifier, and a joint\nlayer connecting all components. We tested our models on WikiQA, TREC-QA, and a\nreal-world dataset. The results show that our models obtain the new state of\nthe art in AS2.\n", "rewritten_text": "This paper presents novel joint models for improving answer sentence selection (AS2) in retrieval-based question answering (QA).  We demonstrate that effectively leveraging candidate answer sets requires modeling inter-answer relationships.  To this end, we developed a three-way multi-classifier that determines the support, refutation, or neutrality of each answer pair. This classifier is integrated with a state-of-the-art AS2 model via a joint layer.  Experiments on WikiQA, TREC-QA, and a real-world dataset show our approach achieves state-of-the-art performance in AS2.\n"}, "2407.05271": {"original_text": "  Name-based gender prediction has traditionally categorized individuals as\neither female or male based on their names, using a binary classification\nsystem. That binary approach can be problematic in the cases of gender-neutral\nnames that do not align with any one gender, among other reasons. Relying\nsolely on binary gender categories without recognizing gender-neutral names can\nreduce the inclusiveness of gender prediction tasks. We introduce an additional\ngender category, i.e., \"neutral\", to study and address potential gender biases\nin Large Language Models (LLMs). We evaluate the performance of several\nfoundational and large language models in predicting gender based on first\nnames only. Additionally, we investigate the impact of adding birth years to\nenhance the accuracy of gender prediction, accounting for shifting associations\nbetween names and genders over time. Our findings indicate that most LLMs\nidentify male and female names with high accuracy (over 80%) but struggle with\ngender-neutral names (under 40%), and the accuracy of gender prediction is\nhigher for English-based first names than non-English names. The experimental\nresults show that incorporating the birth year does not improve the overall\naccuracy of gender prediction, especially for names with evolving gender\nassociations. We recommend using caution when applying LLMs for gender\nidentification in downstream tasks, particularly when dealing with non-binary\ngender labels.\n", "rewritten_text": "Traditional name-based gender prediction uses a binary (male/female) system, which fails to account for gender-neutral names.  To address this limitation and investigate potential biases in Large Language Models (LLMs), we added a \"neutral\" gender category.  Our evaluation of several LLMs, using first names (with and without birth years), revealed high accuracy (over 80%) for male and female names, but significantly lower accuracy (under 40%) for gender-neutral names, and higher accuracy for English names.  Adding birth years did not improve overall accuracy, especially for names with changing gender associations.  We caution against using LLMs for gender identification in applications requiring non-binary gender recognition.\n"}, "2306.09379": {"original_text": "  In this technical report, we briefly introduce the solution of our team\nVIELab-HUST for coded target restoration through atmospheric turbulence in CVPR\n2023 UG$^2$+ Track 2.2. In this task, we propose an efficient multi-stage\nframework to restore a high quality image from distorted frames. Specifically,\neach distorted frame is initially aligned using image registration to suppress\ngeometric distortion. We subsequently select the sharpest set of registered\nframes by employing a frame selection approach based on image sharpness, and\naverage them to produce an image that is largely free of geometric distortion,\nalbeit with blurriness. A learning-based deblurring method is then applied to\nremove the residual blur in the averaged image. Finally, post-processing\ntechniques are utilized to further enhance the quality of the output image. Our\nframework is capable of handling different kinds of coded target dataset\nprovided in the final testing phase, and ranked 1st on the final leaderboard.\nOur code will be available at https://github.com/xsqhust/Turbulence_Removal.\n", "rewritten_text": "This technical report details the VIELab-HUST team's winning (1st place) solution for coded target restoration through atmospheric turbulence in the CVPR 2023 UG\u00b2\u207a Track 2.2.  Our efficient multi-stage framework first aligns distorted frames via image registration to correct geometric distortions.  A sharpness-based frame selection then averages the sharpest registered frames, reducing blur but leaving some residual blurriness.  This is addressed by a learning-based deblurring method, followed by final post-processing for enhanced image quality.  Our robust framework successfully handled diverse coded target datasets in the final testing phase.  Code is available at https://github.com/xsqhust/Turbulence_Removal.\n"}, "2108.03502": {"original_text": "  Automatic summarization techniques aim to shorten and generalize information\ngiven in the text while preserving its core message and the most relevant\nideas. This task can be approached and treated with a variety of methods,\nhowever, not many attempts have been made to produce solutions specifically for\nthe Russian language despite existing localizations of the state-of-the-art\nmodels. In this paper, we aim to showcase ruGPT3 ability to summarize texts,\nfine-tuning it on the corpora of Russian news with their corresponding\nhuman-generated summaries. Additionally, we employ hyperparameter tuning so\nthat the model's output becomes less random and more tied to the original text.\nWe evaluate the resulting texts with a set of metrics, showing that our\nsolution can surpass the state-of-the-art model's performance without\nadditional changes in architecture or loss function. Despite being able to\nproduce sensible summaries, our model still suffers from a number of flaws,\nnamely, it is prone to altering Named Entities present in the original text\n(such as surnames, places, dates), deviating from facts stated in the given\ndocument, and repeating the information in the summary.\n", "rewritten_text": "This paper explores the application of ruGPT3, a Russian language model, to automatic text summarization.  While existing state-of-the-art summarization models often lack robust Russian language capabilities, we fine-tune ruGPT3 on a corpus of Russian news articles and their corresponding human-written summaries, optimizing hyperparameters to improve output coherence and accuracy.  Our results demonstrate performance exceeding existing state-of-the-art models without architectural or loss function modifications.  However, limitations remain, including inaccuracies in Named Entity recognition and occasional factual inconsistencies and repetition within the generated summaries.\n"}, "1901.01574": {"original_text": "  This work systematically analyzes the smoothing effect of vocabulary\nreduction for phrase translation models. We extensively compare various\nword-level vocabularies to show that the performance of smoothing is not\nsignificantly affected by the choice of vocabulary. This result provides\nempirical evidence that the standard phrase translation model is extremely\nsparse. Our experiments also reveal that vocabulary reduction is more effective\nfor smoothing large-scale phrase tables.\n", "rewritten_text": "This study systematically investigates how reducing vocabulary size impacts the smoothing of phrase-based machine translation models.  Our comprehensive comparison of different word vocabularies demonstrates that smoothing performance is largely insensitive to vocabulary selection, empirically confirming the extreme sparsity of standard phrase translation models.  Furthermore, our findings show that vocabulary reduction is particularly beneficial for smoothing large phrase tables.\n"}, "2210.16579": {"original_text": "  Generating videos is a complex task that is accomplished by generating a set\nof temporally coherent images frame-by-frame. This limits the expressivity of\nvideos to only image-based operations on the individual video frames needing\nnetwork designs to obtain temporally coherent trajectories in the underlying\nimage space. We propose INR-V, a video representation network that learns a\ncontinuous space for video-based generative tasks. INR-V parameterizes videos\nusing implicit neural representations (INRs), a multi-layered perceptron that\npredicts an RGB value for each input pixel location of the video. The INR is\npredicted using a meta-network which is a hypernetwork trained on neural\nrepresentations of multiple video instances. Later, the meta-network can be\nsampled to generate diverse novel videos enabling many downstream video-based\ngenerative tasks. Interestingly, we find that conditional regularization and\nprogressive weight initialization play a crucial role in obtaining INR-V. The\nrepresentation space learned by INR-V is more expressive than an image space\nshowcasing many interesting properties not possible with the existing works.\nFor instance, INR-V can smoothly interpolate intermediate videos between known\nvideo instances (such as intermediate identities, expressions, and poses in\nface videos). It can also in-paint missing portions in videos to recover\ntemporally coherent full videos. In this work, we evaluate the space learned by\nINR-V on diverse generative tasks such as video interpolation, novel video\ngeneration, video inversion, and video inpainting against the existing\nbaselines. INR-V significantly outperforms the baselines on several of these\ndemonstrated tasks, clearly showcasing the potential of the proposed\nrepresentation space.\n", "rewritten_text": "Video generation is challenging due to the frame-by-frame process, limiting expressiveness to image-based operations.  We introduce INR-V, a novel video representation network using implicit neural representations (INRs).  INR-V employs a multi-layered perceptron to predict RGB values for each pixel, guided by a meta-network trained on multiple video instances. This meta-network generates diverse, novel videos.  Crucially, conditional regularization and progressive weight initialization are key to INR-V's success.  The resulting continuous representation space surpasses image-based approaches, enabling smooth video interpolation (e.g., between different facial expressions), inpainting of missing video segments, and superior performance on various generative tasks (interpolation, novel video generation, inversion, and inpainting) compared to existing methods.  These results highlight INR-V's potential for advanced video generation.\n"}, "2101.01843": {"original_text": "  Autonomous driving applications use two types of sensor systems to identify\nvehicles - depth sensing LiDAR and radiance sensing cameras. We compare the\nperformance (average precision) of a ResNet for vehicle detection in complex,\ndaytime, driving scenes when the input is a depth map (D = d(x,y)), a radiance\nimage (L = r(x,y)), or both [D,L]. (1) When the spatial sampling resolution of\nthe depth map and radiance image are equal to typical camera resolutions, a\nResNet detects vehicles at higher average precision from depth than radiance.\n(2) As the spatial sampling of the depth map declines to the range of current\nLiDAR devices, the ResNet average precision is higher for radiance than depth.\n(3) For a hybrid system that combines a depth map and radiance image, the\naverage precision is higher than using depth or radiance alone. We established\nthese observations in simulation and then confirmed them using realworld data.\nThe advantage of combining depth and radiance can be explained by noting that\nthe two type of information have complementary weaknesses. The radiance data\nare limited by dynamic range and motion blur. The LiDAR data have relatively\nlow spatial resolution. The ResNet combines the two data sources effectively to\nimprove overall vehicle detection.\n", "rewritten_text": "This study compares the vehicle detection performance of a ResNet neural network using LiDAR depth maps, camera radiance images, and a combination of both, under complex daytime driving conditions.  Results show that at high-resolution, depth data yields superior average precision. However, at the lower resolutions typical of current LiDAR systems, radiance images perform better.  Crucially, combining depth and radiance data consistently outperforms either modality alone, both in simulation and real-world testing. This improvement stems from the complementary nature of the data sources:  LiDAR's low spatial resolution is offset by the camera's susceptibility to motion blur and limited dynamic range.  The ResNet effectively leverages this complementary information to enhance overall vehicle detection accuracy.\n"}, "1511.03328": {"original_text": "  Deep convolutional neural networks (CNNs) are the backbone of state-of-art\nsemantic image segmentation systems. Recent work has shown that complementing\nCNNs with fully-connected conditional random fields (CRFs) can significantly\nenhance their object localization accuracy, yet dense CRF inference is\ncomputationally expensive. We propose replacing the fully-connected CRF with\ndomain transform (DT), a modern edge-preserving filtering method in which the\namount of smoothing is controlled by a reference edge map. Domain transform\nfiltering is several times faster than dense CRF inference and we show that it\nyields comparable semantic segmentation results, accurately capturing object\nboundaries. Importantly, our formulation allows learning the reference edge map\nfrom intermediate CNN features instead of using the image gradient magnitude as\nin standard DT filtering. This produces task-specific edges in an end-to-end\ntrainable system optimizing the target semantic segmentation quality.\n", "rewritten_text": "State-of-the-art semantic image segmentation relies heavily on deep convolutional neural networks (CNNs). While adding fully-connected conditional random fields (CRFs) improves object localization, their computational cost is high.  This paper introduces a faster alternative: replacing the CRF with a domain transform (DT), an edge-preserving filter controlled by a learned reference edge map.  This approach achieves comparable segmentation accuracy to CRFs, accurately delineating object boundaries, while being significantly faster. Crucially, instead of relying on image gradients, our method learns the reference edge map from intermediate CNN features, creating a fully end-to-end trainable system optimized for semantic segmentation performance.\n"}, "1610.03155": {"original_text": "  Convolutional Neural Networks (CNN) have demon- strated its successful\napplications in computer vision, speech recognition, and natural language\nprocessing. For object recog- nition, CNNs might be limited by its strict label\nrequirement and an implicit assumption that images are supposed to be target-\nobject-dominated for optimal solutions. However, the labeling procedure,\nnecessitating laying out the locations of target ob- jects, is very tedious,\nmaking high-quality large-scale dataset prohibitively expensive. Data\naugmentation schemes are widely used when deep networks suffer the insufficient\ntraining data problem. All the images produced through data augmentation share\nthe same label, which may be problematic since not all data augmentation\nmethods are label-preserving. In this paper, we propose a weakly supervised CNN\nframework named Multiple Instance Learning Convolutional Neural Networks\n(MILCNN) to solve this problem. We apply MILCNN framework to object recognition\nand report state-of-the-art performance on three benchmark datasets: CIFAR10,\nCIFAR100 and ILSVRC2015 classification dataset.\n", "rewritten_text": "Convolutional Neural Networks (CNNs) have proven highly effective in computer vision, speech recognition, and natural language processing.  However, their application to object recognition is hampered by a strict label requirement and the assumption of target-object-dominated images.  The laborious process of precisely labeling object locations in images makes creating large, high-quality datasets prohibitively expensive. While data augmentation mitigates limited training data, it can be problematic as not all augmentation techniques preserve labels.  This paper introduces a weakly supervised CNN framework, Multiple Instance Learning Convolutional Neural Networks (MILCNN), to address this issue.  We demonstrate its state-of-the-art performance on three benchmark datasets: CIFAR-10, CIFAR-100, and ILSVRC2015.\n"}, "1711.06606": {"original_text": "  To realize the full potential of deep learning for medical imaging, large\nannotated datasets are required for training. Such datasets are difficult to\nacquire because labeled medical images are not usually available due to privacy\nissues, lack of experts available for annotation, underrepresentation of rare\nconditions and poor standardization. Lack of annotated data has been addressed\nin conventional vision applications using synthetic images refined via\nunsupervised adversarial training to look like real images. However, this\napproach is difficult to extend to general medical imaging because of the\ncomplex and diverse set of features found in real human tissues. We propose an\nalternative framework that uses a reverse flow, where adversarial training is\nused to make real medical images more like synthetic images, and hypothesize\nthat clinically-relevant features can be preserved via self-regularization.\nThese domain-adapted images can then be accurately interpreted by networks\ntrained on large datasets of synthetic medical images. We test this approach\nfor the notoriously difficult task of depth-estimation from endoscopy. We train\na depth estimator on a large dataset of synthetic images generated using an\naccurate forward model of an endoscope and an anatomically-realistic colon.\nThis network predicts significantly better depths when using synthetic-like\ndomain-adapted images compared to the real images, confirming that the\nclinically-relevant features of depth are preserved.\n", "rewritten_text": "Deep learning's potential in medical imaging is hampered by the scarcity of large, annotated datasets.  Acquiring these is challenging due to privacy concerns, annotation expertise limitations, under-representation of rare diseases, and standardization issues. While synthetic images refined via adversarial training have addressed data scarcity in general vision, this approach struggles with the complexity of real medical images.  We propose a novel framework:  adversarially training real medical images to resemble synthetic ones, preserving clinically relevant features through self-regularization.  This domain adaptation allows networks trained on large synthetic datasets to accurately interpret the adapted real images.  We validate this approach on the challenging task of endoscopic depth estimation, training a depth estimator on synthetic images generated from a realistic colon model.  Results show significantly improved depth prediction using our domain-adapted images compared to using raw real images, demonstrating the preservation of clinically relevant depth information.\n"}, "2402.17074": {"original_text": "  Digital Image Correlation (DIC) is an optical technique that measures\ndisplacement and strain by tracking pattern movement in a sequence of captured\nimages during testing. DIC has gained recognition in asphalt pavement\nengineering since the early 2000s. However, users often perceive the DIC\ntechnique as an out-of-box tool and lack a thorough understanding of its\noperational and measurement principles. This article presents a state-of-art\nreview of DIC as a crucial tool for laboratory testing of asphalt concrete\n(AC), primarily focusing on the widely utilized 2D-DIC and 3D-DIC techniques.\nTo address frequently asked questions from users, the review thoroughly\nexamines the optimal methods for preparing speckle patterns, configuring\nsingle-camera or dual-camera imaging systems, conducting DIC analyses, and\nexploring various applications. Furthermore, emerging DIC methodologies such as\nDigital Volume Correlation and deep-learning-based DIC are introduced,\nhighlighting their potential for future applications in pavement engineering.\nThe article also provides a comprehensive and reliable flowchart for\nimplementing DIC in AC characterization. Finally, critical directions for\nfuture research are presented.\n", "rewritten_text": "This article provides a comprehensive review of Digital Image Correlation (DIC), a crucial optical technique for analyzing asphalt concrete (AC) in laboratory settings.  While widely adopted in pavement engineering since the early 2000s, DIC is often misunderstood.  This review addresses this gap by detailing the principles of 2D and 3D DIC, offering practical guidance on speckle pattern preparation, camera system configuration, data analysis, and diverse applications.  It also explores emerging techniques like Digital Volume Correlation and deep-learning-based DIC, and includes a practical flowchart for implementing DIC in AC characterization.  Finally, the article identifies key areas for future research.\n"}, "1805.06173": {"original_text": "  Existing deep convolutional neural networks have found major success in image\nderaining, but at the expense of an enormous number of parameters. This limits\ntheir potential application, for example in mobile devices. In this paper, we\npropose a lightweight pyramid of networks (LPNet) for single image deraining.\nInstead of designing a complex network structures, we use domain-specific\nknowledge to simplify the learning process. Specifically, we find that by\nintroducing the mature Gaussian-Laplacian image pyramid decomposition\ntechnology to the neural network, the learning problem at each pyramid level is\ngreatly simplified and can be handled by a relatively shallow network with few\nparameters. We adopt recursive and residual network structures to build the\nproposed LPNet, which has less than 8K parameters while still achieving\nstate-of-the-art performance on rain removal. We also discuss the potential\nvalue of LPNet for other low- and high-level vision tasks.\n", "rewritten_text": "Deep convolutional neural networks (CNNs) excel at image deraining, but their massive parameter counts hinder deployment on resource-constrained devices like mobile phones.  This paper introduces LPNet, a lightweight pyramid network for single image deraining.  Leveraging Gaussian-Laplacian pyramid decomposition, LPNet simplifies the learning process by decomposing the task into smaller, manageable sub-problems solved by shallow networks.  This recursive, residual architecture achieves state-of-the-art rain removal with fewer than 8,000 parameters.  Its potential applicability to other low- and high-level vision tasks is also explored.\n"}, "2208.00919": {"original_text": "  Visual-inertial localization is a key problem in computer vision and robotics\napplications such as virtual reality, self-driving cars, and aerial vehicles.\nThe goal is to estimate an accurate pose of an object when either the\nenvironment or the dynamics are known. Absolute pose regression (APR)\ntechniques directly regress the absolute pose from an image input in a known\nscene using convolutional and spatio-temporal networks. Odometry methods\nperform relative pose regression (RPR) that predicts the relative pose from a\nknown object dynamic (visual or inertial inputs). The localization task can be\nimproved by retrieving information from both data sources for a cross-modal\nsetup, which is a challenging problem due to contradictory tasks. In this work,\nwe conduct a benchmark to evaluate deep multimodal fusion based on pose graph\noptimization and attention networks. Auxiliary and Bayesian learning are\nutilized for the APR task. We show accuracy improvements for the APR-RPR task\nand for the RPR-RPR task for aerial vehicles and hand-held devices. We conduct\nexperiments on the EuRoC MAV and PennCOSYVIO datasets and record and evaluate a\nnovel industry dataset.\n", "rewritten_text": "Accurate pose estimation (localization) is crucial for various applications like virtual reality, autonomous vehicles, and drones.  This often involves combining visual and inertial sensor data.  While absolute pose regression (APR) directly estimates pose from images in known environments, odometry methods (relative pose regression, RPR) predict relative pose changes based on sensor dynamics.  This work benchmarks deep multimodal fusion techniques, leveraging pose graph optimization and attention networks, to improve localization accuracy.  We employ auxiliary and Bayesian learning for APR, demonstrating improved performance on APR-RPR and RPR-RPR tasks using aerial and handheld devices.  Our experiments utilize the EuRoC MAV and PennCOSYVIO datasets, and a novel industrial dataset.\n"}, "2107.0984": {"original_text": "  Multilingual pre-trained contextual embedding models (Devlin et al., 2019)\nhave achieved impressive performance on zero-shot cross-lingual transfer tasks.\nFinding the most effective fine-tuning strategy to fine-tune these models on\nhigh-resource languages so that it transfers well to the zero-shot languages is\na non-trivial task. In this paper, we propose a novel meta-optimizer to\nsoft-select which layers of the pre-trained model to freeze during fine-tuning.\nWe train the meta-optimizer by simulating the zero-shot transfer scenario.\nResults on cross-lingual natural language inference show that our approach\nimproves over the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al.,\n2020).\n", "rewritten_text": "Pre-trained multilingual language models excel at zero-shot cross-lingual tasks, but optimally fine-tuning them for high-resource languages to benefit low-resource ones remains challenging.  This paper introduces a novel meta-optimizer that selectively freezes pre-trained model layers during fine-tuning, a strategy trained via simulated zero-shot transfer.  Experiments on cross-lingual natural language inference demonstrate superior performance compared to standard fine-tuning and X-MAML.\n"}, "2010.09298": {"original_text": "  Though deep learning has achieved advanced performance recently, it remains a\nchallenging task in the field of medical imaging, as obtaining reliable labeled\ntraining data is time-consuming and expensive. In this paper, we propose a\ndouble-uncertainty weighted method for semi-supervised segmentation based on\nthe teacher-student model. The teacher model provides guidance for the student\nmodel by penalizing their inconsistent prediction on both labeled and unlabeled\ndata. We train the teacher model using Bayesian deep learning to obtain\ndouble-uncertainty, i.e. segmentation uncertainty and feature uncertainty. It\nis the first to extend segmentation uncertainty estimation to feature\nuncertainty, which reveals the capability to capture information among\nchannels. A learnable uncertainty consistency loss is designed for the\nunsupervised learning process in an interactive manner between prediction and\nuncertainty. With no ground-truth for supervision, it can still incentivize\nmore accurate teacher's predictions and facilitate the model to reduce\nuncertain estimations. Furthermore, our proposed double-uncertainty serves as a\nweight on each inconsistency penalty to balance and harmonize supervised and\nunsupervised training processes. We validate the proposed feature uncertainty\nand loss function through qualitative and quantitative analyses. Experimental\nresults show that our method outperforms the state-of-the-art uncertainty-based\nsemi-supervised methods on two public medical datasets.\n", "rewritten_text": "Despite recent advancements, deep learning for medical image segmentation faces challenges due to the high cost and time required for labeled data.  This paper introduces a novel semi-supervised segmentation method using a teacher-student model with a double-uncertainty weighting scheme.  The Bayesian teacher model provides guidance by penalizing prediction inconsistencies on both labeled and unlabeled data, leveraging both segmentation and \u2013 uniquely \u2013 feature uncertainty. This feature uncertainty estimation captures inter-channel information.  A learnable uncertainty consistency loss interactively guides unsupervised learning, encouraging accurate teacher predictions and reduced uncertainty even without ground truth.  This double-uncertainty weighting balances supervised and unsupervised training.  Qualitative and quantitative evaluations on two public medical datasets demonstrate superior performance compared to existing uncertainty-based semi-supervised methods.\n"}, "2108.11575": {"original_text": "  Spatio-temporal representational learning has been widely adopted in various\nfields such as action recognition, video object segmentation, and action\nanticipation. Previous spatio-temporal representational learning approaches\nprimarily employ ConvNets or sequential models,e.g., LSTM, to learn the\nintra-frame and inter-frame features. Recently, Transformer models have\nsuccessfully dominated the study of natural language processing (NLP), image\nclassification, etc. However, the pure-Transformer based spatio-temporal\nlearning can be prohibitively costly on memory and computation to extract\nfine-grained features from a tiny patch. To tackle the training difficulty and\nenhance the spatio-temporal learning, we construct a shifted chunk Transformer\nwith pure self-attention blocks. Leveraging the recent efficient Transformer\ndesign in NLP, this shifted chunk Transformer can learn hierarchical\nspatio-temporal features from a local tiny patch to a global video clip. Our\nshifted self-attention can also effectively model complicated inter-frame\nvariances. Furthermore, we build a clip encoder based on Transformer to model\nlong-term temporal dependencies. We conduct thorough ablation studies to\nvalidate each component and hyper-parameters in our shifted chunk Transformer,\nand it outperforms previous state-of-the-art approaches on Kinetics-400,\nKinetics-600, UCF101, and HMDB51.\n", "rewritten_text": "Spatio-temporal learning, crucial for tasks like action recognition and video segmentation, has traditionally relied on convolutional neural networks (ConvNets) and recurrent models (e.g., LSTMs).  While Transformers have excelled in NLP and image classification, their direct application to spatio-temporal learning is computationally expensive, especially for fine-grained feature extraction from small patches.  To address this, we introduce a shifted chunk Transformer with pure self-attention blocks.  Inspired by efficient Transformer designs in NLP, our model hierarchically learns spatio-temporal features, from local patches to entire video clips, effectively capturing complex inter-frame variations.  A supplementary Transformer-based clip encoder models long-range temporal dependencies.  Extensive ablation studies confirm the effectiveness of our design, which surpasses state-of-the-art results on Kinetics-400, Kinetics-600, UCF101, and HMDB51 datasets.\n"}, "2102.0208": {"original_text": "  We introduce a top-down approach to discourse parsing that is conceptually\nsimpler than its predecessors (Kobayashi et al., 2020; Zhang et al., 2020). By\nframing the task as a sequence labelling problem where the goal is to\niteratively segment a document into individual discourse units, we are able to\neliminate the decoder and reduce the search space for splitting points. We\nexplore both traditional recurrent models and modern pre-trained transformer\nmodels for the task, and additionally introduce a novel dynamic oracle for\ntop-down parsing. Based on the Full metric, our proposed LSTM model sets a new\nstate-of-the-art for RST parsing.\n", "rewritten_text": "This paper presents a novel, conceptually simpler top-down approach to discourse parsing, surpassing previous methods (Kobayashi et al., 2020; Zhang et al., 2020).  By formulating discourse parsing as iterative sequence labeling to segment documents into discourse units, we eliminate the decoder and reduce the search complexity.  We evaluate both recurrent and transformer models, introducing a new dynamic oracle for improved top-down parsing.  Our LSTM model achieves state-of-the-art performance on the Full metric for RST parsing.\n"}, "1112.2386": {"original_text": "  This paper proposes a new procedure in order to improve the performance of\nblock matching and 3-D filtering (BM3D) image denoising algorithm. It is\ndemonstrated that it is possible to achieve a better performance than that of\nBM3D algorithm in a variety of noise levels. This method changes BM3D algorithm\nparameter values according to noise level, removes prefiltering, which is used\nin high noise level; therefore Peak Signal-to-Noise Ratio (PSNR) and visual\nquality get improved, and BM3D complexities and processing time are reduced.\nThis improved BM3D algorithm is extended and used to denoise satellite and\ncolor filter array (CFA) images. Output results show that the performance has\nupgraded in comparison with current methods of denoising satellite and CFA\nimages. In this regard this algorithm is compared with Adaptive PCA algorithm,\nthat has led to superior performance for denoising CFA images, on the subject\nof PSNR and visual quality. Also the processing time has decreased\nsignificantly.\n", "rewritten_text": "This paper presents a modified BM3D image denoising algorithm that achieves superior performance across various noise levels.  By adaptively adjusting parameters based on noise level and eliminating prefiltering in high-noise scenarios, the proposed method improves PSNR, visual quality, and reduces computational complexity.  Furthermore, this enhanced algorithm effectively denoises satellite and CFA images, outperforming existing methods, including Adaptive PCA, in terms of PSNR, visual quality, and processing speed, particularly for CFA image denoising.\n"}, "2103.06678": {"original_text": "  In this paper, we explore the effects of language variants, data sizes, and\nfine-tuning task types in Arabic pre-trained language models. To do so, we\nbuild three pre-trained language models across three variants of Arabic: Modern\nStandard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a\nfourth language model which is pre-trained on a mix of the three. We also\nexamine the importance of pre-training data size by building additional models\nthat are pre-trained on a scaled-down set of the MSA variant. We compare our\ndifferent models to each other, as well as to eight publicly available models\nby fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest\nthat the variant proximity of pre-training data to fine-tuning data is more\nimportant than the pre-training data size. We exploit this insight in defining\nan optimized system selection model for the studied tasks.\n", "rewritten_text": "This paper investigates how variations in Arabic language data (Modern Standard Arabic, dialectal Arabic, Classical Arabic, and a mixed variant), training data size, and fine-tuning task type affect the performance of pre-trained language models.  We trained four models using these Arabic variants, plus additional MSA models trained on reduced datasets.  These models, along with eight publicly available models, were fine-tuned on five NLP tasks across twelve datasets.  Our findings indicate that the similarity between the pre-training and fine-tuning data is a more significant factor than the pre-training data size.  This led to the development of an optimized model selection system for the tasks studied.\n"}, "2408.00331": {"original_text": "  Reliably detecting when a deployed machine learning model is likely to fail\non a given input is crucial for ensuring safe operation. In this work, we\npropose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel\napproach that leverages priors from large language models (LLMs) and\nvision-language models (VLMs) to detect failures in image classification\nmodels. DECIDER utilizes LLMs to specify task-relevant core attributes and\nconstructs a ``debiased'' version of the classifier by aligning its visual\nfeatures to these core attributes using a VLM, and detects potential failure by\nmeasuring disagreement between the original and debiased models. In addition to\nproactively identifying samples on which the model would fail, DECIDER also\nprovides human-interpretable explanations for failure through a novel\nattribute-ablation strategy. Through extensive experiments across diverse\nbenchmarks spanning subpopulation shifts (spurious correlations, class\nimbalance) and covariate shifts (synthetic corruptions, domain shifts), DECIDER\nconsistently achieves state-of-the-art failure detection performance,\nsignificantly outperforming baselines in terms of the overall Matthews\ncorrelation coefficient as well as failure and success recall. Our codes can be\naccessed at~\\url{https://github.com/kowshikthopalli/DECIDER/}\n", "rewritten_text": "Ensuring the safe operation of deployed machine learning models requires reliable failure detection.  DECIDER (Debiasing Classifiers to Identify Errors Reliably) achieves this by using large language models (LLMs) and vision-language models (VLMs).  DECIDER identifies core attributes from LLMs, then \"debiases\" an image classifier by aligning its visual features to these attributes using a VLM.  Failure is detected by comparing the original and debiased model outputs.  Furthermore, DECIDER offers human-interpretable failure explanations via attribute ablation.  Extensive experiments across various benchmarks (including subpopulation and covariate shifts) demonstrate state-of-the-art failure detection performance, surpassing existing methods in Matthews correlation coefficient, failure recall, and success recall.  The code is available at [https://github.com/kowshikthopalli/DECIDER/](https://github.com/kowshikthopalli/DECIDER/).\n"}, "1708.06126": {"original_text": "  This paper presents a novel application to detect counterfeit identity\ndocuments forged by a scan-printing operation. Texture analysis approaches are\nproposed to extract validation features from security background that is\nusually printed in documents as IDs or banknotes. The main contribution of this\nwork is the end-to-end mobile-server architecture, which provides a service for\nnon-expert users and therefore can be used in several scenarios. The system\nalso provides a crowdsourcing mode so labeled images can be gathered,\ngenerating databases for incremental training of the algorithms.\n", "rewritten_text": "This paper introduces a new mobile-server system for detecting counterfeit identity documents created via scan-printing.  The system uses texture analysis of security backgrounds (common in IDs and banknotes) to identify forgeries.  Its key innovation is its user-friendly design, accessible to non-experts across various applications.  Furthermore, a crowdsourcing component enables continuous data collection and model improvement through incremental training.\n"}, "2409.12421": {"original_text": "  Camouflaged object detection (COD) aims to segment camouflaged objects which\nexhibit very similar patterns with the surrounding environment. Recent research\nworks have shown that enhancing the feature representation via the frequency\ninformation can greatly alleviate the ambiguity problem between the foreground\nobjects and the background.With the emergence of vision foundation models, like\nInternImage, Segment Anything Model etc, adapting the pretrained model on COD\ntasks with a lightweight adapter module shows a novel and promising research\ndirection. Existing adapter modules mainly care about the feature adaptation in\nthe spatial domain. In this paper, we propose a novel frequency-guided spatial\nadaptation method for COD task. Specifically, we transform the input features\nof the adapter into frequency domain. By grouping and interacting with\nfrequency components located within non overlapping circles in the spectrogram,\ndifferent frequency components are dynamically enhanced or weakened, making the\nintensity of image details and contour features adaptively adjusted. At the\nsame time, the features that are conducive to distinguishing object and\nbackground are highlighted, indirectly implying the position and shape of\ncamouflaged object. We conduct extensive experiments on four widely adopted\nbenchmark datasets and the proposed method outperforms 26 state-of-the-art\nmethods with large margins. Code will be released.\n", "rewritten_text": "Camouflaged object detection (COD) struggles with the visual similarity between objects and their backgrounds.  Recent advancements leverage frequency information to improve feature representation and address this ambiguity.  Building on the success of vision foundation models like InternImage and Segment Anything Model, this paper introduces a novel frequency-guided spatial adaptation method.  This approach transforms adapter module input features into the frequency domain, dynamically enhancing or suppressing frequency components within non-overlapping circular regions of the spectrogram. This adaptive adjustment of image detail and contour intensity highlights features crucial for object-background discrimination, indirectly revealing camouflaged object location and shape.  Extensive experiments on four benchmark datasets demonstrate superior performance compared to 26 state-of-the-art methods.  Code will be publicly available.\n"}, "1712.03687": {"original_text": "  Because of affected by weather conditions, camera pose and range, etc.\nObjects are usually small, blur, occluded and diverse pose in the images\ngathered from outdoor surveillance cameras or access control system. It is\nchallenging and important to detect faces precisely for face recognition system\nin the field of public security. In this paper, we design a based on context\nmodeling structure named Feature Hierarchy Encoder-Decoder Network for face\ndetection(FHEDN), which can detect small, blur and occluded face with hierarchy\nby hierarchy from the end to the beginning likes encoder-decoder in a single\nnetwork. The proposed network is consist of multiple context modeling and\nprediction modules, which are in order to detect small, blur, occluded and\ndiverse pose faces. In addition, we analyse the influence of distribution of\ntraining set, scale of default box and receipt field size to detection\nperformance in implement stage. Demonstrated by experiments, Our network\nachieves promising performance on WIDER FACE and FDDB benchmarks.\n", "rewritten_text": "Outdoor surveillance and access control images often contain small, blurry, occluded, and variably posed faces due to challenging weather, camera angles, and distances.  Accurate face detection in these images is crucial for public security applications but remains difficult.  This paper introduces the Feature Hierarchy Encoder-Decoder Network (FHEDN), a novel context-modeling architecture for robust face detection.  FHEDN employs a hierarchical encoder-decoder structure to progressively detect faces of varying quality, from small and occluded to clearer ones.  The network incorporates multiple context modeling and prediction modules to handle diverse face poses and conditions.  Furthermore, we analyze the impact of training data distribution, default box scales, and receptive field size on performance.  Experimental results on the WIDER FACE and FDDB benchmarks demonstrate the promising performance of our approach.\n"}, "2010.01305": {"original_text": "  Street view images classification aiming at urban land use analysis is\ndifficult because the class labels (e.g., commercial area), are concepts with\nhigher abstract level compared to the ones of general visual tasks (e.g.,\npersons and cars). Therefore, classification models using only visual features\noften fail to achieve satisfactory performance. In this paper, a novel approach\nbased on a \"Detector-Encoder-Classifier\" framework is proposed. Instead of\nusing visual features of the whole image directly as common image-level models\nbased on convolutional neural networks (CNNs) do, the proposed framework\nfirstly obtains the bounding boxes of buildings in street view images from a\ndetector. Their contextual information such as the co-occurrence patterns of\nbuilding classes and their layout are then encoded into metadata by the\nproposed algorithm \"CODING\" (Context encOding of Detected buildINGs). Finally,\nthese bounding box metadata are classified by a recurrent neural network (RNN).\nIn addition, we made a dual-labeled dataset named \"BEAUTY\" (Building dEtection\nAnd Urban funcTional-zone portraYing) of 19,070 street view images and 38,857\nbuildings based on the existing BIC GSV [1]. The dataset can be used not only\nfor street view image classification, but also for multi-class building\ndetection. Experiments on \"BEAUTY\" show that the proposed approach achieves a\n12.65% performance improvement on macro-precision and 12% on macro-recall over\nimage-level CNN based models. Our code and dataset are available at\nhttps://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/\n", "rewritten_text": "Classifying street view images for urban land use analysis is challenging due to the abstract nature of land use categories (e.g., \"commercial area\") compared to typical image recognition tasks (e.g., identifying cars and people).  Standard convolutional neural network (CNN) approaches, relying solely on visual features, often underperform.  This paper introduces a novel \"Detector-Encoder-Classifier\" framework.  Instead of directly classifying entire images, it first detects building bounding boxes.  A new algorithm, CODING (Context encOding of Detected buildINGs), then encodes contextual information, such as building class co-occurrence and layout, into metadata.  Finally, a recurrent neural network (RNN) classifies this metadata.  We also present BEAUTY (Building dEtection And Urban funcTional-zone portraYing), a new dual-labeled dataset of 19,070 street view images and 38,857 buildings, derived from BIC GSV [1].  Experiments on BEAUTY demonstrate a 12.65% improvement in macro-precision and 12% in macro-recall over image-level CNN baselines.  Our code and dataset are publicly available at https://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/.\n"}, "2403.10737": {"original_text": "  Facial action unit (AU) detection is a fundamental block for objective facial\nexpression analysis. Supervised learning approaches require a large amount of\nmanual labeling which is costly. The limited labeled data are also not diverse\nin terms of gender which can affect model fairness. In this paper, we propose\nto use synthetically generated data and multi-source domain adaptation (MSDA)\nto address the problems of the scarcity of labeled data and the diversity of\nsubjects. Specifically, we propose to generate a diverse dataset through\nsynthetic facial expression re-targeting by transferring the expressions from\nreal faces to synthetic avatars. Then, we use MSDA to transfer the AU detection\nknowledge from a real dataset and the synthetic dataset to a target dataset.\nInstead of aligning the overall distributions of different domains, we propose\nPaired Moment Matching (PM2) to align the features of the paired real and\nsynthetic data with the same facial expression. To further improve gender\nfairness, PM2 matches the features of the real data with a female and a male\nsynthetic image. Our results indicate that synthetic data and the proposed\nmodel improve both AU detection performance and fairness across genders,\ndemonstrating its potential to solve AU detection in-the-wild.\n", "rewritten_text": "Objective facial expression analysis relies on accurate facial action unit (AU) detection.  However, current supervised learning methods suffer from the high cost and limited diversity (particularly regarding gender) of manually labeled data.  This paper addresses these limitations by leveraging synthetic data and multi-source domain adaptation (MSDA).  We generate a diverse synthetic dataset by re-targeting real facial expressions onto synthetic avatars.  A novel paired moment matching (PM2) method then aligns features from real and synthetic datasets, specifically matching real-world images with both male and female synthetic counterparts to enhance gender fairness.  Our results show that this approach significantly improves AU detection accuracy and fairness, promising more robust \"in-the-wild\" performance.\n"}, "2401.03129": {"original_text": "  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n", "rewritten_text": "Large Language Models (LLMs) have shown impressive capabilities across diverse applications, fueling rapid LLM development.  A common practice\u2014continual pre-training on already fine-tuned models\u2014risks catastrophic forgetting.  This research examines this forgetting phenomenon, evaluating the impact of continual pre-training on a fine-tuned LLM's output format, knowledge retention, and reliability.  Our experiments reveal the significant challenge of mitigating catastrophic forgetting, particularly the problem of repeated training data.\n"}, "2105.07197": {"original_text": "  Modern machine learning models for computer vision exceed humans in accuracy\non specific visual recognition tasks, notably on datasets like ImageNet.\nHowever, high accuracy can be achieved in many ways. The particular decision\nfunction found by a machine learning system is determined not only by the data\nto which the system is exposed, but also the inductive biases of the model,\nwhich are typically harder to characterize. In this work, we follow a recent\ntrend of in-depth behavioral analyses of neural network models that go beyond\naccuracy as an evaluation metric by looking at patterns of errors. Our focus is\non comparing a suite of standard Convolutional Neural Networks (CNNs) and a\nrecently-proposed attention-based network, the Vision Transformer (ViT), which\nrelaxes the translation-invariance constraint of CNNs and therefore represents\na model with a weaker set of inductive biases. Attention-based networks have\npreviously been shown to achieve higher accuracy than CNNs on vision tasks, and\nwe demonstrate, using new metrics for examining error consistency with more\ngranularity, that their errors are also more consistent with those of humans.\nThese results have implications both for building more human-like vision\nmodels, as well as for understanding visual object recognition in humans.\n", "rewritten_text": "While modern computer vision models, particularly on benchmarks like ImageNet, surpass human accuracy in specific visual recognition tasks,  performance isn't solely defined by accuracy.  Model inductive biases, often difficult to define, significantly influence the resulting decision-making process.  This study analyzes the error patterns of various Convolutional Neural Networks (CNNs) and the Vision Transformer (ViT), an attention-based network less constrained by translation invariance than CNNs.  Using novel, fine-grained error metrics, we show that, in addition to achieving higher accuracy, ViTs exhibit error patterns more consistent with human error.  These findings offer insights into creating more human-like vision systems and furthering our understanding of human visual object recognition.\n"}, "1902.01955": {"original_text": "  In conventional speech recognition, phoneme-based models outperform\ngrapheme-based models for non-phonetic languages such as English. The\nperformance gap between the two typically reduces as the amount of training\ndata is increased. In this work, we examine the impact of the choice of\nmodeling unit for attention-based encoder-decoder models. We conduct\nexperiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, using various\ntarget units (phoneme, grapheme, and word-piece); across all tasks, we find\nthat grapheme or word-piece models consistently outperform phoneme-based\nmodels, even though they are evaluated without a lexicon or an external\nlanguage model. We also investigate model complementarity: we find that we can\nimprove WERs by up to 9% relative by rescoring N-best lists generated from a\nstrong word-piece based baseline with either the phoneme or the grapheme model.\nRescoring an N-best list generated by the phonemic system, however, provides\nlimited improvements. Further analysis shows that the word-piece-based models\nproduce more diverse N-best hypotheses, and thus lower oracle WERs, than\nphonemic models.\n", "rewritten_text": "While phoneme-based models traditionally outperform grapheme-based models in speech recognition for languages like English, this advantage diminishes with increased training data.  This study investigates the impact of different modeling units (phonemes, graphemes, and word-pieces) in attention-based encoder-decoder models using the LibriSpeech corpus (100hr, 460hr, and 960hr).  Contrary to conventional wisdom, grapheme and word-piece models consistently outperformed phoneme models across all datasets, even without lexicons or external language models.  Furthermore, rescoring N-best lists from a strong word-piece model with either phoneme or grapheme models yielded up to a 9% relative WER reduction.  This improvement was not observed when rescoring from a phoneme-based model.  Analysis revealed that word-piece models generate more diverse N-best lists, leading to lower oracle WERs.\n"}, "2012.12482": {"original_text": "  We address the problem of crowd localization, i.e., the prediction of dots\ncorresponding to people in a crowded scene. Due to various challenges, a\nlocalization method is prone to spatial semantic errors, i.e., predicting\nmultiple dots within a same person or collapsing multiple dots in a cluttered\nregion. We propose a topological approach targeting these semantic errors. We\nintroduce a topological constraint that teaches the model to reason about the\nspatial arrangement of dots. To enforce this constraint, we define a\npersistence loss based on the theory of persistent homology. The loss compares\nthe topographic landscape of the likelihood map and the topology of the ground\ntruth. Topological reasoning improves the quality of the localization algorithm\nespecially near cluttered regions. On multiple public benchmarks, our method\noutperforms previous localization methods. Additionally, we demonstrate the\npotential of our method in improving the performance in the crowd counting\ntask.\n", "rewritten_text": "This paper tackles the challenge of accurately localizing individuals in crowded scenes, a task prone to errors like miscounting (multiple dots for one person or vice-versa).  We introduce a novel topological approach that addresses these errors by incorporating a spatial arrangement constraint.  This constraint, enforced via a persistence loss function based on persistent homology, compares the predicted likelihood map's topology to the ground truth.  Our method significantly improves localization accuracy, particularly in dense areas, outperforming existing techniques on multiple public datasets.  Furthermore, we show its effectiveness in enhancing crowd counting performance.\n"}, "2303.02982": {"original_text": "  Learning from large-scale contrastive language-image pre-training like CLIP\nhas shown remarkable success in a wide range of downstream tasks recently, but\nit is still under-explored on the challenging few-shot action recognition\n(FSAR) task. In this work, we aim to transfer the powerful multimodal knowledge\nof CLIP to alleviate the inaccurate prototype estimation issue due to data\nscarcity, which is a critical problem in low-shot regimes. To this end, we\npresent a CLIP-guided prototype modulating framework called CLIP-FSAR, which\nconsists of two key components: a video-text contrastive objective and a\nprototype modulation. Specifically, the former bridges the task discrepancy\nbetween CLIP and the few-shot video task by contrasting videos and\ncorresponding class text descriptions. The latter leverages the transferable\ntextual concepts from CLIP to adaptively refine visual prototypes with a\ntemporal Transformer. By this means, CLIP-FSAR can take full advantage of the\nrich semantic priors in CLIP to obtain reliable prototypes and achieve accurate\nfew-shot classification. Extensive experiments on five commonly used benchmarks\ndemonstrate the effectiveness of our proposed method, and CLIP-FSAR\nsignificantly outperforms existing state-of-the-art methods under various\nsettings. The source code and models will be publicly available at\nhttps://github.com/alibaba-mmai-research/CLIP-FSAR.\n", "rewritten_text": "While large-scale contrastive language-image models like CLIP excel at many tasks, their application to the challenging few-shot action recognition (FSAR) problem remains limited.  This paper introduces CLIP-FSAR, a novel framework that leverages CLIP's multimodal knowledge to address the inaccurate prototype estimation inherent in FSAR's data scarcity.  CLIP-FSAR uses a video-text contrastive objective to bridge the gap between CLIP and the FSAR task, and a prototype modulation component, employing a temporal Transformer, to refine visual prototypes using CLIP's transferable textual concepts.  Extensive experiments on five benchmark datasets demonstrate CLIP-FSAR's superior performance compared to existing state-of-the-art methods.  The code and models are publicly available at https://github.com/alibaba-mmai-research/CLIP-FSAR.\n"}, "1905.07826": {"original_text": "  Multi-instance video object segmentation is to segment specific instances\nthroughout a video sequence in pixel level, given only an annotated first\nframe. In this paper, we implement an effective fully convolutional networks\nwith U-Net similar structure built on top of OSVOS fine-tuned layer. We use\ninstance isolation to transform this multi-instance segmentation problem into\nbinary labeling problem, and use weighted cross entropy loss and dice\ncoefficient loss as our loss function. Our best model achieves F mean of 0.467\nand J mean of 0.424 on DAVIS dataset, which is a comparable performance with\nthe State-of-the-Art approach. But case analysis shows this model can achieve a\nsmoother contour and better instance coverage, meaning it better for recall\nfocused segmentation scenario. We also did experiments on other convolutional\nneural networks, including Seg-Net, Mask R-CNN, and provide insightful\ncomparison and discussion.\n", "rewritten_text": "This paper presents a fully convolutional network, based on a U-Net architecture and leveraging fine-tuned OSVOS layers, for multi-instance video object segmentation.  By employing instance isolation to convert the problem into a series of binary segmentation tasks, and utilizing a weighted cross-entropy and Dice coefficient loss function, our model achieves a mean F-measure of 0.467 and J-measure of 0.424 on the DAVIS dataset \u2013 competitive with state-of-the-art methods.  Qualitative analysis reveals superior contour smoothness and instance coverage, making it particularly suitable for recall-oriented segmentation.  Comparative experiments with SegNet and Mask R-CNN are also included and discussed.\n"}, "2003.14282": {"original_text": "  A wide variety of transition-based algorithms are currently used for\ndependency parsers. Empirical studies have shown that performance varies across\ndifferent treebanks in such a way that one algorithm outperforms another on one\ntreebank and the reverse is true for a different treebank. There is often no\ndiscernible reason for what causes one algorithm to be more suitable for a\ncertain treebank and less so for another. In this paper we shed some light on\nthis by introducing the concept of an algorithm's inherent dependency\ndisplacement distribution. This characterises the bias of the algorithm in\nterms of dependency displacement, which quantify both distance and direction of\nsyntactic relations. We show that the similarity of an algorithm's inherent\ndistribution to a treebank's displacement distribution is clearly correlated to\nthe algorithm's parsing performance on that treebank, specifically with highly\nsignificant and substantial correlations for the predominant sentence lengths\nin Universal Dependency treebanks. We also obtain results which show a more\ndiscrete analysis of dependency displacement does not result in any meaningful\ncorrelations.\n", "rewritten_text": "Dependency parsing commonly employs various transition-based algorithms, whose performance varies significantly across different treebanks.  No clear reason explains this inconsistency. This paper investigates this by introducing the concept of an algorithm's inherent dependency displacement distribution, characterizing its bias in terms of dependency distance and direction.  We demonstrate a strong correlation between an algorithm's performance on a treebank and the similarity between its inherent distribution and the treebank's displacement distribution, particularly for common sentence lengths in Universal Dependency treebanks.  Conversely, a more granular analysis of dependency displacement yielded no significant correlations.\n"}, "2404.11682": {"original_text": "  Automated methods are becoming increasingly integrated into studies of\nformative feedback on students' science explanation writing. Most of this work,\nhowever, addresses students' responses to short answer questions. We\ninvestigate automated feedback on students' science explanation essays, where\nstudents must articulate multiple ideas. Feedback is based on a rubric that\nidentifies the main ideas students are prompted to include in explanatory\nessays about the physics of energy and mass, given their experiments with a\nsimulated roller coaster. We have found that students generally improve on\nrevised versions of their essays. Here, however, we focus on two factors that\naffect the accuracy of the automated feedback. First, we find that the main\nideas in the rubric differ with respect to how much freedom they afford in\nexplanations of the idea, thus explanation of a natural law is relatively\nconstrained. Students have more freedom in how they explain complex relations\nthey observe in their roller coasters, such as transfer of different forms of\nenergy. Second, by tracing the automated decision process, we can diagnose when\na student's statement lacks sufficient clarity for the automated tool to\nassociate it more strongly with one of the main ideas above all others. This in\nturn provides an opportunity for teachers and peers to help students reflect on\nhow to state their ideas more clearly.\n", "rewritten_text": "Automated feedback on student science writing is increasingly common, but most research focuses on short answers.  This study examines automated feedback on longer, multi-idea science explanation essays about energy and mass in a simulated roller coaster context.  While students generally improved their essays after receiving automated feedback, we analyze two factors affecting feedback accuracy:  the varying degrees of explanatory freedom afforded by different rubric criteria (e.g., explaining natural laws is more constrained than explaining observed energy transfers), and the system's difficulty in interpreting unclear student statements.  Analyzing the automated feedback process reveals opportunities for teachers and peers to support students in clarifying their explanations.\n"}, "2211.08462": {"original_text": "  Recent years have seen an increasing trend in the volume of personal media\ncaptured by users, thanks to the advent of smartphones and smart glasses,\nresulting in large media collections. Despite conversation being an intuitive\nhuman-computer interface, current efforts focus mostly on single-shot natural\nlanguage based media retrieval to aid users query their media and re-live their\nmemories. This severely limits the search functionality as users can neither\nask follow-up queries nor obtain information without first formulating a\nsingle-turn query.\n  In this work, we propose dialogs for connected memories as a powerful tool to\nempower users to search their media collection through a multi-turn,\ninteractive conversation. Towards this, we collect a new task-oriented dialog\ndataset COMET, which contains $11.5k$ user<->assistant dialogs (totaling $103k$\nutterances), grounded in simulated personal memory graphs. We employ a\nresource-efficient, two-phase data collection pipeline that uses: (1) a novel\nmultimodal dialog simulator that generates synthetic dialog flows grounded in\nmemory graphs, and, (2) manual paraphrasing to obtain natural language\nutterances. We analyze COMET, formulate four main tasks to benchmark meaningful\nprogress, and adopt state-of-the-art language models as strong baselines, in\norder to highlight the multimodal challenges captured by our dataset.\n", "rewritten_text": "The proliferation of smartphones and smart glasses has led to a surge in personal media, creating massive collections.  While conversational interfaces are intuitive, current media retrieval systems rely on single-turn natural language queries, limiting search capabilities.  This paper introduces dialog-based media search, enabling multi-turn, interactive exploration of personal memories.  To facilitate research, we present COMET, a new dataset comprising 11.5k user-assistant dialogs (103k utterances) grounded in simulated memory graphs.  COMET was created using a two-phase pipeline: a novel multimodal dialog simulator generating synthetic conversations, followed by manual paraphrasing for natural language.  We define four benchmark tasks to evaluate progress on this challenging multimodal problem, using state-of-the-art language models as baselines.\n"}, "1911.03668": {"original_text": "  Natural Language Inference (NLI) aims to determine the logic relationships\n(i.e., entailment, neutral and contradiction) between a pair of premise and\nhypothesis. Recently, the alignment mechanism effectively helps NLI by\ncapturing the aligned parts (i.e., the similar segments) in the sentence pairs,\nwhich imply the perspective of entailment and contradiction. However, these\naligned parts will sometimes mislead the judgment of neutral relations.\nIntuitively, NLI should rely more on multiple perspectives to form a holistic\nview to eliminate bias. In this paper, we propose the Multi-Perspective\nInferrer (MPI), a novel NLI model that reasons relationships from multiple\nperspectives associated with the three relationships. The MPI determines the\nperspectives of different parts of the sentences via a routing-by-agreement\npolicy and makes the final decision from a holistic view. Additionally, we\nintroduce an auxiliary supervised signal to ensure the MPI to learn the\nexpected perspectives. Experiments on SNLI and MultiNLI show that 1) the MPI\nachieves substantial improvements on the base model, which verifies the\nmotivation of multi-perspective inference; 2) visualized evidence verifies that\nthe MPI learns highly interpretable perspectives as expected; 3) more\nimportantly, the MPI is architecture-free and compatible with the powerful\nBERT.\n", "rewritten_text": "Natural Language Inference (NLI) models determine the logical relationship (entailment, neutral, or contradiction) between a premise and hypothesis.  While alignment mechanisms improve NLI by identifying similar sentence segments, these can bias the model towards incorrect neutral classifications.  To address this, we introduce the Multi-Perspective Inferrer (MPI), a novel NLI model that considers multiple perspectives associated with each relationship type.  MPI uses a routing-by-agreement policy to identify the perspective of different sentence parts and integrates these perspectives for a holistic judgment.  An auxiliary supervised signal further guides perspective learning.  Experiments on SNLI and MultiNLI demonstrate significant improvements over a baseline model, confirming the value of multi-perspective inference, highly interpretable learned perspectives, and architecture-agnostic compatibility with powerful models like BERT.\n"}, "2403.04212": {"original_text": "  Providing emotional support through dialogue systems is becoming increasingly\nimportant in today's world, as it can support both mental health and social\ninteractions in many conversation scenarios. Previous works have shown that\nusing persona is effective for generating empathetic and supportive responses.\nThey have often relied on pre-provided persona rather than inferring them\nduring conversations. However, it is not always possible to obtain a user\npersona before the conversation begins. To address this challenge, we propose\nPESS (Persona Extraction through Semantic Similarity), a novel framework that\ncan automatically infer informative and consistent persona from dialogues. We\ndevise completeness loss and consistency loss based on semantic similarity\nscores. The completeness loss encourages the model to generate missing persona\ninformation, and the consistency loss guides the model to distinguish between\nconsistent and inconsistent persona. Our experimental results demonstrate that\nhigh-quality persona information inferred by PESS is effective in generating\nemotionally supportive responses.\n", "rewritten_text": "The growing need for emotional support in digital interactions necessitates the development of empathetic dialogue systems. While prior research has demonstrated the effectiveness of personas in generating supportive responses, these approaches typically rely on pre-defined user profiles.  To overcome this limitation, we introduce PESS (Persona Extraction through Semantic Similarity), a novel framework for automatically inferring consistent and informative user personas directly from conversation.  PESS leverages completeness and consistency losses, based on semantic similarity, to ensure the generated persona is both comprehensive and internally coherent.  Our experiments show that PESS effectively infers high-quality personas, leading to significantly improved generation of emotionally supportive responses.\n"}, "1810.13391": {"original_text": "  During natural disasters and conflicts, information about what happened is\noften confusing, messy, and distributed across many sources. We would like to\nbe able to automatically identify relevant information and assemble it into\ncoherent narratives of what happened. To make this task accessible to neural\nmodels, we introduce Story Salads, mixtures of multiple documents that can be\ngenerated at scale. By exploiting the Wikipedia hierarchy, we can generate\nsalads that exhibit challenging inference problems. Story salads give rise to a\nnovel, challenging clustering task, where the objective is to group sentences\nfrom the same narratives. We demonstrate that simple bag-of-words similarity\nclustering falls short on this task and that it is necessary to take into\naccount global context and coherence.\n", "rewritten_text": "Natural disasters and conflicts generate fragmented and unreliable information spread across numerous sources.  To enable automated synthesis of coherent event narratives from this chaotic data, we introduce Story Salads: large-scale datasets of mixed documents.  Leveraging Wikipedia's hierarchical structure, we create salads presenting complex inference challenges.  This yields a novel clustering task\u2014grouping sentences from the same narrative\u2014where simple bag-of-words methods prove inadequate, highlighting the need for context-aware and coherent approaches.\n"}, "2407.02047": {"original_text": "  Multi-view counting (MVC) methods have shown their superiority over\nsingle-view counterparts, particularly in situations characterized by heavy\nocclusion and severe perspective distortions. However, hand-crafted heuristic\nfeatures and identical camera layout requirements in conventional MVC methods\nlimit their applicability and scalability in real-world scenarios.In this work,\nwe propose a concise 3D MVC framework called \\textbf{CountFormer}to elevate\nmulti-view image-level features to a scene-level volume representation and\nestimate the 3D density map based on the volume features. By incorporating a\ncamera encoding strategy, CountFormer successfully embeds camera parameters\ninto the volume query and image-level features, enabling it to handle various\ncamera layouts with significant differences.Furthermore, we introduce a feature\nlifting module capitalized on the attention mechanism to transform image-level\nfeatures into a 3D volume representation for each camera view. Subsequently,\nthe multi-view volume aggregation module attentively aggregates various\nmulti-view volumes to create a comprehensive scene-level volume representation,\nallowing CountFormer to handle images captured by arbitrary dynamic camera\nlayouts. The proposed method performs favorably against the state-of-the-art\napproaches across various widely used datasets, demonstrating its greater\nsuitability for real-world deployment compared to conventional MVC frameworks.\n", "rewritten_text": "Multi-view counting (MVC) excels over single-view methods, especially when dealing with occlusion and perspective distortion.  However, traditional MVC's reliance on hand-crafted features and fixed camera setups limits their real-world applicability.  This paper introduces CountFormer, a novel 3D MVC framework that addresses these limitations.  CountFormer elevates image-level features to a 3D scene-level volume representation, estimating a 3D density map.  A novel camera encoding strategy allows it to handle diverse camera configurations.  A feature lifting module, leveraging attention mechanisms, transforms image-level features into 3D volumes for each view, which are then aggregated into a comprehensive scene representation.  CountFormer outperforms state-of-the-art methods on various datasets, demonstrating its superior robustness and suitability for real-world deployment.\n"}, "2312.07530": {"original_text": "  Weakly supervised 3D object detection aims to learn a 3D detector with lower\nannotation cost, e.g., 2D labels. Unlike prior work which still relies on few\naccurate 3D annotations, we propose a framework to study how to leverage\nconstraints between 2D and 3D domains without requiring any 3D labels.\nSpecifically, we employ visual data from three perspectives to establish\nconnections between 2D and 3D domains. First, we design a feature-level\nconstraint to align LiDAR and image features based on object-aware regions.\nSecond, the output-level constraint is developed to enforce the overlap between\n2D and projected 3D box estimations. Finally, the training-level constraint is\nutilized by producing accurate and consistent 3D pseudo-labels that align with\nthe visual data. We conduct extensive experiments on the KITTI dataset to\nvalidate the effectiveness of the proposed three constraints. Without using any\n3D labels, our method achieves favorable performance against state-of-the-art\napproaches and is competitive with the method that uses 500-frame 3D\nannotations. Code will be made publicly available at\nhttps://github.com/kuanchihhuang/VG-W3D.\n", "rewritten_text": "This paper introduces a novel weakly supervised 3D object detection framework that eliminates the need for 3D annotations.  Unlike existing methods relying on scarce 3D ground truth data, our approach leverages constraints between 2D and 3D data using multi-view imagery (three perspectives).  These constraints operate at three levels: feature alignment between LiDAR and image features within object regions, consistency between 2D and projected 3D bounding boxes, and generation of accurate 3D pseudo-labels.  Extensive experiments on the KITTI dataset demonstrate that our label-free method achieves state-of-the-art performance, rivaling methods trained with 500 frames of 3D annotations.  The code is publicly available at https://github.com/kuanchihhuang/VG-W3D.\n"}, "2404.09976": {"original_text": "  Recently, diffusion transformers have gained wide attention with its\nexcellent performance in text-to-image and text-to-vidoe models, emphasizing\nthe need for transformers as backbone for diffusion models. Transformer-based\nmodels have shown better generalization capability compared to CNN-based models\nfor general vision tasks. However, much less has been explored in the existing\nliterature regarding the capabilities of transformer-based diffusion backbones\nand expanding their generative prowess to other datasets. This paper focuses on\nenabling a single pre-trained diffusion transformer model to scale across\nmultiple datasets swiftly, allowing for the completion of diverse generative\ntasks using just one model. To this end, we propose DiffScaler, an efficient\nscaling strategy for diffusion models where we train a minimal amount of\nparameters to adapt to different tasks. In particular, we learn task-specific\ntransformations at each layer by incorporating the ability to utilize the\nlearned subspaces of the pre-trained model, as well as the ability to learn\nadditional task-specific subspaces, which may be absent in the pre-training\ndataset. As these parameters are independent, a single diffusion model with\nthese task-specific parameters can be used to perform multiple tasks\nsimultaneously. Moreover, we find that transformer-based diffusion models\nsignificantly outperform CNN-based diffusion models methods while performing\nfine-tuning over smaller datasets. We perform experiments on four unconditional\nimage generation datasets. We show that using our proposed method, a single\npre-trained model can scale up to perform these conditional and unconditional\ntasks, respectively, with minimal parameter tuning while performing as close as\nfine-tuning an entire diffusion model for that particular task.\n", "rewritten_text": "Diffusion transformers have demonstrated superior performance in text-to-image and text-to-video generation, highlighting their potential as the backbone for diffusion models. While transformers generally outperform CNNs in vision tasks, their application within diffusion models remains under-explored.  This paper introduces DiffScaler, a novel scaling strategy enabling a single pre-trained diffusion transformer to efficiently adapt to diverse datasets and generative tasks.  DiffScaler minimizes parameter training by learning task-specific transformations at each layer, leveraging existing subspaces within the pre-trained model and learning new ones as needed. This independent parameterization allows a single model to handle multiple tasks concurrently.  Experiments on four unconditional image generation datasets demonstrate that DiffScaler significantly outperforms CNN-based methods, achieving near fine-tuning performance with minimal parameter adjustments, enabling a single pre-trained model to effectively handle both conditional and unconditional tasks.\n"}, "2406.06843": {"original_text": "  We introduce a data capture system and a new dataset named HO-Cap that can be\nused to study 3D reconstruction and pose tracking of hands and objects in\nvideos. The capture system uses multiple RGB-D cameras and a HoloLens headset\nfor data collection, avoiding the use of expensive 3D scanners or mocap\nsystems. We propose a semi-automatic method to obtain annotations of shape and\npose of hands and objects in the collected videos, which significantly reduces\nthe required annotation time compared to manual labeling. With this system, we\ncaptured a video dataset of humans using objects to perform different tasks, as\nwell as simple pick-and-place and handover of an object from one hand to the\nother, which can be used as human demonstrations for embodied AI and robot\nmanipulation research. Our data capture setup and annotation framework can be\nused by the community to reconstruct 3D shapes of objects and human hands and\ntrack their poses in videos.\n", "rewritten_text": "We present HO-Cap, a novel dataset and accompanying data capture system for researching 3D hand and object reconstruction and pose tracking in video.  Utilizing multiple RGB-D cameras and a HoloLens, our cost-effective system avoids the need for expensive 3D scanners or motion capture equipment.  A semi-automatic annotation method significantly accelerates the labeling process, generating annotations of hand and object shape and pose.  HO-Cap features videos of humans performing various tasks, including pick-and-place and handovers, providing valuable human demonstrations for embodied AI and robotics.  This readily accessible system and annotation framework empowers researchers to reconstruct 3D shapes and track poses of hands and objects within video data.\n"}, "2407.05769": {"original_text": "  In autonomous driving, LiDAR sensors are vital for acquiring 3D point clouds,\nproviding reliable geometric information. However, traditional sampling methods\nof preprocessing often ignore semantic features, leading to detail loss and\nground point interference in 3D object detection. To address this, we propose a\nmulti-branch two-stage 3D object detection framework using a Semantic-aware\nMulti-branch Sampling (SMS) module and multi-view consistency constraints. The\nSMS module includes random sampling, Density Equalization Sampling (DES) for\nenhancing distant objects, and Ground Abandonment Sampling (GAS) to focus on\nnon-ground points. The sampled multi-view points are processed through a\nConsistent KeyPoint Selection (CKPS) module to generate consistent keypoint\nmasks for efficient proposal sampling. The first-stage detector uses\nmulti-branch parallel learning with multi-view consistency loss for feature\naggregation, while the second-stage detector fuses multi-view data through a\nMulti-View Fusion Pooling (MVFP) module to precisely predict 3D objects. The\nexperimental results on the KITTI dataset and Waymo Open Dataset show that our\nmethod achieves excellent detection performance improvement for a variety of\nbackbones, especially for low-performance backbones with the simple network\nstructures.\n", "rewritten_text": "This paper introduces a novel two-stage 3D object detection framework for autonomous driving that leverages semantic information to improve LiDAR point cloud processing.  Addressing the limitations of traditional sampling methods, our approach employs a Semantic-aware Multi-branch Sampling (SMS) module incorporating random, density-equalized, and ground-abandonment sampling strategies.  A Consistent KeyPoint Selection (CKPS) module then ensures multi-view consistency.  The framework uses multi-branch parallel learning with multi-view consistency loss in the first stage and a Multi-View Fusion Pooling (MVFP) module in the second stage for precise 3D object prediction.  Experiments on KITTI and Waymo Open Datasets demonstrate significant performance gains, particularly for simpler, less powerful backbones.\n"}, "2211.0089": {"original_text": "  Few-shot learning problem focuses on recognizing unseen classes given a few\nlabeled images. In recent effort, more attention is paid to fine-grained\nfeature embedding, ignoring the relationship among different distance metrics.\nIn this paper, for the first time, we investigate the contributions of\ndifferent distance metrics, and propose an adaptive fusion scheme, bringing\nsignificant improvements in few-shot classification. We start from a naive\nbaseline of confidence summation and demonstrate the necessity of exploiting\nthe complementary property of different distance metrics. By finding the\ncompetition problem among them, built upon the baseline, we propose an Adaptive\nMetrics Module (AMM) to decouple metrics fusion into metric-prediction fusion\nand metric-losses fusion. The former encourages mutual complementary, while the\nlatter alleviates metric competition via multi-task collaborative learning.\nBased on AMM, we design a few-shot classification framework AMTNet, including\nthe AMM and the Global Adaptive Loss (GAL), to jointly optimize the few-shot\ntask and auxiliary self-supervised task, making the embedding features more\nrobust. In the experiment, the proposed AMM achieves 2% higher performance than\nthe naive metrics fusion module, and our AMTNet outperforms the\nstate-of-the-arts on multiple benchmark datasets.\n", "rewritten_text": "This paper addresses the challenge of few-shot learning, where accurate classification requires recognizing new classes from limited labeled examples.  While recent work emphasizes fine-grained feature embedding, this research uniquely investigates the impact of different distance metrics.  We introduce an Adaptive Metrics Module (AMM) that adaptively fuses these metrics, significantly improving few-shot classification accuracy.  Starting with a simple confidence summation baseline, we reveal the complementary nature and competitive interactions of various distance metrics.  AMM decouples metric fusion into prediction and loss fusion stages, promoting complementarity and mitigating competition through multi-task learning.  Integrated into our AMTNet framework, which also incorporates a Global Adaptive Loss (GAL) and a self-supervised auxiliary task, AMM achieves a 2% performance gain over naive fusion and establishes state-of-the-art results on multiple benchmark datasets.\n"}, "2407.13520": {"original_text": "  3D deblurring reconstruction techniques have recently seen significant\nadvancements with the development of Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). Although these techniques can recover relatively\nclear 3D reconstructions from blurry image inputs, they still face limitations\nin handling severe blurring and complex camera motion. To address these issues,\nwe propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting\n(EaDeblur-GS), which integrates event camera data to enhance the robustness of\n3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE)\nnetwork to estimate Gaussian center deviations and using novel loss functions,\nEaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating\nperformance comparable to state-of-the-art methods.\n", "rewritten_text": "Recent advancements in 3D deblurring, particularly Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have yielded improved 3D reconstructions from blurry images.  However, these methods struggle with extreme blur and complex camera motion.  To overcome these limitations, we introduce Event-assisted 3D Deblur Reconstruction with Gaussian Splatting (EaDeblur-GS).  EaDeblur-GS leverages event camera data to enhance 3DGS's motion blur robustness.  Through an Adaptive Deviation Estimator (ADE) network and novel loss functions, it achieves real-time, high-quality 3D reconstructions comparable to the best existing techniques.\n"}, "2103.13678": {"original_text": "  Domain Adaptation is widely used in practical applications of neural machine\ntranslation, which aims to achieve good performance on both the general-domain\nand in-domain. However, the existing methods for domain adaptation usually\nsuffer from catastrophic forgetting, domain divergence, and model explosion. To\naddress these three problems, we propose a method of \"divide and conquer\" which\nis based on the importance of neurons or parameters in the translation model.\nIn our method, we first prune the model and only keep the important neurons or\nparameters, making them responsible for both general-domain and in-domain\ntranslation. Then we further train the pruned model supervised by the original\nunpruned model with the knowledge distillation method. Last we expand the model\nto the original size and fine-tune the added parameters for the in-domain\ntranslation. We conduct experiments on different languages and domains and the\nresults show that our method can achieve significant improvements compared with\nseveral strong baselines.\n", "rewritten_text": "Neural machine translation often employs domain adaptation to handle both general and specific domains.  However, existing methods struggle with catastrophic forgetting, domain divergence, and model complexity.  We address these issues with a novel \"divide and conquer\" approach.  This involves pruning the model to retain only crucial neurons/parameters for both general and specific domain translation, then using knowledge distillation from the original model to guide further training. Finally, we expand the model to its original size and fine-tune the added parameters specifically for the target domain.  Experiments across multiple languages and domains demonstrate significant performance gains over state-of-the-art baselines.\n"}, "2003.11536": {"original_text": "  Estimating the actual head orientation from 2D images, with regard to its\nthree degrees of freedom, is a well known problem that is highly significant\nfor a large number of applications involving head pose knowledge. Consequently,\nthis topic has been tackled by a plethora of methods and algorithms the most\npart of which exploits neural networks. Machine learning methods, indeed,\nachieve accurate head rotation values yet require an adequate training stage\nand, to that aim, a relevant number of positive and negative examples. In this\npaper we take a different approach to this topic by using fractal coding theory\nand particularly Partitioned Iterated Function Systems to extract the fractal\ncode from the input head image and to compare this representation to the\nfractal code of a reference model through Hamming distance. According to\nexperiments conducted on both the BIWI and the AFLW2000 databases, the proposed\nPIFS based head pose estimation method provides accurate yaw/pitch/roll angular\nvalues, with a performance approaching that of state of the art of\nmachine-learning based algorithms and exceeding most of non-training based\napproaches.\n", "rewritten_text": "Accurate 3D head orientation estimation from 2D images is crucial for many applications. While deep learning methods dominate this field, achieving high accuracy but requiring extensive training data, this paper proposes a novel approach.  We leverage fractal coding theory, specifically Partitioned Iterated Function Systems (PIFS), to represent head images.  By comparing the PIFS code of an input image to a reference model using Hamming distance, we estimate yaw, pitch, and roll angles.  Experiments on BIWI and AFLW2000 datasets demonstrate that our PIFS-based method achieves accuracy comparable to state-of-the-art machine learning methods and surpasses most non-training-based alternatives.\n"}, "1706.02241": {"original_text": "  Analogy completion has been a popular task in recent years for evaluating the\nsemantic properties of word embeddings, but the standard methodology makes a\nnumber of assumptions about analogies that do not always hold, either in recent\nbenchmark datasets or when expanding into other domains. Through an analysis of\nanalogies in the biomedical domain, we identify three assumptions: that of a\nSingle Answer for any given analogy, that the pairs involved describe the Same\nRelationship, and that each pair is Informative with respect to the other. We\npropose modifying the standard methodology to relax these assumptions by\nallowing for multiple correct answers, reporting MAP and MRR in addition to\naccuracy, and using multiple example pairs. We further present BMASS, a novel\ndataset for evaluating linguistic regularities in biomedical embeddings, and\ndemonstrate that the relationships described in the dataset pose significant\nsemantic challenges to current word embedding methods.\n", "rewritten_text": "While analogy completion is a widely used benchmark for evaluating word embeddings, its standard methodology rests on unrealistic assumptions.  Our analysis of biomedical analogies reveals three key limitations: the assumption of a single correct answer, the assumption of identical relationships between analogy pairs, and the assumption that each pair is equally informative.  To address these, we propose a revised methodology allowing multiple correct answers, employing metrics beyond accuracy (like MAP and MRR), and utilizing multiple example pairs.  We introduce BMASS, a new biomedical analogy dataset, which highlights the significant challenges current word embedding models face in capturing complex biomedical relationships.\n"}, "2312.16894": {"original_text": "  The widespread usage of cars and other large, heavy vehicles necessitates the\ndevelopment of an effective parking infrastructure. Additionally, algorithms\nfor detection and recognition of number plates are often used to identify\nautomobiles all around the world where standardized plate sizes and fonts are\nenforced, making recognition an effortless task. As a result, both kinds of\ndata can be combined to develop an intelligent parking system focuses on the\ntechnology of Automatic Number Plate Recognition (ANPR). Retrieving characters\nfrom an inputted number plate image is the sole purpose of ANPR which is a\ncostly procedure. In this article, we propose Chaurah, a minimal cost ANPR\nsystem that relies on a Raspberry Pi 3 that was specifically created for\nparking facilities. The system employs a dual-stage methodology, with the first\nstage being an ANPR system which makes use of two convolutional neural networks\n(CNNs). The primary locates and recognises license plates from a vehicle image,\nwhile the secondary performs Optical Character Recognition (OCR) to identify\nindividualized numbers from the number plate. An application built with Flutter\nand Firebase for database administration and license plate record comparison\nmakes up the second component of the overall solution. The application also\nacts as an user-interface for the billing mechanism based on parking time\nduration resulting in an all-encompassing software deployment of the study.\n", "rewritten_text": "The proliferation of vehicles necessitates robust parking infrastructure.  While standardized license plates simplify automated number plate recognition (ANPR) globally,  existing ANPR systems are costly. This paper introduces Chaurah, a low-cost ANPR-based intelligent parking system utilizing a Raspberry Pi 3.  Chaurah employs a two-stage CNN-based approach:  the first stage locates and recognizes license plates, and the second performs optical character recognition (OCR).  A Flutter/Firebase application manages the database, compares license plate records, handles billing based on parking duration, and provides a user interface, completing the system's functionality.\n"}, "2110.13032": {"original_text": "  Though there has been a large body of recent works in language modeling (LM)\nfor high resource languages such as English and Chinese, the area is still\nunexplored for low resource languages like Bengali and Hindi. We propose an end\nto end trainable memory efficient CNN architecture named CoCNN to handle\nspecific characteristics such as high inflection, morphological richness,\nflexible word order and phonetical spelling errors of Bengali and Hindi. In\nparticular, we introduce two learnable convolutional sub-models at word and at\nsentence level that are end to end trainable. We show that state-of-the-art\n(SOTA) Transformer models including pretrained BERT do not necessarily yield\nthe best performance for Bengali and Hindi. CoCNN outperforms pretrained BERT\nwith 16X less parameters, and it achieves much better performance than SOTA\nLSTM models on multiple real-world datasets. This is the first study on the\neffectiveness of different architectures drawn from three deep learning\nparadigms - Convolution, Recurrent, and Transformer neural nets for modeling\ntwo widely used languages, Bengali and Hindi.\n", "rewritten_text": "While language modeling has advanced significantly for high-resource languages like English and Chinese, low-resource languages such as Bengali and Hindi remain under-researched.  This paper introduces CoCNN, a novel, memory-efficient, end-to-end trainable convolutional neural network (CNN) architecture designed to address the unique linguistic challenges of Bengali and Hindi (high inflection, rich morphology, flexible word order, and phonetic spelling errors).  CoCNN incorporates two learnable convolutional sub-models operating at the word and sentence levels.  Our results demonstrate that, contrary to expectations, state-of-the-art Transformer models, including pre-trained BERT, do not consistently achieve optimal performance for these languages.  CoCNN surpasses pre-trained BERT with 16 times fewer parameters and significantly outperforms existing LSTM-based state-of-the-art models on multiple real-world datasets. This work represents the first comprehensive comparison of convolutional, recurrent, and transformer architectures for language modeling in Bengali and Hindi.\n"}, "1305.4537": {"original_text": "  We describe a method for visual object detection based on an ensemble of\noptimized decision trees organized in a cascade of rejectors. The trees use\npixel intensity comparisons in their internal nodes and this makes them able to\nprocess image regions very fast. Experimental analysis is provided through a\nface detection problem. The obtained results are encouraging and demonstrate\nthat the method has practical value. Additionally, we analyse its sensitivity\nto noise and show how to perform fast rotation invariant object detection.\nComplete source code is provided at https://github.com/nenadmarkus/pico.\n", "rewritten_text": "This paper presents a fast visual object detection method using a cascade of optimized decision tree ensembles.  These trees leverage pixel intensity comparisons for rapid processing of image regions.  We demonstrate its effectiveness and practical value through a face detection application, analyze its noise sensitivity, and show how to achieve rotation-invariant detection.  Source code is available at https://github.com/nenadmarkus/pico.\n"}, "2311.18835": {"original_text": "  Empowering models to dynamically accomplish tasks specified through natural\nlanguage instructions represents a promising path toward more capable and\ngeneral artificial intelligence. In this work, we introduce InstructSeq, an\ninstruction-conditioned multi-modal modeling framework that unifies diverse\nvision tasks through flexible natural language control and handling of both\nvisual and textual data. InstructSeq employs a multimodal transformer\narchitecture encompassing visual, language, and sequential modeling. We utilize\na visual encoder to extract image features and a text encoder to encode\ninstructions. An autoregressive transformer fuses the representations and\ngenerates sequential task outputs. By training with LLM-generated natural\nlanguage instructions, InstructSeq acquires a strong comprehension of free-form\ninstructions for specifying visual tasks. This provides an intuitive interface\nfor directing capabilities using flexible natural instructions. Without any\ntask-specific tuning, InstructSeq achieves compelling performance on semantic\nsegmentation, referring expression segmentation/comprehension, and image\ncaptioning. The flexible control and multi-task unification empower the model\nwith more human-like versatility and generalizability for computer vision. The\ncode will be released soon at https://github.com/rongyaofang/InstructSeq.\n", "rewritten_text": "InstructSeq is a new multimodal framework that uses natural language instructions to perform diverse computer vision tasks.  This instruction-conditioned model, built on a transformer architecture, processes both images and text to generate sequential outputs like image captions or segmentation masks.  Trained on large language model-generated instructions, InstructSeq understands free-form commands, enabling intuitive control without task-specific fine-tuning.  Demonstrating strong performance on semantic segmentation, referring expression comprehension, and image captioning, InstructSeq offers a more human-like, versatile, and generalizable approach to computer vision.  The code will be available soon at https://github.com/rongyaofang/InstructSeq.\n"}, "2008.07018": {"original_text": "  We present AutoPose, a novel neural architecture search(NAS) framework that\nis capable of automatically discovering multiple parallel branches of\ncross-scale connections towards accurate and high-resolution 2D human pose\nestimation. Recently, high-performance hand-crafted convolutional networks for\npose estimation show growing demands on multi-scale fusion and high-resolution\nrepresentations. However, current NAS works exhibit limited flexibility on\nscale searching, they dominantly adopt simplified search spaces of\nsingle-branch architectures. Such simplification limits the fusion of\ninformation at different scales and fails to maintain high-resolution\nrepresentations. The presentedAutoPose framework is able to search for\nmulti-branch scales and network depth, in addition to the cell-level\nmicrostructure. Motivated by the search space, a novel bi-level optimization\nmethod is presented, where the network-level architecture is searched via\nreinforcement learning, and the cell-level search is conducted by the\ngradient-based method. Within 2.5 GPU days, AutoPose is able to find very\ncompetitive architectures on the MS COCO dataset, that are also transferable to\nthe MPII dataset. Our code is available at\nhttps://github.com/VITA-Group/AutoPose.\n", "rewritten_text": "AutoPose is a new neural architecture search (NAS) framework that automatically designs efficient, high-resolution 2D human pose estimation models.  Unlike existing NAS methods limited to single-branch architectures, AutoPose discovers multiple parallel branches with cross-scale connections, addressing the need for multi-scale fusion in high-performing pose estimation networks.  Employing a novel bi-level optimization strategy (reinforcement learning for network architecture, gradient-based for cell-level details), AutoPose efficiently searches across multi-branch scales, network depth, and cell microstructure.  Within 2.5 GPU days, AutoPose achieves state-of-the-art results on the MS COCO dataset and demonstrates strong transferability to the MPII dataset.  The code is available at https://github.com/VITA-Group/AutoPose.\n"}, "1902.053": {"original_text": "  Deep learning, due to its unprecedented success in tasks such as image\nclassification, has emerged as a new tool in image reconstruction with\npotential to change the field. In this paper we demonstrate a crucial\nphenomenon: deep learning typically yields unstablemethods for image\nreconstruction. The instabilities usually occur in several forms: (1) tiny,\nalmost undetectable perturbations, both in the image and sampling domain, may\nresult in severe artefacts in the reconstruction, (2) a small structural\nchange, for example a tumour, may not be captured in the reconstructed image\nand (3) (a counterintuitive type of instability) more samples may yield poorer\nperformance. Our new stability test with algorithms and easy to use software\ndetects the instability phenomena. The test is aimed at researchers to test\ntheir networks for instabilities and for government agencies, such as the Food\nand Drug Administration (FDA), to secure safe use of deep learning methods.\n", "rewritten_text": "Deep learning's success in image classification has led to its adoption in image reconstruction, but this paper reveals a critical instability.  Deep learning-based reconstruction methods are highly susceptible to perturbations:  minor image or sampling variations can cause significant artifacts; small structural changes, like tumors, may be missed; and counterintuitively, more data can sometimes worsen results.  We introduce a novel stability test, accompanied by user-friendly software, to identify these instabilities. This test is designed for researchers to evaluate their deep learning models and for regulatory bodies, such as the FDA, to ensure the safe deployment of these methods.\n"}, "2203.08543": {"original_text": "  Deep Metric Learning (DML) proposes to learn metric spaces which encode\nsemantic similarities as embedding space distances. These spaces should be\ntransferable to classes beyond those seen during training. Commonly, DML\nmethods task networks to solve contrastive ranking tasks defined over binary\nclass assignments. However, such approaches ignore higher-level semantic\nrelations between the actual classes. This causes learned embedding spaces to\nencode incomplete semantic context and misrepresent the semantic relation\nbetween classes, impacting the generalizability of the learned metric space. To\ntackle this issue, we propose a language guidance objective for visual\nsimilarity learning. Leveraging language embeddings of expert- and\npseudo-classnames, we contextualize and realign visual representation spaces\ncorresponding to meaningful language semantics for better semantic consistency.\nExtensive experiments and ablations provide a strong motivation for our\nproposed approach and show language guidance offering significant,\nmodel-agnostic improvements for DML, achieving competitive and state-of-the-art\nresults on all benchmarks. Code available at\nhttps://github.com/ExplainableML/LanguageGuidance_for_DML.\n", "rewritten_text": "Deep Metric Learning (DML) typically uses contrastive ranking to learn embedding spaces reflecting semantic similarity.  However, this approach overlooks higher-order relationships between classes, leading to incomplete and inaccurate semantic representations and poor generalization.  To address this, we introduce a novel language guidance objective.  By incorporating language embeddings of class names (both expert-defined and automatically generated), we align visual representations with meaningful semantic contexts, improving consistency.  Our extensive experiments demonstrate significant, model-agnostic improvements across various DML benchmarks, achieving state-of-the-art results.  Code is available at https://github.com/ExplainableML/LanguageGuidance_for_DML.\n"}, "1411.5057": {"original_text": "  In this paper, we propose a novel algorithm for analysis-based sparsity\nreconstruction. It can solve the generalized problem by structured sparsity\nregularization with an orthogonal basis and total variation regularization. The\nproposed algorithm is based on the iterative reweighted least squares (IRLS)\nmodel, which is further accelerated by the preconditioned conjugate gradient\nmethod. The convergence rate of the proposed algorithm is almost the same as\nthat of the traditional IRLS algorithms, that is, exponentially fast. Moreover,\nwith the specifically devised preconditioner, the computational cost for each\niteration is significantly less than that of traditional IRLS algorithms, which\nenables our approach to handle large scale problems. In addition to the fast\nconvergence, it is straightforward to apply our method to standard sparsity,\ngroup sparsity, overlapping group sparsity and TV based problems. Experiments\nare conducted on a practical application: compressive sensing magnetic\nresonance imaging. Extensive results demonstrate that the proposed algorithm\nachieves superior performance over 14 state-of-the-art algorithms in terms of\nboth accuracy and computational cost.\n", "rewritten_text": "This paper introduces a novel, fast algorithm for sparsity reconstruction using structured sparsity and total variation regularization within an orthogonal basis.  Based on an accelerated iterative reweighted least squares (IRLS) method employing a preconditioned conjugate gradient, it achieves exponentially fast convergence comparable to traditional IRLS, but with significantly reduced computational cost per iteration thanks to a specially designed preconditioner. This allows efficient handling of large-scale problems.  The algorithm readily adapts to various sparsity scenarios, including standard, group, overlapping group, and total variation-based problems.  Evaluated on compressive sensing magnetic resonance imaging, it outperforms 14 state-of-the-art algorithms in both accuracy and speed.\n"}, "2211.09809": {"original_text": "  Animating portraits using speech has received growing attention in recent\nyears, with various creative and practical use cases. An ideal generated video\nshould have good lip sync with the audio, natural facial expressions and head\nmotions, and high frame quality. In this work, we present SPACE, which uses\nspeech and a single image to generate high-resolution, and expressive videos\nwith realistic head pose, without requiring a driving video. It uses a\nmulti-stage approach, combining the controllability of facial landmarks with\nthe high-quality synthesis power of a pretrained face generator. SPACE also\nallows for the control of emotions and their intensities. Our method\noutperforms prior methods in objective metrics for image quality and facial\nmotions and is strongly preferred by users in pair-wise comparisons. The\nproject website is available at https://deepimagination.cc/SPACE/\n", "rewritten_text": "Recent years have seen a surge in interest in animating portrait images using speech.  Ideally, such animations should feature accurate lip-sync, natural facial expressions and head movements, and high visual quality.  Our new method, SPACE, achieves this using only a single image and the audio track, generating high-resolution, expressive videos with realistic head poses without needing a reference video.  SPACE combines precise facial landmark control with the power of a pre-trained face generator in a multi-stage process, even allowing for control over emotions and their intensity.  Our results surpass existing methods in objective quality metrics and user preference tests.  See our project website for more details: https://deepimagination.cc/SPACE/\n"}, "2307.03602": {"original_text": "  Stereo vision systems have become popular in computer vision applications,\nsuch as 3D reconstruction, object tracking, and autonomous navigation. However,\ntraditional stereo vision systems that use rectilinear lenses may not be\nsuitable for certain scenarios due to their limited field of view. This has led\nto the popularity of vision systems based on one or multiple fisheye cameras in\ndifferent orientations, which can provide a field of view of 180x180 degrees or\nmore. However, fisheye cameras introduce significant distortion at the edges\nthat affects the accuracy of stereo matching and depth estimation. To overcome\nthese limitations, this paper proposes a method for distortion-removal and\ndepth estimation analysis for stereovision system using orthogonally divergent\nfisheye cameras (ODFC). The proposed method uses two virtual pinhole cameras\n(VPC), each VPC captures a small portion of the original view and presents it\nwithout any lens distortions, emulating the behavior of a pinhole camera. By\ncarefully selecting the captured regions, it is possible to create a stereo\npair using two VPCs. The performance of the proposed method is evaluated in\nboth simulation using virtual environment and experiments using real cameras\nand their results compared to stereo cameras with parallel optical axes. The\nresults demonstrate the effectiveness of the proposed method in terms of\ndistortion removal and depth estimation accuracy.\n", "rewritten_text": "While stereo vision using rectilinear lenses is common in applications like 3D reconstruction and autonomous navigation, its limited field of view restricts its use in certain scenarios.  Fishes-eye cameras, offering a much wider field of view (e.g., 180\u00b0 x 180\u00b0), are increasingly popular, but their significant edge distortion hinders accurate stereo matching and depth estimation.  This paper presents a novel method for depth estimation using orthogonally divergent fisheye cameras (ODFCs).  It achieves this by employing two virtual pinhole cameras (VPCs), each capturing a distortion-free portion of the fisheye image.  These VPCs effectively create a stereo pair, enabling accurate depth estimation.  The method's performance is validated through both simulated and real-world experiments, demonstrating improved accuracy compared to traditional parallel-axis stereo cameras.\n"}, "2410.11816": {"original_text": "  The automatic assembly problem has attracted increasing interest due to its\ncomplex challenges that involve 3D representation. This paper introduces\nJigsaw++, a novel generative method designed to tackle the multifaceted\nchallenges of reconstruction for the reassembly problem. Existing approach\nfocusing primarily on piecewise information for both part and fracture\nassembly, often overlooking the integration of complete object prior. Jigsaw++\ndistinguishes itself by learning a category-agnostic shape prior of complete\nobjects. It employs the proposed \"retargeting\" strategy that effectively\nleverages the output of any existing assembly method to generate complete shape\nreconstructions. This capability allows it to function orthogonally to the\ncurrent methods. Through extensive evaluations on Breaking Bad dataset and\nPartNet, Jigsaw++ has demonstrated its effectiveness, reducing reconstruction\nerrors and enhancing the precision of shape reconstruction, which sets a new\ndirection for future reassembly model developments.\n", "rewritten_text": "Driven by the complexities of 3D representation in automatic assembly, this paper presents Jigsaw++, a novel generative method for object reconstruction during reassembly.  Unlike existing methods that rely heavily on piecewise information and neglect global object context, Jigsaw++ learns a category-agnostic shape prior.  Its innovative \"retargeting\" strategy leverages the output of any existing assembly method to produce complete shape reconstructions, offering a complementary approach.  Evaluations on Breaking Bad and PartNet datasets demonstrate Jigsaw++'s superior performance in reducing reconstruction errors and improving accuracy, establishing a new benchmark for future reassembly models.\n"}, "1806.05658": {"original_text": "  Seq2seq learning has produced promising results on summarization. However, in\nmany cases, system summaries still struggle to keep the meaning of the original\nintact. They may miss out important words or relations that play critical roles\nin the syntactic structure of source sentences. In this paper, we present\nstructure-infused copy mechanisms to facilitate copying important words and\nrelations from the source sentence to summary sentence. The approach naturally\ncombines source dependency structure with the copy mechanism of an abstractive\nsentence summarizer. Experimental results demonstrate the effectiveness of\nincorporating source-side syntactic information in the system, and our proposed\napproach compares favorably to state-of-the-art methods.\n", "rewritten_text": "While sequence-to-sequence (seq2seq) models show promise in text summarization, they often fail to preserve the original text's meaning, omitting crucial words and relationships.  This paper introduces structure-infused copy mechanisms to address this, integrating source sentence dependency structures into an abstractive summarizer's copy mechanism to improve the transfer of key words and relationships.  Experiments confirm the effectiveness of this approach, outperforming existing state-of-the-art methods.\n"}, "2108.12545": {"original_text": "  Training deep networks for semantic segmentation requires large amounts of\nlabeled training data, which presents a major challenge in practice, as\nlabeling segmentation masks is a highly labor-intensive process. To address\nthis issue, we present a framework for semi-supervised and domain-adaptive\nsemantic segmentation, which is enhanced by self-supervised monocular depth\nestimation (SDE) trained only on unlabeled image sequences.\n  In particular, we utilize SDE as an auxiliary task comprehensively across the\nentire learning framework: First, we automatically select the most useful\nsamples to be annotated for semantic segmentation based on the correlation of\nsample diversity and difficulty between SDE and semantic segmentation. Second,\nwe implement a strong data augmentation by mixing images and labels using the\ngeometry of the scene. Third, we transfer knowledge from features learned\nduring SDE to semantic segmentation by means of transfer and multi-task\nlearning. And fourth, we exploit additional labeled synthetic data with\nCross-Domain DepthMix and Matching Geometry Sampling to align synthetic and\nreal data.\n  We validate the proposed model on the Cityscapes dataset, where all four\ncontributions demonstrate significant performance gains, and achieve\nstate-of-the-art results for semi-supervised semantic segmentation as well as\nfor semi-supervised domain adaptation. In particular, with only 1/30 of the\nCityscapes labels, our method achieves 92% of the fully-supervised baseline\nperformance and even 97% when exploiting additional data from GTA. The source\ncode is available at\nhttps://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth.\n", "rewritten_text": "Training deep semantic segmentation networks is hampered by the need for extensive labeled data.  To overcome this, we introduce a novel semi-supervised and domain-adaptive framework leveraging self-supervised monocular depth estimation (SDE) trained solely on unlabeled images.  Our framework integrates SDE in four key ways:  1) intelligent sample selection for annotation based on SDE and segmentation difficulty; 2) robust data augmentation using scene geometry; 3) knowledge transfer from SDE to segmentation via transfer and multi-task learning; and 4) synthetic data integration using Cross-Domain DepthMix and Matching Geometry Sampling.  Experiments on Cityscapes demonstrate significant performance improvements, achieving state-of-the-art results in both semi-supervised segmentation and domain adaptation.  Using only 3% of Cityscapes labels, our method achieves 92% of the fully-supervised performance, rising to 97% with additional GTA data.  Code is available at https://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth.\n"}, "2405.12833": {"original_text": "  Automatic radiology report generation can alleviate the workload for\nphysicians and minimize regional disparities in medical resources, therefore\nbecoming an important topic in the medical image analysis field. It is a\nchallenging task, as the computational model needs to mimic physicians to\nobtain information from multi-modal input data (i.e., medical images, clinical\ninformation, medical knowledge, etc.), and produce comprehensive and accurate\nreports. Recently, numerous works emerged to address this issue using deep\nlearning-based methods, such as transformers, contrastive learning, and\nknowledge-base construction. This survey summarizes the key techniques\ndeveloped in the most recent works and proposes a general workflow for deep\nlearning-based report generation with five main components, including\nmulti-modality data acquisition, data preparation, feature learning, feature\nfusion/interaction, and report generation. The state-of-the-art methods for\neach of these components are highlighted. Additionally, training strategies,\npublic datasets, evaluation methods, current challenges, and future directions\nin this field are summarized. We have also conducted a quantitative comparison\nbetween different methods under the same experimental setting. This is the most\nup-to-date survey that focuses on multi-modality inputs and data fusion for\nradiology report generation. The aim is to provide comprehensive and rich\ninformation for researchers interested in automatic clinical report generation\nand medical image analysis, especially when using multimodal inputs, and assist\nthem in developing new algorithms to advance the field.\n", "rewritten_text": "Automating radiology report generation is crucial for reducing physician workload and addressing healthcare disparities.  This challenging task requires AI models to process diverse data (images, clinical notes, medical knowledge) and produce accurate, comprehensive reports.  This survey comprehensively reviews recent deep learning approaches, including transformers, contrastive learning, and knowledge bases, used to achieve this.  We present a five-component workflow encompassing data acquisition, preparation, feature learning, fusion, and report generation, highlighting state-of-the-art methods for each.  The survey also covers training strategies, datasets, evaluation metrics, current limitations, and future research directions, including a quantitative comparison of methods.  This is the most current review focusing on multi-modal data fusion for automated radiology report generation, aiming to guide researchers in developing advanced algorithms for clinical report generation and medical image analysis.\n"}, "1809.10417": {"original_text": "  The tracking-by-detection framework receives growing attentions through the\nintegration with the Convolutional Neural Networks (CNNs). Existing\ntracking-by-detection based methods, however, fail to track objects with severe\nappearance variations. This is because the traditional convolutional operation\nis performed on fixed grids, and thus may not be able to find the correct\nresponse while the object is changing pose or under varying environmental\nconditions. In this paper, we propose a deformable convolution layer to enrich\nthe target appearance representations in the tracking-by-detection framework.\nWe aim to capture the target appearance variations via deformable convolution,\nwhich adaptively enhances its original features. In addition, we also propose a\ngated fusion scheme to control how the variations captured by the deformable\nconvolution affect the original appearance. The enriched feature representation\nthrough deformable convolution facilitates the discrimination of the CNN\nclassifier on the target object and background. Extensive experiments on the\nstandard benchmarks show that the proposed tracker performs favorably against\nstate-of-the-art methods.\n", "rewritten_text": "Tracking-by-detection, enhanced by Convolutional Neural Networks (CNNs), is increasingly popular.  However, existing methods struggle with objects exhibiting significant appearance changes due to limitations of traditional CNNs' fixed grid convolutional operations.  This paper introduces a deformable convolution layer to improve target representation within the tracking-by-detection framework.  This layer adaptively enhances features to capture appearance variations, and a gated fusion scheme controls the influence of these variations on the original appearance.  The resulting enriched feature representation improves the CNN classifier's ability to distinguish target from background.  Benchmark tests demonstrate superior performance compared to existing state-of-the-art trackers.\n"}, "2111.15603": {"original_text": "  Modern neural networks are able to perform at least as well as humans in\nnumerous tasks involving object classification and image generation. However,\nsmall perturbations which are imperceptible to humans may significantly degrade\nthe performance of well-trained deep neural networks. We provide a\nDistributionally Robust Optimization (DRO) framework which integrates\nhuman-based image quality assessment methods to design optimal attacks that are\nimperceptible to humans but significantly damaging to deep neural networks.\nThrough extensive experiments, we show that our attack algorithm generates\nbetter-quality (less perceptible to humans) attacks than other state-of-the-art\nhuman imperceptible attack methods. Moreover, we demonstrate that DRO training\nusing our optimally designed human imperceptible attacks can improve group\nfairness in image classification. Towards the end, we provide an algorithmic\nimplementation to speed up DRO training significantly, which could be of\nindependent interest.\n", "rewritten_text": "While modern neural networks rival human performance in object classification and image generation, they remain vulnerable to subtle, human-imperceptible perturbations that severely impact their accuracy.  This work introduces a Distributionally Robust Optimization (DRO) framework incorporating human image quality assessment to craft highly effective, yet visually undetectable, adversarial attacks.  Experiments demonstrate that our approach generates superior, less perceptible attacks compared to existing methods and, further, that DRO training using these attacks enhances fairness in image classification.  Finally, we present an efficient algorithm to accelerate DRO training.\n"}, "2404.00829": {"original_text": "  Human writers often bookend their writing with ending sentences that relate\nback to the beginning sentences in order to compose a satisfying narrative that\n\"closes the loop.\" Motivated by this observation, we propose RENarGen, a\ncontrollable story-generation paradigm that generates narratives by ensuring\nthe first and last sentences are related and then infilling the middle\nsentences. Our contributions include an initial exploration of how various\nmethods of bookending from Narratology affect language modeling for stories.\nAutomatic and human evaluations indicate RENarGen produces better stories with\nmore narrative closure than current autoregressive models.\n", "rewritten_text": "Inspired by the common literary technique of thematic bookending\u2014where the opening and closing sentences echo each other\u2014we introduce RENarGen, a novel story generation model.  RENarGen generates narratives by first creating related opening and closing sentences, then filling in the middle.  Our work explores the impact of different narrative bookending methods on language models and demonstrates, through both automatic and human evaluation, that RENarGen generates more narratively satisfying stories than existing autoregressive models.\n"}, "1711.01062": {"original_text": "  With the development of depth cameras such as Kinect and Intel Realsense,\nRGB-D based human detection receives continuous research attention due to its\nusage in a variety of applications. In this paper, we propose a new\nMulti-Glimpse LSTM (MG-LSTM) network, in which multi-scale contextual\ninformation is sequentially integrated to promote the human detection\nperformance. Furthermore, we propose a feature fusion strategy based on our\nMG-LSTM network to better incorporate the RGB and depth information. To the\nbest of our knowledge, this is the first attempt to utilize LSTM structure for\nRGB-D based human detection. Our method achieves superior performance on two\npublicly available datasets.\n", "rewritten_text": "Recent advances in depth cameras (e.g., Kinect, Intel Realsense) have fueled significant research into RGB-D human detection.  This paper introduces a novel Multi-Glimpse LSTM (MG-LSTM) network that leverages sequential integration of multi-scale contextual information to improve detection accuracy.  A corresponding feature fusion strategy is also presented to optimally combine RGB and depth data.  This represents the first application of LSTM architecture to RGB-D human detection, achieving state-of-the-art results on two benchmark datasets.\n"}, "1810.07599": {"original_text": "  As facial appearance is subject to significant intra-class variations caused\nby the aging process over time, age-invariant face recognition (AIFR) remains a\nmajor challenge in face recognition community. To reduce the intra-class\ndiscrepancy caused by the aging, in this paper we propose a novel approach\n(namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep\nface features. Specifically, we decompose deep face features into two\northogonal components to represent age-related and identity-related features.\nAs a result, identity-related features that are robust to aging are then used\nfor AIFR. Besides, for complementing the existing cross-age datasets and\nadvancing the research in this field, we construct a brand-new large-scale\nCross-Age Face dataset (CAF). Extensive experiments conducted on the three\npublic domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have\nshown the effectiveness of the proposed approach and the value of the\nconstructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most\npopular general face recognition (GFR) dataset LFW additionally demonstrates\nthe comparable generalization performance on GFR.\n", "rewritten_text": "Age-invariant face recognition (AIFR) is difficult due to significant facial changes with age.  This paper introduces Orthogonal Embedding CNNs (OE-CNNs), a novel approach that decomposes deep facial features into age-related and identity-related components.  The identity-related features, robust to aging effects, are then used for AIFR.  To support this research, we also present a new large-scale Cross-Age Face (CAF) dataset.  Experiments on three public datasets (MORPH Album 2, CACD-VS, and FG-NET) demonstrate the effectiveness of OE-CNNs and the value of the CAF dataset.  Furthermore,  testing on the LFW dataset shows comparable performance to general face recognition (GFR) systems.\n"}, "2107.005": {"original_text": "  Driven by recent advances in object detection with deep neural networks, the\ntracking-by-detection paradigm has gained increasing prevalence in the research\ncommunity of multi-object tracking (MOT). It has long been known that\nappearance information plays an essential role in the detection-to-track\nassociation, which lies at the core of the tracking-by-detection paradigm.\nWhile most existing works consider the appearance distances between the\ndetections and the tracks, they ignore the statistical information implied by\nthe historical appearance distance records in the tracks, which can be\nparticularly useful when a detection has similar distances with two or more\ntracks. In this work, we propose a hybrid track association (HTA) algorithm\nthat models the historical appearance distances of a track with an incremental\nGaussian mixture model (IGMM) and incorporates the derived statistical\ninformation into the calculation of the detection-to-track association cost.\nExperimental results on three MOT benchmarks confirm that HTA effectively\nimproves the target identification performance with a small compromise to the\ntracking speed. Additionally, compared to many state-of-the-art trackers, the\nDeepSORT tracker equipped with HTA achieves better or comparable performance in\nterms of the balance of tracking quality and speed.\n", "rewritten_text": "Recent advancements in deep learning-based object detection have made tracking-by-detection the dominant approach in multi-object tracking (MOT).  Appearance information is crucial for associating detections with existing tracks, but current methods overlook the valuable statistical information contained within a track's historical appearance distances.  This paper introduces a novel hybrid track association (HTA) algorithm that addresses this limitation.  HTA uses an incremental Gaussian mixture model (IGMM) to model a track's historical appearance distances, integrating this statistical information into the association cost calculation.  Evaluated on three MOT benchmarks, HTA significantly improves target identification with minimal impact on tracking speed.  Furthermore, when integrated into the DeepSORT tracker, HTA achieves state-of-the-art performance in balancing tracking quality and speed.\n"}, "1711.08879": {"original_text": "  Objects for detection usually have distinct characteristics in different\nsub-regions and different aspect ratios. However, in prevalent two-stage object\ndetection methods, Region-of-Interest (RoI) features are extracted by RoI\npooling with little emphasis on these translation-variant feature components.\nWe present feature selective networks to reform the feature representations of\nRoIs by exploiting their disparities among sub-regions and aspect ratios. Our\nnetwork produces the sub-region attention bank and aspect ratio attention bank\nfor the whole image. The RoI-based sub-region attention map and aspect ratio\nattention map are selectively pooled from the banks, and then used to refine\nthe original RoI features for RoI classification. Equipped with a light-weight\ndetection subnetwork, our network gets a consistent boost in detection\nperformance based on general ConvNet backbones (ResNet-101, GoogLeNet and\nVGG-16). Without bells and whistles, our detectors equipped with ResNet-101\nachieve more than 3% mAP improvement compared to counterparts on PASCAL VOC\n2007, PASCAL VOC 2012 and MS COCO datasets.\n", "rewritten_text": "Two-stage object detectors often overlook the varying characteristics of objects across sub-regions and aspect ratios.  To address this, we introduce feature selective networks that leverage these variations to improve feature representation.  Our approach generates image-wide attention banks for sub-regions and aspect ratios.  These banks provide context-aware attention maps for each Region of Interest (RoI), refining the RoI features before classification.  This lightweight addition, integrated with common backbones (ResNet-101, GoogLeNet, VGG-16), consistently improves detection performance.  Specifically, our ResNet-101 based detector achieves over a 3% mean Average Precision (mAP) improvement on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO datasets.\n"}, "2402.02145": {"original_text": "  In today's media landscape, where news outlets play a pivotal role in shaping\npublic opinion, it is imperative to address the issue of sentiment manipulation\nwithin news text. News writers often inject their own biases and emotional\nlanguage, which can distort the objectivity of reporting. This paper introduces\na novel approach to tackle this problem by reducing the polarity of latent\nsentiments in news content. Drawing inspiration from adversarial attack-based\nsentence perturbation techniques and a prompt based method using ChatGPT, we\nemploy transformation constraints to modify sentences while preserving their\ncore semantics. Using three perturbation methods: replacement, insertion, and\ndeletion coupled with a context-aware masked language model, we aim to maximize\nthe desired sentiment score for targeted news aspects through a beam search\nalgorithm. Our experiments and human evaluations demonstrate the effectiveness\nof these two models in achieving reduced sentiment polarity with minimal\nmodifications while maintaining textual similarity, fluency, and grammatical\ncorrectness. Comparative analysis confirms the competitive performance of the\nadversarial attack based perturbation methods and prompt-based methods,\noffering a promising solution to foster more objective news reporting and\ncombat emotional language bias in the media.\n", "rewritten_text": "This paper presents a novel method for mitigating sentiment manipulation in news reporting.  Recognizing the significant influence of news media on public opinion, we address the problem of biased and emotionally charged language in news articles.  Our approach uses a combination of adversarial attack-based sentence perturbation (replacement, insertion, and deletion) and a ChatGPT-based prompt method, leveraging a context-aware masked language model and beam search to reduce sentiment polarity while preserving semantic meaning, fluency, and grammatical accuracy.  Experiments and human evaluations demonstrate the effectiveness of both methods in achieving this goal with minimal textual changes.  Comparative analysis shows their comparable performance, offering a promising technique for promoting more objective and less emotionally biased news coverage.\n"}, "2312.10437": {"original_text": "  Tender notices are usually sought by most of the companies at regular\nintervals as a means for obtaining the contracts of various projects. These\nnotices consist of all the required information like description of the work,\nperiod of construction, estimated amount of project, etc. In the context of\nNepal, tender notices are usually published in national as well as local\nnewspapers. The interested bidders should search all the related tender notices\nin newspapers. However, it is very tedious for these companies to manually\nsearch tender notices in every newspaper and figure out which bid is best\nsuited for them. This project is built with the purpose of solving this tedious\ntask of manually searching the tender notices. Initially, the newspapers are\ndownloaded in PDF format using the selenium library of python. After\ndownloading the newspapers, the e-papers are scanned and tender notices are\nautomatically extracted using a neural network. For extraction purposes,\ndifferent architectures of CNN namely ResNet, GoogleNet and Xception are used\nand a model with highest performance has been implemented. Finally, these\nextracted notices are then published on the website and are accessible to the\nusers. This project is helpful for construction companies as well as\ncontractors assuring quality and efficiency. This project has great application\nin the field of competitive bidding as well as managing them in a systematic\nmanner.\n", "rewritten_text": "Many Nepali companies regularly seek construction contracts through tender notices published in national and local newspapers.  Finding suitable bids manually is time-consuming and inefficient.  This project automates this process.  Using Python's Selenium library, it downloads newspapers in PDF format.  A neural network, employing CNN architectures like ResNet, GoogleNet, and Xception, then extracts tender details.  The best-performing model is deployed to publish extracted notices on a website, providing easy access for construction companies and contractors, improving efficiency and streamlining competitive bidding.\n"}, "2405.18132": {"original_text": "  In recent years, the increasing demand for dynamic 3D assets in design and\ngaming applications has given rise to powerful generative pipelines capable of\nsynthesizing high-quality 4D objects. Previous methods generally rely on score\ndistillation sampling (SDS) algorithm to infer the unseen views and motion of\n4D objects, thus leading to unsatisfactory results with defects like\nover-saturation and Janus problem. Therefore, inspired by recent progress of\nvideo diffusion models, we propose to optimize a 4D representation by\nexplicitly generating multi-view videos from one input image. However, it is\nfar from trivial to handle practical challenges faced by such a pipeline,\nincluding dramatic temporal inconsistency, inter-frame geometry and texture\ndiversity, and semantic defects brought by video generation results. To address\nthese issues, we propose DG4D, a novel multi-stage framework that generates\nhigh-quality and consistent 4D assets without score distillation. Specifically,\ncollaborative techniques and solutions are developed, including an attention\ninjection strategy to synthesize temporal-consistent multi-view videos, a\nrobust and efficient dynamic reconstruction method based on Gaussian Splatting,\nand a refinement stage with diffusion prior for semantic restoration. The\nqualitative results and user preference study demonstrate that our framework\noutperforms the baselines in generation quality by a considerable margin. Code\nwill be released at \\url{https://github.com/jasongzy/EG4D}.\n", "rewritten_text": "The rising demand for dynamic 3D assets in design and gaming has fueled the development of powerful generative models for high-quality 4D objects.  Existing methods, relying on score distillation sampling, suffer from artifacts like over-saturation and the Janus problem.  Inspired by video diffusion models, we introduce DG4D, a novel multi-stage framework that generates consistent 4D assets from a single input image by directly synthesizing multi-view videos.  DG4D overcomes challenges like temporal inconsistency, inter-frame variations, and semantic errors through an attention-based temporal consistency mechanism, efficient dynamic reconstruction using Gaussian Splatting, and a diffusion-prior-based refinement stage.  Our qualitative results and user study demonstrate significantly improved generation quality over existing methods.  Code is available at [https://github.com/jasongzy/EG4D](https://github.com/jasongzy/EG4D).\n"}, "2101.04929": {"original_text": "  Motivated by applications from computer vision to bioinformatics, the field\nof shape analysis deals with problems where one wants to analyze geometric\nobjects, such as curves, while ignoring actions that preserve their shape, such\nas translations, rotations, or reparametrizations. Mathematical tools have been\ndeveloped to define notions of distances, averages, and optimal deformations\nfor geometric objects. One such framework, which has proven to be successful in\nmany applications, is based on the square root velocity (SRV) transform, which\nallows one to define a computable distance between spatial curves regardless of\nhow they are parametrized. This paper introduces a supervised deep learning\nframework for the direct computation of SRV distances between curves, which\nusually requires an optimization over the group of reparametrizations that act\non the curves. The benefits of our approach in terms of computational speed and\naccuracy are illustrated via several numerical experiments.\n", "rewritten_text": "Shape analysis, driven by applications ranging from computer vision to bioinformatics, focuses on analyzing geometric objects (e.g., curves) while disregarding shape-preserving transformations like translations, rotations, and reparametrizations.  Existing mathematical tools define distances, averages, and optimal deformations for these objects.  The square root velocity (SRV) transform is a particularly successful framework for computing distances between curves, irrespective of their parametrization. This paper presents a novel supervised deep learning method for directly computing SRV distances, bypassing the computationally expensive optimization over reparametrization groups typically required.  Numerical experiments demonstrate the superior speed and accuracy of our approach.\n"}, "1811.07461": {"original_text": "  Humans naturally perceive a 3D scene in front of them through accumulation of\ninformation obtained from multiple interconnected projections of the scene and\nby interpreting their correspondence. This phenomenon has inspired artificial\nintelligence models to extract the depth and view angle of the observed scene\nby modeling the correspondence between different views of that scene. Our paper\nis built upon previous works in the field of unsupervised depth and relative\ncamera pose estimation from temporal consecutive video frames using deep\nlearning (DL) models. Our approach uses a hybrid learning framework introduced\nin a recent work called GeoNet, which leverages geometric constraints in the 3D\nscenes to synthesize a novel view from intermediate DL-based predicted depth\nand relative pose. However, the state-of-the-art unsupervised depth and pose\nestimation DL models are exclusively trained/tested on a few available outdoor\nscene datasets and we have shown they are hardly transferable to new scenes,\nespecially from indoor environments, in which estimation requires higher\nprecision and dealing with probable occlusions. This paper introduces \"Indoor\nGeoNet\", a weakly supervised depth and camera pose estimation model targeted\nfor indoor scenes. In Indoor GeoNet, we take advantage of the availability of\nindoor RGBD datasets collected by human or robot navigators, and added partial\n(i.e. weak) supervision in depth training into the model. Experimental results\nshowed that our model effectively generalizes to new scenes from different\nbuildings. Indoor GeoNet demonstrated significant depth and pose estimation\nerror reduction when compared to the original GeoNet, while showing 3 times\nmore reconstruction accuracy in synthesizing novel views in indoor\nenvironments.\n", "rewritten_text": "Human 3D perception arises from integrating multiple viewpoints.  Inspired by this, AI models estimate depth and viewpoint by analyzing correspondences between different views.  Building on unsupervised deep learning methods for depth and pose estimation from video, this paper introduces Indoor GeoNet.  While existing state-of-the-art models, such as GeoNet, struggle to generalize beyond limited outdoor datasets, Indoor GeoNet leverages weakly supervised learning with indoor RGBD datasets.  This allows it to effectively generalize to unseen indoor scenes, significantly improving depth and pose estimation accuracy compared to GeoNet, and achieving three times higher novel view synthesis accuracy in indoor environments.\n"}, "2208.04254": {"original_text": "  Image captioning models are usually trained according to human annotated\nground-truth captions, which could generate accurate but generic captions. In\nthis paper, we focus on generating distinctive captions that can distinguish\nthe target image from other similar images. To evaluate the distinctiveness of\ncaptions, we introduce a series of metrics that use large-scale vision-language\npre-training model CLIP to quantify the distinctiveness. To further improve the\ndistinctiveness of captioning models, we propose a simple and effective\ntraining strategy that trains the model by comparing target image with similar\nimage group and optimizing the group embedding gap. Extensive experiments are\nconducted on various baseline models to demonstrate the wide applicability of\nour strategy and the consistency of metric results with human evaluation. By\ncomparing the performance of our best model with existing state-of-the-art\nmodels, we claim that our model achieves new state-of-the-art towards\ndistinctiveness objective.\n", "rewritten_text": "Current image captioning models, trained on human-annotated captions, produce accurate but often generic descriptions.  This paper introduces a novel approach to generate more distinctive captions that better differentiate an image from visually similar ones.  We propose new metrics, leveraging CLIP, to quantify caption distinctiveness and a training strategy that optimizes the embedding gap between a target image and similar images.  Extensive experiments across various models demonstrate the effectiveness and broad applicability of our method, which achieves state-of-the-art results in distinctiveness, as confirmed by both our metrics and human evaluation.\n"}, "2212.07181": {"original_text": "  Neuromorphic vision or event vision is an advanced vision technology, where\nin contrast to the visible camera that outputs pixels, the event vision\ngenerates neuromorphic events every time there is a brightness change which\nexceeds a specific threshold in the field of view (FOV). This study focuses on\nleveraging neuromorphic event data for roadside object detection. This is a\nproof of concept towards building artificial intelligence (AI) based pipelines\nwhich can be used for forward perception systems for advanced vehicular\napplications. The focus is on building efficient state-of-the-art object\ndetection networks with better inference results for fast-moving forward\nperception using an event camera. In this article, the event-simulated A2D2\ndataset is manually annotated and trained on two different YOLOv5 networks\n(small and large variants). To further assess its robustness, single model\ntesting and ensemble model testing are carried out.\n", "rewritten_text": "This research explores the use of neuromorphic event-based vision for roadside object detection in advanced driver-assistance systems (ADAS).  Unlike traditional cameras, event cameras only record changes in brightness, generating data far more efficiently.  We demonstrate a proof-of-concept AI pipeline using a manually annotated event-simulated A2D2 dataset and two YOLOv5 object detection models (small and large).  The robustness of both single and ensemble model approaches is evaluated, aiming to achieve state-of-the-art performance in fast-moving scenarios.\n"}, "2402.01217": {"original_text": "  Implicit neural representations, represented by Neural Radiance Fields\n(NeRF), have dominated research in 3D computer vision by virtue of high-quality\nvisual results and data-driven benefits. However, their realistic applications\nare hindered by the need for dense inputs and per-scene optimization. To solve\nthis problem, previous methods implement generalizable NeRFs by extracting\nlocal features from sparse inputs as conditions for the NeRF decoder. However,\nalthough this way can allow feed-forward reconstruction, they suffer from the\ninherent drawback of yielding sub-optimal results caused by erroneous\nreprojected features. In this paper, we focus on this problem and aim to\naddress it by introducing pre-trained generative priors to enable high-quality\ngeneralizable novel view synthesis. Specifically, we propose a novel Indirect\nDiffusion-guided NeRF framework, termed ID-NeRF, which leverages pre-trained\ndiffusion priors as a guide for the reprojected features created by the\nprevious paradigm. Notably, to enable 3D-consistent predictions, the proposed\nID-NeRF discards the way of direct supervision commonly used in prior 3D\ngenerative models and instead adopts a novel indirect prior injection strategy.\nThis strategy is implemented by distilling pre-trained knowledge into an\nimaginative latent space via score-based distillation, and an attention-based\nrefinement module is then proposed to leverage the embedded priors to improve\nreprojected features extracted from sparse inputs. We conduct extensive\nexperiments on multiple datasets to evaluate our method, and the results\ndemonstrate the effectiveness of our method in synthesizing novel views in a\ngeneralizable manner, especially in sparse settings.\n", "rewritten_text": "Neural Radiance Fields (NeRFs) have revolutionized 3D computer vision, but their reliance on dense input data and per-scene optimization limits real-world applicability.  While existing generalizable NeRFs address this by using sparse input features to condition the NeRF decoder, they often produce suboptimal results due to inaccurate feature reprojection.  This paper introduces ID-NeRF, a novel Indirect Diffusion-guided NeRF framework that leverages pre-trained diffusion models to improve the quality of novel view synthesis from sparse inputs.  Unlike previous 3D generative models, ID-NeRF avoids direct supervision, instead indirectly injecting pre-trained knowledge via score-based distillation into a latent space.  An attention-based refinement module then uses these priors to enhance the reprojected features.  Extensive experiments demonstrate ID-NeRF's superior performance in generating high-quality novel views, particularly with sparse input data.\n"}, "2207.02803": {"original_text": "  Recent advances in face forgery techniques produce nearly visually\nuntraceable deepfake videos, which could be leveraged with malicious\nintentions. As a result, researchers have been devoted to deepfake detection.\nPrevious studies have identified the importance of local low-level cues and\ntemporal information in pursuit to generalize well across deepfake methods,\nhowever, they still suffer from robustness problem against post-processings. In\nthis work, we propose the Local- & Temporal-aware Transformer-based Deepfake\nDetection (LTTD) framework, which adopts a local-to-global learning protocol\nwith a particular focus on the valuable temporal information within local\nsequences. Specifically, we propose a Local Sequence Transformer (LST), which\nmodels the temporal consistency on sequences of restricted spatial regions,\nwhere low-level information is hierarchically enhanced with shallow layers of\nlearned 3D filters. Based on the local temporal embeddings, we then achieve the\nfinal classification in a global contrastive way. Extensive experiments on\npopular datasets validate that our approach effectively spots local forgery\ncues and achieves state-of-the-art performance.\n", "rewritten_text": "Deepfake videos are becoming increasingly realistic, posing a significant threat.  While existing deepfake detection methods utilize local and temporal cues, they remain vulnerable to post-processing.  This paper introduces LTTD, a novel deepfake detection framework. LTTD employs a local-to-global learning approach, leveraging a Local Sequence Transformer (LST) to analyze temporal consistency within localized spatial regions.  LST enhances low-level information using shallow 3D filters, generating temporal embeddings for final global contrastive classification.  Extensive experiments demonstrate LTTD's superior performance and effectiveness in identifying subtle forgery artifacts, achieving state-of-the-art results.\n"}, "2210.16621": {"original_text": "  Transformer-based architectures like BERT have achieved great success in a\nwide range of Natural Language tasks. Despite their decent performance, the\nmodels still have numerous parameters and high computational complexity,\nimpeding their deployment in resource-constrained environments. Post-Training\nQuantization (PTQ), which enables low-bit computations without extra training,\ncould be a promising tool. In this work, we conduct an empirical evaluation of\nthree PTQ methods on BERT-Base and BERT-Large: Linear Quantization (LQ),\nAnalytical Clipping for Integer Quantization (ACIQ), and Outlier Channel\nSplitting (OCS). OCS theoretically surpasses the others in minimizing the Mean\nSquare quantization Error and avoiding distorting the weights' outliers. That\nis consistent with the evaluation results of most language tasks of GLUE\nbenchmark and a reading comprehension task, SQuAD. Moreover, low-bit quantized\nBERT models could outperform the corresponding 32-bit baselines on several\nsmall language tasks, which we attribute to the alleviation of\nover-parameterization. We further explore the limit of quantization bit and\nshow that OCS could quantize BERT-Base and BERT-Large to 3-bits and retain 98%\nand 96% of the performance on the GLUE benchmark accordingly. Moreover, we\nconduct quantization on the whole BERT family, i.e., BERT models in different\nconfigurations, and comprehensively evaluate their performance on the GLUE\nbenchmark and SQuAD, hoping to provide valuable guidelines for their deployment\nin various computation environments.\n", "rewritten_text": "Large language models like BERT achieve impressive results but are computationally expensive, hindering their use in resource-constrained settings.  This work empirically evaluates three post-training quantization (PTQ) methods \u2013 Linear Quantization (LQ), Analytical Clipping for Integer Quantization (ACIQ), and Outlier Channel Splitting (OCS) \u2013 on BERT-Base and BERT-Large.  OCS, theoretically minimizing quantization error by preserving outlier weights, consistently outperforms LQ and ACI in experiments across GLUE and SQuAD benchmarks.  Interestingly, low-bit quantized models even surpassed their 32-bit counterparts on some smaller tasks, potentially due to reduced over-parameterization.  We demonstrate that OCS can quantize BERT-Base and BERT-Large to 3 bits while retaining 98% and 96% of GLUE benchmark performance, respectively.  Finally, we comprehensively evaluate the performance of various BERT configurations across different quantization levels on GLUE and SQuAD, offering practical guidelines for deploying BERT in diverse computational environments.\n"}, "2401.10768": {"original_text": "  While large language models (LLMs) have demonstrated exceptional performance\nacross various tasks following human alignment, they may still generate\nresponses that sound plausible but contradict factual knowledge, a phenomenon\nknown as hallucination. In this paper, we demonstrate the feasibility of\nmitigating hallucinations by verifying and minimizing the inconsistency between\nexternal knowledge present in the alignment data and the intrinsic knowledge\nembedded within foundation LLMs. Specifically, we propose a novel approach\ncalled Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM\nto automatically formulate assessments based on external knowledge to evaluate\nthe knowledge boundaries of foundation LLMs. To address knowledge\ninconsistencies in the alignment data, KCA implements several specific\nstrategies to deal with these data instances. We demonstrate the superior\nefficacy of KCA in reducing hallucinations across six benchmarks, utilizing\nfoundation LLMs of varying backbones and scales. This confirms the\neffectiveness of mitigating hallucinations by reducing knowledge inconsistency.\nOur code, model weights, and data are openly accessible at\n\\url{https://github.com/fanqiwan/KCA}.\n", "rewritten_text": "Large language models (LLMs), despite their impressive performance, can sometimes generate factually incorrect yet plausible responses\u2014a phenomenon called hallucination.  This paper introduces Knowledge Consistent Alignment (KCA), a novel method to mitigate this issue. KCA uses a well-aligned LLM to assess the knowledge boundaries of foundation LLMs by verifying consistency between external knowledge in the training data and the LLM's internal knowledge.  It addresses inconsistencies in the training data through several specific strategies.  Experiments across six benchmarks and various LLMs demonstrate KCA's superior effectiveness in reducing hallucinations, confirming that minimizing knowledge inconsistency improves factual accuracy.  Our code, model weights, and data are publicly available at [https://github.com/fanqiwan/KCA](https://github.com/fanqiwan/KCA).\n"}, "2201.01016": {"original_text": "  Recovering detailed facial geometry from a set of calibrated multi-view\nimages is valuable for its wide range of applications. Traditional multi-view\nstereo (MVS) methods adopt an optimization-based scheme to regularize the\nmatching cost. Recently, learning-based methods integrate all these into an\nend-to-end neural network and show superiority of efficiency. In this paper, we\npropose a novel architecture to recover extremely detailed 3D faces within\ndozens of seconds. Unlike previous learning-based methods that regularize the\ncost volume via 3D CNN, we propose to learn an implicit function for regressing\nthe matching cost. By fitting a 3D morphable model from multi-view images, the\nfeatures of multiple images are extracted and aggregated in the mesh-attached\nUV space, which makes the implicit function more effective in recovering\ndetailed facial shape. Our method outperforms SOTA learning-based MVS in\naccuracy by a large margin on the FaceScape dataset. The code and data are\nreleased in https://github.com/zhuhao-nju/mvfr.\n", "rewritten_text": "This paper presents a novel, efficient method for reconstructing highly detailed 3D facial geometry from multiple calibrated images.  Unlike existing learning-based multi-view stereo (MVS) approaches that rely on 3D convolutional neural networks (CNNs) to regularize cost volumes, our architecture learns an implicit function to regress the matching cost.  This function operates in a mesh-attached UV space, leveraging features extracted and aggregated from a 3D morphable model fitted to the input images.  Our method achieves significantly improved accuracy over state-of-the-art learning-based MVS methods on the FaceScape dataset, producing highly detailed 3D face models in just tens of seconds.  Code and data are available at https://github.com/zhuhao-nju/mvfr.\n"}, "1809.05752": {"original_text": "  Readmission after discharge from a hospital is disruptive and costly,\nregardless of the reason. However, it can be particularly problematic for\npsychiatric patients, so predicting which patients may be readmitted is\ncritically important but also very difficult. Clinical narratives in\npsychiatric electronic health records (EHRs) span a wide range of topics and\nvocabulary; therefore, a psychiatric readmission prediction model must begin\nwith a robust and interpretable topic extraction component. We created a data\npipeline for using document vector similarity metrics to perform topic\nextraction on psychiatric EHR data in service of our long-term goal of creating\na readmission risk classifier. We show initial results for our topic extraction\nmodel and identify additional features we will be incorporating in the future.\n", "rewritten_text": "Hospital readmission is costly and disruptive, especially for psychiatric patients.  Predicting readmission in this population is crucial but challenging due to the diverse and unstructured nature of psychiatric electronic health record (EHR) narratives.  To address this, we developed a data pipeline employing document vector similarity for topic extraction from psychiatric EHR data. This forms the foundation of our future readmission risk prediction model.  This paper presents preliminary topic extraction results and outlines planned model enhancements.\n"}, "2302.1443": {"original_text": "  3D hand tracking methods based on monocular RGB videos are easily affected by\nmotion blur, while event camera, a sensor with high temporal resolution and\ndynamic range, is naturally suitable for this task with sparse output and low\npower consumption. However, obtaining 3D annotations of fast-moving hands is\ndifficult for constructing event-based hand-tracking datasets. In this paper,\nwe provided an event-based speed adaptive hand tracker (ESAHT) to solve the\nhand tracking problem based on event camera. We enabled a CNN model trained on\na hand tracking dataset with slow motion, which enabled the model to leverage\nthe knowledge of RGB-based hand tracking solutions, to work on fast hand\ntracking tasks. To realize our solution, we constructed the first 3D hand\ntracking dataset captured by an event camera in a real-world environment,\nfigured out two data augment methods to narrow the domain gap between slow and\nfast motion data, developed a speed adaptive event stream segmentation method\nto handle hand movements in different moving speeds, and introduced a new\nevent-to-frame representation method adaptive to event streams with different\nlengths. Experiments showed that our solution outperformed RGB-based as well as\nprevious event-based solutions in fast hand tracking tasks, and our codes and\ndataset will be publicly available.\n", "rewritten_text": "Monocular RGB video struggles with 3D hand tracking due to motion blur.  Event cameras, offering high temporal resolution and low power consumption, are a promising alternative.  However, creating large, annotated event-based hand-tracking datasets is challenging.  This paper introduces ESAHT, a speed-adaptive event-based hand tracker.  We address the data scarcity by training a CNN on a slow-motion hand tracking dataset, leveraging existing RGB-based techniques.  This involved creating the first real-world 3D event-camera hand tracking dataset, developing novel data augmentation methods to bridge the slow/fast motion gap, implementing a speed-adaptive event stream segmentation technique, and designing a new event-to-frame representation.  Our results demonstrate superior performance to both RGB and existing event-based methods in fast hand tracking scenarios.  The code and dataset will be publicly released.\n"}, "2407.15153": {"original_text": "  Video generation has drawn significant interest recently, pushing the\ndevelopment of large-scale models capable of producing realistic videos with\ncoherent motion. Due to memory constraints, these models typically generate\nshort video segments that are then combined into long videos. The merging\nprocess poses a significant challenge, as it requires ensuring smooth\ntransitions and overall consistency. In this paper, we introduce Anchored\nDiffusion, a novel method for synthesizing relatively long and seamless videos.\nWe extend Diffusion Transformers (DiTs) to incorporate temporal information,\ncreating our sequence-DiT (sDiT) model for generating short video segments.\nUnlike previous works, we train our model on video sequences with random\nnon-uniform temporal spacing and incorporate temporal information via external\nguidance, increasing flexibility and allowing it to capture both short and\nlong-term relationships. Furthermore, during inference, we leverage the\ntransformer architecture to modify the diffusion process, generating a batch of\nnon-uniform sequences anchored to a common frame, ensuring consistency\nregardless of temporal distance. To demonstrate our method, we focus on face\nreenactment, the task of creating a video from a source image that replicates\nthe facial expressions and movements from a driving video. Through\ncomprehensive experiments, we show our approach outperforms current techniques\nin producing longer consistent high-quality videos while offering editing\ncapabilities.\n", "rewritten_text": "Recent advancements in video generation leverage large-scale models to create realistic videos with fluid motion.  However, memory limitations necessitate generating short segments, which then require complex merging to create longer videos.  This paper introduces Anchored Diffusion, a novel method for directly synthesizing longer, seamless videos.  Our approach extends Diffusion Transformers (DiTs) into a sequence-DiT (sDiT) model that incorporates temporal information via external guidance during training on videos with randomly spaced frames. This allows for capturing both short and long-term temporal relationships.  At inference, the transformer architecture enables the generation of a batch of non-uniform video sequences anchored to a common frame, ensuring consistency across time.  We demonstrate the effectiveness of Anchored Diffusion on face reenactment, achieving superior results in generating longer, high-quality, consistent videos with enhanced editing capabilities compared to existing methods.\n"}, "2009.03116": {"original_text": "  This paper presents the first Swedish evaluation benchmark for textual\nsemantic similarity. The benchmark is compiled by simply running the English\nSTS-B dataset through the Google machine translation API. This paper discusses\npotential problems with using such a simple approach to compile a Swedish\nevaluation benchmark, including translation errors, vocabulary variation, and\nproductive compounding. Despite some obvious problems with the resulting\ndataset, we use the benchmark to compare the majority of the currently existing\nSwedish text representations, demonstrating that native models outperform\nmultilingual ones, and that simple bag of words performs remarkably well.\n", "rewritten_text": "This paper introduces the first Swedish textual semantic similarity benchmark, created by translating the English STS-B dataset using Google Translate.  While acknowledging limitations stemming from translation inaccuracies, vocabulary discrepancies, and the complexities of Swedish compounding, we utilize this benchmark to compare existing Swedish text representation models.  Our results reveal that monolingual models surpass multilingual ones, and surprisingly, a simple bag-of-words approach achieves strong performance.\n"}, "2106.16138": {"original_text": "  In this paper, we introduce ELECTRA-style tasks to cross-lingual language\nmodel pre-training. Specifically, we present two pre-training tasks, namely\nmultilingual replaced token detection, and translation replaced token\ndetection. Besides, we pretrain the model, named as XLM-E, on both multilingual\nand parallel corpora. Our model outperforms the baseline models on various\ncross-lingual understanding tasks with much less computation cost. Moreover,\nanalysis shows that XLM-E tends to obtain better cross-lingual transferability.\n", "rewritten_text": "This paper introduces ELECTRA-style pre-training tasks\u2014multilingual and translation replaced token detection\u2014for cross-lingual language models.  The resulting model, XLM-E, is pre-trained on both multilingual and parallel corpora and significantly outperforms baselines on various cross-lingual understanding tasks, while requiring substantially less computation.  Analysis further demonstrates XLM-E's improved cross-lingual transferability.\n"}, "2101.03929": {"original_text": "  Learning to capture dependencies between spatial positions is essential to\nmany visual tasks, especially the dense labeling problems like scene parsing.\nExisting methods can effectively capture long-range dependencies with\nself-attention mechanism while short ones by local convolution. However, there\nis still much gap between long-range and short-range dependencies, which\nlargely reduces the models' flexibility in application to diverse spatial\nscales and relationships in complicated natural scene images. To fill such a\ngap, we develop a Middle-Range (MR) branch to capture middle-range dependencies\nby restricting self-attention into local patches. Also, we observe that the\nspatial regions which have large correlations with others can be emphasized to\nexploit long-range dependencies more accurately, and thus propose a Reweighed\nLong-Range (RLR) branch. Based on the proposed MR and RLR branches, we build an\nOmni-Range Dependencies Network (ORDNet) which can effectively capture short-,\nmiddle- and long-range dependencies. Our ORDNet is able to extract more\ncomprehensive context information and well adapt to complex spatial variance in\nscene images. Extensive experiments show that our proposed ORDNet outperforms\nprevious state-of-the-art methods on three scene parsing benchmarks including\nPASCAL Context, COCO Stuff and ADE20K, demonstrating the superiority of\ncapturing omni-range dependencies in deep models for scene parsing task.\n", "rewritten_text": "Effective scene parsing requires capturing spatial dependencies at various scales.  While current methods leverage self-attention for long-range and convolutions for short-range dependencies, a significant gap remains.  To bridge this gap, we introduce ORDNet, a novel architecture incorporating a Middle-Range (MR) branch (local self-attention) and a Reweighted Long-Range (RLR) branch (emphasizing highly correlated regions).  ORDNet comprehensively captures short, middle, and long-range dependencies, leading to improved context understanding and adaptability to complex scenes.  Experiments on PASCAL Context, COCO Stuff, and ADE20K datasets demonstrate ORDNet's superior performance over state-of-the-art methods, highlighting the importance of omni-range dependency modeling for scene parsing.\n"}, "2012.08514": {"original_text": "  In this paper, we propose an end-end model for producing furniture layout for\ninterior scene synthesis from the random vector. This proposed model is aimed\nto support professional interior designers to produce the interior decoration\nsolutions more quickly. The proposed model combines a conditional floor-plan\nmodule of the room, a conditional graphical floor-plan module of the room and a\nconditional layout module. As compared with the prior work on scene synthesis,\nour proposed three modules enhance the ability of auto-layout generation given\nthe dimensional category of the room. We conduct our experiments on the\nproposed real-world interior layout dataset that contains $191208$ designs from\nthe professional designers. Our numerical results demonstrate that the proposed\nmodel yields higher-quality layouts in comparison with the state-of-the-art\nmodel. The dataset and code are released\n\\href{https://github.com/CODE-SUBMIT/dataset3}{Dataset,Code}\n", "rewritten_text": "This paper introduces an end-to-end model that generates furniture layouts for interior scenes from random vectors, accelerating the interior design process for professionals.  The model comprises three conditional modules: floor plan, graphical floor plan, and layout generation.  Compared to existing scene synthesis methods, our three-module approach significantly improves automatic layout generation, particularly considering room dimensions.  Evaluated on a new dataset of 191,208 professionally designed layouts, our model outperforms state-of-the-art methods. The dataset and code are publicly available at [https://github.com/CODE-SUBMIT/dataset3](https://github.com/CODE-SUBMIT/dataset3).\n"}, "2004.14525": {"original_text": "  Inverted bottleneck layers, which are built upon depthwise convolutions, have\nbeen the predominant building blocks in state-of-the-art object detection\nmodels on mobile devices. In this work, we investigate the optimality of this\ndesign pattern over a broad range of mobile accelerators by revisiting the\nusefulness of regular convolutions. We discover that regular convolutions are a\npotent component to boost the latency-accuracy trade-off for object detection\non accelerators, provided that they are placed strategically in the network via\nneural architecture search. By incorporating regular convolutions in the search\nspace and directly optimizing the network architectures for object detection,\nwe obtain a family of object detection models, MobileDets, that achieve\nstate-of-the-art results across mobile accelerators. On the COCO object\ndetection task, MobileDets outperform MobileNetV3+SSDLite by 1.7 mAP at\ncomparable mobile CPU inference latencies. MobileDets also outperform\nMobileNetV2+SSDLite by 1.9 mAP on mobile CPUs, 3.7 mAP on Google EdgeTPU, 3.4\nmAP on Qualcomm Hexagon DSP and 2.7 mAP on Nvidia Jetson GPU without increasing\nlatency. Moreover, MobileDets are comparable with the state-of-the-art MnasFPN\non mobile CPUs even without using the feature pyramid, and achieve better mAP\nscores on both EdgeTPUs and DSPs with up to 2x speedup. Code and models are\navailable in the TensorFlow Object Detection API:\nhttps://github.com/tensorflow/models/tree/master/research/object_detection.\n", "rewritten_text": "Mobile object detection models typically use inverted bottleneck layers with depthwise convolutions.  This work challenges that design by exploring the benefits of strategically placed regular convolutions, optimized via neural architecture search.  The resulting models, MobileDets, achieve state-of-the-art accuracy across various mobile accelerators.  Compared to MobileNetV3+SSDLite, MobileDets show improvements of 1.7 mAP on mobile CPUs at similar latency, and significant gains (1.9-3.7 mAP) on other accelerators (EdgeTPU, Hexagon DSP, Jetson GPU) without latency increase.  Even without a feature pyramid, MobileDets match MnasFPN performance on mobile CPUs and surpass it on EdgeTPUs and DSPs, offering up to a 2x speedup.  The code and models are publicly available via the TensorFlow Object Detection API.\n"}, "2310.00031": {"original_text": "  Diffusion models are generative models with impressive text-to-image\nsynthesis capabilities and have spurred a new wave of creative methods for\nclassical machine learning tasks. However, the best way to harness the\nperceptual knowledge of these generative models for visual tasks is still an\nopen question. Specifically, it is unclear how to use the prompting interface\nwhen applying diffusion backbones to vision tasks. We find that automatically\ngenerated captions can improve text-image alignment and significantly enhance a\nmodel's cross-attention maps, leading to better perceptual performance. Our\napproach improves upon the current state-of-the-art (SOTA) in diffusion-based\nsemantic segmentation on ADE20K and the current overall SOTA for depth\nestimation on NYUv2. Furthermore, our method generalizes to the cross-domain\nsetting. We use model personalization and caption modifications to align our\nmodel to the target domain and find improvements over unaligned baselines. Our\ncross-domain object detection model, trained on Pascal VOC, achieves SOTA\nresults on Watercolor2K. Our cross-domain segmentation method, trained on\nCityscapes, achieves SOTA results on Dark Zurich-val and Nighttime Driving.\nProject page: https://www.vision.caltech.edu/tadp/. Code:\nhttps://github.com/damaggu/TADP.\n", "rewritten_text": "Diffusion models excel at text-to-image generation, revolutionizing classical machine learning.  However, optimally leveraging their perceptual understanding for visual tasks remains a challenge, particularly regarding effective prompt engineering.  This work demonstrates that automatically generated captions significantly improve text-image alignment and cross-attention maps, boosting performance.  Our approach achieves state-of-the-art results in diffusion-based semantic segmentation (ADE20K), depth estimation (NYUv2), and generalizes effectively across domains.  Using model personalization and caption adjustments, we surpass baseline performance in cross-domain object detection (Watercolor2K, trained on Pascal VOC) and segmentation (Dark Zurich-val and Nighttime Driving, trained on Cityscapes).  See our project page ([https://www.vision.caltech.edu/tadp/](https://www.vision.caltech.edu/tadp/)) and code ([https://github.com/damaggu/TADP](https://github.com/damaggu/TADP)).\n"}, "2402.15930": {"original_text": "  The writing examples of English language learners may be different from those\nof native speakers. Given that there is a significant differences in second\nlanguage (L2) learners' error types by their proficiency levels, this paper\nattempts to reduce overcorrection by examining the interaction between LLM's\nperformance and L2 language proficiency. Our method focuses on zero-shot and\nfew-shot prompting and fine-tuning models for GEC for learners of English as a\nforeign language based on the different proficiency. We investigate GEC results\nand find that overcorrection happens primarily in advanced language learners'\nwriting (proficiency C) rather than proficiency A (a beginner level) and\nproficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot\nprompting with writing examples of English learners, actually tend to exhibit\ndecreased recall measures. To make our claim concrete, we conduct a\ncomprehensive examination of GEC outcomes and their evaluation results based on\nlanguage proficiency.\n", "rewritten_text": "This paper investigates the overcorrection tendencies of large language models (LLMs) in grammatical error correction (GEC) for English as a foreign language (EFL) learners.  We hypothesize that overcorrection varies with learner proficiency.  Using zero-shot, few-shot prompting, and fine-tuning, we analyzed LLM performance across three proficiency levels (beginner, intermediate, advanced).  Our findings show that overcorrection is most prevalent in advanced learners' writing.  Furthermore, fine-tuning LLMs, even with EFL learner examples, surprisingly reduced recall, suggesting a need for more nuanced approaches to LLM-based GEC for EFL learners.  A detailed analysis of GEC outcomes across proficiency levels supports these conclusions.\n"}, "2306.10963": {"original_text": "  Adversarial patches are still a simple yet powerful white box attack that can\nbe used to fool object detectors by suppressing possible detections. The\npatches of these so-called evasion attacks are computational expensive to\nproduce and require full access to the attacked detector. This paper addresses\nthe problem of computational expensiveness by analyzing 375 generated patches,\ncalculating the principal components of these and show, that linear\ncombinations of the resulting \"eigenpatches\" can be used to fool object\ndetections successfully.\n", "rewritten_text": "Adversarial patches, while a simple yet effective method for evading object detectors, are computationally expensive to generate and require complete knowledge of the target detector.  This paper proposes a computationally efficient alternative. By analyzing 375 generated patches and deriving their principal components (\"eigenpatches\"), we demonstrate that linear combinations of these eigenpatches can successfully fool object detectors.\n"}, "1806.03028": {"original_text": "  Vehicle Make and Model Recognition (MMR) systems provide a fully automatic\nframework to recognize and classify different vehicle models. Several\napproaches have been proposed to address this challenge, however they can\nperform in restricted conditions. Here, we formulate the vehicle make and model\nrecognition as a fine-grained classification problem and propose a new\nconfigurable on-road vehicle make and model recognition framework. We benefit\nfrom the unsupervised feature learning methods and in more details we employ\nLocality constraint Linear Coding (LLC) method as a fast feature encoder for\nencoding the input SIFT features. The proposed method can perform in real\nenvironments of different conditions. This framework can recognize fifty models\nof vehicles and has an advantage to classify every other vehicle not belonging\nto one of the specified fifty classes as an unknown vehicle. The proposed MMR\nframework can be configured to become faster or more accurate based on the\napplication domain. The proposed approach is examined on two datasets including\nIranian on-road vehicle dataset and CompuCar dataset. The Iranian on-road\nvehicle dataset contains images of 50 models of vehicles captured in real\nsituations by traffic cameras in different weather and lighting conditions.\nExperimental results show superiority of the proposed framework over the\nstate-of-the-art methods on Iranian on-road vehicle datatset and comparable\nresults on CompuCar dataset with 97.5% and 98.4% accuracies, respectively.\n", "rewritten_text": "This paper introduces a novel, configurable framework for vehicle make and model recognition (MMR), addressing limitations of existing methods.  We frame MMR as a fine-grained classification problem, leveraging unsupervised feature learning, specifically Locality-Constrained Linear Coding (LLC), to efficiently encode SIFT features.  Our framework robustly performs in diverse real-world conditions, identifying 50 specific vehicle models and classifying others as \"unknown.\"  Its speed and accuracy are configurable, adapting to application needs.  Evaluated on the Iranian on-road vehicle dataset (50 models, diverse weather/lighting) and the CompuCar dataset, our framework achieves state-of-the-art performance (97.5% accuracy) on the former and comparable results (98.4% accuracy) on the latter.\n"}, "2110.10575": {"original_text": "  Recent research in opinion mining proposed word embedding-based topic\nmodeling methods that provide superior coherence compared to traditional topic\nmodeling. In this paper, we demonstrate how these methods can be used to\ndisplay correlated topic models on social media texts using SocialVisTUM, our\nproposed interactive visualization toolkit. It displays a graph with topics as\nnodes and their correlations as edges. Further details are displayed\ninteractively to support the exploration of large text collections, e.g.,\nrepresentative words and sentences of topics, topic and sentiment\ndistributions, hierarchical topic clustering, and customizable, predefined\ntopic labels. The toolkit optimizes automatically on custom data for optimal\ncoherence. We show a working instance of the toolkit on data crawled from\nEnglish social media discussions about organic food consumption. The\nvisualization confirms findings of a qualitative consumer research study.\nSocialVisTUM and its training procedures are accessible online.\n", "rewritten_text": "This paper introduces SocialVisTUM, an interactive visualization toolkit for exploring correlated topic models in social media data.  Leveraging recent advancements in word embedding-based topic modeling, SocialVisTUM displays topics as nodes in a graph, with edges representing correlations.  The toolkit provides interactive exploration features, including representative words and sentences, sentiment and topic distributions, hierarchical clustering, and customizable topic labels.  SocialVisTUM automatically optimizes for coherence on custom datasets.  We demonstrate its effectiveness using English social media data on organic food consumption, validating findings from a parallel qualitative consumer study.  The toolkit and its training procedures are publicly available online.\n"}, "2404.00851": {"original_text": "  Pre-trained vision-language models have shown impressive success on various\ncomputer vision tasks with their zero-shot generalizability. Recently, prompt\nlearning approaches have been explored to efficiently and effectively adapt the\nvision-language models to a variety of downstream tasks. However, most existing\nprompt learning methods suffer from task overfitting since the general\nknowledge of the pre-trained vision language models is forgotten while the\nprompts are finetuned on a small data set from a specific target task. To\naddress this issue, we propose a Prompt Meta-Regularization (ProMetaR) to\nimprove the generalizability of prompt learning for vision-language models.\nSpecifically, ProMetaR meta-learns both the regularizer and the soft prompts to\nharness the task-specific knowledge from the downstream tasks and task-agnostic\ngeneral knowledge from the vision-language models. Further, ProMetaR augments\nthe task to generate multiple virtual tasks to alleviate the meta-overfitting.\nIn addition, we provide the analysis to comprehend how ProMetaR improves the\ngeneralizability of prompt tuning in the perspective of the gradient alignment.\nOur extensive experiments demonstrate that our ProMetaR improves the\ngeneralizability of conventional prompt learning methods under\nbase-to-base/base-to-new and domain generalization settings. The code of\nProMetaR is available at https://github.com/mlvlab/ProMetaR.\n", "rewritten_text": "Pre-trained vision-language models excel at computer vision tasks due to their zero-shot capabilities.  While prompt learning effectively adapts these models to new tasks, existing methods often overfit, losing the models' general knowledge.  To overcome this, we introduce Prompt Meta-Regularization (ProMetaR).  ProMetaR meta-learns both a regularizer and soft prompts, balancing task-specific and general knowledge.  It further mitigates overfitting by generating virtual tasks.  We analyze ProMetaR's improved generalizability through the lens of gradient alignment.  Extensive experiments, across base-to-base/base-to-new and domain generalization settings, demonstrate ProMetaR's superior performance.  The code is available at https://github.com/mlvlab/ProMetaR.\n"}, "2409.12539": {"original_text": "  In radiation therapy (RT), the reliance on pre-treatment computed tomography\n(CT) images encounter challenges due to anatomical changes, necessitating\nadaptive planning. Daily cone-beam CT (CBCT) imaging, pivotal for therapy\nadjustment, falls short in tissue density accuracy. To address this, our\ninnovative approach integrates diffusion models for CT image generation,\noffering precise control over data synthesis. Leveraging a self-training method\nwith knowledge distillation, we maximize CBCT data during therapy, complemented\nby sparse paired fan-beam CTs. This strategy, incorporated into\nstate-of-the-art diffusion-based models, surpasses conventional methods like\nPix2pix and CycleGAN. A meticulously curated dataset of 2800 paired CBCT and CT\nscans, supplemented by 4200 CBCT scans, undergoes preprocessing and teacher\nmodel training, including the Brownian Bridge Diffusion Model (BBDM).\nPseudo-label CT images are generated, resulting in a dataset combining 5600 CT\nimages with corresponding CBCT images. Thorough evaluation using MSE, SSIM,\nPSNR and LPIPS demonstrates superior performance against Pix2pix and CycleGAN.\nOur approach shows promise in generating high-quality CT images from CBCT scans\nin RT.\n", "rewritten_text": "Adaptive radiation therapy (RT) requires accurate imaging, but daily cone-beam CT (CBCT) scans lack the tissue density precision of pre-treatment CTs.  To overcome this, we developed a novel method using diffusion models to generate high-quality synthetic CT images from CBCT scans.  Our self-training approach, incorporating knowledge distillation and leveraging a sparse set of paired fan-beam CTs, maximizes the use of CBCT data acquired during treatment.  Trained on a large dataset (2800 paired CBCT/CT scans + 4200 CBCT scans) using a Brownian Bridge Diffusion Model (BBDM), our method outperforms existing techniques like Pix2Pix and CycleGAN, as demonstrated by superior MSE, SSIM, PSNR, and LPIPS scores. This approach promises to significantly improve image quality in adaptive RT.\n"}, "1904.0169": {"original_text": "  We present MonoPSR, a monocular 3D object detection method that leverages\nproposals and shape reconstruction. First, using the fundamental relations of a\npinhole camera model, detections from a mature 2D object detector are used to\ngenerate a 3D proposal per object in a scene. The 3D location of these\nproposals prove to be quite accurate, which greatly reduces the difficulty of\nregressing the final 3D bounding box detection. Simultaneously, a point cloud\nis predicted in an object centered coordinate system to learn local scale and\nshape information. However, the key challenge is how to exploit shape\ninformation to guide 3D localization. As such, we devise aggregate losses,\nincluding a novel projection alignment loss, to jointly optimize these tasks in\nthe neural network to improve 3D localization accuracy. We validate our method\non the KITTI benchmark where we set new state-of-the-art results among\npublished monocular methods, including the harder pedestrian and cyclist\nclasses, while maintaining efficient run-time.\n", "rewritten_text": "MonoPSR is a novel monocular 3D object detection method that improves accuracy by combining 2D detection proposals with 3D shape reconstruction.  It generates accurate 3D proposals from 2D detections using a pinhole camera model, simplifying the subsequent 3D bounding box regression.  Simultaneously, it predicts object-centered point clouds to capture shape and scale.  To effectively integrate shape information for improved 3D localization, we introduce aggregate losses, including a novel projection alignment loss.  Our method achieves state-of-the-art results on the KITTI benchmark, surpassing existing monocular approaches, particularly for challenging pedestrian and cyclist detection, while maintaining computational efficiency.\n"}, "2408.02291": {"original_text": "  Unsupervised 3D keypoints estimation from Point Cloud Data (PCD) is a complex\ntask, even more challenging when an object shape is deforming. As keypoints\nshould be semantically and geometrically consistent across all the 3D frames -\neach keypoint should be anchored to a specific part of the deforming shape\nirrespective of intrinsic and extrinsic motion. This paper presents, \"SelfGeo\",\na self-supervised method that computes persistent 3D keypoints of non-rigid\nobjects from arbitrary PCDs without the need of human annotations. The gist of\nSelfGeo is to estimate keypoints between frames that respect invariant\nproperties of deforming bodies. Our main contribution is to enforce that\nkeypoints deform along with the shape while keeping constant geodesic distances\namong them. This principle is then propagated to the design of a set of losses\nwhich minimization let emerge repeatable keypoints in specific semantic\nlocations of the non-rigid shape. We show experimentally that the use of\ngeodesic has a clear advantage in challenging dynamic scenes and with different\nclasses of deforming shapes (humans and animals). Code and data are available\nat: https://github.com/IIT-PAVIS/SelfGeo\n", "rewritten_text": "Estimating consistent 3D keypoints from point cloud data (PCD) of deforming objects is a difficult problem.  This paper introduces SelfGeo, a self-supervised method that addresses this challenge.  SelfGeo identifies persistent 3D keypoints on non-rigid objects across multiple PCD frames without human annotation.  It achieves this by enforcing keypoint correspondence across frames, maintaining constant geodesic distances between keypoints as the object deforms.  This constraint is incorporated into a loss function, resulting in repeatable keypoints at semantically meaningful locations.  Experiments demonstrate SelfGeo's effectiveness on diverse deforming shapes (humans and animals) in dynamic scenes, leveraging the advantages of geodesic distance preservation.  Code and data are available at: https://github.com/IIT-PAVIS/SelfGeo\n"}, "2304.0723": {"original_text": "  Pedestrian attribute recognition (PAR) has received increasing attention\nbecause of its wide application in video surveillance and pedestrian analysis.\nExtracting robust feature representation is one of the key challenges in this\ntask. The existing methods mainly use the convolutional neural network (CNN) as\nthe backbone network to extract features. However, these methods mainly focus\non small discriminative regions while ignoring the global perspective. To\novercome these limitations, we propose a pure transformer-based multi-task PAR\nnetwork named PARFormer, which includes four modules. In the feature extraction\nmodule, we build a transformer-based strong baseline for feature extraction,\nwhich achieves competitive results on several PAR benchmarks compared with the\nexisting CNN-based baseline methods. In the feature processing module, we\npropose an effective data augmentation strategy named batch random mask (BRM)\nblock to reinforce the attentive feature learning of random patches.\nFurthermore, we propose a multi-attribute center loss (MACL) to enhance the\ninter-attribute discriminability in the feature representations. In the\nviewpoint perception module, we explore the impact of viewpoints on pedestrian\nattributes, and propose a multi-view contrastive loss (MCVL) that enables the\nnetwork to exploit the viewpoint information. In the attribute recognition\nmodule, we alleviate the negative-positive imbalance problem to generate the\nattribute predictions. The above modules interact and jointly learn a highly\ndiscriminative feature space, and supervise the generation of the final\nfeatures. Extensive experimental results show that the proposed PARFormer\nnetwork performs well compared to the state-of-the-art methods on several\npublic datasets, including PETA, RAP, and PA100K. Code will be released at\nhttps://github.com/xwf199/PARFormer.\n", "rewritten_text": "Pedestrian attribute recognition (PAR) is increasingly important for video surveillance and pedestrian analysis.  A key challenge is robust feature extraction, where current Convolutional Neural Network (CNN)-based methods often neglect global context in favor of small, discriminative regions.  To address this, we introduce PARFormer, a novel multi-task PAR network built entirely on transformers.  PARFormer comprises four modules: a transformer-based feature extraction module (achieving state-of-the-art results on benchmark datasets); a feature processing module employing a novel Batch Random Mask (BRM) data augmentation strategy; a multi-attribute center loss (MACL) module to improve inter-attribute discrimination; and a multi-view contrastive loss (MCVL) module leveraging viewpoint information.  These modules work together to learn a highly discriminative feature space, mitigating class imbalance and improving attribute prediction accuracy.  Extensive experiments on PETA, RAP, and PA100K datasets demonstrate PARFormer's superior performance.  Code is available at https://github.com/xwf199/PARFormer.\n"}, "2402.09967": {"original_text": "  Large Language Models (LLMs) excel in generating personalized content and\nfacilitating interactive dialogues, showcasing their remarkable aptitude for a\nmyriad of applications. However, their capabilities in reasoning and providing\nexplainable outputs, especially within the context of reasoning abilities,\nremain areas for improvement. In this study, we delve into the reasoning\nabilities of LLMs, highlighting the current challenges and limitations that\nhinder their effectiveness in complex reasoning scenarios.\n", "rewritten_text": "Large language models (LLMs) are adept at personalized content creation and interactive conversations, demonstrating broad applicability.  However, their reasoning capabilities and ability to provide understandable explanations for their outputs need significant improvement. This study examines the limitations of LLMs in complex reasoning tasks, exploring the challenges that hinder their performance.\n"}, "2212.05911": {"original_text": "  Deep learning has emerged as an effective solution for solving the task of\nobject detection in images but at the cost of requiring large labeled datasets.\nTo mitigate this cost, semi-supervised object detection methods, which consist\nin leveraging abundant unlabeled data, have been proposed and have already\nshown impressive results. However, most of these methods require linking a\npseudo-label to a ground-truth object by thresholding. In previous works, this\nthreshold value is usually determined empirically, which is time consuming, and\nonly done for a single data distribution. When the domain, and thus the data\ndistribution, changes, a new and costly parameter search is necessary. In this\nwork, we introduce our method Adaptive Self-Training for Object Detection\n(ASTOD), which is a simple yet effective teacher-student method. ASTOD\ndetermines without cost a threshold value based directly on the ground value of\nthe score histogram. To improve the quality of the teacher predictions, we also\npropose a novel pseudo-labeling procedure. We use different views of the\nunlabeled images during the pseudo-labeling step to reduce the number of missed\npredictions and thus obtain better candidate labels. Our teacher and our\nstudent are trained separately, and our method can be used in an iterative\nfashion by replacing the teacher by the student. On the MS-COCO dataset, our\nmethod consistently performs favorably against state-of-the-art methods that do\nnot require a threshold parameter, and shows competitive results with methods\nthat require a parameter sweep search. Additional experiments with respect to a\nsupervised baseline on the DIOR dataset containing satellite images lead to\nsimilar conclusions, and prove that it is possible to adapt the score threshold\nautomatically in self-training, regardless of the data distribution. The code\nis available at https:// github.com/rvandeghen/ASTOD\n", "rewritten_text": "Deep learning excels at object detection but demands extensive labeled data.  Semi-supervised methods, utilizing unlabeled data, offer a solution, but often rely on empirically determined thresholds for pseudo-labeling, a time-consuming process sensitive to data distribution changes.  This paper introduces Adaptive Self-Training for Object Detection (ASTOD), a simple yet effective teacher-student approach that automatically determines this threshold using the score histogram.  ASTOD also incorporates a novel pseudo-labeling procedure using multiple image views to improve prediction accuracy.  Trained iteratively, ASTOD outperforms comparable threshold-free methods and achieves competitive results against methods requiring parameter sweeps on the MS-COCO dataset.  Experiments on the DIOR satellite imagery dataset further demonstrate ASTOD's adaptability across different data distributions.  Code is available at https://github.com/rvandeghen/ASTOD.\n"}, "1507.02772": {"original_text": "  Data encoded as symmetric positive definite (SPD) matrices frequently arise\nin many areas of computer vision and machine learning. While these matrices\nform an open subset of the Euclidean space of symmetric matrices, viewing them\nthrough the lens of non-Euclidean Riemannian geometry often turns out to be\nbetter suited in capturing several desirable data properties. However,\nformulating classical machine learning algorithms within such a geometry is\noften non-trivial and computationally expensive. Inspired by the great success\nof dictionary learning and sparse coding for vector-valued data, our goal in\nthis paper is to represent data in the form of SPD matrices as sparse conic\ncombinations of SPD atoms from a learned dictionary via a Riemannian geometric\napproach. To that end, we formulate a novel Riemannian optimization objective\nfor dictionary learning and sparse coding in which the representation loss is\ncharacterized via the affine invariant Riemannian metric. We also present a\ncomputationally simple algorithm for optimizing our model. Experiments on\nseveral computer vision datasets demonstrate superior classification and\nretrieval performance using our approach when compared to sparse coding via\nalternative non-Riemannian formulations.\n", "rewritten_text": "Many computer vision and machine learning applications generate data represented as symmetric positive definite (SPD) matrices.  While these matrices are technically part of Euclidean space, a Riemannian geometric approach better captures their inherent properties.  However, adapting standard machine learning algorithms to this geometry is challenging.  This paper proposes a novel Riemannian geometric method for dictionary learning and sparse coding of SPD matrix data.  We formulate a new optimization objective using the affine invariant Riemannian metric to represent the data as sparse conic combinations of learned SPD matrix atoms.  Our computationally efficient algorithm, evaluated on several computer vision datasets, demonstrates improved classification and retrieval performance compared to non-Riemannian sparse coding methods.\n"}, "1603.09742": {"original_text": "  Semantic segmentation is critical to image content understanding and object\nlocalization. Recent development in fully-convolutional neural network (FCN)\nhas enabled accurate pixel-level labeling. One issue in previous works is that\nthe FCN based method does not exploit the object boundary information to\ndelineate segmentation details since the object boundary label is ignored in\nthe network training. To tackle this problem, we introduce a double branch\nfully convolutional neural network, which separates the learning of the\ndesirable semantic class labeling with mask-level object proposals guided by\nrelabeled boundaries. This network, called object boundary guided FCN\n(OBG-FCN), is able to integrate the distinct properties of object shape and\nclass features elegantly in a fully convolutional way with a designed masking\narchitecture. We conduct experiments on the PASCAL VOC segmentation benchmark,\nand show that the end-to-end trainable OBG-FCN system offers great improvement\nin optimizing the target semantic segmentation quality.\n", "rewritten_text": "Accurate pixel-level image segmentation, crucial for understanding image content and object location, has been advanced by fully convolutional networks (FCNs).  However, existing FCN methods often neglect object boundary information, hindering detailed segmentation.  To address this, we propose OBG-FCN, a novel double-branch FCN architecture.  OBG-FCN separates semantic class labeling from mask-level object proposals guided by refined boundary information, elegantly integrating object shape and class features within a fully convolutional framework.  Experiments on the PASCAL VOC dataset demonstrate OBG-FCN's significant improvement in semantic segmentation accuracy.\n"}, "2004.03677": {"original_text": "  Image manipulation can be considered a special case of image generation where\nthe image to be produced is a modification of an existing image. Image\ngeneration and manipulation have been, for the most part, tasks that operate on\nraw pixels. However, the remarkable progress in learning rich image and object\nrepresentations has opened the way for tasks such as text-to-image or\nlayout-to-image generation that are mainly driven by semantics. In our work, we\naddress the novel problem of image manipulation from scene graphs, in which a\nuser can edit images by merely applying changes in the nodes or edges of a\nsemantic graph that is generated from the image. Our goal is to encode image\ninformation in a given constellation and from there on generate new\nconstellations, such as replacing objects or even changing relationships\nbetween objects, while respecting the semantics and style from the original\nimage. We introduce a spatio-semantic scene graph network that does not require\ndirect supervision for constellation changes or image edits. This makes it\npossible to train the system from existing real-world datasets with no\nadditional annotation effort.\n", "rewritten_text": "Image manipulation is a form of image generation focused on modifying existing images.  Traditionally, both image generation and manipulation operated directly on pixels.  Recent advancements in semantic image representations, however, enable generation driven by high-level concepts (e.g., text-to-image).  This work introduces a novel approach: image manipulation via scene graphs.  Users edit images by modifying a semantically-rich graph representing the image's content, allowing for object replacement and relationship changes while preserving the original image's style and semantics.  Our spatio-semantic scene graph network requires no explicit supervision for these edits, enabling training on readily available real-world datasets without extra annotation.\n"}, "2208.09669": {"original_text": "  Contextualized word embeddings in language models have given much advance to\nNLP. Intuitively, sentential information is integrated into the representation\nof words, which can help model polysemy. However, context sensitivity also\nleads to the variance of representations, which may break the semantic\nconsistency for synonyms. We quantify how much the contextualized embeddings of\neach word sense vary across contexts in typical pre-trained models. Results\nshow that contextualized embeddings can be highly consistent across contexts.\nIn addition, part-of-speech, number of word senses, and sentence length have an\ninfluence on the variance of sense representations. Interestingly, we find that\nword representations are position-biased, where the first words in different\ncontexts tend to be more similar. We analyze such a phenomenon and also propose\na simple way to alleviate such bias in distance-based word sense disambiguation\nsettings.\n", "rewritten_text": "Recent advancements in NLP are largely due to contextualized word embeddings in language models.  While these embeddings effectively capture word meaning within sentences (addressing polysemy), their context-sensitivity can lead to inconsistent representations for synonyms.  We measured this contextual variance in several pre-trained models, finding surprisingly high consistency across contexts for many word senses.  However, factors like part-of-speech, word sense ambiguity, and sentence length do influence this variance.  Furthermore, we observed a positional bias, with initial words exhibiting greater similarity across contexts.  This phenomenon is analyzed, and a simple method for mitigating its impact on distance-based word sense disambiguation is proposed.\n"}, "1706.00842": {"original_text": "  We present a fully automatic method employing convolutional neural networks\nbased on the 2D U-net architecture and random forest classifier to solve the\nautomatic liver lesion segmentation problem of the ISBI 2017 Liver Tumor\nSegmentation Challenge (LiTS). In order to constrain the ROI in which the\ntumors could be located, a liver segmentation is performed first. For the organ\nsegmentation, an ensemble of convolutional networks is trained to segment a\nliver using a set of 179 liver CT datasets from liver surgery planning. Inside\nof the liver ROI a neural network, trained using 127 challenge training\ndatasets, identifies tumor candidates, which are subsequently filtered with a\nrandom forest classifier yielding the final tumor segmentation. The evaluation\non the 70 challenge test cases resulted in a mean Dice coefficient of 0.65,\nranking our method in the second place.\n", "rewritten_text": "This paper introduces a fully automated liver lesion segmentation method for the ISBI 2017 LiTS challenge.  Our approach uses a two-stage process: first, an ensemble of convolutional neural networks segments the liver from 179 CT scans; then, within this region of interest, a U-Net-based convolutional neural network, trained on 127 challenge datasets, identifies tumor candidates.  A random forest classifier refines these candidates, producing the final segmentation.  Evaluation on 70 test cases achieved a mean Dice coefficient of 0.65, securing second place in the challenge.\n"}, "2108.07848": {"original_text": "  Identifying players in sports videos by recognizing their jersey numbers is a\nchallenging task in computer vision. We have designed and implemented a\nmulti-task learning network for jersey number recognition. In order to train a\nnetwork to recognize jersey numbers, two output label representations are used\n(1) Holistic - considers the entire jersey number as one class, and (2)\nDigit-wise - considers the two digits in a jersey number as two separate\nclasses. The proposed network learns both holistic and digit-wise\nrepresentations through a multi-task loss function. We determine the optimal\nweights to be assigned to holistic and digit-wise losses through an ablation\nstudy. Experimental results demonstrate that the proposed multi-task learning\nnetwork performs better than the constituent holistic and digit-wise\nsingle-task learning networks.\n", "rewritten_text": "This paper presents a novel multi-task learning network for robust jersey number recognition in sports videos, a challenging computer vision problem.  The network simultaneously learns holistic (entire number as a single class) and digit-wise (each digit as a separate class) representations of jersey numbers, using a multi-task loss function with optimally weighted components determined via ablation study.  Experimental results show that this approach outperforms single-task networks trained on either holistic or digit-wise representations alone.\n"}, "2104.01136": {"original_text": "  We design a family of image classification architectures that optimize the\ntrade-off between accuracy and efficiency in a high-speed regime. Our work\nexploits recent findings in attention-based architectures, which are\ncompetitive on highly parallel processing hardware. We revisit principles from\nthe extensive literature on convolutional neural networks to apply them to\ntransformers, in particular activation maps with decreasing resolutions. We\nalso introduce the attention bias, a new way to integrate positional\ninformation in vision transformers. As a result, we propose LeVIT: a hybrid\nneural network for fast inference image classification. We consider different\nmeasures of efficiency on different hardware platforms, so as to best reflect a\nwide range of application scenarios. Our extensive experiments empirically\nvalidate our technical choices and show they are suitable to most\narchitectures. Overall, LeViT significantly outperforms existing convnets and\nvision transformers with respect to the speed/accuracy tradeoff. For example,\nat 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on\nCPU. We release the code at https://github.com/facebookresearch/LeViT\n", "rewritten_text": "LeViT is a novel hybrid neural network designed for fast image classification.  Leveraging advancements in attention-based architectures and convolutional neural networks, including a novel attention bias for positional information and decreasing resolution activation maps, LeViT achieves a superior speed-accuracy trade-off.  Evaluated across diverse hardware platforms, LeViT significantly outperforms existing convnets and vision transformers; for instance, it's five times faster than EfficientNet on a CPU while maintaining 80% ImageNet top-1 accuracy.  The code is publicly available at https://github.com/facebookresearch/LeViT.\n"}, "2103.02984": {"original_text": "  Abrupt motion of camera or objects in a scene result in a blurry video, and\ntherefore recovering high quality video requires two types of enhancements:\nvisual enhancement and temporal upsampling. A broad range of research attempted\nto recover clean frames from blurred image sequences or temporally upsample\nframes by interpolation, yet there are very limited studies handling both\nproblems jointly. In this work, we present a novel framework for deblurring,\ninterpolating and extrapolating sharp frames from a motion-blurred video in an\nend-to-end manner. We design our framework by first learning the pixel-level\nmotion that caused the blur from the given inputs via optical flow estimation\nand then predict multiple clean frames by warping the decoded features with the\nestimated flows. To ensure temporal coherence across predicted frames and\naddress potential temporal ambiguity, we propose a simple, yet effective\nflow-based rule. The effectiveness and favorability of our approach are\nhighlighted through extensive qualitative and quantitative evaluations on\nmotion-blurred datasets from high speed videos.\n", "rewritten_text": "Blurry videos, caused by abrupt camera or object movement, require both visual enhancement and temporal upsampling for restoration. While existing research addresses these individually, few tackle them jointly.  This work introduces a novel end-to-end framework for deblurring, interpolating, and extrapolating sharp frames from motion-blurred videos.  We estimate pixel-level motion via optical flow, then warp decoded features using these flows to predict multiple clean frames.  A novel flow-based rule ensures temporal consistency.  Extensive qualitative and quantitative evaluations on high-speed video datasets demonstrate the effectiveness of our approach.\n"}, "1611.09932": {"original_text": "  Compared to earlier multistage frameworks using CNN features, recent\nend-to-end deep approaches for fine-grained recognition essentially enhance the\nmid-level learning capability of CNNs. Previous approaches achieve this by\nintroducing an auxiliary network to infuse localization information into the\nmain classification network, or a sophisticated feature encoding method to\ncapture higher order feature statistics. We show that mid-level representation\nlearning can be enhanced within the CNN framework, by learning a bank of\nconvolutional filters that capture class-specific discriminative patches\nwithout extra part or bounding box annotations. Such a filter bank is well\nstructured, properly initialized and discriminatively learned through a novel\nasymmetric multi-stream architecture with convolutional filter supervision and\na non-random layer initialization. Experimental results show that our approach\nachieves state-of-the-art on three publicly available fine-grained recognition\ndatasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and\nvisualizations are provided to understand our approach.\n", "rewritten_text": "Recent end-to-end deep learning methods for fine-grained image recognition significantly improve upon earlier multi-stage CNN-based approaches by enhancing mid-level feature learning within the CNN itself.  Unlike previous methods that relied on auxiliary networks for localization or complex feature encoding, our approach learns a bank of class-specific convolutional filters that identify discriminative image patches without requiring part or bounding box annotations.  This filter bank is effectively structured, initialized, and trained using a novel asymmetric multi-stream architecture with convolutional filter supervision and non-random layer initialization.  Our method achieves state-of-the-art results on three benchmark datasets (CUB-200-2011, Stanford Cars, and FGVC-Aircraft), as demonstrated by experimental results, ablation studies, and visualizations.\n"}, "1504.01013": {"original_text": "  Recent advances in semantic image segmentation have mostly been achieved by\ntraining deep convolutional neural networks (CNNs). We show how to improve\nsemantic segmentation through the use of contextual information; specifically,\nwe explore `patch-patch' context between image regions, and `patch-background'\ncontext. For learning from the patch-patch context, we formulate Conditional\nRandom Fields (CRFs) with CNN-based pairwise potential functions to capture\nsemantic correlations between neighboring patches. Efficient piecewise training\nof the proposed deep structured model is then applied to avoid repeated\nexpensive CRF inference for back propagation. For capturing the\npatch-background context, we show that a network design with traditional\nmulti-scale image input and sliding pyramid pooling is effective for improving\nperformance. Our experimental results set new state-of-the-art performance on a\nnumber of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC\n2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an\nintersection-over-union score of 78.0 on the challenging PASCAL VOC 2012\ndataset.\n", "rewritten_text": "This paper advances semantic image segmentation by leveraging contextual information.  We improve upon existing deep convolutional neural network (CNN) approaches by incorporating both \"patch-patch\" and \"patch-background\" context.  Patch-patch context is modeled using Conditional Random Fields (CRFs) with CNN-based pairwise potentials, efficiently trained via piecewise methods to avoid computationally expensive backpropagation.  Patch-background context is captured using a multi-scale network architecture with sliding pyramid pooling.  Our method achieves state-of-the-art results on several benchmark datasets (NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow), including a 78.0% intersection-over-union score on the challenging PASCAL VOC 2012 dataset.\n"}, "2106.1262": {"original_text": "  The self-attention-based model, transformer, is recently becoming the leading\nbackbone in the field of computer vision. In spite of the impressive success\nmade by transformers in a variety of vision tasks, it still suffers from heavy\ncomputation and intensive memory costs. To address this limitation, this paper\npresents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).\nWe start by observing a large amount of redundant computation, mainly spent on\nuncorrelated input patches, and then introduce an interpretable module to\ndynamically and gracefully drop these redundant patches. This novel framework\nis then extended to a hierarchical structure, where uncorrelated tokens at\ndifferent stages are gradually removed, resulting in a considerable shrinkage\nof computational cost. We include extensive experiments on both image and video\ntasks, where our method could deliver up to 1.4x speed-up for state-of-the-art\nmodels like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy.\nMore importantly, contrary to other acceleration approaches, our method is\ninherently interpretable with substantial visual evidence, making vision\ntransformer closer to a more human-understandable architecture while being\nlighter. We demonstrate that the interpretability that naturally emerged in our\nframework can outperform the raw attention learned by the original visual\ntransformer, as well as those generated by off-the-shelf interpretation\nmethods, with both qualitative and quantitative results. Project Page:\nhttp://people.csail.mit.edu/bpan/ia-red/.\n", "rewritten_text": "Transformers are revolutionizing computer vision, but their high computational cost remains a challenge.  This paper introduces IA-RED\u00b2, an interpretability-aware redundancy reduction framework, that addresses this limitation.  By identifying and selectively discarding computations on uncorrelated image patches, IA-RED\u00b2 achieves significant speed improvements (up to 1.4x) in state-of-the-art models like DeiT and TimeSformer, with minimal accuracy loss (less than 0.7%).  Unlike other acceleration techniques, IA-RED\u00b2's inherent interpretability, demonstrated through visual evidence, enhances the model's transparency and surpasses the performance of existing interpretation methods.  This work makes vision transformers more efficient and human-understandable.  See the project page for details: http://people.csail.mit.edu/bpan/ia-red/.\n"}, "1803.03852": {"original_text": "  Tracking the pose of instruments is a central problem in image-guided\nsurgery. For microscopic scenarios, optical coherence tomography (OCT) is\nincreasingly used as an imaging modality. OCT is suitable for accurate pose\nestimation due to its micrometer range resolution and volumetric field of view.\nHowever, OCT image processing is challenging due to speckle noise and\nreflection artifacts in addition to the images' 3D nature. We address pose\nestimation from OCT volume data with a new deep learning-based tracking\nframework. For this purpose, we design a new 3D convolutional neural network\n(CNN) architecture to directly predict the 6D pose of a small marker geometry\nfrom OCT volumes. We use a hexapod robot to automatically acquire labeled data\npoints which we use to train 3D CNN architectures for multi-output regression.\nWe use this setup to provide an in-depth analysis on deep learning-based pose\nestimation from volumes. Specifically, we demonstrate that exploiting volume\ninformation for pose estimation yields higher accuracy than relying on 2D\nrepresentations with depth information. Supporting this observation, we provide\nquantitative and qualitative results that 3D CNNs effectively exploit the depth\nstructure of marker objects. Regarding the deep learning aspect, we present\nefficient design principles for 3D CNNs, making use of insights from the 2D\ndeep learning community. In particular, we present Inception3D as a new\narchitecture which performs best for our application. We show that our deep\nlearning approach reaches errors at our ground-truth label's resolution. We\nachieve a mean average error of $\\SI{14.89 \\pm 9.3}{\\micro\\metre}$ and\n$\\SI{0.096 \\pm 0.072}{\\degree}$ for position and orientation learning,\nrespectively.\n", "rewritten_text": "Accurate instrument pose tracking is crucial in image-guided microsurgery, where optical coherence tomography (OCT) is gaining popularity.  OCT's high resolution and volumetric imaging capabilities are ideal for pose estimation, but its 3D data is hampered by speckle noise and artifacts.  This work introduces a novel deep learning framework for 6D pose estimation from OCT volumes.  A custom 3D convolutional neural network (CNN), Inception3D, directly predicts the pose of a small marker from the volumetric data.  A hexapod robot facilitates automated data acquisition for training the network via multi-output regression.  Our results demonstrate that using 3D volume information significantly improves pose estimation accuracy compared to 2D methods with depth.  Quantitative and qualitative analyses confirm the effectiveness of 3D CNNs in exploiting the marker's depth structure.  Inception3D achieves state-of-the-art performance, reaching the ground truth resolution with a mean average error of 14.89 \u00b1 9.3 \u00b5m for position and 0.096 \u00b1 0.072\u00b0 for orientation.  This work also contributes efficient 3D CNN design principles, leveraging insights from the 2D deep learning field.\n"}, "2104.04986": {"original_text": "  Aspect-based Sentiment Analysis (ABSA), aiming at predicting the polarities\nfor aspects, is a fine-grained task in the field of sentiment analysis.\nPrevious work showed syntactic information, e.g. dependency trees, can\neffectively improve the ABSA performance. Recently, pre-trained models (PTMs)\nalso have shown their effectiveness on ABSA. Therefore, the question naturally\narises whether PTMs contain sufficient syntactic information for ABSA so that\nwe can obtain a good ABSA model only based on PTMs. In this paper, we firstly\ncompare the induced trees from PTMs and the dependency parsing trees on several\npopular models for the ABSA task, showing that the induced tree from fine-tuned\nRoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis\nexperiments reveal that the FT-RoBERTa Induced Tree is more\nsentiment-word-oriented and could benefit the ABSA task. The experiments also\nshow that the pure RoBERTa-based model can outperform or approximate to the\nprevious SOTA performances on six datasets across four languages since it\nimplicitly incorporates the task-oriented syntactic information.\n", "rewritten_text": "Aspect-based sentiment analysis (ABSA) is a challenging task focused on determining sentiment polarity towards specific aspects. While previous work leveraged syntactic information like dependency trees to improve ABSA performance, the rise of pre-trained models (PTMs) raises the question of whether they implicitly capture sufficient syntactic information for effective ABSA.  This paper compares dependency trees generated by PTMs with those from traditional parsers.  We find that trees induced from a fine-tuned RoBERTa model (FT-RoBERTa) surpass parser-generated trees, exhibiting a stronger focus on sentiment words and thus improving ABSA performance.  Furthermore, our experiments demonstrate that a purely RoBERTa-based model achieves state-of-the-art or comparable results on six datasets across four languages, suggesting that it effectively incorporates task-relevant syntactic information.\n"}, "2012.11581": {"original_text": "  Humans live within a 3D space and constantly interact with it to perform\ntasks. Such interactions involve physical contact between surfaces that is\nsemantically meaningful. Our goal is to learn how humans interact with scenes\nand leverage this to enable virtual characters to do the same. To that end, we\nintroduce a novel Human-Scene Interaction (HSI) model that encodes proximal\nrelationships, called POSA for \"Pose with prOximitieS and contActs\". The\nrepresentation of interaction is body-centric, which enables it to generalize\nto new scenes. Specifically, POSA augments the SMPL-X parametric human body\nmodel such that, for every mesh vertex, it encodes (a) the contact probability\nwith the scene surface and (b) the corresponding semantic scene label. We learn\nPOSA with a VAE conditioned on the SMPL-X vertices, and train on the PROX\ndataset, which contains SMPL-X meshes of people interacting with 3D scenes, and\nthe corresponding scene semantics from the PROX-E dataset. We demonstrate the\nvalue of POSA with two applications. First, we automatically place 3D scans of\npeople in scenes. We use a SMPL-X model fit to the scan as a proxy and then\nfind its most likely placement in 3D. POSA provides an effective representation\nto search for \"affordances\" in the scene that match the likely contact\nrelationships for that pose. We perform a perceptual study that shows\nsignificant improvement over the state of the art on this task. Second, we show\nthat POSA's learned representation of body-scene interaction supports monocular\nhuman pose estimation that is consistent with a 3D scene, improving on the\nstate of the art. Our model and code are available for research purposes at\nhttps://posa.is.tue.mpg.de.\n", "rewritten_text": "This paper introduces POSA, a novel Human-Scene Interaction (HSI) model that enables virtual characters to interact realistically with 3D environments.  POSA, short for \"Pose with prOximitieS and contActs,\" is a body-centric representation augmenting the SMPL-X human body model.  For each vertex, it encodes contact probability with the scene and its semantic label.  Trained on the PROX and PROX-E datasets using a conditional Variational Autoencoder (VAE), POSA effectively captures proximal relationships.  We demonstrate its effectiveness in two applications:  first, significantly improving state-of-the-art automatic placement of 3D human scans in scenes by leveraging affordances; and second, enhancing monocular human pose estimation for scene consistency, again surpassing existing methods.  Our model and code are publicly available at https://posa.is.tue.mpg.de.\n"}, "1609.04079": {"original_text": "  We present a single-shot system to recover surface geometry of objects with\nspatially-varying albedos, from images captured under a calibrated RGB\nphotometric stereo setup---with three light directions multiplexed across\ndifferent color channels in the observed RGB image. Since the problem is\nill-posed point-wise, we assume that the albedo map can be modeled as\npiece-wise constant with a restricted number of distinct albedo values. We show\nthat under ideal conditions, the shape of a non-degenerate local constant\nalbedo surface patch can theoretically be recovered exactly. Moreover, we\npresent a practical and efficient algorithm that uses this model to robustly\nrecover shape from real images. Our method first reasons about shape locally in\na dense set of patches in the observed image, producing shape distributions for\nevery patch. These local distributions are then combined to produce a single\nconsistent surface normal map. We demonstrate the efficacy of the approach\nthrough experiments on both synthetic renderings as well as real captured\nimages.\n", "rewritten_text": "This paper introduces a novel single-shot method for reconstructing object surface geometry from RGB images acquired under calibrated photometric stereo.  The system multiplexes three light sources across the RGB channels.  Addressing the ill-posed nature of the problem, we model albedo as piecewise constant with a limited number of values.  Theoretically, we prove exact shape recovery for non-degenerate, locally constant albedo patches under ideal conditions.  Practically, we present an efficient algorithm that robustly recovers shape from real images by first estimating local shape distributions in dense patches and then globally integrating these to produce a consistent surface normal map.  Experimental results on synthetic and real data validate our approach.\n"}, "2305.15699": {"original_text": "  Understanding action recognition in egocentric videos has emerged as a vital\nresearch topic with numerous practical applications. With the limitation in the\nscale of egocentric data collection, learning robust deep learning-based action\nrecognition models remains difficult. Transferring knowledge learned from the\nlarge-scale exocentric data to the egocentric data is challenging due to the\ndifference in videos across views. Our work introduces a novel cross-view\nlearning approach to action recognition (CVAR) that effectively transfers\nknowledge from the exocentric to the selfish view. First, we present a novel\ngeometric-based constraint into the self-attention mechanism in Transformer\nbased on analyzing the camera positions between two views. Then, we propose a\nnew cross-view self-attention loss learned on unpaired cross-view data to\nenforce the self-attention mechanism learning to transfer knowledge across\nviews. Finally, to further improve the performance of our cross-view learning\napproach, we present the metrics to measure the correlations in videos and\nattention maps effectively. Experimental results on standard egocentric action\nrecognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, and\nEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-art\nperformance.\n", "rewritten_text": "Egocentric action recognition is a crucial research area with significant practical implications, but limited data hinders robust deep learning model development.  Bridging the gap between readily available exocentric data and scarce egocentric data is challenging due to differing viewpoints.  This paper introduces CVAR, a novel cross-view learning approach that effectively transfers knowledge from exocentric to egocentric videos.  CVAR incorporates a geometrically-constrained self-attention mechanism in a Transformer network, leveraging camera position analysis.  Furthermore, a novel cross-view self-attention loss, trained on unpaired data, encourages cross-view knowledge transfer.  Finally, new metrics assess video and attention map correlations, further enhancing performance.  Experiments on Charades-Ego, EPIC-Kitchens-55, and EPIC-Kitchens-100 demonstrate CVAR's effectiveness and state-of-the-art results.\n"}, "2407.10753": {"original_text": "  Accurate depth information is crucial for enhancing the performance of\nmulti-view 3D object detection. Despite the success of some existing multi-view\n3D detectors utilizing pixel-wise depth supervision, they overlook two\nsignificant phenomena: 1) the depth supervision obtained from LiDAR points is\nusually distributed on the surface of the object, which is not so friendly to\nexisting DETR-based 3D detectors due to the lack of the depth of 3D object\ncenter; 2) for distant objects, fine-grained depth estimation of the whole\nobject is more challenging. Therefore, we argue that the object-wise depth (or\n3D center of the object) is essential for accurate detection. In this paper, we\npropose a new multi-view 3D object detector named OPEN, whose main idea is to\neffectively inject object-wise depth information into the network through our\nproposed object-wise position embedding. Specifically, we first employ an\nobject-wise depth encoder, which takes the pixel-wise depth map as a prior, to\naccurately estimate the object-wise depth. Then, we utilize the proposed\nobject-wise position embedding to encode the object-wise depth information into\nthe transformer decoder, thereby producing 3D object-aware features for final\ndetection. Extensive experiments verify the effectiveness of our proposed\nmethod. Furthermore, OPEN achieves a new state-of-the-art performance with\n64.4% NDS and 56.7% mAP on the nuScenes test benchmark.\n", "rewritten_text": "Accurate multi-view 3D object detection relies heavily on depth information.  Current methods using pixel-wise depth supervision from LiDAR often fall short because they lack the object's center depth and struggle with accurate depth estimation for distant objects.  To address this, we introduce OPEN, a novel multi-view 3D detector that incorporates object-wise depth information.  OPEN uses an object-wise depth encoder to estimate the depth of each object's center from the pixel-wise depth map, then integrates this information into the transformer decoder via a novel object-wise position embedding.  This approach significantly improves 3D object-aware feature generation, leading to state-of-the-art performance on the nuScenes test benchmark (64.4% NDS and 56.7% mAP).\n"}, "2308.10445": {"original_text": "  Incremental learning aims to overcome catastrophic forgetting when learning\ndeep networks from sequential tasks. With impressive learning efficiency and\nperformance, prompt-based methods adopt a fixed backbone to sequential tasks by\nlearning task-specific prompts. However, existing prompt-based methods heavily\nrely on strong pretraining (typically trained on ImageNet-21k), and we find\nthat their models could be trapped if the potential gap between the pretraining\ntask and unknown future tasks is large. In this work, we develop a learnable\nAdaptive Prompt Generator (APG). The key is to unify the prompt retrieval and\nprompt learning processes into a learnable prompt generator. Hence, the whole\nprompting process can be optimized to reduce the negative effects of the gap\nbetween tasks effectively. To make our APG avoid learning ineffective\nknowledge, we maintain a knowledge pool to regularize APG with the feature\ndistribution of each class. Extensive experiments show that our method\nsignificantly outperforms advanced methods in exemplar-free incremental\nlearning without (strong) pretraining. Besides, under strong retraining, our\nmethod also has comparable performance to existing prompt-based models, showing\nthat our method can still benefit from pretraining. Codes can be found at\nhttps://github.com/TOM-tym/APG\n", "rewritten_text": "This paper addresses catastrophic forgetting in deep learning by introducing a novel Adaptive Prompt Generator (APG) for incremental learning.  Unlike existing prompt-based methods that rely heavily on strong pre-training (e.g., ImageNet-21k) and suffer from performance degradation when encountering significantly different tasks, APG unifies prompt retrieval and learning into a single learnable module.  This allows for optimization of the entire prompting process, mitigating the negative impact of task discrepancies.  Further, a knowledge pool regularizes APG, preventing the learning of ineffective knowledge.  Experiments demonstrate APG's superior performance compared to state-of-the-art methods in exemplar-free incremental learning, even without strong pre-training.  Moreover, APG achieves comparable results to pre-trained prompt-based models when pre-training is used, highlighting its adaptability.  Code is available at https://github.com/TOM-tym/APG.\n"}, "2409.18355": {"original_text": "  Cone Beam Computed Tomography (CBCT) finds diverse applications in medicine.\nEnsuring high image quality in CBCT scans is essential for accurate diagnosis\nand treatment delivery. Yet, the susceptibility of CBCT images to noise and\nartifacts undermines both their usefulness and reliability. Existing methods\ntypically address CBCT artifacts through image-to-image translation approaches.\nThese methods, however, are limited by the artifact types present in the\ntraining data, which may not cover the complete spectrum of CBCT degradations\nstemming from variations in imaging protocols. Gathering additional data to\nencompass all possible scenarios can often pose a challenge. To address this,\nwe present SinoSynth, a physics-based degradation model that simulates various\nCBCT-specific artifacts to generate a diverse set of synthetic CBCT images from\nhigh-quality CT images without requiring pre-aligned data. Through extensive\nexperiments, we demonstrate that several different generative networks trained\non our synthesized data achieve remarkable results on heterogeneous\nmulti-institutional datasets, outperforming even the same networks trained on\nactual data. We further show that our degradation model conveniently provides\nan avenue to enforce anatomical constraints in conditional generative models,\nyielding high-quality and structure-preserving synthetic CT images.\n", "rewritten_text": "Cone Beam Computed Tomography (CBCT) image quality is crucial for accurate medical diagnosis and treatment, but noise and artifacts significantly hinder its reliability.  Current artifact reduction methods, relying on image-to-image translation, are limited by the variability of artifacts in training data.  To overcome this, we introduce SinoSynth, a physics-based model that generates diverse synthetic CBCT images from high-quality CT scans, simulating various CBCT-specific artifacts without needing pre-aligned data.  Our experiments show that generative networks trained on SinoSynth's synthetic data outperform those trained on real data across multiple institutions, even achieving superior structure-preserving results by enforcing anatomical constraints.\n"}, "1904.07904": {"original_text": "  Spoken question answering (SQA) is challenging due to complex reasoning on\ntop of the spoken documents. The recent studies have also shown the\ncatastrophic impact of automatic speech recognition (ASR) errors on SQA.\nTherefore, this work proposes to mitigate the ASR errors by aligning the\nmismatch between ASR hypotheses and their corresponding reference\ntranscriptions. An adversarial model is applied to this domain adaptation task,\nwhich forces the model to learn domain-invariant features the QA model can\neffectively utilize in order to improve the SQA results. The experiments\nsuccessfully demonstrate the effectiveness of our proposed model, and the\nresults are better than the previous best model by 2% EM score.\n", "rewritten_text": "Spoken Question Answering (SQA) suffers from the complexities of reasoning with spoken documents, exacerbated by the significant negative impact of Automatic Speech Recognition (ASR) errors.  This work addresses this by aligning mismatched ASR hypotheses and reference transcriptions using an adversarial model. This domain adaptation approach learns domain-invariant features, improving the SQA model's performance.  Experiments show a 2% improvement in Exact Match (EM) score over the previous state-of-the-art.\n"}, "2310.00936": {"original_text": "  Recent studies on StyleGAN variants show promising performances for various\ngeneration tasks. In these models, latent codes have traditionally been\nmanipulated and searched for the desired images. However, this approach\nsometimes suffers from a lack of photorealism in generated images due to a lack\nof knowledge about the geometry of the trained latent space. In this paper, we\nshow a simple unsupervised method that provides well-trained local latent\nsubspace, enabling latent code navigation while preserving the photorealism of\nthe generated images. Specifically, the method identifies densely mapped latent\nspaces and restricts latent manipulations within the local latent subspace.\nExperimental results demonstrate that images generated within the local latent\nsubspace maintain photorealism even when the latent codes are significantly and\nrepeatedly manipulated. Moreover, experiments show that the method can be\napplied to latent code optimization for various types of style-based models.\nOur empirical evidence of the method will benefit applications in style-based\nmodels.\n", "rewritten_text": "StyleGAN variants have shown great potential for image generation, but traditional latent code manipulation often compromises photorealism due to the complex, poorly understood geometry of the latent space.  This paper introduces a simple, unsupervised method to create well-trained local latent subspaces.  By restricting manipulations within these subspaces, we maintain photorealism even with extensive latent code modifications.  Our method, applicable to various style-based generative models, is demonstrated to improve latent code optimization and enhance the photorealism of generated images across multiple experiments.  These findings offer significant advantages for applications using style-based models.\n"}, "2310.00796": {"original_text": "  Strong inductive biases enable learning from little data and help\ngeneralization outside of the training distribution. Popular neural\narchitectures such as Transformers lack strong structural inductive biases for\nseq2seq NLP tasks on their own. Consequently, they struggle with systematic\ngeneralization beyond the training distribution, e.g. with extrapolating to\nlonger inputs, even when pre-trained on large amounts of text. We show how a\nstructural inductive bias can be efficiently injected into a seq2seq model by\npre-training it to simulate structural transformations on synthetic data.\nSpecifically, we inject an inductive bias towards Finite State Transducers\n(FSTs) into a Transformer by pre-training it to simulate FSTs given their\ndescriptions. Our experiments show that our method imparts the desired\ninductive bias, resulting in improved systematic generalization and better\nfew-shot learning for FST-like tasks. Our analysis shows that fine-tuned models\naccurately capture the state dynamics of the unseen underlying FSTs, suggesting\nthat the simulation process is internalized by the fine-tuned model.\n", "rewritten_text": "Large language models like Transformers, while powerful, lack inherent biases that facilitate generalization beyond their training data, particularly for sequence-to-sequence tasks.  This limits their ability to handle inputs outside the training distribution, such as longer sequences.  We introduce a novel method to address this limitation by pre-training a Transformer to simulate Finite State Transducers (FSTs) using synthetic data. This injects a structural inductive bias towards FSTs, improving the model's systematic generalization and few-shot learning capabilities on FST-related tasks.  Experiments demonstrate that our approach successfully instills the desired bias, leading to better performance and the internalization of FST state dynamics, even for unseen FSTs.\n"}, "2309.11566": {"original_text": "  We introduce SignBank+, a clean version of the SignBank dataset, optimized\nfor machine translation between spoken language text and SignWriting, a\nphonetic sign language writing system. In addition to previous work that\nemploys complex factorization techniques to enable translation between text and\nSignWriting, we show that a traditional text-to-text translation approach\nperforms equally effectively on the cleaned SignBank+ dataset. Our evaluation\nresults indicate that models trained on SignBank+ surpass those on the original\ndataset, establishing a new benchmark for SignWriting-based sign language\ntranslation and providing an open resource for future research.\n", "rewritten_text": "SignBank+, a cleaned and optimized version of the SignBank dataset, significantly improves machine translation between spoken language and SignWriting.  Unlike previous work relying on complex factorization, we demonstrate that a standard text-to-text translation approach achieves comparable results on SignBank+.  Models trained on this new dataset outperform those trained on the original SignBank, setting a new benchmark for SignWriting-based sign language translation and offering a valuable open-source resource for future research.\n"}, "2212.00244": {"original_text": "  Domain adaptation for Cross-LiDAR 3D detection is challenging due to the\nlarge gap on the raw data representation with disparate point densities and\npoint arrangements. By exploring domain-invariant 3D geometric characteristics\nand motion patterns, we present an unsupervised domain adaptation method that\novercomes above difficulties. First, we propose the Spatial Geometry Alignment\nmodule to extract similar 3D shape geometric features of the same object class\nto align two domains, while eliminating the effect of distinct point\ndistributions. Second, we present Temporal Motion Alignment module to utilize\nmotion features in sequential frames of data to match two domains. Prototypes\ngenerated from two modules are incorporated into the pseudo-label reweighting\nprocedure and contribute to our effective self-training framework for the\ntarget domain. Extensive experiments show that our method achieves\nstate-of-the-art performance on cross-device datasets, especially for the\ndatasets with large gaps captured by mechanical scanning LiDARs and solid-state\nLiDARs in various scenes. Project homepage is at\nhttps://github.com/4DVLab/CL3D.git\n", "rewritten_text": "Cross-LiDAR 3D object detection is hampered by significant differences in raw data, including point density and arrangement.  This paper introduces an unsupervised domain adaptation method addressing this challenge by leveraging domain-invariant 3D geometric and motion features.  A novel Spatial Geometry Alignment module extracts similar 3D shape features to align data from different LiDAR systems, irrespective of point distribution.  A complementary Temporal Motion Alignment module uses motion features from sequential frames for further domain alignment.  These modules generate prototypes used in a pseudo-label reweighting scheme within a self-training framework.  Our method achieves state-of-the-art results on cross-device datasets, particularly those with large discrepancies between mechanical and solid-state LiDAR data across diverse scenes.  See our project page at https://github.com/4DVLab/CL3D.git for more details.\n"}, "2210.10239": {"original_text": "  This paper aims to investigate representation learning for large scale visual\nplace recognition, which consists of determining the location depicted in a\nquery image by referring to a database of reference images. This is a\nchallenging task due to the large-scale environmental changes that can occur\nover time (i.e., weather, illumination, season, traffic, occlusion). Progress\nis currently challenged by the lack of large databases with accurate ground\ntruth. To address this challenge, we introduce GSV-Cities, a new image dataset\nproviding the widest geographic coverage to date with highly accurate ground\ntruth, covering more than 40 cities across all continents over a 14-year\nperiod. We subsequently explore the full potential of recent advances in deep\nmetric learning to train networks specifically for place recognition, and\nevaluate how different loss functions influence performance. In addition, we\nshow that performance of existing methods substantially improves when trained\non GSV-Cities. Finally, we introduce a new fully convolutional aggregation\nlayer that outperforms existing techniques, including GeM, NetVLAD and\nCosPlace, and establish a new state-of-the-art on large-scale benchmarks, such\nas Pittsburgh, Mapillary-SLS, SPED and Nordland. The dataset and code are\navailable for research purposes at https://github.com/amaralibey/gsv-cities.\n", "rewritten_text": "This paper introduces GSV-Cities, a novel, large-scale image dataset spanning 40+ cities across all continents over 14 years, offering highly accurate ground truth for visual place recognition.  This addresses the current lack of suitable datasets hindering progress in this challenging field, which involves identifying the location of a query image within a vast database of reference images despite significant environmental variations.  Leveraging GSV-Cities, we explore deep metric learning techniques, analyzing the impact of different loss functions and introducing a novel fully convolutional aggregation layer that surpasses existing methods (GeM, NetVLAD, CosPlace).  Our approach achieves state-of-the-art results on established benchmarks (Pittsburgh, Mapillary-SLS, SPED, Nordland), significantly improving upon existing methods. The dataset and code are publicly available at https://github.com/amaralibey/gsv-cities.\n"}, "2202.04101": {"original_text": "  Photoplethysmography (PPG) signals have become a key technology in many\nfields, such as medicine, well-being, or sports. Our work proposes a set of\npipelines to extract remote PPG signals (rPPG) from the face robustly,\nreliably, and configurable. We identify and evaluate the possible choices in\nthe critical steps of unsupervised rPPG methodologies. We assess a\nstate-of-the-art processing pipeline in six different datasets, incorporating\nimportant corrections in the methodology that ensure reproducible and fair\ncomparisons. In addition, we extend the pipeline by proposing three novel\nideas; 1) a new method to stabilize the detected face based on a rigid mesh\nnormalization; 2) a new method to dynamically select the different regions in\nthe face that provide the best raw signals, and 3) a new RGB to rPPG\ntransformation method, called Orthogonal Matrix Image Transformation (OMIT)\nbased on QR decomposition, that increases robustness against compression\nartifacts. We show that all three changes introduce noticeable improvements in\nretrieving rPPG signals from faces, obtaining state-of-the-art results compared\nwith unsupervised, non-learning-based methodologies and, in some databases,\nvery close to supervised, learning-based methods. We perform a comparative\nstudy to quantify the contribution of each proposed idea. In addition, we\ndepict a series of observations that could help in future implementations.\n", "rewritten_text": "Remote photoplethysmography (rPPG) from facial video is increasingly important across diverse fields.  This work presents a robust, reliable, and configurable pipeline for rPPG extraction, comprehensively evaluating key steps within unsupervised methodologies across six datasets.  We improve a state-of-the-art pipeline by introducing three novel contributions:  a rigid mesh-based face stabilization method, a dynamic facial region selection algorithm, and a novel RGB-to-rPPG transformation (Orthogonal Matrix Image Transformation, or OMIT) using QR decomposition, enhancing robustness to compression artifacts.  These improvements yield state-of-the-art results among unsupervised methods, approaching the performance of supervised learning-based approaches in some cases.  A comparative study quantifies the impact of each contribution, and we offer insights for future rPPG research.\n"}, "1803.05449": {"original_text": "  We introduce SentEval, a toolkit for evaluating the quality of universal\nsentence representations. SentEval encompasses a variety of tasks, including\nbinary and multi-class classification, natural language inference and sentence\nsimilarity. The set of tasks was selected based on what appears to be the\ncommunity consensus regarding the appropriate evaluations for universal\nsentence representations. The toolkit comes with scripts to download and\npreprocess datasets, and an easy interface to evaluate sentence encoders. The\naim is to provide a fairer, less cumbersome and more centralized way for\nevaluating sentence representations.\n", "rewritten_text": "SentEval is a new toolkit designed to standardize and simplify the evaluation of universal sentence representations.  It offers a comprehensive suite of tasks\u2014binary and multi-class classification, natural language inference, and sentence similarity\u2014reflecting widely accepted best practices within the research community.  The toolkit provides scripts for dataset download and preprocessing, along with a user-friendly interface for evaluating sentence encoders, thus offering a more efficient and equitable evaluation process.\n"}, "1506.05929": {"original_text": "  Previous work has shown that the artist of an artwork can be identified by\nuse of computational methods that analyse digital images. However, the\ndigitised artworks are often investigated at a coarse scale discarding many of\nthe important details that may define an artist's style. In recent years high\nresolution images of artworks have become available, which, combined with\nincreased processing power and new computational techniques, allow us to\nanalyse digital images of artworks at a very fine scale. In this work we train\nand evaluate a Convolutional Neural Network (CNN) on the task of artist\nattribution using artwork images of varying resolutions. To this end, we\ncombine two existing methods to enable the application of high resolution\nimages to CNNs. By comparing the attribution performances obtained at different\nscales, we find that in most cases finer scales are beneficial to the\nattribution performance, whereas for a minority of the artists, coarser scales\nappear to be preferable. We conclude that artist attribution would benefit from\na multi-scale CNN approach which vastly expands the possibilities for\ncomputational art forensics.\n", "rewritten_text": "Computational methods have successfully identified artists from digital artwork, but previous analyses often used low-resolution images, overlooking crucial stylistic details.  Advances in imaging technology, processing power, and computational techniques now allow for high-resolution analysis.  This study trains and evaluates a Convolutional Neural Network (CNN) on artist attribution using images at varying resolutions, combining existing methods to handle high-resolution data.  Our results demonstrate that finer-scale analysis generally improves attribution accuracy, although coarser scales proved superior for a small subset of artists.  We conclude that a multi-scale CNN approach would significantly enhance computational art forensics.\n"}, "2112.03803": {"original_text": "  Despite the great progress in video understanding made by deep convolutional\nneural networks, feature representation learned by existing methods may be\nbiased to static visual cues. To address this issue, we propose a novel method\nto suppress static visual cues (SSVC) based on probabilistic analysis for\nself-supervised video representation learning. In our method, video frames are\nfirst encoded to obtain latent variables under standard normal distribution via\nnormalizing flows. By modelling static factors in a video as a random variable,\nthe conditional distribution of each latent variable becomes shifted and scaled\nnormal. Then, the less-varying latent variables along time are selected as\nstatic cues and suppressed to generate motion-preserved videos. Finally,\npositive pairs are constructed by motion-preserved videos for contrastive\nlearning to alleviate the problem of representation bias to static cues. The\nless-biased video representation can be better generalized to various\ndownstream tasks. Extensive experiments on publicly available benchmarks\ndemonstrate that the proposed method outperforms the state of the art when only\nsingle RGB modality is used for pre-training.\n", "rewritten_text": "Deep convolutional neural networks have significantly advanced video understanding, but their learned representations often overemphasize static visual cues.  To mitigate this bias, we introduce SSVC (Suppressing Static Visual Cues), a novel self-supervised video representation learning method.  SSVC uses normalizing flows to encode video frames into latent variables following a standard normal distribution.  Static cues are modeled as random variables, resulting in shifted and scaled normal conditional distributions for each latent variable.  We identify and suppress less temporally varying latent variables, generating motion-preserved videos.  These videos then form positive pairs for contrastive learning, reducing the reliance on static cues.  This leads to less-biased representations that generalize better to various downstream tasks.  Our experiments on standard benchmarks show that SSVC achieves state-of-the-art performance using only RGB input for pre-training.\n"}, "2103.16889": {"original_text": "  Current state-of-the-art visual recognition systems usually rely on the\nfollowing pipeline: (a) pretraining a neural network on a large-scale dataset\n(e.g., ImageNet) and (b) finetuning the network weights on a smaller,\ntask-specific dataset. Such a pipeline assumes the sole weight adaptation is\nable to transfer the network capability from one domain to another domain,\nbased on a strong assumption that a fixed architecture is appropriate for all\ndomains. However, each domain with a distinct recognition target may need\ndifferent levels/paths of feature hierarchy, where some neurons may become\nredundant, and some others are re-activated to form new network structures. In\nthis work, we prove that dynamically adapting network architectures tailored\nfor each domain task along with weight finetuning benefits in both efficiency\nand effectiveness, compared to the existing image recognition pipeline that\nonly tunes the weights regardless of the architecture. Our method can be easily\ngeneralized to an unsupervised paradigm by replacing supernet training with\nself-supervised learning in the source domain tasks and performing linear\nevaluation in the downstream tasks. This further improves the search efficiency\nof our method. Moreover, we also provide principled and empirical analysis to\nexplain why our approach works by investigating the ineffectiveness of existing\nneural architecture search. We find that preserving the joint distribution of\nthe network architecture and weights is of importance. This analysis not only\nbenefits image recognition but also provides insights for crafting neural\nnetworks. Experiments on five representative image recognition tasks such as\nperson re-identification, age estimation, gender recognition, image\nclassification, and unsupervised domain adaptation demonstrate the\neffectiveness of our method.\n", "rewritten_text": "Most visual recognition systems use a two-step process:  pretraining on a large dataset (like ImageNet) followed by fine-tuning on a smaller, task-specific dataset. This assumes a fixed network architecture suffices across domains, adapting only the weights.  However, different tasks may require different network structures.  This work demonstrates that dynamically adapting the network architecture *along with* weight fine-tuning significantly improves both efficiency and accuracy.  We extend this to an unsupervised setting using self-supervised learning and linear evaluation, further enhancing efficiency.  Our analysis reveals the importance of preserving the joint distribution of architecture and weights, explaining the limitations of existing neural architecture search methods.  Experiments across five diverse image recognition tasks (person re-identification, age/gender recognition, image classification, and unsupervised domain adaptation) confirm our method's effectiveness.\n"}, "2003.13239": {"original_text": "  Cross view feature fusion is the key to address the occlusion problem in\nhuman pose estimation. The current fusion methods need to train a separate\nmodel for every pair of cameras making them difficult to scale. In this work,\nwe introduce MetaFuse, a pre-trained fusion model learned from a large number\nof cameras in the Panoptic dataset. The model can be efficiently adapted or\nfinetuned for a new pair of cameras using a small number of labeled images. The\nstrong adaptation power of MetaFuse is due in large part to the proposed\nfactorization of the original fusion model into two parts (1) a generic fusion\nmodel shared by all cameras, and (2) lightweight camera-dependent\ntransformations. Furthermore, the generic model is learned from many cameras by\na meta-learning style algorithm to maximize its adaptation capability to\nvarious camera poses. We observe in experiments that MetaFuse finetuned on the\npublic datasets outperforms the state-of-the-arts by a large margin which\nvalidates its value in practice.\n", "rewritten_text": "Occlusion in human pose estimation is effectively addressed by cross-view feature fusion.  However, existing fusion methods require training separate models for each camera pair, hindering scalability.  This paper introduces MetaFuse, a pre-trained fusion model leveraging the Panoptic dataset's numerous cameras.  MetaFuse factorizes the fusion process into a generic, shared model and lightweight camera-specific transformations, enabling efficient adaptation to new camera pairs with minimal labeled data.  MetaFuse's meta-learning training maximizes its adaptability to diverse camera viewpoints.  Experiments demonstrate significant state-of-the-art performance improvements across public datasets, validating its practical effectiveness.\n"}, "2011.12528": {"original_text": "  We propose a novel reference-based video colorization framework with\nspatiotemporal correspondence. Reference-based methods colorize grayscale\nframes referencing a user input color frame. Existing methods suffer from the\ncolor leakage between objects and the emergence of average colors, derived from\nnon-local semantic correspondence in space. To address this issue, we warp\ncolors only from the regions on the reference frame restricted by\ncorrespondence in time. We propagate masks as temporal correspondences, using\ntwo complementary tracking approaches: off-the-shelf instance tracking for high\nperformance segmentation, and newly proposed dense tracking to track various\ntypes of objects. By restricting temporally-related regions for referencing\ncolors, our approach propagates faithful colors throughout the video.\nExperiments demonstrate that our method outperforms state-of-the-art methods\nquantitatively and qualitatively.\n", "rewritten_text": "This paper introduces a new video colorization method that uses a reference color frame provided by the user.  Unlike existing methods which suffer from color bleeding and averaging due to inaccurate spatial correspondences, our approach leverages spatiotemporal consistency.  We achieve this by warping colors only from temporally corresponding regions in the reference frame, using a combination of off-the-shelf instance tracking and a novel dense tracking method to accurately propagate masks and maintain object boundaries. This temporal constraint ensures faithful color propagation throughout the video.  Quantitative and qualitative evaluations show our method surpasses current state-of-the-art techniques.\n"}}