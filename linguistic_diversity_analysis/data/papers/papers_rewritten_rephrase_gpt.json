{"1812.10788": {"original_text": "  Hyperspectral remote sensing is a prominent research topic in data\nprocessing. Most of the spectral unmixing algorithms are developed by adopting\nthe linear mixing models. Nonnegative matrix factorization (NMF) and its\ndevelopments are used widely for estimation of signatures and fractional\nabundances in the SU problem. Sparsity constraints was added to NMF, and was\nregularized by $ L_ {q} $ norm. In this paper, at first hyperspectral images\nare clustered by fuzzy c- means method, and then a new algorithm based on\nsparsity constrained distributed optimization is used for spectral unmixing. In\nthe proposed algorithm, a network including clusters is employed. Each pixel in\nthe hyperspectral images considered as a node in this network. The proposed\nalgorithm is optimized with diffusion LMS strategy, and then the update\nequations for fractional abundance and signature matrices are obtained.\nSimulation results based on defined performance metrics illustrate advantage of\nthe proposed algorithm in spectral unmixing of hyperspectral data compared with\nother methods.\n", "rewritten_text": "\"Hyperspectral remote sensing is a key area of study in data processing. Many spectral unmixing algorithms rely on linear mixing models. Nonnegative matrix factorization (NMF) and its variations are commonly used to estimate signatures and fractional abundances in spectral unmixing. NMF with sparsity constraints, regularized by the $ L_ {q} $ norm, has been developed. This study introduces a new approach for spectral unmixing using a sparsity-constrained distributed optimization algorithm after clustering hyperspectral images with fuzzy c-means. The algorithm utilizes a network structure with clusters, treating each pixel in the hyperspectral images as a node. The algorithm is optimized using diffusion LMS strategy, resulting in update equations for fractional abundance and signature matrices. Simulation results demonstrate the effectiveness of the proposed algorithm in spectral unmixing hyperspectral data compared to other methods.\""}, "2305.04628": {"original_text": "  In this paper, we introduce a novel framework for the challenging problem of\nOne-Shot Unsupervised Domain Adaptation (OSUDA), which aims to adapt to a\ntarget domain with only a single unlabeled target sample. Unlike existing\napproaches that rely on large labeled source and unlabeled target data, our\nTarget-driven One-Shot UDA (TOS-UDA) approach employs a learnable augmentation\nstrategy guided by the target sample's style to align the source distribution\nwith the target distribution. Our method consists of three modules: an\naugmentation module, a style alignment module, and a classifier. Unlike\nexisting methods, our augmentation module allows for strong transformations of\nthe source samples, and the style of the single target sample available is\nexploited to guide the augmentation by ensuring perceptual similarity.\nFurthermore, our approach integrates augmentation with style alignment,\neliminating the need for separate pre-training on additional datasets. Our\nmethod outperforms or performs comparably to existing OS-UDA methods on the\nDigits and DomainNet benchmarks.\n", "rewritten_text": "This paper presents a new framework for addressing the complex challenge of One-Shot Unsupervised Domain Adaptation (OSUDA), which involves adapting to a target domain using only one unlabeled target sample. Unlike current methods that depend on extensive labeled source and unlabeled target data, our Target-driven One-Shot UDA (TOS-UDA) approach utilizes a trainable augmentation strategy guided by the style of the target sample to align the source distribution with the target distribution. Our method comprises three key modules: an augmentation module, a style alignment module, and a classifier. In contrast to existing techniques, our augmentation module allows for significant transformations of the source samples, leveraging the style of the single target sample to guide the augmentation process and ensure perceptual similarity. Additionally, our approach combines augmentation with style alignment, removing the need for separate pre-training on additional datasets. Our method demonstrates superior or comparable performance to existing OS-UDA methods on the Digits and DomainNet benchmarks."}, "2011.14589": {"original_text": "  Monocular 3D object detection, with the aim of predicting the geometric\nproperties of on-road objects, is a promising research topic for the\nintelligent perception systems of autonomous driving. Most state-of-the-art\nmethods follow a keypoint-based paradigm, where the keypoints of objects are\npredicted and employed as the basis for regressing the other geometric\nproperties. In this work, a unified network named as FADNet is presented to\naddress the task of monocular 3D object detection. In contrast to previous\nkeypoint-based methods, we propose to divide the output modalities into\ndifferent groups according to the estimation difficulty of object properties.\nDifferent groups are treated differently and sequentially associated by a\nconvolutional Gated Recurrent Unit. Another contribution of this work is the\nstrategy of depth hint augmentation. To provide characterized depth patterns as\nhints for depth estimation, a dedicated depth hint module is designed to\ngenerate row-wise features named as depth hints, which are explicitly\nsupervised in a bin-wise manner. The contributions of this work are validated\nby conducting experiments and ablation study on the KITTI benchmark. Without\nutilizing depth priors, post optimization, or other refinement modules, our\nnetwork performs competitively against state-of-the-art methods while\nmaintaining a decent running speed.\n", "rewritten_text": "Monocular 3D object detection is a promising area of research for autonomous driving systems, focusing on predicting the geometric properties of objects on the road. Many current methods use a keypoint-based approach, where object keypoints are predicted and used to determine other geometric properties. This study introduces a unified network called FADNet for monocular 3D object detection. Unlike previous keypoint-based methods, this network divides the output into different groups based on the difficulty of estimating object properties. These groups are processed sequentially using a convolutional Gated Recurrent Unit. Additionally, a depth hint augmentation strategy is proposed to provide depth patterns as hints for depth estimation. A dedicated depth hint module generates row-wise features called depth hints, which are supervised in a bin-wise manner. Experimental validation and ablation studies on the KITTI benchmark demonstrate the effectiveness of these contributions. The network performs competitively against state-of-the-art methods without relying on depth priors, post-optimization, or other refinement modules, while maintaining a decent running speed."}, "2307.13958": {"original_text": "  Recently, vision transformer based multimodal learning methods have been\nproposed to improve the robustness of face anti-spoofing (FAS) systems.\nHowever, multimodal face data collected from the real world is often imperfect\ndue to missing modalities from various imaging sensors. Recently,\nflexible-modal FAS~\\cite{yu2023flexible} has attracted more attention, which\naims to develop a unified multimodal FAS model using complete multimodal face\ndata but is insensitive to test-time missing modalities. In this paper, we\ntackle one main challenge in flexible-modal FAS, i.e., when missing modality\noccurs either during training or testing in real-world situations. Inspired by\nthe recent success of the prompt learning in language models, we propose\n\\textbf{V}isual \\textbf{P}rompt flexible-modal \\textbf{FAS} (VP-FAS), which\nlearns the modal-relevant prompts to adapt the frozen pre-trained foundation\nmodel to downstream flexible-modal FAS task. Specifically, both vanilla visual\nprompts and residual contextual prompts are plugged into multimodal\ntransformers to handle general missing-modality cases, while only requiring\nless than 4\\% learnable parameters compared to training the entire model.\nFurthermore, missing-modality regularization is proposed to force models to\nlearn consistent multimodal feature embeddings when missing partial modalities.\nExtensive experiments conducted on two multimodal FAS benchmark datasets\ndemonstrate the effectiveness of our VP-FAS framework that improves the\nperformance under various missing-modality cases while alleviating the\nrequirement of heavy model re-training.\n", "rewritten_text": "Recently, there has been a rise in the use of vision transformer-based multimodal learning techniques to enhance the resilience of face anti-spoofing (FAS) systems. However, real-world multimodal face data often lacks certain modalities from different imaging sensors, leading to imperfections. A new approach called flexible-modal FAS has gained attention for developing a unified multimodal FAS model that utilizes complete face data but remains robust to missing modalities during testing. This paper addresses a key challenge in flexible-modal FAS, specifically dealing with missing modalities during training or testing in real-world scenarios. Drawing inspiration from successful prompt learning in language models, we introduce Visual Prompt flexible-modal FAS (VP-FAS), which leverages modal-relevant prompts to fine-tune a pre-trained foundation model for the flexible-modal FAS task. By incorporating vanilla visual prompts and residual contextual prompts into multimodal transformers, our approach can handle various missing-modality scenarios with minimal learnable parameters, requiring less than 4% compared to training the entire model. Additionally, we propose a missing-modality regularization technique to encourage models to learn consistent multimodal feature embeddings even when some modalities are missing. Extensive experiments on two benchmark datasets for multimodal FAS demonstrate the effectiveness of our VP-FAS framework in improving performance across different missing-modality scenarios while reducing the need for extensive model re-training."}, "1803.0634": {"original_text": "  We present a method for estimating detailed scene illumination using human\nfaces in a single image. In contrast to previous works that estimate lighting\nin terms of low-order basis functions or distant point lights, our technique\nestimates illumination at a higher precision in the form of a non-parametric\nenvironment map. Based on the observation that faces can exhibit strong\nhighlight reflections from a broad range of lighting directions, we propose a\ndeep neural network for extracting highlights from faces, and then trace these\nreflections back to the scene to acquire the environment map. Since real\ntraining data for highlight extraction is very limited, we introduce an\nunsupervised scheme for finetuning the network on real images, based on the\nconsistent diffuse chromaticity of a given face seen in multiple real images.\nIn tracing the estimated highlights to the environment, we reduce the blurring\neffect of skin reflectance on reflected light through a deconvolution\ndetermined by prior knowledge on face material properties. Comparisons to\nprevious techniques for highlight extraction and illumination estimation show\nthe state-of-the-art performance of this approach on a variety of indoor and\noutdoor scenes.\n", "rewritten_text": "We introduce a novel method for accurately estimating scene lighting by analyzing human faces within a single image. Unlike previous approaches that rely on low-order basis functions or distant point lights to estimate lighting, our technique achieves a higher level of precision by generating a non-parametric environment map. By leveraging the fact that faces can display distinct highlight reflections from various lighting angles, we propose a deep neural network to identify highlights on faces and then use this information to construct the environment map. Given the scarcity of real training data for highlight extraction, we implement an unsupervised method to fine-tune the network using real images, focusing on the consistent diffuse chromaticity of a face across multiple images. To enhance the accuracy of tracing highlights to the environment, we mitigate the blurring effect caused by skin reflectance through a deconvolution process informed by prior knowledge of face material properties. Comparative evaluations against existing techniques for highlight extraction and illumination estimation demonstrate the superior performance of our approach across a range of indoor and outdoor scenes."}, "1902.00595": {"original_text": "  Intuitively, human readers cope easily with errors in text; typos,\nmisspelling, word substitutions, etc. do not unduly disrupt natural reading.\nPrevious work indicates that letter transpositions result in increased reading\ntimes, but it is unclear if this effect generalizes to more natural errors. In\nthis paper, we report an eye-tracking study that compares two error types\n(letter transpositions and naturally occurring misspelling) and two error rates\n(10% or 50% of all words contain errors). We find that human readers show\nunimpaired comprehension in spite of these errors, but error words cause more\nreading difficulty than correct words. Also, transpositions are more difficult\nthan misspellings, and a high error rate increases difficulty for all words,\nincluding correct ones. We then present a computational model that uses\ncharacter-based (rather than traditional word-based) surprisal to account for\nthese results. The model explains that transpositions are harder than\nmisspellings because they contain unexpected letter combinations. It also\nexplains the error rate effect: upcoming words are more difficultto predict\nwhen the context is degraded, leading to increased surprisal.\n", "rewritten_text": "In this study, we investigate how human readers handle errors in text, such as typos, misspellings, and word substitutions. While previous research has shown that letter transpositions can slow down reading, it is unclear if this applies to more natural errors. Our eye-tracking study compares two types of errors (letter transpositions and misspellings) at two different error rates (10% and 50% of words containing errors). We find that readers can still comprehend text despite errors, but error words pose more challenges than correct words. Transpositions are more difficult than misspellings, and a higher error rate makes reading harder for all words, not just the ones with errors. We introduce a computational model that uses character-based surprisal to explain these findings. The model suggests that transpositions are tougher because they contain unexpected letter combinations, and higher error rates make it harder to predict upcoming words, leading to increased surprisal."}, "cs/0406031": {"original_text": "  This paper describes a standalone, publicly-available implementation of the\nResolution of Anaphora Procedure (RAP) given by Lappin and Leass (1994). The\nRAP algorithm resolves third person pronouns, lexical anaphors, and identifies\npleonastic pronouns. Our implementation, JavaRAP, fills a current need in\nanaphora resolution research by providing a reference implementation that can\nbe benchmarked against current algorithms. The implementation uses the\nstandard, publicly available Charniak (2000) parser as input, and generates a\nlist of anaphora-antecedent pairs as output. Alternately, an in-place\nannotation or substitution of the anaphors with their antecedents can be\nproduced. Evaluation on the MUC-6 co-reference task shows that JavaRAP has an\naccuracy of 57.9%, similar to the performance given previously in the\nliterature (e.g., Preiss 2002).\n", "rewritten_text": "This paper presents a stand-alone, publicly accessible version of the Resolution of Anaphora Procedure (RAP) developed by Lappin and Leass (1994). The RAP algorithm resolves third-person pronouns, lexical anaphors, and identifies pleonastic pronouns. Our implementation, known as JavaRAP, addresses a current need in anaphora resolution research by offering a standard implementation for comparison with existing algorithms. JavaRAP utilizes the widely available Charniak (2000) parser as input and produces a list of anaphora-antecedent pairs as output. Additionally, it can perform in-place annotation or substitution of anaphors with their antecedents. Evaluation on the MUC-6 co-reference task demonstrates that JavaRAP achieves an accuracy of 57.9%, consistent with previous findings in the literature (e.g., Preiss 2002)."}, "2010.13508": {"original_text": "  The SHApe Recovery from Partial textured 3D scans challenge, SHARP 2020, is\nthe first edition of a challenge fostering and benchmarking methods for\nrecovering complete textured 3D scans from raw incomplete data. SHARP 2020 is\norganised as a workshop in conjunction with ECCV 2020. There are two\ncomplementary challenges, the first one on 3D human scans, and the second one\non generic objects. Challenge 1 is further split into two tracks, focusing,\nfirst, on large body and clothing regions, and, second, on fine body details. A\nnovel evaluation metric is proposed to quantify jointly the shape\nreconstruction, the texture reconstruction and the amount of completed data.\nAdditionally, two unique datasets of 3D scans are proposed, to provide raw\nground-truth data for the benchmarks. The datasets are released to the\nscientific community. Moreover, an accompanying custom library of software\nroutines is also released to the scientific community. It allows for processing\n3D scans, generating partial data and performing the evaluation. Results of the\ncompetition, analysed in comparison to baselines, show the validity of the\nproposed evaluation metrics, and highlight the challenging aspects of the task\nand of the datasets. Details on the SHARP 2020 challenge can be found at\nhttps://cvi2.uni.lu/sharp2020/.\n", "rewritten_text": "The inaugural SHApe Recovery from Partial textured 3D scans challenge, SHARP 2020, aims to promote and assess methods for reconstructing complete textured 3D scans from incomplete raw data. This challenge is part of a workshop held in conjunction with ECCV 2020. It consists of two main challenges: one focusing on 3D human scans and the other on generic objects. The first challenge is divided into two tracks, one concentrating on large body and clothing regions and the other on fine body details. A new evaluation metric is introduced to measure shape and texture reconstruction as well as the completeness of the data. Additionally, two distinct datasets of 3D scans are provided as ground-truth data for benchmarking purposes. These datasets are made available to the scientific community along with a specialized software library for processing 3D scans, generating partial data, and conducting evaluations. The competition results, compared against baseline models, demonstrate the effectiveness of the evaluation metrics and underscore the challenges posed by the task and datasets. For more information on the SHARP 2020 challenge, visit https://cvi2.uni.lu/sharp2020/."}, "2305.08844": {"original_text": "  Despite their unprecedented success, even the largest language models make\nmistakes. Similar to how humans learn and improve using feedback, previous work\nproposed providing language models with natural language feedback to guide them\nin repairing their outputs. Because human-generated critiques are expensive to\nobtain, researchers have devised learned critique generators in lieu of human\ncritics while assuming one can train downstream models to utilize generated\nfeedback. However, this approach does not apply to black-box or limited access\nmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\nlarge general-purpose language agents, fine-tuning is neither computationally\nnor spatially efficient as it results in multiple copies of the network. In\nthis work, we introduce RL4F (Reinforcement Learning for Feedback), a\nmulti-agent collaborative framework where the critique generator is trained to\nmaximize end-task performance of GPT-3, a fixed model more than 200 times its\nsize. RL4F produces critiques that help GPT-3 revise its outputs. We study\nthree datasets for action planning, summarization and alphabetization and show\nrelative improvements up to 10% in multiple text similarity metrics over other\nlearned, retrieval-augmented or prompting-based critique generators.\n", "rewritten_text": "\"Despite achieving unprecedented success, even the largest language models are prone to errors. Previous research has suggested providing natural language feedback to guide these models in improving their outputs, similar to how humans learn from feedback. Obtaining human-generated critiques can be costly, leading researchers to develop learned critique generators as an alternative. However, this approach may not be suitable for black-box or limited access models like ChatGPT, which cannot be fine-tuned. Additionally, fine-tuning large general-purpose language agents like GPT-3 is inefficient due to computational and spatial constraints. In this study, we present RL4F (Reinforcement Learning for Feedback), a collaborative framework where a critique generator is trained to enhance the performance of GPT-3, a fixed model significantly larger in size. RL4F generates critiques to assist GPT-3 in refining its outputs. We evaluate this framework on three datasets for action planning, summarization, and alphabetization, demonstrating improvements of up to 10% in various text similarity metrics compared to other critique generators based on learning, retrieval augmentation, or prompting.\""}, "2312.14697": {"original_text": "  Polarization information of the light can provide rich cues for computer\nvision and scene understanding tasks, such as the type of material, pose, and\nshape of the objects. With the advent of new and cheap polarimetric sensors,\nthis imaging modality is becoming accessible to a wider public for solving\nproblems such as pose estimation, 3D reconstruction, underwater navigation, and\ndepth estimation. However, we observe several limitations regarding the usage\nof this sensorial modality, as well as a lack of standards and publicly\navailable tools to analyze polarization images. Furthermore, although\npolarization camera manufacturers usually provide acquisition tools to\ninterface with their cameras, they rarely include processing algorithms that\nmake use of the polarization information. In this paper, we review recent\nadvances in applications that involve polarization imaging, including a\ncomprehensive survey of recent advances on polarization for vision and robotics\nperception tasks. We also introduce a complete software toolkit that provides\ncommon standards to communicate with and process information from most of the\nexisting micro-grid polarization cameras on the market. The toolkit also\nimplements several image processing algorithms for this modality, and it is\npublicly available on GitHub: https://github.com/vibot-lab/Pola4all_JEI_2023.\n", "rewritten_text": "The use of polarization information in light can offer valuable insights for computer vision and scene analysis tasks, such as determining material type, object pose, and shape. With the introduction of affordable polarimetric sensors, this imaging technique is becoming more accessible to a broader audience for applications like pose estimation, 3D reconstruction, underwater navigation, and depth estimation. However, challenges exist in utilizing this sensory modality, including a lack of standards and readily available tools for analyzing polarization images. While polarization camera manufacturers typically provide tools for camera interaction, they often do not include algorithms that leverage polarization data. This paper reviews recent developments in polarization imaging applications, including a detailed examination of advancements in vision and robotics perception tasks. Additionally, a comprehensive software toolkit is introduced to facilitate communication and data processing for most micro-grid polarization cameras on the market. The toolkit incorporates various image processing algorithms specific to this modality and is openly accessible on GitHub at https://github.com/vibot-lab/Pola4all_JEI_2023."}, "1809.04191": {"original_text": "  To realize the promise of ubiquitous embedded deep network inference, it is\nessential to seek limits of energy and area efficiency. To this end,\nlow-precision networks offer tremendous promise because both energy and area\nscale down quadratically with the reduction in precision. Here we demonstrate\nResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on\nthe ImageNet classification benchmark that, at 8-bit precision exceed the\naccuracy of the full-precision baseline networks after one epoch of finetuning,\nthereby leveraging the availability of pretrained models. We also demonstrate\nResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn 4-bit models that match\nthe accuracy of the full-precision baseline networks -- the highest scores to\ndate. Surprisingly, the weights of the low-precision networks are very close\n(in cosine similarity) to the weights of the corresponding baseline networks,\nmaking training from scratch unnecessary.\n  We find that gradient noise due to quantization during training increases\nwith reduced precision, and seek ways to overcome this noise. The number of\niterations required by SGD to achieve a given training error is related to the\nsquare of (a) the distance of the initial solution from the final plus (b) the\nmaximum variance of the gradient estimates. Therefore, we (a) reduce solution\ndistance by starting with pretrained fp32 precision baseline networks and\nfine-tuning, and (b) combat gradient noise introduced by quantization by\ntraining longer and reducing learning rates. Sensitivity analysis indicates\nthat these simple techniques, coupled with proper activation function range\ncalibration to take full advantage of the limited precision, are sufficient to\ndiscover low-precision networks, if they exist, close to fp32 precision\nbaseline networks. The results herein provide evidence that 4-bits suffice for\nclassification.\n", "rewritten_text": "In order to achieve widespread use of embedded deep network inference, it is crucial to focus on improving energy and area efficiency. Low-precision networks show great potential in this regard, as energy and area requirements decrease significantly with reduced precision. In this study, we tested various networks such as ResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn on the ImageNet classification benchmark. We found that at 8-bit precision, these networks outperformed full-precision baseline networks after just one epoch of finetuning, thanks to the availability of pretrained models. Additionally, we developed 4-bit models for some networks that matched the accuracy of full-precision baseline networks, achieving the highest scores to date. Surprisingly, the weights of low-precision networks closely resembled those of the baseline networks, eliminating the need for training from scratch.\n\nDuring training, we observed that gradient noise due to quantization increases with reduced precision. To address this issue, we focused on reducing solution distance by starting with pretrained fp32 precision baseline networks and fine-tuning, as well as combating gradient noise by training longer and adjusting learning rates. Our sensitivity analysis revealed that these simple techniques, along with proper calibration of activation function ranges to optimize limited precision, are effective in discovering low-precision networks close to fp32 precision baseline networks. Overall, our results suggest that 4-bit precision is sufficient for classification tasks."}, "2103.14799": {"original_text": "  Image representation is an important topic in computer vision and pattern\nrecognition. It plays a fundamental role in a range of applications towards\nunderstanding visual contents. Moment-based image representation has been\nreported to be effective in satisfying the core conditions of semantic\ndescription due to its beneficial mathematical properties, especially geometric\ninvariance and independence. This paper presents a comprehensive survey of the\northogonal moments for image representation, covering recent advances in\nfast/accurate calculation, robustness/invariance optimization, definition\nextension, and application. We also create a software package for a variety of\nwidely-used orthogonal moments and evaluate such methods in a same base. The\npresented theory analysis, software implementation, and evaluation results can\nsupport the community, particularly in developing novel techniques and\npromoting real-world applications.\n", "rewritten_text": "The representation of images is a crucial aspect in computer vision and pattern recognition, playing a key role in various applications aimed at understanding visual content. Moment-based image representation has been found to be effective in meeting the essential requirements of semantic description, thanks to its advantageous mathematical properties such as geometric invariance and independence. This study offers a thorough overview of orthogonal moments for image representation, discussing recent progress in efficient/accurate computation, robustness/invariance enhancement, definition expansion, and practical use. Additionally, we have developed a software package that includes a range of commonly used orthogonal moments and have assessed these methods on a consistent basis. The theoretical analysis, software development, and evaluation findings presented here can benefit the research community, particularly in the creation of innovative techniques and the advancement of real-world applications."}, "2406.19297": {"original_text": "  Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we\ndemonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)\napproach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.\n", "rewritten_text": "The concept of continual learning involves gradually training a model on a series of tasks to learn new tasks while minimizing performance decline on previous tasks. Current methods in the field of Continual Learning and Visual Question Answering (VQA) do not explore how the multimodal nature of input impacts a model's learning process. This study reveals that different modalities evolve at varying rates across tasks, even in established encoder-only models and modern Vision & Language (VL) models. Building on this finding, we introduce a modality-aware feature distillation (MAFED) technique that surpasses existing methods across models of different sizes in three multimodal continual learning scenarios. Additionally, we demonstrate through experiments that modality-aware distillation complements experience replay. These findings underscore the significance of considering modality-specific dynamics to prevent forgetting in multimodal continual learning."}, "2410.15865": {"original_text": "  Grammatical features such as number and gender serve two central functions in\nhuman languages. While they encode salient semantic attributes like numerosity\nand animacy, they also offload sentence processing cost by predictably linking\nwords together via grammatical agreement. Grammars exhibit consistent\norganizational patterns across diverse languages, invariably rooted in a\nsemantic foundation, a widely confirmed but still theoretically unexplained\nphenomenon. To explain the basis of universal grammatical patterns, we unify\ntwo fundamental properties of grammar, semantic encoding and agreement-based\npredictability, into a single information-theoretic objective under cognitive\nconstraints. Our analyses reveal that grammatical organization provably\ninherits from perceptual attributes, but that grammars empirically prioritize\nfunctional goals, promoting efficient language processing over semantic\nencoding.\n", "rewritten_text": "Grammatical features such as number and gender play dual roles in human languages. They not only convey important semantic information like quantity and liveliness, but also help streamline sentence comprehension by establishing predictable connections between words through grammatical agreement. Despite the consistent structural patterns found in various languages, which are typically grounded in semantics, the underlying reasons for this phenomenon remain largely unexplained. In an effort to shed light on the universal nature of grammatical patterns, we propose a unified approach that combines semantic encoding and agreement-based predictability within an information-theoretic framework, taking into account cognitive limitations. Our analysis suggests that the organization of grammar is influenced by perceptual factors, yet languages tend to prioritize efficient communication over strict adherence to semantic rules."}, "2306.05119": {"original_text": "  Factuality is important to dialogue summarization. Factual error correction\n(FEC) of model-generated summaries is one way to improve factuality. Current\nFEC evaluation that relies on factuality metrics is not reliable and detailed\nenough. To address this problem, we are the first to manually annotate a FEC\ndataset for dialogue summarization containing 4000 items and propose FERRANTI,\na fine-grained evaluation framework based on reference correction that\nautomatically evaluates the performance of FEC models on different error\ncategories. Using this evaluation framework, we conduct sufficient experiments\nwith FEC approaches under a variety of settings and find the best training\nmodes and significant differences in the performance of the existing approaches\non different factual error categories.\n", "rewritten_text": "\"Ensuring accuracy in dialogue summarization is crucial. Correcting factual errors in model-generated summaries can enhance accuracy. However, current evaluation methods for factual error correction lack reliability and detail. To address this issue, we have created a dataset of 4000 annotated items for dialogue summarization error correction. We introduce FERRANTI, a comprehensive evaluation framework that assesses the performance of error correction models across various error categories. Through extensive experiments, we identify optimal training methods and significant variations in performance among different error categories.\""}, "2410.16190": {"original_text": "  This work explores how human judgement about salient regions of an image can\nbe introduced into deep convolutional neural network (DCNN) training.\nTraditionally, training of DCNNs is purely data-driven. This often results in\nlearning features of the data that are only coincidentally correlated with\nclass labels. Human saliency can guide network training using our proposed new\ncomponent of the loss function that ConveYs Brain Oversight to Raise\nGeneralization (CYBORG) and penalizes the model for using non-salient regions.\nThis mechanism produces DCNNs achieving higher accuracy and generalization\ncompared to using the same training data without human salience. Experimental\nresults demonstrate that CYBORG applies across multiple network architectures\nand problem domains (detection of synthetic faces, iris presentation attacks\nand anomalies in chest X-rays), while requiring significantly less data than\ntraining without human saliency guidance. Visualizations show that\nCYBORG-trained models' saliency is more consistent across independent training\nruns than traditionally-trained models, and also in better agreement with\nhumans. To lower the cost of collecting human annotations, we also explore\nusing deep learning to provide automated annotations. CYBORG training of CNNs\naddresses important issues such as reducing the appetite for large training\nsets, increasing interpretability, and reducing fragility by generalizing\nbetter to new types of data.\n", "rewritten_text": "This study investigates the incorporation of human judgement on important areas of an image into the training of deep convolutional neural networks (DCNN). Traditionally, DCNN training relies solely on data-driven methods, which can lead to learning features that are only coincidentally related to class labels. By integrating human saliency into the training process through a new component of the loss function called ConveYs Brain Oversight to Raise Generalization (CYBORG), the network is guided to focus on salient regions and penalize the use of non-salient areas. This approach results in DCNNs that achieve higher accuracy and generalization compared to training without human saliency. Experimental findings demonstrate the effectiveness of CYBORG across various network architectures and problem domains, such as detecting synthetic faces, iris presentation attacks, and anomalies in chest X-rays, while requiring less training data. Visualizations show that models trained with CYBORG exhibit more consistent saliency patterns across different training runs and are more aligned with human perception compared to traditionally-trained models. Additionally, to reduce the cost of collecting human annotations, the study explores the use of deep learning for automated annotations. The CYBORG training of CNNs addresses key issues such as reducing the need for large training datasets, enhancing interpretability, and improving generalization to new data types."}, "2311.07611": {"original_text": "  In this study we intentionally introduce biases into large language model\nresponses in an attempt to create specific personas for interactive media\npurposes. We explore the differences between open source models such as\nFalcon-7b and the GPT-4 model from Open AI, and we quantify some differences in\nresponses afforded by the two systems. We find that the guardrails in the GPT-4\nmixture of experts models with a supervisor, while useful in assuring AI\nalignment in general, are detrimental in trying to construct personas with a\nvariety of uncommon viewpoints. This study aims to set the groundwork for\nfuture exploration in intentional biases of large language models such that\nthese practices can be applied in the creative field, and new forms of media.\n", "rewritten_text": "\"In this research, we deliberately introduce biases into responses generated by large language models to develop specific personas for interactive media applications. We compare open source models like Falcon-7b with OpenAI's GPT-4 model, analyzing the differences in responses produced by these systems. Our findings indicate that while the guardrails in GPT-4's mixture of experts models with a supervisor are beneficial for ensuring AI alignment in general, they hinder the creation of personas with diverse and unconventional viewpoints. This study lays the foundation for future investigations into intentional biases in large language models, with the goal of applying these techniques in creative fields and new media formats.\""}, "2409.01522": {"original_text": "  Long-term motion generation is a challenging task that requires producing\ncoherent and realistic sequences over extended durations. Current methods\nprimarily rely on framewise motion representations, which capture only static\nspatial details and overlook temporal dynamics. This approach leads to\nsignificant redundancy across the temporal dimension, complicating the\ngeneration of effective long-term motion. To overcome these limitations, we\nintroduce the novel concept of Lagrangian Motion Fields, specifically designed\nfor long-term motion generation. By treating each joint as a Lagrangian\nparticle with uniform velocity over short intervals, our approach condenses\nmotion representations into a series of \"supermotions\" (analogous to\nsuperpixels). This method seamlessly integrates static spatial information with\ninterpretable temporal dynamics, transcending the limitations of existing\nnetwork architectures and motion sequence content types. Our solution is\nversatile and lightweight, eliminating the need for neural network\npreprocessing. Our approach excels in tasks such as long-term music-to-dance\ngeneration and text-to-motion generation, offering enhanced efficiency,\nsuperior generation quality, and greater diversity compared to existing\nmethods. Additionally, the adaptability of Lagrangian Motion Fields extends to\napplications like infinite motion looping and fine-grained controlled motion\ngeneration, highlighting its broad utility. Video demonstrations are available\nat \\url{https://plyfager.github.io/LaMoG}.\n", "rewritten_text": "Generating motion over extended periods is a complex challenge that requires creating coherent and realistic sequences. Current methods mainly use framewise motion representations, which focus on static spatial details and neglect temporal dynamics. This results in redundancy across time, making long-term motion generation difficult. To address these issues, we propose Lagrangian Motion Fields, a new concept tailored for long-term motion generation. By treating each joint as a Lagrangian particle moving at a constant velocity in short intervals, our approach condenses motion into \"supermotions,\" similar to superpixels. This technique seamlessly combines static spatial information with interpretable temporal dynamics, surpassing the limitations of existing network architectures and motion content types. Our method is versatile, lightweight, and does not require neural network preprocessing. It excels in tasks like music-to-dance and text-to-motion generation, offering improved efficiency, quality, and diversity compared to current approaches. Lagrangian Motion Fields can also be applied to infinite motion looping and precise motion control, showcasing its wide range of uses. For video demonstrations, visit \\url{https://plyfager.github.io/LaMoG}."}, "1704.06756": {"original_text": "  We have developed convolutional neural networks (CNN) for a facial expression\nrecognition task. The goal is to classify each facial image into one of the\nseven facial emotion categories considered in this study. We trained CNN models\nwith different depth using gray-scale images. We developed our models in Torch\nand exploited Graphics Processing Unit (GPU) computation in order to expedite\nthe training process. In addition to the networks performing based on raw pixel\ndata, we employed a hybrid feature strategy by which we trained a novel CNN\nmodel with the combination of raw pixel data and Histogram of Oriented\nGradients (HOG) features. To reduce the overfitting of the models, we utilized\ndifferent techniques including dropout and batch normalization in addition to\nL2 regularization. We applied cross validation to determine the optimal\nhyper-parameters and evaluated the performance of the developed models by\nlooking at their training histories. We also present the visualization of\ndifferent layers of a network to show what features of a face can be learned by\nCNN models.\n", "rewritten_text": "We have created convolutional neural networks (CNN) to recognize facial expressions. The objective is to categorize each facial image into one of the seven emotional categories studied. Our CNN models were trained with varying depths using gray-scale images. We built our models in Torch and utilized GPU computation to speed up the training process. In addition to the networks analyzing raw pixel data, we implemented a hybrid feature approach where a new CNN model was trained using a combination of raw pixel data and Histogram of Oriented Gradients (HOG) features. To prevent overfitting, we employed various techniques such as dropout, batch normalization, and L2 regularization. Cross-validation was used to find the best hyperparameters, and we assessed the models' performance by examining their training progress. Furthermore, we visualized different layers of the network to demonstrate the facial features learned by the CNN models."}, "2312.17292": {"original_text": "  Word embedding methods (WEMs) are extensively used for representing text\ndata. The dimensionality of these embeddings varies across various tasks and\nimplementations. The effect of dimensionality change on the accuracy of the\ndownstream task is a well-explored question. However, how the dimensionality\nchange affects the bias of word embeddings needs to be investigated. Using the\nEnglish Wikipedia corpus, we study this effect for two static (Word2Vec and\nfastText) and two context-sensitive (ElMo and BERT) WEMs. We have two\nobservations. First, there is a significant variation in the bias of word\nembeddings with the dimensionality change. Second, there is no uniformity in\nhow the dimensionality change affects the bias of word embeddings. These\nfactors should be considered while selecting the dimensionality of word\nembeddings.\n", "rewritten_text": "\"Word embedding methods (WEMs) are commonly used to represent text data, with the dimensionality of these embeddings varying depending on the task and implementation. While the impact of dimensionality changes on task accuracy is well-studied, the effect on the bias of word embeddings requires further investigation. In this study using the English Wikipedia corpus, we examine how dimensionality changes affect bias in two static (Word2Vec and fastText) and two context-sensitive (ElMo and BERT) WEMs. Our findings reveal significant variations in word embedding bias with changes in dimensionality, and no consistent pattern in how bias is affected by dimensionality changes. These findings highlight the importance of considering these factors when choosing the dimensionality of word embeddings.\""}, "2401.07745": {"original_text": "  Open-vocabulary 3D instance segmentation is cutting-edge for its ability to\nsegment 3D instances without predefined categories. However, progress in 3D\nlags behind its 2D counterpart due to limited annotated 3D data. To address\nthis, recent works first generate 2D open-vocabulary masks through 2D models\nand then merge them into 3D instances based on metrics calculated between two\nneighboring frames. In contrast to these local metrics, we propose a novel\nmetric, view consensus rate, to enhance the utilization of multi-view\nobservations. The key insight is that two 2D masks should be deemed part of the\nsame 3D instance if a significant number of other 2D masks from different views\ncontain both these two masks. Using this metric as edge weight, we construct a\nglobal mask graph where each mask is a node. Through iterative clustering of\nmasks showing high view consensus, we generate a series of clusters, each\nrepresenting a distinct 3D instance. Notably, our model is training-free.\nThrough extensive experiments on publicly available datasets, including\nScanNet++, ScanNet200 and MatterPort3D, we demonstrate that our method achieves\nstate-of-the-art performance in open-vocabulary 3D instance segmentation. Our\nproject page is at https://pku-epic.github.io/MaskClustering.\n", "rewritten_text": "Cutting-edge open-vocabulary 3D instance segmentation is notable for its ability to segment 3D instances without predefined categories. However, progress in the 3D domain lags behind its 2D counterpart due to limited annotated 3D data. To overcome this challenge, recent studies have adopted a strategy where they first generate 2D open-vocabulary masks using 2D models and then combine them into 3D instances based on metrics calculated between adjacent frames. In contrast to these local metrics, we introduce a novel metric called view consensus rate to improve the utilization of multi-view observations. The fundamental idea is that two 2D masks should be considered part of the same 3D instance if a significant number of other 2D masks from different views contain both of these masks. By using this metric as an edge weight, we construct a global mask graph where each mask serves as a node. By iteratively clustering masks with high view consensus, we create a series of clusters, each representing a unique 3D instance. Notably, our model does not require training. Through extensive experiments on publicly available datasets such as ScanNet++, ScanNet200, and MatterPort3D, we demonstrate that our approach achieves state-of-the-art performance in open-vocabulary 3D instance segmentation. For more information, please visit our project page at https://pku-epic.github.io/MaskClustering."}, "2404.05916": {"original_text": "  Echocardiography segmentation for cardiac analysis is time-consuming and\nresource-intensive due to the variability in image quality and the necessity to\nprocess scans from various standard views. While current automated segmentation\nmethods in echocardiography show promising performance, they are trained on\nspecific scan views to analyze corresponding data. However, this solution has a\nlimitation as the number of required models increases with the number of\nstandard views. To address this, in this paper, we present a prompt-driven\nuniversal method for view-agnostic echocardiography analysis. Considering the\ndomain shift between standard views, we first introduce a method called prompt\nmatching, aimed at learning prompts specific to different views by matching\nprompts and querying input embeddings using a pre-trained vision model. Then,\nwe utilized a pre-trained medical language model to align textual information\nwith pixel data for accurate segmentation. Extensive experiments on three\nstandard views showed that our approach significantly outperforms the\nstate-of-the-art universal methods and achieves comparable or even better\nperformances over the segmentation model trained and tested on same views.\n", "rewritten_text": "Segmenting echocardiography images for cardiac analysis can be time-consuming and resource-intensive due to variations in image quality and the need to process scans from different standard views. While current automated segmentation methods in echocardiography show promise, they are typically trained on specific scan views, which can be limiting as the number of required models increases with the number of standard views. In this paper, we propose a novel approach for view-agnostic echocardiography analysis. Our method, called prompt matching, aims to learn prompts specific to different views by matching prompts and querying input embeddings using a pre-trained vision model. We also leverage a pre-trained medical language model to align textual information with pixel data for accurate segmentation. Our extensive experiments on three standard views demonstrate that our approach outperforms existing universal methods and achieves comparable or even superior performance compared to segmentation models trained and tested on the same views."}, "1908.09884": {"original_text": "  We consider the problem of discovering novel object categories in an image\ncollection. While these images are unlabelled, we also assume prior knowledge\nof related but different image classes. We use such prior knowledge to reduce\nthe ambiguity of clustering, and improve the quality of the newly discovered\nclasses. Our contributions are twofold. The first contribution is to extend\nDeep Embedded Clustering to a transfer learning setting; we also improve the\nalgorithm by introducing a representation bottleneck, temporal ensembling, and\nconsistency. The second contribution is a method to estimate the number of\nclasses in the unlabelled data. This also transfers knowledge from the known\nclasses, using them as probes to diagnose different choices for the number of\nclasses in the unlabelled subset. We thoroughly evaluate our method,\nsubstantially outperforming state-of-the-art techniques in a large number of\nbenchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN.\n", "rewritten_text": "We address the challenge of identifying new object categories within a collection of images. Although these images lack labels, we leverage prior knowledge of related but distinct image classes to enhance clustering accuracy and elevate the quality of newly identified classes. Our work offers two key contributions. Firstly, we enhance Deep Embedded Clustering for transfer learning by incorporating a representation bottleneck, temporal ensembling, and consistency. Secondly, we introduce a technique to estimate the number of classes in unlabeled data by leveraging knowledge from known classes as probes. Our method surpasses existing techniques in various benchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN."}, "2212.08334": {"original_text": "  Scene understanding has made tremendous progress over the past few years, as\ndata acquisition systems are now providing an increasing amount of data of\nvarious modalities (point cloud, depth, RGB...). However, this improvement\ncomes at a large cost on computation resources and data annotation\nrequirements. To analyze geometric information and images jointly, many\napproaches rely on both a 2D loss and 3D loss, requiring not only 2D per\npixel-labels but also 3D per-point labels. However, obtaining a 3D groundtruth\nis challenging, time-consuming and error-prone. In this paper, we show that\nimage segmentation can benefit from 3D geometric information without requiring\na 3D groundtruth, by training the geometric feature extraction and the 2D\nsegmentation network jointly, in an end-to-end fashion, using only the 2D\nsegmentation loss. Our method starts by extracting a map of 3D features\ndirectly from a provided point cloud by using a lightweight 3D neural network.\nThe 3D feature map, merged with the RGB image, is then used as an input to a\nclassical image segmentation network. Our method can be applied to many 2D\nsegmentation networks, improving significantly their performance with only a\nmarginal network weight increase and light input dataset requirements, since no\n3D groundtruth is required.\n", "rewritten_text": "In recent years, there has been significant progress in scene understanding due to advancements in data acquisition systems that now provide a wealth of data in various forms such as point cloud, depth, and RGB. However, this progress has come at a high cost in terms of computational resources and data annotation needs. Many methods that aim to analyze both geometric information and images simultaneously use a combination of 2D and 3D loss functions, necessitating detailed labeling at both pixel and point levels. Yet, obtaining accurate 3D ground truth data is a complex, time-consuming, and error-prone task. This study demonstrates that image segmentation can benefit from 3D geometric information without the need for 3D ground truth data. By training the geometric feature extraction and 2D segmentation network together in an end-to-end manner using only the 2D segmentation loss, our approach leverages a map of 3D features extracted directly from a provided point cloud using a lightweight 3D neural network. This 3D feature map, combined with the RGB image, serves as input to a traditional image segmentation network. Our method can enhance the performance of various 2D segmentation networks with minimal increase in network complexity and data requirements, as it does not rely on 3D ground truth data."}, "2309.09379": {"original_text": "  Nowadays, photogrammetrically derived point clouds are widely used in many\ncivilian applications due to their low cost and flexibility in acquisition.\nTypically, photogrammetric point clouds are assessed through reference data\nsuch as LiDAR point clouds. However, when reference data are not available, the\nassessment of photogrammetric point clouds may be challenging. Since these\npoint clouds are algorithmically derived, their accuracies and precisions are\nhighly varying with the camera networks, scene complexity, and dense image\nmatching (DIM) algorithms, and there is no standard error metric to determine\nper-point errors. The theory of internal reliability of camera networks has\nbeen well studied through first-order error estimation of Bundle Adjustment\n(BA), which is used to understand the errors of 3D points assuming known\nmeasurement errors. However, the measurement errors of the DIM algorithms are\nintricate to an extent that every single point may have its error function\ndetermined by factors such as pixel intensity, texture entropy, and surface\nsmoothness. Despite the complexity, there exist a few common metrics that may\naid the process of estimating the posterior reliability of the derived points,\nespecially in a multi-view stereo (MVS) setup when redundancies are present. In\nthis paper, by using an aerial oblique photogrammetric block with LiDAR\nreference data, we analyze several internal matching metrics within a common\nMVS framework, including statistics in ray convergence, intersection angles,\nDIM energy, etc.\n", "rewritten_text": "Currently, point clouds generated through photogrammetry are widely utilized in various civilian applications due to their cost-effectiveness and ease of acquisition. Typically, these point clouds are evaluated by comparing them to reference data from LiDAR point clouds. However, assessing photogrammetric point clouds can be challenging when reference data is unavailable. The accuracy and precision of these point clouds vary significantly based on factors such as camera networks, scene complexity, and dense image matching algorithms. There is no standardized error metric to determine errors on a per-point basis. While the reliability of camera networks has been extensively studied through Bundle Adjustment for first-order error estimation, the measurement errors of dense image matching algorithms are complex and can be influenced by factors like pixel intensity, texture entropy, and surface smoothness. Despite this complexity, there are a few common metrics that can assist in estimating the reliability of derived points, particularly in a multi-view stereo setup with redundancies. This study analyzes various internal matching metrics within a multi-view stereo framework using an aerial oblique photogrammetric block and LiDAR reference data, including statistics on ray convergence, intersection angles, and dense image matching energy."}, "2306.05061": {"original_text": "  Multi-task visual perception has a wide range of applications in scene\nunderstanding such as autonomous driving. In this work, we devise an efficient\nunified framework to solve multiple common perception tasks, including instance\nsegmentation, semantic segmentation, monocular 3D detection, and depth\nestimation. Simply sharing the same visual feature representations for these\ntasks impairs the performance of tasks, while independent task-specific feature\nextractors lead to parameter redundancy and latency. Thus, we design two\nfeature-merge branches to learn feature basis, which can be useful to, and thus\nshared by, multiple perception tasks. Then, each task takes the corresponding\nfeature basis as the input of the prediction task head to fulfill a specific\ntask. In particular, one feature merge branch is designed for instance-level\nrecognition the other for dense predictions. To enhance inter-branch\ncommunication, the instance branch passes pixel-wise spatial information of\neach instance to the dense branch using efficient dynamic convolution\nweighting. Moreover, a simple but effective dynamic routing mechanism is\nproposed to isolate task-specific features and leverage common properties among\ntasks. Our proposed framework, termed D2BNet, demonstrates a unique approach to\nparameter-efficient predictions for multi-task perception. In addition, as\ntasks benefit from co-training with each other, our solution achieves on par\nresults on partially labeled settings on nuScenes and outperforms previous\nworks for 3D detection and depth estimation on the Cityscapes dataset with full\nsupervision.\n", "rewritten_text": "The concept of multi-task visual perception has various practical applications, such as in autonomous driving. In this study, we introduce an effective unified framework to address multiple common perception tasks, including instance segmentation, semantic segmentation, monocular 3D detection, and depth estimation. Simply using the same visual feature representations for these tasks can hinder performance, while employing independent task-specific feature extractors can result in redundant parameters and delays. To overcome these challenges, we have developed two feature-merge branches to learn a shared feature basis that can be utilized by multiple perception tasks. Each task then utilizes the corresponding feature basis as input for its prediction task head to accomplish its specific goal. One feature merge branch is dedicated to instance-level recognition, while the other focuses on dense predictions. To facilitate communication between the branches, the instance branch conveys pixel-wise spatial information of each instance to the dense branch through efficient dynamic convolution weighting. Additionally, we propose a straightforward yet effective dynamic routing mechanism to separate task-specific features and leverage common characteristics among tasks. Our framework, known as D2BNet, presents a novel approach to achieving parameter-efficient predictions for multi-task perception. Furthermore, through co-training with each other, our solution delivers comparable results in partially labeled settings on nuScenes and surpasses previous works in 3D detection and depth estimation on the Cityscapes dataset with full supervision."}, "2107.00977": {"original_text": "  Cardiac ultrasound imaging is used to diagnose various heart diseases. Common\nanalysis pipelines involve manual processing of the video frames by expert\nclinicians. This suffers from intra- and inter-observer variability. We propose\na novel approach to ultrasound video analysis using a transformer architecture\nbased on a Residual Auto-Encoder Network and a BERT model adapted for token\nclassification. This enables videos of any length to be processed. We apply our\nmodel to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection\nand the automated computation of the left ventricular ejection fraction. We\nachieve an average frame distance of 3.36 frames for the ES and 7.17 frames for\nthe ED on videos of arbitrary length. Our end-to-end learnable approach can\nestimate the ejection fraction with a MAE of 5.95 and $R^2$ of 0.52 in 0.15s\nper video, showing that segmentation is not the only way to predict ejection\nfraction. Code and models are available at https://github.com/HReynaud/UVT.\n", "rewritten_text": "Cardiac ultrasound imaging is utilized for diagnosing a variety of heart conditions. Traditional analysis methods involve manual review of video frames by skilled clinicians, which can lead to inconsistencies. Our innovative approach employs a transformer architecture incorporating a Residual Auto-Encoder Network and a BERT model customized for token classification to analyze ultrasound videos of any length. We focus on detecting End-Systolic (ES) and End-Diastolic (ED) frames and automatically calculating the left ventricular ejection fraction. Our model achieves an average frame distance of 3.36 frames for ES and 7.17 frames for ED in videos of varying lengths. With an end-to-end learnable process, we can estimate ejection fraction with a Mean Absolute Error of 5.95 and an R-squared value of 0.52 in just 0.15 seconds per video, demonstrating an alternative to segmentation for predicting ejection fraction. The code and models can be accessed at https://github.com/HReynaud/UVT."}, "1710.05994": {"original_text": "  Recent advancements in neutron and X-ray sources, instrumentation and data\ncollection modes have significantly increased the experimental data size (which\ncould easily contain 10$^{8}$ -- 10$^{10}$ data points), so that conventional\nvolumetric visualization approaches become inefficient for both still imaging\nand interactive OpenGL rendition in a 3D setting. We introduce a new approach\nbased on the unsupervised machine learning algorithm, Density-Based Spatial\nClustering of Applications with Noise (DBSCAN), to efficiently analyze and\nvisualize large volumetric datasets. Here we present two examples of analyzing\nand visualizing datasets from the diffuse scattering experiment of a single\ncrystal sample and the tomographic reconstruction of a neutron scanning of a\nturbine blade. We found that by using the intensity as the weighting factor in\nthe clustering process, DBSCAN becomes very effective in de-noising and\nfeature/boundary detection, and thus enables better visualization of the\nhierarchical internal structures of the neutron scattering data.\n", "rewritten_text": "Recent advancements in neutron and X-ray sources, instrumentation, and data collection methods have led to a significant increase in the size of experimental datasets, which can now easily contain between 10$^{8}$ and 10$^{10}$ data points. As a result, traditional volumetric visualization techniques are no longer efficient for both static imaging and interactive OpenGL rendering in a 3D environment. To address this challenge, we propose a novel approach utilizing the unsupervised machine learning algorithm, Density-Based Spatial Clustering of Applications with Noise (DBSCAN), to effectively analyze and visualize large volumetric datasets. In this study, we demonstrate the application of this approach in analyzing datasets from a diffuse scattering experiment on a single crystal sample and the tomographic reconstruction of a neutron scan of a turbine blade. Our results show that by incorporating intensity as a weighting factor in the clustering process, DBSCAN proves to be highly effective in noise reduction, feature detection, and boundary delineation. This enables improved visualization of the intricate internal structures present in neutron scattering data."}, "1812.0081": {"original_text": "  It is well known that the generative adversarial nets (GANs) are remarkably\ndifficult to train. The recently proposed Wasserstein GAN (WGAN) creates\nprincipled research directions towards addressing these issues. But we found in\npractice that gradient penalty WGANs (GP-WGANs) still suffer from training\ninstability. In this paper, we combine a Total Variational (TV) regularizing\nterm into the WGAN formulation instead of weight clipping or gradient penalty,\nwhich implies that the Lipschitz constraint is enforced on the critic network.\nOur proposed method is more stable at training than GP-WGANs and works well\nacross varied GAN architectures. We also present a method to control the\ntrade-off between image diversity and visual quality. It does not bring any\ncomputation burden.\n", "rewritten_text": "\"It is widely recognized that training generative adversarial networks (GANs) can be quite challenging. The introduction of Wasserstein GAN (WGAN) has provided a structured approach to tackle these difficulties. However, our practical experience has shown that gradient penalty WGANs (GP-WGANs) still encounter issues with training stability. In this study, we incorporate a Total Variational (TV) regularization term into the WGAN framework instead of relying on weight clipping or gradient penalty. This approach ensures that the critic network adheres to the Lipschitz constraint. Our proposed method demonstrates improved training stability compared to GP-WGANs and performs effectively across various GAN architectures. Additionally, we introduce a technique to manage the balance between image diversity and visual quality without adding any computational overhead.\""}, "1709.06247": {"original_text": "  Most of convolutional neural networks share the same characteristic: each\nconvolutional layer is followed by a nonlinear activation layer where Rectified\nLinear Unit (ReLU) is the most widely used. In this paper, we argue that the\ndesigned structure with the equal ratio between these two layers may not be the\nbest choice since it could result in the poor generalization ability. Thus, we\ntry to investigate a more suitable method on using ReLU to explore the better\nnetwork architectures. Specifically, we propose a proportional module to keep\nthe ratio between convolution and ReLU amount to be N:M (N>M). The proportional\nmodule can be applied in almost all networks with no extra computational cost\nto improve the performance. Comprehensive experimental results indicate that\nthe proposed method achieves better performance on different benchmarks with\ndifferent network architectures, thus verify the superiority of our work.\n", "rewritten_text": "The majority of convolutional neural networks exhibit a common feature: after each convolutional layer, there is a subsequent nonlinear activation layer typically utilizing Rectified Linear Unit (ReLU). This study challenges the conventional approach of maintaining an equal ratio between these two layers, suggesting that it may not yield optimal results in terms of generalization ability. To address this, we explore a more effective method of incorporating ReLU to enhance network architectures. Our proposed solution involves a proportional module that maintains a ratio of N:M (N>M) between convolution and ReLU layers. This module can be seamlessly integrated into existing networks without incurring additional computational costs, leading to performance improvements. Extensive experimental findings demonstrate the superior performance of our approach across various benchmarks and network structures, validating the effectiveness of our work."}, "2201.06674": {"original_text": "  Providing feedback on the argumentation of the learner is essential for\ndeveloping critical thinking skills, however, it requires a lot of time and\neffort. To mitigate the overload on teachers, we aim to automate a process of\nproviding feedback, especially giving diagnostic comments which point out the\nweaknesses inherent in the argumentation. It is recommended to give specific\ndiagnostic comments so that learners can recognize the diagnosis without\nmisinterpretation. However, it is not obvious how the task of providing\nspecific diagnostic comments should be formulated. We present a formulation of\nthe task as template selection and slot filling to make an automatic evaluation\neasier and the behavior of the model more tractable. The key to the formulation\nis the possibility of creating a template set that is sufficient for practical\nuse. In this paper, we define three criteria that a template set should\nsatisfy: expressiveness, informativeness, and uniqueness, and verify the\nfeasibility of creating a template set that satisfies these criteria as a first\ntrial. We will show that it is feasible through an annotation study that\nconverts diagnostic comments given in a text to a template format. The corpus\nused in the annotation study is publicly available.\n", "rewritten_text": "Providing feedback on a learner's argumentation is crucial for developing critical thinking skills, but it can be time-consuming and demanding. To alleviate the burden on teachers, we aim to automate the feedback process, particularly by offering diagnostic comments that highlight weaknesses in the argumentation. It is important to provide specific diagnostic comments to ensure learners understand the feedback accurately. However, determining how to generate these specific comments is not straightforward. We propose a method of template selection and slot filling to streamline the automatic evaluation process and make the model's behavior more manageable. The key to this approach is creating a template set that is practical for use. In this paper, we establish three criteria for an effective template set: expressiveness, informativeness, and uniqueness. We demonstrate the feasibility of creating such a template set through an annotation study that converts diagnostic comments from text into a template format. The corpus used in this study is publicly accessible."}, "2008.01167": {"original_text": "  Current anchor-free object detectors label all the features that spatially\nfall inside a predefined central region of a ground-truth box as positive. This\napproach causes label noise during training, since some of these positively\nlabeled features may be on the background or an occluder object, or they are\nsimply not discriminative features. In this paper, we propose a new labeling\nstrategy aimed to reduce the label noise in anchor-free detectors. We sum-pool\npredictions stemming from individual features into a single prediction. This\nallows the model to reduce the contributions of non-discriminatory features\nduring training. We develop a new one-stage, anchor-free object detector,\nPPDet, to employ this labeling strategy during training and a similar\nprediction pooling method during inference. On the COCO dataset, PPDet achieves\nthe best performance among anchor-free top-down detectors and performs on-par\nwith the other state-of-the-art methods. It also outperforms all major\none-stage and two-stage methods in small object detection (${AP}_{S}$ $31.4$).\nCode is available at https://github.com/nerminsamet/ppdet\n", "rewritten_text": "The current approach used by anchor-free object detectors involves labeling all features within a predefined central region of a ground-truth box as positive. However, this method can lead to label noise during training, as some of these positively labeled features may actually belong to the background, an occluder object, or may not be discriminative features. In this study, we introduce a new labeling strategy to reduce label noise in anchor-free detectors. Our approach involves sum-pooling predictions from individual features to generate a single prediction, enabling the model to diminish the impact of non-discriminatory features during training. We present a new one-stage, anchor-free object detector called PPDet, which incorporates this labeling strategy during training and a similar prediction pooling method during inference. On the COCO dataset, PPDet achieves superior performance compared to other anchor-free top-down detectors and is on par with state-of-the-art methods. Notably, it outperforms major one-stage and two-stage methods in small object detection (AP_S 31.4). The code for PPDet is available at https://github.com/nerminsamet/ppdet."}, "1807.10657": {"original_text": "  Saliency map estimation in computer vision aims to estimate the locations\nwhere people gaze in images. Since people tend to look at objects in images,\nthe parameters of the model pretrained on ImageNet for image classification are\nuseful for the saliency map estimation. However, there is no research on the\nrelationship between the image classification accuracy and the performance of\nthe saliency map estimation. In this paper, it is shown that there is a strong\ncorrelation between image classification accuracy and saliency map estimation\naccuracy. We also investigated the effective architecture based on multi scale\nimages and the upsampling layers to refine the saliency-map resolution. Our\nmodel achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003\ndatasets. In the MIT Saliency Benchmark, our model achieved the best\nperformance in some metrics and competitive results in the other metrics.\n", "rewritten_text": "The goal of saliency map estimation in computer vision is to predict where people focus their attention in images. By leveraging a model pretrained on ImageNet for image classification, which captures common objects people look at, we can enhance saliency map estimation. However, little research has explored the link between image classification accuracy and saliency map estimation performance. This study reveals a strong correlation between the two. We also propose an effective architecture utilizing multi-scale images and upsampling layers to enhance saliency-map resolution. Our model outperformed existing methods on datasets like PASCAL-S, OSIE, and MIT1003. In the MIT Saliency Benchmark, our model excelled in certain metrics and delivered competitive results in others."}, "1812.05642": {"original_text": "  Unsupervised learning for geometric perception (depth, optical flow, etc.) is\nof great interest to autonomous systems. Recent works on unsupervised learning\nhave made considerable progress on perceiving geometry; however, they usually\nignore the coherence of objects and perform poorly under scenarios with dark\nand noisy environments. In contrast, supervised learning algorithms, which are\nrobust, require large labeled geometric dataset. This paper introduces SIGNet,\na novel framework that provides robust geometry perception without requiring\ngeometrically informative labels. Specifically, SIGNet integrates semantic\ninformation to make depth and flow predictions consistent with objects and\nrobust to low lighting conditions. SIGNet is shown to improve upon the\nstate-of-the-art unsupervised learning for depth prediction by 30% (in squared\nrelative error). In particular, SIGNet improves the dynamic object class\nperformance by 39% in depth prediction and 29% in flow prediction. Our code\nwill be made available at https://github.com/mengyuest/SIGNet\n", "rewritten_text": "Unsupervised learning in geometric perception, such as depth and optical flow, is a key focus for autonomous systems. While recent advancements in unsupervised learning have enhanced geometry perception, they often struggle in maintaining object coherence and performing well in dark and noisy environments. On the other hand, supervised learning algorithms, though robust, rely on extensive labeled geometric datasets. This paper presents SIGNet, a new framework that achieves reliable geometry perception without the need for geometrically informative labels. SIGNet leverages semantic information to ensure depth and flow predictions align with objects and remain effective in low-light conditions. SIGNet outperforms current unsupervised learning methods for depth prediction by 30% (in squared relative error), particularly enhancing dynamic object class performance by 39% in depth prediction and 29% in flow prediction. The code for SIGNet will be accessible at https://github.com/mengyuest/SIGNet."}, "2212.09098": {"original_text": "  Fine-grained semantic segmentation of a person's face and head, including\nfacial parts and head components, has progressed a great deal in recent years.\nHowever, it remains a challenging task, whereby considering ambiguous\nocclusions and large pose variations are particularly difficult. To overcome\nthese difficulties, we propose a novel framework termed Mask-FPAN. It uses a\nde-occlusion module that learns to parse occluded faces in a semi-supervised\nway. In particular, face landmark localization, face occlusionstimations, and\ndetected head poses are taken into account. A 3D morphable face model combined\nwith the UV GAN improves the robustness of 2D face parsing. In addition, we\nintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for face\nparing work. The proposed Mask-FPAN framework addresses the face parsing\nproblem in the wild and shows significant performance improvements with MIOU\nfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging face\ndatasets.\n", "rewritten_text": "In recent years, there has been significant progress in the detailed segmentation of a person's face and head, including facial features and head components. However, this task remains challenging due to issues such as ambiguous occlusions and varying poses. To address these challenges, we present a new framework called Mask-FPAN. This framework incorporates a de-occlusion module that learns to identify obscured faces in a semi-supervised manner, taking into consideration factors like face landmark localization, face occlusion estimations, and detected head poses. By combining a 3D morphable face model with UV GAN, the robustness of 2D face segmentation is enhanced. Additionally, we have introduced two new datasets, FaceOccMask-HQ and CelebAMaskOcc-HQ, for face segmentation tasks. The Mask-FPAN framework aims to tackle face segmentation challenges in real-world scenarios and has shown significant performance improvements, achieving an MIOU increase from 0.7353 to 0.9013 compared to current state-of-the-art methods on challenging face datasets."}, "2206.08347": {"original_text": "  By leveraging contrastive learning, clustering, and other pretext tasks,\nunsupervised methods for learning image representations have reached impressive\nresults on standard benchmarks. The result has been a crowded field - many\nmethods with substantially different implementations yield results that seem\nnearly identical on popular benchmarks, such as linear evaluation on ImageNet.\nHowever, a single result does not tell the whole story. In this paper, we\ncompare methods using performance-based benchmarks such as linear evaluation,\nnearest neighbor classification, and clustering for several different datasets,\ndemonstrating the lack of a clear front-runner within the current\nstate-of-the-art. In contrast to prior work that performs only supervised vs.\nunsupervised comparison, we compare several different unsupervised methods\nagainst each other. To enrich this comparison, we analyze embeddings with\nmeasurements such as uniformity, tolerance, and centered kernel alignment\n(CKA), and propose two new metrics of our own: nearest neighbor graph\nsimilarity and linear prediction overlap. We reveal through our analysis that\nin isolation, single popular methods should not be treated as though they\nrepresent the field as a whole, and that future work ought to consider how to\nleverage the complimentary nature of these methods. We also leverage CKA to\nprovide a framework to robustly quantify augmentation invariance, and provide a\nreminder that certain types of invariance will be undesirable for downstream\ntasks.\n", "rewritten_text": "Through the utilization of contrastive learning, clustering, and other pretext tasks, unsupervised methods for learning image representations have achieved impressive results on standard benchmarks. This has led to a competitive landscape, where numerous methods with varying implementations produce results that appear very similar on popular benchmarks like linear evaluation on ImageNet. However, it is important to note that a single result does not provide the complete picture. In this paper, we conduct a comparative analysis of methods using performance-based benchmarks such as linear evaluation, nearest neighbor classification, and clustering across multiple datasets. Our findings demonstrate that there is no clear frontrunner among the current state-of-the-art methods. Unlike previous studies that primarily focus on comparing supervised versus unsupervised approaches, we compare different unsupervised methods against each other. To enhance this comparison, we evaluate embeddings using metrics such as uniformity, tolerance, and centered kernel alignment (CKA), and introduce two new metrics: nearest neighbor graph similarity and linear prediction overlap. Our analysis reveals that individual popular methods should not be considered representative of the entire field, and future research should explore how to leverage the complementary aspects of these methods. Additionally, we leverage CKA to establish a robust framework for quantifying augmentation invariance and emphasize that certain types of invariance may not be beneficial for downstream tasks."}, "2108.08432": {"original_text": "  Deep learning has achieved remarkable success in medicalimage segmentation,\nbut it usually requires a large numberof images labeled with fine-grained\nsegmentation masks, andthe annotation of these masks can be very expensive\nandtime-consuming. Therefore, recent methods try to use un-supervised domain\nadaptation (UDA) methods to borrow in-formation from labeled data from other\ndatasets (source do-mains) to a new dataset (target domain). However, due tothe\nabsence of labels in the target domain, the performance ofUDA methods is much\nworse than that of the fully supervisedmethod. In this paper, we propose a\nweakly supervised do-main adaptation setting, in which we can partially label\nnewdatasets with bounding boxes, which are easier and cheaperto obtain than\nsegmentation masks. Accordingly, we proposea new weakly-supervised domain\nadaptation method calledBox-Adapt, which fully explores the fine-grained\nsegmenta-tion mask in the source domain and the weak bounding boxin the target\ndomain. Our Box-Adapt is a two-stage methodthat first performs joint training\non the source and target do-mains, and then conducts self-training with the\npseudo-labelsof the target domain. We demonstrate the effectiveness of\nourmethod in the liver segmentation task. Weakly supervised do-main adaptation\n", "rewritten_text": "Deep learning has made significant strides in medical image segmentation, but typically requires a large dataset with meticulously labeled segmentation masks, which can be costly and time-consuming to annotate. To address this challenge, recent approaches have turned to unsupervised domain adaptation (UDA) techniques to leverage information from labeled data in other datasets (source domains) for a new dataset (target domain). However, UDA methods often struggle in the absence of labels in the target domain, resulting in lower performance compared to fully supervised methods. In this study, we introduce a weakly supervised domain adaptation framework where new datasets can be partially labeled with bounding boxes, which are more accessible and cost-effective than segmentation masks. Our proposed weakly-supervised domain adaptation method, Box-Adapt, leverages fine-grained segmentation masks from the source domain and weak bounding boxes from the target domain. Box-Adapt operates in two stages: joint training on both domains followed by self-training using pseudo-labels from the target domain. We showcase the efficacy of our approach in liver segmentation tasks within the weakly supervised domain adaptation context."}, "2203.02231": {"original_text": "  Light field disparity estimation is an essential task in computer vision with\nvarious applications. Although supervised learning-based methods have achieved\nboth higher accuracy and efficiency than traditional optimization-based\nmethods, the dependency on ground-truth disparity for training limits the\noverall generalization performance not to say for real-world scenarios where\nthe ground-truth disparity is hard to capture. In this paper, we argue that\nunsupervised methods can achieve comparable accuracy, but, more importantly,\nmuch higher generalization capacity and efficiency than supervised methods.\nSpecifically, we present the Occlusion Pattern Aware Loss, named OPAL, which\nsuccessfully extracts and encodes the general occlusion patterns inherent in\nthe light field for loss calculation. OPAL enables: i) accurate and robust\nestimation by effectively handling occlusions without using any ground-truth\ninformation for training and ii) much efficient performance by significantly\nreducing the network parameters required for accurate inference. Besides, a\ntransformer-based network and a refinement module are proposed for achieving\neven more accurate results. Extensive experiments demonstrate our method not\nonly significantly improves the accuracy compared with the SOTA unsupervised\nmethods, but also possesses strong generalization capacity, even for real-world\ndata, compared with supervised methods. Our code will be made publicly\navailable.\n", "rewritten_text": "The estimation of light field disparity is a crucial task in computer vision with a range of applications. While supervised learning methods have shown higher accuracy and efficiency compared to traditional optimization-based approaches, their reliance on ground-truth disparity data for training can limit overall performance, especially in real-world scenarios where capturing ground-truth disparity is challenging. This paper argues that unsupervised methods can achieve comparable accuracy, along with superior generalization capacity and efficiency compared to supervised methods. The proposed Occlusion Pattern Aware Loss (OPAL) effectively captures and encodes occlusion patterns in light fields for loss calculation, enabling accurate and robust estimation without the need for ground-truth data during training. OPAL also enhances efficiency by reducing the required network parameters for accurate inference. Additionally, a transformer-based network and refinement module are introduced to further improve accuracy. Extensive experiments demonstrate that the proposed method not only enhances accuracy significantly compared to state-of-the-art unsupervised methods but also exhibits strong generalization capacity, even with real-world data, surpassing supervised methods. The code for our method will be publicly available."}, "2006.1038": {"original_text": "  Video semantic segmentation is active in recent years benefited from the\ngreat progress of image semantic segmentation. For such a task, the per-frame\nimage segmentation is generally unacceptable in practice due to high\ncomputation cost. To tackle this issue, many works use the flow-based feature\npropagation to reuse the features of previous frames. However, the optical flow\nestimation inevitably suffers inaccuracy and then causes the propagated\nfeatures distorted. In this paper, we propose distortion-aware feature\ncorrection to alleviate the issue, which improves video segmentation\nperformance by correcting distorted propagated features. To be specific, we\nfirstly propose to transfer distortion patterns from feature into image space\nand conduct effective distortion map prediction. Benefited from the guidance of\ndistortion maps, we proposed Feature Correction Module (FCM) to rectify\npropagated features in the distorted areas. Our proposed method can\nsignificantly boost the accuracy of video semantic segmentation at a low price.\nThe extensive experimental results on Cityscapes and CamVid show that our\nmethod outperforms the recent state-of-the-art methods.\n", "rewritten_text": "In recent years, video semantic segmentation has seen significant advancements, largely due to the progress made in image semantic segmentation. However, performing per-frame image segmentation for this task is often impractical due to the high computational cost involved. To address this challenge, many studies have turned to flow-based feature propagation to reuse features from previous frames. Unfortunately, the accuracy of optical flow estimation can lead to distorted propagated features. In this study, we introduce a distortion-aware feature correction approach to mitigate this issue, enhancing video segmentation performance by rectifying distorted features. Specifically, we propose transferring distortion patterns from features to image space to predict effective distortion maps. With the guidance of these maps, our Feature Correction Module (FCM) corrects features in distorted areas. Our method significantly improves video semantic segmentation accuracy at a minimal cost. Extensive experiments on Cityscapes and CamVid datasets demonstrate that our approach surpasses recent state-of-the-art methods."}, "2101.11251": {"original_text": "  Junctions reflect the important geometrical structure information of the\nimage, and are of primary significance to applications such as image matching\nand motion analysis. Previous event-based feature extraction methods are mainly\nfocused on corners, which mainly find their locations, however, ignoring the\ngeometrical structure information like orientations and scales of edges. This\npaper adapts the frame-based a-contrario junction detector(ACJ) to event data,\nproposing the event-based a-contrario junction detector(e-ACJ), which yields\njunctions' locations while giving the scales and orientations of their\nbranches. The proposed method relies on an a-contrario model and can operate on\nasynchronous events directly without generating synthesized event frames. We\nevaluate the performance on public event datasets. The result shows our method\nsuccessfully finds the orientations and scales of branches, while maintaining\nhigh accuracy in junction's location.\n", "rewritten_text": "\"Junctions are crucial for applications like image matching and motion analysis as they provide important geometric structure information in images. While previous feature extraction methods have mainly focused on corners, this paper introduces the event-based a-contrario junction detector (e-ACJ) by adapting the frame-based ACJ to event data. The e-ACJ not only identifies junction locations but also captures the orientations and scales of their branches. This method utilizes an a-contrario model and can process asynchronous events directly without the need for synthesized event frames. Performance evaluation on public event datasets demonstrates that our approach successfully determines branch orientations and scales while maintaining high accuracy in locating junctions.\""}, "2210.16239": {"original_text": "  The Hyperspectral image (HSI) contains several hundred bands of the same\nregion called the Ground Truth (GT). The bands are taken in juxtaposed\nfrequencies, but some of them are noisily measured or contain no information.\nFor the classification, the selection of bands, affects significantly the\nresults of classification, in fact, using a subset of relevant bands, these\nresults can be better than those obtained using all bands, from which the need\nto reduce the dimensionality of the HSI. In this paper, a categorization of\ndimensionality reduction methods, according to the generation process, is\npresented. Furthermore, we reproduce an algorithm based on mutual information\n(MI) to reduce dimensionality by features selection and we introduce an\nalgorithm using mutual information and homogeneity. The two schemas are a\nfilter strategy. Finally, to validate this, we consider the case study AVIRIS\nHSI 92AV3C.\n  Keywords: Hyperspectrale images; classification; features selection; mutual\ninformation; homogeneity\n", "rewritten_text": "The Hyperspectral image (HSI) includes numerous bands capturing the same area known as the Ground Truth (GT). These bands are captured at different frequencies, with some being noisy or lacking useful information. Band selection plays a crucial role in classification accuracy, as using a subset of relevant bands can yield better results than using all bands. Therefore, reducing the dimensionality of the HSI is necessary. This paper categorizes dimensionality reduction methods based on their generation process. Additionally, we present an algorithm utilizing mutual information (MI) for feature selection and introduce another algorithm combining mutual information and homogeneity. Both approaches act as filter strategies. To validate these methods, we apply them to the AVIRIS HSI 92AV3C case study. Keywords: Hyperspectral images; classification; feature selection; mutual information; homogeneity."}, "2108.07127": {"original_text": "  We translate a closed text that is known in advance and available in many\nlanguages into a new and severely low resource language. Most human translation\nefforts adopt a portion-based approach to translate consecutive pages/chapters\nin order, which may not suit machine translation. We compare the portion-based\napproach that optimizes coherence of the text locally with the random sampling\napproach that increases coverage of the text globally. Our results show that\nthe random sampling approach performs better. When training on a seed corpus of\n~1,000 lines from the Bible and testing on the rest of the Bible (~30,000\nlines), random sampling gives a performance gain of +11.0 BLEU using English as\na simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a\nMayan language. Furthermore, we compare three ways of updating machine\ntranslation models with increasing amount of human post-edited data through\niterations. We find that adding newly post-edited data to training after\nvocabulary update without self-supervision performs the best. We propose an\nalgorithm for human and machine to work together seamlessly to translate a\nclosed text into a severely low resource language.\n", "rewritten_text": "We take a pre-existing closed text available in multiple languages and translate it into a new and severely low-resource language. While most human translation efforts focus on translating consecutive pages or chapters in a portion-based manner, this may not be ideal for machine translation. In our study, we compare a portion-based approach that prioritizes local text coherence with a random sampling approach that aims to enhance global text coverage. Our findings indicate that the random sampling approach yields better results. For instance, when training on a seed corpus of approximately 1,000 lines from the Bible and testing on the remaining 30,000 lines, random sampling leads to an improvement of +11.0 BLEU when using English as a simulated low-resource language, and +4.9 BLEU when using Eastern Pokomchi, a Mayan language. Additionally, we explore different methods of updating machine translation models with increasing amounts of human post-edited data over iterations. Our research suggests that incorporating newly post-edited data into training after updating the vocabulary, without self-supervision, produces the most favorable outcomes. We propose an algorithm that facilitates seamless collaboration between humans and machines to translate a closed text into a severely low-resource language."}, "2103.10978": {"original_text": "  This paper addresses the problem of 3D human body shape and pose estimation\nfrom RGB images. Recent progress in this field has focused on single images,\nvideo or multi-view images as inputs. In contrast, we propose a new task: shape\nand pose estimation from a group of multiple images of a human subject, without\nconstraints on subject pose, camera viewpoint or background conditions between\nimages in the group. Our solution to this task predicts distributions over SMPL\nbody shape and pose parameters conditioned on the input images in the group. We\nprobabilistically combine predicted body shape distributions from each image to\nobtain a final multi-image shape prediction. We show that the additional body\nshape information present in multi-image input groups improves 3D human shape\nestimation metrics compared to single-image inputs on the SSP-3D dataset and a\nprivate dataset of tape-measured humans. In addition, predicting distributions\nover 3D bodies allows us to quantify pose prediction uncertainty, which is\nuseful when faced with challenging input images with significant occlusion. Our\nmethod demonstrates meaningful pose uncertainty on the 3DPW dataset and is\ncompetitive with the state-of-the-art in terms of pose estimation metrics.\n", "rewritten_text": "This paper discusses the challenge of estimating the 3D shape and pose of a human body from RGB images. While previous research has focused on using single images, videos, or multiple views as inputs, we introduce a novel approach: estimating shape and pose from a collection of multiple images of a human subject, without restrictions on subject pose, camera angle, or background variations between images. Our method involves predicting distributions over SMPL body shape and pose parameters based on the input images in the group. By combining these predicted distributions from each image probabilistically, we generate a final prediction of the human body shape using multiple images. Our results demonstrate that utilizing information from multiple images enhances 3D human shape estimation accuracy compared to using single images, as shown on the SSP-3D dataset and a private dataset of tape-measured humans. Furthermore, by predicting distributions over 3D bodies, we are able to quantify uncertainty in pose prediction, particularly useful for challenging images with significant occlusion. Our approach shows meaningful pose uncertainty on the 3DPW dataset and performs competitively with state-of-the-art methods in terms of pose estimation metrics."}, "2307.01447": {"original_text": "  Accurately matching local features between a pair of images is a challenging\ncomputer vision task. Previous studies typically use attention based graph\nneural networks (GNNs) with fully-connected graphs over keypoints within/across\nimages for visual and geometric information reasoning. However, in the context\nof feature matching, considerable keypoints are non-repeatable due to occlusion\nand failure of the detector, and thus irrelevant for message passing. The\nconnectivity with non-repeatable keypoints not only introduces redundancy,\nresulting in limited efficiency, but also interferes with the representation\naggregation process, leading to limited accuracy. Targeting towards high\naccuracy and efficiency, we propose MaKeGNN, a sparse attention-based GNN\narchitecture which bypasses non-repeatable keypoints and leverages matchable\nones to guide compact and meaningful message passing. More specifically, our\nBilateral Context-Aware Sampling Module first dynamically samples two small\nsets of well-distributed keypoints with high matchability scores from the image\npair. Then, our Matchable Keypoint-Assisted Context Aggregation Module regards\nsampled informative keypoints as message bottlenecks and thus constrains each\nkeypoint only to retrieve favorable contextual information from intra- and\ninter- matchable keypoints, evading the interference of irrelevant and\nredundant connectivity with non-repeatable ones. Furthermore, considering the\npotential noise in initial keypoints and sampled matchable ones, the MKACA\nmodule adopts a matchability-guided attentional aggregation operation for purer\ndata-dependent context propagation. By these means, we achieve the\nstate-of-the-art performance on relative camera estimation, fundamental matrix\nestimation, and visual localization, while significantly reducing computational\nand memory complexity compared to typical attentional GNNs.\n", "rewritten_text": "Matching local features accurately between a pair of images is a complex task in computer vision. Previous research often utilizes attention-based graph neural networks (GNNs) with fully-connected graphs over keypoints within and across images to reason about visual and geometric information. However, many keypoints are non-repeatable due to occlusion and detector failures, making them irrelevant for message passing. This connectivity with non-repeatable keypoints not only introduces redundancy and limits efficiency but also hinders representation aggregation, resulting in reduced accuracy. To address these challenges and improve both accuracy and efficiency, we introduce MaKeGNN, a sparse attention-based GNN architecture. MaKeGNN bypasses non-repeatable keypoints and focuses on matchable ones to facilitate compact and meaningful message passing. Our approach includes a Bilateral Context-Aware Sampling Module that dynamically selects well-distributed keypoints with high matchability scores from the image pair. Additionally, our Matchable Keypoint-Assisted Context Aggregation Module treats informative keypoints as message bottlenecks, allowing each keypoint to gather relevant contextual information from intra- and inter-matchable keypoints while avoiding irrelevant and redundant connections with non-repeatable ones. Furthermore, to address potential noise in initial keypoints and sampled matchable ones, the MKACA module incorporates a matchability-guided attentional aggregation operation for more accurate context propagation. These innovations enable MaKeGNN to achieve state-of-the-art performance in relative camera estimation, fundamental matrix estimation, and visual localization, while also reducing computational and memory complexity compared to traditional attentional GNNs."}, "1703.00087": {"original_text": "  Lesion segmentation is the first step in most automatic melanoma recognition\nsystems. Deficiencies and difficulties in dermoscopic images such as color\ninconstancy, hair occlusion, dark corners and color charts make lesion\nsegmentation an intricate task. In order to detect the lesion in the presence\nof these problems, we propose a supervised saliency detection method tailored\nfor dermoscopic images based on the discriminative regional feature integration\n(DRFI). DRFI method incorporates multi-level segmentation, regional contrast,\nproperty, background descriptors, and a random forest regressor to create\nsaliency scores for each region in the image. In our improved saliency\ndetection method, mDRFI, we have added some new features to regional property\ndescriptors. Also, in order to achieve more robust regional background\ndescriptors, a thresholding algorithm is proposed to obtain a new\npseudo-background region. Findings reveal that mDRFI is superior to DRFI in\ndetecting the lesion as the salient object in dermoscopic images. The proposed\noverall lesion segmentation framework uses detected saliency map to construct\nan initial mask of the lesion through thresholding and post-processing\noperations. The initial mask is then evolving in a level set framework to fit\nbetter on the lesion's boundaries. The results of evaluation tests on three\npublic datasets show that our proposed segmentation method outperforms the\nother conventional state-of-the-art segmentation algorithms and its performance\nis comparable with most recent approaches that are based on deep convolutional\nneural networks.\n", "rewritten_text": "Segmenting lesions is the primary task in many automated systems for identifying melanoma. Challenges in dermoscopic images, such as inconsistent colors, hair covering, dark areas, and color variations, make lesion segmentation complex. To address these issues, we introduce a supervised saliency detection method specifically designed for dermoscopic images, utilizing the Discriminative Regional Feature Integration (DRFI) approach. The DRFI method combines multi-level segmentation, regional contrast, property descriptors, background descriptors, and a random forest regressor to assign saliency scores to different regions in the image. Our enhanced saliency detection method, mDRFI, incorporates additional features in regional property descriptors and introduces a thresholding algorithm to improve regional background descriptors. Results demonstrate that mDRFI outperforms DRFI in identifying lesions as salient objects in dermoscopic images. The proposed lesion segmentation framework utilizes the detected saliency map to create an initial lesion mask through thresholding and post-processing steps. This initial mask is then refined using a level set framework to better align with the lesion boundaries. Evaluation tests on three public datasets indicate that our segmentation method surpasses traditional algorithms and performs comparably to recent deep convolutional neural network-based approaches."}, "2405.12209": {"original_text": "  Recent advancements in large language models (LLMs) have showcased\nsignificant improvements in mathematics. However, traditional math benchmarks\nlike GSM8k offer a unidimensional perspective, falling short in providing a\nholistic assessment of the LLMs' math capabilities. To address this gap, we\nintroduce MathBench, a new benchmark that rigorously assesses the mathematical\ncapabilities of large language models. MathBench spans a wide range of\nmathematical disciplines, offering a detailed evaluation of both theoretical\nunderstanding and practical problem-solving skills. The benchmark progresses\nthrough five distinct stages, from basic arithmetic to college mathematics, and\nis structured to evaluate models at various depths of knowledge. Each stage\nincludes theoretical questions and application problems, allowing us to measure\na model's mathematical proficiency and its ability to apply concepts in\npractical scenarios. MathBench aims to enhance the evaluation of LLMs'\nmathematical abilities, providing a nuanced view of their knowledge\nunderstanding levels and problem solving skills in a bilingual context. The\nproject is released at https://github.com/open-compass/MathBench .\n", "rewritten_text": "Recent advancements in large language models have demonstrated significant improvements in mathematics. However, traditional math benchmarks such as GSM8k only offer a one-dimensional view, lacking a comprehensive assessment of the mathematical capabilities of these models. To fill this gap, we present MathBench, a new benchmark designed to thoroughly evaluate the math skills of large language models. MathBench covers a wide range of mathematical disciplines, providing a detailed assessment of both theoretical comprehension and practical problem-solving abilities. The benchmark consists of five distinct stages, ranging from basic arithmetic to college-level mathematics, and is structured to assess models at varying levels of expertise. Each stage includes theoretical questions and real-world application problems, enabling us to gauge a model's mathematical proficiency and its capacity to apply concepts in practical settings. MathBench aims to improve the evaluation of large language models' mathematical skills, offering a nuanced perspective on their knowledge levels and problem-solving capabilities in a bilingual context. The project can be accessed at https://github.com/open-compass/MathBench."}, "2407.03993": {"original_text": "  Natural language counterfactual generation aims to minimally modify a given\ntext such that the modified text will be classified into a different class. The\ngenerated counterfactuals provide insight into the reasoning behind a model's\npredictions by highlighting which words significantly influence the outcomes.\nAdditionally, they can be used to detect model fairness issues and augment the\ntraining data to enhance the model's robustness. A substantial amount of\nresearch has been conducted to generate counterfactuals for various NLP tasks,\nemploying different models and methodologies. With the rapid growth of studies\nin this field, a systematic review is crucial to guide future researchers and\ndevelopers. To bridge this gap, this survey provides a comprehensive overview\nof textual counterfactual generation methods, particularly those based on Large\nLanguage Models. We propose a new taxonomy that systematically categorizes the\ngeneration methods into four groups and summarizes the metrics for evaluating\nthe generation quality. Finally, we discuss ongoing research challenges and\noutline promising directions for future work.\n", "rewritten_text": "The goal of natural language counterfactual generation is to make minimal changes to a given text so that the modified text is classified into a different category. These generated counterfactuals offer insights into how a model makes predictions by highlighting the words that have a significant impact on the outcomes. They can also help identify fairness issues in models and improve model robustness by enhancing training data. Numerous studies have explored generating counterfactuals for various NLP tasks using different models and approaches. Given the rapid growth in this area, a systematic review is essential to guide future research and development. This survey presents a comprehensive overview of methods for generating textual counterfactuals, particularly those based on Large Language Models. It introduces a new taxonomy that categorizes generation methods into four groups and outlines metrics for evaluating generation quality. The survey also discusses current research challenges and suggests promising directions for future work."}, "1909.13411": {"original_text": "  Mesoscale eddies play a significant role in marine energy transport, marine\nbiological environment and marine climate. Due to their huge impact on the\nocean, mesoscale eddy detection has become a hot research area in recent years.\nTherefore, more and more people are entering the field of mesoscale eddy\ndetection. However, the existing detection methods mainly based on traditional\ndetection methods typically only use Sea Surface Height (SSH) as a variable to\ndetect, resulting in inaccurate performance. In this paper, we propose a\nmesoscale eddy detection method based on multivariate fusion data to solve this\nproblem. We not only use the SSH variable, but also add the two variables: Sea\nSurface Temperature (SST) and velocity of flow, achieving a multivariate\ninformation fusion input. We design a novel symmetric network, which merges\nlow-level feature maps from the downsampling pathway and high-level feature\nmaps from the upsampling pathway by lateral connection. In addition, we apply\ndilated convolutions to network structure to increase the receptive field and\nobtain more contextual information in the case of constant parameter. In the\nend, we demonstrate the effectiveness of our method on dataset provided by us,\nachieving the test set performance of 97.06% , greatly improved the performance\nof previous methods of mesoscale eddy detection.\n", "rewritten_text": "Mesoscale eddies play a crucial role in the transport of marine energy, the marine biological environment, and marine climate. The detection of mesoscale eddies has become a popular research area due to their significant impact on the ocean. While traditional detection methods typically rely solely on Sea Surface Height (SSH) as a variable, resulting in inaccuracies, this paper introduces a new method for mesoscale eddy detection. By incorporating Sea Surface Temperature (SST) and flow velocity in addition to SSH, a multivariate fusion approach is utilized. A novel symmetric network is designed to merge low-level and high-level features, enhancing contextual information through dilated convolutions. The proposed method demonstrates superior performance, achieving a test set accuracy of 97.06% and outperforming previous detection methods."}, "1808.07733": {"original_text": "  We analyze the performance of different sentiment classification models on\nsyntactically complex inputs like A-but-B sentences. The first contribution of\nthis analysis addresses reproducible research: to meaningfully compare\ndifferent models, their accuracies must be averaged over far more random seeds\nthan what has traditionally been reported. With proper averaging in place, we\nnotice that the distillation model described in arXiv:1603.06318v4 [cs.LG],\nwhich incorporates explicit logic rules for sentiment classification, is\nineffective. In contrast, using contextualized ELMo embeddings\n(arXiv:1802.05365v2 [cs.CL]) instead of logic rules yields significantly better\nperformance. Additionally, we provide analysis and visualizations that\ndemonstrate ELMo's ability to implicitly learn logic rules. Finally, a\ncrowdsourced analysis reveals how ELMo outperforms baseline models even on\nsentences with ambiguous sentiment labels.\n", "rewritten_text": "We examine how various sentiment classification models perform on complex syntactic inputs, such as A-but-B sentences. Our first finding emphasizes the importance of reproducible research: to compare models accurately, their accuracies should be averaged over a larger number of random seeds than previously reported. After implementing proper averaging, we observe that the distillation model outlined in arXiv:1603.06318v4 [cs.LG], which integrates explicit logic rules for sentiment classification, proves to be ineffective. Conversely, utilizing contextualized ELMo embeddings (arXiv:1802.05365v2 [cs.CL]) instead of logic rules results in significantly improved performance. Furthermore, we present analyses and visualizations that showcase ELMo's capacity to implicitly learn logic rules. Lastly, a crowdsourced evaluation demonstrates how ELMo surpasses baseline models even in cases of sentences with ambiguous sentiment labels."}, "2011.09634": {"original_text": "  In this paper, we teach machines to understand visuals and natural language\nby learning the mapping between sentences and noisy video snippets without\nexplicit annotations. Firstly, we define a self-supervised learning framework\nthat captures the cross-modal information. A novel adversarial learning module\nis then introduced to explicitly handle the noises in the natural videos, where\nthe subtitle sentences are not guaranteed to be strongly corresponded to the\nvideo snippets. For training and evaluation, we contribute a new dataset\n`ApartmenTour' that contains a large number of online videos and subtitles. We\ncarry out experiments on the bidirectional retrieval tasks between sentences\nand videos, and the results demonstrate that our proposed model achieves the\nstate-of-the-art performance on both retrieval tasks and exceeds several strong\nbaselines. The dataset can be downloaded at https://github.com/zyj-13/WAL.\n", "rewritten_text": "This paper presents a method for teaching machines to comprehend visuals and natural language without explicit annotations. The approach involves learning the relationship between sentences and noisy video snippets. The framework includes a self-supervised learning component to capture cross-modal information. An adversarial learning module is introduced to address noise in natural videos where subtitles may not directly correspond to the video content. The study introduces a new dataset called 'ApartmenTour' comprising online videos and subtitles for training and evaluation. Experimental results on bidirectional retrieval tasks show that the proposed model outperforms existing methods and achieves state-of-the-art performance. The dataset is available for download at https://github.com/zyj-13/WAL."}, "1708.01654": {"original_text": "  We demonstrate the use of shape-from-shading (SfS) to improve both the\nquality and the robustness of 3D reconstruction of dynamic objects captured by\na single camera. Unlike previous approaches that made use of SfS as a\npost-processing step, we offer a principled integrated approach that solves\ndynamic object tracking and reconstruction and SfS as a single unified cost\nfunction. Moving beyond Lambertian S f S , we propose a general approach that\nmodels both specularities and shading while simultaneously tracking and\nreconstructing general dynamic objects. Solving these problems jointly prevents\nthe kinds of tracking failures which can not be recovered from by pipeline\napproaches. We show state-of-the-art results both qualitatively and\nquantitatively.\n", "rewritten_text": "We present a novel method utilizing shape-from-shading (SfS) to enhance the quality and reliability of 3D reconstruction for dynamic objects captured by a single camera. In contrast to previous techniques that treated SfS as a separate post-processing step, our integrated approach combines dynamic object tracking, reconstruction, and SfS into a unified cost function. Going beyond traditional Lambertian SfS, our method incorporates specularities and shading while simultaneously tracking and reconstructing diverse dynamic objects. By addressing these challenges together, we avoid tracking failures that cannot be rectified by conventional pipeline methods. Our results demonstrate cutting-edge performance both qualitatively and quantitatively."}, "1711.07721": {"original_text": "  Post capture refocusing effect in smartphone cameras is achievable by using\nfocal stacks. However, the accuracy of this effect is totally dependent on the\ncombination of the depth layers in the stack. The accuracy of the extended\ndepth of field effect in this application can be improved significantly by\ncomputing an accurate depth map which has been an open issue for decades. To\ntackle this issue, in this paper, a framework is proposed based on\nPreconditioned Alternating Direction Method of Multipliers (PADMM) for depth\nfrom the focal stack and synthetic defocus application. In addition to its\nability to provide high structural accuracy and occlusion handling, the\noptimization function of the proposed method can, in fact, converge faster and\nbetter than state of the art methods. The evaluation has been done on 21 sets\nof focal stacks and the optimization function has been compared against 5 other\nmethods. Preliminary results indicate that the proposed method has a better\nperformance in terms of structural accuracy and optimization in comparison to\nthe current state of the art methods.\n", "rewritten_text": "The post-capture refocusing effect in smartphone cameras can be achieved using focal stacks. However, the accuracy of this effect relies on the combination of depth layers in the stack. Enhancing the accuracy of the extended depth of field effect can be achieved by generating a precise depth map, which has been a longstanding challenge. To address this issue, this paper introduces a framework based on the Preconditioned Alternating Direction Method of Multipliers (PADMM) for depth estimation from the focal stack and synthetic defocus application. Apart from offering high structural accuracy and effective occlusion handling, the optimization function of the proposed method demonstrates faster and superior convergence compared to existing methods. The evaluation was conducted on 21 sets of focal stacks, with the optimization function being compared against five other methods. Initial findings suggest that the proposed method outperforms current state-of-the-art methods in terms of structural accuracy and optimization performance."}, "2004.06376": {"original_text": "  Understanding the shape of a scene from a single color image is a formidable\ncomputer vision task. However, most methods aim to predict the geometry of\nsurfaces that are visible to the camera, which is of limited use when planning\npaths for robots or augmented reality agents. Such agents can only move when\ngrounded on a traversable surface, which we define as the set of classes which\nhumans can also walk over, such as grass, footpaths and pavement. Models which\npredict beyond the line of sight often parameterize the scene with voxels or\nmeshes, which can be expensive to use in machine learning frameworks.\n  We introduce a model to predict the geometry of both visible and occluded\ntraversable surfaces, given a single RGB image as input. We learn from stereo\nvideo sequences, using camera poses, per-frame depth and semantic segmentation\nto form training data, which is used to supervise an image-to-image network. We\ntrain models from the KITTI driving dataset, the indoor Matterport dataset, and\nfrom our own casually captured stereo footage. We find that a surprisingly low\nbar for spatial coverage of training scenes is required. We validate our\nalgorithm against a range of strong baselines, and include an assessment of our\npredictions for a path-planning task.\n", "rewritten_text": "Understanding the shape of a scene from a single color image is a challenging task in computer vision. Most existing methods focus on predicting the geometry of visible surfaces, which has limited utility for tasks like robot path planning or augmented reality. These tasks require knowledge of traversable surfaces, defined as areas where humans can walk, such as grass, footpaths, and pavement. Models that go beyond what is directly visible often use voxels or meshes to represent the scene, which can be computationally expensive.\nWe propose a model that predicts the geometry of both visible and occluded traversable surfaces using a single RGB image as input. Our approach leverages stereo video sequences, camera poses, depth information, and semantic segmentation to create training data for an image-to-image network. We train our models on datasets like KITTI, Matterport, and our own stereo footage, demonstrating that even a modest spatial coverage of training scenes is sufficient. We evaluate our algorithm against strong baselines and assess its performance in a path-planning scenario."}, "2107.08173": {"original_text": "  This ability to learn consecutive tasks without forgetting how to perform\npreviously trained problems is essential for developing an online dialogue\nsystem. This paper proposes an effective continual learning for the\ntask-oriented dialogue system with iterative network pruning, expanding and\nmasking (TPEM), which preserves performance on previously encountered tasks\nwhile accelerating learning progress on subsequent tasks. Specifically, TPEM\n(i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts\nnetwork expanding to create free weights for new tasks, and (iii) introduces\ntask-specific network masking to alleviate the negative impact of fixed weights\nof old tasks on new tasks. We conduct extensive experiments on seven different\ntasks from three benchmark datasets and show empirically that TPEM leads to\nsignificantly improved results over the strong competitors. For\nreproducibility, we submit the code and data at:\nhttps://github.com/siat-nlp/TPEM\n", "rewritten_text": "This paper introduces a method for continual learning in task-oriented dialogue systems called iterative network pruning, expanding, and masking (TPEM). TPEM allows the system to learn new tasks while retaining knowledge of previously trained tasks, resulting in improved performance. The approach involves network pruning to maintain knowledge of old tasks, network expanding to accommodate new tasks, and task-specific network masking to mitigate the impact of fixed weights on new tasks. Experimental results on multiple tasks demonstrate the effectiveness of TPEM compared to other methods. The code and data for reproducibility are available at: https://github.com/siat-nlp/TPEM"}, "1811.09789": {"original_text": "  There has been much recent work on image captioning models that describe the\nfactual aspects of an image. Recently, some models have incorporated\nnon-factual aspects into the captions, such as sentiment or style. However,\nsuch models typically have difficulty in balancing the semantic aspects of the\nimage and the non-factual dimensions of the caption; in addition, it can be\nobserved that humans may focus on different aspects of an image depending on\nthe chosen sentiment or style of the caption. To address this, we design an\nattention-based model to better add sentiment to image captions. The model\nembeds and learns sentiment with respect to image-caption data, and uses both\nhigh-level and word-level sentiment information during the learning process.\nThe model outperforms the state-of-the-art work in image captioning with\nsentiment using standard evaluation metrics. An analysis of generated captions\nalso shows that our model does this by a better selection of the\nsentiment-bearing adjectives and adjective-noun pairs.\n", "rewritten_text": "Recent research has focused on enhancing image captioning models to include not only factual details but also non-factual elements like sentiment and style. However, existing models struggle to effectively balance the semantic content of the image with these additional dimensions in the captions. Furthermore, human perception of an image can vary based on the sentiment or style conveyed in the caption. To tackle this challenge, we propose an attention-based model that integrates sentiment into image captions more effectively. This model incorporates sentiment into the image-caption data, utilizing both high-level and word-level sentiment information during training. Our model surpasses current state-of-the-art methods in image captioning with sentiment, as demonstrated by standard evaluation metrics. Analysis of the generated captions reveals that our model excels in selecting sentiment-bearing adjectives and adjective-noun pairs."}, "2310.15724": {"original_text": "  Pre-trained language models (PLMs) have achieved remarkable results on NLP\ntasks but at the expense of huge parameter sizes and the consequent\ncomputational costs. In this paper, we propose Variator, a parameter-efficient\nacceleration method that enhances computational efficiency through\nplug-and-play compression plugins. Compression plugins are designed to reduce\nthe sequence length via compressing multiple hidden vectors into one and\ntrained with original PLMs frozen. Different from traditional model\nacceleration methods, which compress PLMs to smaller sizes, Variator offers two\ndistinct advantages: (1) In real-world applications, the plug-and-play nature\nof our compression plugins enables dynamic selection of different compression\nplugins with varying acceleration ratios based on the current workload. (2) The\ncompression plugin comprises a few compact neural network layers with minimal\nparameters, significantly saving storage and memory overhead, particularly in\nscenarios with a growing number of tasks. We validate the effectiveness of\nVariator on seven datasets. Experimental results show that Variator can save\n53% computational costs using only 0.9% additional parameters with a\nperformance drop of less than 2%. Moreover, when the model scales to billions\nof parameters, Variator matches the strong performance of uncompressed PLMs.\n", "rewritten_text": "In this paper, we introduce Variator, a method for accelerating pre-trained language models (PLMs) that aims to improve computational efficiency by utilizing compression plugins. These plugins are designed to reduce sequence length by combining multiple hidden vectors into one, while the original PLMs remain frozen during training. Unlike traditional model acceleration techniques that simply shrink PLMs to smaller sizes, Variator offers two key advantages. Firstly, in practical applications, the plug-and-play nature of our compression plugins allows for dynamic selection of different compression options based on workload, offering varying acceleration ratios. Secondly, the compression plugin consists of a few compact neural network layers with minimal parameters, leading to significant savings in storage and memory overhead, especially in scenarios with multiple tasks. Our experiments on seven datasets demonstrate that Variator can reduce computational costs by 53% using only 0.9% additional parameters, with a performance drop of less than 2%. Furthermore, as the model scales to billions of parameters, Variator achieves performance comparable to uncompressed PLMs."}, "2110.02929": {"original_text": "  Event-based dynamic vision sensors provide very sparse output in the form of\nspikes, which makes them suitable for low-power applications. Convolutional\nspiking neural networks model such event-based data and develop their full\nenergy-saving potential when deployed on asynchronous neuromorphic hardware.\nEvent-based vision being a nascent field, the sensitivity of spiking neural\nnetworks to potentially malicious adversarial attacks has received little\nattention so far. We show how white-box adversarial attack algorithms can be\nadapted to the discrete and sparse nature of event-based visual data, and\ndemonstrate smaller perturbation magnitudes at higher success rates than the\ncurrent state-of-the-art algorithms. For the first time, we also verify the\neffectiveness of these perturbations directly on neuromorphic hardware.\nFinally, we discuss the properties of the resulting perturbations, the effect\nof adversarial training as a defense strategy, and future directions.\n", "rewritten_text": "\"Dynamic vision sensors that operate based on events produce sparse output in the form of spikes, making them suitable for low-power applications. Convolutional spiking neural networks are designed to model such event-based data and achieve maximum energy efficiency when implemented on asynchronous neuromorphic hardware. Despite the emerging nature of event-based vision technology, the vulnerability of spiking neural networks to potential adversarial attacks has not been extensively studied. In this work, we demonstrate how white-box adversarial attack algorithms can be tailored to the discrete and sparse characteristics of event-based visual data, resulting in smaller perturbations with higher success rates compared to current state-of-the-art methods. Additionally, we validate the impact of these perturbations directly on neuromorphic hardware for the first time. Furthermore, we explore the properties of the generated perturbations, the effectiveness of adversarial training as a defense mechanism, and potential future research directions.\""}, "2405.20259": {"original_text": "  The proliferation of deep learning solutions and the scarcity of large\nannotated datasets pose significant challenges in real-world applications.\nVarious strategies have been explored to overcome this challenge, with data\naugmentation (DA) approaches emerging as prominent solutions. DA approaches\ninvolve generating additional examples by transforming existing labeled data,\nthereby enriching the dataset and helping deep learning models achieve improved\ngeneralization without succumbing to overfitting. In real applications, where\nsolutions based on deep learning are widely used, there is facial expression\nrecognition (FER), which plays an essential role in human communication,\nimproving a range of knowledge areas (e.g., medicine, security, and marketing).\nIn this paper, we propose a simple and comprehensive face data augmentation\napproach based on mixed face component regularization that outperforms the\nclassical DA approaches from the literature, including the MixAugment which is\na specific approach for the target task in two well-known FER datasets existing\nin the literature.\n", "rewritten_text": "The widespread use of deep learning solutions and the limited availability of large annotated datasets present significant challenges in practical applications. To address this issue, various strategies have been explored, with data augmentation (DA) methods emerging as effective solutions. DA involves creating additional examples by modifying existing labeled data, thereby enhancing the dataset and helping deep learning models improve generalization without overfitting. One key application of deep learning is facial expression recognition (FER), which is crucial for human communication and has diverse implications in fields such as medicine, security, and marketing. This paper introduces a novel face data augmentation approach based on mixed face component regularization, which surpasses traditional DA methods in the literature, including the specific MixAugment approach designed for FER tasks in two well-known datasets."}, "2308.09983": {"original_text": "  Early detection of dysplasia of the cervix is critical for cervical cancer\ntreatment. However, automatic cervical dysplasia diagnosis via visual\ninspection, which is more appropriate in low-resource settings, remains a\nchallenging problem. Though promising results have been obtained by recent deep\nlearning models, their performance is significantly hindered by the limited\nscale of the available cervix datasets. Distinct from previous methods that\nlearn from a single dataset, we propose to leverage cross-domain cervical\nimages that were collected in different but related clinical studies to improve\nthe model's performance on the targeted cervix dataset. To robustly learn the\ntransferable information across datasets, we propose a novel prototype-based\nknowledge filtering method to estimate the transferability of cross-domain\nsamples. We further optimize the shared feature space by aligning the\ncross-domain image representations simultaneously on domain level with early\nalignment and class level with supervised contrastive learning, which endows\nmodel training and knowledge transfer with stronger robustness. The empirical\nresults on three real-world benchmark cervical image datasets show that our\nproposed method outperforms the state-of-the-art cervical dysplasia visual\ninspection by an absolute improvement of 4.7% in top-1 accuracy, 7.0% in\nprecision, 1.4% in recall, 4.6% in F1 score, and 0.05 in ROC-AUC.\n", "rewritten_text": "Detecting dysplasia of the cervix early is crucial for effective treatment of cervical cancer. However, diagnosing cervical dysplasia automatically through visual inspection, which is more suitable for low-resource settings, remains a challenging task. While recent deep learning models have shown promise, their performance is limited by the small size of available cervix datasets. In contrast to previous approaches that rely on a single dataset, we propose utilizing cross-domain cervical images from various related clinical studies to enhance the model's performance on the specific cervix dataset. To effectively transfer information across datasets, we introduce a novel prototype-based knowledge filtering method to assess the transferability of cross-domain samples. Additionally, we enhance the shared feature space by aligning cross-domain image representations at both the domain and class levels through early alignment and supervised contrastive learning, thereby improving the model's robustness during training and knowledge transfer. Our experimental results on three real-world benchmark cervical image datasets demonstrate that our proposed method surpasses the current state-of-the-art in cervical dysplasia visual inspection, achieving an absolute improvement of 4.7% in top-1 accuracy, 7.0% in precision, 1.4% in recall, 4.6% in F1 score, and 0.05 in ROC-AUC."}, "1704.02268": {"original_text": "  Unsupervised learning of visual similarities is of paramount importance to\ncomputer vision, particularly due to lacking training data for fine-grained\nsimilarities. Deep learning of similarities is often based on relationships\nbetween pairs or triplets of samples. Many of these relations are unreliable\nand mutually contradicting, implying inconsistencies when trained without\nsupervision information that relates different tuples or triplets to each\nother. To overcome this problem, we use local estimates of reliable\n(dis-)similarities to initially group samples into compact surrogate classes\nand use local partial orders of samples to classes to link classes to each\nother. Similarity learning is then formulated as a partial ordering task with\nsoft correspondences of all samples to classes. Adopting a strategy of\nself-supervision, a CNN is trained to optimally represent samples in a mutually\nconsistent manner while updating the classes. The similarity learning and\ngrouping procedure are integrated in a single model and optimized jointly. The\nproposed unsupervised approach shows competitive performance on detailed pose\nestimation and object classification.\n", "rewritten_text": "Learning visual similarities without supervision is crucial in computer vision, especially when fine-grained training data is limited. Deep learning typically relies on relationships between pairs or triplets of samples, but these relationships can be unreliable and conflicting. This inconsistency can be addressed by using local estimates of reliable similarities to group samples into compact classes and establish partial orders between classes. By formulating similarity learning as a partial ordering task with soft correspondences, a CNN can be trained to represent samples consistently while updating classes. This unsupervised approach integrates similarity learning and grouping in a single model, resulting in competitive performance in tasks such as detailed pose estimation and object classification."}, "1909.07598": {"original_text": "  Multi-hop question answering (QA) requires an information retrieval (IR)\nsystem that can find \\emph{multiple} supporting evidence needed to answer the\nquestion, making the retrieval process very challenging. This paper introduces\nan IR technique that uses information of entities present in the initially\nretrieved evidence to learn to `\\emph{hop}' to other relevant evidence. In a\nsetting, with more than \\textbf{5 million} Wikipedia paragraphs, our approach\nleads to significant boost in retrieval performance. The retrieved evidence\nalso increased the performance of an existing QA model (without any training)\non the \\hotpot benchmark by \\textbf{10.59} F1.\n", "rewritten_text": "The concept of multi-hop question answering (QA) involves finding multiple pieces of supporting evidence to answer a question, which poses a significant challenge for information retrieval (IR) systems. This study presents an IR technique that leverages entity information from initially retrieved evidence to navigate to other relevant evidence, improving retrieval performance. With a vast dataset of over 5 million Wikipedia paragraphs, our approach demonstrated a substantial enhancement in retrieval performance. Additionally, the retrieved evidence boosted the performance of an existing QA model by 10.59 F1 on the Hotpot benchmark, even without any additional training."}, "2211.06726": {"original_text": "  Vision Transformer (ViT) is a pioneering deep learning framework that can\naddress real-world computer vision issues, such as image classification and\nobject recognition. Importantly, ViTs are proven to outperform traditional deep\nlearning models, such as convolutional neural networks (CNNs). Relatively\nrecently, a number of ViT mutations have been transplanted into the field of\nmedical imaging, thereby resolving a variety of critical classification and\nsegmentation challenges, especially in terms of brain imaging data. In this\nwork, we provide a novel multimodal deep learning pipeline, MultiCrossViT,\nwhich is capable of analyzing both structural MRI (sMRI) and static functional\nnetwork connectivity (sFNC) data for the prediction of schizophrenia disease.\nOn a dataset with minimal training subjects, our novel model can achieve an AUC\nof 0.832. Finally, we visualize multiple brain regions and covariance patterns\nmost relevant to schizophrenia based on the resulting ViT attention maps by\nextracting features from transformer encoders.\n", "rewritten_text": "The Vision Transformer (ViT) is an innovative deep learning framework that tackles practical computer vision challenges like image classification and object recognition. Notably, ViTs have been shown to surpass traditional deep learning models such as convolutional neural networks (CNNs). Recently, various ViT variations have been applied to medical imaging, effectively addressing critical classification and segmentation issues, particularly in brain imaging data. This study introduces a new multimodal deep learning approach called MultiCrossViT, which can analyze both structural MRI (sMRI) and static functional network connectivity (sFNC) data to predict schizophrenia. With limited training data, our model achieves an AUC of 0.832. Additionally, we use ViT attention maps to visualize key brain regions and covariance patterns related to schizophrenia by extracting features from transformer encoders."}, "1607.02504": {"original_text": "  We present a method to predict image deformations based on patch-wise image\nappearance. Specifically, we design a patch-based deep encoder-decoder network\nwhich learns the pixel/voxel-wise mapping between image appearance and\nregistration parameters. Our approach can predict general deformation\nparameterizations, however, we focus on the large deformation diffeomorphic\nmetric mapping (LDDMM) registration model. By predicting the LDDMM\nmomentum-parameterization we retain the desirable theoretical properties of\nLDDMM, while reducing computation time by orders of magnitude: combined with\npatch pruning, we achieve a 1500x/66x speed up compared to GPU-based\noptimization for 2D/3D image registration. Our approach has better prediction\naccuracy than predicting deformation or velocity fields and results in\ndiffeomorphic transformations. Additionally, we create a Bayesian probabilistic\nversion of our network, which allows evaluation of deformation field\nuncertainty through Monte Carlo sampling using dropout at test time. We show\nthat deformation uncertainty highlights areas of ambiguous deformations. We\ntest our method on the OASIS brain image dataset in 2D and 3D.\n", "rewritten_text": "We introduce a technique for anticipating image distortions by analyzing image appearance in patches. Specifically, we develop a deep encoder-decoder network based on patches that learns the mapping between image appearance and registration parameters at the pixel/voxel level. While our method can predict various deformation parameterizations, our primary focus is on the large deformation diffeomorphic metric mapping (LDDMM) registration model. By predicting the LDDMM momentum parameterization, we maintain the advantageous theoretical characteristics of LDDMM while significantly reducing computation time. Through the combination of patch pruning, we achieve a substantial speed increase of 1500x/66x compared to GPU-based optimization for 2D/3D image registration. Our approach outperforms predicting deformation or velocity fields, resulting in diffeomorphic transformations. Furthermore, we develop a Bayesian probabilistic version of our network, enabling the assessment of deformation field uncertainty through Monte Carlo sampling using dropout during testing. We demonstrate that uncertainty in deformation highlights regions with ambiguous distortions. The effectiveness of our method is validated through testing on the OASIS brain image dataset in both 2D and 3D."}, "1911.08007": {"original_text": "  The classification of streets on road networks has been focused on the\nvehicular transportational features of streets such as arterials, major roads,\nminor roads and so forth based on their transportational use. City authorities\non the other hand have been shifting to more urban inclusive planning of\nstreets, encompassing the side use of a street combined with the\ntransportational features of a street. In such classification schemes, streets\nare labeled for example as commercial throughway, residential neighborhood,\npark etc. This modern approach to urban planning has been adopted by major\ncities such as the city of San Francisco, the states of Florida and\nPennsylvania among many others. Currently, the process of labeling streets\naccording to their contexts is manual and hence is tedious and time consuming.\nIn this paper, we propose an approach to collect and label imagery data then\ndeploy advancements in computer vision towards modern urban planning. We\ncollect and label street imagery then train deep convolutional neural networks\n(CNN) to perform the classification of street context. We show that CNN models\ncan perform well achieving accuracies in the 81% to 87%, we then visualize\nsamples from the embedding space of streets using the t-SNE method and apply\nclass activation mapping methods to interpret the features in street imagery\ncontributing to output classification from a model.\n", "rewritten_text": "The focus of street classification on road networks has traditionally been on the transportation-related aspects of streets, such as arterials, major roads, and minor roads, based on their transportation usage. However, city authorities are now moving towards a more comprehensive urban planning approach for streets, considering both their transportation function and their surrounding uses. This new classification system categorizes streets as commercial throughways, residential neighborhoods, parks, and so on. Major cities like San Francisco, as well as states like Florida and Pennsylvania, have embraced this modern urban planning strategy. Currently, the manual process of labeling streets based on their contexts is laborious and time-consuming. In this paper, we propose a method to collect and label street imagery and leverage advancements in computer vision for contemporary urban planning. By collecting and labeling street images and training deep convolutional neural networks (CNN), we demonstrate that CNN models can achieve high accuracies ranging from 81% to 87%. We also visualize street samples in the embedding space using the t-SNE method and utilize class activation mapping techniques to interpret the features in street images that influence the model's classification output."}, "1811.11387": {"original_text": "  The success of deep neural networks generally requires a vast amount of\ntraining data to be labeled, which is expensive and unfeasible in scale,\nespecially for video collections. To alleviate this problem, in this paper, we\npropose 3DRotNet: a fully self-supervised approach to learn spatiotemporal\nfeatures from unlabeled videos. A set of rotations are applied to all videos,\nand a pretext task is defined as prediction of these rotations. When\naccomplishing this task, 3DRotNet is actually trained to understand the\nsemantic concepts and motions in videos. In other words, it learns a\nspatiotemporal video representation, which can be transferred to improve video\nunderstanding tasks in small datasets. Our extensive experiments successfully\ndemonstrate the effectiveness of the proposed framework on action recognition,\nleading to significant improvements over the state-of-the-art self-supervised\nmethods. With the self-supervised pre-trained 3DRotNet from large datasets, the\nrecognition accuracy is boosted up by 20.4% on UCF101 and 16.7% on HMDB51\nrespectively, compared to the models trained from scratch.\n", "rewritten_text": "The achievement of deep neural networks typically relies on having a large amount of labeled training data, which can be costly and impractical, especially for video collections. To address this issue, this paper introduces 3DRotNet: a fully self-supervised method for extracting spatiotemporal features from unlabeled videos. By applying a series of rotations to all videos and defining a pretext task of predicting these rotations, 3DRotNet is trained to comprehend the semantic concepts and movements within videos. Essentially, it learns a spatiotemporal representation of videos that can be transferred to enhance video understanding tasks with limited datasets. Extensive experiments confirm the effectiveness of this approach in action recognition, showing significant advancements over existing self-supervised techniques. By utilizing the self-supervised pre-trained 3DRotNet on large datasets, recognition accuracy is boosted by 20.4% on UCF101 and 16.7% on HMDB51 compared to models trained from scratch."}, "2402.01619": {"original_text": "  Program induction (PI) has become a promising paradigm for using knowledge\nbases (KBs) to help large language models (LLMs) answer complex\nknowledge-intensive questions. Nonetheless, PI typically relies on a large\nnumber of parallel question-program pairs to make the LLM aware of the schema\nof the given KB, and is thus challenging for many low-resourced KBs that lack\nannotated data. To this end, we propose KB-Plugin, a plug-and-play framework\nthat enables LLMs to induce programs over any low-resourced KB. Firstly,\nKB-Plugin adopts self-supervised learning to encode the detailed schema\ninformation of a given KB into a pluggable module, namely schema plugin.\nSecondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB\nto train another pluggable module, namely PI plugin, which can help the LLM\nextract question-relevant schema information from the schema plugin of any KB\nand utilize this information to induce programs over this KB. Experiments on\nfive heterogeneous KBQA datasets show that KB-Plugin achieves better or\ncomparable performance with 25$\\times$ smaller backbone LLM compared to SoTA PI\nmethods for low-resourced KBs, and even approaches the performance of\nsupervised methods. Our code and data are available at\nhttps://github.com/THU-KEG/KB-Plugin.\n", "rewritten_text": "Program induction (PI) has emerged as a promising approach to leverage knowledge bases (KBs) in assisting large language models (LLMs) in answering complex questions that require knowledge. However, PI typically necessitates a substantial number of parallel question-program pairs to familiarize the LLM with the structure of the KB, posing challenges for low-resourced KBs lacking annotated data. In response, we introduce KB-Plugin, a flexible framework that allows LLMs to generate programs for any low-resourced KB. Firstly, KB-Plugin employs self-supervised learning to encode the detailed schema information of a given KB into a modular component called the schema plugin. Secondly, KB-Plugin leverages annotated data from a well-resourced KB to train another modular component, the PI plugin, which assists the LLM in extracting question-relevant schema information from the schema plugin of any KB and using it to generate programs. Experiments conducted on five diverse KBQA datasets demonstrate that KB-Plugin achieves comparable or superior performance with a backbone LLM that is 25 times smaller than state-of-the-art PI methods for low-resourced KBs, and even approaches the performance of supervised methods. Our code and data can be accessed at https://github.com/THU-KEG/KB-Plugin."}, "1812.01458": {"original_text": "  Recent advances in deep learning have shown exciting promise in filling large\nholes and lead to another orientation for image inpainting. However, existing\nlearning-based methods often create artifacts and fallacious textures because\nof insufficient cognition understanding. Previous generative networks are\nlimited with single receptive type and give up pooling in consideration of\ndetail sharpness. Human cognition is constant regardless of the target\nattribute. As multiple receptive fields improve the ability of abstract image\ncharacterization and pooling can keep feature invariant, specifically, deep\ninception learning is adopted to promote high-level feature representation and\nenhance model learning capacity for local patches. Moreover, approaches for\ngenerating diverse mask images are introduced and a random mask dataset is\ncreated. We benchmark our methods on ImageNet, Places2 dataset, and CelebA-HQ.\nExperiments for regular, irregular, and custom regions completion are all\nperformed and free-style image inpainting is also presented. Quantitative\ncomparisons with previous state-of-the-art methods show that ours obtain much\nmore natural image completions.\n", "rewritten_text": "Recent advancements in deep learning have demonstrated promising potential in addressing large gaps in images, leading to a new approach for image inpainting. However, current learning-based techniques often result in artifacts and incorrect textures due to a lack of comprehensive understanding. Previous generative networks were limited by a single receptive type and sacrificed pooling for sharp detail consideration. Human cognition remains consistent regardless of the target attribute. By utilizing multiple receptive fields to enhance abstract image characterization and incorporating pooling to maintain feature consistency, deep inception learning is employed to improve high-level feature representation and boost model learning capacity for local patches. Additionally, methods for generating diverse mask images are introduced, along with the creation of a random mask dataset. Our methods are evaluated on ImageNet, Places2 dataset, and CelebA-HQ, with experiments conducted on regular, irregular, and custom regions completion, as well as free-style image inpainting. Quantitative comparisons with previous cutting-edge techniques demonstrate that our approach achieves more natural image completions."}, "cs/0207005": {"original_text": "  We present a broad coverage Japanese grammar written in the HPSG formalism\nwith MRS semantics. The grammar is created for use in real world applications,\nsuch that robustness and performance issues play an important role. It is\nconnected to a POS tagging and word segmentation tool. This grammar is being\ndeveloped in a multilingual context, requiring MRS structures that are easily\ncomparable across languages.\n", "rewritten_text": "Here is a rephrased version of the text: \"We introduce a comprehensive Japanese grammar designed using the HPSG formalism and incorporating MRS semantics. This grammar is specifically developed for practical applications, emphasizing robustness and performance. It is integrated with a POS tagging and word segmentation tool. The development of this grammar takes place within a multilingual framework, necessitating MRS structures that can be easily compared across different languages.\""}, "2307.10696": {"original_text": "  Improving the feature representation ability is the foundation of many whole\nslide pathological image (WSIs) tasks. Recent works have achieved great success\nin pathological-specific self-supervised learning (SSL). However, most of them\nonly focus on learning patch-level representations, thus there is still a gap\nbetween pretext and slide-level downstream tasks, e.g., subtyping, grading and\nstaging. Aiming towards slide-level representations, we propose Slide-Level\nPrototypical Distillation (SLPD) to explore intra- and inter-slide semantic\nstructures for context modeling on WSIs. Specifically, we iteratively perform\nintra-slide clustering for the regions (4096x4096 patches) within each WSI to\nyield the prototypes and encourage the region representations to be closer to\nthe assigned prototypes. By representing each slide with its prototypes, we\nfurther select similar slides by the set distance of prototypes and assign the\nregions by cross-slide prototypes for distillation. SLPD achieves\nstate-of-the-art results on multiple slide-level benchmarks and demonstrates\nthat representation learning of semantic structures of slides can make a\nsuitable proxy task for WSI analysis. Code will be available at\nhttps://github.com/Carboxy/SLPD.\n", "rewritten_text": "Enhancing the ability to represent features is crucial for various tasks involving whole slide pathological images (WSIs). While recent studies have made significant progress in self-supervised learning specific to pathology, most have focused solely on learning representations at the patch level. This has created a gap between preliminary and slide-level tasks such as subtyping, grading, and staging. To address this gap and aim for slide-level representations, we introduce Slide-Level Prototypical Distillation (SLPD). SLPD explores both intra- and inter-slide semantic structures to model context in WSIs. The approach involves iteratively clustering regions (4096x4096 patches) within each WSI to generate prototypes and guide region representations to align with these prototypes. By representing each slide using its prototypes, similar slides are selected based on prototype distances and regions are assigned cross-slide prototypes for distillation. SLPD has achieved state-of-the-art performance on multiple slide-level benchmarks, showcasing that learning semantic structures of slides can serve as a suitable proxy task for WSI analysis. The code will be accessible at https://github.com/Carboxy/SLPD."}, "2205.05869": {"original_text": "  We address the task of view synthesis, generating novel views of a scene\ngiven a set of images as input. In many recent works such as NeRF (Mildenhall\net al., 2020), the scene geometry is parameterized using neural implicit\nrepresentations (i.e., MLPs). Implicit neural representations have achieved\nimpressive visual quality but have drawbacks in computational efficiency. In\nthis work, we propose a new approach that performs view synthesis using point\nclouds. It is the first point-based method that achieves better visual quality\nthan NeRF while being 100x faster in rendering speed. Our approach builds on\nexisting works on differentiable point-based rendering but introduces a novel\ntechnique we call \"Sculpted Neural Points (SNP)\", which significantly improves\nthe robustness to errors and holes in the reconstructed point cloud. We further\npropose to use view-dependent point features based on spherical harmonics to\ncapture non-Lambertian surfaces, and new designs in the point-based rendering\npipeline that further boost the performance. Finally, we show that our system\nsupports fine-grained scene editing. Code is available at\nhttps://github.com/princeton-vl/SNP.\n", "rewritten_text": "We focus on the task of generating new views of a scene by synthesizing views based on a set of input images. Recent works, such as NeRF (Mildenhall et al., 2020), have utilized neural implicit representations, specifically Multi-Layer Perceptrons (MLPs), to parameterize scene geometry. While implicit neural representations have shown impressive visual quality, they are not computationally efficient. In this study, we introduce a novel approach that utilizes point clouds for view synthesis. Our method, the first of its kind, surpasses NeRF in visual quality while rendering 100 times faster. Building upon previous research on differentiable point-based rendering, we introduce a new technique called \"Sculpted Neural Points (SNP)\" to enhance robustness in handling errors and gaps in the reconstructed point cloud. Additionally, we propose the use of view-dependent point features based on spherical harmonics to capture non-Lambertian surfaces, along with improvements in the point-based rendering process to enhance performance. Our system also supports detailed scene editing capabilities. The code for our approach is available at https://github.com/princeton-vl/SNP."}, "1910.03484": {"original_text": "  In Natural Language Generation (NLG), End-to-End (E2E) systems trained\nthrough deep learning have recently gained a strong interest. Such deep models\nneed a large amount of carefully annotated data to reach satisfactory\nperformance. However, acquiring such datasets for every new NLG application is\na tedious and time-consuming task. In this paper, we propose a semi-supervised\ndeep learning scheme that can learn from non-annotated data and annotated data\nwhen available. It uses an NLG and a Natural Language Understanding (NLU)\nsequence-to-sequence models which are learned jointly to compensate for the\nlack of annotation. Experiments on two benchmark datasets show that, with\nlimited amount of annotated data, the method can achieve very competitive\nresults while not using any pre-processing or re-scoring tricks. These findings\nopen the way to the exploitation of non-annotated datasets which is the current\nbottleneck for the E2E NLG system development to new applications.\n", "rewritten_text": "\"In the field of Natural Language Generation (NLG), there has been a growing interest in End-to-End (E2E) systems trained using deep learning. These deep models require a significant amount of meticulously annotated data to perform well. However, creating such datasets for each new NLG application is a laborious and time-consuming process. This paper introduces a semi-supervised deep learning approach that can leverage both annotated and non-annotated data. The method involves training NLG and Natural Language Understanding (NLU) sequence-to-sequence models together to address the lack of annotations. Experimental results on two standard datasets demonstrate that, even with limited annotated data, the approach can achieve competitive performance without relying on preprocessing or re-scoring techniques. These findings suggest a promising avenue for utilizing non-annotated datasets, which is currently a bottleneck in the development of E2E NLG systems for new applications.\""}, "2109.09883": {"original_text": "  Few-shot classification aims at classifying categories of a novel task by\nlearning from just a few (typically, 1 to 5) labelled examples. An effective\napproach to few-shot classification involves a prior model trained on a\nlarge-sample base domain, which is then finetuned over the novel few-shot task\nto yield generalizable representations. However, task-specific finetuning is\nprone to overfitting due to the lack of enough training examples. To alleviate\nthis issue, we propose a new finetuning approach based on contrastive learning\nthat reuses unlabelled examples from the base domain in the form of\ndistractors. Unlike the nature of unlabelled data used in prior works,\ndistractors belong to classes that do not overlap with the novel categories. We\ndemonstrate for the first time that inclusion of such distractors can\nsignificantly boost few-shot generalization. Our technical novelty includes a\nstochastic pairing of examples sharing the same category in the few-shot task\nand a weighting term that controls the relative influence of task-specific\nnegatives and distractors. An important aspect of our finetuning objective is\nthat it is agnostic to distractor labels and hence applicable to various base\ndomain settings. Compared to state-of-the-art approaches, our method shows\naccuracy gains of up to $12\\%$ in cross-domain and up to $5\\%$ in unsupervised\nprior-learning settings.\n", "rewritten_text": "\"Few-shot classification involves categorizing new tasks with limited labeled examples, typically ranging from 1 to 5. A successful approach to this task is to use a pre-trained model on a large dataset and fine-tune it on the new few-shot task to create adaptable representations. However, fine-tuning specifically for the new task can lead to overfitting due to the scarcity of training examples. To address this issue, we propose a novel fine-tuning method based on contrastive learning that utilizes unlabeled examples from the base dataset as distractors. These distractors come from classes that are different from the new categories, and incorporating them has been shown to significantly improve few-shot generalization. Our approach includes stochastic pairing of examples from the same category in the new task and a weighting term to balance the influence of task-specific negatives and distractors. Importantly, our fine-tuning objective is independent of distractor labels, making it suitable for various base dataset scenarios. Compared to existing methods, our technique demonstrates accuracy improvements of up to 12% in cross-domain and up to 5% in unsupervised prior-learning setups.\""}, "2003.12137": {"original_text": "  We explore novel approaches to the task of image generation from their\nrespective captions, building on state-of-the-art GAN architectures.\nParticularly, we baseline our models with the Attention-based GANs that learn\nattention mappings from words to image features. To better capture the features\nof the descriptions, we then built a novel cyclic design that learns an inverse\nfunction to maps the image back to original caption. Additionally, we\nincorporated recently developed BERT pretrained word embeddings as our initial\ntext featurizer and observe a noticeable improvement in qualitative and\nquantitative performance compared to the Attention GAN baseline.\n", "rewritten_text": "We are exploring new methods for generating images based on their captions, using advanced GAN architectures. Our models are initially based on Attention-based GANs, which learn how to focus on specific parts of the image based on the words in the caption. To enhance the accuracy of the descriptions, we have developed a unique cyclic design that can reverse the process and generate captions from images. We have also integrated BERT pretrained word embeddings as our starting point for text analysis, resulting in significant improvements in both the quality and quantity of our results compared to the Attention GAN approach."}, "2003.02683": {"original_text": "  We introduce the first method for automatic image generation from scene-level\nfreehand sketches. Our model allows for controllable image generation by\nspecifying the synthesis goal via freehand sketches. The key contribution is an\nattribute vector bridged Generative Adversarial Network called EdgeGAN, which\nsupports high visual-quality object-level image content generation without\nusing freehand sketches as training data. We have built a large-scale composite\ndataset called SketchyCOCO to support and evaluate the solution. We validate\nour approach on the tasks of both object-level and scene-level image generation\non SketchyCOCO. Through quantitative, qualitative results, human evaluation and\nablation studies, we demonstrate the method's capacity to generate realistic\ncomplex scene-level images from various freehand sketches.\n", "rewritten_text": "We present a novel method for automatically generating images from freehand sketches at the scene level. Our model enables controlled image creation by defining the desired outcome through freehand sketches. The main innovation is a Generative Adversarial Network known as EdgeGAN, which utilizes an attribute vector to produce high-quality object-level images without the need for sketch training data. To support our solution, we have curated a comprehensive dataset named SketchyCOCO. We assess the effectiveness of our approach in generating both object-level and scene-level images on SketchyCOCO. Through a combination of quantitative analysis, qualitative assessment, human feedback, and detailed studies, we showcase the method's ability to generate realistic and intricate scene-level images from diverse freehand sketches."}, "2409.00942": {"original_text": "  Normalizing flows, a category of probabilistic models famed for their\ncapabilities in modeling complex data distributions, have exhibited remarkable\nefficacy in unsupervised anomaly detection. This paper explores the potential\nof normalizing flows in multi-class anomaly detection, wherein the normal data\nis compounded with multiple classes without providing class labels. Through the\nintegration of vector quantization (VQ), we empower the flow models to\ndistinguish different concepts of multi-class normal data in an unsupervised\nmanner, resulting in a novel flow-based unified method, named VQ-Flow.\nSpecifically, our VQ-Flow leverages hierarchical vector quantization to\nestimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for\nconcept distinction and its concomitant Concept-Specific Pattern Codebook\n(CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow\nare conditioned on the concept-specific patterns captured in CSPC, capable of\nmodeling specific normal patterns associated with different concepts. Moreover,\nCPC further enables our VQ-Flow for concept-aware distribution modeling,\nfaithfully mimicking the intricate multi-class normal distribution through a\nmixed Gaussian distribution reparametrized on the conceptual prototypes.\nThrough the introduction of vector quantization, the proposed VQ-Flow advances\nthe state-of-the-art in multi-class anomaly detection within a unified training\nscheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase\nis publicly available at https://github.com/cool-xuan/vqflow.\n", "rewritten_text": "\"Normalizing flows, a type of probabilistic models known for their ability to model complex data distributions, have shown impressive effectiveness in detecting anomalies without supervision. This study investigates the potential of normalizing flows in detecting anomalies across multiple classes, where normal data is mixed with various classes without class labels. By incorporating vector quantization (VQ), we enhance the flow models to differentiate between different concepts of multi-class normal data in an unsupervised manner, resulting in a new unified method called VQ-Flow. Our VQ-Flow utilizes hierarchical vector quantization to estimate two codebooks: a Conceptual Prototype Codebook (CPC) for concept differentiation and a Concept-Specific Pattern Codebook (CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow are conditioned on the concept-specific patterns in CSPC, enabling them to model specific normal patterns associated with different concepts. Additionally, CPC allows VQ-Flow to model concept-aware distributions, accurately representing the complex multi-class normal distribution through a mixed Gaussian distribution reparametrized based on conceptual prototypes. By introducing vector quantization, VQ-Flow enhances the current state-of-the-art in multi-class anomaly detection with a unified training approach, achieving a Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase is publicly accessible at https://github.com/cool-xuan/vqflow.\""}, "1708.00801": {"original_text": "  We study the impact of big models (in terms of the degree of lexicalization)\nand big data (in terms of the training corpus size) on dependency grammar\ninduction. We experimented with L-DMV, a lexicalized version of Dependency\nModel with Valence and L-NDMV, our lexicalized extension of the Neural\nDependency Model with Valence. We find that L-DMV only benefits from very small\ndegrees of lexicalization and moderate sizes of training corpora. L-NDMV can\nbenefit from big training data and lexicalization of greater degrees,\nespecially when enhanced with good model initialization, and it achieves a\nresult that is competitive with the current state-of-the-art.\n", "rewritten_text": "We investigate how the size of models and data impact the induction of dependency grammar. Our study involves testing L-DMV, a lexicalized version of Dependency Model with Valence, and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence. Our findings show that L-DMV performs best with minimal lexicalization and moderate training corpus sizes. On the other hand, L-NDMV benefits from larger training data and higher degrees of lexicalization, particularly when combined with effective model initialization. This approach yields competitive results compared to the current leading methods."}, "2303.1309": {"original_text": "  Recent trends in semi-supervised learning have significantly boosted the\nperformance of 3D semi-supervised medical image segmentation. Compared with 2D\nimages, 3D medical volumes involve information from different directions, e.g.,\ntransverse, sagittal, and coronal planes, so as to naturally provide\ncomplementary views. These complementary views and the intrinsic similarity\namong adjacent 3D slices inspire us to develop a novel annotation way and its\ncorresponding semi-supervised model for effective segmentation. Specifically,\nwe firstly propose the orthogonal annotation by only labeling two orthogonal\nslices in a labeled volume, which significantly relieves the burden of\nannotation. Then, we perform registration to obtain the initial pseudo labels\nfor sparsely labeled volumes. Subsequently, by introducing unlabeled volumes,\nwe propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that\nexploits dense pseudo labels in early stage and sparse labels in later stage\nand meanwhile forces consistent output of two networks. Experimental results on\nthree benchmark datasets validated our effectiveness in performance and\nefficiency in annotation. For example, with only 10 annotated slices, our\nmethod reaches a Dice up to 86.93% on KiTS19 dataset.\n", "rewritten_text": "Recent advancements in semi-supervised learning have greatly enhanced the efficacy of 3D semi-supervised medical image segmentation. Unlike 2D images, 3D medical volumes incorporate information from various perspectives, such as transverse, sagittal, and coronal planes, offering complementary viewpoints. Leveraging these complementary perspectives and the inherent similarities among adjacent 3D slices, we have devised a new annotation approach and a corresponding semi-supervised model for improved segmentation. Initially, we introduce orthogonal annotation, where only two orthogonal slices in a labeled volume are annotated, reducing the annotation workload significantly. Subsequently, registration is performed to generate initial pseudo labels for sparsely labeled volumes. By incorporating unlabeled volumes, we introduce a dual-network framework called Dense-Sparse Co-training (DeSCO), which leverages dense pseudo labels in the early stages and sparse labels in the later stages while ensuring consistent outputs from both networks. Our experimental results on three benchmark datasets confirm the effectiveness and efficiency of our approach in terms of performance and annotation. For instance, with just 10 annotated slices, our method achieves a Dice score of up to 86.93% on the KiTS19 dataset."}, "2401.08123": {"original_text": "  Guided depth super-resolution (GDSR) involves restoring missing depth details\nusing the high-resolution RGB image of the same scene. Previous approaches have\nstruggled with the heterogeneity and complementarity of the multi-modal inputs,\nand neglected the issues of modal misalignment, geometrical misalignment, and\nfeature selection. In this study, we rethink some essential components in GDSR\nnetworks and propose a simple yet effective Dynamic Dual Alignment and\nAggregation network (D2A2). D2A2 mainly consists of 1) a dynamic dual alignment\nmodule that adapts to alleviate the modal misalignment via a learnable domain\nalignment block and geometrically align cross-modal features by learning the\noffset; and 2) a mask-to-pixel feature aggregate module that uses the gated\nmechanism and pixel attention to filter out irrelevant texture noise from RGB\nfeatures and combine the useful features with depth features. By combining the\nstrengths of RGB and depth features while minimizing disturbance introduced by\nthe RGB image, our method with simple reuse and redesign of basic components\nachieves state-of-the-art performance on multiple benchmark datasets. The code\nis available at https://github.com/JiangXinni/D2A2.\n", "rewritten_text": "The process of guided depth super-resolution (GDSR) aims to enhance missing depth details by utilizing a high-resolution RGB image of the same scene. Previous methods have encountered challenges due to the diverse nature of the multi-modal inputs and have overlooked issues such as modal misalignment, geometric misalignment, and feature selection. In this research, we reevaluate key aspects of GDSR networks and introduce a straightforward yet effective Dynamic Dual Alignment and Aggregation network (D2A2). D2A2 comprises a dynamic dual alignment module that mitigates modal misalignment through a trainable domain alignment block and aligns cross-modal features geometrically by learning the offset. Additionally, it includes a mask-to-pixel feature aggregate module that employs a gated mechanism and pixel attention to eliminate irrelevant texture noise from RGB features and merge valuable features with depth features. By leveraging the strengths of both RGB and depth features while minimizing disturbances from the RGB image, our approach, which involves the simple reuse and redesign of fundamental components, achieves top-tier performance across various benchmark datasets. The code can be accessed at https://github.com/JiangXinni/D2A2."}, "2112.09414": {"original_text": "  By highlighting the regions of the input image that contribute the most to\nthe decision, saliency maps have become a popular method to make neural\nnetworks interpretable. In medical imaging, they are particularly well-suited\nto explain neural networks in the context of abnormality localization. However,\nfrom our experiments, they are less suited to classification problems where the\nfeatures that allow to distinguish between the different classes are spatially\ncorrelated, scattered and definitely non-trivial. In this paper we thus propose\na new paradigm for better interpretability. To this end we provide the user\nwith relevant and easily interpretable information so that he can form his own\nopinion. We use Disentangled Variational Auto-Encoders which latent\nrepresentation is divided into two components: the non-interpretable part and\nthe disentangled part. The latter accounts for the categorical variables\nexplicitly representing the different classes of interest. In addition to\nproviding the class of a given input sample, such a model offers the\npossibility to transform the sample from a given class to a sample of another\nclass, by modifying the value of the categorical variables in the latent\nrepresentation. This paves the way to easier interpretation of class\ndifferences. We illustrate the relevance of this approach in the context of\nautomatic sex determination from hip bones in forensic medicine. The features\nencoded by the model, that distinguish the different classes were found to be\nconsistent with expert knowledge.\n", "rewritten_text": "Saliency maps have gained popularity for making neural networks more interpretable by highlighting the key regions in input images that influence decisions. In medical imaging, they are useful for explaining neural networks in abnormality localization. However, our experiments show that they are less effective for classification tasks with spatially correlated and scattered features. In this study, we propose a new approach for better interpretability. We introduce a model using Disentangled Variational Auto-Encoders, which separate the latent representation into an interpretable part and a disentangled part representing different classes. This model not only provides the class of an input sample but also allows for transforming it into another class by adjusting categorical variables in the latent representation. This facilitates easier interpretation of class differences. We demonstrate the effectiveness of this method in automatic sex determination from hip bones in forensic medicine, showing that the model's encoded features align with expert knowledge."}, "1910.06431": {"original_text": "  There has been great success recently in tackling challenging NLP tasks by\nneural networks which have been pre-trained and fine-tuned on large amounts of\ntask data. In this paper, we investigate one such model, BERT for\nquestion-answering, with the aim to analyze why it is able to achieve\nsignificantly better results than other models. We run DeepLIFT on the model\npredictions and test the outcomes to monitor shift in the attention values for\ninput. We also cluster the results to analyze any possible patterns similar to\nhuman reasoning depending on the kind of input paragraph and question the model\nis trying to answer.\n", "rewritten_text": "Recently, there has been significant success in addressing complex NLP tasks using neural networks that are pre-trained and fine-tuned on extensive task data. This study explores the effectiveness of one such model, BERT, for question-answering, aiming to understand the reasons behind its superior performance compared to other models. By applying DeepLIFT to the model's predictions, we examine changes in attention values for input and analyze clustered results to identify patterns resembling human reasoning based on the input paragraph and question the model is tasked with answering."}, "2410.15277": {"original_text": "  Retrieval-augmented generation (RAG) can supplement large language models\n(LLMs) by integrating external knowledge. However, as the number of retrieved\ndocuments increases, the input length to LLMs grows linearly, causing a\ndramatic increase in latency and a degradation in long-context understanding.\nThis is particularly serious for multi-hop questions that require a chain of\nreasoning across documents. To accelerate inference, reduce costs, and minimize\ndistractions, this paper presents BRIEF (Bridging Retrieval and Inference\nthrough Evidence Fusion), a lightweight approach that performs query-aware\nmulti-hop reasoning by compressing retrieved documents into highly dense\ntextual summaries to integrate into in-context learning. To enable learning\ncompression for multi-hop reasoning, we curate synthetic data by extracting\natomic proposition expressions that encapsulate distinct factoids from the\nsource documents to compose synthetic summaries. Based on our synthetic data\nbuilt entirely by open-source models, BRIEF generates more concise summaries\nand enables a range of LLMs to achieve exceptional open-domain question\nanswering (QA) performance. For example, on HotpotQA, BRIEF improves the\ncompression rate by 2 times compared to the state-of-the-art baseline, while\noutperforming it by 3.00% EM and 4.16% F1 with Flan-UL2 as the reader LM. It\nalso generates more concise summaries than proprietary GPT-3.5, while\ndemonstrating nearly identical QA performance.\n", "rewritten_text": "The retrieval-augmented generation (RAG) technique enhances large language models (LLMs) by incorporating external knowledge. However, increasing the number of retrieved documents leads to longer input for LLMs, resulting in higher latency and reduced understanding of long contexts. This is especially problematic for multi-hop questions that require reasoning across multiple documents. To address these challenges and improve efficiency, this paper introduces BRIEF (Bridging Retrieval and Inference through Evidence Fusion), a lightweight method that conducts query-aware multi-hop reasoning by condensing retrieved documents into concise textual summaries for seamless integration into contextual learning. By creating synthetic data that captures key information from source documents to form compact summaries, BRIEF facilitates efficient compression for multi-hop reasoning. Leveraging entirely open-source models, BRIEF produces more succinct summaries and enables various LLMs to excel in open-domain question answering (QA). For instance, on HotpotQA, BRIEF achieves a compression rate twice as efficient as the leading baseline, while surpassing it by 3.00% in EM and 4.16% in F1 when using Flan-UL2 as the reader LM. Furthermore, BRIEF generates more concise summaries compared to proprietary GPT-3.5, while maintaining nearly identical QA performance."}, "2403.17512": {"original_text": "  Improving the efficiency of current neural networks and modeling them in\nbiological neural systems have become popular research directions in recent\nyears. Pulse-coupled neural network (PCNN) is a well applicated model for\nimitating the computation characteristics of the human brain in computer vision\nand neural network fields. However, differences between the PCNN and biological\nneural systems remain: limited neural connection, high computational cost, and\nlack of stochastic property. In this study, random-coupled neural network\n(RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic\ncomputing via a random inactivation process. This process randomly closes some\nneural connections in the RCNN model, realized by the random inactivation\nweight matrix of link input. This releases the computational burden of PCNN,\nmaking it affordable to achieve vast neural connections. Furthermore, the image\nand video processing mechanisms of RCNN are researched. It encodes constant\nstimuli as periodic spike trains and periodic stimuli as chaotic spike trains,\nthe same as biological neural information encoding characteristics. Finally,\nthe RCNN is applicated to image segmentation, fusion, and pulse shape\ndiscrimination subtasks. It is demonstrated to be robust, efficient, and highly\nanti-noised, with outstanding performance in all applications mentioned above.\n", "rewritten_text": "In recent years, there has been a growing interest in enhancing the efficiency of current neural networks and modeling them after biological neural systems. The Pulse-coupled neural network (PCNN) has been widely used to simulate the computational features of the human brain in computer vision and neural network fields. However, there are still differences between the PCNN and biological neural systems, such as limited neural connections, high computational costs, and a lack of stochastic properties. This study introduces the random-coupled neural network (RCNN) as a solution to these challenges in PCNN's neuromorphic computing through a random inactivation process. By randomly closing some neural connections in the RCNN model using a random inactivation weight matrix, the computational load of PCNN is reduced, allowing for a larger number of neural connections. Additionally, the image and video processing mechanisms of RCNN are explored, encoding constant stimuli as periodic spike trains and periodic stimuli as chaotic spike trains, mirroring biological neural information encoding characteristics. Finally, RCNN is applied to tasks such as image segmentation, fusion, and pulse shape discrimination, demonstrating robustness, efficiency, high resistance to noise, and exceptional performance across all mentioned applications."}, "1907.0916": {"original_text": "  Facial Micro-Expressions (MEs) are spontaneous, involuntary facial movements\nwhen a person experiences an emotion but deliberately or unconsciously attempts\nto conceal his or her genuine emotions. Recently, ME recognition has attracted\nincreasing attention due to its potential applications such as clinical\ndiagnosis, business negotiation, interrogations, and security. However, it is\nexpensive to build large scale ME datasets, mainly due to the difficulty of\ninducing spontaneous MEs. This limits the application of deep learning\ntechniques which require lots of training data. In this paper, we propose a\nsimple, efficient yet robust descriptor called Extended Local Binary Patterns\non Three Orthogonal Planes (ELBPTOP) for ME recognition. ELBPTOP consists of\nthree complementary binary descriptors: LBPTOP and two novel ones Radial\nDifference LBPTOP (RDLBPTOP) and Angular Difference LBPTOP (ADLBPTOP), which\nexplore the local second order information along the radial and angular\ndirections contained in ME video sequences. ELBPTOP is a novel ME descriptor\ninspired by unique and subtle facial movements. It is computationally efficient\nand only marginally increases the cost of computing LBPTOP, yet is extremely\neffective for ME recognition. In addition, by firstly introducing Whitened\nPrincipal Component Analysis (WPCA) to ME recognition, we can further obtain\nmore compact and discriminative feature representations, then achieve\nsignificantly computational savings. Extensive experimental evaluation on three\npopular spontaneous ME datasets SMIC, CASME II and SAMM show that our proposed\nELBPTOP approach significantly outperforms the previous state-of-the-art on all\nthree single evaluated datasets and achieves promising results on\ncross-database recognition.Our code will be made available.\n", "rewritten_text": "Facial Micro-Expressions (MEs) are brief facial movements that occur involuntarily when a person tries to hide their true emotions. The recognition of MEs has gained attention for its potential in various fields such as healthcare, business, and security. However, creating large datasets for ME recognition is costly due to the challenge of capturing spontaneous expressions. This hinders the use of deep learning techniques that rely on extensive training data. In this study, we introduce a new descriptor called Extended Local Binary Patterns on Three Orthogonal Planes (ELBPTOP) for ME recognition. ELBPTOP includes three binary descriptors: LBPTOP, Radial Difference LBPTOP (RDLBPTOP), and Angular Difference LBPTOP (ADLBPTOP), which capture local second-order information in ME video sequences. ELBPTOP is a computationally efficient and effective descriptor inspired by subtle facial movements. Additionally, we propose the use of Whitened Principal Component Analysis (WPCA) to enhance feature representation and reduce computational costs. Our experimental results on three ME datasets demonstrate that ELBPTOP outperforms existing methods and shows promise for cross-database recognition. Our code will be available for further research."}, "2402.11431": {"original_text": "  To address the issue of increased triangulation uncertainty caused by\nselecting views with small camera baselines in Structure from Motion (SFM) view\nselection, this paper proposes a robust error-resistant view selection method.\nThe method utilizes a triangulation-based computation to obtain an\nerror-resistant model, which is then used to construct an error-resistant\nmatrix. The sorting results of each row in the error-resistant matrix determine\nthe candidate view set for each view. By traversing the candidate view sets of\nall views and completing the missing views based on the error-resistant matrix,\nthe integrity of 3D reconstruction is ensured. Experimental comparisons between\nthis method and the exhaustive method with the highest accuracy in the COLMAP\nprogram are conducted in terms of average reprojection error and absolute\ntrajectory error in the reconstruction results. The proposed method\ndemonstrates an average reduction of 29.40% in reprojection error accuracy and\n5.07% in absolute trajectory error on the TUM dataset and DTU dataset.\n", "rewritten_text": "This paper introduces a method to address the challenge of increased uncertainty in triangulation due to selecting views with small camera baselines in Structure from Motion (SFM) view selection. The proposed approach is a robust error-resistant view selection method that employs triangulation-based computation to create an error-resistant model and matrix. The sorting of rows in the error-resistant matrix determines the candidate view set for each view. By analyzing these candidate view sets and filling in missing views using the error-resistant matrix, the method ensures the integrity of 3D reconstruction. Experimental comparisons with the exhaustive method in the COLMAP program show that the proposed method achieves a 29.40% reduction in reprojection error accuracy and a 5.07% decrease in absolute trajectory error on the TUM and DTU datasets."}, "2306.12693": {"original_text": "  This paper gives an Indic-to-Indic (IL-IL) MNMT baseline model for 11 ILs\nimplemented on the Samanantar corpus and analyzed on the Flores-200 corpus. All\nthe models are evaluated using the BLEU score. In addition, the languages are\nclassified under three groups namely East Indo- Aryan (EI), Dravidian (DR), and\nWest Indo-Aryan (WI). The effect of language relatedness on MNMT model\nefficiency is studied. Owing to the presence of large corpora from English (EN)\nto ILs, MNMT IL-IL models using EN as a pivot are also built and examined. To\nachieve this, English- Indic (EN-IL) models are also developed, with and\nwithout the usage of related languages. Results reveal that using related\nlanguages is beneficial for the WI group only, while it is detrimental for the\nEI group and shows an inconclusive effect on the DR group, but it is useful for\nEN-IL models. Thus, related language groups are used to develop pivot MNMT\nmodels. Furthermore, the IL corpora are transliterated from the corresponding\nscripts to a modified ITRANS script, and the best MNMT models from the previous\napproaches are built on the transliterated corpus. It is observed that the\nusage of pivot models greatly improves MNMT baselines with AS-TA achieving the\nminimum BLEU score and PA-HI achieving the maximum score. Among languages, AS,\nML, and TA achieve the lowest BLEU score, whereas HI, PA, and GU perform the\nbest. Transliteration also helps the models with few exceptions. The best\nincrement of scores is observed in ML, TA, and BN and the worst average\nincrement is observed in KN, HI, and PA, across all languages. The best model\nobtained is the PA-HI language pair trained on PAWI transliterated corpus which\ngives 24.29 BLEU.\n", "rewritten_text": "This study presents a baseline model for Indic-to-Indic (IL-IL) machine translation, focusing on 11 Indic languages. The model is trained on the Samanantar corpus and evaluated using the Flores-200 corpus, with performance measured using the BLEU score. The languages are categorized into three groups: East Indo-Aryan (EI), Dravidian (DR), and West Indo-Aryan (WI). The impact of language relatedness on the efficiency of the machine translation model is investigated. Additionally, models using English as a pivot for translation between English (EN) and Indic languages are developed and analyzed. The results indicate that leveraging related languages is beneficial for the WI group but not for the EI group, with inconclusive effects on the DR group. However, related languages are found to be useful for EN-IL models. The study also involves transliterating the Indic language corpora into a modified ITRANS script, leading to improved performance in machine translation models. Among the languages, AS, ML, and TA have the lowest BLEU scores, while HI, PA, and GU perform the best. Notably, transliteration has varying effects on different languages, with ML, TA, and BN showing the most improvement and KN, HI, and PA showing the least. The top-performing model is the PA-HI language pair trained on the PAWI transliterated corpus, achieving a BLEU score of 24.29."}, "2403.08487": {"original_text": "  Diffusion models pose risks of privacy breaches and copyright disputes,\nprimarily stemming from the potential utilization of unauthorized data during\nthe training phase. The Training Membership Inference (TMI) task aims to\ndetermine whether a specific sample has been used in the training process of a\ntarget model, representing a critical tool for privacy violation verification.\nHowever, the increased stochasticity inherent in diffusion renders traditional\nshadow-model-based or metric-based methods ineffective when applied to\ndiffusion models. Moreover, existing methods only yield binary classification\nlabels which lack necessary comprehensibility in practical applications. In\nthis paper, we explore a novel perspective for the TMI task by leveraging the\nintrinsic generative priors within the diffusion model. Compared with unseen\nsamples, training samples exhibit stronger generative priors within the\ndiffusion model, enabling the successful reconstruction of substantially\ndegraded training images. Consequently, we propose the Degrade Restore Compare\n(DRC) framework. In this framework, an image undergoes sequential degradation\nand restoration, and its membership is determined by comparing it with the\nrestored counterpart. Experimental results verify that our approach not only\nsignificantly outperforms existing methods in terms of accuracy but also\nprovides comprehensible decision criteria, offering evidence for potential\nprivacy violations.\n", "rewritten_text": "Diffusion models present risks of privacy breaches and copyright disputes due to the possible use of unauthorized data in the training phase. The Training Membership Inference (TMI) task seeks to identify if a specific sample was included in the training of a target model, serving as a crucial tool for verifying privacy violations. However, the increased randomness inherent in diffusion makes traditional shadow-model-based or metric-based methods ineffective for diffusion models. Additionally, current methods only offer binary classification labels that lack practical applicability. This study introduces a new approach to the TMI task by leveraging the generative priors within the diffusion model. Training samples demonstrate stronger generative priors within the model compared to unseen samples, allowing for successful reconstruction of degraded training images. The proposed Degrade Restore Compare (DRC) framework involves sequentially degrading and restoring an image, with membership determined by comparing it to the restored version. Experimental results show that this approach not only surpasses existing methods in accuracy but also provides clear decision criteria, offering insights into potential privacy violations."}, "2309.03390": {"original_text": "  In this paper, we seek a new method in designing an iris recognition system.\nIn this method, first the Haar wavelet features are extracted from iris images.\nThe advantage of using these features is the high-speed extraction, as well as\nbeing unique to each iris. Then the back propagation neural network (BPNN) is\nused as a classifier. In this system, the BPNN parallel algorithms and their\nimplementation on GPUs have been used by the aid of CUDA in order to speed up\nthe learning process. Finally, the system performance and the speeding outcomes\nin a way that this algorithm is done in series are presented.\n", "rewritten_text": "This paper introduces a novel approach to developing an iris recognition system. The method involves extracting Haar wavelet features from iris images, known for their quick extraction speed and uniqueness to each iris. A back propagation neural network (BPNN) is utilized as a classifier in this system. To enhance the learning process, BPNN parallel algorithms are implemented on GPUs using CUDA technology. The performance of the system and the acceleration achieved by running the algorithm in series are also discussed."}, "2311.10436": {"original_text": "  Since their inception, embeddings have become a primary ingredient in many\nflavours of Natural Language Processing (NLP) tasks supplanting earlier types\nof representation. Even though multilingual embeddings have been used for the\nincreasing number of multilingual tasks, due to the scarcity of parallel\ntraining data, low-resource languages such as Sinhala, tend to focus more on\nmonolingual embeddings. Then when it comes to the aforementioned multi-lingual\ntasks, it is challenging to utilize these monolingual embeddings given that\neven if the embedding spaces have a similar geometric arrangement due to an\nidentical training process, the embeddings of the languages considered are not\naligned. This is solved by the embedding alignment task. Even in this,\nhigh-resource language pairs are in the limelight while low-resource languages\nsuch as Sinhala which is in dire need of help seem to have fallen by the\nwayside. In this paper, we try to align Sinhala and English word embedding\nspaces based on available alignment techniques and introduce a benchmark for\nSinhala language embedding alignment. In addition to that, to facilitate the\nsupervised alignment, as an intermediate task, we also introduce\nSinhala-English alignment datasets. These datasets serve as our anchor datasets\nfor supervised word embedding alignment. Even though we do not obtain results\ncomparable to the high-resource languages such as French, German, or Chinese,\nwe believe our work lays the groundwork for more specialized alignment between\nEnglish and Sinhala embeddings.\n", "rewritten_text": "\"Since their inception, embeddings have become a key component in various Natural Language Processing (NLP) tasks, replacing previous types of representation. While multilingual embeddings have been utilized for a growing number of multilingual tasks, low-resource languages like Sinhala tend to rely more on monolingual embeddings due to limited parallel training data. However, when it comes to multi-lingual tasks, using these monolingual embeddings poses a challenge as the embedding spaces of different languages, even if trained similarly, are not aligned geometrically. This misalignment is addressed through the embedding alignment task, with a focus on high-resource language pairs while low-resource languages like Sinhala often receive less attention. This study aims to align Sinhala and English word embedding spaces using existing alignment techniques and introduces a benchmark for Sinhala language embedding alignment. Additionally, to support supervised alignment, Sinhala-English alignment datasets are introduced as intermediate tasks. While the results may not be as strong as those for high-resource languages such as French, German, or Chinese, this work establishes a foundation for more specialized alignment between English and Sinhala embeddings.\""}, "2301.08245": {"original_text": "  Estimating depth from images nowadays yields outstanding results, both in\nterms of in-domain accuracy and generalization. However, we identify two main\nchallenges that remain open in this field: dealing with non-Lambertian\nmaterials and effectively processing high-resolution images. Purposely, we\npropose a novel dataset that includes accurate and dense ground-truth labels at\nhigh resolution, featuring scenes containing several specular and transparent\nsurfaces. Our acquisition pipeline leverages a novel deep space-time stereo\nframework, enabling easy and accurate labeling with sub-pixel precision. The\ndataset is composed of 606 samples collected in 85 different scenes, each\nsample includes both a high-resolution pair (12 Mpx) as well as an unbalanced\nstereo pair (Left: 12 Mpx, Right: 1.1 Mpx), typical of modern mobile devices\nthat mount sensors with different resolutions. Additionally, we provide\nmanually annotated material segmentation masks and 15K unlabeled samples. The\ndataset is composed of a train set and two test sets, the latter devoted to the\nevaluation of stereo and monocular depth estimation networks. Our experiments\nhighlight the open challenges and future research directions in this field.\n", "rewritten_text": "Estimating depth from images has shown remarkable progress in terms of accuracy and generalization. However, there are two main challenges that still need to be addressed in this area: handling non-Lambertian materials and effectively processing high-resolution images. To address these challenges, we have created a new dataset with detailed ground-truth labels at high resolution, featuring scenes with specular and transparent surfaces. Our dataset consists of 606 samples from 85 different scenes, each including a high-resolution pair (12 Mpx) and an unbalanced stereo pair (Left: 12 Mpx, Right: 1.1 Mpx) to mimic modern mobile devices with sensors of varying resolutions. We also provide manually annotated material segmentation masks and 15K unlabeled samples. The dataset is divided into a training set and two test sets for evaluating stereo and monocular depth estimation networks. Our experiments shed light on the remaining challenges and future research directions in this field."}, "1709.00813": {"original_text": "  Various text analysis techniques exist, which attempt to uncover unstructured\ninformation from text. In this work, we explore using statistical dependence\nmeasures for textual classification, representing text as word vectors. Student\nsatisfaction scores on a 3-point scale and their free text comments written\nabout university subjects are used as the dataset. We have compared two textual\nrepresentations: a frequency word representation and term frequency\nrelationship to word vectors, and found that word vectors provide a greater\naccuracy. However, these word vectors have a large number of features which\naggravates the burden of computational complexity. Thus, we explored using a\nnon-linear dependency measure for feature selection by maximizing the\ndependence between the text reviews and corresponding scores. Our quantitative\nand qualitative analysis on a student satisfaction dataset shows that our\napproach achieves comparable accuracy to the full feature vector, while being\nan order of magnitude faster in testing. These text analysis and feature\nreduction techniques can be used for other textual data applications such as\nsentiment analysis.\n", "rewritten_text": "Various techniques for analyzing text are available to extract unstructured information. In this study, we investigate the use of statistical dependence measures for classifying text by representing it as word vectors. The dataset consists of student satisfaction scores on a 3-point scale and their free-text comments about university subjects. We compared two textual representations: frequency word representation and term frequency relationship to word vectors, and found that word vectors offer higher accuracy. However, the large number of features in word vectors increases computational complexity. Therefore, we explored using a non-linear dependency measure for feature selection by maximizing the dependence between text reviews and scores. Our analysis on a student satisfaction dataset demonstrates that our approach achieves comparable accuracy to the full feature vector but is significantly faster during testing. These text analysis and feature reduction techniques can be applied to other textual data applications, such as sentiment analysis."}, "1704.0803": {"original_text": "  Some lung diseases are related to bronchial airway structures and morphology.\nAlthough airway segmentation from chest CT volumes is an important task in the\ncomputer-aided diagnosis and surgery assistance systems for the chest, complete\n3-D airway structure segmentation is a quite challenging task due to its\ncomplex tree-like structure. In this paper, we propose a new airway\nsegmentation method from 3D chest CT volumes based on volume of interests (VOI)\nusing gradient vector flow (GVF). This method segments the bronchial regions by\napplying the cavity enhancement filter (CEF) to trace the bronchial tree\nstructure from the trachea. It uses the CEF in the VOI to segment each branch.\nAnd a tube-likeness function based on GVF and the GVF magnitude map in each VOI\nare utilized to assist predicting the positions and directions of child\nbranches. By calculating the tube-likeness function based on GVF and the GVF\nmagnitude map, the airway-like candidate structures are identified and their\ncentrelines are extracted. Based on the extracted centrelines, we can detect\nthe branch points of the bifurcations and directions of the airway branches in\nthe next level. At the same time, a leakage detection is performed to avoid the\nleakage by analysing the pixel information and the shape information of airway\ncandidate regions extracted in the VOI. Finally, we unify all of the extracted\nbronchial regions to form an integrated airway tree. Preliminary experiments\nusing four cases of chest CT volumes demonstrated that the proposed method can\nextract more bronchial branches in comparison with other methods.\n", "rewritten_text": "Certain lung diseases are associated with the structures and morphology of the bronchial airways. While segmenting airways from chest CT scans is crucial for computer-aided diagnosis and surgical assistance systems, fully delineating the 3D airway structure is a complex task due to its tree-like nature. This study introduces a novel method for airway segmentation from 3D chest CT scans based on volume of interests (VOI) using gradient vector flow (GVF). The method identifies bronchial regions by applying a cavity enhancement filter (CEF) to trace the bronchial tree structure from the trachea within the VOI. A tube-likeness function, incorporating GVF and the GVF magnitude map in each VOI, aids in predicting the positions and directions of child branches. By calculating the tube-likeness function, airway-like candidate structures are identified, and their centrelines are extracted. Branch points and directions of airway branches are detected based on the extracted centrelines, with leakage detection performed to prevent leakage by analyzing pixel and shape information of airway candidate regions within the VOI. Ultimately, all extracted bronchial regions are combined to create an integrated airway tree. Initial experiments on four chest CT cases show that this method can extract more bronchial branches compared to other techniques."}, "1906.03249": {"original_text": "  Word embeddings are traditionally trained on a large corpus in an\nunsupervised setting, with no specific design for incorporating domain\nknowledge. This can lead to unsatisfactory performances when training data\noriginate from heterogeneous domains. In this paper, we propose two novel\nmechanisms for domain-aware word embedding training, namely domain indicator\nand domain attention, which integrate domain-specific knowledge into the widely\nused SG and CBOW models, respectively. The two methods are based on a joint\nlearning paradigm and ensure that words in a target domain are intensively\nfocused when trained on a source domain corpus. Qualitative and quantitative\nevaluation confirm the validity and effectiveness of our models. Compared to\nbaseline methods, our method is particularly effective in near-cold-start\nscenarios.\n", "rewritten_text": "In this study, we introduce two innovative approaches for training word embeddings that take into account domain-specific knowledge. Traditionally, word embeddings are trained on a large corpus without considering domain expertise, which can result in subpar performance when dealing with diverse data sources. Our proposed mechanisms, domain indicator and domain attention, integrate domain-specific information into the popular SG and CBOW models. These methods, based on joint learning, ensure that words from a target domain receive focused attention when trained on a corpus from a different domain. Both qualitative and quantitative assessments validate the effectiveness of our models, showing superior performance compared to standard methods, especially in scenarios with limited prior data."}, "2305.17975": {"original_text": "  Automated assembly of 3D fractures is essential in orthopedics, archaeology,\nand our daily life. This paper presents Jigsaw, a novel framework for\nassembling physically broken 3D objects from multiple pieces. Our approach\nleverages hierarchical features of global and local geometry to match and align\nthe fracture surfaces. Our framework consists of four components: (1) front-end\npoint feature extractor with attention layers, (2) surface segmentation to\nseparate fracture and original parts, (3) multi-parts matching to find\ncorrespondences among fracture surface points, and (4) robust global alignment\nto recover the global poses of the pieces. We show how to jointly learn\nsegmentation and matching and seamlessly integrate feature matching and\nrigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and\nachieve superior performance compared to state-of-the-art methods. Our method\nalso generalizes well to diverse fracture modes, objects, and unseen instances.\nTo the best of our knowledge, this is the first learning-based method designed\nspecifically for 3D fracture assembly over multiple pieces. Our code is\navailable at https://jiaxin-lu.github.io/Jigsaw/.\n", "rewritten_text": "Automated assembly of 3D fractures is crucial in various fields such as orthopedics, archaeology, and everyday life. This study introduces Jigsaw, an innovative framework for reconstructing physically damaged 3D objects using multiple pieces. Our method utilizes both global and local geometric features to match and align fracture surfaces. The framework comprises four main components: (1) a front-end point feature extractor with attention layers, (2) surface segmentation to distinguish between fracture and original parts, (3) multi-parts matching to identify correspondences among fracture surface points, and (4) robust global alignment to determine the overall positions of the pieces. We demonstrate the simultaneous learning of segmentation and matching, as well as the seamless integration of feature matching and rigidity constraints. Evaluation on the Breaking Bad dataset shows that Jigsaw outperforms existing methods. Our approach also demonstrates strong generalization capabilities across various fracture types, objects, and unseen instances. To our knowledge, this is the first learning-based method specifically tailored for assembling 3D fractures from multiple pieces. The code for our framework is accessible at https://jiaxin-lu.github.io/Jigsaw/."}, "2312.08367": {"original_text": "  In this work, we propose an efficient Video-Language Alignment (ViLA)\nnetwork. Our ViLA model addresses both efficient frame sampling and effective\ncross-modal alignment in a unified way. In our ViLA network, we design a new\nlearnable text-guided Frame-Prompter together with a new cross-modal\ndistillation (QFormer-Distiller) module. Pre-trained large image-language\nmodels have shown promising results on problems such as visual question\nanswering (VQA). However, how to efficiently and effectively sample video\nframes when adapting pre-trained large image-language model to video-language\nalignment is still the major challenge. Compared with prior work, our ViLA\nmodel demonstrates the capability of selecting key frames with critical\ncontents, thus improving the video-language alignment accuracy while reducing\nthe inference latency +3.3% on NExT-QA Temporal with 3.0X speed up). Overall,\nour ViLA network outperforms the state-of-the-art methods on the video\nquestion-answering benchmarks: +4.6% on STAR Interaction, +2.2% on STAR average\nwith 3.0X speed up, ours 2-frames out-perform SeViLA 4-frames on the VLEP\ndataset with 4.2X speed-up. The code will be available at\nhttps://github.com/xijun-cs/ViLA.\n", "rewritten_text": "In this study, we introduce a Video-Language Alignment (ViLA) network that is efficient and effective. Our ViLA model tackles the challenges of frame sampling and cross-modal alignment in a cohesive manner. Within our ViLA network, we introduce a novel learnable text-guided Frame-Prompter and a cross-modal distillation module called QFormer-Distiller. While pre-trained large image-language models have shown promise in tasks like visual question answering (VQA), efficiently sampling video frames for video-language alignment remains a significant hurdle. Our ViLA model excels in selecting key frames with crucial content, enhancing video-language alignment accuracy and reducing inference latency by 3.3% on NExT-QA Temporal with a 3.0X speedup. Overall, our ViLA network surpasses state-of-the-art methods in video question-answering benchmarks, achieving a 4.6% improvement on STAR Interaction, a 2.2% improvement on STAR average with a 3.0X speedup, and outperforming SeViLA 4-frames on the VLEP dataset with a 4.2X speedup using only 2 frames. The code for our ViLA network will be accessible at https://github.com/xijun-cs/ViLA."}, "1809.10692": {"original_text": "  In this paper, we target the problem of fracture classification from clinical\nX-Ray images towards an automated Computer Aided Diagnosis (CAD) system.\nAlthough primarily dealing with an image classification problem, we argue that\nlocalizing the fracture in the image is crucial to make good class predictions.\nTherefore, we propose and thoroughly analyze several schemes for simultaneous\nfracture localization and classification. We show that using an auxiliary\nlocalization task, in general, improves the classification performance.\nMoreover, it is possible to avoid the need for additional localization\nannotations thanks to recent advancements in weakly-supervised deep learning\napproaches. Among such approaches, we investigate and adapt Spatial\nTransformers (ST), Self-Transfer Learning (STL), and localization from global\npooling layers. We provide a detailed quantitative and qualitative validation\non a dataset of 1347 femur fractures images and report high accuracy with\nregard to inter-expert correlation values reported in the literature. Our\ninvestigations show that i) lesion localization improves the classification\noutcome, ii) weakly-supervised methods improve baseline classification without\nany additional cost, iii) STL guides feature activations and boost performance.\nWe plan to make both the dataset and code available.\n", "rewritten_text": "This paper focuses on the issue of classifying fractures in clinical X-Ray images using a Computer Aided Diagnosis (CAD) system. While the main task is image classification, we emphasize the importance of accurately locating the fracture within the image to enhance classification accuracy. To address this, we propose and evaluate various methods for simultaneously localizing and classifying fractures. Our research demonstrates that incorporating a localization task generally enhances classification results. Additionally, recent advancements in weakly-supervised deep learning techniques allow us to avoid the need for extra localization annotations. We explore and adapt Spatial Transformers (ST), Self-Transfer Learning (STL), and global pooling layer localization methods. Through a comprehensive analysis on a dataset of 1347 femur fracture images, we achieve high accuracy compared to existing inter-expert correlation values. Our findings indicate that lesion localization enhances classification outcomes, weakly-supervised methods improve baseline performance at no extra cost, and STL enhances feature activations to boost performance. We intend to share both the dataset and code for further research."}, "2309.16039": {"original_text": "  We present a series of long-context LLMs that support effective context\nwindows of up to 32,768 tokens. Our model series are built through continual\npretraining from Llama 2 with longer training sequences and on a dataset where\nlong texts are upsampled. We perform extensive evaluation on language modeling,\nsynthetic context probing tasks, and a wide range of research benchmarks. On\nresearch benchmarks, our models achieve consistent improvements on most regular\ntasks and significant improvements on long-context tasks over Llama 2. Notably,\nwith a cost-effective instruction tuning procedure that does not require\nhuman-annotated long instruction data, the 70B variant can already surpass\ngpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.\nAlongside these results, we provide an in-depth analysis on the individual\ncomponents of our method. We delve into Llama's position encodings and discuss\nits limitation in modeling long dependencies. We also examine the impact of\nvarious design choices in the pretraining process, including the data mix and\nthe training curriculum of sequence lengths -- our ablation experiments suggest\nthat having abundant long texts in the pretrain dataset is not the key to\nachieving strong performance, and we empirically verify that long context\ncontinual pretraining is more efficient and similarly effective compared to\npretraining from scratch with long sequences.\n", "rewritten_text": "We introduce a series of long-context Large Language Models (LLMs) that can effectively handle context windows of up to 32,768 tokens. These models are developed by continuously pretraining from Llama 2 using longer training sequences and an upsampled dataset containing long texts. Our evaluation includes language modeling, synthetic context probing tasks, and various research benchmarks. Our models consistently outperform Llama 2 on most tasks and show significant improvements on tasks requiring long-context understanding. Notably, our 70B variant surpasses gpt-3.5-turbo-16k's performance on long-context tasks without the need for human-annotated long instruction data. We also conduct a detailed analysis of our method, exploring aspects such as Llama's position encodings and the impact of different design choices in the pretraining process. Our experiments indicate that having a large amount of long texts in the pretraining dataset is not crucial for achieving strong performance, and we demonstrate that continual pretraining with long context is more efficient and equally effective compared to starting from scratch with long sequences."}, "2403.02211": {"original_text": "  Popular methods usually use a degradation model in a supervised way to learn\na watermark removal model. However, it is true that reference images are\ndifficult to obtain in the real world, as well as collected images by cameras\nsuffer from noise. To overcome these drawbacks, we propose a perceptive\nself-supervised learning network for noisy image watermark removal (PSLNet) in\nthis paper. PSLNet depends on a parallel network to remove noise and\nwatermarks. The upper network uses task decomposition ideas to remove noise and\nwatermarks in sequence. The lower network utilizes the degradation model idea\nto simultaneously remove noise and watermarks. Specifically, mentioned paired\nwatermark images are obtained in a self supervised way, and paired noisy images\n(i.e., noisy and reference images) are obtained in a supervised way. To enhance\nthe clarity of obtained images, interacting two sub-networks and fusing\nobtained clean images are used to improve the effects of image watermark\nremoval in terms of structural information and pixel enhancement. Taking into\ntexture information account, a mixed loss uses obtained images and features to\nachieve a robust model of noisy image watermark removal. Comprehensive\nexperiments show that our proposed method is very effective in comparison with\npopular convolutional neural networks (CNNs) for noisy image watermark removal.\nCodes can be obtained at https://github.com/hellloxiaotian/PSLNet.\n", "rewritten_text": "Commonly used approaches typically employ a supervised degradation model to train a watermark removal model. However, obtaining reference images in real-world scenarios can be challenging, and images captured by cameras often contain noise. To address these issues, we introduce a perceptual self-supervised learning network for removing noisy image watermarks, known as PSLNet. PSLNet utilizes a dual-network approach to eliminate noise and watermarks. The upper network employs task decomposition techniques to sequentially remove noise and watermarks, while the lower network leverages the degradation model concept to simultaneously eliminate noise and watermarks. Paired watermark images are acquired through self-supervised learning, while paired noisy images (i.e., noisy and reference images) are obtained through supervised learning. To enhance image clarity, the interaction of two sub-networks and the fusion of clean images are employed to improve the effectiveness of image watermark removal in terms of structural information and pixel enhancement. By considering texture information, a combined loss function utilizes obtained images and features to establish a robust model for removing noisy image watermarks. Extensive experiments demonstrate the superior performance of our proposed method compared to popular convolutional neural networks (CNNs) for noisy image watermark removal. The source code is available at https://github.com/hellloxiaotian/PSLNet."}, "2104.09386": {"original_text": "  Mapping a single exposure low dynamic range (LDR) image into a high dynamic\nrange (HDR) is considered among the most strenuous image to image translation\ntasks due to exposure-related missing information. This study tackles the\nchallenges of single-shot LDR to HDR mapping by proposing a novel two-stage\ndeep network. Notably, our proposed method aims to reconstruct an HDR image\nwithout knowing hardware information, including camera response function (CRF)\nand exposure settings. Therefore, we aim to perform image enhancement task like\ndenoising, exposure correction, etc., in the first stage. Additionally, the\nsecond stage of our deep network learns tone mapping and bit-expansion from a\nconvex set of data samples. The qualitative and quantitative comparisons\ndemonstrate that the proposed method can outperform the existing LDR to HDR\nworks with a marginal difference. Apart from that, we collected an LDR image\ndataset incorporating different camera systems. The evaluation with our\ncollected real-world LDR images illustrates that the proposed method can\nreconstruct plausible HDR images without presenting any visual artefacts. Code\navailable: https://github. com/sharif-apu/twostageHDR_NTIRE21.\n", "rewritten_text": "Converting a single low dynamic range (LDR) image to a high dynamic range (HDR) is considered a challenging task due to missing exposure-related information. This study addresses this challenge by introducing a new two-stage deep network for LDR to HDR mapping. Our method aims to reconstruct HDR images without requiring specific hardware information such as camera response function (CRF) or exposure settings. The first stage focuses on tasks like denoising and exposure correction, while the second stage learns tone mapping and bit-expansion from a set of data samples. Comparative analysis shows that our method outperforms existing LDR to HDR techniques with only a slight difference. We also created an LDR image dataset with various camera systems for evaluation, demonstrating that our method can generate realistic HDR images without visual artifacts. Source code is available at: https://github.com/sharif-apu/twostageHDR_NTIRE21."}, "2409.09170": {"original_text": "  The diagnosis and treatment of individuals with communication disorders\noffers many opportunities for the application of speech technology, but\nresearch so far has not adequately considered: the diversity of conditions, the\nrole of pragmatic deficits, and the challenges of limited data. This paper\nexplores how a general-purpose model of perceived pragmatic similarity may\novercome these limitations. It explains how it might support several use cases\nfor clinicians and clients, and presents evidence that a simple model can\nprovide value, and in particular can capture utterance aspects that are\nrelevant to diagnoses of autism and specific language impairment.\n", "rewritten_text": "The identification and management of people with communication disorders present numerous chances for utilizing speech technology. However, current research has not fully addressed the variety of conditions, the impact of pragmatic deficits, and the difficulties posed by limited data. This paper investigates how a universal model of perceived pragmatic similarity could address these issues. It discusses how this model could assist clinicians and patients in various scenarios and demonstrates that a basic model can be beneficial, especially in capturing speech elements crucial for diagnosing autism and specific language impairment."}, "2309.04462": {"original_text": "  Real-world application of chest X-ray abnormality classification requires\ndealing with several challenges: (i) limited training data; (ii) training and\nevaluation sets that are derived from different domains; and (iii) classes that\nappear during training may have partial overlap with classes of interest during\nevaluation. To address these challenges, we present an integrated framework\ncalled Generalized Cross-Domain Multi-Label Few-Shot Learning (GenCDML-FSL).\nThe framework supports overlap in classes during training and evaluation,\ncross-domain transfer, adopts meta-learning to learn using few training\nsamples, and assumes each chest X-ray image is either normal or associated with\none or more abnormalities. Furthermore, we propose Generalized Episodic\nTraining (GenET), a training strategy that equips models to operate with\nmultiple challenges observed in the GenCDML-FSL scenario. Comparisons with\nwell-established methods such as transfer learning, hybrid transfer learning,\nand multi-label meta-learning on multiple datasets show the superiority of our\napproach.\n", "rewritten_text": "The practical use of classifying abnormal chest X-ray findings in real-world scenarios presents various challenges, including limited training data, differences between training and evaluation sets from various domains, and potential overlap between classes during training and evaluation. To tackle these obstacles, we introduce a comprehensive framework called Generalized Cross-Domain Multi-Label Few-Shot Learning (GenCDML-FSL). This framework accommodates class overlap, cross-domain transfer, utilizes meta-learning with minimal training data, and assumes each chest X-ray image is either normal or exhibits one or more abnormalities. Additionally, we propose Generalized Episodic Training (GenET) as a training strategy to prepare models for the complexities encountered in the GenCDML-FSL setting. Comparative analyses with established methods like transfer learning, hybrid transfer learning, and multi-label meta-learning across multiple datasets demonstrate the effectiveness of our approach."}, "1811.071": {"original_text": "  Few-shot deep learning is a topical challenge area for scaling visual\nrecognition to open ended growth of unseen new classes with limited labeled\nexamples. A promising approach is based on metric learning, which trains a deep\nembedding to support image similarity matching. Our insight is that effective\ngeneral purpose matching requires non-linear comparison of features at multiple\nabstraction levels. We thus propose a new deep comparison network comprised of\nembedding and relation modules that learn multiple non-linear distance metrics\nbased on different levels of features simultaneously. Furthermore, to reduce\nover-fitting and enable the use of deeper embeddings, we represent images as\ndistributions rather than vectors via learning parameterized Gaussian noise\nregularization. The resulting network achieves excellent performance on both\nminiImageNet and tieredImageNet.\n", "rewritten_text": "\"Addressing the challenge of few-shot deep learning involves scaling visual recognition to accommodate the growth of new classes with limited labeled examples. One promising approach utilizes metric learning to train a deep embedding for image similarity matching. Our key insight is that effective matching requires nonlinear feature comparison at various levels of abstraction. To this end, we introduce a novel deep comparison network consisting of embedding and relation modules that learn multiple nonlinear distance metrics across different feature levels simultaneously. Additionally, to prevent overfitting and facilitate the use of deeper embeddings, we represent images as distributions rather than vectors by incorporating parameterized Gaussian noise regularization. This network demonstrates outstanding performance on both miniImageNet and tieredImageNet datasets.\""}, "2306.07279": {"original_text": "  We introduce Cap3D, an automatic approach for generating descriptive text for\n3D objects. This approach utilizes pretrained models from image captioning,\nimage-text alignment, and LLM to consolidate captions from multiple views of a\n3D asset, completely side-stepping the time-consuming and costly process of\nmanual annotation. We apply Cap3D to the recently introduced large-scale 3D\ndataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted\nusing 41k human annotations from the same dataset, demonstrates that Cap3D\nsurpasses human-authored descriptions in terms of quality, cost, and speed.\nThrough effective prompt engineering, Cap3D rivals human performance in\ngenerating geometric descriptions on 17k collected annotations from the ABO\ndataset. Finally, we finetune Text-to-3D models on Cap3D and human captions,\nand show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E,\nand DreamFusion.\n", "rewritten_text": "We present Cap3D, a method that automatically generates descriptive text for 3D objects. This method leverages pretrained models from image captioning, image-text alignment, and LLM to combine captions from various perspectives of a 3D asset, eliminating the need for manual annotation which is time-consuming and expensive. Cap3D was applied to the Objaverse dataset, resulting in 660k pairs of 3D objects and text. Evaluation using 41k human annotations from the same dataset shows that Cap3D surpasses human-written descriptions in terms of quality, cost, and speed. By utilizing effective prompt engineering, Cap3D achieves performance comparable to humans in generating geometric descriptions based on 17k annotations from the ABO dataset. Additionally, we fine-tune Text-to-3D models on Cap3D and human captions, demonstrating Cap3D's superior performance compared to the state-of-the-art models including Point-E, Shape-E, and DreamFusion."}, "2311.08469": {"original_text": "  Language technologies that accurately model the dynamics of events must\nperform commonsense reasoning. Existing work evaluating commonsense reasoning\nfocuses on making inferences about common, everyday situations. To instead\ninvestigate the ability to model unusual, unexpected, and unlikely situations,\nwe explore the task of uncommonsense abductive reasoning. Given a piece of\ncontext with an unexpected outcome, this task requires reasoning abductively to\ngenerate an explanation that makes the unexpected outcome more likely in the\ncontext. To this end, we curate and release a new English language corpus\ncalled UNcommonsense. We characterize the performance differences between human\nexplainers and the best-performing large language models, finding that\nmodel-enhanced human-written explanations achieve the highest quality by\ntrading off between specificity and diversity. Finally, we experiment with\nseveral imitation learning algorithms to train open and accessible language\nmodels on this task. When compared with the vanilla supervised fine-tuning\napproach, these methods consistently reduce lose rates on both common and\nuncommonsense abductive reasoning judged by human evaluators.\n", "rewritten_text": "\"Language technologies that accurately capture event dynamics require common sense reasoning. Current research on common sense reasoning focuses on drawing conclusions about everyday situations. In contrast, our study delves into the realm of uncommonsense abductive reasoning to explore the ability to model unusual and unexpected scenarios. This task involves generating explanations for unexpected outcomes in a given context through abductive reasoning. We introduce a new English language corpus named UNcommonsense for this purpose. By comparing human explainers with large language models, we find that a combination of model-enhanced human-written explanations strikes a balance between specificity and diversity, resulting in the highest quality. Additionally, we experiment with various imitation learning algorithms to train language models on this task, which consistently improve performance in both common and uncommonsense abductive reasoning according to human evaluators.\""}, "2305.1374": {"original_text": "  Tense inconsistency frequently occurs in machine translation. However, there\nare few criteria to assess the model's mastery of tense prediction from a\nlinguistic perspective. In this paper, we present a parallel tense test set,\ncontaining French-English 552 utterances. We also introduce a corresponding\nbenchmark, tense prediction accuracy. With the tense test set and the\nbenchmark, researchers are able to measure the tense consistency performance of\nmachine translation systems for the first time.\n", "rewritten_text": "Tense inconsistency is a common issue in machine translation, yet there is a lack of criteria for evaluating the model's ability to predict tense accurately from a linguistic standpoint. This paper introduces a parallel tense test set consisting of 552 French-English utterances. Additionally, a benchmark for tense prediction accuracy is presented. By using the tense test set and benchmark, researchers can now assess the tense consistency performance of machine translation systems for the first time."}, "2410.19294": {"original_text": "  Vision-language models, such as CLIP, have shown impressive generalization\ncapacities when using appropriate text descriptions. While optimizing prompts\non downstream labeled data has proven effective in improving performance, these\nmethods entail labor costs for annotations and are limited by their quality.\nAdditionally, since CLIP is pre-trained on highly imbalanced Web-scale data, it\nsuffers from inherent label bias that leads to suboptimal performance. To\ntackle the above challenges, we propose a label-Free prompt distribution\nlearning and bias correction framework, dubbed as **Frolic**, which boosts\nzero-shot performance without the need for labeled data. Specifically, our\nFrolic learns distributions over prompt prototypes to capture diverse visual\nrepresentations and adaptively fuses these with the original CLIP through\nconfidence matching. This fused model is further enhanced by correcting label\nbias via a label-free logit adjustment. Notably, our method is not only\ntraining-free but also circumvents the necessity for hyper-parameter tuning.\nExtensive experimental results across 16 datasets demonstrate the efficacy of\nour approach, particularly outperforming the state-of-the-art by an average of\n$2.6\\%$ on 10 datasets with CLIP ViT-B/16 and achieving an average margin of\n$1.5\\%$ on ImageNet and its five distribution shifts with CLIP ViT-B/16. Codes\nare available in https://github.com/zhuhsingyuu/Frolic.\n", "rewritten_text": "Vision-language models like CLIP have demonstrated strong generalization abilities with appropriate text descriptions. While optimizing prompts using labeled data has been effective, it comes with labor costs and quality limitations. CLIP, pre-trained on imbalanced Web-scale data, faces label bias issues that impact performance. To address these challenges, we introduce Frolic, a label-free prompt distribution learning and bias correction framework. Frolic enhances zero-shot performance without requiring labeled data by learning distributions over prompt prototypes and integrating them with CLIP through confidence matching. The model is further improved by correcting label bias through a label-free logit adjustment. Our method is training-free and eliminates the need for hyper-parameter tuning. Extensive experiments on 16 datasets show that Frolic outperforms the state-of-the-art by an average of 2.6% on 10 datasets with CLIP ViT-B/16 and achieves an average margin of 1.5% on ImageNet and its five distribution shifts with CLIP ViT-B/16. The code is available at https://github.com/zhuhsingyuu/Frolic."}, "1809.01219": {"original_text": "  A novel graph-to-tree conversion mechanism called the deep-tree generation\n(DTG) algorithm is first proposed to predict text data represented by graphs.\nThe DTG method can generate a richer and more accurate representation for nodes\n(or vertices) in graphs. It adds flexibility in exploring the vertex\nneighborhood information to better reflect the second order proximity and\nhomophily equivalence in a graph. Then, a Deep-Tree Recursive Neural Network\n(DTRNN) method is presented and used to classify vertices that contains text\ndata in graphs. To demonstrate the effectiveness of the DTRNN method, we apply\nit to three real-world graph datasets and show that the DTRNN method\noutperforms several state-of-the-art benchmarking methods.\n", "rewritten_text": "A new approach called the deep-tree generation (DTG) algorithm is introduced for converting graphs into trees in order to predict text data. The DTG method enhances the representation of nodes in graphs, allowing for better exploration of vertex neighborhood information to capture second order proximity and homophily equivalence. Subsequently, a Deep-Tree Recursive Neural Network (DTRNN) is proposed for classifying text-containing vertices in graphs. The effectiveness of the DTRNN method is demonstrated through experiments on three real-world graph datasets, showing superior performance compared to existing benchmark methods."}, "2403.09636": {"original_text": "  Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.\n", "rewritten_text": "Transformers have become essential for large language models (LLMs), but generating output is inefficient due to the need to store a cache of key-value representations for previous tokens in memory. This cache size grows linearly with the input sequence length and batch size. To address this issue, we introduce Dynamic Memory Compression (DMC), a method for compressing the key-value cache in real-time during inference. Notably, the model learns to apply varying compression ratios across different heads and layers. By integrating DMC into pre-trained LLMs like Llama 2 (7B, 13B, and 70B), we achieve a significant throughput increase of up to 7x during auto-regressive inference on an NVIDIA H100 GPU. DMC is implemented through additional pre-training on a small portion of the original data without introducing extra parameters. With DMC, we maintain the original downstream performance while achieving up to 4x cache compression, surpassing other methods like up-trained grouped-query attention (GQA) and key-value eviction policies (H$_2$O, TOVA). Combining GQA and DMC can lead to even greater improvements. Therefore, DMC can seamlessly replace key-value caching in existing LLMs to accommodate longer contexts and larger batches within a given memory constraint."}, "2108.06536": {"original_text": "  We address the problem of generalized zero-shot semantic segmentation (GZS3)\npredicting pixel-wise semantic labels for seen and unseen classes. Most GZS3\nmethods adopt a generative approach that synthesizes visual features of unseen\nclasses from corresponding semantic ones (e.g., word2vec) to train novel\nclassifiers for both seen and unseen classes. Although generative methods show\ndecent performance, they have two limitations: (1) the visual features are\nbiased towards seen classes; (2) the classifier should be retrained whenever\nnovel unseen classes appear. We propose a discriminative approach to address\nthese limitations in a unified framework. To this end, we leverage visual and\nsemantic encoders to learn a joint embedding space, where the semantic encoder\ntransforms semantic features to semantic prototypes that act as centers for\nvisual features of corresponding classes. Specifically, we introduce\nboundary-aware regression (BAR) and semantic consistency (SC) losses to learn\ndiscriminative features. Our approach to exploiting the joint embedding space,\ntogether with BAR and SC terms, alleviates the seen bias problem. At test time,\nwe avoid the retraining process by exploiting semantic prototypes as a\nnearest-neighbor (NN) classifier. To further alleviate the bias problem, we\nalso propose an inference technique, dubbed Apollonius calibration (AC), that\nmodulates the decision boundary of the NN classifier to the Apollonius circle\nadaptively. Experimental results demonstrate the effectiveness of our\nframework, achieving a new state of the art on standard benchmarks.\n", "rewritten_text": "We focus on the challenge of generalized zero-shot semantic segmentation (GZS3), which involves predicting semantic labels at the pixel level for both seen and unseen classes. While most existing GZS3 methods use a generative approach to create visual features for unseen classes based on semantic information, we identify two main drawbacks: the bias towards seen classes in visual features and the need to retrain the classifier for new unseen classes. In response, we propose a discriminative approach within a unified framework. Our method utilizes visual and semantic encoders to establish a shared embedding space, where semantic features are transformed into prototypes that serve as reference points for visual features of corresponding classes. By introducing boundary-aware regression (BAR) and semantic consistency (SC) losses, we aim to learn discriminative features and mitigate the bias issue. During testing, we avoid the retraining process by employing semantic prototypes as a nearest-neighbor (NN) classifier. Additionally, we introduce an inference technique called Apollonius calibration (AC) to dynamically adjust the decision boundary of the NN classifier to the Apollonius circle, further addressing the bias problem. Experimental results showcase the effectiveness of our framework, setting a new benchmark in this field."}, "1907.10815": {"original_text": "  Improvements in data-capture and face modeling techniques have enabled us to\ncreate high-fidelity realistic face models. However, driving these realistic\nface models requires special input data, e.g. 3D meshes and unwrapped textures.\nAlso, these face models expect clean input data taken under controlled lab\nenvironments, which is very different from data collected in the wild. All\nthese constraints make it challenging to use the high-fidelity models in\ntracking for commodity cameras. In this paper, we propose a self-supervised\ndomain adaptation approach to enable the animation of high-fidelity face models\nfrom a commodity camera. Our approach first circumvents the requirement for\nspecial input data by training a new network that can directly drive a face\nmodel just from a single 2D image. Then, we overcome the domain mismatch\nbetween lab and uncontrolled environments by performing self-supervised domain\nadaptation based on \"consecutive frame texture consistency\" based on the\nassumption that the appearance of the face is consistent over consecutive\nframes, avoiding the necessity of modeling the new environment such as lighting\nor background. Experiments show that we are able to drive a high-fidelity face\nmodel to perform complex facial motion from a cellphone camera without\nrequiring any labeled data from the new domain.\n", "rewritten_text": "Advancements in data-capture and face modeling techniques have allowed us to develop highly realistic face models. However, operating these realistic face models necessitates specific input data, such as 3D meshes and unwrapped textures. Moreover, these face models rely on clean input data obtained in controlled lab settings, which differs significantly from data gathered in natural environments. These limitations pose challenges in utilizing high-fidelity models for tracking with standard cameras. In this study, we introduce a self-supervised domain adaptation method to animate high-fidelity face models using a standard camera. Our approach eliminates the need for specialized input data by training a new network capable of driving a face model directly from a single 2D image. Additionally, we address the domain disparity between controlled lab environments and uncontrolled settings by employing self-supervised domain adaptation based on \"consecutive frame texture consistency,\" assuming that facial appearance remains consistent across consecutive frames. This approach avoids the need to model new environmental factors like lighting or background. Experimental results demonstrate our ability to animate a high-fidelity face model to perform intricate facial movements using a cellphone camera without requiring labeled data from the new environment."}, "2311.10651": {"original_text": "  Analysis of the 3D Texture is indispensable for various tasks, such as\nretrieval, segmentation, classification, and inspection of sculptures, knitted\nfabrics, and biological tissues. A 3D texture is a locally repeated surface\nvariation independent of the surface's overall shape and can be determined\nusing the local neighborhood and its characteristics. Existing techniques\ntypically employ computer vision techniques that analyze a 3D mesh globally,\nderive features, and then utilize the obtained features for retrieval or\nclassification. Several traditional and learning-based methods exist in the\nliterature, however, only a few are on 3D texture, and nothing yet, to the best\nof our knowledge, on the unsupervised schemes. This paper presents an original\nframework for the unsupervised segmentation of the 3D texture on the mesh\nmanifold. We approach this problem as binary surface segmentation, partitioning\nthe mesh surface into textured and non-textured regions without prior\nannotation. We devise a mutual transformer-based system comprising a label\ngenerator and a cleaner. The two models take geometric image representations of\nthe surface mesh facets and label them as texture or non-texture across an\niterative mutual learning scheme. Extensive experiments on three publicly\navailable datasets with diverse texture patterns demonstrate that the proposed\nframework outperforms standard and SOTA unsupervised techniques and competes\nreasonably with supervised methods.\n", "rewritten_text": "The analysis of 3D textures is crucial for various tasks like retrieval, segmentation, classification, and inspection of sculptures, knitted fabrics, and biological tissues. A 3D texture refers to a surface variation that is locally repeated and independent of the overall shape of the surface. It can be identified by examining the local neighborhood and its characteristics. Current methods typically use computer vision techniques to analyze the entire 3D mesh, extract features, and then apply these features for retrieval or classification purposes. While there are various traditional and learning-based approaches in the literature, only a few focus on 3D textures, and as far as we know, none address unsupervised schemes. This study introduces an innovative framework for unsupervised segmentation of 3D textures on the mesh surface. The problem is approached as binary surface segmentation, dividing the mesh surface into regions with textures and those without textures without any prior annotations. The proposed system consists of a label generator and a cleaner, both based on a mutual transformer approach. These models analyze geometric image representations of the surface mesh facets and classify them as textured or non-textured through an iterative mutual learning process. Extensive experiments conducted on three publicly available datasets with diverse texture patterns show that the proposed framework outperforms standard and state-of-the-art unsupervised techniques and performs competitively with supervised methods."}, "1906.05416": {"original_text": "  We introduce a novel method of generating synthetic question answering\ncorpora by combining models of question generation and answer extraction, and\nby filtering the results to ensure roundtrip consistency. By pretraining on the\nresulting corpora we obtain significant improvements on SQuAD2 and NQ,\nestablishing a new state-of-the-art on the latter. Our synthetic data\ngeneration models, for both question generation and answer extraction, can be\nfully reproduced by finetuning a publicly available BERT model on the\nextractive subsets of SQuAD2 and NQ. We also describe a more powerful variant\nthat does full sequence-to-sequence pretraining for question generation,\nobtaining exact match and F1 at less than 0.1% and 0.4% from human performance\non SQuAD2.\n", "rewritten_text": "We present a new approach to creating artificial question answering datasets by merging question generation and answer extraction models, and refining the outcomes for consistency. Training on these datasets leads to significant enhancements on SQuAD2 and NQ, achieving a new state-of-the-art on the latter. Our models for generating synthetic data, in both question generation and answer extraction, can be replicated by fine-tuning a publicly available BERT model on the extractive sections of SQuAD2 and NQ. Additionally, we outline a more advanced version that conducts full sequence-to-sequence pretraining for question generation, achieving exact match and F1 scores within 0.1% and 0.4% of human performance on SQuAD2."}, "2404.02393": {"original_text": "  While multilingual machine translation (MNMT) systems hold substantial\npromise, they also have security vulnerabilities. Our research highlights that\nMNMT systems can be susceptible to a particularly devious style of backdoor\nattack, whereby an attacker injects poisoned data into a low-resource language\npair to cause malicious translations in other languages, including\nhigh-resource languages. Our experimental results reveal that injecting less\nthan 0.01% poisoned data into a low-resource language pair can achieve an\naverage 20% attack success rate in attacking high-resource language pairs. This\ntype of attack is of particular concern, given the larger attack surface of\nlanguages inherent to low-resource settings. Our aim is to bring attention to\nthese vulnerabilities within MNMT systems with the hope of encouraging the\ncommunity to address security concerns in machine translation, especially in\nthe context of low-resource languages.\n", "rewritten_text": "\"While multilingual machine translation (MNMT) systems show great potential, they also have security weaknesses. Our study demonstrates that MNMT systems can be vulnerable to a sophisticated backdoor attack, where an attacker inserts tainted data into a language pair with limited resources to generate harmful translations in other languages, including those with abundant resources. Our experiments indicate that introducing less than 0.01% tainted data into a low-resource language pair can result in an average 20% success rate in attacking high-resource language pairs. This type of attack is particularly worrisome due to the broader range of languages at risk in low-resource environments. Our goal is to raise awareness about these vulnerabilities in MNMT systems in the hopes of prompting the community to address security issues in machine translation, especially in the context of languages with limited resources.\""}, "2104.00556": {"original_text": "  Two-view structure-from-motion (SfM) is the cornerstone of 3D reconstruction\nand visual SLAM. Existing deep learning-based approaches formulate the problem\nby either recovering absolute pose scales from two consecutive frames or\npredicting a depth map from a single image, both of which are ill-posed\nproblems. In contrast, we propose to revisit the problem of deep two-view SfM\nby leveraging the well-posedness of the classic pipeline. Our method consists\nof 1) an optical flow estimation network that predicts dense correspondences\nbetween two frames; 2) a normalized pose estimation module that computes\nrelative camera poses from the 2D optical flow correspondences, and 3) a\nscale-invariant depth estimation network that leverages epipolar geometry to\nreduce the search space, refine the dense correspondences, and estimate\nrelative depth maps. Extensive experiments show that our method outperforms all\nstate-of-the-art two-view SfM methods by a clear margin on KITTI depth, KITTI\nVO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth\nestimation.\n", "rewritten_text": "The foundation of 3D reconstruction and visual SLAM lies in the two-view structure-from-motion (SfM) technique. Current deep learning approaches tackle this problem by either determining absolute pose scales from two consecutive frames or generating a depth map from a single image, both of which are challenging due to their ill-posed nature. In contrast, our method proposes a new approach to deep two-view SfM by capitalizing on the well-posed characteristics of the traditional pipeline. Our method comprises: 1) a network for optical flow estimation that predicts detailed correspondences between two frames; 2) a module for normalized pose estimation that calculates relative camera poses from the 2D optical flow correspondences; and 3) a network for scale-invariant depth estimation that utilizes epipolar geometry to narrow down the search space, enhance the detailed correspondences, and estimate relative depth maps. Extensive experiments demonstrate that our method surpasses all existing two-view SfM methods by a significant margin in terms of relative pose and depth estimation on various datasets including KITTI depth, KITTI VO, MVS, Scenes11, and SUN3D."}, "2309.03903": {"original_text": "  Training data for video segmentation are expensive to annotate. This impedes\nextensions of end-to-end algorithms to new video segmentation tasks, especially\nin large-vocabulary settings. To 'track anything' without training on video\ndata for every individual task, we develop a decoupled video segmentation\napproach (DEVA), composed of task-specific image-level segmentation and\nclass/task-agnostic bi-directional temporal propagation. Due to this design, we\nonly need an image-level model for the target task (which is cheaper to train)\nand a universal temporal propagation model which is trained once and\ngeneralizes across tasks. To effectively combine these two modules, we use\nbi-directional propagation for (semi-)online fusion of segmentation hypotheses\nfrom different frames to generate a coherent segmentation. We show that this\ndecoupled formulation compares favorably to end-to-end approaches in several\ndata-scarce tasks including large-vocabulary video panoptic segmentation,\nopen-world video segmentation, referring video segmentation, and unsupervised\nvideo object segmentation. Code is available at:\nhttps://hkchengrex.github.io/Tracking-Anything-with-DEVA\n", "rewritten_text": "Annotating training data for video segmentation is costly, hindering the expansion of end-to-end algorithms to new video segmentation tasks, particularly in scenarios with a large vocabulary. To address this challenge and enable tracking without the need to train on video data for each specific task, we introduce a decoupled video segmentation approach called DEVA. DEVA consists of task-specific image-level segmentation and a class/task-agnostic bi-directional temporal propagation component. This design allows us to utilize a more affordable image-level model for the target task and a universal temporal propagation model that can be trained once and applied across various tasks. By employing bi-directional propagation for the fusion of segmentation hypotheses from different frames, we achieve a coherent segmentation output. Our decoupled approach demonstrates superior performance compared to end-to-end methods in data-scarce tasks such as large-vocabulary video panoptic segmentation, open-world video segmentation, referring video segmentation, and unsupervised video object segmentation. The code for DEVA is accessible at: https://hkchengrex.github.io/Tracking-Anything-with-DEVA"}, "2312.08869": {"original_text": "  We are living in a world surrounded by diverse and \"smart\" devices with rich\nmodalities of sensing ability. Conveniently capturing the interactions between\nus humans and these objects remains far-reaching. In this paper, we present\nI'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the\nhuman and object in a novel setting: using a minimal amount of RGB camera and\nobject-mounted Inertial Measurement Unit (IMU). It combines general motion\ninference and category-aware refinement. For the former, we introduce a\nholistic human-object tracking method to fuse the IMU signals and the RGB\nstream and progressively recover the human motions and subsequently the\ncompanion object motions. For the latter, we tailor a category-aware motion\ndiffusion model, which is conditioned on both the raw IMU observations and the\nresults from the previous stage under over-parameterization representation. It\nsignificantly refines the initial results and generates vivid body, hand, and\nobject motions. Moreover, we contribute a large dataset with ground truth human\nand object motions, dense RGB inputs, and rich object-mounted IMU measurements.\nExtensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid\ncapture setting. Our dataset and code will be released to the community.\n", "rewritten_text": "\"We currently inhabit a world filled with a variety of advanced devices equipped with sophisticated sensing capabilities. However, effectively capturing the interactions between humans and these devices remains a challenge. This paper introduces I'm-HOI, a monocular system designed to accurately record the 3D movements of both humans and objects using a minimal setup consisting of an RGB camera and an Inertial Measurement Unit (IMU) attached to the object. The system combines general motion estimation with category-specific refinement techniques. The holistic human-object tracking method integrates IMU data and RGB video to track human and object movements. The category-aware motion diffusion model refines the results by considering both raw IMU data and previous stage outputs. This model enhances the accuracy of body, hand, and object movements. Additionally, a comprehensive dataset containing ground truth human and object motions, detailed RGB inputs, and IMU measurements is provided. Extensive experiments demonstrate the effectiveness of I'm-HOI in a mixed capture environment. The dataset and code will be made available to the research community.\""}, "1810.04864": {"original_text": "  We present a comparison of word-based and character-based\nsequence-to-sequence models for data-to-text natural language generation, which\ngenerate natural language descriptions for structured inputs. On the datasets\nof two recent generation challenges, our models achieve comparable or better\nautomatic evaluation results than the best challenge submissions. Subsequent\ndetailed statistical and human analyses shed light on the differences between\nthe two input representations and the diversity of the generated texts. In a\ncontrolled experiment with synthetic training data generated from templates, we\ndemonstrate the ability of neural models to learn novel combinations of the\ntemplates and thereby generalize beyond the linguistic structures they were\ntrained on.\n", "rewritten_text": "We compare word-based and character-based sequence-to-sequence models for generating natural language descriptions from structured inputs. Our models perform as well as or better than top submissions in recent challenges, based on automatic evaluation. Further analysis reveals insights into the differences between the input representations and the variety of generated texts. Through a controlled experiment using synthetic training data, we show that neural models can learn new combinations of templates and generalize beyond their original training."}, "2405.16116": {"original_text": "  Scene Graph Generation (SGG) can extract abstract semantic relations between\nentities in images as graph representations. This task holds strong promises\nfor other downstream tasks such as the embodied cognition of an autonomous\nagent. However, to power such applications, SGG needs to solve the gap of\nreal-time latency. In this work, we propose to investigate the bottlenecks of\ncurrent approaches for real-time constraint applications. Then, we propose a\nsimple yet effective implementation of a real-time SGG approach using YOLOV8 as\nan object detection backbone. Our implementation is the first to obtain more\nthan 48 FPS for the task with no loss of accuracy, successfully outperforming\nany other lightweight approaches. Our code is freely available at\nhttps://github.com/Maelic/SGG-Benchmark.\n", "rewritten_text": "The generation of Scene Graphs (SGG) involves extracting abstract semantic relationships between entities in images in the form of graph representations. This capability shows great potential for various applications, such as enhancing the cognitive abilities of autonomous agents. However, in order to support such applications, SGG must address the challenge of real-time processing speed. In this study, we aim to identify the limitations of current methods for real-time applications and propose a straightforward yet efficient real-time SGG approach using YOLOV8 as the object detection framework. Our implementation achieves a processing speed of over 48 frames per second without sacrificing accuracy, surpassing other lightweight approaches. The code for our implementation is openly accessible at https://github.com/Maelic/SGG-Benchmark."}, "2012.08549": {"original_text": "  Voice Assistants such as Alexa, Siri, and Google Assistant typically use a\ntwo-stage Spoken Language Understanding pipeline; first, an Automatic Speech\nRecognition (ASR) component to process customer speech and generate text\ntranscriptions, followed by a Natural Language Understanding (NLU) component to\nmap transcriptions to an actionable hypothesis. An end-to-end (E2E) system that\ngoes directly from speech to a hypothesis is a more attractive option. These\nsystems were shown to be smaller, faster, and better optimized. However, they\nrequire massive amounts of end-to-end training data and in addition, don't take\nadvantage of the already available ASR and NLU training data.\n  In this work, we propose an E2E system that is designed to jointly train on\nmultiple speech-to-text tasks, such as ASR (speech-transcription) and SLU\n(speech-hypothesis), and text-to-text tasks, such as NLU (text-hypothesis). We\ncall this the Audio-Text All-Task (AT-AT) Model and we show that it beats the\nperformance of E2E models trained on individual tasks, especially ones trained\non limited data. We show this result on an internal music dataset and two\npublic datasets, FluentSpeech and SNIPS Audio, where we achieve\nstate-of-the-art results. Since our model can process both speech and text\ninput sequences and learn to predict a target sequence, it also allows us to do\nzero-shot E2E SLU by training on only text-hypothesis data (without any speech)\nfrom a new domain. We evaluate this ability of our model on the Facebook TOP\ndataset and set a new benchmark for zeroshot E2E performance. We will soon\nrelease the audio data collected for the TOP dataset for future research.\n", "rewritten_text": "Voice assistants like Alexa, Siri, and Google Assistant typically follow a two-stage process for understanding spoken language. First, they use Automatic Speech Recognition (ASR) to convert customer speech into text transcriptions. Then, a Natural Language Understanding (NLU) component interprets the transcriptions to form an actionable hypothesis. While an end-to-end (E2E) system that directly translates speech into a hypothesis is more efficient, it requires extensive training data and does not leverage existing ASR and NLU data. \n\nIn this study, we introduce an E2E system that simultaneously trains on various speech-to-text tasks (ASR and SLU) and text-to-text tasks (NLU). Known as the Audio-Text All-Task (AT-AT) Model, our approach outperforms E2E models trained on individual tasks, particularly those with limited data. We demonstrate this on internal and public datasets, achieving state-of-the-art results on FluentSpeech and SNIPS Audio. Our model's ability to process both speech and text inputs enables zero-shot E2E SLU by training solely on text-hypothesis data from a new domain. We showcase this capability on the Facebook TOP dataset, setting a new benchmark for zero-shot E2E performance. The audio data collected for the TOP dataset will be made available for future research endeavors."}, "2207.09059": {"original_text": "  Few-shot open-set recognition aims to classify both seen and novel images\ngiven only limited training data of seen classes. The challenge of this task is\nthat the model is required not only to learn a discriminative classifier to\nclassify the pre-defined classes with few training data but also to reject\ninputs from unseen classes that never appear at training time. In this paper,\nwe propose to solve the problem from two novel aspects. First, instead of\nlearning the decision boundaries between seen classes, as is done in standard\nclose-set classification, we reserve space for unseen classes, such that images\nlocated in these areas are recognized as the unseen classes. Second, to\neffectively learn such decision boundaries, we propose to utilize the\nbackground features from seen classes. As these background regions do not\nsignificantly contribute to the decision of close-set classification, it is\nnatural to use them as the pseudo unseen classes for classifier learning. Our\nextensive experiments show that our proposed method not only outperforms\nmultiple baselines but also sets new state-of-the-art results on three popular\nbenchmarks, namely tieredImageNet, miniImageNet, and Caltech-USCD\nBirds-200-2011 (CUB).\n", "rewritten_text": "The goal of few-shot open-set recognition is to classify both familiar and new images with limited training data for familiar categories. This task presents a challenge as the model must not only learn to classify known categories with minimal training data, but also to distinguish and reject images from unfamiliar categories that were not seen during training. In this study, we introduce two innovative approaches to address this issue. Firstly, instead of focusing solely on defining boundaries between known categories as in traditional closed-set classification, we allocate space for unfamiliar categories so that images falling within these regions are identified as such. Secondly, to effectively define these boundaries, we propose utilizing background features from known categories. Since these background regions do not significantly impact closed-set classification decisions, they can be used as pseudo unfamiliar classes for training the classifier. Our extensive experiments demonstrate that our method surpasses several baseline approaches and achieves state-of-the-art performance on three widely-used benchmarks: tieredImageNet, miniImageNet, and Caltech-USCD Birds-200-2011 (CUB)."}, "1710.04943": {"original_text": "  In this paper, we report on our efforts for using Deep Learning for\nclassifying artifacts and their features in digital visuals as a part of the\nNeoclassica framework. It was conceived to provide scholars with new methods\nfor analyzing and classifying artifacts and aesthetic forms from the era of\nClassicism. The framework accommodates both traditional knowledge\nrepresentation as a formal ontology and data-driven knowledge discovery, where\ncultural patterns will be identified by means of algorithms in statistical\nanalysis and machine learning. We created a Deep Learning approach trained on\nphotographs to classify the objects inside these photographs. In a next step,\nwe will apply a different Deep Learning approach. It is capable of locating\nmultiple objects inside an image and classifying them with a high accuracy.\n", "rewritten_text": "In this paper, we discuss our use of Deep Learning to classify artifacts and their features in digital images within the Neoclassica framework. This framework aims to offer scholars new methods for analyzing and categorizing artifacts and aesthetic forms from the Classical era. It combines traditional knowledge representation with data-driven knowledge discovery, utilizing algorithms for statistical analysis and machine learning to identify cultural patterns. We have developed a Deep Learning model trained on photographs to classify objects within them. Next, we plan to implement a different Deep Learning approach that can accurately locate and classify multiple objects within an image."}, "2111.07239": {"original_text": "  Object detection has achieved promising performance on clean datasets, but\nhow to achieve better tradeoff between the adversarial robustness and clean\nprecision is still under-explored. Adversarial training is the mainstream\nmethod to improve robustness, but most of the works will sacrifice clean\nprecision to gain robustness than standard training. In this paper, we propose\nUnified Decoupled Feature Alignment (UDFA), a novel fine-tuning paradigm which\nachieves better performance than existing methods, by fully exploring the\ncombination between self-knowledge distillation and adversarial training for\nobject detection. We first use decoupled fore/back-ground features to construct\nself-knowledge distillation branch between clean feature representation from\npretrained detector (served as teacher) and adversarial feature representation\nfrom student detector. Then we explore the self-knowledge distillation from a\nnew angle by decoupling original branch into a self-supervised learning branch\nand a new self-knowledge distillation branch. With extensive experiments on the\nPASCAL-VOC and MS-COCO benchmarks, the evaluation results show that UDFA can\nsurpass the standard training and state-of-the-art adversarial training methods\nfor object detection. For example, compared with teacher detector, our approach\non GFLV2 with ResNet-50 improves clean precision by 2.2 AP on PASCAL-VOC;\ncompared with SOTA adversarial training methods, our approach improves clean\nprecision by 1.6 AP, while improving adversarial robustness by 0.5 AP. Our code\nwill be available at https://github.com/grispeut/udfa.\n", "rewritten_text": "Object detection has shown promising performance on clean datasets, but there is still a need to explore how to achieve a better balance between adversarial robustness and clean precision. While adversarial training is commonly used to enhance robustness, many existing methods tend to sacrifice clean precision in favor of robustness compared to standard training. This paper introduces Unified Decoupled Feature Alignment (UDFA), a novel fine-tuning approach that outperforms existing methods by leveraging a combination of self-knowledge distillation and adversarial training for object detection. The proposed method utilizes decoupled foreground and background features to establish a self-knowledge distillation branch between clean feature representations from a pretrained detector (acting as the teacher) and adversarial feature representations from the student detector. Additionally, a new perspective on self-knowledge distillation is explored by splitting the original branch into a self-supervised learning branch and a new self-knowledge distillation branch. Extensive experiments conducted on the PASCAL-VOC and MS-COCO benchmarks demonstrate that UDFA surpasses both standard training and state-of-the-art adversarial training methods for object detection. For instance, when applied to GFLV2 with ResNet-50, our approach improves clean precision by 2.2 average precision (AP) on PASCAL-VOC compared to the teacher detector, and by 1.6 AP compared to current adversarial training methods, while enhancing adversarial robustness by 0.5 AP. The code for our method will be made available at https://github.com/grispeut/udfa."}, "2304.05694": {"original_text": "  Self-attention modules have demonstrated remarkable capabilities in capturing\nlong-range relationships and improving the performance of point cloud tasks.\nHowever, point cloud objects are typically characterized by complex,\ndisordered, and non-Euclidean spatial structures with multiple scales, and\ntheir behavior is often dynamic and unpredictable. The current self-attention\nmodules mostly rely on dot product multiplication and dimension alignment among\nquery-key-value features, which cannot adequately capture the multi-scale\nnon-Euclidean structures of point cloud objects. To address these problems,\nthis paper proposes a self-attention plug-in module with its variants,\nMulti-scale Geometry-aware Transformer (MGT). MGT processes point cloud data\nwith multi-scale local and global geometric information in the following three\naspects. At first, the MGT divides point cloud data into patches with multiple\nscales. Secondly, a local feature extractor based on sphere mapping is proposed\nto explore the geometry inner each patch and generate a fixed-length\nrepresentation for each patch. Thirdly, the fixed-length representations are\nfed into a novel geodesic-based self-attention to capture the global\nnon-Euclidean geometry between patches. Finally, all the modules are integrated\ninto the framework of MGT with an end-to-end training scheme. Experimental\nresults demonstrate that the MGT vastly increases the capability of capturing\nmulti-scale geometry using the self-attention mechanism and achieves strong\ncompetitive performance on mainstream point cloud benchmarks.\n", "rewritten_text": "Self-attention modules have shown impressive abilities in capturing long-range connections and enhancing the performance of tasks involving point clouds. However, point cloud objects typically exhibit intricate, disordered, and non-Euclidean spatial structures with various scales, and their behavior can be dynamic and unpredictable. Existing self-attention modules primarily rely on dot product multiplication and dimension alignment of query-key-value features, which may not effectively capture the multi-scale non-Euclidean structures of point cloud objects. To tackle these challenges, this study introduces a self-attention plug-in module and its variations, known as Multi-scale Geometry-aware Transformer (MGT). MGT processes point cloud data by incorporating multi-scale local and global geometric information in three key ways. Firstly, MGT divides point cloud data into patches with different scales. Secondly, it proposes a local feature extractor based on sphere mapping to analyze the geometry within each patch and generate a fixed-length representation for each patch. Thirdly, the fixed-length representations are inputted into a novel geodesic-based self-attention mechanism to capture the global non-Euclidean geometry between patches. These modules are then integrated into the MGT framework with an end-to-end training approach. Experimental findings demonstrate that MGT significantly enhances the ability to capture multi-scale geometry using the self-attention mechanism and achieves strong competitive performance on popular point cloud benchmarks."}, "2108.07708": {"original_text": "  Can language models learn grounded representations from text distribution\nalone? This question is both central and recurrent in natural language\nprocessing; authors generally agree that grounding requires more than textual\ndistribution. We propose to experimentally test this claim: if any two words\nhave different meanings and yet cannot be distinguished from distribution\nalone, then grounding is out of the reach of text-based models. To that end, we\npresent early work on an online game for the collection of human judgments on\nthe distributional similarity of word pairs in five languages. We further\nreport early results of our data collection campaign.\n", "rewritten_text": "\"Is it possible for language models to acquire meaningful representations solely from the distribution of text? This question is fundamental and frequently discussed in the field of natural language processing. Most authors agree that true grounding requires more than just textual patterns. Our proposal aims to investigate this assertion through experiments: if two words have distinct meanings but cannot be differentiated based on their distribution alone, then text-based models fall short of achieving grounding. To explore this, we introduce a preliminary study involving an online game to gather human judgments on the distributional similarity of word pairs in five languages. Additionally, we share initial findings from our data collection efforts.\""}, "2302.03194": {"original_text": "  We propose two methods to make unsupervised domain adaptation (UDA) more\nparameter efficient using adapters, small bottleneck layers interspersed with\nevery layer of the large-scale pre-trained language model (PLM). The first\nmethod deconstructs UDA into a two-step process: first by adding a domain\nadapter to learn domain-invariant information and then by adding a task adapter\nthat uses domain-invariant information to learn task representations in the\nsource domain. The second method jointly learns a supervised classifier while\nreducing the divergence measure. Compared to strong baselines, our simple\nmethods perform well in natural language inference (MNLI) and the cross-domain\nsentiment classification task. We even outperform unsupervised domain\nadaptation methods such as DANN and DSN in sentiment classification, and we are\nwithin 0.85% F1 for natural language inference task, by fine-tuning only a\nfraction of the full model parameters. We release our code at\nhttps://github.com/declare-lab/domadapter\n", "rewritten_text": "We present two approaches to enhance the efficiency of unsupervised domain adaptation (UDA) by incorporating adapters, which are small bottleneck layers inserted within each layer of a large-scale pre-trained language model (PLM). The first approach breaks down UDA into a two-step process: initially adding a domain adapter to capture domain-invariant information, followed by incorporating a task adapter that utilizes this domain-invariant information to learn task representations in the source domain. The second approach involves simultaneously training a supervised classifier while minimizing the divergence measure. Our straightforward methods demonstrate strong performance in tasks such as natural language inference (MNLI) and cross-domain sentiment classification when compared to robust baselines. In sentiment classification, we surpass unsupervised domain adaptation techniques like DANN and DSN, and in the natural language inference task, we achieve results within 0.85% F1 score by fine-tuning only a portion of the full model parameters. The code for our methods is available at https://github.com/declare-lab/domadapter."}, "2103.04059": {"original_text": "  Few-shot class incremental learning (FSCIL) portrays the problem of learning\nnew concepts gradually, where only a few examples per concept are available to\nthe learner. Due to the limited number of examples for training, the techniques\ndeveloped for standard incremental learning cannot be applied verbatim to\nFSCIL. In this work, we introduce a distillation algorithm to address the\nproblem of FSCIL and propose to make use of semantic information during\ntraining. To this end, we make use of word embeddings as semantic information\nwhich is cheap to obtain and which facilitate the distillation process.\nFurthermore, we propose a method based on an attention mechanism on multiple\nparallel embeddings of visual data to align visual and semantic vectors, which\nreduces issues related to catastrophic forgetting. Via experiments on\nMiniImageNet, CUB200, and CIFAR100 dataset, we establish new state-of-the-art\nresults by outperforming existing approaches.\n", "rewritten_text": "The concept of Few-shot class incremental learning (FSCIL) involves gradually learning new concepts with limited examples per concept. Traditional incremental learning techniques are not directly applicable to FSCIL due to the scarcity of training examples. In this study, we present a distillation algorithm to tackle the challenges of FSCIL by incorporating semantic information into the training process. We utilize word embeddings as a cost-effective way to enhance the distillation process. Additionally, we propose an attention-based method that aligns visual and semantic vectors using multiple parallel embeddings of visual data, mitigating issues related to catastrophic forgetting. Through experiments on MiniImageNet, CUB200, and CIFAR100 datasets, we demonstrate superior performance compared to existing methods, establishing new state-of-the-art results."}, "1710.07395": {"original_text": "  In the wake of a polarizing election, the cyber world is laden with hate\nspeech. Context accompanying a hate speech text is useful for identifying hate\nspeech, which however has been largely overlooked in existing datasets and hate\nspeech detection models. In this paper, we provide an annotated corpus of hate\nspeech with context information well kept. Then we propose two types of hate\nspeech detection models that incorporate context information, a logistic\nregression model with context features and a neural network model with learning\ncomponents for context. Our evaluation shows that both models outperform a\nstrong baseline by around 3% to 4% in F1 score and combining these two models\nfurther improve the performance by another 7% in F1 score.\n", "rewritten_text": "Following a divisive election, the online sphere is rife with hateful rhetoric. While context is crucial for identifying hate speech, it has been largely overlooked in current datasets and detection models. This study presents a hate speech corpus with detailed context information. Two detection models are proposed: a logistic regression model with context features and a neural network model with context-aware learning components. Evaluation demonstrates that both models surpass a strong baseline by 3% to 4% in F1 score, and combining them further enhances performance by an additional 7% in F1 score."}, "2401.17597": {"original_text": "  Multi-turn dialogues are characterized by their extended length and the\npresence of turn-taking conversations. Traditional language models often\noverlook the distinct features of these dialogues by treating them as regular\ntext. In this paper, we propose a speaker-enhanced pre-training method for long\ndialogue summarization, which leverages the inherent structure of multiple-turn\ndialogues. To support our study, we curate a diverse dataset that includes\ntranscripts from real-world scenarios, movie or TV show transcripts, and\ndialogues generated by a Large Language Model. We then perform a pre-training,\nwhich encompasses the detection of speaker changes, and masked utterance\ngeneration. Experimental results of fine-tuned models demonstrate that our\nmodel achieves state-of-the-art performance on downstream benchmarks with long\ncontext, surpassing baseline models and highlighting the effectiveness of our\napproach. Our findings highlight the importance of curating pre-training\ndatasets that exhibit diversity and variations in length distribution to ensure\neffective alignment with downstream datasets.\n", "rewritten_text": "In this paper, we introduce a method for enhancing speaker pre-training in order to summarize long dialogues more effectively. Multi-turn dialogues, characterized by their extended length and turn-taking conversations, are often overlooked by traditional language models. Our approach leverages the structure of multiple-turn dialogues and involves curating a diverse dataset containing real-world transcripts, movie or TV show dialogues, and dialogues generated by a Large Language Model. The pre-training process includes detecting speaker changes and generating masked utterances. Experimental results show that our model outperforms baseline models on downstream benchmarks with long context, emphasizing the importance of diverse pre-training datasets for effective alignment with downstream datasets."}, "2309.01036": {"original_text": "  Spatial transcriptomics is an emerging technology that aligns histopathology\nimages with spatially resolved gene expression profiling. It holds the\npotential for understanding many diseases but faces significant bottlenecks\nsuch as specialized equipment and domain expertise. In this work, we present\nSEPAL, a new model for predicting genetic profiles from visual tissue\nappearance. Our method exploits the biological biases of the problem by\ndirectly supervising relative differences with respect to mean expression, and\nleverages local visual context at every coordinate to make predictions using a\ngraph neural network. This approach closes the gap between complete locality\nand complete globality in current methods. In addition, we propose a novel\nbenchmark that aims to better define the task by following current best\npractices in transcriptomics and restricting the prediction variables to only\nthose with clear spatial patterns. Our extensive evaluation in two different\nhuman breast cancer datasets indicates that SEPAL outperforms previous\nstate-of-the-art methods and other mechanisms of including spatial context.\n", "rewritten_text": "Spatial transcriptomics is a cutting-edge technology that combines histopathology images with gene expression profiling based on location. While it has the potential to enhance our understanding of various diseases, it faces challenges such as the need for specialized equipment and expertise in the field. In this study, we introduce SEPAL, a novel model designed to predict genetic profiles from visual tissue characteristics. Our approach capitalizes on the inherent biological biases of the problem by directly comparing relative differences in gene expression to the mean, and utilizes local visual information at each point to make predictions using a graph neural network. This method bridges the gap between purely local and global approaches seen in current techniques. Furthermore, we introduce a new benchmark that aims to refine the task by adhering to best practices in transcriptomics and focusing on prediction variables with clear spatial patterns. Our thorough evaluation using two distinct human breast cancer datasets demonstrates that SEPAL surpasses previous state-of-the-art methods and other strategies for incorporating spatial context."}, "2011.0212": {"original_text": "  Diabetic retinopathy (DR) is one of the leading causes of blindness. However,\nno specific symptoms of early DR lead to a delayed diagnosis, which results in\ndisease progression in patients. To determine the disease severity levels,\nophthalmologists need to focus on the discriminative parts of the fundus\nimages. In recent years, deep learning has achieved great success in medical\nimage analysis. However, most works directly employ algorithms based on\nconvolutional neural networks (CNNs), which ignore the fact that the difference\namong classes is subtle and gradual. Hence, we consider automatic image grading\nof DR as a fine-grained classification task, and construct a bilinear model to\nidentify the pathologically discriminative areas. In order to leverage the\nordinal information among classes, we use an ordinal regression method to\nobtain the soft labels. In addition, other than only using a categorical loss\nto train our network, we also introduce the metric loss to learn a more\ndiscriminative feature space. Experimental results demonstrate the superior\nperformance of the proposed method on two public IDRiD and DeepDR datasets.\n", "rewritten_text": "\"Diabetic retinopathy (DR) is a leading cause of blindness, with early stages often lacking specific symptoms that can lead to delayed diagnosis and disease progression. Ophthalmologists rely on analyzing fundus images to determine disease severity levels. While deep learning has shown promise in medical image analysis, many existing approaches utilize convolutional neural networks (CNNs) that may overlook subtle differences between disease classes. To address this, we approach automatic image grading of DR as a fine-grained classification task and develop a bilinear model to identify key pathological features. We incorporate ordinal regression to capture the ordinal information among classes and introduce a metric loss to enhance feature discrimination during network training. Our experimental results, conducted on the IDRiD and DeepDR datasets, demonstrate the superior performance of our proposed method.\""}, "1905.02462": {"original_text": "  Recently, image super-resolution has been widely studied and achieved\nsignificant progress by leveraging the power of deep convolutional neural\nnetworks. However, there has been limited advancement in video super-resolution\n(VSR) due to the complex temporal patterns in videos. In this paper, we\ninvestigate how to adapt state-of-the-art methods of image super-resolution for\nvideo super-resolution. The proposed adapting method is straightforward. The\ninformation among successive frames is well exploited, while the overhead on\nthe original image super-resolution method is negligible. Furthermore, we\npropose a learning-based method to ensemble the outputs from multiple\nsuper-resolution models. Our methods show superior performance and rank second\nin the NTIRE2019 Video Super-Resolution Challenge Track 1.\n", "rewritten_text": "\"Recently, there has been extensive research and notable progress in the field of image super-resolution through the utilization of deep convolutional neural networks. However, advancements in video super-resolution (VSR) have been limited due to the intricate temporal patterns present in videos. This study explores the adaptation of cutting-edge image super-resolution techniques for video super-resolution. The proposed adaptation method is simple yet effective, effectively leveraging information across consecutive frames while maintaining minimal additional computational overhead. Additionally, a learning-based approach is introduced to combine outputs from multiple super-resolution models. Our methods demonstrate superior performance, achieving the second position in the NTIRE2019 Video Super-Resolution Challenge Track 1.\""}, "2312.14157": {"original_text": "  3D hand tracking from a monocular video is a very challenging problem due to\nhand interactions, occlusions, left-right hand ambiguity, and fast motion. Most\nexisting methods rely on RGB inputs, which have severe limitations under\nlow-light conditions and suffer from motion blur. In contrast, event cameras\ncapture local brightness changes instead of full image frames and do not suffer\nfrom the described effects. Unfortunately, existing image-based techniques\ncannot be directly applied to events due to significant differences in the data\nmodalities. In response to these challenges, this paper introduces the first\nframework for 3D tracking of two fast-moving and interacting hands from a\nsingle monocular event camera. Our approach tackles the left-right hand\nambiguity with a novel semi-supervised feature-wise attention mechanism and\nintegrates an intersection loss to fix hand collisions. To facilitate advances\nin this research domain, we release a new synthetic large-scale dataset of two\ninteracting hands, Ev2Hands-S, and a new real benchmark with real event streams\nand ground-truth 3D annotations, Ev2Hands-R. Our approach outperforms existing\nmethods in terms of the 3D reconstruction accuracy and generalises to real data\nunder severe light conditions.\n", "rewritten_text": "Tracking hand movements in 3D using a single monocular video presents significant challenges such as hand interactions, occlusions, left-right hand confusion, and rapid motion. Current methods primarily rely on RGB inputs, which are limited in low-light situations and susceptible to motion blur. In contrast, event cameras capture local brightness changes rather than full image frames, avoiding these issues. However, existing image-based techniques are not directly applicable to event data due to substantial differences in data types. This paper introduces a novel framework for tracking the 3D movements of two fast-moving and interacting hands using a single monocular event camera. Our approach addresses left-right hand confusion with a unique semi-supervised feature-wise attention mechanism and incorporates an intersection loss to resolve hand collisions. To advance research in this field, we introduce a new synthetic dataset, Ev2Hands-S, and a real benchmark dataset, Ev2Hands-R, containing real event streams and accurate 3D annotations. Our method surpasses existing techniques in terms of 3D reconstruction accuracy and performs well in challenging lighting conditions with real data."}, "2010.08243": {"original_text": "  3D object detectors based only on LiDAR point clouds hold the\nstate-of-the-art on modern street-view benchmarks. However, LiDAR-based\ndetectors poorly generalize across domains due to domain shift. In the case of\nLiDAR, in fact, domain shift is not only due to changes in the environment and\nin the object appearances, as for visual data from RGB cameras, but is also\nrelated to the geometry of the point clouds (e.g., point density variations).\nThis paper proposes SF-UDA$^{3D}$, the first Source-Free Unsupervised Domain\nAdaptation (SF-UDA) framework to domain-adapt the state-of-the-art PointRCNN 3D\ndetector to target domains for which we have no annotations (unsupervised),\nneither we hold images nor annotations of the source domain (source-free).\nSF-UDA$^{3D}$ is novel on both aspects. Our approach is based on\npseudo-annotations, reversible scale-transformations and motion coherency.\nSF-UDA$^{3D}$ outperforms both previous domain adaptation techniques based on\nfeatures alignment and state-of-the-art 3D object detection methods which\nadditionally use few-shot target annotations or target annotation statistics.\nThis is demonstrated by extensive experiments on two large-scale datasets,\ni.e., KITTI and nuScenes.\n", "rewritten_text": "\"Current top-performing 3D object detectors rely solely on LiDAR point clouds for street-view benchmarks. However, these LiDAR-based detectors struggle to generalize effectively across different domains due to domain shift. Unlike visual data from RGB cameras, domain shift in LiDAR is not only influenced by changes in the environment and object appearances, but also by variations in point cloud geometry, such as point density.\n\nThis study introduces SF-UDA$^{3D}$, the first Source-Free Unsupervised Domain Adaptation (SF-UDA) framework designed to adapt the leading PointRCNN 3D detector to target domains lacking annotations (unsupervised) and without access to images or annotations from the source domain (source-free). SF-UDA$^{3D}$ is innovative in both respects, utilizing pseudo-annotations, reversible scale-transformations, and motion coherency.\n\nOur approach surpasses previous domain adaptation techniques based on feature alignment and state-of-the-art 3D object detection methods that rely on few-shot target annotations or target annotation statistics. Extensive experiments on the KITTI and nuScenes datasets demonstrate the effectiveness of SF-UDA$^{3D}$.\""}, "2305.09407": {"original_text": "  Visual quality inspection in high performance manufacturing can benefit from\nautomation, due to cost savings and improved rigor. Deep learning techniques\nare the current state of the art for generic computer vision tasks like\nclassification and object detection. Manufacturing data can pose a challenge\nfor deep learning because data is highly repetitive and there are few images of\ndefects or deviations to learn from. Deep learning models trained with such\ndata can be fragile and sensitive to context, and can under-detect new defects\nnot found in the training data. In this work, we explore training defect\ndetection models to learn specific defects out of context, so that they are\nmore likely to be detected in new situations. We demonstrate how models trained\non diverse images containing a common defect type can pick defects out in new\ncircumstances. Such generic models could be more robust to new defects not\nfound data collected for training, and can reduce data collection impediments\nto implementing visual inspection on production lines. Additionally, we\ndemonstrate that object detection models trained to predict a label and\nbounding box outperform classifiers that predict a label only on held out test\ndata typical of manufacturing inspection tasks. Finally, we studied the factors\nthat affect generalization in order to train models that work under a wider\nrange of conditions.\n", "rewritten_text": "Automating visual quality inspection in high-performance manufacturing can lead to cost savings and improved accuracy. Deep learning techniques, such as classification and object detection, are currently the most advanced methods for computer vision tasks. However, applying deep learning to manufacturing data can be challenging due to the repetitive nature of the data and the limited number of images showing defects or deviations. Models trained on such data may struggle to detect new defects not present in the training set. This study focuses on training defect detection models to recognize specific defects regardless of context, increasing the likelihood of detecting them in new scenarios. By training models on diverse images featuring a common defect type, they can better identify defects in unfamiliar situations. These generic models are more resilient to new defects not seen during training, reducing the need for extensive data collection for visual inspection on production lines. Furthermore, the study shows that object detection models, which predict both a label and a bounding box, outperform classifiers that only predict a label on test data typical of manufacturing inspection tasks. Finally, the research investigates factors influencing generalization to develop models that can perform effectively under various conditions."}, "2406.09858": {"original_text": "  The visual quality of an image is confounded by a number of intertwined\nfactors including its semantic content, distortion characteristics and\nappearance properties such as brightness, contrast, sharpness, and\ncolourfulness. Distilling high level knowledge about all these quality bearing\nattributes is crucial for developing objective Image Quality Assessment\n(IQA).While existing solutions have modeled some of these aspects, a\ncomprehensive solution that involves all these important quality related\nattributes has not yet been developed. In this paper, we present a new blind\nIQA (BIQA) model termed Self-supervision and Vision-Language supervision Image\nQUality Evaluator (SLIQUE) that features a joint vision-language and visual\ncontrastive representation learning framework for acquiring high level\nknowledge about the images semantic contents, distortion characteristics and\nappearance properties for IQA. For training SLIQUE, we have developed a\nsystematic approach to constructing a first of its kind large image database\nannotated with all three categories of quality relevant texts. The Text\nAnnotated Distortion, Appearance and Content (TADAC) database has over 1.6\nmillion images annotated with textual descriptions of their semantic contents,\ndistortion characteristics and appearance properties. The method for\nconstructing TADAC and the database itself will be particularly useful for\nexploiting vision-language modeling for advanced IQA applications. Extensive\nexperimental results show that SLIQUE has superior performances over state of\nthe art, demonstrating the soundness of its design principle and the\neffectiveness of its implementation.\n", "rewritten_text": "The quality of an image is influenced by various factors such as its semantic content, distortion characteristics, and appearance properties like brightness, contrast, sharpness, and color. Understanding these attributes is essential for developing objective Image Quality Assessment (IQA). While some existing solutions address certain aspects, a comprehensive solution incorporating all key quality-related attributes is lacking. This paper introduces a new blind IQA model called Self-supervision and Vision-Language supervision Image Quality Evaluator (SLIQUE), which utilizes a joint vision-language and visual contrastive representation learning framework to gain high-level knowledge about an image's semantic content, distortion characteristics, and appearance properties for IQA. To train SLIQUE, a systematic approach was used to create a unique large image database called Text Annotated Distortion, Appearance, and Content (TADAC), containing over 1.6 million images annotated with textual descriptions of their semantic content, distortion characteristics, and appearance properties. The construction method and database of TADAC are valuable for leveraging vision-language modeling in advanced IQA applications. Extensive experimental results demonstrate that SLIQUE outperforms current methods, showcasing the effectiveness of its design and implementation."}, "2406.16860": {"original_text": "  We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, addressing the difficulties involved in consolidating and\ninterpreting results from various tasks, and introduce a new vision-centric\nbenchmark, CV-Bench. To further improve visual grounding, we propose the\nSpatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that\nintegrates high-resolution vision features with LLMs while reducing the number\nof tokens. Additionally, we discuss the curation of high-quality visual\ninstruction-tuning data from publicly available sources, emphasizing the\nimportance of data source balancing and distribution ratio. Collectively,\nCambrian-1 not only achieves state-of-the-art performance but also serves as a\ncomprehensive, open cookbook for instruction-tuned MLLMs. We provide model\nweights, code, supporting tools, datasets, and detailed instruction-tuning and\nevaluation recipes. We hope our release will inspire and accelerate\nadvancements in multimodal systems and visual representation learning.\n", "rewritten_text": "We present Cambrian-1, a series of multimodal Large Language Models (LLMs) developed with a focus on vision. While more powerful language models can improve multimodal capabilities, the design decisions for vision components are often not thoroughly explored and are disconnected from visual representation learning research. This gap impedes accurate sensory grounding in real-world situations. Our research utilizes LLMs and visual instruction tuning to assess different visual representations, providing new insights into various models and architectures - whether self-supervised, strongly supervised, or a combination of both. Through experiments involving over 20 vision encoders, we evaluate existing MLLM benchmarks, highlighting the challenges in consolidating and interpreting results from different tasks. We introduce a new vision-centric benchmark, CV-Bench, and propose the Spatial Vision Aggregator (SVA) to enhance visual grounding by integrating high-resolution vision features with LLMs while reducing token count. We also discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of balancing data sources and distribution ratios. Cambrian-1 not only achieves cutting-edge performance but also acts as a comprehensive resource for instruction-tuned MLLMs. We offer model weights, code, tools, datasets, and detailed instruction-tuning and evaluation guidelines, aiming to inspire and expedite progress in multimodal systems and visual representation learning."}, "2403.09593": {"original_text": "  Names are essential to both human cognition and vision-language models.\nOpen-vocabulary models utilize class names as text prompts to generalize to\ncategories unseen during training. However, the precision of these names is\noften overlooked in existing datasets. In this paper, we address this\nunderexplored problem by presenting a framework for \"renovating\" names in\nopen-vocabulary segmentation benchmarks (RENOVATE). Our framework features a\nrenaming model that enhances the quality of names for each visual segment.\nThrough experiments, we demonstrate that our renovated names help train\nstronger open-vocabulary models with up to 15% relative improvement and\nsignificantly enhance training efficiency with improved data quality. We also\nshow that our renovated names improve evaluation by better measuring\nmisclassification and enabling fine-grained model analysis. We will provide our\ncode and relabelings for several popular segmentation datasets (MS COCO,\nADE20K, Cityscapes) to the research community.\n", "rewritten_text": "The importance of names in human cognition and vision-language models cannot be overstated. Open-vocabulary models leverage class names as prompts to generalize to unseen categories, but the accuracy of these names is often neglected in current datasets. This paper introduces a solution to this overlooked issue with a framework called RENOVATE (Renovating Names in Open-Vocabulary Segmentation Benchmarks). Our framework includes a renaming model that improves the quality of names for visual segments. Through experiments, we demonstrate that our updated names enhance the training of open-vocabulary models, leading to up to a 15% relative improvement and increased training efficiency due to better data quality. Additionally, our revised names enhance evaluation by improving misclassification measurement and enabling detailed model analysis. We will make our code and relabeled datasets for popular segmentation datasets (MS COCO, ADE20K, Cityscapes) available to the research community."}, "2310.13201": {"original_text": "  Early identification of abnormalities in plants is an important task for\nensuring proper growth and achieving high yields from crops. Precision\nagriculture can significantly benefit from modern computer vision tools to make\nfarming strategies addressing these issues efficient and effective. As farming\nlands are typically quite large, farmers have to manually check vast areas to\ndetermine the status of the plants and apply proper treatments. In this work,\nwe consider the problem of automatically identifying abnormal regions in maize\nplants from images captured by a UAV. Using deep learning techniques, we have\ndeveloped a methodology which can detect different levels of abnormality (i.e.,\nlow, medium, high or no abnormality) in maize plants independently of their\ngrowth stage. The primary goal is to identify anomalies at the earliest\npossible stage in order to maximize the effectiveness of potential treatments.\nAt the same time, the proposed system can provide valuable information to human\nannotators for ground truth data collection by helping them to focus their\nattention on a much smaller set of images only. We have experimented with two\ndifferent but complimentary approaches, the first considering abnormality\ndetection as a classification problem and the second considering it as a\nregression problem. Both approaches can be generalized to different types of\nabnormalities and do not make any assumption about the abnormality occurring at\nan early plant growth stage which might be easier to detect due to the plants\nbeing smaller and easier to separate. As a case study, we have considered a\npublicly available data set which exhibits mostly Nitrogen deficiency in maize\nplants of various growth stages. We are reporting promising preliminary results\nwith an 88.89\\% detection accuracy of low abnormality and 100\\% detection\naccuracy of no abnormality.\n", "rewritten_text": "Identifying plant abnormalities early on is crucial for ensuring optimal growth and maximizing crop yields. Modern computer vision tools can greatly enhance precision agriculture by efficiently detecting and addressing these issues. Given the vast size of farming lands, farmers often face the challenge of manually inspecting large areas to assess plant health and apply necessary treatments. This study focuses on automatically identifying abnormal regions in maize plants using images captured by a UAV. Through the application of deep learning techniques, a methodology has been developed to detect varying levels of abnormality in maize plants regardless of their growth stage. The main objective is to detect anomalies at an early stage to improve treatment effectiveness. Additionally, the system can assist human annotators by narrowing down the focus to a smaller set of images for ground truth data collection. Two approaches were explored: classifying abnormality detection and treating it as a regression problem. Both methods are adaptable to different types of abnormalities and do not assume that abnormalities occur only in early plant growth stages. The study utilized a publicly available dataset showcasing Nitrogen deficiency in maize plants at different growth stages, achieving promising preliminary results with an 88.89% accuracy in detecting low abnormality and 100% accuracy in detecting no abnormality."}, "1711.0664": {"original_text": "  We investigate the problem of producing structured graph representations of\nvisual scenes. Our work analyzes the role of motifs: regularly appearing\nsubstructures in scene graphs. We present new quantitative insights on such\nrepeated structures in the Visual Genome dataset. Our analysis shows that\nobject labels are highly predictive of relation labels but not vice-versa. We\nalso find that there are recurring patterns even in larger subgraphs: more than\n50% of graphs contain motifs involving at least two relations. Our analysis\nmotivates a new baseline: given object detections, predict the most frequent\nrelation between object pairs with the given labels, as seen in the training\nset. This baseline improves on the previous state-of-the-art by an average of\n3.6% relative improvement across evaluation settings. We then introduce Stacked\nMotif Networks, a new architecture designed to capture higher order motifs in\nscene graphs that further improves over our strong baseline by an average 7.1%\nrelative gain. Our code is available at github.com/rowanz/neural-motifs.\n", "rewritten_text": "We explore the issue of creating structured graph representations for visual scenes by examining the significance of motifs, which are recurring substructures in scene graphs. Our research provides fresh quantitative insights into these repeated structures using the Visual Genome dataset. Our analysis reveals that object labels can predict relation labels effectively, but not the other way around. We also observe recurring patterns in larger subgraphs, with over 50% of graphs containing motifs involving at least two relations. Based on our findings, we propose a new baseline approach: predicting the most common relation between object pairs with given labels from the training set. This baseline outperforms the previous state-of-the-art by an average of 3.6% across evaluation scenarios. Additionally, we introduce Stacked Motif Networks, a novel architecture aimed at capturing higher-order motifs in scene graphs, which further enhances performance over our strong baseline by an average of 7.1%. Our code can be found at github.com/rowanz/neural-motifs."}, "2207.01932": {"original_text": "  Image Coding for Machines (ICM) aims to compress images for AI tasks analysis\nrather than meeting human perception. Learning a kind of feature that is both\ngeneral (for AI tasks) and compact (for compression) is pivotal for its\nsuccess. In this paper, we attempt to develop an ICM framework by learning\nuniversal features while also considering compression. We name such features as\nomnipotent features and the corresponding framework as Omni-ICM. Considering\nself-supervised learning (SSL) improves feature generalization, we integrate it\nwith the compression task into the Omni-ICM framework to learn omnipotent\nfeatures. However, it is non-trivial to coordinate semantics modeling in SSL\nand redundancy removing in compression, so we design a novel information\nfiltering (IF) module between them by co-optimization of instance\ndistinguishment and entropy minimization to adaptively drop information that is\nweakly related to AI tasks (e.g., some texture redundancy). Different from\nprevious task-specific solutions, Omni-ICM could directly support AI tasks\nanalysis based on the learned omnipotent features without joint training or\nextra transformation. Albeit simple and intuitive, Omni-ICM significantly\noutperforms existing traditional and learning-based codecs on multiple\nfundamental vision tasks.\n", "rewritten_text": "The goal of Image Coding for Machines (ICM) is to compress images for AI analysis, focusing on creating features that are both general for AI tasks and compact for compression. This paper introduces the Omni-ICM framework, which aims to learn universal features called omnipotent features while also considering compression. By incorporating self-supervised learning (SSL) to enhance feature generalization, the Omni-ICM framework is designed to learn omnipotent features. To address the challenge of coordinating semantics modeling and redundancy removal, an information filtering (IF) module is introduced to selectively drop irrelevant information. Unlike previous solutions, Omni-ICM can directly support AI analysis using the learned omnipotent features without additional training or transformation. Despite its simplicity, Omni-ICM demonstrates superior performance compared to traditional and learning-based codecs across various vision tasks."}, "1711.07183": {"original_text": "  Generating adversarial examples is an intriguing problem and an important way\nof understanding the working mechanism of deep neural networks. Most existing\napproaches generated perturbations in the image space, i.e., each pixel can be\nmodified independently. However, in this paper we pay special attention to the\nsubset of adversarial examples that correspond to meaningful changes in 3D\nphysical properties (like rotation and translation, illumination condition,\netc.). These adversaries arguably pose a more serious concern, as they\ndemonstrate the possibility of causing neural network failure by easy\nperturbations of real-world 3D objects and scenes.\n  In the contexts of object classification and visual question answering, we\naugment state-of-the-art deep neural networks that receive 2D input images with\na rendering module (either differentiable or not) in front, so that a 3D scene\n(in the physical space) is rendered into a 2D image (in the image space), and\nthen mapped to a prediction (in the output space). The adversarial\nperturbations can now go beyond the image space, and have clear meanings in the\n3D physical world. Though image-space adversaries can be interpreted as\nper-pixel albedo change, we verify that they cannot be well explained along\nthese physically meaningful dimensions, which often have a non-local effect.\nBut it is still possible to successfully attack beyond the image space on the\nphysical space, though this is more difficult than image-space attacks,\nreflected in lower success rates and heavier perturbations required.\n", "rewritten_text": "Creating adversarial examples is a fascinating challenge that provides valuable insights into how deep neural networks operate. While many existing methods focus on manipulating individual pixels in images, this paper delves into a subset of adversarial examples that involve meaningful alterations in 3D physical properties such as rotation, translation, and illumination. These types of adversaries are particularly concerning as they demonstrate the potential for neural network failures through subtle changes to real-world 3D objects and scenes.\n\nIn the realm of object classification and visual question answering, we enhance cutting-edge deep neural networks designed for 2D image inputs with a rendering module at the forefront. This module, whether differentiable or not, transforms a 3D scene in physical space into a 2D image in the image space, which is then used for predictions in the output space. By extending adversarial perturbations beyond the image space, we can now introduce meaningful changes in the 3D physical world. While image-space adversaries may be seen as alterations in per-pixel albedo, we find that they do not align well with these physically significant dimensions, which often have a broader impact. Despite the increased difficulty compared to image-space attacks, successfully targeting the physical space beyond the image space is achievable, albeit with lower success rates and more substantial perturbations."}, "2105.02039": {"original_text": "  In this paper, we fill the research gap by adopting state-of-the-art computer\nvision techniques for the data extraction stage in a data mining system. As\nshown in Fig.1, this stage contains two subtasks, namely, plot element\ndetection and data conversion. For building a robust box detector, we\ncomprehensively compare different deep learning-based methods and find a\nsuitable method to detect box with high precision. For building a robust point\ndetector, a fully convolutional network with feature fusion module is adopted,\nwhich can distinguish close points compared to traditional methods. The\nproposed system can effectively handle various chart data without making\nheuristic assumptions. For data conversion, we translate the detected element\ninto data with semantic value. A network is proposed to measure feature\nsimilarities between legends and detected elements in the legend matching\nphase. Furthermore, we provide a baseline on the competition of Harvesting raw\ntables from Infographics. Some key factors have been found to improve the\nperformance of each stage. Experimental results demonstrate the effectiveness\nof the proposed system.\n", "rewritten_text": "In this paper, we address a research gap by utilizing advanced computer vision techniques for the data extraction phase in a data mining system. The data extraction stage, illustrated in Fig.1, involves two main tasks: detecting plot elements and converting data. To create a reliable box detector, we extensively compare various deep learning methods and identify a suitable approach for precise box detection. For the point detection task, we employ a fully convolutional network with a feature fusion module, which can differentiate closely located points better than traditional methods. Our system is capable of handling diverse chart data without relying on heuristic assumptions. In terms of data conversion, we transform the identified elements into semantically meaningful data. We introduce a network to assess feature similarities between legends and detected elements during the legend matching process. Additionally, we establish a performance baseline for extracting raw tables from infographics. Through experimentation, we demonstrate the effectiveness of our proposed system and identify key factors that enhance the performance of each stage."}, "2401.08508": {"original_text": "  Sentiment analysis and emotion detection are important research topics in\nnatural language processing (NLP) and benefit many downstream tasks. With the\nwidespread application of LLMs, researchers have started exploring the\napplication of LLMs based on instruction-tuning in the field of sentiment\nanalysis. However, these models only focus on single aspects of affective\nclassification tasks (e.g. sentimental polarity or categorical emotions), and\noverlook the regression tasks (e.g. sentiment strength or emotion intensity),\nwhich leads to poor performance in downstream tasks. The main reason is the\nlack of comprehensive affective instruction tuning datasets and evaluation\nbenchmarks, which cover various affective classification and regression tasks.\nMoreover, although emotional information is useful for downstream tasks,\nexisting downstream datasets lack high-quality and comprehensive affective\nannotations. In this paper, we propose EmoLLMs, the first series of\nopen-sourced instruction-following LLMs for comprehensive affective analysis\nbased on fine-tuning various LLMs with instruction data, the first multi-task\naffective analysis instruction dataset (AAID) with 234K data samples based on\nvarious classification and regression tasks to support LLM instruction tuning,\nand a comprehensive affective evaluation benchmark (AEB) with 14 tasks from\nvarious sources and domains to test the generalization ability of LLMs. We\npropose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various\naffective instruction tasks. We compare our model with a variety of LLMs on\nAEB, where our models outperform all other open-sourced LLMs, and surpass\nChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve\nthe ChatGPT-level and GPT-4-level generalization capabilities on affective\nanalysis tasks, and demonstrates our models can be used as affective annotation\ntools.\n", "rewritten_text": "Sentiment analysis and emotion detection are significant areas of study in natural language processing (NLP) and offer benefits to various downstream tasks. The rise of Large Language Models (LLMs) has prompted researchers to explore the use of instruction-tuning LLMs in sentiment analysis. However, existing models tend to focus solely on specific aspects of affective classification tasks, such as sentimental polarity or categorical emotions, while neglecting regression tasks like sentiment strength or emotion intensity. This limited focus results in subpar performance in downstream tasks due to the absence of comprehensive affective instruction tuning datasets and evaluation benchmarks covering a wide range of affective classification and regression tasks. Additionally, although emotional information is valuable for downstream tasks, current datasets lack thorough and high-quality affective annotations.\n\nIn this study, we introduce EmoLLMs, a novel series of open-source instruction-following LLMs designed for comprehensive affective analysis. EmoLLMs are created by fine-tuning various LLMs with instruction data, utilizing the first multi-task affective analysis instruction dataset (AAID) comprising 234K data samples across diverse classification and regression tasks to facilitate LLM instruction tuning. Furthermore, we introduce a comprehensive affective evaluation benchmark (AEB) featuring 14 tasks sourced from various domains to assess the generalization capabilities of LLMs.\n\nBy fine-tuning LLMs with AAID, we develop a range of EmoLLMs to address different affective instruction tasks. Our models are evaluated against various LLMs on AEB, demonstrating superior performance compared to other open-source LLMs and even outperforming ChatGPT and GPT-4 in most tasks. This highlights the ChatGPT-level and GPT-4-level generalization abilities of EmoLLMs in affective analysis tasks, positioning our models as effective tools for affective annotation."}, "2210.05564": {"original_text": "  Semantic segmentation is a fundamental topic in computer vision. Several deep\nlearning methods have been proposed for semantic segmentation with outstanding\nresults. However, these models require a lot of densely annotated images. To\naddress this problem, we propose a new algorithm that uses HyperGraph\nConvolutional Networks for Weakly-supervised Semantic Segmentation\n(HyperGCN-WSS). Our algorithm constructs spatial and k-Nearest Neighbor (k-NN)\ngraphs from the images in the dataset to generate the hypergraphs. Then, we\ntrain a specialized HyperGraph Convolutional Network (HyperGCN) architecture\nusing some weak signals. The outputs of the HyperGCN are denominated\npseudo-labels, which are later used to train a DeepLab model for semantic\nsegmentation. HyperGCN-WSS is evaluated on the PASCAL VOC 2012 dataset for\nsemantic segmentation, using scribbles or clicks as weak signals. Our algorithm\nshows competitive performance against previous methods.\n", "rewritten_text": "\"Semantic segmentation is a key area of focus in computer vision, with numerous deep learning approaches achieving impressive results in this field. However, these models typically rely on a large number of meticulously annotated images. To tackle this challenge, we introduce a novel algorithm called HyperGraph Convolutional Networks for Weakly-supervised Semantic Segmentation (HyperGCN-WSS). Our method leverages spatial and k-Nearest Neighbor (k-NN) graphs generated from dataset images to form hypergraphs. Subsequently, we train a specialized HyperGraph Convolutional Network (HyperGCN) architecture using limited signals. The HyperGCN produces pseudo-labels, which are then utilized to train a DeepLab model for semantic segmentation. Evaluation on the PASCAL VOC 2012 dataset demonstrates the effectiveness of HyperGCN-WSS, particularly when using scribbles or clicks as weak signals, showcasing competitive performance compared to existing methods.\""}, "2112.0618": {"original_text": "  We present 360-DFPE, a sequential floor plan estimation method that directly\ntakes 360-images as input without relying on active sensors or 3D information.\nOur approach leverages a loosely coupled integration between a monocular visual\nSLAM solution and a monocular 360-room layout approach, which estimate camera\nposes and layout geometries, respectively. Since our task is to sequentially\ncapture the floor plan using monocular images, the entire scene structure, room\ninstances, and room shapes are unknown. To tackle these challenges, we first\nhandle the scale difference between visual odometry and layout geometry via\nformulating an entropy minimization process, which enables us to directly align\n360-layouts without knowing the entire scene in advance. Second, to\nsequentially identify individual rooms, we propose a novel room identification\nalgorithm that tracks every room along the camera exploration using geometry\ninformation. Lastly, to estimate the final shape of the room, we propose a\nshortest path algorithm with an iterative coarse-to-fine strategy, which\nimproves prior formulations with higher accuracy and faster run-time. Moreover,\nwe collect a new floor plan dataset with challenging large-scale scenes,\nproviding both point clouds and sequential 360-image information. Experimental\nresults show that our monocular solution achieves favorable performance against\nthe current state-of-the-art algorithms that rely on active sensors and require\nthe entire scene reconstruction data in advance.\n", "rewritten_text": "We introduce 360-DFPE, a method for estimating floor plans sequentially using 360-degree images as input, without the need for active sensors or 3D data. Our approach combines a monocular visual SLAM solution with a monocular 360-room layout method to determine camera poses and layout geometries. As we aim to capture floor plans sequentially with monocular images, the scene structure, room instances, and room shapes are initially unknown. To address these challenges, we first address the scale difference between visual odometry and layout geometry through an entropy minimization process, allowing us to align 360-layouts without prior knowledge of the entire scene. Next, we introduce a novel room identification algorithm to track individual rooms as the camera explores the space using geometry information. Finally, we propose a shortest path algorithm with a coarse-to-fine strategy to estimate the final room shape, enhancing accuracy and runtime efficiency. Additionally, we have created a new dataset with complex large-scale scenes, including point clouds and sequential 360-image data. Experimental results demonstrate that our monocular approach outperforms current state-of-the-art algorithms that rely on active sensors and require full scene reconstruction data in advance."}, "2205.08811": {"original_text": "  Object pose estimation is crucial for robotic applications and augmented\nreality. Beyond instance level 6D object pose estimation methods, estimating\ncategory-level pose and shape has become a promising trend. As such, a new\nresearch field needs to be supported by well-designed datasets. To provide a\nbenchmark with high-quality ground truth annotations to the community, we\nintroduce a multimodal dataset for category-level object pose estimation with\nphotometrically challenging objects termed PhoCaL. PhoCaL comprises 60 high\nquality 3D models of household objects over 8 categories including highly\nreflective, transparent and symmetric objects. We developed a novel\nrobot-supported multi-modal (RGB, depth, polarisation) data acquisition and\nannotation process. It ensures sub-millimeter accuracy of the pose for opaque\ntextured, shiny and transparent objects, no motion blur and perfect camera\nsynchronisation. To set a benchmark for our dataset, state-of-the-art RGB-D and\nmonocular RGB methods are evaluated on the challenging scenes of PhoCaL.\n", "rewritten_text": "\"Accurate object pose estimation is essential for robotic applications and augmented reality. In addition to methods that estimate the 6D pose of individual objects, there is a growing interest in estimating the pose and shape of object categories. To support this new research direction, high-quality datasets are needed. To address this, we present the PhoCaL dataset, which focuses on category-level object pose estimation for challenging objects such as highly reflective, transparent, and symmetric items. The dataset includes 60 high-quality 3D models across 8 categories. We have developed a novel data acquisition and annotation process using multiple modalities (RGB, depth, polarization) with robot support to ensure accurate pose estimation for various types of objects. The dataset serves as a benchmark for evaluating state-of-the-art methods on challenging scenes.\""}, "2005.14439": {"original_text": "  Dynamic routing networks, aimed at finding the best routing paths in the\nnetworks, have achieved significant improvements to neural networks in terms of\naccuracy and efficiency. In this paper, we see dynamic routing networks in a\nfresh light, formulating a routing method as a mapping from a sample space to a\nrouting space. From the perspective of space mapping, prevalent methods of\ndynamic routing didn't consider how inference paths would be distributed in the\nrouting space. Thus, we propose a novel method, termed CoDiNet, to model the\nrelationship between a sample space and a routing space by regularizing the\ndistribution of routing paths with the properties of consistency and diversity.\nSpecifically, samples with similar semantics should be mapped into the same\narea in routing space, while those with dissimilar semantics should be mapped\ninto different areas. Moreover, we design a customizable dynamic routing\nmodule, which can strike a balance between accuracy and efficiency. When\ndeployed upon ResNet models, our method achieves higher performance and\neffectively reduces average computational cost on four widely used datasets.\n", "rewritten_text": "Dynamic routing networks have made significant advancements in improving the accuracy and efficiency of neural networks by finding optimal routing paths within the network. This paper introduces a fresh perspective on dynamic routing networks by defining a routing method as a mapping from a sample space to a routing space. Unlike existing methods, which do not consider the distribution of inference paths in the routing space, we propose a new approach called CoDiNet. CoDiNet aims to regulate the distribution of routing paths by emphasizing consistency and diversity, ensuring that samples with similar meanings are grouped together in the routing space while those with different meanings are separated. Additionally, we introduce a customizable dynamic routing module that balances accuracy and efficiency. When applied to ResNet models, our method outperforms existing techniques, achieving higher performance and reducing computational costs across four popular datasets."}, "2110.05594": {"original_text": "  We present a modern solution to the multi-view photometric stereo problem\n(MVPS). Our work suitably exploits the image formation model in a MVPS\nexperimental setup to recover the dense 3D reconstruction of an object from\nimages. We procure the surface orientation using a photometric stereo (PS)\nimage formation model and blend it with a multi-view neural radiance field\nrepresentation to recover the object's surface geometry. Contrary to the\nprevious multi-staged framework to MVPS, where the position, iso-depth\ncontours, or orientation measurements are estimated independently and then\nfused later, our method is simple to implement and realize. Our method performs\nneural rendering of multi-view images while utilizing surface normals estimated\nby a deep photometric stereo network. We render the MVPS images by considering\nthe object's surface normals for each 3D sample point along the viewing\ndirection rather than explicitly using the density gradient in the volume space\nvia 3D occupancy information. We optimize the proposed neural radiance field\nrepresentation for the MVPS setup efficiently using a fully connected deep\nnetwork to recover the 3D geometry of an object. Extensive evaluation on the\nDiLiGenT-MV benchmark dataset shows that our method performs better than the\napproaches that perform only PS or only multi-view stereo (MVS) and provides\ncomparable results against the state-of-the-art multi-stage fusion methods.\n", "rewritten_text": "We introduce a modern approach to solving the multi-view photometric stereo problem (MVPS). Our method leverages the image formation model within an MVPS experimental setup to achieve a detailed 3D reconstruction of an object from images. By utilizing a photometric stereo (PS) image formation model to determine surface orientation and combining it with a multi-view neural radiance field representation, we are able to recover the object's surface geometry. Unlike previous multi-staged frameworks for MVPS that independently estimate position, iso-depth contours, or orientation measurements before fusing them, our method is straightforward to implement and execute. Our approach involves neural rendering of multi-view images while incorporating surface normals estimated by a deep photometric stereo network. Instead of relying on 3D occupancy information, we render MVPS images by considering the object's surface normals for each 3D sample point along the viewing direction. We efficiently optimize the proposed neural radiance field representation for the MVPS setup using a fully connected deep network to reconstruct an object's 3D geometry. Extensive evaluation on the DiLiGenT-MV benchmark dataset demonstrates that our method outperforms approaches that solely use PS or multi-view stereo (MVS), and it yields comparable results to state-of-the-art multi-stage fusion methods."}, "2206.13263": {"original_text": "  Robust maritime obstacle detection is critical for safe navigation of\nautonomous boats and timely collision avoidance. The current state-of-the-art\nis based on deep segmentation networks trained on large datasets. However,\nper-pixel ground truth labeling of such datasets is labor-intensive and\nexpensive. We propose a new scaffolding learning regime (SLR) that leverages\nweak annotations consisting of water edges, the horizon location, and obstacle\nbounding boxes to train segmentation-based obstacle detection networks, thereby\nreducing the required ground truth labeling effort by a factor of twenty. SLR\ntrains an initial model from weak annotations and then alternates between\nre-estimating the segmentation pseudo-labels and improving the network\nparameters. Experiments show that maritime obstacle segmentation networks\ntrained using SLR on weak annotations not only match but outperform the same\nnetworks trained with dense ground truth labels, which is a remarkable result.\nIn addition to the increased accuracy, SLR also increases domain generalization\nand can be used for domain adaptation with a low manual annotation load. The\nSLR code and pre-trained models are available at\nhttps://github.com/lojzezust/SLR .\n", "rewritten_text": "\"Ensuring safe navigation and timely collision avoidance for autonomous boats relies heavily on robust detection of maritime obstacles. The current leading approach involves utilizing deep segmentation networks trained on extensive datasets. However, the process of labeling ground truth data for each pixel in these datasets is both laborious and costly. To address this challenge, we introduce a novel learning method called Scaffolding Learning Regime (SLR) that utilizes weak annotations such as water edges, horizon location, and obstacle bounding boxes to train segmentation-based obstacle detection networks. This approach significantly reduces the amount of manual labeling required by a factor of twenty. The SLR process involves training an initial model using weak annotations and then iteratively refining segmentation pseudo-labels while improving network parameters. Experimental results demonstrate that obstacle segmentation networks trained using SLR on weak annotations not only match but surpass networks trained with dense ground truth labels, showcasing a remarkable achievement. Furthermore, SLR enhances domain generalization and facilitates domain adaptation with minimal manual annotation effort. The SLR code and pre-trained models can be accessed at https://github.com/lojzezust/SLR.\""}, "2204.07126": {"original_text": "  Recent development of neural implicit function has shown tremendous success\non high-quality 3D shape reconstruction. However, most works divide the space\ninto inside and outside of the shape, which limits their representing power to\nsingle-layer and watertight shapes. This limitation leads to tedious data\nprocessing (converting non-watertight raw data to watertight) as well as the\nincapability of representing general object shapes in the real world. In this\nwork, we propose a novel method to represent general shapes including\nnon-watertight shapes and shapes with multi-layer surfaces. We introduce\nGeneral Implicit Function for 3D Shape (GIFS), which models the relationships\nbetween every two points instead of the relationships between points and\nsurfaces. Instead of dividing 3D space into predefined inside-outside regions,\nGIFS encodes whether two points are separated by any surface. Experiments on\nShapeNet show that GIFS outperforms previous state-of-the-art methods in terms\nof reconstruction quality, rendering efficiency, and visual fidelity. Project\npage is available at https://jianglongye.com/gifs .\n", "rewritten_text": "The recent advancement in neural implicit function has achieved great success in reconstructing high-quality 3D shapes. However, most existing approaches partition the space into interior and exterior regions of the shape, limiting their ability to represent only single-layer and watertight shapes. This constraint results in laborious data processing tasks, such as converting non-watertight raw data to watertight, and an inability to accurately represent diverse object shapes found in the real world. In this study, we propose a new method called General Implicit Function for 3D Shape (GIFS) that can represent general shapes, including non-watertight shapes and those with multi-layer surfaces. GIFS models the relationships between pairs of points rather than points and surfaces, avoiding predefined inside-outside regions in 3D space. Experimental results on ShapeNet demonstrate that GIFS surpasses previous state-of-the-art methods in terms of reconstruction quality, rendering efficiency, and visual fidelity. More information can be found on the project page at https://jianglongye.com/gifs."}, "2103.03166": {"original_text": "  Annotated medical images are typically rarer than labeled natural images\nsince they are limited by domain knowledge and privacy constraints. Recent\nadvances in transfer and contrastive learning have provided effective solutions\nto tackle such issues from different perspectives. The state-of-the-art\ntransfer learning (e.g., Big Transfer (BiT)) and contrastive learning (e.g.,\nSimple Siamese Contrastive Learning (SimSiam)) approaches have been\ninvestigated independently, without considering the complementary nature of\nsuch techniques. It would be appealing to accelerate contrastive learning with\ntransfer learning, given that slow convergence speed is a critical limitation\nof modern contrastive learning approaches. In this paper, we investigate the\nfeasibility of aligning BiT with SimSiam. From empirical analyses, different\nnormalization techniques (Group Norm in BiT vs. Batch Norm in SimSiam) are the\nkey hurdle of adapting BiT to SimSiam. When combining BiT with SimSiam, we\nevaluated the performance of using BiT, SimSiam, and BiT+SimSiam on CIFAR-10\nand HAM10000 datasets. The results suggest that the BiT models accelerate the\nconvergence speed of SimSiam. When used together, the model gives superior\nperformance over both of its counterparts. We hope this study will motivate\nresearchers to revisit the task of aggregating big pre-trained models with\ncontrastive learning models for image analysis.\n", "rewritten_text": "\"Annotated medical images are less common compared to labeled natural images due to limitations in domain knowledge and privacy constraints. Recent advancements in transfer and contrastive learning have offered effective solutions to address these challenges from various angles. State-of-the-art transfer learning methods like Big Transfer (BiT) and contrastive learning techniques such as Simple Siamese Contrastive Learning (SimSiam) have been explored independently, overlooking the potential synergy between them. Combining contrastive learning with transfer learning could enhance the speed of convergence, a key drawback of current contrastive learning approaches. This study investigates the feasibility of aligning BiT with SimSiam, highlighting the importance of normalization techniques (Group Norm in BiT versus Batch Norm in SimSiam). Performance evaluations on CIFAR-10 and HAM10000 datasets show that combining BiT with SimSiam accelerates convergence and outperforms using either method alone. The findings aim to inspire researchers to explore the integration of large pre-trained models with contrastive learning for image analysis.\""}, "2401.00642": {"original_text": "  During times of increasing antibiotic resistance and the spread of infectious\ndiseases like COVID-19, it is important to classify genes related to antibiotic\nresistance. As natural language processing has advanced with transformer-based\nlanguage models, many language models that learn characteristics of nucleotide\nsequences have also emerged. These models show good performance in classifying\nvarious features of nucleotide sequences. When classifying nucleotide\nsequences, not only the sequence itself, but also various background knowledge\nis utilized. In this study, we use not only a nucleotide sequence-based\nlanguage model but also a text language model based on PubMed articles to\nreflect more biological background knowledge in the model. We propose a method\nto fine-tune the nucleotide sequence language model and the text language model\nbased on various databases of antibiotic resistance genes. We also propose an\nLLM-based augmentation technique to supplement the data and an ensemble method\nto effectively combine the two models. We also propose a benchmark for\nevaluating the model. Our method achieved better performance than the\nnucleotide sequence language model in the drug resistance class prediction.\n", "rewritten_text": "\"In the face of rising antibiotic resistance and the spread of infectious diseases such as COVID-19, it is crucial to categorize genes associated with antibiotic resistance. With the advancement of natural language processing through transformer-based language models, numerous models have been developed to learn the characteristics of nucleotide sequences. These models demonstrate strong performance in categorizing various aspects of nucleotide sequences. When categorizing nucleotide sequences, not only the sequence itself but also diverse background knowledge is leveraged. In this research, we utilize both a nucleotide sequence-based language model and a text language model based on PubMed articles to incorporate more biological background knowledge into the model. We introduce a method to fine-tune the nucleotide sequence language model and the text language model using various databases of antibiotic resistance genes. Additionally, we propose an LLM-based augmentation technique to enhance the data and an ensemble method to effectively merge the two models. Furthermore, we suggest a benchmark for assessing the model. Our approach outperformed the nucleotide sequence language model in predicting drug resistance classes.\""}, "2406.15718": {"original_text": "  As large language models (LLMs) increasingly permeate daily lives, there is a\ngrowing demand for real-time interactions that mirror human conversations.\nTraditional turn-based chat systems driven by LLMs prevent users from verbally\ninteracting with the system while it is generating responses. To overcome these\nlimitations, we adapt existing LLMs to \\textit{duplex models} so that these\nLLMs can listen for users while generating output and dynamically adjust\nthemselves to provide users with instant feedback. % such as in response to\ninterruptions. Specifically, we divide the queries and responses of\nconversations into several time slices and then adopt a\ntime-division-multiplexing (TDM) encoding-decoding strategy to\npseudo-simultaneously process these slices. Furthermore, to make LLMs\nproficient enough to handle real-time conversations, we build a fine-tuning\ndataset consisting of alternating time slices of queries and responses as well\nas covering typical feedback types in instantaneous interactions. Our\nexperiments show that although the queries and responses of conversations are\nsegmented into incomplete slices for processing, LLMs can preserve their\noriginal performance on standard benchmarks with a few fine-tuning steps on our\ndataset. Automatic and human evaluation indicate that duplex models make\nuser-AI interactions more natural and human-like, and greatly improve user\nsatisfaction compared to vanilla LLMs. Our duplex model and dataset will be\nreleased.\n", "rewritten_text": "\"As the prevalence of large language models (LLMs) in everyday life continues to grow, there is an increasing need for real-time interactions that mimic human conversations. Traditional turn-based chat systems driven by LLMs hinder users from engaging verbally with the system while it generates responses. To address these limitations, we modify existing LLMs into 'duplex models' that can listen to users while generating output and adapt in real-time to provide instant feedback, even in response to interruptions. This involves dividing conversations into time slices and employing a time-division-multiplexing (TDM) encoding-decoding strategy to process these slices simultaneously. Additionally, to enhance LLMs' ability to handle real-time conversations, we create a fine-tuning dataset containing alternating time slices of queries and responses, covering various feedback types in instantaneous interactions. Our experiments demonstrate that despite processing segmented slices of conversations, LLMs maintain their original performance on standard benchmarks with minimal fine-tuning on our dataset. Both automatic and human evaluations confirm that duplex models enhance user-AI interactions, making them more natural and human-like, leading to increased user satisfaction compared to conventional LLMs. Our duplex model and dataset will be made publicly available.\""}, "2011.01535": {"original_text": "  3D-LaneNet+ is a camera-based DNN method for anchor free 3D lane detection\nwhich is able to detect 3d lanes of any arbitrary topology such as splits,\nmerges, as well as short and perpendicular lanes. We follow recently proposed\n3D-LaneNet, and extend it to enable the detection of these previously\nunsupported lane topologies. Our output representation is an anchor free,\nsemi-local tile representation that breaks down lanes into simple lane segments\nwhose parameters can be learnt. In addition we learn, per lane instance,\nfeature embedding that reasons for the global connectivity of locally detected\nsegments to form full 3d lanes. This combination allows 3D-LaneNet+ to avoid\nusing lane anchors, non-maximum suppression, and lane model fitting as in the\noriginal 3D-LaneNet. We demonstrate the efficacy of 3D-LaneNet+ using both\nsynthetic and real world data. Results show significant improvement relative to\nthe original 3D-LaneNet that can be attributed to better generalization to\ncomplex lane topologies, curvatures and surface geometries.\n", "rewritten_text": "3D-LaneNet+ is a camera-based deep neural network (DNN) approach for detecting 3D lanes without the need for anchors. It can identify 3D lanes with various configurations, including splits, merges, short lanes, and perpendicular lanes. Building upon the previously introduced 3D-LaneNet, we have expanded its capabilities to detect these previously unsupported lane structures. Our output format is a tile-based representation that does not rely on anchors, breaking down lanes into simpler segments with learnable parameters. Additionally, we develop a feature embedding for each lane instance to establish global connectivity among locally detected segments, forming complete 3D lanes. By combining these techniques, 3D-LaneNet+ eliminates the need for lane anchors, non-maximum suppression, and lane model fitting present in the original 3D-LaneNet. Our experiments with synthetic and real-world data demonstrate the effectiveness of 3D-LaneNet+, showing significant enhancements compared to the original 3D-LaneNet in terms of handling complex lane configurations, curvatures, and surface geometries."}, "1612.06496": {"original_text": "  Image segmentation is a popular area of research in computer vision that has\nmany applications in automated image processing. A recent technique called\npiecewise flat embeddings (PFE) has been proposed for use in image\nsegmentation; PFE transforms image pixel data into a lower dimensional\nrepresentation where similar pixels are pulled close together and dissimilar\npixels are pushed apart. This technique has shown promising results, but its\noriginal formulation is not computationally feasible for large images. We\npropose two improvements to the algorithm for computing PFE: first, we\nreformulate portions of the algorithm to enable various linear algebra\noperations to be performed in parallel; second, we propose utilizing an\niterative linear solver (preconditioned conjugate gradient) to quickly solve a\nlinear least-squares problem that occurs in the inner loop of a nested\niteration. With these two computational improvements, we show on a publicly\navailable image database that PFE can be sped up by an order of magnitude\nwithout sacrificing segmentation performance. Our results make this technique\nmore practical for use on large data sets, not only for image segmentation, but\nfor general data clustering problems.\n", "rewritten_text": "\"Image segmentation is a well-researched field in computer vision with numerous applications in automated image processing. A recent method known as piecewise flat embeddings (PFE) has been introduced for image segmentation. PFE works by transforming pixel data into a lower-dimensional representation, grouping similar pixels together and separating dissimilar ones. While PFE has shown promise, its original formulation is not suitable for processing large images. To address this limitation, we propose two enhancements to the algorithm: first, restructuring parts of the algorithm to allow for parallel linear algebra operations, and second, incorporating an iterative linear solver (preconditioned conjugate gradient) to efficiently solve a linear least-squares problem within the nested iteration loop. By implementing these computational enhancements, we demonstrate a tenfold acceleration of PFE on a publicly available image dataset without compromising segmentation quality. These findings render PFE more practical for handling large datasets, not only for image segmentation but also for general data clustering tasks.\""}, "1511.06783": {"original_text": "  We present a novel dataset and a novel algorithm for recognizing activities\nof daily living (ADL) from a first-person wearable camera. Handled objects are\ncrucially important for egocentric ADL recognition. For specific examination of\nobjects related to users' actions separately from other objects in an\nenvironment, many previous works have addressed the detection of handled\nobjects in images captured from head-mounted and chest-mounted cameras.\nNevertheless, detecting handled objects is not always easy because they tend to\nappear small in images. They can be occluded by a user's body. As described\nherein, we mount a camera on a user's wrist. A wrist-mounted camera can capture\nhandled objects at a large scale, and thus it enables us to skip object\ndetection process. To compare a wrist-mounted camera and a head-mounted camera,\nwe also develop a novel and publicly available dataset that includes videos and\nannotations of daily activities captured simultaneously by both cameras.\nAdditionally, we propose a discriminative video representation that retains\nspatial and temporal information after encoding frame descriptors extracted by\nConvolutional Neural Networks (CNN).\n", "rewritten_text": "We introduce a new dataset and algorithm for identifying daily activities using a wearable camera from a first-person perspective. Recognizing objects being handled is crucial for accurately identifying these activities. Previous studies have focused on detecting handled objects in images from head-mounted and chest-mounted cameras, but this can be challenging due to the small size and potential occlusion of these objects. In our approach, we attach the camera to the user's wrist, allowing for better visibility and larger-scale capture of handled objects, eliminating the need for separate object detection. To compare the effectiveness of wrist-mounted and head-mounted cameras, we have created a new dataset containing videos and annotations of daily activities captured simultaneously by both types of cameras. Additionally, we have developed a novel video representation that preserves spatial and temporal information by encoding frame descriptors extracted using Convolutional Neural Networks (CNN)."}, "2305.11806": {"original_text": "  Neural metrics for machine translation evaluation, such as COMET, exhibit\nsignificant improvements in their correlation with human judgments, as compared\nto traditional metrics based on lexical overlap, such as BLEU. Yet, neural\nmetrics are, to a great extent, \"black boxes\" returning a single sentence-level\nscore without transparency about the decision-making process. In this work, we\ndevelop and compare several neural explainability methods and demonstrate their\neffectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our\nstudy reveals that these metrics leverage token-level information that can be\ndirectly attributed to translation errors, as assessed through comparison of\ntoken-level neural saliency maps with Multidimensional Quality Metrics (MQM)\nannotations and with synthetically-generated critical translation errors. To\nease future research, we release our code at:\nhttps://github.com/Unbabel/COMET/tree/explainable-metrics.\n", "rewritten_text": "Neural metrics used to evaluate machine translation, such as COMET, have shown significant improvements in their correlation with human judgments compared to traditional metrics like BLEU, which are based on lexical overlap. However, neural metrics are often seen as \"black boxes\" because they provide a single sentence-level score without revealing their decision-making process. In this study, we introduce and compare various neural explainability methods that prove effective in interpreting advanced fine-tuned neural metrics. Our research demonstrates that these metrics utilize token-level information that can be directly linked to translation errors. This is evidenced by comparing token-level neural saliency maps with Multidimensional Quality Metrics (MQM) annotations and with artificially created critical translation errors. To facilitate further research in this area, we have made our code available at: https://github.com/Unbabel/COMET/tree/explainable-metrics."}, "1906.11143": {"original_text": "  Accurate segmentation of the optic disc (OD) and cup (OC)in fundus images\nfrom different datasets is critical for glaucoma disease screening. The\ncross-domain discrepancy (domain shift) hinders the generalization of deep\nneural networks to work on different domain datasets.In this work, we present\nan unsupervised domain adaptation framework,called Boundary and Entropy-driven\nAdversarial Learning (BEAL), to improve the OD and OC segmentation performance,\nespecially on the ambiguous boundary regions. In particular, our proposed BEAL\nframe-work utilizes the adversarial learning to encourage the boundary\nprediction and mask probability entropy map (uncertainty map) of the target\ndomain to be similar to the source ones, generating more accurate boundaries\nand suppressing the high uncertainty predictions of OD and OC segmentation. We\nevaluate the proposed BEAL framework on two public retinal fundus image\ndatasets (Drishti-GS and RIM-ONE-r3), and the experiment results demonstrate\nthat our method outperforms the state-of-the-art unsupervised domain adaptation\nmethods. Codes will be available at https://github.com/EmmaW8/BEAL.\n", "rewritten_text": "Precise segmentation of the optic disc (OD) and cup (OC) in fundus images across various datasets is crucial for glaucoma screening. The challenge of domain shift limits the ability of deep neural networks to perform well on diverse datasets. This study introduces an unsupervised domain adaptation approach named Boundary and Entropy-driven Adversarial Learning (BEAL) to enhance OD and OC segmentation accuracy, particularly in areas with unclear boundaries. The BEAL framework leverages adversarial learning to promote similarity between boundary predictions and uncertainty maps of the target and source domains, resulting in more accurate boundaries and reduced uncertainty in segmentation. The effectiveness of BEAL is demonstrated on two public retinal fundus image datasets (Drishti-GS and RIM-ONE-r3), showing superior performance compared to existing unsupervised domain adaptation methods. The code will be accessible at https://github.com/EmmaW8/BEAL."}, "2108.08109": {"original_text": "  Illustrations are an essential transmission instrument. For an historian, the\nfirst step in studying their evolution in a corpus of similar manuscripts is to\nidentify which ones correspond to each other. This image collation task is\ndaunting for manuscripts separated by many lost copies, spreading over\ncenturies, which might have been completely re-organized and greatly modified\nto adapt to novel knowledge or belief and include hundreds of illustrations.\nOur contributions in this paper are threefold. First, we introduce the task of\nillustration collation and a large annotated public dataset to evaluate\nsolutions, including 6 manuscripts of 2 different texts with more than 2 000\nillustrations and 1 200 annotated correspondences. Second, we analyze state of\nthe art similarity measures for this task and show that they succeed in simple\ncases but struggle for large manuscripts when the illustrations have undergone\nvery significant changes and are discriminated only by fine details. Finally,\nwe show clear evidence that significant performance boosts can be expected by\nexploiting cycle-consistent correspondences. Our code and data are available on\nhttp://imagine.enpc.fr/~shenx/ImageCollation.\n", "rewritten_text": "\"Images play a crucial role in transmitting information. When studying the evolution of illustrations in a collection of similar manuscripts, historians must first identify corresponding images. This can be challenging for manuscripts that are separated by lost copies and have been reorganized and modified over centuries to incorporate new knowledge or beliefs, often containing hundreds of illustrations. In this paper, we present three main contributions. Firstly, we introduce the task of image collation and provide a large annotated dataset for evaluation, consisting of 6 manuscripts from 2 different texts with over 2,000 illustrations and 1,200 annotated correspondences. Secondly, we examine current similarity measures for this task and find that they perform well in simple cases but struggle with large manuscripts where illustrations have undergone significant changes and are distinguished by subtle details. Lastly, we demonstrate that leveraging cycle-consistent correspondences can greatly enhance performance. Our code and data can be accessed at http://imagine.enpc.fr/~shenx/ImageCollation.\""}, "2204.01062": {"original_text": "  Several popular computer vision (CV) datasets, specifically employed for\nObject Detection (OD) in autonomous driving tasks exhibit biases due to a range\nof factors including weather and lighting conditions. These biases may impair a\nmodel's generalizability, rendering it ineffective for OD in novel and unseen\ndatasets. Especially, in autonomous driving, it may prove extremely high risk\nand unsafe for the vehicle and its surroundings. This work focuses on\nunderstanding these datasets better by identifying such \"good-weather\" bias.\nMethods to mitigate such bias which allows the OD models to perform better and\nimprove the robustness are also demonstrated. A simple yet effective OD\nframework for studying bias mitigation is proposed. Using this framework, the\nperformance on popular datasets is analyzed and a significant difference in\nmodel performance is observed. Additionally, a knowledge transfer technique and\na synthetic image corruption technique are proposed to mitigate the identified\nbias. Finally, using the DAWN dataset, the findings are validated on the OD\ntask, demonstrating the effectiveness of our techniques in mitigating\nreal-world \"good-weather\" bias. The experiments show that the proposed\ntechniques outperform baseline methods by averaged fourfold improvement.\n", "rewritten_text": "The text discusses how popular computer vision datasets used for Object Detection in autonomous driving can have biases due to factors like weather and lighting conditions. These biases can limit a model's ability to work effectively with new datasets, posing risks in autonomous driving scenarios. The focus of this work is on identifying and addressing biases related to \"good-weather\" conditions in these datasets. Methods to reduce bias and enhance the performance of Object Detection models are presented, along with a simple yet effective framework for bias mitigation. The study analyzes model performance on well-known datasets, revealing significant improvements. Techniques such as knowledge transfer and synthetic image corruption are proposed to address the identified bias. The effectiveness of these techniques is validated using the DAWN dataset, showing a fourfold improvement over baseline methods."}, "2302.01097": {"original_text": "  Tree kernels have been proposed to be used in many areas as the automatic\nlearning of natural language applications. In this paper, we propose a new\nlinear time algorithm based on the concept of weighted tree automata for\nSubTree kernel computation. First, we introduce a new class of weighted tree\nautomata, called Root-Weighted Tree Automata, and their associated formal tree\nseries. Then we define, from this class, the SubTree automata that represent\ncompact computational models for finite tree languages. This allows us to\ndesign a theoretically guaranteed linear-time algorithm for computing the\nSubTree Kernel based on weighted tree automata intersection. The key idea\nbehind the proposed algorithm is to replace DAG reduction and nodes sorting\nsteps used in previous approaches by states equivalence classes computation\nallowed in the weighted tree automata approach. Our approach has three major\nadvantages: it is output-sensitive, it is free sensitive from the tree types\n(ordered trees versus unordered trees), and it is well adapted to any\nincremental tree kernel based learning methods. Finally, we conduct a variety\nof comparative experiments on a wide range of synthetic tree languages datasets\nadapted for a deep algorithm analysis. The obtained results show that the\nproposed algorithm outperforms state-of-the-art methods.\n", "rewritten_text": "Tree kernels have been suggested for use in various applications involving the automatic learning of natural language. In this paper, we present a novel linear time algorithm that utilizes weighted tree automata for computing the SubTree kernel. We introduce a new type of weighted tree automata known as Root-Weighted Tree Automata and their corresponding formal tree series. Subsequently, we define SubTree automata from this category, which serve as concise computational models for finite tree languages. This enables the development of a linear-time algorithm for computing the SubTree Kernel based on the intersection of weighted tree automata. The key concept of our algorithm involves computing equivalence classes of states in weighted tree automata, eliminating the need for DAG reduction and node sorting steps seen in previous methods. Our approach offers three main advantages: it is output-sensitive, agnostic to tree types (ordered or unordered), and compatible with incremental tree kernel-based learning techniques. We then conduct a series of comparative experiments on diverse synthetic tree language datasets tailored for in-depth algorithm analysis. The results demonstrate that our proposed algorithm surpasses existing methods in performance."}, "2404.10877": {"original_text": "  In this paper, we aim to generate text classification data given arbitrary\nclass definitions (i.e., user instruction), so one can train a small text\nclassifier without any human annotation or raw corpus. Compared with pioneer\nattempts, our proposed Incubator is the first framework that can handle\ncomplicated and even mutually dependent classes (e.g., \"TED Talk given by\nEducator\" and \"Other\"). Specifically, Incubator is an LLM firstly tuned on the\ninstruction-to-data mappings that we obtained from classification datasets and\ndescriptions on HuggingFace together with in-context augmentation by GPT-4. We\nthen refine Incubator by learning on the cluster centers of semantic textual\nembeddings to emphasize the uniformity and semantic diversity in generations.\nWe compare Incubator on various classification tasks with strong baselines such\nas direct LLM-based inference and training data generation by prompt\nengineering. Experiments show Incubator is able to (1) perform well on\ntraditional benchmarks, (2) take label dependency and user preference into\nconsideration, and (3) enable logical text mining by incubating multiple\nclassifiers.\n", "rewritten_text": "\"In this paper, our goal is to create text classification data based on user-defined class definitions, allowing for the training of a small text classifier without the need for human annotation or raw corpus. Our proposed framework, Incubator, is unique in its ability to handle complex and interdependent classes, such as \"TED Talk given by Educator\" and \"Other.\" Initially, Incubator is fine-tuned using instruction-to-data mappings from classification datasets and descriptions on HuggingFace, with additional in-context augmentation by GPT-4. We further enhance Incubator by training on cluster centers of semantic textual embeddings to promote consistency and diversity in generated text. Through comparisons with established methods like direct LLM-based inference and prompt engineering, we demonstrate that Incubator excels in various classification tasks, considering label dependencies, user preferences, and facilitating logical text mining by incubating multiple classifiers.\""}, "2210.09782": {"original_text": "  This paper focuses on developing a more effective method of hierarchical\npropagation for semi-supervised Video Object Segmentation (VOS). Based on\nvision transformers, the recently-developed Associating Objects with\nTransformers (AOT) approach introduces hierarchical propagation into VOS and\nhas shown promising results. The hierarchical propagation can gradually\npropagate information from past frames to the current frame and transfer the\ncurrent frame feature from object-agnostic to object-specific. However, the\nincrease of object-specific information will inevitably lead to the loss of\nobject-agnostic visual information in deep propagation layers. To solve such a\nproblem and further facilitate the learning of visual embeddings, this paper\nproposes a Decoupling Features in Hierarchical Propagation (DeAOT) approach.\nFirstly, DeAOT decouples the hierarchical propagation of object-agnostic and\nobject-specific embeddings by handling them in two independent branches.\nSecondly, to compensate for the additional computation from dual-branch\npropagation, we propose an efficient module for constructing hierarchical\npropagation, i.e., Gated Propagation Module, which is carefully designed with\nsingle-head attention. Extensive experiments show that DeAOT significantly\noutperforms AOT in both accuracy and efficiency. On YouTube-VOS, DeAOT can\nachieve 86.0% at 22.4fps and 82.0% at 53.4fps. Without test-time augmentations,\nwe achieve new state-of-the-art performance on four benchmarks, i.e.,\nYouTube-VOS (86.2%), DAVIS 2017 (86.2%), DAVIS 2016 (92.9%), and VOT 2020\n(0.622). Project page: https://github.com/z-x-yang/AOT.\n", "rewritten_text": "This study aims to enhance the hierarchical propagation method for semi-supervised Video Object Segmentation (VOS) by introducing a new approach called Decoupling Features in Hierarchical Propagation (DeAOT). The DeAOT approach addresses the challenge of losing object-agnostic visual information in deep propagation layers by separating object-agnostic and object-specific embeddings into two independent branches. Additionally, a Gated Propagation Module is proposed to efficiently construct hierarchical propagation using single-head attention. Experimental results demonstrate that DeAOT outperforms the previous Associating Objects with Transformers (AOT) approach in terms of accuracy and efficiency. Specifically, on YouTube-VOS, DeAOT achieves 86.0% accuracy at 22.4fps and 82.0% accuracy at 53.4fps. Furthermore, without test-time augmentations, DeAOT sets new state-of-the-art performance benchmarks on YouTube-VOS (86.2%), DAVIS 2017 (86.2%), DAVIS 2016 (92.9%), and VOT 2020 (0.622). For more information, visit the project page at https://github.com/z-x-yang/AOT."}, "2406.18173": {"original_text": "  Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases.\n", "rewritten_text": "This study presents UIO-LLMs, a method for optimizing memory-enhanced transformers in long-context scenarios. The approach involves a streamlined encoder-decoder framework where shared weights are used to store context segments in memories and predict subsequent outputs. By treating the transformers as fully-connected recurrent neural networks and applying the Truncated Backpropagation Through Time algorithm, training is refined with incremental optimization techniques. These techniques reduce time complexity and address gradient computation bias. UIO-LLMs effectively manage long contexts, expanding the context window of Llama2-7b-chat from 4K to 100K tokens with minimal parameter increase and maintaining linear inference cost with longer context lengths."}, "2306.07713": {"original_text": "  Segment anything model (SAM), as the name suggests, is claimed to be capable\nof cutting out any object and demonstrates impressive zero-shot transfer\nperformance with the guidance of prompts. However, there is currently a lack of\ncomprehensive evaluation regarding its robustness under various corruptions.\nUnderstanding the robustness of SAM across different corruption scenarios is\ncrucial for its real-world deployment. Prior works show that SAM is biased\ntowards texture (style) rather than shape, motivated by which we start by\ninvestigating its robustness against style transfer, which is synthetic\ncorruption. Following by interpreting the effects of synthetic corruption as\nstyle changes, we proceed to conduct a comprehensive evaluation for its\nrobustness against 15 types of common corruption. These corruptions mainly fall\ninto categories such as digital, noise, weather, and blur, and within each\ncorruption category, we explore 5 severity levels to simulate real-world\ncorruption scenarios. Beyond the corruptions, we further assess the robustness\nof SAM against local occlusion and local adversarial patch attacks. To the best\nof our knowledge, our work is the first of its kind to evaluate the robustness\nof SAM under style change, local occlusion, and local adversarial patch\nattacks. Given that patch attacks visible to human eyes are easily detectable,\nwe further assess its robustness against global adversarial attacks that are\nimperceptible to human eyes. Overall, this work provides a comprehensive\nempirical study of the robustness of SAM, evaluating its performance under\nvarious corruptions and extending the assessment to critical aspects such as\nlocal occlusion, local adversarial patch attacks, and global adversarial\nattacks. These evaluations yield valuable insights into the practical\napplicability and effectiveness of SAM in addressing real-world challenges.\n", "rewritten_text": "The Segment Anything Model (SAM) is known for its ability to accurately isolate objects and exhibit impressive performance in transferring knowledge without prior training when given prompts. However, there is a need for a thorough evaluation of its resilience to different types of corruptions. Understanding how SAM performs under various corruption scenarios is essential for its practical use. Previous studies have shown that SAM tends to prioritize texture over shape. To investigate its resilience against style transfer, a form of synthetic corruption, we first analyze its response to style changes. We then conduct a comprehensive assessment of SAM's robustness against 15 common types of corruptions, including digital, noise, weather, and blur. Each corruption category is explored at five severity levels to simulate real-world challenges. Additionally, we evaluate SAM's ability to withstand local occlusion and local adversarial patch attacks. This study is the first of its kind to assess SAM's resilience to style changes, local occlusion, and local adversarial patch attacks. We also test SAM's resistance to global adversarial attacks that are invisible to the human eye, as attacks visible to humans are easily detectable. Overall, this research offers a detailed examination of SAM's robustness, assessing its performance under various corruptions and addressing critical aspects such as local occlusion, local adversarial patch attacks, and global adversarial attacks. These findings provide valuable insights into the practical effectiveness of SAM in tackling real-world challenges."}, "2103.1399": {"original_text": "  A fundamental challenge faced by existing Fine-Grained Sketch-Based Image\nRetrieval (FG-SBIR) models is the data scarcity -- model performances are\nlargely bottlenecked by the lack of sketch-photo pairs. Whilst the number of\nphotos can be easily scaled, each corresponding sketch still needs to be\nindividually produced. In this paper, we aim to mitigate such an upper-bound on\nsketch data, and study whether unlabelled photos alone (of which they are many)\ncan be cultivated for performances gain. In particular, we introduce a novel\nsemi-supervised framework for cross-modal retrieval that can additionally\nleverage large-scale unlabelled photos to account for data scarcity. At the\ncentre of our semi-supervision design is a sequential photo-to-sketch\ngeneration model that aims to generate paired sketches for unlabelled photos.\nImportantly, we further introduce a discriminator guided mechanism to guide\nagainst unfaithful generation, together with a distillation loss based\nregularizer to provide tolerance against noisy training samples. Last but not\nleast, we treat generation and retrieval as two conjugate problems, where a\njoint learning procedure is devised for each module to mutually benefit from\neach other. Extensive experiments show that our semi-supervised model yields\nsignificant performance boost over the state-of-the-art supervised\nalternatives, as well as existing methods that can exploit unlabelled photos\nfor FG-SBIR.\n", "rewritten_text": "One of the main challenges faced by current Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) models is the lack of sufficient data, which limits their performance. While it is easy to increase the number of photos, creating corresponding sketches for each photo remains a time-consuming task. This study aims to address this limitation by exploring the potential of using unlabelled photos, which are abundant, to improve model performance. A novel semi-supervised framework for cross-modal retrieval is introduced, which leverages large-scale unlabelled photos to overcome data scarcity. The framework includes a sequential photo-to-sketch generation model to create paired sketches for unlabelled photos. Additionally, a discriminator-guided mechanism is implemented to ensure accurate generation, along with a distillation loss-based regularizer to handle noisy training samples. The study treats generation and retrieval as interconnected problems, with a joint learning approach designed to enhance both modules. Experimental results demonstrate that the proposed semi-supervised model outperforms state-of-the-art supervised methods and existing approaches that utilize unlabelled photos for FG-SBIR."}, "1412.7854": {"original_text": "  Traditional object recognition approaches apply feature extraction, part\ndeformation handling, occlusion handling and classification sequentially while\nthey are independent from each other. Ouyang and Wang proposed a model for\njointly learning of all of the mentioned processes using one deep neural\nnetwork. We utilized, and manipulated their toolbox in order to apply it in car\ndetection scenarios where it had not been tested. Creating a single deep\narchitecture from these components, improves the interaction between them and\ncan enhance the performance of the whole system. We believe that the approach\ncan be used as a general purpose object detection toolbox. We tested the\nalgorithm on UIUC car dataset, and achieved an outstanding result. The accuracy\nof our method was 97 % while the previously reported results showed an accuracy\nof up to 91 %. We strongly believe that having an experiment on a larger\ndataset can show the advantage of using deep models over shallow ones.\n", "rewritten_text": "\"Conventional methods for object recognition typically involve extracting features, handling part deformations, managing occlusions, and performing classification in a sequential manner, with each step being independent. Ouyang and Wang introduced a model that integrates all these processes into a single deep neural network for joint learning. We adapted and applied their framework to car detection scenarios, where it had not been previously tested. By combining these components into a unified deep architecture, we improved their interaction and boosted the overall system performance. We consider this approach to be a versatile object detection toolbox that can be applied broadly. Testing the algorithm on the UIUC car dataset yielded exceptional results, with our method achieving 97% accuracy compared to the previously reported 91%. We are confident that further experiments on larger datasets will demonstrate the advantages of using deep models over shallow ones.\""}, "1905.03672": {"original_text": "  In this paper, we are interested in boosting the representation capability of\nconvolution neural networks which utilizing the inverted residual structure.\nBased on the success of Inverted Residual structure[Sandler et al. 2018] and\nInterleaved Low-Rank Group Convolutions[Sun et al. 2018], we rethink this two\npattern of neural network structure, rather than NAS(Neural architecture\nsearch) method[Zoph and Le 2017; Pham et al. 2018; Liu et al. 2018b], we\nintroduce uneven point-wise group convolution, which provide a novel search\nspace for designing basic blocks to obtain better trade-off between\nrepresentation capability and computational cost. Meanwhile, we propose two\nnovel information flow patterns that will enable cross-group information flow\nfor multiple group convolution layers with and without any channel\npermute/shuffle operation. Dense experiments on image classification task show\nthat our proposed model, named Seesaw-Net, achieves state-of-the-art(SOTA)\nperformance with limited computation and memory cost. Our code will be\nopen-source and available together with pre-trained models.\n", "rewritten_text": "\"In this paper, our focus is on enhancing the representation capacity of convolutional neural networks by utilizing the inverted residual structure. Drawing from the successful implementations of the Inverted Residual structure by Sandler et al. (2018) and Interleaved Low-Rank Group Convolutions by Sun et al. (2018), we reconsider these two neural network patterns. Instead of relying on the Neural Architecture Search (NAS) method proposed by Zoph and Le (2017), Pham et al. (2018), and Liu et al. (2018b), we introduce uneven point-wise group convolution. This introduces a new search space for designing basic blocks that strike a better balance between representation capacity and computational cost. Additionally, we introduce two novel information flow patterns that facilitate cross-group information flow for multiple group convolution layers, with or without channel permute/shuffle operations. Extensive experiments on image classification tasks demonstrate that our proposed model, Seesaw-Net, achieves state-of-the-art performance while maintaining limited computation and memory costs. Our code will be open-source and accompanied by pre-trained models.\""}, "1912.1163": {"original_text": "  Person re-identification has attracted many researchers' attention for its\nwide application, but it is still a very challenging task because only part of\nthe image information can be used for personnel matching. Most of current\nmethods uses CNN to learn to embeddings that can capture semantic similarity\ninformation among data points. Many of the state-of-the-arts methods use\ncomplex network structures with multiple branches that fuse multiple features\nwhile training or testing, using classification loss, Triplet loss or a\ncombination of the two as loss function. However, the method that using Triplet\nloss as loss function converges slowly, and the method in which pull features\nof the same class as close as possible in features space leads to poor feature\nstability. This paper will combine the ranking motivated structured loss,\nproposed a new metric learning loss function that make the features of the same\nclass are sparsely distributed into the range of small hyperspheres and the\nfeatures of different classes are uniformly distributed at a clearly angle. And\nadopted a new single-branch network structure that only using global feature\ncan also get great performance. The validity of our method is verified on the\nMarket1501 and DukeMTMC-ReID person re-identification datasets. Finally\nacquires 90.9% rank-1 accuracy and 80.8% mAP on DukeMTMC-reID, 95.3% rank-1\naccuracy and 88.7% mAP on Market1501. Codes and models are available in\nGithub.https://github.com/Qidian213/Ranked_Person_ReID.\n", "rewritten_text": "Person re-identification has garnered significant interest among researchers due to its broad range of applications. However, it remains a formidable challenge as only a portion of the image information can be utilized for matching individuals. Many current approaches employ Convolutional Neural Networks (CNNs) to learn embeddings that can capture semantic similarities among data points. State-of-the-art methods often utilize complex network structures with multiple branches to fuse various features during training or testing, employing classification loss, Triplet loss, or a combination of both as loss functions. Nevertheless, using Triplet loss as a loss function tends to result in slow convergence, and pulling features of the same class as close as possible in feature space can lead to poor feature stability. This paper introduces a novel metric learning loss function that incorporates a ranking-motivated structured loss, aiming to sparsely distribute features of the same class within small hyperspheres and uniformly distribute features of different classes at distinct angles. Additionally, a new single-branch network structure that solely relies on global features demonstrates excellent performance. The effectiveness of our method is validated on the Market1501 and DukeMTMC-ReID person re-identification datasets, achieving a rank-1 accuracy of 90.9% and mAP of 80.8% on DukeMTMC-reID, and a rank-1 accuracy of 95.3% and mAP of 88.7% on Market1501. The codes and models are accessible on Github at https://github.com/Qidian213/Ranked_Person_ReID."}, "1907.1144": {"original_text": "  Pooling is one of the main elements in convolutional neural networks. The\npooling reduces the size of the feature map, enabling training and testing with\na limited amount of computation. This paper proposes a new pooling method named\nuniversal pooling. Unlike the existing pooling methods such as average pooling,\nmax pooling, and stride pooling with fixed pooling function, universal pooling\ngenerates any pooling function, depending on a given problem and dataset.\nUniversal pooling was inspired by attention methods and can be considered as a\nchannel-wise form of local spatial attention. Universal pooling is trained\njointly with the main network and it is shown that it includes the existing\npooling methods. Finally, when applied to two benchmark problems, the proposed\nmethod outperformed the existing pooling methods and performed with the\nexpected diversity, adapting to the given problem.\n", "rewritten_text": "Pooling is a key component in convolutional neural networks, as it helps reduce the size of the feature map, making training and testing more computationally efficient. This study introduces a novel pooling technique called universal pooling, which differs from traditional methods like average pooling, max pooling, and stride pooling by dynamically generating pooling functions based on the specific problem and dataset. Inspired by attention mechanisms, universal pooling can be viewed as a channel-wise version of local spatial attention. It is trained alongside the main network and encompasses existing pooling methods. Experimental results on two standard tasks demonstrate that the proposed universal pooling method surpasses traditional pooling techniques and exhibits the desired adaptability to different problem scenarios."}, "2406.17236": {"original_text": "  Although recent years have witnessed significant advancements in image\nediting thanks to the remarkable progress of text-to-image diffusion models,\nthe problem of non-rigid image editing still presents its complexities and\nchallenges. Existing methods often fail to achieve consistent results due to\nthe absence of unique identity characteristics. Thus, learning a personalized\nidentity prior might help with consistency in the edited results. In this\npaper, we explore a novel task: learning the personalized identity prior for\ntext-based non-rigid image editing. To address the problems in jointly learning\nprior and editing the image, we present LIPE, a two-stage framework designed to\ncustomize the generative model utilizing a limited set of images of the same\nsubject, and subsequently employ the model with learned prior for non-rigid\nimage editing. Experimental results demonstrate the advantages of our approach\nin various editing scenarios over past related leading methods in qualitative\nand quantitative ways.\n", "rewritten_text": "In recent years, there have been significant advancements in image editing due to the progress of text-to-image diffusion models. However, non-rigid image editing remains complex and challenging. Existing methods often struggle to produce consistent results because they lack unique identity characteristics. Therefore, incorporating a personalized identity prior could improve consistency in edited results. This paper introduces a new task: learning a personalized identity prior for text-based non-rigid image editing. To tackle the challenges of simultaneously learning the prior and editing the image, we propose LIPE, a two-stage framework that customizes the generative model using a limited set of images of the same subject. We then apply the model with the learned prior for non-rigid image editing. Experimental results show that our approach outperforms previous methods in various editing scenarios both qualitatively and quantitatively."}, "2406.12679": {"original_text": "  Large Language Models (LLMs) are increasingly being used in educational and\nlearning applications. Research has demonstrated that controlling for style, to\nfit the needs of the learner, fosters increased understanding, promotes\ninclusion, and helps with knowledge distillation. To understand the\ncapabilities and limitations of contemporary LLMs in style control, we\nevaluated five state-of-the-art models: GPT-3.5, GPT-4, GPT-4o, Llama-3, and\nMistral-instruct- 7B across two style control tasks. We observed significant\ninconsistencies in the first task, with model performances averaging between\n5th and 8th grade reading levels for tasks intended for first-graders, and\nstandard deviations up to 27.6. For our second task, we observed a\nstatistically significant improvement in performance from 0.02 to 0.26.\nHowever, we find that even without stereotypes in reference texts, LLMs often\ngenerated culturally insensitive content during their tasks. We provide a\nthorough analysis and discussion of the results.\n", "rewritten_text": "Large Language Models (LLMs) are increasingly utilized in educational and learning settings. Studies have shown that adjusting the style to suit the learner's needs enhances comprehension, encourages inclusivity, and aids in knowledge transfer. To explore the capabilities and constraints of current LLMs in style adaptation, we assessed five cutting-edge models: GPT-3.5, GPT-4, GPT-4o, Llama-3, and Mistral-instruct-7B in two style control tasks. In the first task, we noted significant inconsistencies, with model performance ranging from 5th to 8th grade reading levels for tasks designed for first-graders, and standard deviations reaching up to 27.6. In the second task, we observed a statistically significant performance improvement from 0.02 to 0.26. Nevertheless, we discovered that even without biases in the source texts, LLMs frequently generated culturally insensitive content during the tasks. Our findings are thoroughly analyzed and discussed."}, "1907.06882": {"original_text": "  Majority of state-of-the-art monocular depth estimation methods are\nsupervised learning approaches. The success of such approaches heavily depends\non the high-quality depth labels which are expensive to obtain. Some recent\nmethods try to learn depth networks by leveraging unsupervised cues from\nmonocular videos which are easier to acquire but less reliable. In this paper,\nwe propose to resolve this dilemma by transferring knowledge from synthetic\nvideos with easily obtainable ground-truth depth labels. Due to the stylish\ndifference between synthetic and real images, we propose a\ntemporally-consistent domain adaptation (TCDA) approach that simultaneously\nexplores labels in the synthetic domain and temporal constraints in the videos\nto improve style transfer and depth prediction. Furthermore, we make use of the\nground-truth optical flow and pose information in the synthetic data to learn\nmoving mask and pose prediction networks. The learned moving masks can filter\nout moving regions that produces erroneous temporal constraints and the\nestimated poses provide better initializations for estimating temporal\nconstraints. Experimental results demonstrate the effectiveness of our method\nand comparable performance against state-of-the-art.\n", "rewritten_text": "The majority of advanced methods for estimating depth from a single image rely on supervised learning. However, these methods require high-quality depth labels, which can be costly to obtain. Some recent approaches aim to learn depth networks using unsupervised cues from monocular videos, which are easier to obtain but less reliable. In this study, we propose a solution to this challenge by transferring knowledge from synthetic videos that have readily available ground-truth depth labels. To address the differences between synthetic and real images, we introduce a temporally-consistent domain adaptation (TCDA) approach. This method simultaneously leverages labels from synthetic data and temporal constraints from videos to enhance style transfer and depth prediction. Additionally, we utilize ground-truth optical flow and pose information from synthetic data to train networks for moving mask and pose prediction. The learned moving masks help filter out moving regions that may introduce incorrect temporal constraints, while the estimated poses provide improved initializations for temporal constraint estimation. Our experimental results demonstrate the effectiveness of our approach, showing comparable performance to state-of-the-art methods."}, "2104.14839": {"original_text": "  Recently, various neural encoder-decoder models pioneered by Seq2Seq\nframework have been proposed to achieve the goal of generating more abstractive\nsummaries by learning to map input text to output text. At a high level, such\nneural models can freely generate summaries without any constraint on the words\nor phrases used. Moreover, their format is closer to human-edited summaries and\noutput is more readable and fluent. However, the neural model's abstraction\nability is a double-edged sword. A commonly observed problem with the generated\nsummaries is the distortion or fabrication of factual information in the\narticle. This inconsistency between the original text and the summary has\ncaused various concerns over its applicability, and the previous evaluation\nmethods of text summarization are not suitable for this issue. In response to\nthe above problems, the current research direction is predominantly divided\ninto two categories, one is to design fact-aware evaluation metrics to select\noutputs without factual inconsistency errors, and the other is to develop new\nsummarization systems towards factual consistency. In this survey, we focus on\npresenting a comprehensive review of these fact-specific evaluation methods and\ntext summarization models.\n", "rewritten_text": "Lately, a number of neural encoder-decoder models within the Seq2Seq framework have emerged with the aim of producing more abstract summaries by learning to convert input text into output text. These neural models have the capability to generate summaries without restrictions on vocabulary or phrasing, resulting in summaries that closely resemble those written by humans and are easier to read. However, the ability of these models to abstract information can sometimes lead to inaccuracies in the generated summaries, causing concerns about their reliability. Traditional evaluation methods for text summarization are not effective in addressing this issue. To tackle this challenge, current research is focused on two main approaches: developing evaluation metrics that prioritize factual accuracy and creating new summarization systems that prioritize factual consistency. This survey aims to provide a comprehensive overview of these fact-specific evaluation methods and text summarization models."}, "1701.01619": {"original_text": "  We present an approach to effectively use millions of images with noisy\nannotations in conjunction with a small subset of cleanly-annotated images to\nlearn powerful image representations. One common approach to combine clean and\nnoisy data is to first pre-train a network using the large noisy dataset and\nthen fine-tune with the clean dataset. We show this approach does not fully\nleverage the information contained in the clean set. Thus, we demonstrate how\nto use the clean annotations to reduce the noise in the large dataset before\nfine-tuning the network using both the clean set and the full set with reduced\nnoise. The approach comprises a multi-task network that jointly learns to clean\nnoisy annotations and to accurately classify images. We evaluate our approach\non the recently released Open Images dataset, containing ~9 million images,\nmultiple annotations per image and over 6000 unique classes. For the small\nclean set of annotations we use a quarter of the validation set with ~40k\nimages. Our results demonstrate that the proposed approach clearly outperforms\ndirect fine-tuning across all major categories of classes in the Open Image\ndataset. Further, our approach is particularly effective for a large number of\nclasses with wide range of noise in annotations (20-80% false positive\nannotations).\n", "rewritten_text": "We propose a method to effectively utilize a large number of images with noisy annotations alongside a smaller set of accurately annotated images to develop strong image representations. Instead of the common practice of pre-training a network with the noisy dataset and then fine-tuning with the clean dataset, we suggest a different approach that leverages the clean annotations to reduce noise in the large dataset before fine-tuning. Our method involves a multi-task network that simultaneously cleans noisy annotations and classifies images accurately. We tested our approach on the Open Images dataset, which includes approximately 9 million images with multiple annotations and over 6000 unique classes. The clean annotations were derived from a quarter of the validation set, comprising around 40k images. Our results indicate that our proposed method outperforms direct fine-tuning across all major categories in the Open Images dataset, especially for classes with varying levels of noise in annotations (20-80% false positives)."}, "1905.04215": {"original_text": "  We study the problem of unsupervised domain adaptation which aims to adapt\nmodels trained on a labeled source domain to a completely unlabeled target\ndomain. Recently, the cluster assumption has been applied to unsupervised\ndomain adaptation and achieved strong performance. One critical factor in\nsuccessful training of the cluster assumption is to impose the\nlocally-Lipschitz constraint to the model. Existing methods only impose the\nlocally-Lipschitz constraint around the training points while miss the other\nareas, such as the points in-between training data. In this paper, we address\nthis issue by encouraging the model to behave linearly in-between training\npoints. We propose a new regularization method called Virtual Mixup Training\n(VMT), which is able to incorporate the locally-Lipschitz constraint to the\nareas in-between training data. Unlike the traditional mixup model, our method\nconstructs the combination samples without using the label information,\nallowing it to apply to unsupervised domain adaptation. The proposed method is\ngeneric and can be combined with most existing models such as the recent\nstate-of-the-art model called VADA. Extensive experiments demonstrate that VMT\nsignificantly improves the performance of VADA on six domain adaptation\nbenchmark datasets. For the challenging task of adapting MNIST to SVHN, VMT can\nimprove the accuracy of VADA by over 30\\%. Code is available at\n\\url{https://github.com/xudonmao/VMT}.\n", "rewritten_text": "We explore the issue of unsupervised domain adaptation, which involves adjusting models trained on a labeled source domain to an entirely unlabeled target domain. Recently, the cluster assumption has been utilized in unsupervised domain adaptation with great success. A key aspect of effectively implementing the cluster assumption is enforcing the locally-Lipschitz constraint on the model. However, existing methods typically only apply this constraint around the training points and overlook other areas, such as the points between training data. In this study, we tackle this challenge by promoting linear behavior of the model between training points. We introduce a novel regularization technique called Virtual Mixup Training (VMT), which integrates the locally-Lipschitz constraint into the areas between training data. Unlike traditional mixup models, our approach generates combination samples without relying on label information, making it suitable for unsupervised domain adaptation. This method is versatile and can be integrated with various existing models, including the state-of-the-art VADA model. Extensive experiments show that VMT significantly enhances the performance of VADA on six domain adaptation benchmark datasets. Particularly, when adapting MNIST to SVHN, VMT boosts VADA's accuracy by more than 30%. The code is accessible at \\url{https://github.com/xudonmao/VMT}."}, "2311.17590": {"original_text": "  Achieving high synchronization in the synthesis of realistic, speech-driven\ntalking head videos presents a significant challenge. Traditional Generative\nAdversarial Networks (GAN) struggle to maintain consistent facial identity,\nwhile Neural Radiance Fields (NeRF) methods, although they can address this\nissue, often produce mismatched lip movements, inadequate facial expressions,\nand unstable head poses. A lifelike talking head requires synchronized\ncoordination of subject identity, lip movements, facial expressions, and head\nposes. The absence of these synchronizations is a fundamental flaw, leading to\nunrealistic and artificial outcomes. To address the critical issue of\nsynchronization, identified as the \"devil\" in creating realistic talking heads,\nwe introduce SyncTalk. This NeRF-based method effectively maintains subject\nidentity, enhancing synchronization and realism in talking head synthesis.\nSyncTalk employs a Face-Sync Controller to align lip movements with speech and\ninnovatively uses a 3D facial blendshape model to capture accurate facial\nexpressions. Our Head-Sync Stabilizer optimizes head poses, achieving more\nnatural head movements. The Portrait-Sync Generator restores hair details and\nblends the generated head with the torso for a seamless visual experience.\nExtensive experiments and user studies demonstrate that SyncTalk outperforms\nstate-of-the-art methods in synchronization and realism. We recommend watching\nthe supplementary video: https://ziqiaopeng.github.io/synctalk\n", "rewritten_text": "Achieving precise synchronization in creating lifelike talking head videos driven by speech is a significant challenge. Traditional Generative Adversarial Networks (GAN) struggle to maintain consistent facial identity, while Neural Radiance Fields (NeRF) methods, although capable of addressing this issue, often result in mismatched lip movements, inadequate facial expressions, and unstable head poses. A realistic talking head necessitates the harmonious coordination of subject identity, lip movements, facial expressions, and head poses. The lack of such synchronization is a fundamental flaw that leads to artificial and unrealistic outcomes. To tackle this critical synchronization issue, referred to as the \"devil\" in the creation of realistic talking heads, we introduce SyncTalk. This NeRF-based approach effectively preserves subject identity, enhancing synchronization and realism in talking head synthesis. SyncTalk utilizes a Face-Sync Controller to synchronize lip movements with speech and employs a 3D facial blendshape model to capture precise facial expressions. Our Head-Sync Stabilizer optimizes head poses, resulting in more natural head movements. The Portrait-Sync Generator restores hair details and seamlessly integrates the generated head with the torso for a cohesive visual experience. Extensive experiments and user studies demonstrate that SyncTalk surpasses current methods in terms of synchronization and realism. We recommend viewing the supplementary video at: https://ziqiaopeng.github.io/synctalk"}, "1901.07656": {"original_text": "  Word vector representations are well developed tools for various NLP and\nMachine Learning tasks and are known to retain significant semantic and\nsyntactic structure of languages. But they are prone to carrying and amplifying\nbias which can perpetrate discrimination in various applications. In this work,\nwe explore new simple ways to detect the most stereotypically gendered words in\nan embedding and remove the bias from them. We verify how names are masked\ncarriers of gender bias and then use that as a tool to attenuate bias in\nembeddings. Further, we extend this property of names to show how names can be\nused to detect other types of bias in the embeddings such as bias based on\nrace, ethnicity, and age.\n", "rewritten_text": "\"Word vector representations are powerful tools widely used in NLP and Machine Learning tasks, as they capture the semantic and syntactic structures of languages. However, they are susceptible to bias, which can lead to discrimination in various applications. This study introduces new methods to identify and mitigate gender bias in word embeddings. By examining the gendered nature of names, we demonstrate how bias can be reduced in embeddings. Additionally, we show how names can be leveraged to detect and address biases related to race, ethnicity, and age.\""}, "2210.02318": {"original_text": "  Recently, two-stage Deformable DETR introduced the query-based two-stage\nhead, a new type of two-stage head different from the region-based two-stage\nheads of classical detectors as Faster R-CNN. In query-based two-stage heads,\nthe second stage selects one feature per detection processed by a transformer,\ncalled the query, as opposed to pooling a rectangular grid of features\nprocessed by CNNs as in region-based detectors. In this work, we improve the\nquery-based head by improving the prior of the cross-attention operation with\nanchors, significantly speeding up the convergence while increasing its\nperformance. Additionally, we empirically show that by improving the\ncross-attention prior, auxiliary losses and iterative bounding box mechanisms\ntypically used by DETR-based detectors are no longer needed. By combining the\nbest of both the classical and the DETR-based detectors, our FQDet head peaks\nat 45.4 AP on the 2017 COCO validation set when using a ResNet-50+TPN backbone,\nonly after training for 12 epochs using the 1x schedule. We outperform other\nhigh-performing two-stage heads such as e.g. Cascade R-CNN, while using the\nsame backbone and while being computationally cheaper. Additionally, when using\nthe large ResNeXt-101-DCN+TPN backbone and multi-scale testing, our FQDet head\nachieves 52.9 AP on the 2017 COCO test-dev set after only 12 epochs of\ntraining. Code is released at https://github.com/CedricPicron/FQDet .\n", "rewritten_text": "Recently, a new type of two-stage head called query-based two-stage head was introduced in the two-stage Deformable DETR model. This differs from the region-based two-stage heads seen in traditional detectors like Faster R-CNN. In query-based two-stage heads, the second stage selects a single feature per detection processed by a transformer, known as the query, instead of pooling a grid of features processed by CNNs as in region-based detectors. This study enhances the query-based head by improving the cross-attention operation with anchors, leading to faster convergence and improved performance. It is demonstrated that with this enhancement, auxiliary losses and iterative bounding box mechanisms commonly used in DETR-based detectors are no longer necessary. By combining features from classical and DETR-based detectors, the FQDet head achieves a peak performance of 45.4 AP on the 2017 COCO validation set using a ResNet-50+TPN backbone after 12 epochs of training on a 1x schedule. This outperforms other two-stage heads like Cascade R-CNN while maintaining computational efficiency. Furthermore, with a larger ResNeXt-101-DCN+TPN backbone and multi-scale testing, the FQDet head achieves 52.9 AP on the 2017 COCO test-dev set after only 12 epochs of training. The code is available at https://github.com/CedricPicron/FQDet."}, "1411.634": {"original_text": "  While widely acknowledged as highly effective in computer vision, multi-label\nMRFs with non-convex priors are difficult to optimize. To tackle this, we\nintroduce an algorithm that iteratively approximates the original energy with\nan appropriately weighted surrogate energy that is easier to minimize. Our\nalgorithm guarantees that the original energy decreases at each iteration. In\nparticular, we consider the scenario where the global minimizer of the weighted\nsurrogate energy can be obtained by a multi-label graph cut algorithm, and show\nthat our algorithm then lets us handle of large variety of non-convex priors.\nWe demonstrate the benefits of our method over state-of-the-art MRF energy\nminimization techniques on stereo and inpainting problems.\n", "rewritten_text": "\"While widely recognized for their effectiveness in computer vision, multi-label Markov Random Fields (MRFs) with non-convex priors pose challenges in optimization. To address this issue, we propose an algorithm that progressively approximates the original energy function with a suitably weighted surrogate energy function that is easier to minimize. Our algorithm ensures a decrease in the original energy function at each iteration. Specifically, we focus on a scenario where the global minimum of the weighted surrogate energy function can be found using a multi-label graph cut algorithm, enabling us to handle a wide range of non-convex priors. We showcase the advantages of our approach compared to existing MRF energy minimization methods in stereo and inpainting tasks.\""}, "2009.02649": {"original_text": "  Causality visualization can help people understand temporal chains of events,\nsuch as messages sent in a distributed system, cause and effect in a historical\nconflict, or the interplay between political actors over time. However, as the\nscale and complexity of these event sequences grows, even these visualizations\ncan become overwhelming to use. In this paper, we propose the use of textual\nnarratives as a data-driven storytelling method to augment causality\nvisualization. We first propose a design space for how textual narratives can\nbe used to describe causal data. We then present results from a crowdsourced\nuser study where participants were asked to recover causality information from\ntwo causality visualizations--causal graphs and Hasse diagrams--with and\nwithout an associated textual narrative. Finally, we describe CAUSEWORKS, a\ncausality visualization system for understanding how specific interventions\ninfluence a causal model. The system incorporates an automatic textual\nnarrative mechanism based on our design space. We validate CAUSEWORKS through\ninterviews with experts who used the system for understanding complex events.\n", "rewritten_text": "The visualization of causality can aid in comprehending the chronological order of events, such as messages transmitted in a distributed system, cause and effect in historical conflicts, or the interactions among political figures over time. However, as the complexity and scope of these event sequences increase, even these visual representations can become overwhelming. In this paper, we suggest utilizing textual narratives as a data-driven storytelling technique to enhance causality visualization. We outline a framework for how textual narratives can be employed to depict causal data, and share findings from a user study where participants were tasked with interpreting causality information from various visualizations with and without accompanying textual narratives. Additionally, we introduce CAUSEWORKS, a causality visualization system designed to elucidate how specific interventions impact a causal model. This system includes an automated textual narrative feature based on our framework. We validate CAUSEWORKS through interviews with experts who utilized the system to analyze complex events."}, "2204.07183": {"original_text": "  We propose an interactive approach for 3D instance segmentation, where users\ncan iteratively collaborate with a deep learning model to segment objects in a\n3D point cloud directly. Current methods for 3D instance segmentation are\ngenerally trained in a fully-supervised fashion, which requires large amounts\nof costly training labels, and does not generalize well to classes unseen\nduring training. Few works have attempted to obtain 3D segmentation masks using\nhuman interactions. Existing methods rely on user feedback in the 2D image\ndomain. As a consequence, users are required to constantly switch between 2D\nimages and 3D representations, and custom architectures are employed to combine\nmultiple input modalities. Therefore, integration with existing standard 3D\nmodels is not straightforward. The core idea of this work is to enable users to\ninteract directly with 3D point clouds by clicking on desired 3D objects of\ninterest~(or their background) to interactively segment the scene in an\nopen-world setting. Specifically, our method does not require training data\nfrom any target domain, and can adapt to new environments where no appropriate\ntraining sets are available. Our system continuously adjusts the object\nsegmentation based on the user feedback and achieves accurate dense 3D\nsegmentation masks with minimal human effort (few clicks per object). Besides\nits potential for efficient labeling of large-scale and varied 3D datasets, our\napproach, where the user directly interacts with the 3D environment, enables\nnew applications in AR/VR and human-robot interaction.\n", "rewritten_text": "We present an interactive method for 3D instance segmentation that allows users to collaborate with a deep learning model to segment objects in a 3D point cloud. Traditional approaches to 3D instance segmentation rely on fully-supervised training, necessitating extensive labeled data and struggling to generalize to unseen classes. Limited efforts have been made to involve human interactions in obtaining 3D segmentation masks, with existing methods relying on user feedback in 2D images. This leads to frequent switching between 2D images and 3D representations, requiring custom architectures to merge different input modalities and complicating integration with standard 3D models. Our key concept is to empower users to interact directly with 3D point clouds by selecting desired objects to segment the scene interactively in an open-world scenario. Our method does not rely on training data from a specific domain and can adapt to new environments lacking appropriate training sets. By continuously adjusting object segmentation based on user feedback, our system achieves precise dense 3D segmentation masks with minimal human input (few clicks per object). In addition to its potential for efficiently labeling large and diverse 3D datasets, our approach, where users engage directly with the 3D environment, opens up new possibilities in AR/VR and human-robot interaction."}, "2012.0536": {"original_text": "  Semantic aware reconstruction is more advantageous than geometric-only\nreconstruction for future robotic and AR/VR applications because it represents\nnot only where things are, but also what things are. Object-centric mapping is\na task to build an object-level reconstruction where objects are separate and\nmeaningful entities that convey both geometry and semantic information. In this\npaper, we present MOLTR, a solution to object-centric mapping using only\nmonocular image sequences and camera poses. It is able to localise, track, and\nreconstruct multiple objects in an online fashion when an RGB camera captures a\nvideo of the surrounding. Given a new RGB frame, MOLTR firstly applies a\nmonocular 3D detector to localise objects of interest and extract their shape\ncodes that represent the object shapes in a learned embedding space. Detections\nare then merged to existing objects in the map after data association. Motion\nstate (i.e. kinematics and the motion status) of each object is tracked by a\nmultiple model Bayesian filter and object shape is progressively refined by\nfusing multiple shape code. We evaluate localisation, tracking, and\nreconstruction on benchmarking datasets for indoor and outdoor scenes, and show\nsuperior performance over previous approaches.\n", "rewritten_text": "Semantic-aware reconstruction offers more benefits compared to geometric-only reconstruction for upcoming robotic and AR/VR applications. This is because it not only indicates the location of objects but also provides information about their identity. Object-centric mapping involves creating a reconstruction at the object level, where objects are treated as distinct entities conveying both geometric and semantic details. In this study, we introduce MOLTR, a method for object-centric mapping that utilizes monocular image sequences and camera positions. MOLTR can identify, track, and reconstruct multiple objects in real-time using an RGB camera capturing the surroundings. When presented with a new RGB frame, MOLTR employs a monocular 3D detector to locate objects of interest and extract their shape codes, representing their shapes in a learned embedding space. The detections are then integrated into the existing map through data association. The motion state of each object is tracked using a multiple model Bayesian filter, and the object shape is refined by combining multiple shape codes. We assess the performance of localisation, tracking, and reconstruction on benchmark datasets for indoor and outdoor environments, demonstrating superior results compared to previous methods."}, "1605.06052": {"original_text": "  Similarity scores in face recognition represent the proximity between pairs\nof images as computed by a matching algorithm. Given a large set of images and\nthe proximities between all pairs, a similarity score space is defined. Cluster\nanalysis was applied to the similarity score space to develop various\ntaxonomies. Given the number of subjects in the dataset, we used hierarchical\nmethods to aggregate images of the same subject. We also explored the hierarchy\nabove and below the subject level, including clusters that reflect gender and\nethnicity. Evidence supports the existence of clustering by race, gender,\nsubject, and illumination condition.\n", "rewritten_text": "The similarity scores in face recognition indicate the closeness between image pairs calculated by a matching algorithm. By analyzing a vast collection of images and their proximities, a similarity score space is established. Cluster analysis was utilized on this space to create different taxonomies. To group images of the same subject based on the dataset size, hierarchical methods were employed. Additionally, investigations were conducted on hierarchies beyond and below the subject level, encompassing clusters related to gender and ethnicity. Research findings suggest the presence of clusters based on race, gender, subject, and lighting conditions."}, "2401.00653": {"original_text": "  Deceptive images can be shared in seconds with social networking services,\nposing substantial risks. Tampering traces, such as boundary artifacts and\nhigh-frequency information, have been significantly emphasized by massive\nnetworks in the Image Manipulation Localization (IML) field. However, they are\nprone to image post-processing operations, which limit the generalization and\nrobustness of existing methods. We present a novel Prompt-IML framework. We\nobserve that humans tend to discern the authenticity of an image based on both\nsemantic and high-frequency information, inspired by which, the proposed\nframework leverages rich semantic knowledge from pre-trained visual foundation\nmodels to assist IML. We are the first to design a framework that utilizes\nvisual foundation models specially for the IML task. Moreover, we design a\nFeature Alignment and Fusion module to align and fuse features of semantic\nfeatures with high-frequency features, which aims at locating tampered regions\nfrom multiple perspectives. Experimental results demonstrate that our model can\nachieve better performance on eight typical fake image datasets and outstanding\nrobustness.\n", "rewritten_text": "\"Sharing deceptive images quickly through social networking platforms can pose significant risks. The Image Manipulation Localization (IML) field has focused on identifying tampering traces like boundary artifacts and high-frequency information, but these methods are often limited by post-processing operations. Introducing a new Prompt-IML framework, we leverage human perception of image authenticity based on semantic and high-frequency information. Our framework utilizes rich semantic knowledge from pre-trained visual models to enhance IML accuracy, a unique approach in the field. Additionally, we introduce a Feature Alignment and Fusion module to combine semantic and high-frequency features for improved tampered region detection. Experimental results show that our model outperforms existing methods on various fake image datasets and exhibits exceptional robustness.\""}, "1809.08495": {"original_text": "  Earlier work demonstrates the promise of deep-learning-based approaches for\npoint cloud segmentation; however, these approaches need to be improved to be\npractically useful. To this end, we introduce a new model SqueezeSegV2 that is\nmore robust to dropout noise in LiDAR point clouds. With improved model\nstructure, training loss, batch normalization and additional input channel,\nSqueezeSegV2 achieves significant accuracy improvement when trained on real\ndata. Training models for point cloud segmentation requires large amounts of\nlabeled point-cloud data, which is expensive to obtain. To sidestep the cost of\ncollection and annotation, simulators such as GTA-V can be used to create\nunlimited amounts of labeled, synthetic data. However, due to domain shift,\nmodels trained on synthetic data often do not generalize well to the real\nworld. We address this problem with a domain-adaptation training pipeline\nconsisting of three major components: 1) learned intensity rendering, 2)\ngeodesic correlation alignment, and 3) progressive domain calibration. When\ntrained on real data, our new model exhibits segmentation accuracy improvements\nof 6.0-8.6% over the original SqueezeSeg. When training our new model on\nsynthetic data using the proposed domain adaptation pipeline, we nearly double\ntest accuracy on real-world data, from 29.0% to 57.4%. Our source code and\nsynthetic dataset will be open-sourced.\n", "rewritten_text": "Previous research has shown the potential of utilizing deep learning methods for segmenting point clouds, but there is a need for enhancements to make these methods more practical. In response, we present a novel model called SqueezeSegV2, designed to better handle dropout noise in LiDAR point clouds. By refining the model's structure, training loss, batch normalization, and incorporating an additional input channel, SqueezeSegV2 demonstrates a significant increase in accuracy when trained on actual data. The process of training models for point cloud segmentation demands a substantial amount of labeled point-cloud data, which can be costly to acquire. To mitigate this expense, simulators like GTA-V can generate abundant labeled synthetic data. However, models trained on synthetic data often struggle to generalize to real-world scenarios due to domain discrepancies. To tackle this issue, we introduce a domain-adaptation training approach comprising three key elements: 1) learned intensity rendering, 2) geodesic correlation alignment, and 3) progressive domain calibration. When trained on real data, our new model showcases a 6.0-8.6% enhancement in segmentation accuracy compared to the original SqueezeSeg. Furthermore, training our new model on synthetic data using the proposed domain adaptation pipeline nearly doubles the test accuracy on real-world data, increasing from 29.0% to 57.4%. We plan to release our source code and synthetic dataset to the public."}, "2209.11214": {"original_text": "  Automatic tomato disease recognition from leaf images is vital to avoid crop\nlosses by applying control measures on time. Even though recent deep\nlearning-based tomato disease recognition methods with classical training\nprocedures showed promising recognition results, they demand large labelled\ndata and involve expensive training. The traditional deep learning models\nproposed for tomato disease recognition also consume high memory and storage\nbecause of a high number of parameters. While lightweight networks overcome\nsome of these issues to a certain extent, they continue to show low performance\nand struggle to handle imbalanced data. In this paper, a novel Siamese\nnetwork-based lightweight framework is proposed for automatic tomato leaf\ndisease recognition. This framework achieves the highest accuracy of 96.97% on\nthe tomato subset obtained from the PlantVillage dataset and 95.48% on the\nTaiwan tomato leaf disease dataset. Experimental results further confirm that\nthe proposed framework is effective with imbalanced and small data. The\nbackbone deep network integrated with this framework is lightweight with\napproximately 2.9629 million trainable parameters, which is way lower than\nexisting lightweight deep networks.\n", "rewritten_text": "Recognition of tomato diseases from leaf images using automated methods is crucial for preventing crop losses through timely application of control measures. While recent deep learning approaches have shown promise in this area, they often require large labeled datasets and costly training procedures. Traditional deep learning models for tomato disease recognition can be memory-intensive due to a high number of parameters. Although lightweight networks have attempted to address these challenges, they have struggled with performance and imbalanced data. This paper introduces a novel lightweight framework based on Siamese networks for automatic tomato leaf disease recognition, achieving high accuracy rates of 96.97% on the PlantVillage dataset subset and 95.48% on the Taiwan tomato leaf disease dataset. Experimental results demonstrate the effectiveness of this framework with imbalanced and limited data. The integrated deep network backbone of this framework is lightweight, with approximately 2.9629 million trainable parameters, significantly fewer than existing lightweight deep networks."}, "1912.01496": {"original_text": "  Stories are diverse and highly personalized, resulting in a large possible\noutput space for story generation. Existing end-to-end approaches produce\nmonotonous stories because they are limited to the vocabulary and knowledge in\na single training dataset. This paper introduces KG-Story, a three-stage\nframework that allows the story generation model to take advantage of external\nKnowledge Graphs to produce interesting stories. KG-Story distills a set of\nrepresentative words from the input prompts, enriches the word set by using\nexternal knowledge graphs, and finally generates stories based on the enriched\nword set. This distill-enrich-generate framework allows the use of external\nresources not only for the enrichment phase, but also for the distillation and\ngeneration phases. In this paper, we show the superiority of KG-Story for\nvisual storytelling, where the input prompt is a sequence of five photos and\nthe output is a short story. Per the human ranking evaluation, stories\ngenerated by KG-Story are on average ranked better than that of the\nstate-of-the-art systems. Our code and output stories are available at\nhttps://github.com/zychen423/KE-VIST.\n", "rewritten_text": "The diversity and personalization of stories lead to a wide range of possibilities for story generation. Current end-to-end methods often result in repetitive stories due to their reliance on a single training dataset's vocabulary and knowledge. This study presents KG-Story, a three-stage framework that leverages external Knowledge Graphs to create engaging stories. KG-Story first extracts key words from input prompts, then enhances this word set using external knowledge graphs, and finally generates stories based on the enriched vocabulary. By incorporating external resources throughout the process, KG-Story demonstrates superior performance in visual storytelling compared to existing systems, as confirmed by human evaluation. The code and output stories can be accessed at https://github.com/zychen423/KE-VIST."}, "2307.16825": {"original_text": "  With sufficient paired training samples, the supervised deep learning methods\nhave attracted much attention in image denoising because of their superior\nperformance. However, it is still very challenging to widely utilize the\nsupervised methods in real cases due to the lack of paired noisy-clean images.\nMeanwhile, most self-supervised denoising methods are ineffective as well when\napplied to the real-world denoising tasks because of their strict assumptions\nin applications. For example, as a typical method for self-supervised\ndenoising, the original blind spot network (BSN) assumes that the noise is\npixel-wise independent, which is much different from the real cases. To solve\nthis problem, we propose a novel self-supervised real image denoising framework\nnamed Sampling Difference As Perturbation (SDAP) based on Random Sub-samples\nGeneration (RSG) with a cyclic sample difference loss. Specifically, we dig\ndeeper into the properties of BSN to make it more suitable for real noise.\nSurprisingly, we find that adding an appropriate perturbation to the training\nimages can effectively improve the performance of BSN. Further, we propose that\nthe sampling difference can be considered as perturbation to achieve better\nresults. Finally we propose a new BSN framework in combination with our RSG\nstrategy. The results show that it significantly outperforms other\nstate-of-the-art self-supervised denoising methods on real-world datasets. The\ncode is available at https://github.com/p1y2z3/SDAP.\n", "rewritten_text": "\"Supervised deep learning methods for image denoising have gained attention due to their high performance with sufficient paired training samples. However, the widespread use of supervised methods in real-world scenarios is still challenging due to the limited availability of paired noisy-clean images. Additionally, many self-supervised denoising methods are ineffective in practical denoising tasks because of their strict assumptions. For instance, the blind spot network (BSN) assumes pixel-wise independent noise, which differs from real-world scenarios. To address this issue, we introduce a new self-supervised real image denoising framework called Sampling Difference As Perturbation (SDAP) based on Random Sub-samples Generation (RSG) with a cyclic sample difference loss. By enhancing the properties of BSN to better suit real noise, we discovered that adding appropriate perturbations to training images can enhance BSN performance. We propose using sampling difference as a perturbation to achieve improved results and present a new BSN framework combined with our RSG strategy. Our results demonstrate that this approach surpasses other state-of-the-art self-supervised denoising methods on real-world datasets. The code can be accessed at https://github.com/p1y2z3/SDAP.\""}, "2409.16685": {"original_text": "  Integrating aerial imagery-based scene generation into applications like\nautonomous driving and gaming enhances realism in 3D environments, but\nchallenges remain in creating detailed content for occluded areas and ensuring\nreal-time, consistent rendering. In this paper, we introduce Skyeyes, a novel\nframework that can generate photorealistic sequences of ground view images\nusing only aerial view inputs, thereby creating a ground roaming experience.\nMore specifically, we combine a 3D representation with a view consistent\ngeneration model, which ensures coherence between generated images. This method\nallows for the creation of geometrically consistent ground view images, even\nwith large view gaps. The images maintain improved spatial-temporal coherence\nand realism, enhancing scene comprehension and visualization from aerial\nperspectives. To the best of our knowledge, there are no publicly available\ndatasets that contain pairwise geo-aligned aerial and ground view imagery.\nTherefore, we build a large, synthetic, and geo-aligned dataset using Unreal\nEngine. Both qualitative and quantitative analyses on this synthetic dataset\ndisplay superior results compared to other leading synthesis approaches. See\nthe project page for more results:\nhttps://chaoren2357.github.io/website-skyeyes/.\n", "rewritten_text": "Incorporating aerial imagery-based scene generation into applications such as autonomous driving and gaming elevates the realism of 3D environments. However, challenges persist in creating detailed content for hidden areas and ensuring real-time, consistent rendering. This paper introduces Skyeyes, a new framework that can produce photorealistic sequences of ground view images using only aerial view inputs, offering a ground roaming experience. By combining a 3D representation with a view-consistent generation model, coherence between generated images is maintained, enabling the creation of geometrically consistent ground view images even with significant view gaps. These images exhibit enhanced spatial-temporal coherence and realism, improving scene comprehension and visualization from aerial perspectives. Notably, there are currently no publicly available datasets containing paired geo-aligned aerial and ground view imagery. To address this gap, we have constructed a large, synthetic, and geo-aligned dataset using Unreal Engine. Both qualitative and quantitative analyses on this synthetic dataset demonstrate superior results compared to other leading synthesis approaches. For additional results, please visit the project page at: https://chaoren2357.github.io/website-skyeyes/."}, "2210.02844": {"original_text": "  In this paper, we explore the following question: Are synonym substitution\nattacks really synonym substitution attacks (SSAs)? We approach this question\nby examining how SSAs replace words in the original sentence and show that\nthere are still unresolved obstacles that make current SSAs generate invalid\nadversarial samples. We reveal that four widely used word substitution methods\ngenerate a large fraction of invalid substitution words that are ungrammatical\nor do not preserve the original sentence's semantics. Next, we show that the\nsemantic and grammatical constraints used in SSAs for detecting invalid word\nreplacements are highly insufficient in detecting invalid adversarial samples.\n", "rewritten_text": "In this paper, we investigate whether synonym substitution attacks (SSAs) truly qualify as such. Our approach involves analyzing how SSAs replace words in the original sentence and uncovering persistent challenges that prevent current SSAs from creating valid adversarial samples. We find that four commonly employed word substitution techniques often produce a significant number of invalid replacement words that are either ungrammatical or fail to maintain the original sentence's meaning. Furthermore, we demonstrate that the semantic and grammatical criteria employed in SSAs to identify improper word substitutions are largely inadequate for detecting invalid adversarial samples."}, "2307.09769": {"original_text": "  Unsupervised domain adaptation (UDA) has increasingly gained interests for\nits capacity to transfer the knowledge learned from a labeled source domain to\nan unlabeled target domain. However, typical UDA methods require concurrent\naccess to both the source and target domain data, which largely limits its\napplication in medical scenarios where source data is often unavailable due to\nprivacy concern. To tackle the source data-absent problem, we present a novel\ntwo-stage source-free domain adaptation (SFDA) framework for medical image\nsegmentation, where only a well-trained source segmentation model and unlabeled\ntarget data are available during domain adaptation. Specifically, in the\nprototype-anchored feature alignment stage, we first utilize the weights of the\npre-trained pixel-wise classifier as source prototypes, which preserve the\ninformation of source features. Then, we introduce the bi-directional transport\nto align the target features with class prototypes by minimizing its expected\ncost. On top of that, a contrastive learning stage is further devised to\nutilize those pixels with unreliable predictions for a more compact target\nfeature distribution. Extensive experiments on a cross-modality medical\nsegmentation task demonstrate the superiority of our method in large domain\ndiscrepancy settings compared with the state-of-the-art SFDA approaches and\neven some UDA methods. Code is available at\nhttps://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) has been increasingly attracting attention for its ability to transfer knowledge acquired from a labeled source domain to an unlabeled target domain. However, traditional UDA methods typically require simultaneous access to both the source and target domain data, which greatly restricts their use in medical scenarios where source data is often unavailable due to privacy concerns. To address the issue of missing source data, we propose a new two-stage source-free domain adaptation (SFDA) framework for medical image segmentation. This framework only requires a well-trained source segmentation model and unlabeled target data during the domain adaptation process. In the first stage, called prototype-anchored feature alignment, we use the weights of the pre-trained pixel-wise classifier as source prototypes to retain source feature information. Subsequently, we introduce bi-directional transport to align target features with class prototypes by minimizing the expected cost. Additionally, a contrastive learning stage is implemented to leverage pixels with unreliable predictions for a more condensed target feature distribution. Extensive experiments on a cross-modality medical segmentation task demonstrate the effectiveness of our approach in scenarios with significant domain discrepancies compared to current SFDA methods and even some UDA methods. The code can be found at https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA."}, "1811.08015": {"original_text": "  This paper introduces the problem of automatic font pairing. Font pairing is\nan important design task that is difficult for novices. Given a font selection\nfor one part of a document (e.g., header), our goal is to recommend a font to\nbe used in another part (e.g., body) such that the two fonts used together look\nvisually pleasing. There are three main challenges in font pairing. First, this\nis a fine-grained problem, in which the subtle distinctions between fonts may\nbe important. Second, rules and conventions of font pairing given by human\nexperts are difficult to formalize. Third, font pairing is an asymmetric\nproblem in that the roles played by header and body fonts are not\ninterchangeable. To address these challenges, we propose automatic font pairing\nthrough learning visual relationships from large-scale human-generated font\npairs. We introduce a new database for font pairing constructed from millions\nof PDF documents available on the Internet. We propose two font pairing\nalgorithms: dual-space k-NN and asymmetric similarity metric learning (ASML).\nThese two methods automatically learn fine-grained relationships from\nlarge-scale data. We also investigate several baseline methods based on the\nrules from professional designers. Experiments and user studies demonstrate the\neffectiveness of our proposed dataset and methods.\n", "rewritten_text": "This paper discusses the issue of automatic font pairing, which is a challenging design task for beginners. The goal is to recommend a font for one part of a document based on the font selected for another part, ensuring that the combination looks visually appealing. Font pairing presents three main challenges: it is a detailed problem requiring attention to subtle font differences, formalizing rules and conventions set by experts is complex, and the roles of header and body fonts are not interchangeable. To tackle these challenges, the paper proposes automatic font pairing by learning visual relationships from a vast database of human-generated font pairs. A new font pairing database is introduced, created from millions of PDF documents online. Two font pairing algorithms are presented: dual-space k-NN and asymmetric similarity metric learning (ASML), which learn relationships from extensive data. Additionally, baseline methods based on professional designers' rules are explored. Experiments and user studies confirm the effectiveness of the proposed dataset and methods."}, "2403.15119": {"original_text": "  Person re-identification (ReID) has made great strides thanks to the\ndata-driven deep learning techniques. However, the existing benchmark datasets\nlack diversity, and models trained on these data cannot generalize well to\ndynamic wild scenarios. To meet the goal of improving the explicit\ngeneralization of ReID models, we develop a new Open-World, Diverse,\nCross-Spatial-Temporal dataset named OWD with several distinct features. 1)\nDiverse collection scenes: multiple independent open-world and highly dynamic\ncollecting scenes, including streets, intersections, shopping malls, etc. 2)\nDiverse lighting variations: long time spans from daytime to nighttime with\nabundant illumination changes. 3) Diverse person status: multiple camera\nnetworks in all seasons with normal/adverse weather conditions and diverse\npedestrian appearances (e.g., clothes, personal belongings, poses, etc.). 4)\nProtected privacy: invisible faces for privacy critical applications. To\nimprove the implicit generalization of ReID, we further propose a Latent Domain\nExpansion (LDE) method to develop the potential of source data, which decouples\ndiscriminative identity-relevant and trustworthy domain-relevant features and\nimplicitly enforces domain-randomized identity feature space expansion with\nricher domain diversity to facilitate domain invariant representations. Our\ncomprehensive evaluations with most benchmark datasets in the community are\ncrucial for progress, although this work is far from the grand goal toward\nopen-world and dynamic wild applications.\n", "rewritten_text": "Advancements in person re-identification (ReID) have been significant due to the utilization of data-driven deep learning techniques. However, current benchmark datasets lack diversity, leading to models trained on these datasets struggling to generalize effectively in dynamic real-world scenarios. In order to enhance the explicit generalization capabilities of ReID models, we have introduced a novel dataset called OWD (Open-World, Diverse, Cross-Spatial-Temporal) with unique features. These include diverse collection scenes such as streets, intersections, and shopping malls, varied lighting conditions spanning from daytime to nighttime, diverse person statuses captured across different seasons and weather conditions, and privacy protection measures like obscured faces for sensitive applications. To further enhance the implicit generalization of ReID models, we have also proposed a method called Latent Domain Expansion (LDE) to unlock the potential of source data by separating identity-relevant and domain-relevant features and expanding the identity feature space with diverse domain representations. While our evaluations against existing benchmark datasets are essential for progress, there is still much work to be done to achieve the ultimate goal of open-world and dynamic real-world applications."}, "1906.03731": {"original_text": "  Attention mechanisms have recently boosted performance on a range of NLP\ntasks. Because attention layers explicitly weight input components'\nrepresentations, it is also often assumed that attention can be used to\nidentify information that models found important (e.g., specific contextualized\nword tokens). We test whether that assumption holds by manipulating attention\nweights in already-trained text classification models and analyzing the\nresulting differences in their predictions. While we observe some ways in which\nhigher attention weights correlate with greater impact on model predictions, we\nalso find many ways in which this does not hold, i.e., where gradient-based\nrankings of attention weights better predict their effects than their\nmagnitudes. We conclude that while attention noisily predicts input components'\noverall importance to a model, it is by no means a fail-safe indicator.\n", "rewritten_text": "Recently, attention mechanisms have significantly improved performance across various NLP tasks. These mechanisms explicitly prioritize input components' representations, leading to the assumption that attention can help identify crucial information for models, such as specific contextualized word tokens. To test this assumption, we manipulated attention weights in pre-trained text classification models and examined the resulting changes in their predictions. While we did find instances where higher attention weights corresponded to a greater impact on model predictions, we also discovered cases where this relationship did not hold true. In these situations, gradient-based rankings of attention weights were more indicative of their effects than their actual magnitudes. Our findings suggest that while attention can somewhat predict the overall importance of input components to a model, it is not a foolproof indicator."}, "1611.08134": {"original_text": "  Color based re-identification methods usually rely on a distance function to\nmeasure the similarity between individuals. In this paper we study the behavior\nof several histogram distance measures in different color spaces. We wonder\nwhether there is a particular histogram distance measure better than others,\nlikewise also, if there is a color space that present better discrimination\nfeatures. Several experiments are designed and evaluated in several images to\nobtain measures against various color spaces. We test in several image\ndatabases. A measure ranking is generated to calculate the area under the CMC,\nthis area is the indicator used to evaluate which distance measure and color\nspace present the best performance for the considered databases. Also, other\nparameters such as the image division in horizontal stripes and number of\nhistogram bins, have been studied.\n", "rewritten_text": "The study in this paper examines the effectiveness of different histogram distance measures in various color spaces for color-based re-identification methods. The research aims to determine if certain distance measures and color spaces outperform others in terms of discrimination capabilities. Multiple experiments are conducted using diverse images and databases to compare performance. A measure ranking is created to assess the area under the CMC curve, which indicates the optimal distance measure and color space for the databases. Additionally, factors like image segmentation and histogram bin numbers are also investigated."}, "2003.05065": {"original_text": "  For many of the physical phenomena around us, we have developed sophisticated\nmodels explaining their behavior. Nevertheless, measuring physical properties\nfrom visual observations is challenging due to the high number of causally\nunderlying physical parameters -- including material properties and external\nforces. In this paper, we propose to measure latent physical properties for\ncloth in the wind without ever having seen a real example before. Our solution\nis an iterative refinement procedure with simulation at its core. The algorithm\ngradually updates the physical model parameters by running a simulation of the\nobserved phenomenon and comparing the current simulation to a real-world\nobservation. The correspondence is measured using an embedding function that\nmaps physically similar examples to nearby points. We consider a case study of\ncloth in the wind, with curling flags as our leading example -- a seemingly\nsimple phenomena but physically highly involved. Based on the physics of cloth\nand its visual manifestation, we propose an instantiation of the embedding\nfunction. For this mapping, modeled as a deep network, we introduce a spectral\nlayer that decomposes a video volume into its temporal spectral power and\ncorresponding frequencies. Our experiments demonstrate that the proposed method\ncompares favorably to prior work on the task of measuring cloth material\nproperties and external wind force from a real-world video.\n", "rewritten_text": "\"We have developed advanced models to explain the behavior of many physical phenomena, but measuring physical properties from visual observations is difficult due to numerous underlying parameters. In this study, we suggest a method to measure hidden physical properties of cloth in the wind without prior examples. Our approach involves an iterative refinement process with simulation as the foundation. The algorithm updates physical model parameters by simulating the observed phenomenon and comparing it to real-world data. We use an embedding function to assess the similarity between simulations and real examples. Our focus is on cloth in the wind, particularly curling flags, which is a complex physical phenomenon. By leveraging the physics of cloth and its visual appearance, we propose a deep network-based embedding function that decomposes video data into spectral power and frequencies. Our experiments show that our method outperforms previous approaches in measuring cloth properties and wind force from real-world videos.\""}, "2408.14860": {"original_text": "  This paper presents DiffSurf, a transformer-based denoising diffusion model\nfor generating and reconstructing 3D surfaces. Specifically, we design a\ndiffusion transformer architecture that predicts noise from noisy 3D surface\nvertices and normals. With this architecture, DiffSurf is able to generate 3D\nsurfaces in various poses and shapes, such as human bodies, hands, animals and\nman-made objects. Further, DiffSurf is versatile in that it can address various\n3D downstream tasks including morphing, body shape variation and 3D human mesh\nfitting to 2D keypoints. Experimental results on 3D human model benchmarks\ndemonstrate that DiffSurf can generate shapes with greater diversity and higher\nquality than previous generative models. Furthermore, when applied to the task\nof single-image 3D human mesh recovery, DiffSurf achieves accuracy comparable\nto prior techniques at a near real-time rate.\n", "rewritten_text": "This paper introduces DiffSurf, a denoising diffusion model based on transformers for creating and reconstructing 3D surfaces. The model utilizes a diffusion transformer architecture to predict noise from noisy 3D surface vertices and normals. With this architecture, DiffSurf can produce 3D surfaces of various poses and shapes, such as human bodies, hands, animals, and man-made objects. Additionally, DiffSurf is adaptable and can be used for different 3D tasks like morphing, body shape variation, and fitting 3D human meshes to 2D keypoints. Experimental results on 3D human model benchmarks show that DiffSurf can generate shapes with more diversity and higher quality compared to previous generative models. Moreover, in the context of single-image 3D human mesh recovery, DiffSurf achieves accuracy similar to existing techniques at a nearly real-time speed."}, "2111.1341": {"original_text": "  Manual annotation of medical images is highly subjective, leading to\ninevitable and huge annotation biases. Deep learning models may surpass human\nperformance on a variety of tasks, but they may also mimic or amplify these\nbiases. Although we can have multiple annotators and fuse their annotations to\nreduce stochastic errors, we cannot use this strategy to handle the bias caused\nby annotators' preferences. In this paper, we highlight the issue of\nannotator-related biases on medical image segmentation tasks, and propose a\nPreference-involved Annotation Distribution Learning (PADL) framework to\naddress it from the perspective of disentangling an annotator's preference from\nstochastic errors using distribution learning so as to produce not only a meta\nsegmentation but also the segmentation possibly made by each annotator. Under\nthis framework, a stochastic error modeling (SEM) module estimates the meta\nsegmentation map and average stochastic error map, and a series of human\npreference modeling (HPM) modules estimate each annotator's segmentation and\nthe corresponding stochastic error. We evaluated our PADL framework on two\nmedical image benchmarks with different imaging modalities, which have been\nannotated by multiple medical professionals, and achieved promising performance\non all five medical image segmentation tasks.\n", "rewritten_text": "The manual labeling of medical images is subjective, resulting in significant annotation biases. While deep learning models can outperform humans in various tasks, they may also replicate or magnify these biases. Although combining annotations from multiple annotators can help reduce random errors, it does not address biases stemming from annotators' preferences. This paper discusses the impact of annotator-related biases on medical image segmentation and introduces a Preference-involved Annotation Distribution Learning (PADL) framework to tackle this issue. PADL aims to separate an annotator's preference from random errors through distribution learning, generating both a consolidated segmentation and individual annotator segmentations. The framework includes a Stochastic Error Modeling (SEM) module to estimate the consolidated segmentation and average error map, as well as Human Preference Modeling (HPM) modules to estimate each annotator's segmentation and corresponding error. Evaluation on two medical image benchmarks, annotated by multiple professionals across different imaging modalities, demonstrates promising performance across five segmentation tasks."}, "2312.06598": {"original_text": "  Early action recognition is an important and challenging problem that enables\nthe recognition of an action from a partially observed video stream where the\nactivity is potentially unfinished or even not started. In this work, we\npropose a novel model that learns a prototypical representation of the full\naction for each class and uses it to regularize the architecture and the visual\nrepresentations of the partial observations. Our model is very simple in design\nand also efficient. We decompose the video into short clips, where a visual\nencoder extracts features from each clip independently. Later, a decoder\naggregates together in an online fashion features from all the clips for the\nfinal class prediction. During training, for each partial observation, the\nmodel is jointly trained to both predict the label as well as the action\nprototypical representation which acts as a regularizer. We evaluate our method\non multiple challenging real-world datasets and outperform the current\nstate-of-the-art by a significant margin. For example, on early recognition\nobserving only the first 10% of each video, our method improves the SOTA by\n+2.23 Top-1 accuracy on Something-Something-v2, +3.55 on UCF-101, +3.68 on\nSSsub21, and +5.03 on EPIC-Kitchens-55, where prior work used either\nmulti-modal inputs (e.g. optical-flow) or batched inference. Finally, we also\npresent exhaustive ablation studies to motivate the design choices we made, as\nwell as gather insights regarding what our model is learning semantically.\n", "rewritten_text": "The recognition of actions in early stages is a significant and complex task that involves identifying actions from a video stream that may be incomplete or not yet started. In this study, we introduce a new model that learns a standard representation of the complete action for each category, which helps to refine the architecture and visual features of partially observed actions. Our model is straightforward and efficient, breaking down the video into short clips for individual feature extraction by a visual encoder. These features are then combined by a decoder in real-time for the final classification. During training, the model is trained to predict both the label and the action representation, acting as a regularizer. We tested our approach on various challenging real-world datasets and surpassed the current best performance by a significant margin. For instance, in early recognition using only the initial 10% of each video, our method improved Top-1 accuracy by +2.23 on Something-Something-v2, +3.55 on UCF-101, +3.68 on SSsub21, and +5.03 on EPIC-Kitchens-55 compared to prior methods that utilized multi-modal inputs or batched inference. Additionally, we conducted thorough ablation studies to justify our design decisions and gain insights into the semantic learning of our model."}, "2007.1562": {"original_text": "  Named Entity Recognition (NER) is a fundamental NLP task, commonly formulated\nas classification over a sequence of tokens. Morphologically-Rich Languages\n(MRLs) pose a challenge to this basic formulation, as the boundaries of Named\nEntities do not necessarily coincide with token boundaries, rather, they\nrespect morphological boundaries. To address NER in MRLs we then need to answer\ntwo fundamental questions, namely, what are the basic units to be labeled, and\nhow can these units be detected and classified in realistic settings, i.e.,\nwhere no gold morphology is available. We empirically investigate these\nquestions on a novel NER benchmark, with parallel tokenlevel and morpheme-level\nNER annotations, which we develop for Modern Hebrew, a morphologically\nrich-and-ambiguous language. Our results show that explicitly modeling\nmorphological boundaries leads to improved NER performance, and that a novel\nhybrid architecture, in which NER precedes and prunes morphological\ndecomposition, greatly outperforms the standard pipeline, where morphological\ndecomposition strictly precedes NER, setting a new performance bar for both\nHebrew NER and Hebrew morphological decomposition tasks.\n", "rewritten_text": "Named Entity Recognition (NER) is a key task in Natural Language Processing (NLP) that involves classifying entities within a sequence of tokens. Morphologically-Rich Languages (MRLs) present a challenge for NER due to the fact that Named Entities may not align perfectly with token boundaries, but rather follow morphological boundaries. To tackle NER in MRLs, we must address two key questions: what are the basic units to be labeled, and how can these units be identified and classified in real-world scenarios where gold standard morphology is not available. We conduct an empirical study on a new NER benchmark for Modern Hebrew, a language known for its complex morphology. Our findings demonstrate that explicitly considering morphological boundaries leads to enhanced NER performance. Additionally, a novel hybrid approach, where NER precedes and refines morphological decomposition, outperforms the traditional pipeline where morphological decomposition precedes NER. This sets a new benchmark for both Hebrew NER and morphological decomposition tasks."}, "2102.12452": {"original_text": "  Probing classifiers have emerged as one of the prominent methodologies for\ninterpreting and analyzing deep neural network models of natural language\nprocessing. The basic idea is simple -- a classifier is trained to predict some\nlinguistic property from a model's representations -- and has been used to\nexamine a wide variety of models and properties. However, recent studies have\ndemonstrated various methodological limitations of this approach. This article\ncritically reviews the probing classifiers framework, highlighting their\npromises, shortcomings, and advances.\n", "rewritten_text": "\"Probing classifiers have become a significant approach in understanding and evaluating deep neural network models in natural language processing. The concept is straightforward: a classifier is taught to forecast a linguistic feature based on a model's representations. This method has been applied to explore different models and features. Yet, recent research has revealed certain methodological constraints of this technique. This article provides a critical assessment of the probing classifiers framework, discussing its potential, drawbacks, and advancements.\""}, "2408.11567": {"original_text": "  Point cloud analysis has achieved significant development and is\nwell-performed in multiple downstream tasks like point cloud classification and\nsegmentation, etc. Being conscious of the simplicity of the position encoding\nstructure in Transformer-based architectures, we attach importance to the\nposition encoding as a high-dimensional part and the patch encoder to offer\nmulti-scale information. Together with the sequential Transformer, the whole\nmodule with position encoding comprehensively constructs a multi-scale feature\nabstraction module that considers both the local parts from the patch and the\nglobal parts from center points as position encoding. With only a few\nparameters, the position embedding module fits the setting of PEFT\n(Parameter-Efficient Fine-Tuning) tasks pretty well. Thus we unfreeze these\nparameters as a fine-tuning part. At the same time, we review the existing\nprompt and adapter tuning methods, proposing a fresh way of prompts and\nsynthesizing them with adapters as dynamic adjustments. Our Proposed method of\nPEFT tasks, namely PPT, with only 1.05% of parameters for training, gets\nstate-of-the-art results in several mainstream datasets, such as 95.01%\naccuracy in the ScanObjectNN OBJ_BG dataset. Codes will be released at\nhttps://github.com/zsc000722/PPT.\n", "rewritten_text": "The analysis of point clouds has made significant advancements and performs well in various tasks such as classification and segmentation. Recognizing the simplicity of position encoding in Transformer-based architectures, we emphasize the importance of position encoding as a high-dimensional component and the patch encoder to provide multi-scale information. By combining these elements with the sequential Transformer, a comprehensive multi-scale feature abstraction module is created, taking into account both local and global aspects. The position embedding module, with minimal parameters, is suitable for Parameter-Efficient Fine-Tuning (PEFT) tasks, allowing for fine-tuning of these parameters. Additionally, we introduce a novel approach to prompts and adapters tuning, known as PPT, which achieves state-of-the-art results with minimal parameters in various datasets. The code will be available at https://github.com/zsc000722/PPT."}, "1607.0003": {"original_text": "  Human evaluation of machine translation normally uses sentence-level measures\nsuch as relative ranking or adequacy scales. However, these provide no insight\ninto possible errors, and do not scale well with sentence length. We argue for\na semantics-based evaluation, which captures what meaning components are\nretained in the MT output, thus providing a more fine-grained analysis of\ntranslation quality, and enabling the construction and tuning of\nsemantics-based MT. We present a novel human semantic evaluation measure, Human\nUCCA-based MT Evaluation (HUME), building on the UCCA semantic representation\nscheme. HUME covers a wider range of semantic phenomena than previous methods\nand does not rely on semantic annotation of the potentially garbled MT output.\nWe experiment with four language pairs, demonstrating HUME's broad\napplicability, and report good inter-annotator agreement rates and correlation\nwith human adequacy scores.\n", "rewritten_text": "The typical way humans evaluate machine translation is by using sentence-level measures like relative ranking or adequacy scales. However, these methods do not provide insight into potential errors and are not effective for longer sentences. We propose a semantics-based evaluation approach that focuses on the meaning components preserved in the machine translation output. This allows for a more detailed analysis of translation quality and facilitates the development and refinement of semantics-based machine translation systems. Our new evaluation measure, Human UCCA-based MT Evaluation (HUME), is based on the UCCA semantic representation scheme. HUME covers a wider range of semantic aspects compared to previous methods and does not require semantic annotation of the machine translation output. We conducted experiments with four language pairs to demonstrate the versatility of HUME and found high inter-annotator agreement rates as well as correlation with human adequacy scores."}, "2205.0809": {"original_text": "  Event cameras are bio-inspired sensors that capture per-pixel asynchronous\nintensity change rather than the synchronous absolute intensity frames captured\nby a classical camera sensor. Such cameras are ideal for robotics applications\nsince they have high temporal resolution, high dynamic range and low latency.\nHowever, due to their high temporal resolution, event cameras are particularly\nsensitive to flicker such as from fluorescent or LED lights. During every cycle\nfrom bright to dark, pixels that image a flickering light source generate many\nevents that provide little or no useful information for a robot, swamping the\nuseful data in the scene. In this paper, we propose a novel linear filter to\npreprocess event data to remove unwanted flicker events from an event stream.\nThe proposed algorithm achieves over 4.6 times relative improvement in the\nsignal-to-noise ratio when compared to the raw event stream due to the\neffective removal of flicker from fluorescent lighting. Thus, it is ideally\nsuited to robotics applications that operate in indoor settings or scenes\nilluminated by flickering light sources.\n", "rewritten_text": "Event cameras are sensors inspired by biology that capture asynchronous changes in intensity per pixel, as opposed to the synchronous absolute intensity frames captured by traditional camera sensors. These cameras are well-suited for robotics applications due to their high temporal resolution, dynamic range, and low latency. However, their sensitivity to flicker, such as from fluorescent or LED lights, can be a challenge. Flickering light sources can generate excessive events during each bright-to-dark cycle, overwhelming the useful data in the scene. In this study, we introduce a new linear filter to preprocess event data and eliminate unwanted flicker events from the stream. Our algorithm significantly improves the signal-to-noise ratio by over 4.6 times compared to the raw event stream, effectively reducing flicker from fluorescent lighting. This makes it an excellent choice for robotics applications in indoor environments or scenes lit by flickering sources."}, "2205.00627": {"original_text": "  White matter fiber clustering is an important strategy for white matter\nparcellation, which enables quantitative analysis of brain connections in\nhealth and disease. In combination with expert neuroanatomical labeling,\ndata-driven white matter fiber clustering is a powerful tool for creating\natlases that can model white matter anatomy across individuals. While widely\nused fiber clustering approaches have shown good performance using classical\nunsupervised machine learning techniques, recent advances in deep learning\nreveal a promising direction toward fast and effective fiber clustering. In\nthis work, we propose a novel deep learning framework for white matter fiber\nclustering, Deep Fiber Clustering (DFC), which solves the unsupervised\nclustering problem as a self-supervised learning task with a domain-specific\npretext task to predict pairwise fiber distances. This process learns a\nhigh-dimensional embedding feature representation for each fiber, regardless of\nthe order of fiber points reconstructed during tractography. We design a novel\nnetwork architecture that represents input fibers as point clouds and allows\nthe incorporation of additional sources of input information from gray matter\nparcellation to improve anatomical coherence of clusters. In addition, DFC\nconducts outlier removal naturally by rejecting fibers with low cluster\nassignment probability. We evaluate DFC on three independently acquired\ncohorts, including data from 220 individuals across genders, ages (young and\nelderly adults), and different health conditions (healthy control and multiple\nneuropsychiatric disorders). We compare DFC to several state-of-the-art white\nmatter fiber clustering algorithms. Experimental results demonstrate superior\nperformance of DFC in terms of cluster compactness, generalization ability,\nanatomical coherence, and computational efficiency.\n", "rewritten_text": "Clustering white matter fibers is a key method for dividing white matter into sections, allowing for the analysis of brain connections in various health states. By combining expert labeling with data-driven techniques, white matter fiber clustering can create atlases that represent white matter structure across individuals. While traditional clustering methods have been effective, recent advancements in deep learning offer a faster and more efficient approach. This study introduces a new deep learning framework, Deep Fiber Clustering (DFC), which addresses the clustering challenge through self-supervised learning. DFC predicts fiber distances as a pretext task, generating high-dimensional representations for each fiber. The network architecture treats input fibers as point clouds and can incorporate additional information from gray matter parcellation to enhance anatomical coherence. DFC also naturally removes outliers by excluding fibers with low cluster assignment probability. The framework is evaluated on three distinct datasets, covering a range of individuals and health conditions. Comparative analysis with other clustering algorithms shows that DFC excels in terms of cluster quality, generalization, anatomical consistency, and computational efficiency."}, "1707.08401": {"original_text": "  In the last two decades Computer Aided Diagnostics (CAD) systems were\ndeveloped to help radiologists analyze screening mammograms. The benefits of\ncurrent CAD technologies appear to be contradictory and they should be improved\nto be ultimately considered useful. Since 2012 deep convolutional neural\nnetworks (CNN) have been a tremendous success in image recognition, reaching\nhuman performance. These methods have greatly surpassed the traditional\napproaches, which are similar to currently used CAD solutions. Deep CNN-s have\nthe potential to revolutionize medical image analysis. We propose a CAD system\nbased on one of the most successful object detection frameworks, Faster R-CNN.\nThe system detects and classifies malignant or benign lesions on a mammogram\nwithout any human intervention. The proposed method sets the state of the art\nclassification performance on the public INbreast database, AUC = 0.95 . The\napproach described here has achieved the 2nd place in the Digital Mammography\nDREAM Challenge with AUC = 0.85 . When used as a detector, the system reaches\nhigh sensitivity with very few false positive marks per image on the INbreast\ndataset. Source code, the trained model and an OsiriX plugin are availaible\nonline at https://github.com/riblidezso/frcnn_cad .\n", "rewritten_text": "Over the past two decades, Computer Aided Diagnostics (CAD) systems have been developed to assist radiologists in analyzing screening mammograms. The current CAD technologies have shown conflicting benefits and require enhancements to be truly effective. Since 2012, deep convolutional neural networks (CNN) have achieved remarkable success in image recognition, matching human performance levels. These advanced methods have outperformed traditional approaches, including existing CAD solutions. Deep CNNs hold the potential to transform medical image analysis. Our proposal introduces a CAD system based on the highly successful object detection framework, Faster R-CNN. This system autonomously detects and categorizes malignant or benign lesions on mammograms without human intervention. The method we present demonstrates cutting-edge classification performance on the publicly available INbreast database, with an AUC of 0.95. In the Digital Mammography DREAM Challenge, our approach secured the 2nd position with an AUC of 0.85. When utilized as a detector, the system achieves high sensitivity with minimal false positive identifications per image on the INbreast dataset. The source code, trained model, and an OsiriX plugin are accessible online at https://github.com/riblidezso/frcnn_cad."}, "1604.02975": {"original_text": "  We propose a novel Coupled Projection multi-task Metric Learning (CP-mtML)\nmethod for large scale face retrieval. In contrast to previous works which were\nlimited to low dimensional features and small datasets, the proposed method\nscales to large datasets with high dimensional face descriptors. It utilises\npairwise (dis-)similarity constraints as supervision and hence does not require\nexhaustive class annotation for every training image. While, traditionally,\nmulti-task learning methods have been validated on same dataset but different\ntasks, we work on the more challenging setting with heterogeneous datasets and\ndifferent tasks. We show empirical validation on multiple face image datasets\nof different facial traits, e.g. identity, age and expression. We use classic\nLocal Binary Pattern (LBP) descriptors along with the recent Deep Convolutional\nNeural Network (CNN) features. The experiments clearly demonstrate the\nscalability and improved performance of the proposed method on the tasks of\nidentity and age based face image retrieval compared to competitive existing\nmethods, on the standard datasets and with the presence of a million distractor\nface images.\n", "rewritten_text": "We introduce a new method called Coupled Projection multi-task Metric Learning (CP-mtML) for large-scale face retrieval. Unlike previous approaches that were limited to small datasets and low-dimensional features, our method is designed to handle large datasets with high-dimensional face descriptors. It uses pairwise similarity constraints as supervision, eliminating the need for exhaustive class annotation for each training image. While traditional multi-task learning methods are typically tested on the same dataset but different tasks, we tackle a more challenging scenario involving heterogeneous datasets and tasks. Our method is empirically validated on various face image datasets with different facial traits such as identity, age, and expression. We combine classic Local Binary Pattern (LBP) descriptors with modern Deep Convolutional Neural Network (CNN) features. Our experiments clearly show that our method outperforms existing approaches in terms of scalability and performance for identity and age-based face image retrieval tasks, even when faced with a million distractor face images in standard datasets."}, "1705.10872": {"original_text": "  We introduce a novel loss for learning local feature descriptors which is\ninspired by the Lowe's matching criterion for SIFT. We show that the proposed\nloss that maximizes the distance between the closest positive and closest\nnegative patch in the batch is better than complex regularization methods; it\nworks well for both shallow and deep convolution network architectures.\nApplying the novel loss to the L2Net CNN architecture results in a compact\ndescriptor -- it has the same dimensionality as SIFT (128) that shows\nstate-of-art performance in wide baseline stereo, patch verification and\ninstance retrieval benchmarks. It is fast, computing a descriptor takes about 1\nmillisecond on a low-end GPU.\n", "rewritten_text": "We present a new loss function for training local feature descriptors, inspired by Lowe's matching criterion for SIFT. Our proposed loss maximizes the distance between the nearest positive and negative patches in a batch, outperforming complex regularization techniques. It is effective for both shallow and deep convolutional network designs. When applied to the L2Net CNN architecture, this novel loss produces a compact descriptor with a dimensionality of 128, matching SIFT's performance in various benchmarks such as wide baseline stereo, patch verification, and instance retrieval. The descriptor computation is fast, taking approximately 1 millisecond on a low-end GPU."}, "2103.17107": {"original_text": "  In this paper, the multi-task learning of lightweight convolutional neural\nnetworks is studied for face identification and classification of facial\nattributes (age, gender, ethnicity) trained on cropped faces without margins.\nThe necessity to fine-tune these networks to predict facial expressions is\nhighlighted. Several models are presented based on MobileNet, EfficientNet and\nRexNet architectures. It was experimentally demonstrated that they lead to near\nstate-of-the-art results in age, gender and race recognition on the UTKFace\ndataset and emotion classification on the AffectNet dataset. Moreover, it is\nshown that the usage of the trained models as feature extractors of facial\nregions in video frames leads to 4.5% higher accuracy than the previously known\nstate-of-the-art single models for the AFEW and the VGAF datasets from the\nEmotiW challenges. The models and source code are publicly available at\nhttps://github.com/HSE-asavchenko/face-emotion-recognition.\n", "rewritten_text": "This paper explores the multi-task learning of lightweight convolutional neural networks for face identification and classification of facial attributes (age, gender, ethnicity) using cropped faces without margins. The importance of fine-tuning these networks for predicting facial expressions is emphasized. Various models are introduced based on MobileNet, EfficientNet, and RexNet architectures. Experimental results demonstrate that these models achieve close to state-of-the-art performance in age, gender, and race recognition on the UTKFace dataset, as well as emotion classification on the AffectNet dataset. Additionally, utilizing these trained models as feature extractors for facial regions in video frames yields a 4.5% increase in accuracy compared to previously known state-of-the-art single models for the AFEW and VGAF datasets from the EmotiW challenges. The models and source code can be accessed publicly at https://github.com/HSE-asavchenko/face-emotion-recognition."}, "2310.06474": {"original_text": "  While large language models (LLMs) exhibit remarkable capabilities across a\nwide range of tasks, they pose potential safety concerns, such as the\n``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to\nexhibit undesirable behavior. Although several preventive measures have been\ndeveloped to mitigate the potential risks associated with LLMs, they have\nprimarily focused on English. In this study, we reveal the presence of\nmultilingual jailbreak challenges within LLMs and consider two potential risky\nscenarios: unintentional and intentional. The unintentional scenario involves\nusers querying LLMs using non-English prompts and inadvertently bypassing the\nsafety mechanisms, while the intentional scenario concerns malicious users\ncombining malicious instructions with multilingual prompts to deliberately\nattack LLMs. The experimental results reveal that in the unintentional\nscenario, the rate of unsafe content increases as the availability of languages\ndecreases. Specifically, low-resource languages exhibit about three times the\nlikelihood of encountering harmful content compared to high-resource languages,\nwith both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts\ncan exacerbate the negative impact of malicious instructions, with\nastonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for\nGPT-4. To handle such a challenge in the multilingual context, we propose a\nnovel \\textsc{Self-Defense} framework that automatically generates multilingual\ntraining data for safety fine-tuning. Experimental results show that ChatGPT\nfine-tuned with such data can achieve a substantial reduction in unsafe content\ngeneration. Data is available at\n\\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.\n", "rewritten_text": "\"While large language models (LLMs) demonstrate impressive abilities in various tasks, they raise safety concerns, such as the 'jailbreak' issue, where malicious instructions can manipulate LLMs to exhibit undesirable behavior. Despite efforts to address risks associated with LLMs, existing preventive measures have mainly focused on English. This study uncovers multilingual jailbreak challenges in LLMs and explores two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users unintentionally bypassing safety mechanisms by using non-English prompts, while the intentional scenario involves malicious users deliberately attacking LLMs by combining malicious instructions with multilingual prompts. Experimental findings show that in the unintentional scenario, the likelihood of encountering harmful content increases as the availability of languages decreases, with low-resource languages having a higher risk compared to high-resource languages in both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can amplify the impact of malicious instructions, resulting in high rates of unsafe output. To address this challenge in a multilingual context, we propose a new Self-Defense framework that automatically generates multilingual training data for safety fine-tuning. Experimental results demonstrate that ChatGPT fine-tuned with this data significantly reduces unsafe content generation. The data is accessible at the following link: https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs.\""}, "2110.13793": {"original_text": "  With camera resolution ever increasing and the need to rapidly recalibrate\nrobotic platforms in less than ideal environments, there is a need for faster\nand more robust chessboard fiducial marker detectors. A new chessboard detector\nis proposed that is specifically designed for: high resolution images,\nfocus/motion blur, harsh lighting conditions, and background clutter. This is\naccomplished using a new x-corner detector, where for the first time blur is\nestimated and used in a novel way to enhance corner localization, edge\nvalidation, and connectivity. Performance is measured and compared against\nother libraries using a diverse set of images created by combining multiple\nthird party datasets and including new specially crafted scenarios designed to\nstress the state-of-the-art. The proposed detector has the best F1- Score of\n0.97, runs 1.9x faster than next fastest, and is a top performer for corner\naccuracy, while being the only detector to have consistent good performance in\nall scenarios.\n", "rewritten_text": "A new chessboard fiducial marker detector is introduced to address the increasing camera resolution and the need for quick recalibration of robotic platforms in challenging environments. This detector is specifically designed to handle high-resolution images, focus/motion blur, harsh lighting conditions, and background clutter. It utilizes a novel x-corner detection method that estimates blur to enhance corner localization, edge validation, and connectivity. Performance evaluation against other libraries is conducted using a diverse image dataset, including specially crafted scenarios to push the limits of current technology. The proposed detector achieves an impressive F1-Score of 0.97, runs 1.9 times faster than the next best alternative, excels in corner accuracy, and consistently performs well across all scenarios."}, "2309.13492": {"original_text": "  Today's image style transfer methods have difficulty retaining humans face\nindividual features after the whole stylizing process. This occurs because the\nfeatures like face geometry and people's expressions are not captured by the\ngeneral-purpose image classifiers like the VGG-19 pre-trained models. This\npaper proposes the use of embeddings from an auxiliary pre-trained face\nrecognition model to encourage the algorithm to propagate human face features\nfrom the content image to the final stylized result.\n", "rewritten_text": "Current image style transfer techniques struggle to preserve the unique facial features of humans throughout the stylization process. This challenge arises due to the inability of standard image classifiers, such as the VGG-19 pre-trained models, to capture specific elements like facial geometry and expressions. To address this issue, the paper suggests leveraging embeddings from a supplementary pre-trained face recognition model to guide the algorithm in transferring human facial characteristics from the original image to the stylized output."}, "2206.04242": {"original_text": "  Despite advances in image classification methods, detecting the samples not\nbelonging to the training classes is still a challenging problem. There has\nbeen a burst of interest in this subject recently, which is called Open-Set\nRecognition (OSR). In OSR, the goal is to achieve both the classification and\ndetecting out-of-distribution (OOD) samples. Several ideas have been proposed\nto push the empirical result further through complicated techniques. We believe\nthat such complication is indeed not necessary. To this end, we have shown that\nMaximum Softmax Probability (MSP), as the simplest baseline for OSR, applied on\nVision Transformers (ViTs) as the base classifier that is trained with non-OOD\naugmentations can surprisingly outperform many recent methods. Non-OOD\naugmentations are the ones that do not alter the data distribution by much. Our\nresults outperform state-of-the-art in CIFAR-10 datasets, and is also better\nthan most of the current methods in SVHN and MNIST. We show that training\naugmentation has a significant effect on the performance of ViTs in the OSR\ntasks, and while they should produce significant diversity in the augmented\nsamples, the generated sample OOD-ness must remain limited.\n", "rewritten_text": "\"Despite advancements in image classification techniques, identifying samples that do not belong to the training classes remains a challenging task. Recently, there has been a growing interest in this area known as Open-Set Recognition (OSR). In OSR, the objective is to both classify and detect out-of-distribution (OOD) samples. Various complex methods have been proposed to improve empirical results, but we believe that simplicity can be effective. Our study demonstrates that utilizing Maximum Softmax Probability (MSP) as a basic approach for OSR, combined with Vision Transformers (ViTs) as the base classifier trained with non-OOD augmentations, can outperform many recent techniques. Non-OOD augmentations are those that minimally alter the data distribution. Our findings show superior performance compared to state-of-the-art methods on CIFAR-10 datasets, as well as outperforming most current approaches on SVHN and MNIST datasets. We highlight the importance of training augmentations in enhancing ViTs' performance in OSR tasks, emphasizing the need for diversity in augmented samples while maintaining limited OOD-ness.\""}, "2010.01041": {"original_text": "  Planar homography estimation is foundational to many computer vision\nproblems, such as Simultaneous Localization and Mapping (SLAM) and Augmented\nReality (AR). However, conditions of high variance confound even the\nstate-of-the-art algorithms. In this report, we analyze the performance of two\nrecently published methods using Convolutional Neural Networks (CNNs) that are\nmeant to replace the more traditional feature-matching based approaches to the\nestimation of homography. Our evaluation of the CNN based methods focuses\nparticularly on measuring the performance under conditions of significant\nnoise, illumination shift, and occlusion. We also measure the benefits of\ntraining CNNs to varying degrees of noise. Additionally, we compare the effect\nof using color images instead of grayscale images for inputs to CNNs. Finally,\nwe compare the results against baseline feature-matching based homography\nestimation methods using SIFT, SURF, and ORB. We find that CNNs can be trained\nto be more robust against noise, but at a small cost to accuracy in the\nnoiseless case. Additionally, CNNs perform significantly better in conditions\nof extreme variance than their feature-matching based counterparts. With regard\nto color inputs, we conclude that with no change in the CNN architecture to\ntake advantage of the additional information in the color planes, the\ndifference in performance using color inputs or grayscale inputs is negligible.\nAbout the CNNs trained with noise-corrupted inputs, we show that training a CNN\nto a specific magnitude of noise leads to a \"Goldilocks Zone\" with regard to\nthe noise levels where that CNN performs best.\n", "rewritten_text": "Estimating planar homography is crucial for various computer vision applications like Simultaneous Localization and Mapping (SLAM) and Augmented Reality (AR). Despite advancements in algorithms, challenges arise in high variance scenarios. This study evaluates two recent Convolutional Neural Network (CNN) methods designed to replace traditional feature-matching approaches for homography estimation. The focus is on assessing performance in noisy, varying illumination, and occluded conditions. The study also explores training CNNs with different noise levels and compares using color versus grayscale images as inputs. Results are compared against baseline feature-matching methods like SIFT, SURF, and ORB. Findings suggest that CNNs can be trained to be more robust against noise, albeit with a slight decrease in accuracy in noiseless scenarios. CNNs outperform feature-matching methods in extreme variance conditions. Regarding color inputs, the study concludes that without modifying the CNN architecture to leverage color information, performance differences between color and grayscale inputs are minimal. Training CNNs with noise-corrupted inputs reveals an optimal noise level range where CNNs perform best, termed the \"Goldilocks Zone.\""}, "2308.10168": {"original_text": "  Since the recent prosperity of Large Language Models (LLMs), there have been\ninterleaved discussions regarding how to reduce hallucinations from LLM\nresponses, how to increase the factuality of LLMs, and whether Knowledge Graphs\n(KGs), which store the world knowledge in a symbolic form, will be replaced\nwith LLMs. In this paper, we try to answer these questions from a new angle:\nHow knowledgeable are LLMs?\n  To answer this question, we constructed Head-to-Tail, a benchmark that\nconsists of 18K question-answer (QA) pairs regarding head, torso, and tail\nfacts in terms of popularity. We designed an automated evaluation method and a\nset of metrics that closely approximate the knowledge an LLM confidently\ninternalizes. Through a comprehensive evaluation of 16 publicly available LLMs,\nwe show that existing LLMs are still far from being perfect in terms of their\ngrasp of factual knowledge, especially for facts of torso-to-tail entities.\n", "rewritten_text": "\"Given the recent success of Large Language Models (LLMs), there have been ongoing discussions on how to minimize inaccuracies in LLM responses, enhance the accuracy of LLMs, and whether Knowledge Graphs (KGs) will be replaced by LLMs. This study aims to address these questions by evaluating the knowledge level of LLMs. \nTo assess this, we developed Head-to-Tail, a benchmark comprising 18K question-answer pairs related to head, torso, and tail facts based on their popularity. We created an automated evaluation approach and a set of metrics to measure the depth of knowledge that LLMs possess. Our evaluation of 16 publicly available LLMs reveals that current models still have room for improvement in terms of factual knowledge, particularly regarding torso-to-tail entities.\""}, "1912.06683": {"original_text": "  Semantic image segmentation plays a pivotal role in many vision applications\nincluding autonomous driving and medical image analysis. Most of the former\napproaches move towards enhancing the performance in terms of accuracy with a\nlittle awareness of computational efficiency. In this paper, we introduce\nLiteSeg, a lightweight architecture for semantic image segmentation. In this\nwork, we explore a new deeper version of Atrous Spatial Pyramid Pooling module\n(ASPP) and apply short and long residual connections, and depthwise separable\nconvolution, resulting in a faster and efficient model. LiteSeg architecture is\nintroduced and tested with multiple backbone networks as Darknet19, MobileNet,\nand ShuffleNet to provide multiple trade-offs between accuracy and\ncomputational cost. The proposed model LiteSeg, with MobileNetV2 as a backbone\nnetwork, achieves an accuracy of 67.81% mean intersection over union at 161\nframes per second with $640 \\times 360$ resolution on the Cityscapes dataset.\n", "rewritten_text": "Semantic image segmentation is crucial for various vision applications such as autonomous driving and medical image analysis. While many existing methods focus on improving accuracy without considering computational efficiency, this paper presents LiteSeg, a lightweight architecture for semantic image segmentation. LiteSeg incorporates a new, deeper version of the Atrous Spatial Pyramid Pooling module (ASPP), along with short and long residual connections and depthwise separable convolution, resulting in a faster and more efficient model. The LiteSeg architecture is evaluated using different backbone networks like Darknet19, MobileNet, and ShuffleNet to balance accuracy and computational cost. The proposed LiteSeg model, utilizing MobileNetV2 as the backbone network, achieves an accuracy of 67.81% mean intersection over union at 161 frames per second with a resolution of $640 \\times 360$ on the Cityscapes dataset."}, "2402.14281": {"original_text": "  Map representation learned by expert demonstrations has shown promising\nresearch value. However, recent advancements in the visual navigation field\nface challenges due to the lack of human datasets in the real world for\nefficient supervised representation learning of the environments. We present a\nLandmark-Aware Visual Navigation (LAVN) dataset to allow for supervised\nlearning of human-centric exploration policies and map building. We collect RGB\nobservation and human point-click pairs as a human annotator explores virtual\nand real-world environments with the goal of full coverage exploration of the\nspace. The human annotators also provide distinct landmark examples along each\ntrajectory, which we intuit will simplify the task of map or graph building and\nlocalization. These human point-clicks serve as direct supervision for waypoint\nprediction when learning to explore in environments. Our dataset covers a wide\nspectrum of scenes, including rooms in indoor environments, as well as walkways\noutdoors. Dataset is available at DOI: 10.5281/zenodo.10608067.\n", "rewritten_text": "The use of map representation acquired from expert demonstrations has shown promise in research. However, recent advancements in visual navigation are facing challenges due to the limited availability of human datasets in real-world settings for effective supervised learning of environments. To address this, we introduce the Landmark-Aware Visual Navigation (LAVN) dataset, which enables supervised learning of exploration strategies and map creation centered around human interactions. This dataset includes RGB observations and human point-click pairs collected as a human annotator explores virtual and real-world environments with the aim of thoroughly covering the space. Additionally, distinct landmark examples are provided by the annotators along each trajectory, which are expected to simplify map and graph building as well as localization tasks. These human point-click pairs serve as direct guidance for predicting waypoints during exploration. The dataset encompasses a variety of scenes, such as indoor rooms and outdoor walkways. For access to the dataset, please refer to the DOI: 10.5281/zenodo.10608067."}, "2408.05184": {"original_text": "  This paper describes our solution of the first subtask from the AXOLOTL-24\nshared task on Semantic Change Modeling. The goal of this subtask is to\ndistribute a given set of usages of a polysemous word from a newer time period\nbetween senses of this word from an older time period and clusters representing\ngained senses of this word. We propose and experiment with three new methods\nsolving this task. Our methods achieve SOTA results according to both official\nmetrics of the first substask. Additionally, we develop a model that can tell\nif a given word usage is not described by any of the provided sense\ndefinitions. This model serves as a component in one of our methods, but can\npotentially be useful on its own.\n", "rewritten_text": "This paper presents our approach to the initial part of the AXOLOTL-24 shared task on Semantic Change Modeling. The objective of this task is to allocate a given set of uses of a word with multiple meanings from a more recent time period among the senses of the word from an earlier time period and clusters representing newly acquired meanings. We introduce and test three novel methods for addressing this challenge. Our methods achieve state-of-the-art results based on the official metrics for the first subtask. Furthermore, we develop a model that can determine if a particular word usage does not align with any of the provided sense definitions. While this model is integrated into one of our methods, it could potentially be beneficial as a standalone tool."}, "2312.00335": {"original_text": "  Self-supervised learning (SSL) approaches have recently shown substantial\nsuccess in learning visual representations from unannotated images. Compared\nwith photographic images, medical images acquired with the same imaging\nprotocol exhibit high consistency in anatomy. To exploit this anatomical\nconsistency, this paper introduces a novel SSL approach, called PEAC (patch\nembedding of anatomical consistency), for medical image analysis. Specifically,\nin this paper, we propose to learn global and local consistencies via stable\ngrid-based matching, transfer pre-trained PEAC models to diverse downstream\ntasks, and extensively demonstrate that (1) PEAC achieves significantly better\nperformance than the existing state-of-the-art fully/self-supervised methods,\nand (2) PEAC captures the anatomical structure consistency across views of the\nsame patient and across patients of different genders, weights, and healthy\nstatuses, which enhances the interpretability of our method for medical image\nanalysis.\n", "rewritten_text": "Recently, self-supervised learning (SSL) methods have demonstrated significant success in acquiring visual representations from unannotated images. In comparison to regular photographic images, medical images obtained using the same imaging protocol display a high level of consistency in terms of anatomy. To leverage this anatomical consistency, this study introduces a new SSL approach known as PEAC (patch embedding of anatomical consistency) for analyzing medical images. The paper proposes learning global and local consistencies through stable grid-based matching, transferring pre-trained PEAC models to various downstream tasks, and extensively showcasing that PEAC outperforms existing state-of-the-art fully/self-supervised methods. Additionally, PEAC captures anatomical structure consistency across different views of the same patient and among patients with varying genders, weights, and health statuses, thereby enhancing the interpretability of the method for medical image analysis."}, "2408.15159": {"original_text": "  Translating written sentences from oral languages to a sequence of manual and\nnon-manual gestures plays a crucial role in building a more inclusive society\nfor deaf and hard-of-hearing people. Facial expressions (non-manual), in\nparticular, are responsible for encoding the grammar of the sentence to be\nspoken, applying punctuation, pronouns, or emphasizing signs. These non-manual\ngestures are closely related to the semantics of the sentence being spoken and\nalso to the utterance of the speaker's emotions. However, most Sign Language\nProduction (SLP) approaches are centered on synthesizing manual gestures and do\nnot focus on modeling the speakers expression. This paper introduces a new\nmethod focused in synthesizing facial expressions for sign language. Our goal\nis to improve sign language production by integrating sentiment information in\nfacial expression generation. The approach leverages a sentence sentiment and\nsemantic features to sample from a meaningful representation space, integrating\nthe bias of the non-manual components into the sign language production\nprocess. To evaluate our method, we extend the Frechet Gesture Distance (FGD)\nand propose a new metric called Frechet Expression Distance (FED) and apply an\nextensive set of metrics to assess the quality of specific regions of the face.\nThe experimental results showed that our method achieved state of the art,\nbeing superior to the competitors on How2Sign and PHOENIX14T datasets.\nMoreover, our architecture is based on a carefully designed graph pyramid that\nmakes it simpler, easier to train, and capable of leveraging emotions to\nproduce facial expressions.\n", "rewritten_text": "The translation of written sentences from spoken languages into a series of manual and non-manual gestures is essential for creating a more inclusive society for individuals who are deaf or hard of hearing. Non-manual gestures, particularly facial expressions, play a key role in encoding sentence grammar, applying punctuation, pronouns, and emphasizing signs. These gestures are closely tied to the meaning of the spoken sentence and the emotions conveyed by the speaker. However, existing Sign Language Production (SLP) methods mainly focus on synthesizing manual gestures and often overlook modeling facial expressions. This paper presents a novel approach that concentrates on generating facial expressions for sign language, aiming to enhance sign language production by incorporating sentiment information into facial expression synthesis. The method utilizes sentence sentiment and semantic features to generate meaningful facial expressions, integrating non-manual components into the sign language production process. To evaluate the effectiveness of the method, a new metric called Frechet Expression Distance (FED) is proposed, in addition to extending the Frechet Gesture Distance (FGD) metric. Various metrics are employed to assess the quality of specific facial regions. Experimental results demonstrate that the proposed method outperforms competitors on the How2Sign and PHOENIX14T datasets, achieving state-of-the-art performance. Furthermore, the architecture is based on a well-designed graph pyramid, making it easier to train and capable of leveraging emotions to generate facial expressions effectively."}, "2011.08894": {"original_text": "  Medical image segmentation is a relevant task as it serves as the first step\nfor several diagnosis processes, thus it is indispensable in clinical usage.\nWhilst major success has been reported using supervised techniques, they assume\na large and well-representative labelled set. This is a strong assumption in\nthe medical domain where annotations are expensive, time-consuming, and\ninherent to human bias. To address this problem, unsupervised techniques have\nbeen proposed in the literature yet it is still an open problem due to the\ndifficulty of learning any transformation pattern. In this work, we present a\nnovel optimisation model framed into a new CNN-based contrastive registration\narchitecture for unsupervised medical image segmentation. The core of our\napproach is to exploit image-level registration and feature-level from a\ncontrastive learning mechanism, to perform registration-based segmentation.\nFirstly, we propose an architecture to capture the image-to-image\ntransformation pattern via registration for unsupervised medical image\nsegmentation. Secondly, we embed a contrastive learning mechanism into the\nregistration architecture to enhance the discriminating capacity of the network\nin the feature-level. We show that our proposed technique mitigates the major\ndrawbacks of existing unsupervised techniques. We demonstrate, through\nnumerical and visual experiments, that our technique substantially outperforms\nthe current state-of-the-art unsupervised segmentation methods on two major\nmedical image datasets.\n", "rewritten_text": "Segmenting medical images is a crucial task in the field of diagnosis, serving as the initial step in various clinical processes. While supervised techniques have shown significant success, they rely on having a large and representative labeled dataset, which can be challenging in the medical field due to the high cost, time, and potential bias associated with annotations. To address this issue, unsupervised techniques have been proposed in the literature, but learning transformation patterns remains a difficult task. In this study, we introduce a new optimization model within a novel CNN-based contrastive registration framework for unsupervised medical image segmentation. Our approach leverages image-level registration and feature-level contrastive learning to achieve segmentation based on registration. We first design an architecture to capture image-to-image transformation patterns for unsupervised segmentation, and then integrate contrastive learning to enhance the network's discriminative capabilities at the feature level. Our technique overcomes the limitations of existing unsupervised methods, as demonstrated through numerical and visual experiments showing superior performance compared to current state-of-the-art unsupervised segmentation methods on two major medical image datasets."}, "2301.02307": {"original_text": "  Narrated ''how-to'' videos have emerged as a promising data source for a wide\nrange of learning problems, from learning visual representations to training\nrobot policies. However, this data is extremely noisy, as the narrations do not\nalways describe the actions demonstrated in the video. To address this problem\nwe introduce the novel task of visual narration detection, which entails\ndetermining whether a narration is visually depicted by the actions in the\nvideo. We propose What You Say is What You Show (WYS^2), a method that\nleverages multi-modal cues and pseudo-labeling to learn to detect visual\nnarrations with only weakly labeled data. Our model successfully detects visual\nnarrations in in-the-wild videos, outperforming strong baselines, and we\ndemonstrate its impact for state-of-the-art summarization and temporal\nalignment of instructional videos.\n", "rewritten_text": "Narrated instructional videos have become a valuable resource for various learning tasks, such as developing visual representations and training robot behaviors. However, these videos often contain inaccuracies, as the spoken descriptions may not always match the actions shown. To tackle this issue, we introduce a new challenge called visual narration detection, which involves determining if the narration aligns with the visual content in the video. We present a method called What You Say is What You Show (WYS^2), which utilizes multiple cues and pseudo-labeling to learn how to identify visual narrations using weakly labeled data. Our model effectively detects visual narrations in real-world videos, surpassing existing methods, and we showcase its benefits for improving video summarization and temporal alignment in instructional content."}, "2403.05170": {"original_text": "  This paper proposes a new pipeline for long-tail (LT) recognition. Instead of\nre-weighting or re-sampling, we utilize the long-tailed dataset itself to\ngenerate a balanced proxy that can be optimized through cross-entropy (CE).\nSpecifically, a randomly initialized diffusion model, trained exclusively on\nthe long-tailed dataset, is employed to synthesize new samples for\nunderrepresented classes. Then, we utilize the inherent information in the\noriginal dataset to filter out harmful samples and keep the useful ones. Our\nstrategy, Diffusion model for Long-Tail recognition (DiffuLT), represents a\npioneering utilization of generative models in long-tail recognition. DiffuLT\nachieves state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT,\nsurpassing the best competitors with non-trivial margins. Abundant ablations\nmake our pipeline interpretable, too. The whole generation pipeline is done\nwithout any external data or pre-trained model weights, making it highly\ngeneralizable to real-world long-tailed settings.\n", "rewritten_text": "This paper introduces a novel approach to long-tail recognition by utilizing the long-tailed dataset itself to create a balanced proxy for optimization through cross-entropy. The method involves using a diffusion model trained on the long-tailed dataset to generate new samples for underrepresented classes. Harmful samples are filtered out while useful ones are retained using information from the original dataset. This strategy, known as Diffusion model for Long-Tail recognition (DiffuLT), is a pioneering use of generative models in long-tail recognition. DiffuLT outperforms competitors on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT, with significant improvements. The pipeline is interpretable through various analyses and does not rely on external data or pre-trained model weights, making it applicable to real-world long-tailed scenarios."}, "2404.14779": {"original_text": "  This study presents a comprehensive analysis and comparison of two\npredominant fine-tuning methodologies - full-parameter fine-tuning and\nparameter-efficient tuning - within the context of medical Large Language\nModels (LLMs). We developed and refined a series of LLMs, based on the Llama-2\narchitecture, specifically designed to enhance medical knowledge retrieval,\nreasoning, and question-answering capabilities. Our experiments systematically\nevaluate the effectiveness of these tuning strategies across various well-known\nmedical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of\n72% on the US Medical Licensing Examination (USMLE) datasets, setting a new\nstandard in performance for openly available medical LLMs. Through this\ncomparative analysis, we aim to identify the most effective and efficient\nmethod for fine-tuning LLMs in the medical domain, thereby contributing\nsignificantly to the advancement of AI-driven healthcare applications.\n", "rewritten_text": "This research study provides a thorough examination and comparison of two main fine-tuning approaches - full-parameter fine-tuning and parameter-efficient tuning - in the context of medical Large Language Models (LLMs). We created and improved a series of LLMs, based on the Llama-2 architecture, specifically tailored to improve medical knowledge retrieval, reasoning, and question-answering abilities. Our experiments methodically assess the efficacy of these tuning methods across various established medical benchmarks. Notably, our medical LLM Med42 achieved a 72% accuracy rate on the US Medical Licensing Examination (USMLE) datasets, setting a new performance benchmark for publicly available medical LLMs. Through this comparative analysis, our goal is to determine the most effective and efficient approach for fine-tuning LLMs in the medical field, thereby making a significant contribution to the progress of AI-driven healthcare applications."}, "2411.09694": {"original_text": "  Reranking a list of candidates from a machine translation system with an\nexternal scoring model and returning the highest-scoring candidate remains a\nsimple and effective method for improving the overall output quality.\nTranslation scoring models continue to grow in size, with the best models being\ncomparable to generation models. Thus, reranking can add substantial\ncomputational cost to the translation pipeline. In this work, we pose reranking\nas a Bayesian optimization (BayesOpt) problem. By strategically selecting\ncandidates to score based on a balance of exploration and exploitation, we show\nthat it is possible to find top-scoring candidates when scoring only a fraction\nof the candidate list. For instance, our method achieves the same CometKiwi\nscore using only 70 scoring evaluations compared a baseline system using 180.\nWe present a multi-fidelity setting for BayesOpt, where the candidates are\nfirst scored with a cheaper but noisier proxy scoring model, which further\nimproves the cost-performance tradeoff when using smaller but well-trained\ndistilled proxy scorers.\n", "rewritten_text": "\"Reordering a list of candidates generated by a machine translation system using an external scoring model and selecting the highest-scoring candidate is a straightforward and effective approach to enhancing overall output quality. Translation scoring models are becoming increasingly sophisticated, with top models rivaling generation models in complexity. However, reranking can significantly increase the computational burden of the translation process. In this study, we frame reranking as a Bayesian optimization (BayesOpt) challenge. By strategically choosing candidates to evaluate based on a balance of exploration and exploitation, we demonstrate that it is feasible to identify top-scoring candidates by evaluating only a subset of the candidate list. For example, our method achieves the same CometKiwi score with just 70 evaluations, compared to the 180 evaluations required by a baseline system. We introduce a multi-fidelity approach for BayesOpt, where candidates are initially assessed using a less expensive but noisier proxy scoring model, leading to improved cost-effectiveness when employing smaller yet well-trained proxy scorers.\""}, "2407.07080": {"original_text": "  Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.\n", "rewritten_text": "Training large language models (LLMs) in low-resource languages like Hebrew presents unique challenges. This paper introduces DictaLM2.0 and DictaLM2.0-Instruct, two LLMs based on the Mistral model, trained on a substantial corpus of around 200 billion tokens in Hebrew and English. Adapting a pre-trained model to a new language requires specialized techniques different from training a model from scratch or further training existing models in well-resourced languages such as English. The paper outlines these innovative training methods that aid in effective learning and adaptation to Hebrew's linguistic characteristics. Additionally, DictaLM2.0-Instruct was fine-tuned on a comprehensive instruct dataset to improve its performance on task-specific instructions. To thoroughly evaluate the models, a new benchmark suite for Hebrew LLM assessment is introduced, covering various tasks like Question Answering, Sentiment Analysis, Winograd Schema Challenge, Translation, and Summarization. This work not only tackles the complexities of training LLMs in low-resource languages but also proposes a framework that can be utilized to adapt other LLMs to different non-English languages, contributing to the wider field of multilingual NLP."}, "2211.08544": {"original_text": "  Quantization-aware training (QAT) receives extensive popularity as it well\nretains the performance of quantized networks. In QAT, the contemporary\nexperience is that all quantized weights are updated for an entire training\nprocess. In this paper, this experience is challenged based on an interesting\nphenomenon we observed. Specifically, a large portion of quantized weights\nreaches the optimal quantization level after a few training epochs, which we\nrefer to as the partly scratch-off lottery ticket. This\nstraightforward-yet-valuable observation naturally inspires us to zero out\ngradient calculations of these weights in the remaining training period to\navoid meaningless updating. To effectively find the ticket, we develop a\nheuristic method, dubbed lottery ticket scratcher (LTS), which freezes a weight\nonce the distance between the full-precision one and its quantization level is\nsmaller than a controllable threshold. Surprisingly, the proposed LTS typically\neliminates 50%-70% weight updating and 25%-35% FLOPs of the backward pass,\nwhile still resulting on par with or even better performance than the compared\nbaseline. For example, compared with the baseline, LTS improves 2-bit\nMobileNetV2 by 5.05%, eliminating 46% weight updating and 23% FLOPs of the\nbackward pass. Code is at url{https://github.com/zysxmu/LTS}.\n", "rewritten_text": "Quantization-aware training (QAT) has gained widespread popularity for maintaining the performance of quantized networks. In QAT, it is commonly believed that all quantized weights are updated throughout the entire training process. However, this paper challenges this belief based on an interesting observation. We found that a significant portion of quantized weights reaches the optimal quantization level after just a few training epochs, which we refer to as the \"partly scratch-off lottery ticket.\" This insight led us to propose a method called the lottery ticket scratcher (LTS), which freezes weights that have reached their optimal quantization level to avoid unnecessary updates. The LTS method effectively reduces weight updating by 50%-70% and FLOPs of the backward pass by 25%-35%, while maintaining or even improving performance compared to the baseline. For instance, LTS enhances 2-bit MobileNetV2 by 5.05%, reducing weight updating by 46% and FLOPs of the backward pass by 23% compared to the baseline. The code for LTS can be found at https://github.com/zysxmu/LTS."}, "2001.04642": {"original_text": "  We address the dual problems of novel view synthesis and environment\nreconstruction from hand-held RGBD sensors. Our contributions include 1)\nmodeling highly specular objects, 2) modeling inter-reflections and Fresnel\neffects, and 3) enabling surface light field reconstruction with the same input\nneeded to reconstruct shape alone. In cases where scene surface has a strong\nmirror-like material component, we generate highly detailed environment images,\nrevealing room composition, objects, people, buildings, and trees visible\nthrough windows. Our approach yields state of the art view synthesis\ntechniques, operates on low dynamic range imagery, and is robust to geometric\nand calibration errors.\n", "rewritten_text": "We focus on solving the challenges of creating new perspectives and reconstructing environments using handheld RGBD sensors. Our contributions involve: 1) accurately modeling shiny objects, 2) accounting for reflections and Fresnel effects, and 3) reconstructing surface light fields using the same data used for shape reconstruction. When dealing with surfaces that reflect like mirrors, we can generate detailed images of the surroundings, showing room contents, people, buildings, and trees visible through windows. Our method offers cutting-edge view synthesis capabilities, works with low dynamic range images, and is resilient to errors in geometry and calibration."}, "1601.00199": {"original_text": "  Active Appearance Models (AAMs) are one of the most popular and\nwell-established techniques for modeling deformable objects in computer vision.\nIn this paper, we study the problem of fitting AAMs using Compositional\nGradient Descent (CGD) algorithms. We present a unified and complete view of\nthese algorithms and classify them with respect to three main characteristics:\ni) cost function; ii) type of composition; and iii) optimization method.\nFurthermore, we extend the previous view by: a) proposing a novel Bayesian cost\nfunction that can be interpreted as a general probabilistic formulation of the\nwell-known project-out loss; b) introducing two new types of composition,\nasymmetric and bidirectional, that combine the gradients of both image and\nappearance model to derive better conver- gent and more robust CGD algorithms;\nand c) providing new valuable insights into existent CGD algorithms by\nreinterpreting them as direct applications of the Schur complement and the\nWiberg method. Finally, in order to encourage open research and facilitate\nfuture comparisons with our work, we make the implementa- tion of the\nalgorithms studied in this paper publicly available as part of the Menpo\nProject.\n", "rewritten_text": "Active Appearance Models (AAMs) are widely used in computer vision for modeling deformable objects. This study focuses on fitting AAMs using Compositional Gradient Descent (CGD) algorithms. The paper offers a comprehensive analysis of these algorithms, categorizing them based on cost function, composition type, and optimization method. Additionally, it introduces a new Bayesian cost function, asymmetric and bidirectional composition types, and sheds light on existing CGD algorithms by reinterpreting them using the Schur complement and the Wiberg method. The implementation of the algorithms discussed in this paper is made publicly available as part of the Menpo Project to encourage further research and comparisons."}, "1709.01695": {"original_text": "  3D action recognition was shown to benefit from a covariance representation\nof the input data (joint 3D positions). A kernel machine feed with such feature\nis an effective paradigm for 3D action recognition, yielding state-of-the-art\nresults. Yet, the whole framework is affected by the well-known scalability\nissue. In fact, in general, the kernel function has to be evaluated for all\npairs of instances inducing a Gram matrix whose complexity is quadratic in the\nnumber of samples. In this work we reduce such complexity to be linear by\nproposing a novel and explicit feature map to approximate the kernel function.\nThis allows to train a linear classifier with an explicit feature encoding,\nwhich implicitly implements a Log-Euclidean machine in a scalable fashion. Not\nonly we prove that the proposed approximation is unbiased, but also we work out\nan explicit strong bound for its variance, attesting a theoretical superiority\nof our approach with respect to existing ones. Experimentally, we verify that\nour representation provides a compact encoding and outperforms other\napproximation schemes on a number of publicly available benchmark datasets for\n3D action recognition.\n", "rewritten_text": "The study demonstrates that utilizing a covariance representation of joint 3D positions can enhance 3D action recognition. Employing a kernel machine fed with such features proves to be an effective approach, leading to cutting-edge results. However, the scalability of the entire framework is a concern due to the need to evaluate the kernel function for all pairs of instances, resulting in a quadratic complexity in the number of samples. To address this issue, a novel and explicit feature map is proposed in this research to approximate the kernel function, reducing the complexity to a linear level. This enables training a linear classifier with an explicit feature encoding, effectively implementing a Log-Euclidean machine in a scalable manner. The study demonstrates that the proposed approximation is unbiased and provides a strong bound for its variance, showcasing the theoretical superiority of this approach over existing methods. Experimentally, it is confirmed that this representation offers a concise encoding and surpasses other approximation techniques on various publicly available benchmark datasets for 3D action recognition."}, "2212.0583": {"original_text": "  Directly training a document-to-document (Doc2Doc) neural machine translation\n(NMT) via Transformer from scratch, especially on small datasets usually fails\nto converge. Our dedicated probing tasks show that 1) both the absolute\nposition and relative position information gets gradually weakened or even\nvanished once it reaches the upper encoder layers, and 2) the vanishing of\nabsolute position information in encoder output causes the training failure of\nDoc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer\n(P-Transformer) to enhance both the absolute and relative position information\nin both self-attention and cross-attention. Specifically, we integrate absolute\npositional information, i.e., position embeddings, into the query-key pairs\nboth in self-attention and cross-attention through a simple yet effective\naddition operation. Moreover, we also integrate relative position encoding in\nself-attention. The proposed P-Transformer utilizes sinusoidal position\nencoding and does not require any task-specified position embedding, segment\nembedding, or attention mechanism. Through the above methods, we build a\nDoc2Doc NMT model with P-Transformer, which ingests the source document and\ncompletely generates the target document in a sequence-to-sequence (seq2seq)\nway. In addition, P-Transformer can be applied to seq2seq-based\ndocument-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)\ntranslation. Extensive experimental results of Doc2Doc NMT show that\nP-Transformer significantly outperforms strong baselines on widely-used 9\ndocument-level datasets in 7 language pairs, covering small-, middle-, and\nlarge-scales, and achieves a new state-of-the-art. Experimentation on discourse\nphenomena shows that our Doc2Doc NMT models improve the translation quality in\nboth BLEU and discourse coherence. We make our code available on Github.\n", "rewritten_text": "Training a document-to-document (Doc2Doc) neural machine translation (NMT) model directly using a Transformer from scratch, especially with small datasets, often fails to converge. Our investigation reveals that as the model progresses through the upper encoder layers, both absolute and relative position information gradually diminishes or disappears entirely. The absence of absolute position information in the encoder output leads to training difficulties for Doc2Doc NMT. To address this issue, we introduce a position-aware Transformer (P-Transformer) that enhances both absolute and relative position information in self-attention and cross-attention mechanisms. Specifically, we incorporate position embeddings into the query-key pairs in both self-attention and cross-attention using a simple yet effective addition operation. Additionally, we integrate relative position encoding in self-attention. The proposed P-Transformer utilizes sinusoidal position encoding and does not rely on task-specific position embedding, segment embedding, or attention mechanisms. By implementing these enhancements, we develop a Doc2Doc NMT model with P-Transformer that takes in a source document and generates the target document in a sequence-to-sequence manner. Furthermore, P-Transformer can be applied to document-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent) translation tasks. Extensive experiments on Doc2Doc NMT demonstrate that P-Transformer outperforms strong baseline models on nine widely-used document-level datasets across seven language pairs of varying scales, achieving state-of-the-art performance. Evaluation on discourse phenomena indicates that our Doc2Doc NMT models enhance translation quality in terms of both BLEU scores and discourse coherence. Our code is publicly available on Github."}, "2308.05430": {"original_text": "  In this work, we propose an ensemble modeling approach for multimodal action\nrecognition. We independently train individual modality models using a variant\nof focal loss tailored to handle the long-tailed distribution of the MECCANO\n[21] dataset. Based on the underlying principle of focal loss, which captures\nthe relationship between tail (scarce) classes and their prediction\ndifficulties, we propose an exponentially decaying variant of focal loss for\nour current task. It initially emphasizes learning from the hard misclassified\nexamples and gradually adapts to the entire range of examples in the dataset.\nThis annealing process encourages the model to strike a balance between\nfocusing on the sparse set of hard samples, while still leveraging the\ninformation provided by the easier ones. Additionally, we opt for the late\nfusion strategy to combine the resultant probability distributions from RGB and\nDepth modalities for final action prediction. Experimental evaluations on the\nMECCANO dataset demonstrate the effectiveness of our approach.\n", "rewritten_text": "\"In this study, we introduce a method for multimodal action recognition using ensemble modeling. We train individual modality models separately with a modified focal loss to address the skewed distribution of the MECCANO dataset. Our approach, inspired by focal loss, focuses on the challenging misclassified examples initially and gradually incorporates the entire dataset. This adaptive process encourages the model to balance learning from difficult samples while utilizing information from easier ones. We employ a late fusion strategy to combine probability distributions from RGB and Depth modalities for action prediction. Our experimental results on the MECCANO dataset show the efficacy of our approach.\""}, "1810.1161": {"original_text": "  Despite remarkable advances in image synthesis research, existing works often\nfail in manipulating images under the context of large geometric\ntransformations. Synthesizing person images conditioned on arbitrary poses is\none of the most representative examples where the generation quality largely\nrelies on the capability of identifying and modeling arbitrary transformations\non different body parts. Current generative models are often built on local\nconvolutions and overlook the key challenges (e.g. heavy occlusions, different\nviews or dramatic appearance changes) when distinct geometric changes happen\nfor each part, caused by arbitrary pose manipulations. This paper aims to\nresolve these challenges induced by geometric variability and spatial\ndisplacements via a new Soft-Gated Warping Generative Adversarial Network\n(Warping-GAN), which is composed of two stages: 1) it first synthesizes a\ntarget part segmentation map given a target pose, which depicts the\nregion-level spatial layouts for guiding image synthesis with higher-level\nstructure constraints; 2) the Warping-GAN equipped with a soft-gated\nwarping-block learns feature-level mapping to render textures from the original\nimage into the generated segmentation map. Warping-GAN is capable of\ncontrolling different transformation degrees given distinct target poses.\nMoreover, the proposed warping-block is light-weight and flexible enough to be\ninjected into any networks. Human perceptual studies and quantitative\nevaluations demonstrate the superiority of our Warping-GAN that significantly\noutperforms all existing methods on two large datasets.\n", "rewritten_text": "Despite significant progress in image synthesis research, current works often struggle to manipulate images when faced with large geometric transformations. One notable example is the synthesis of person images based on various poses, where the quality of generation heavily depends on the ability to recognize and model diverse transformations across different body parts. Existing generative models typically rely on local convolutions and overlook the challenges posed by complex geometric changes resulting from arbitrary pose adjustments, such as occlusions, different viewpoints, and drastic appearance alterations for each body part. This study aims to address these challenges related to geometric variability and spatial shifts through a novel approach called Soft-Gated Warping Generative Adversarial Network (Warping-GAN). The Warping-GAN consists of two stages: first, it generates a target part segmentation map based on a specified pose to provide spatial guidance for image synthesis with structural constraints; second, the Warping-GAN, featuring a soft-gated warping-block, learns to map features from the original image onto the generated segmentation map to render textures. This approach allows for controlling transformation levels based on different poses, and the warping-block is lightweight and adaptable for integration into various networks. Human perception studies and quantitative assessments confirm the superior performance of our Warping-GAN compared to existing methods on two extensive datasets."}, "2410.10407": {"original_text": "  The widespread dissemination of false information through manipulative\ntactics that combine deceptive text and images threatens the integrity of\nreliable sources of information. While there has been research on detecting\nfake news in high resource languages using multimodal approaches, methods for\nlow resource Indic languages primarily rely on textual analysis. This\ndifference highlights the need for robust methods that specifically address\nmultimodal fake news in Indic languages, where the lack of extensive datasets\nand tools presents a significant obstacle to progress. To this end, we\nintroduce the Multimodal Multilingual dataset for Indic Fake News Detection\n(MMIFND). This meticulously curated dataset consists of 28,085 instances\ndistributed across Hindi, Bengali, Marathi, Malayalam, Tamil, Gujarati and\nPunjabi. We further propose the Multimodal Multilingual Caption-aware framework\nfor Fake News Detection (MMCFND). MMCFND utilizes pre-trained unimodal encoders\nand pairwise encoders from a foundational model that aligns vision and\nlanguage, allowing for extracting deep representations from visual and textual\ncomponents of news articles. The multimodal fusion encoder in the foundational\nmodel integrates text and image representations derived from its pairwise\nencoders to generate a comprehensive cross modal representation. Furthermore,\nwe generate descriptive image captions that provide additional context to\ndetect inconsistencies and manipulations. The retrieved features are then fused\nand fed into a classifier to determine the authenticity of news articles. The\ncurated dataset can potentially accelerate research and development in low\nresource environments significantly. Thorough experimentation on MMIFND\ndemonstrates that our proposed framework outperforms established methods for\nextracting relevant fake news detection features.\n", "rewritten_text": "The widespread spread of false information using deceptive text and images poses a threat to trustworthy sources of information. While research has focused on identifying fake news in well-resourced languages through various methods, the detection of fake news in low-resource Indic languages primarily relies on textual analysis. This disparity underscores the necessity for robust techniques tailored to address multimodal fake news in Indic languages, where limited datasets and tools present a major challenge. Introducing the Multimodal Multilingual dataset for Indic Fake News Detection (MMIFND), which comprises 28,085 instances across Hindi, Bengali, Marathi, Malayalam, Tamil, Gujarati, and Punjabi. Additionally, we propose the Multimodal Multilingual Caption-aware framework for Fake News Detection (MMCFND), leveraging pre-trained unimodal and pairwise encoders to align vision and language, extracting deep representations from visual and textual elements of news articles. The multimodal fusion encoder integrates text and image representations to create a comprehensive cross-modal representation. Descriptive image captions are generated to provide context for detecting inconsistencies and manipulations. These features are then fused and input into a classifier to determine the authenticity of news articles. The curated dataset has the potential to significantly advance research and development in low-resource settings. Extensive experimentation on MMIFND demonstrates that our proposed framework surpasses established methods in extracting relevant features for fake news detection."}, "2401.17207": {"original_text": "  A comprehensive understanding of the organizational principles in the human\nbrain requires, among other factors, well-quantifiable descriptors of nerve\nfiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a\nmicroscopic imaging technique that enables insights into the fine-grained\norganization of myelinated nerve fibers with high resolution. Descriptors\ncharacterizing the fiber architecture observed in 3D-PLI would enable\ndownstream analysis tasks such as multimodal correlation studies, clustering,\nand mapping. However, best practices for observer-independent characterization\nof fiber architecture in 3D-PLI are not yet available. To this end, we propose\nthe application of a fully data-driven approach to characterize nerve fiber\narchitecture in 3D-PLI images using self-supervised representation learning. We\nintroduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the\nspatial neighborhood of texture examples across histological brain sections of\na 3D reconstructed volume to sample positive pairs for contrastive learning. We\ncombine this sampling strategy with specifically designed image augmentations\nto gain robustness to typical variations in 3D-PLI parameter maps. The approach\nis demonstrated for the 3D reconstructed occipital lobe of a vervet monkey\nbrain. We show that extracted features are highly sensitive to different\nconfigurations of nerve fibers, yet robust to variations between consecutive\nbrain sections arising from histological processing. We demonstrate their\npractical applicability for retrieving clusters of homogeneous fiber\narchitecture and performing data mining for interactively selected templates of\nspecific components of fiber architecture such as U-fibers.\n", "rewritten_text": "To fully understand how the human brain is organized, it is essential to have precise measurements of nerve fiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a powerful microscopic technique that provides detailed insights into the structure of myelinated nerve fibers. Analyzing the fiber architecture observed in 3D-PLI images can facilitate various research tasks, including correlation studies and mapping. However, there is currently a lack of standardized methods for objectively characterizing fiber architecture in 3D-PLI. In this study, we propose a data-driven approach using self-supervised representation learning to analyze nerve fiber architecture in 3D-PLI images. We introduce a 3D-Context Contrastive Learning (CL-3D) objective that leverages spatial information from texture examples in histological brain sections to enhance contrastive learning. By combining this approach with tailored image augmentations, we ensure robustness to common variations in 3D-PLI data. Our method is demonstrated on the occipital lobe of a vervet monkey brain, showing that the extracted features are sensitive to different nerve fiber configurations while remaining stable across consecutive brain sections. These features can be used to identify clusters of similar fiber architecture and to mine data for specific components like U-fibers."}, "1703.04454": {"original_text": "  We address the problem of estimating human pose and body shape from 3D scans\nover time. Reliable estimation of 3D body shape is necessary for many\napplications including virtual try-on, health monitoring, and avatar creation\nfor virtual reality. Scanning bodies in minimal clothing, however, presents a\npractical barrier to these applications. We address this problem by estimating\nbody shape under clothing from a sequence of 3D scans. Previous methods that\nhave exploited body models produce smooth shapes lacking personalized details.\nWe contribute a new approach to recover a personalized shape of the person. The\nestimated shape deviates from a parametric model to fit the 3D scans. We\ndemonstrate the method using high quality 4D data as well as sequences of\nvisual hulls extracted from multi-view images. We also make available BUFF, a\nnew 4D dataset that enables quantitative evaluation\n(http://buff.is.tue.mpg.de). Our method outperforms the state of the art in\nboth pose estimation and shape estimation, qualitatively and quantitatively.\n", "rewritten_text": "We focus on the challenge of determining human pose and body shape from 3D scans over time. Accurate estimation of 3D body shape is crucial for various applications such as virtual try-on, health monitoring, and creating avatars for virtual reality. However, scanning bodies in minimal clothing poses a practical obstacle for these applications. To address this issue, we propose a method to estimate body shape under clothing using a series of 3D scans. Existing techniques that utilize body models tend to generate smooth shapes without personalized details. Our novel approach aims to recover a personalized shape of the individual by deviating from a parametric model to better fit the 3D scans. We showcase our method using high-quality 4D data and sequences of visual hulls obtained from multi-view images. Additionally, we introduce BUFF, a new 4D dataset that facilitates quantitative evaluation (http://buff.is.tue.mpg.de). Our method surpasses the current state of the art in both pose and shape estimation, demonstrating superior performance both qualitatively and quantitatively."}, "2002.01359": {"original_text": "  This paper gives an overview of the Schema-Guided Dialogue State Tracking\ntask of the 8th Dialogue System Technology Challenge. The goal of this task is\nto develop dialogue state tracking models suitable for large-scale virtual\nassistants, with a focus on data-efficient joint modeling across domains and\nzero-shot generalization to new APIs. This task provided a new dataset\nconsisting of over 16000 dialogues in the training set spanning 16 domains to\nhighlight these challenges, and a baseline model capable of zero-shot\ngeneralization to new APIs. Twenty-five teams participated, developing a range\nof neural network models, exceeding the performance of the baseline model by a\nvery high margin. The submissions incorporated a variety of pre-trained\nencoders and data augmentation techniques. This paper describes the task\ndefinition, dataset and evaluation methodology. We also summarize the approach\nand results of the submitted systems to highlight the overall trends in the\nstate-of-the-art.\n", "rewritten_text": "This document provides an overview of the Schema-Guided Dialogue State Tracking task from the 8th Dialogue System Technology Challenge. The objective of this task is to create dialogue state tracking models suitable for large-scale virtual assistants, emphasizing efficient modeling across different domains and the ability to generalize to new APIs without prior training data. The task introduced a new dataset with over 16,000 dialogues across 16 domains to showcase these challenges, along with a baseline model capable of generalizing to new APIs without prior exposure. Twenty-five teams participated, utilizing various neural network models that outperformed the baseline model significantly. The submissions incorporated diverse pre-trained encoders and data augmentation techniques. This paper outlines the task's definition, dataset, and evaluation methods, as well as summarizes the approaches and results of the submitted systems to highlight the current trends in state-of-the-art technology."}, "2403.10574": {"original_text": "  The rich spatio-temporal information is crucial to capture the complicated\ntarget appearance variations in visual tracking. However, most top-performing\ntracking algorithms rely on many hand-crafted components for spatio-temporal\ninformation aggregation. Consequently, the spatio-temporal information is far\naway from being fully explored. To alleviate this issue, we propose an adaptive\ntracker with spatio-temporal transformers (named AQATrack), which adopts simple\nautoregressive queries to effectively learn spatio-temporal information without\nmany hand-designed components. Firstly, we introduce a set of learnable and\nautoregressive queries to capture the instantaneous target appearance changes\nin a sliding window fashion. Then, we design a novel attention mechanism for\nthe interaction of existing queries to generate a new query in current frame.\nFinally, based on the initial target template and learnt autoregressive\nqueries, a spatio-temporal information fusion module (STM) is designed for\nspatiotemporal formation aggregation to locate a target object. Benefiting from\nthe STM, we can effectively combine the static appearance and instantaneous\nchanges to guide robust tracking. Extensive experiments show that our method\nsignificantly improves the tracker's performance on six popular tracking\nbenchmarks: LaSOT, LaSOText, TrackingNet, GOT-10k, TNL2K, and UAV123.\n", "rewritten_text": "The rich spatio-temporal information plays a crucial role in capturing the complex variations in target appearance during visual tracking. However, many leading tracking algorithms heavily rely on manually crafted components for aggregating spatio-temporal information, leaving much of its potential untapped. To address this limitation, we propose AQATrack, an adaptive tracker with spatio-temporal transformers. This approach utilizes simple autoregressive queries to effectively learn spatio-temporal information without the need for extensive hand-designed components. Our method introduces learnable autoregressive queries to capture instantaneous target appearance changes in a sliding window manner. Additionally, we develop a novel attention mechanism for query interaction to generate new queries in each frame. By combining the initial target template with the learned autoregressive queries, we design a spatio-temporal information fusion module (STM) for aggregating spatiotemporal information to accurately locate a target object. The STM enables the effective integration of static appearance and instantaneous changes for robust tracking. Experimental results demonstrate a significant improvement in tracking performance across various benchmarks, including LaSOT, LaSOText, TrackingNet, GOT-10k, TNL2K, and UAV123."}, "2206.1503": {"original_text": "  Question Answering (QA) is one of the most important natural language\nprocessing (NLP) tasks. It aims using NLP technologies to generate a\ncorresponding answer to a given question based on the massive unstructured\ncorpus. With the development of deep learning, more and more challenging QA\ndatasets are being proposed, and lots of new methods for solving them are also\nemerging. In this paper, we investigate influential QA datasets that have been\nreleased in the era of deep learning. Specifically, we begin with introducing\ntwo of the most common QA tasks - textual question answer and visual question\nanswering - separately, covering the most representative datasets, and then\ngive some current challenges of QA research.\n", "rewritten_text": "\"Question Answering (QA) is a crucial task in natural language processing (NLP), where NLP technologies are utilized to generate answers to questions from unstructured text. The advancement of deep learning has led to the creation of more complex QA datasets and innovative methods for solving them. This study explores significant QA datasets introduced during the deep learning era. It starts by discussing two common QA tasks - textual question answering and visual question answering - individually, highlighting key datasets, and addressing current challenges in QA research.\""}, "1604.04333": {"original_text": "  Deep Convolutional Neural Networks (CNN) have exhibited superior performance\nin many visual recognition tasks including image classification, object\ndetection, and scene label- ing, due to their large learning capacity and\nresistance to overfit. For the image classification task, most of the current\ndeep CNN- based approaches take the whole size-normalized image as input and\nhave achieved quite promising results. Compared with the previously dominating\napproaches based on feature extraction, pooling, and classification, the deep\nCNN-based approaches mainly rely on the learning capability of deep CNN to\nachieve superior results: the burden of minimizing intra-class variation while\nmaximizing inter-class difference is entirely dependent on the implicit feature\nlearning component of deep CNN; we rely upon the implicitly learned filters and\npooling component to select the discriminative regions, which correspond to the\nactivated neurons. However, if the irrelevant regions constitute a large\nportion of the image of interest, the classification performance of the deep\nCNN, which takes the whole image as input, can be heavily affected. To solve\nthis issue, we propose a novel latent CNN framework, which treats the most\ndiscriminate region as a latent variable. We can jointly learn the global CNN\nwith the latent CNN to avoid the aforementioned big irrelevant region issue,\nand our experimental results show the evident advantage of the proposed latent\nCNN over traditional deep CNN: latent CNN outperforms the state-of-the-art\nperformance of deep CNN on standard benchmark datasets including the CIFAR-10,\nCIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset.\n", "rewritten_text": "Deep Convolutional Neural Networks (CNN) have demonstrated superior performance in various visual recognition tasks such as image classification, object detection, and scene labeling, thanks to their large learning capacity and resistance to overfitting. In image classification, current deep CNN-based methods typically use the entire size-normalized image as input and have achieved promising results. Unlike previous approaches that relied on feature extraction, pooling, and classification, deep CNN-based methods leverage the learning capabilities of deep CNN to achieve better results. The deep CNN handles the task of minimizing intra-class variation and maximizing inter-class differences through its implicit feature learning component. By relying on the learned filters and pooling, the deep CNN identifies discriminative regions corresponding to activated neurons. However, if irrelevant regions make up a significant portion of the input image, the classification performance of the deep CNN may suffer. To address this issue, we introduce a novel latent CNN framework that treats the most discriminative region as a latent variable. By jointly training the global CNN with the latent CNN, we mitigate the impact of irrelevant regions and our experiments demonstrate the clear advantage of the proposed latent CNN over traditional deep CNN. The latent CNN surpasses the state-of-the-art performance of deep CNN on standard benchmark datasets such as CIFAR-10, CIFAR-100, MNIST, and PASCAL VOC 2007 Classification dataset."}, "2301.0734": {"original_text": "  Semi-Supervised Semantic Segmentation aims at training the segmentation model\nwith limited labeled data and a large amount of unlabeled data. To effectively\nleverage the unlabeled data, pseudo labeling, along with the teacher-student\nframework, is widely adopted in semi-supervised semantic segmentation. Though\nproved to be effective, this paradigm suffers from incorrect pseudo labels\nwhich inevitably exist and are taken as auxiliary training data. To alleviate\nthe negative impact of incorrect pseudo labels, we delve into the current\nSemi-Supervised Semantic Segmentation frameworks. We argue that the unlabeled\ndata with pseudo labels can facilitate the learning of representative features\nin the feature extractor, but it is unreliable to supervise the mask predictor.\nMotivated by this consideration, we propose a novel framework, Gentle Teaching\nAssistant (GTA-Seg) to disentangle the effects of pseudo labels on feature\nextractor and mask predictor of the student model. Specifically, in addition to\nthe original teacher-student framework, our method introduces a teaching\nassistant network which directly learns from pseudo labels generated by the\nteacher network. The gentle teaching assistant (GTA) is coined gentle since it\nonly transfers the beneficial feature representation knowledge in the feature\nextractor to the student model in an Exponential Moving Average (EMA) manner,\nprotecting the student model from the negative influences caused by unreliable\npseudo labels in the mask predictor. The student model is also supervised by\nreliable labeled data to train an accurate mask predictor, further facilitating\nfeature representation. Extensive experiment results on benchmark datasets\nvalidate that our method shows competitive performance against previous\nmethods. Code is available at https://github.com/Jin-Ying/GTA-Seg.\n", "rewritten_text": "\"Semi-Supervised Semantic Segmentation involves training a segmentation model using a combination of limited labeled data and a large amount of unlabeled data. To make the most of the unlabeled data, the approach of pseudo labeling within a teacher-student framework is commonly used. While this method has been proven effective, it can be hindered by incorrect pseudo labels that are used as additional training data. To address this issue, we examine existing Semi-Supervised Semantic Segmentation frameworks. We propose a new approach called Gentle Teaching Assistant (GTA-Seg) to separate the impact of pseudo labels on the feature extractor and mask predictor of the student model. Our method introduces a teaching assistant network that learns directly from pseudo labels generated by the teacher network. The gentle teaching assistant (GTA) selectively transfers beneficial feature representation knowledge from the feature extractor to the student model in a controlled manner, safeguarding the student model from the negative effects of unreliable pseudo labels on the mask predictor. The student model is also trained using reliable labeled data to improve the accuracy of the mask predictor and enhance feature representation. Extensive experiments on standard datasets demonstrate that our approach performs competitively compared to previous methods. The code can be found at https://github.com/Jin-Ying/GTA-Seg.\""}, "2108.03886": {"original_text": "  A meme is an part of media created to share an opinion or emotion across the\ninternet. Due to its popularity, memes have become the new forms of\ncommunication on social media. However, due to its nature, they are being used\nin harmful ways such as trolling and cyberbullying progressively. Various data\nmodelling methods create different possibilities in feature extraction and\nturning them into beneficial information. The variety of modalities included in\ndata plays a significant part in predicting the results. We try to explore the\nsignificance of visual features of images in classifying memes. Memes are a\nblend of both image and text, where the text is embedded into the image. We try\nto incorporate the memes as troll and non-trolling memes based on the images\nand the text on them. However, the images are to be analysed and combined with\nthe text to increase performance. Our work illustrates different textual\nanalysis methods and contrasting multimodal methods ranging from simple merging\nto cross attention to utilising both worlds' - best visual and textual\nfeatures. The fine-tuned cross-lingual language model, XLM, performed the best\nin textual analysis, and the multimodal transformer performs the best in\nmultimodal analysis.\n", "rewritten_text": "A meme is a form of media designed to convey opinions or emotions online. Memes have become a popular means of communication on social media, but they are also being misused for harmful purposes like trolling and cyberbullying. Various data modeling techniques offer different ways to extract features and convert them into useful information. The different types of data involved play a crucial role in predicting outcomes. This study focuses on the importance of visual elements in classifying memes, which combine images and text. The goal is to categorize memes as either trolling or non-trolling based on their visual and textual content. Analyzing images and text together is key to improving performance. The research explores various textual analysis methods and multimodal approaches, from simple merging to cross attention, to leverage the best visual and textual features. The fine-tuned cross-lingual language model XLM excelled in textual analysis, while the multimodal transformer performed best in analyzing both visual and textual elements."}, "2305.03973": {"original_text": "  Implicit Discourse Relation Recognition (IDRR) is a sophisticated and\nchallenging task to recognize the discourse relations between the arguments\nwith the absence of discourse connectives. The sense labels for each discourse\nrelation follow a hierarchical classification scheme in the annotation process\n(Prasad et al., 2008), forming a hierarchy structure. Most existing works do\nnot well incorporate the hierarchy structure but focus on the syntax features\nand the prior knowledge of connectives in the manner of pure text\nclassification. We argue that it is more effective to predict the paths inside\nthe hierarchical tree (e.g., \"Comparison -> Contrast -> however\") rather than\nflat labels (e.g., Contrast) or connectives (e.g., however). We propose a\nprompt-based path prediction method to utilize the interactive information and\nintrinsic senses among the hierarchy in IDRR. This is the first work that\ninjects such structure information into pre-trained language models via prompt\ntuning, and the performance of our solution shows significant and consistent\nimprovement against competitive baselines.\n", "rewritten_text": "Recognizing Implicit Discourse Relations (IDRR) is a complex task that involves identifying the relationships between arguments without the use of explicit discourse connectives. The annotations for each discourse relation are categorized hierarchically, creating a structured hierarchy. While many existing approaches focus on syntax features and known connectives for text classification, we propose a more effective method that predicts paths within the hierarchical tree (e.g., \"Comparison -> Contrast -> however\") rather than flat labels (e.g., Contrast) or connectives (e.g., however). Our approach, which utilizes a prompt-based path prediction method, leverages the interactive information and inherent relationships within the hierarchy of IDRR. This work is the first to incorporate such structural information into pre-trained language models through prompt tuning, resulting in significant and consistent performance improvements compared to other methods."}, "1908.06665": {"original_text": "  Recently, significant progresses have been made in object detection on common\nbenchmarks (i.e., Pascal VOC). However, object detection in real world is still\nchallenging due to the serious data imbalance. Images in real world are\ndominated by easy samples like the wide range of background and some easily\nrecognizable objects, for example. Although two-stage detectors like Faster\nR-CNN achieved big successes in object detection due to the strategy of\nextracting region proposals by region proposal network, they show their poor\nadaption in real-world object detection as a result of without considering\nmining hard samples during extracting region proposals. To address this issue,\nwe propose a Cascade framework of Region Proposal Networks, referred to as\nC-RPNs. The essence of C-RPNs is adopting multiple stages to mine hard samples\nwhile extracting region proposals and learn stronger classifiers. Meanwhile, a\nfeature chain and a score chain are proposed to help learning more\ndiscriminative representations for proposals. Moreover, a loss function of\ncascade stages is designed to train cascade classifiers through\nbackpropagation. Our proposed method has been evaluated on Pascal VOC and\nseveral challenging datasets like BSBDV 2017, CityPersons, etc. Our method\nachieves competitive results compared with the current state-of-the-arts and\nall-sided improvements in error analysis, validating its efficacy for detection\nin real world.\n", "rewritten_text": "Lately, there have been significant advancements in object detection performance on standard benchmarks such as Pascal VOC. However, detecting objects in real-world scenarios remains a challenge due to imbalanced data. Real-world images are predominantly composed of easy samples like various backgrounds and easily recognizable objects. While two-stage detectors like Faster R-CNN have been successful in object detection by utilizing region proposal networks to extract region proposals, they struggle in real-world scenarios as they do not consider mining hard samples during this process. To tackle this issue, we introduce a Cascade framework of Region Proposal Networks, known as C-RPNs. C-RPNs employ multiple stages to mine hard samples while extracting region proposals and enhance classifier learning. Additionally, we propose a feature chain and a score chain to improve the discriminative representations of proposals. Furthermore, a cascade stages loss function is designed to train cascade classifiers using backpropagation. Our method has been tested on Pascal VOC and challenging datasets like BSBDV 2017, CityPersons, among others. It achieves competitive results compared to current state-of-the-art methods and demonstrates comprehensive improvements in error analysis, confirming its effectiveness in real-world object detection."}, "2001.04388": {"original_text": "  The efficient fusion of depth maps is a key part of most state-of-the-art 3D\nreconstruction methods. Besides requiring high accuracy, these depth fusion\nmethods need to be scalable and real-time capable. To this end, we present a\nnovel real-time capable machine learning-based method for depth map fusion.\nSimilar to the seminal depth map fusion approach by Curless and Levoy, we only\nupdate a local group of voxels to ensure real-time capability. Instead of a\nsimple linear fusion of depth information, we propose a neural network that\npredicts non-linear updates to better account for typical fusion errors. Our\nnetwork is composed of a 2D depth routing network and a 3D depth fusion network\nwhich efficiently handle sensor-specific noise and outliers. This is especially\nuseful for surface edges and thin objects for which the original approach\nsuffers from thickening artifacts. Our method outperforms the traditional\nfusion approach and related learned approaches on both synthetic and real data.\nWe demonstrate the performance of our method in reconstructing fine geometric\ndetails from noise and outlier contaminated data on various scenes.\n", "rewritten_text": "The effective merging of depth maps plays a crucial role in modern 3D reconstruction techniques. In addition to demanding high precision, these depth fusion methods must also be scalable and capable of real-time operation. In this regard, we introduce a new machine learning-based approach for real-time depth map fusion. Similar to the pioneering method by Curless and Levoy, our technique focuses on updating a localized group of voxels to ensure real-time performance. Instead of a basic linear fusion of depth data, we propose a neural network that predicts non-linear adjustments to better address common fusion errors. Our network consists of a 2D depth routing network and a 3D depth fusion network, which effectively handle noise and outliers specific to sensors. This is particularly beneficial for surface edges and slender objects, areas where the original method may produce thickening artifacts. Our approach surpasses traditional fusion methods and other learned techniques in performance evaluations using both synthetic and real-world data. We showcase the effectiveness of our method in accurately reconstructing intricate geometric details from data contaminated with noise and outliers across various scenes."}, "1810.08768": {"original_text": "  Motion estimation (ME) and motion compensation (MC) have been widely used for\nclassical video frame interpolation systems over the past decades. Recently, a\nnumber of data-driven frame interpolation methods based on convolutional neural\nnetworks have been proposed. However, existing learning based methods typically\nestimate either flow or compensation kernels, thereby limiting performance on\nboth computational efficiency and interpolation accuracy. In this work, we\npropose a motion estimation and compensation driven neural network for video\nframe interpolation. A novel adaptive warping layer is developed to integrate\nboth optical flow and interpolation kernels to synthesize target frame pixels.\nThis layer is fully differentiable such that both the flow and kernel\nestimation networks can be optimized jointly. The proposed model benefits from\nthe advantages of motion estimation and compensation methods without using\nhand-crafted features. Compared to existing methods, our approach is\ncomputationally efficient and able to generate more visually appealing results.\nFurthermore, the proposed MEMC-Net can be seamlessly adapted to several video\nenhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive\nquantitative and qualitative evaluations demonstrate that the proposed method\nperforms favorably against the state-of-the-art video frame interpolation and\nenhancement algorithms on a wide range of datasets.\n", "rewritten_text": "Motion estimation (ME) and motion compensation (MC) have long been utilized in traditional video frame interpolation systems. Recently, there has been a surge in data-driven frame interpolation techniques leveraging convolutional neural networks. However, existing learning-based methods often focus on estimating either flow or compensation kernels, leading to limitations in computational efficiency and interpolation accuracy. In this study, we introduce a neural network driven by motion estimation and compensation for video frame interpolation. A novel adaptive warping layer is introduced to combine optical flow and interpolation kernels for synthesizing target frame pixels. This layer is fully differentiable, enabling joint optimization of the flow and kernel estimation networks. Our proposed model harnesses the benefits of motion estimation and compensation methods without relying on hand-crafted features. Compared to existing approaches, our method is computationally efficient and produces visually appealing results. Additionally, the MEMC-Net model can be seamlessly adapted for various video enhancement tasks such as super-resolution, denoising, and deblocking. Extensive quantitative and qualitative evaluations demonstrate the superior performance of our method compared to state-of-the-art video frame interpolation and enhancement algorithms across a diverse range of datasets."}, "2303.12793": {"original_text": "  This work focuses on sign language retrieval-a recently proposed task for\nsign language understanding. Sign language retrieval consists of two sub-tasks:\ntext-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval.\nDifferent from traditional video-text retrieval, sign language videos, not only\ncontain visual signals but also carry abundant semantic meanings by themselves\ndue to the fact that sign languages are also natural languages. Considering\nthis character, we formulate sign language retrieval as a cross-lingual\nretrieval problem as well as a video-text retrieval task. Concretely, we take\ninto account the linguistic properties of both sign languages and natural\nlanguages, and simultaneously identify the fine-grained cross-lingual (i.e.,\nsign-to-word) mappings while contrasting the texts and the sign videos in a\njoint embedding space. This process is termed as cross-lingual contrastive\nlearning. Another challenge is raised by the data scarcity issue-sign language\ndatasets are orders of magnitude smaller in scale than that of speech\nrecognition. We alleviate this issue by adopting a domain-agnostic sign encoder\npre-trained on large-scale sign videos into the target domain via\npseudo-labeling. Our framework, termed as domain-aware sign language retrieval\nvia Cross-lingual Contrastive learning or CiCo for short, outperforms the\npioneering method by large margins on various datasets, e.g., +22.4 T2V and\n+28.0 V2T R@1 improvements on How2Sign dataset, and +13.7 T2V and +17.1 V2T R@1\nimprovements on PHOENIX-2014T dataset. Code and models are available at:\nhttps://github.com/FangyunWei/SLRT.\n", "rewritten_text": "This study focuses on sign language retrieval, a recently introduced task in the field of sign language comprehension. Sign language retrieval involves two main components: text-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval. Unlike traditional video-text retrieval, sign language videos not only convey visual information but also carry significant semantic meanings on their own, as sign languages are natural languages. To address this unique characteristic, we approach sign language retrieval as both a cross-lingual retrieval problem and a video-text retrieval task. Specifically, we consider the linguistic properties of sign languages and natural languages, and simultaneously establish detailed cross-lingual mappings (sign-to-word) while comparing texts and sign videos in a shared embedding space. This approach is referred to as cross-lingual contrastive learning. \n\nOne of the challenges faced in this study is the limited availability of sign language data, which is significantly smaller in scale compared to speech recognition datasets. To mitigate this challenge, we incorporate a domain-agnostic sign encoder that is pre-trained on large-scale sign videos into the target domain using pseudo-labeling. Our proposed framework, known as domain-aware sign language retrieval via Cross-lingual Contrastive learning (CiCo), significantly outperforms existing methods on various datasets. For example, we achieve improvements of +22.4 in T2V and +28.0 in V2T R@1 on the How2Sign dataset, and +13.7 in T2V and +17.1 in V2T R@1 on the PHOENIX-2014T dataset. The code and models for this study can be accessed at: https://github.com/FangyunWei/SLRT."}, "2410.01928": {"original_text": "  In the domain of battery research, the processing of high-resolution\nmicroscopy images is a challenging task, as it involves dealing with complex\nimages and requires a prior understanding of the components involved. The\nutilization of deep learning methodologies for image analysis has attracted\nconsiderable interest in recent years, with multiple investigations employing\nsuch techniques for image segmentation and analysis within the realm of battery\nresearch. However, the automated analysis of high-resolution microscopy images\nfor detecting phases and components in composite materials is still an\nunderexplored area. This work proposes a novel workflow for detecting\ncomponents and phase segmentation from raw high resolution transmission\nelectron microscopy (TEM) images using a trained U-Net segmentation model. The\ndeveloped model can expedite the detection of components and phase\nsegmentation, diminishing the temporal and cognitive demands associated with\nscrutinizing an extensive array of TEM images, thereby mitigating the potential\nfor human errors. This approach presents a novel and efficient image analysis\napproach with broad applicability beyond the battery field and holds potential\nfor application in other related domains characterized by phase and composition\ndistribution, such as alloy production.\n", "rewritten_text": "\"In the field of battery research, analyzing high-resolution microscopy images is a complex task that requires understanding the components involved. Deep learning methods for image analysis have gained interest recently, with many studies using these techniques for image segmentation in battery research. However, automated analysis of high-resolution microscopy images to detect phases and components in composite materials is an area that has not been explored much. This study introduces a new method for detecting components and segmenting phases in raw high-resolution transmission electron microscopy (TEM) images using a trained U-Net segmentation model. The model speeds up the detection process, reducing the time and effort needed to analyze a large number of TEM images and minimizing the risk of human errors. This approach offers an innovative and effective way of analyzing images that can be applied beyond the battery field, potentially benefiting other areas with phase and composition distribution challenges, such as alloy production.\""}, "2103.15467": {"original_text": "  Unsupervised domain adaptation (UDA) becomes more and more popular in\ntackling real-world problems without ground truth of the target domain. Though\ntedious annotation work is not required, UDA unavoidably faces two problems: 1)\nhow to narrow the domain discrepancy to boost the transferring performance; 2)\nhow to improve pseudo annotation producing mechanism for self-supervised\nlearning (SSL). In this paper, we focus on UDA for semantic segmentation task.\nFirstly, we introduce adversarial learning into style gap bridging mechanism to\nkeep the style information from two domains in the similar space. Secondly, to\nkeep the balance of pseudo labels on each category, we propose a\ncategory-adaptive threshold mechanism to choose category-wise pseudo labels for\nSSL. The experiments are conducted using GTA5 as the source domain, Cityscapes\nas the target domain. The results show that our model outperforms the\nstate-of-the-arts with a noticeable gain on cross-domain adaptation tasks.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) is increasingly popular for addressing real-world challenges without access to ground truth data from the target domain. While UDA eliminates the need for laborious annotation, it encounters two key challenges: 1) reducing domain differences to enhance transfer performance, and 2) enhancing the generation of pseudo annotations for self-supervised learning (SSL). This study focuses on UDA for semantic segmentation tasks. Firstly, we incorporate adversarial learning to bridge the style gap between domains and align style information. Secondly, we propose a category-adaptive threshold mechanism to ensure balanced pseudo labels for SSL across categories. Experiments are conducted using GTA5 as the source domain and Cityscapes as the target domain. Results demonstrate that our model surpasses existing methods, achieving significant improvements in cross-domain adaptation tasks."}, "2203.17013": {"original_text": "  Video streams are utilised to guide minimally-invasive surgery and diagnostic\nprocedures in a wide range of procedures, and many computer assisted techniques\nhave been developed to automatically analyse them. These approaches can provide\nadditional information to the surgeon such as lesion detection, instrument\nnavigation, or anatomy 3D shape modeling. However, the necessary image features\nto recognise these patterns are not always reliably detected due to the\npresence of irregular light patterns such as specular highlight reflections. In\nthis paper, we aim at removing specular highlights from endoscopic videos using\nmachine learning. We propose using a temporal generative adversarial network\n(GAN) to inpaint the hidden anatomy under specularities, inferring its\nappearance spatially and from neighbouring frames where they are not present in\nthe same location. This is achieved using in-vivo data of gastric endoscopy\n(Hyper-Kvasir) in a fully unsupervised manner that relies on automatic\ndetection of specular highlights. System evaluations show significant\nimprovements to traditional methods through direct comparison as well as other\nmachine learning techniques through an ablation study that depicts the\nimportance of the network's temporal and transfer learning components. The\ngeneralizability of our system to different surgical setups and procedures was\nalso evaluated qualitatively on in-vivo data of gastric endoscopy and ex-vivo\nporcine data (SERV-CT, SCARED). We also assess the effect of our method in\ncomputer vision tasks that underpin 3D reconstruction and camera motion\nestimation, namely stereo disparity, optical flow, and sparse point feature\nmatching. These are evaluated quantitatively and qualitatively and results show\na positive effect of specular highlight inpainting on these tasks in a novel\ncomprehensive analysis.\n", "rewritten_text": "The use of video streams is common in guiding minimally-invasive surgeries and diagnostic procedures across various medical fields. Numerous computer-assisted techniques have been developed to automatically analyze these video streams, providing surgeons with additional information such as lesion detection, instrument navigation, and 3D shape modeling of anatomy. However, detecting the necessary image features to identify these patterns can be challenging due to irregular light patterns like specular highlight reflections. This study focuses on removing specular highlights from endoscopic videos using machine learning. The proposed approach involves utilizing a temporal generative adversarial network (GAN) to fill in the obscured anatomy under specular highlights, predicting its appearance spatially and across neighboring frames where they are not present in the same location. The method is applied to in-vivo data from gastric endoscopy (Hyper-Kvasir) in a fully unsupervised manner, relying on automatic detection of specular highlights. Evaluation of the system demonstrates significant enhancements over traditional methods through direct comparison and other machine learning techniques, highlighting the importance of the network's temporal and transfer learning components. The system's generalizability to various surgical setups and procedures is qualitatively assessed using in-vivo data from gastric endoscopy and ex-vivo porcine data (SERV-CT, SCARED). Additionally, the impact of the method on computer vision tasks essential for 3D reconstruction and camera motion estimation, such as stereo disparity, optical flow, and sparse point feature matching, is evaluated both quantitatively and qualitatively. The results indicate a positive effect of specular highlight inpainting on these tasks, as demonstrated in a comprehensive analysis."}, "2010.02808": {"original_text": "  We propose a method to learn image representations from uncurated videos. We\ncombine a supervised loss from off-the-shelf object detectors and\nself-supervised losses which naturally arise from the video-shot-frame-object\nhierarchy present in each video. We report competitive results on 19 transfer\nlearning tasks of the Visual Task Adaptation Benchmark (VTAB), and on 8\nout-of-distribution-generalization tasks, and discuss the benefits and\nshortcomings of the proposed approach. In particular, it improves over the\nbaseline on all 18/19 few-shot learning tasks and 8/8 out-of-distribution\ngeneralization tasks. Finally, we perform several ablation studies and analyze\nthe impact of the pretrained object detector on the performance across this\nsuite of tasks.\n", "rewritten_text": "We present a method for acquiring image representations from uncurated videos. Our approach involves utilizing a combination of supervised loss from standard object detectors and self-supervised losses derived from the inherent video-shot-frame-object structure. We demonstrate strong performance on 19 transfer learning tasks within the Visual Task Adaptation Benchmark (VTAB), as well as on 8 out-of-distribution generalization tasks. We discuss the advantages and limitations of our method, highlighting its enhancements over the baseline in 18 out of 19 few-shot learning tasks and all 8 out-of-distribution generalization tasks. Additionally, we conduct various ablation studies to examine the impact of the pretrained object detector on performance across these tasks."}, "1903.12529": {"original_text": "  While deep neural networks (DNN) based single image super-resolution (SISR)\nmethods are rapidly gaining popularity, they are mainly designed for the\nwidely-used bicubic degradation, and there still remains the fundamental\nchallenge for them to super-resolve low-resolution (LR) image with arbitrary\nblur kernels. In the meanwhile, plug-and-play image restoration has been\nrecognized with high flexibility due to its modular structure for easy plug-in\nof denoiser priors. In this paper, we propose a principled formulation and\nframework by extending bicubic degradation based deep SISR with the help of\nplug-and-play framework to handle LR images with arbitrary blur kernels.\nSpecifically, we design a new SISR degradation model so as to take advantage of\nexisting blind deblurring methods for blur kernel estimation. To optimize the\nnew degradation induced energy function, we then derive a plug-and-play\nalgorithm via variable splitting technique, which allows us to plug any\nsuper-resolver prior rather than the denoiser prior as a modular part.\nQuantitative and qualitative evaluations on synthetic and real LR images\ndemonstrate that the proposed deep plug-and-play super-resolution framework is\nflexible and effective to deal with blurry LR images.\n", "rewritten_text": "\"While single image super-resolution (SISR) methods based on deep neural networks (DNN) are becoming increasingly popular, they are primarily tailored for the common bicubic degradation. However, a significant challenge remains in enhancing low-resolution (LR) images with various blur kernels. On the other hand, plug-and-play image restoration is known for its adaptability thanks to its modular structure that allows for easy integration of denoiser priors. This study introduces a systematic approach and framework that extends the capabilities of DNN-based SISR from bicubic degradation to handling LR images with arbitrary blur kernels using the plug-and-play framework. A novel SISR degradation model is developed to leverage existing blind deblurring techniques for blur kernel estimation. By optimizing the new degradation-induced energy function, a plug-and-play algorithm is derived through variable splitting, enabling the integration of any super-resolver prior as a modular component instead of the denoiser prior. Evaluation on both synthetic and real LR images demonstrates the flexibility and effectiveness of the proposed deep plug-and-play super-resolution framework in addressing blurry LR images.\""}, "2407.05092": {"original_text": "  Computational and human perception are often considered separate approaches\nfor studying sound changes over time; few works have touched on the\nintersection of both. To fill this research gap, we provide a pioneering review\ncontrasting computational with human perception from the perspectives of\nmethods and tasks. Overall, computational approaches rely on computer-driven\nmodels to perceive historical sound changes on etymological datasets, while\nhuman approaches use listener-driven models to perceive ongoing sound changes\non recording corpora. Despite their differences, both approaches complement\neach other on phonetic and acoustic levels, showing the potential to achieve a\nmore comprehensive perception of sound change. Moreover, we call for a\ncomparative study on the datasets used by both approaches to investigate the\ninfluence of historical sound changes on ongoing changes. Lastly, we discuss\nthe applications of sound change in computational linguistics, and point out\nthat perceiving sound change alone is insufficient, as many processes of\nlanguage change are complex, with entangled changes at syntactic, semantic, and\nphonetic levels.\n", "rewritten_text": "The study of sound changes over time is often approached separately through computational and human perception methods. Few works have explored the intersection of these two approaches. To address this gap in research, we present a groundbreaking review that compares computational and human perception methods in terms of techniques and objectives. Computational approaches typically use computer-driven models to analyze historical sound changes in etymological datasets, while human approaches rely on listener-driven models to study ongoing sound changes in recording collections. Despite their differences, both approaches complement each other at phonetic and acoustic levels, offering a more comprehensive understanding of sound change. We propose a comparative analysis of the datasets used by both approaches to investigate how historical sound changes influence ongoing changes. Additionally, we discuss the relevance of sound change in computational linguistics and emphasize that understanding sound change alone is insufficient, as language evolution involves complex interactions at syntactic, semantic, and phonetic levels."}, "1812.03621": {"original_text": "  The majority of Multi-Object Tracking (MOT) algorithms based on the\ntracking-by-detection scheme do not use higher order dependencies among objects\nor tracklets, which makes them less effective in handling complex scenarios. In\nthis work, we present a new near-online MOT algorithm based on non-uniform\nhypergraph, which can model different degrees of dependencies among tracklets\nin a unified objective. The nodes in the hypergraph correspond to the tracklets\nand the hyperedges with different degrees encode various kinds of dependencies\namong them. Specifically, instead of setting the weights of hyperedges with\ndifferent degrees empirically, they are learned automatically using the\nstructural support vector machine algorithm (SSVM). Several experiments are\ncarried out on various challenging datasets (i.e., PETS09, ParkingLot sequence,\nSubwayFace, and MOT16 benchmark), to demonstrate that our method achieves\nfavorable performance against the state-of-the-art MOT methods.\n", "rewritten_text": "The majority of Multi-Object Tracking (MOT) algorithms that follow the tracking-by-detection approach often overlook higher order dependencies among objects or tracklets, limiting their effectiveness in handling complex scenarios. In this study, a novel near-online MOT algorithm is introduced, utilizing a non-uniform hypergraph to capture varying degrees of dependencies among tracklets within a unified framework. The nodes in the hypergraph represent tracklets, while hyperedges with different degrees encode diverse dependencies among them. Instead of manually setting hyperedge weights, a structural support vector machine algorithm (SSVM) is employed to learn these weights automatically. Extensive experiments conducted on challenging datasets such as PETS09, ParkingLot sequence, SubwayFace, and MOT16 benchmark demonstrate that our approach outperforms existing state-of-the-art MOT methods."}, "2211.08358": {"original_text": "  Few-shot classification has made great strides due to foundation models that,\nthrough priming and prompting, are highly effective few-shot learners. However,\nthis approach has high variance both across different sets of few shots (data\nselection) and across different finetuning runs (run variability). This is\nproblematic not only because it impedes the fair comparison of different\napproaches, but especially because it makes few-shot learning too unreliable\nfor many real-world applications. To alleviate these issues, we make two\ncontributions for more stable and effective few-shot learning: First, we\npropose novel ensembling methods and show that they substantially reduce run\nvariability. Second, we introduce a new active learning (AL) criterion for data\nselection and present the first AL-based approach specifically tailored towards\nprompt-based learning. In our experiments, we show that our combined method,\nMEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),\nimproves overall performance of prompt-based finetuning by 2.3 points on five\ndiverse tasks. We publicly share our code and data splits in\nhttps://github.com/akoksal/MEAL.\n", "rewritten_text": "\"Recent advancements in few-shot classification have been driven by foundation models that excel at learning with minimal training data through priming and prompting. However, this approach faces challenges such as high variance in data selection and run variability, which hinders fair comparisons and limits the reliability of few-shot learning in practical scenarios. To address these issues, we propose two key enhancements for more consistent and efficient few-shot learning. Firstly, we introduce innovative ensembling techniques that significantly reduce run variability. Secondly, we present a new active learning criterion for data selection, tailored specifically for prompt-based learning. Our combined approach, named MEAL (Multiprompt finetuning and prediction Ensembling with Active Learning), demonstrates a 2.3-point improvement in overall performance across five diverse tasks in our experiments. We have made our code and data splits publicly available at https://github.com/akoksal/MEAL.\""}, "1909.09934": {"original_text": "  We propose methods to train convolutional neural networks (CNNs) with both\nbinarized weights and activations, leading to quantized models that are\nspecifically friendly to mobile devices with limited power capacity and\ncomputation resources. Previous works on quantizing CNNs often seek to\napproximate the floating-point information using a set of discrete values,\nwhich we call value approximation, typically assuming the same architecture as\nthe full-precision networks. Here we take a novel \"structure approximation\"\nview of quantization -- it is very likely that different architectures designed\nfor low-bit networks may be better for achieving good performance. In\nparticular, we propose a \"network decomposition\" strategy, termed Group-Net, in\nwhich we divide the network into groups. Thus, each full-precision group can be\neffectively reconstructed by aggregating a set of homogeneous binary branches.\nIn addition, we learn effective connections among groups to improve the\nrepresentation capability. Moreover, the proposed Group-Net shows strong\ngeneralization to other tasks. For instance, we extend Group-Net for accurate\nsemantic segmentation by embedding rich context into the binary structure.\nFurthermore, for the first time, we apply binary neural networks to object\ndetection. Experiments on both classification, semantic segmentation and object\ndetection tasks demonstrate the superior performance of the proposed methods\nover various quantized networks in the literature. Our methods outperform the\nprevious best binary neural networks in terms of accuracy and computation\nefficiency.\n", "rewritten_text": "We present techniques for training convolutional neural networks (CNNs) using binarized weights and activations, resulting in quantized models optimized for mobile devices with limited power and computational resources. While previous approaches to quantizing CNNs focus on approximating floating-point information with discrete values, known as value approximation, we introduce a new perspective called \"structure approximation.\" This approach suggests that different architectures tailored for low-bit networks may yield better performance. Our novel strategy, Group-Net, involves dividing the network into groups, allowing each group to be reconstructed using binary branches. We also optimize connections between groups to enhance representation capabilities. Group-Net demonstrates strong generalization across various tasks, such as semantic segmentation and object detection. Our experiments show that our methods outperform existing quantized networks in terms of accuracy and computational efficiency, marking a significant advancement in the field of binary neural networks."}, "2407.04024": {"original_text": "  Deep unfolding methods and transformer architecture have recently shown\npromising results in hyperspectral image (HSI) reconstruction. However, there\nstill exist two issues: (1) in the data subproblem, most methods represents the\nstepsize utilizing a learnable parameter. Nevertheless, for different spectral\nchannel, error between features and ground truth is unequal. (2) Transformer\nstruggles to balance receptive field size with pixel-wise detail information.\nTo overcome the aforementioned drawbacks, We proposed an adaptive step-size\nperception unfolding network (ASPUN), a deep unfolding network based on FISTA\nalgorithm, which uses an adaptive step-size perception module to estimate the\nupdate step-size of each spectral channel. In addition, we design a Non-local\nHybrid Attention Transformer(NHAT) module for fully leveraging the receptive\nfield advantage of transformer. By plugging the NLHA into the Non-local\nInformation Aggregation (NLIA) module, the unfolding network can achieve better\nreconstruction results. Experimental results show that our ASPUN is superior to\nthe existing SOTA algorithms and achieves the best performance.\n", "rewritten_text": "Recently, deep unfolding methods and transformer architecture have demonstrated promising outcomes in reconstructing hyperspectral images (HSI). However, two challenges persist: (1) existing methods often use a learnable parameter to represent the stepsize in the data subproblem, leading to unequal errors between features and ground truth across different spectral channels. (2) Transformers struggle to effectively balance receptive field size and pixel-wise detail information. To address these issues, we introduce an adaptive step-size perception unfolding network (ASPUN) based on the FISTA algorithm. ASPUN incorporates an adaptive step-size perception module to estimate the update step-size for each spectral channel. Additionally, we introduce a Non-local Hybrid Attention Transformer (NHAT) module to fully leverage the receptive field advantages of transformers. By integrating NHAT into the Non-local Information Aggregation (NLIA) module, our unfolding network achieves improved reconstruction results. Experimental findings demonstrate that ASPUN outperforms state-of-the-art algorithms, delivering the best performance."}, "1810.10519": {"original_text": "  With the increasing use of social networks and mobile devices, the number of\nvideos posted on the Internet is growing exponentially. Among the inappropriate\ncontents published on the Internet, pornography is one of the most worrying as\nit can be accessed by teens and children. Two spatiotemporal CNNs, VGG-C3D CNN\nand ResNet R(2+1)D CNN, were assessed for pornography detection in videos in\nthe present study. Experimental results using the Pornography-800 dataset\nshowed that these spatiotemporal CNNs performed better than some\nstate-of-the-art methods based on bag of visual words and are competitive with\nother CNN-based approaches, reaching accuracy of 95.1%.\n", "rewritten_text": "\"With the rise in social media usage and mobile device adoption, the quantity of videos shared online is rapidly increasing. Among the inappropriate content found on the web, pornography is particularly concerning due to its accessibility to teenagers and children. This study evaluated two spatiotemporal CNNs, VGG-C3D CNN and ResNet R(2+1)D CNN, for detecting pornography in videos. Results from experiments using the Pornography-800 dataset demonstrated that these CNNs outperformed some traditional methods like bag of visual words and were on par with other CNN-based approaches, achieving an accuracy of 95.1%.\""}, "2406.10209": {"original_text": "  Large language models can memorize and repeat their training data, causing\nprivacy and copyright risks. To mitigate memorization, we introduce a subtle\nmodification to the next-token training objective that we call the goldfish\nloss. During training, randomly sampled subsets of tokens are excluded from the\nloss computation. These dropped tokens are not memorized by the model, which\nprevents verbatim reproduction of a complete chain of tokens from the training\nset. We run extensive experiments training billion-scale Llama-2 models, both\npre-trained and trained from scratch, and demonstrate significant reductions in\nextractable memorization with little to no impact on downstream benchmarks.\n", "rewritten_text": "\"Large language models have the ability to remember and replicate their training data, which poses risks related to privacy and copyright. To address this issue, we propose a modification to the training objective known as the goldfish loss. This modification involves excluding randomly sampled subsets of tokens from the loss computation during training. By not memorizing these dropped tokens, the model is prevented from reproducing an entire sequence of tokens from the training data verbatim. Through extensive experiments with billion-scale Llama-2 models, both pre-trained and trained from scratch, we show significant reductions in memorization without negatively impacting downstream benchmarks.\""}, "2110.14398": {"original_text": "  To consider Hawrami and Zaza (Zazaki) standalone languages or dialects of a\nlanguage have been discussed and debated for a while among linguists active in\nstudying Iranian languages. The question of whether those languages/dialects\nbelong to the Kurdish language or if they are independent descendants of\nIranian languages was answered by MacKenzie (1961). However, a majority of\npeople who speak the dialects are against that answer. Their disapproval mainly\nseems to be based on the sociological, cultural, and historical relationship\namong the speakers of the dialects. While the case of Hawrami and Zaza has\nremained unexplored and under-examined, an almost unanimous agreement exists\nabout the classification of Kurmanji and Sorani as Kurdish dialects. The\nrelated studies to address the mentioned cases are primarily qualitative.\nHowever, computational linguistics could approach the question from a\nquantitative perspective. In this research, we look into three questions from a\nlinguistic distance point of view. First, how similar or dissimilar Hawrami and\nZaza are, considering no common geographical coexistence between the two.\nSecond, what about Kurmanji and Sorani that have geographical overlap. Finally,\nwhat is the distance among all these dialects, pair by pair? We base our\ncomputation on phonetic presentations of these dialects (languages), and we\ncalculate various linguistic distances among the pairs. We analyze the data and\ndiscuss the results to conclude.\n", "rewritten_text": "The classification of Hawrami and Zaza (Zazaki) as standalone languages or dialects of a language has been a topic of debate among linguists specializing in Iranian languages. MacKenzie (1961) provided an answer to whether these languages/dialects are part of the Kurdish language or independent descendants of Iranian languages. However, many speakers of these dialects disagree with this conclusion, citing sociological, cultural, and historical connections among the speakers. While Hawrami and Zaza have not been extensively studied, there is general consensus on Kurmanji and Sorani being Kurdish dialects. Existing studies on this topic are primarily qualitative, but computational linguistics could offer a quantitative perspective. This research aims to examine three questions from a linguistic distance standpoint: the similarity between Hawrami and Zaza despite no geographical overlap, the relationship between Kurmanji and Sorani which do share a geographical region, and the distances between all dialects when compared pair by pair. The analysis is based on phonetic representations of these dialects/languages, calculating various linguistic distances between them. The data is analyzed, and the results are discussed to draw conclusions."}, "2401.14718": {"original_text": "  Future Frame Synthesis (FFS) aims to enable models to generate sequences of\nfuture frames based on existing content. This survey comprehensively reviews\nhistorical and contemporary works in FFS, including widely used datasets and\nalgorithms. It scrutinizes the challenges and the evolving landscape of FFS\nwithin computer vision, with a focus on the transition from deterministic to\ngenerative synthesis methodologies. Our taxonomy highlights the significant\nadvancements and shifts in approach, underscoring the growing importance of\ngenerative models in achieving realistic and diverse future frame predictions.\n", "rewritten_text": "The goal of Future Frame Synthesis (FFS) is to empower models to create sequences of upcoming frames using current content. This review thoroughly examines past and present research in FFS, covering popular datasets and algorithms. It delves into the obstacles and changes in FFS within computer vision, particularly the shift from deterministic to generative synthesis methods. Our classification system emphasizes the notable progress and changes in strategy, emphasizing the increasing significance of generative models in producing realistic and varied future frame forecasts."}, "1808.0841": {"original_text": "  Training a good deep learning model often requires a lot of annotated data.\nAs a large amount of labeled data is typically difficult to collect and even\nmore difficult to annotate, data augmentation and data generation are widely\nused in the process of training deep neural networks. However, there is no\nclear common understanding on how much labeled data is needed to get\nsatisfactory performance. In this paper, we try to address such a question\nusing vehicle license plate character recognition as an example application. We\napply computer graphic scripts and Generative Adversarial Networks to generate\nand augment a large number of annotated, synthesized license plate images with\nrealistic colors, fonts, and character composition from a small number of real,\nmanually labeled license plate images. Generated and augmented data are mixed\nand used as training data for the license plate recognition network modified\nfrom DenseNet. The experimental results show that the model trained from the\ngenerated mixed training data has good generalization ability, and the proposed\napproach achieves a new state-of-the-art accuracy on Dataset-1 and AOLP, even\nwith a very limited number of original real license plates. In addition, the\naccuracy improvement caused by data generation becomes more significant when\nthe number of labeled images is reduced. Data augmentation also plays a more\nsignificant role when the number of labeled images is increased.\n", "rewritten_text": "Training an effective deep learning model often necessitates a substantial amount of annotated data. Given the challenges of collecting and annotating a large volume of labeled data, data augmentation and generation techniques are commonly employed during the training of deep neural networks. However, there is a lack of consensus on the optimal amount of labeled data required to achieve satisfactory performance. This study aims to address this issue by focusing on vehicle license plate character recognition as a case study. By leveraging computer graphic scripts and Generative Adversarial Networks, we generate and augment a vast number of annotated, synthetic license plate images featuring realistic colors, fonts, and character compositions using only a small set of manually labeled real license plate images. The combined generated and augmented data are utilized as training data for a license plate recognition network based on DenseNet. Our experimental findings demonstrate that the model trained on the mixed generated training data exhibits strong generalization capabilities, achieving state-of-the-art accuracy on Dataset-1 and AOLP, even with a limited number of original real license plates. Furthermore, the impact of data generation on accuracy improvement becomes more pronounced as the number of labeled images decreases, while data augmentation becomes increasingly influential as the number of labeled images rises."}, "2401.03183": {"original_text": "  Defeasibility in causal reasoning implies that the causal relationship\nbetween cause and effect can be strengthened or weakened. Namely, the causal\nstrength between cause and effect should increase or decrease with the\nincorporation of strengthening arguments (supporters) or weakening arguments\n(defeaters), respectively. However, existing works ignore defeasibility in\ncausal reasoning and fail to evaluate existing causal strength metrics in\ndefeasible settings. In this work, we present $\\delta$-CAUSAL, the first\nbenchmark dataset for studying defeasibility in causal reasoning.\n$\\delta$-CAUSAL includes around 11K events spanning ten domains, featuring\ndefeasible causality pairs, i.e., cause-effect pairs accompanied by supporters\nand defeaters. We further show current causal strength metrics fail to reflect\nthe change of causal strength with the incorporation of supporters or defeaters\nin $\\delta$-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation\nwith Attention Rating), a metric that measures causal strength based on\ntoken-level causal relationships. CESAR achieves a significant 69.7% relative\nimprovement over existing metrics, increasing from 47.2% to 80.1% in capturing\nthe causal strength change brought by supporters and defeaters. We further\ndemonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and\n10.7 points behind humans in generating supporters and defeaters, emphasizing\nthe challenge posed by $\\delta$-CAUSAL.\n", "rewritten_text": "Defeasibility in causal reasoning refers to the idea that the strength of the causal relationship between a cause and its effect can be either enhanced or diminished. This means that the connection between cause and effect should either strengthen or weaken when additional supporting arguments or opposing arguments are introduced. Despite the importance of defeasibility in causal reasoning, current research overlooks this aspect and does not adequately assess how existing metrics for measuring causal strength perform in situations involving defeasibility. In this study, we introduce $\\delta$-CAUSAL, a novel benchmark dataset designed to explore defeasibility in causal reasoning. $\\delta$-CAUSAL comprises approximately 11,000 events across ten different domains, each featuring causal pairs accompanied by both supporting evidence and counterarguments. Our analysis reveals that current metrics for evaluating causal strength do not accurately capture the changes in causal relationships when supporters or defeaters are introduced in $\\delta$-CAUSAL. To address this gap, we propose CESAR (Causal Embedding aSsociation with Attention Rating), a metric that assesses causal strength based on token-level causal connections. CESAR demonstrates a substantial 69.7% improvement over existing metrics, increasing from 47.2% to 80.1% in accurately reflecting the impact of supporters and defeaters on causal strength. Additionally, we find that even advanced language models like GPT-3.5 still fall short by 4.5 and 10.7 points compared to human performance in generating supporting evidence and counterarguments, underscoring the complexity presented by $\\delta$-CAUSAL."}, "1310.5767": {"original_text": "  Salient object detection aims to locate objects that capture human attention\nwithin images. Previous approaches often pose this as a problem of image\ncontrast analysis. In this work, we model an image as a hypergraph that\nutilizes a set of hyperedges to capture the contextual properties of image\npixels or regions. As a result, the problem of salient object detection becomes\none of finding salient vertices and hyperedges in the hypergraph. The main\nadvantage of hypergraph modeling is that it takes into account each pixel's (or\nregion's) affinity with its neighborhood as well as its separation from image\nbackground. Furthermore, we propose an alternative approach based on\ncenter-versus-surround contextual contrast analysis, which performs salient\nobject detection by optimizing a cost-sensitive support vector machine (SVM)\nobjective function. Experimental results on four challenging datasets\ndemonstrate the effectiveness of the proposed approaches against the\nstate-of-the-art approaches to salient object detection.\n", "rewritten_text": "Salient object detection is the task of identifying objects that attract human attention in images. Traditional methods have typically approached this by analyzing image contrast. In this study, we present a novel approach where images are represented as hypergraphs, using hyperedges to capture contextual relationships among image pixels or regions. This transforms salient object detection into the identification of important vertices and hyperedges within the hypergraph. The key benefit of hypergraph modeling is its ability to consider the affinity of each pixel (or region) with its surroundings and its distinction from the image background. Additionally, we introduce an alternative method based on center-versus-surround contrast analysis, which employs a cost-sensitive support vector machine (SVM) objective function for salient object detection. Results from experiments on four challenging datasets showcase the effectiveness of our proposed techniques compared to existing state-of-the-art methods in salient object detection."}, "2111.02394": {"original_text": "  We propose an accurate and efficient scene text detection framework, termed\nFAST (i.e., faster arbitrarily-shaped text detector). Different from recent\nadvanced text detectors that used complicated post-processing and hand-crafted\nnetwork architectures, resulting in low inference speed, FAST has two new\ndesigns. (1) We design a minimalist kernel representation (only has 1-channel\noutput) to model text with arbitrary shape, as well as a GPU-parallel\npost-processing to efficiently assemble text lines with a negligible time\noverhead. (2) We search the network architecture tailored for text detection,\nleading to more powerful features than most networks that are searched for\nimage classification. Benefiting from these two designs, FAST achieves an\nexcellent trade-off between accuracy and efficiency on several challenging\ndatasets, including Total Text, CTW1500, ICDAR 2015, and MSRA-TD500. For\nexample, FAST-T yields 81.6% F-measure at 152 FPS on Total-Text, outperforming\nthe previous fastest method by 1.7 points and 70 FPS in terms of accuracy and\nspeed. With TensorRT optimization, the inference speed can be further\naccelerated to over 600 FPS. Code and models will be released at\nhttps://github.com/czczup/FAST.\n", "rewritten_text": "We introduce a new scene text detection framework called FAST (faster arbitrarily-shaped text detector) that is both accurate and efficient. Unlike other recent text detectors that rely on complex post-processing and network architectures, resulting in slow inference speeds, FAST incorporates two innovative designs. Firstly, we utilize a minimalist kernel representation with a single-channel output to capture text of any shape, along with a GPU-parallel post-processing method for quickly assembling text lines with minimal time overhead. Secondly, we have developed a network architecture specifically tailored for text detection, which generates more powerful features compared to networks designed for image classification. These design choices enable FAST to strike a balance between accuracy and efficiency across various challenging datasets such as Total Text, CTW1500, ICDAR 2015, and MSRA-TD500. For instance, FAST-T achieves an F-measure of 81.6% at 152 FPS on Total-Text, surpassing the previous fastest method by 1.7 points and 70 FPS in terms of both accuracy and speed. By leveraging TensorRT optimization, the inference speed can be further boosted to over 600 FPS. The code and models will be made available at https://github.com/czczup/FAST."}, "2403.17856": {"original_text": "  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.\n", "rewritten_text": "\"English morphology is characterized by lexical-syntactic flexibility, demonstrated through conversion (or zero-derivation). This process involves placing a word in a context where it behaves as a different part of speech than its original form. Despite its widespread impact on the English lexicon, there has been limited research on how well language models capture this type of generalization. This study presents the first examination of large language models' performance in relation to conversion. A task was designed to assess lexical-syntactic flexibility, measuring models' ability to generalize over words in non-prototypical part-of-speech constructions within a natural language inference framework. Five language models were tested: two proprietary models (GPT-3.5 and GPT-4) and three open-source models (Mistral 7B, Falcon 40B, and Llama 2 70B). Results show that GPT-4 excels in the task, followed by GPT-3.5, while the open-source models also demonstrate competence. Interestingly, Mistral 7B shows minimal performance difference between its baseline on natural language inference and the non-prototypical syntactic category task, similar to the larger GPT-4 model.\""}, "1712.05277": {"original_text": "  Depth cameras allow to set up reliable solutions for people monitoring and\nbehavior understanding, especially when unstable or poor illumination\nconditions make unusable common RGB sensors. Therefore, we propose a complete\nframework for the estimation of the head and shoulder pose based on depth\nimages only. A head detection and localization module is also included, in\norder to develop a complete end-to-end system. The core element of the\nframework is a Convolutional Neural Network, called POSEidon+, that receives as\ninput three types of images and provides the 3D angles of the pose as output.\nMoreover, a Face-from-Depth component based on a Deterministic Conditional GAN\nmodel is able to hallucinate a face from the corresponding depth image. We\nempirically demonstrate that this positively impacts the system performances.\nWe test the proposed framework on two public datasets, namely Biwi Kinect Head\nPose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by\nthe automotive setup. Experimental results show that our method overcomes\nseveral recent state-of-art works based on both intensity and depth input data,\nrunning in real-time at more than 30 frames per second.\n", "rewritten_text": "Depth cameras enable the creation of reliable solutions for monitoring people and understanding their behavior, particularly in situations where common RGB sensors are ineffective due to unstable or poor lighting conditions. To address this, we present a comprehensive framework for estimating head and shoulder poses using only depth images. The framework includes a module for detecting and localizing heads to develop a complete end-to-end system. At the core of the framework is a Convolutional Neural Network named POSEidon+, which takes three types of images as input and outputs the 3D angles of the pose. Additionally, a Face-from-Depth component utilizes a Deterministic Conditional GAN model to generate a face from the corresponding depth image, leading to improved system performance. Our framework is evaluated on public datasets such as Biwi Kinect Head Pose and ICT-3DHP, as well as on Pandora, a new challenging dataset inspired by automotive settings. Experimental results demonstrate that our method outperforms recent state-of-the-art approaches using both intensity and depth data, achieving real-time performance of over 30 frames per second."}, "2404.19287": {"original_text": "  Pretrained vision-language models (VLMs) like CLIP exhibit exceptional\ngeneralization across diverse downstream tasks. While recent studies reveal\ntheir vulnerability to adversarial attacks, research to date has primarily\nfocused on enhancing the robustness of image encoders against image-based\nattacks, with defenses against text-based and multimodal attacks remaining\nlargely unexplored. To this end, this work presents the first comprehensive\nstudy on improving the adversarial robustness of VLMs against attacks targeting\nimage, text, and multimodal inputs. This is achieved by proposing multimodal\ncontrastive adversarial training (MMCoA). Such an approach strengthens the\nrobustness of both image and text encoders by aligning the clean text\nembeddings with adversarial image embeddings, and adversarial text embeddings\nwith clean image embeddings. The robustness of the proposed MMCoA is examined\nagainst existing defense methods over image, text, and multimodal attacks on\nthe CLIP model. Extensive experiments on 15 datasets across two tasks reveal\nthe characteristics of different adversarial defense methods under distinct\ndistribution shifts and dataset complexities across the three attack types.\nThis paves the way for a unified framework of adversarial robustness against\ndifferent modality attacks, opening up new possibilities for securing VLMs\nagainst multimodal attacks. The code is available at\nhttps://github.com/ElleZWQ/MMCoA.git.\n", "rewritten_text": "\"Pretrained vision-language models (VLMs) such as CLIP demonstrate remarkable generalization capabilities across various downstream tasks. While recent research has shown their susceptibility to adversarial attacks, current studies have mainly concentrated on enhancing the resilience of image encoders to image-based attacks, neglecting defenses against text-based and multimodal attacks. This study introduces a comprehensive investigation into enhancing the adversarial robustness of VLMs against attacks targeting image, text, and multimodal inputs. The proposed approach, multimodal contrastive adversarial training (MMCoA), aims to bolster the robustness of both image and text encoders by aligning clean text embeddings with adversarial image embeddings, and adversarial text embeddings with clean image embeddings. The effectiveness of MMCoA is evaluated against existing defense methods for image, text, and multimodal attacks on the CLIP model through extensive experiments on 15 datasets spanning two tasks. These experiments shed light on the performance of different adversarial defense methods under various distribution shifts and dataset complexities across the three types of attacks, laying the groundwork for a unified framework for defending against different modality attacks and enhancing the security of VLMs. The code for MMCoA can be accessed at https://github.com/ElleZWQ/MMCoA.git.\""}, "2211.09469": {"original_text": "  Generating consecutive descriptions for videos, i.e., Video Captioning,\nrequires taking full advantage of visual representation along with the\ngeneration process. Existing video captioning methods focus on making an\nexploration of spatial-temporal representations and their relationships to\nproduce inferences. However, such methods only exploit the superficial\nassociation contained in the video itself without considering the intrinsic\nvisual commonsense knowledge that existed in a video dataset, which may hinder\ntheir capabilities of knowledge cognitive to reason accurate descriptions. To\naddress this problem, we propose a simple yet effective method, called Visual\nCommonsense-aware Representation Network (VCRN), for video captioning.\nSpecifically, we construct a Video Dictionary, a plug-and-play component,\nobtained by clustering all video features from the total dataset into multiple\nclustered centers without additional annotation. Each center implicitly\nrepresents a visual commonsense concept in the video domain, which is utilized\nin our proposed Visual Concept Selection (VCS) to obtain a video-related\nconcept feature. Next, a Conceptual Integration Generation (CIG) is proposed to\nenhance the caption generation. Extensive experiments on three publicly video\ncaptioning benchmarks: MSVD, MSR-VTT, and VATEX, demonstrate that our method\nreaches state-of-the-art performance, indicating the effectiveness of our\nmethod. In addition, our approach is integrated into the existing method of\nvideo question answering and improves this performance, further showing the\ngeneralization of our method. Source code has been released at\nhttps://github.com/zchoi/VCRN.\n", "rewritten_text": "Creating sequential descriptions for videos, known as Video Captioning, requires leveraging visual representation in conjunction with the generation process. Current video captioning techniques primarily focus on exploring spatial-temporal representations and their interconnections to generate inferences. However, these methods often only utilize the surface-level associations present within the video itself, neglecting the inherent visual common sense knowledge within the video dataset. This oversight may limit their ability to generate accurate descriptions based on cognitive reasoning. To tackle this issue, we introduce a straightforward yet efficient approach called the Visual Commonsense-aware Representation Network (VCRN) for video captioning. Specifically, we establish a Video Dictionary, a versatile component derived by clustering all video features from the entire dataset into multiple clustered centers without requiring additional annotation. Each center implicitly represents a visual commonsense concept within the video domain, which is harnessed in our proposed Visual Concept Selection (VCS) to extract a video-relevant concept feature. Subsequently, we introduce a Conceptual Integration Generation (CIG) to enhance the caption generation process. Through extensive experiments on three widely-used video captioning benchmarks - MSVD, MSR-VTT, and VATEX - our method demonstrates state-of-the-art performance, underscoring its effectiveness. Furthermore, our approach is seamlessly integrated into existing video question answering methods, leading to performance improvements and showcasing the versatility of our approach. The source code is available at https://github.com/zchoi/VCRN."}, "2203.12788": {"original_text": "  A fundamental characteristic of natural language is the high rate at which\nspeakers produce novel expressions. Because of this novelty, a heavy-tail of\nrare events accounts for a significant amount of the total probability mass of\ndistributions in language (Baayen, 2001). Standard language modeling metrics\nsuch as perplexity quantify the performance of language models (LM) in\naggregate. As a result, we have relatively little understanding of whether\nneural LMs accurately estimate the probability of sequences in this heavy-tail\nof rare events. To address this gap, we develop a controlled evaluation scheme\nwhich uses generative models trained on natural data as artificial languages\nfrom which we can exactly compute sequence probabilities. Training LMs on\ngenerations from these artificial languages, we compare the sequence-level\nprobability estimates given by LMs to the true probabilities in the target\nlanguage. Our experiments reveal that LSTM and Transformer language models (i)\nsystematically underestimate the probability of sequences drawn from the target\nlanguage, and (ii) do so more severely for less-probable sequences.\nInvestigating where this probability mass went, (iii) we find that LMs tend to\noverestimate the probability of ill formed (perturbed) sequences. In addition,\nwe find that this underestimation behaviour (iv) is weakened, but not\neliminated by greater amounts of training data, and (v) is exacerbated for\ntarget distributions with lower entropy.\n", "rewritten_text": "One key feature of natural language is the frequent creation of new expressions by speakers. This novelty leads to a distribution in language where rare events play a significant role in the overall probability mass. Traditional metrics like perplexity assess the performance of language models in a general sense, but they do not provide insight into how well neural language models estimate the probabilities of rare events. To address this gap, we introduce a method for evaluating language models using artificial languages based on natural data. By training language models on these artificial languages, we can compare their probability estimates for sequences to the actual probabilities in the target language. Our experiments show that LSTM and Transformer models tend to underestimate the probabilities of sequences from the target language, especially for less probable sequences. We also observe that language models often overestimate the probabilities of incorrectly formed sequences. Furthermore, this underestimation tendency is not completely eliminated with more training data and is more pronounced in distributions with lower entropy."}, "2207.09884": {"original_text": "  We present a Momentum Re-identification (MoReID) framework that can leverage\na very large number of negative samples in training for general\nre-identification task. The design of this framework is inspired by Momentum\nContrast (MoCo), which uses a dictionary to store current and past batches to\nbuild a large set of encoded samples. As we find it less effective to use past\npositive samples which may be highly inconsistent to the encoded feature\nproperty formed with the current positive samples, MoReID is designed to use\nonly a large number of negative samples stored in the dictionary. However, if\nwe train the model using the widely used Triplet loss that uses only one sample\nto represent a set of positive/negative samples, it is hard to effectively\nleverage the enlarged set of negative samples acquired by the MoReID framework.\nTo maximize the advantage of using the scaled-up negative sample set, we newly\nintroduce Hard-distance Elastic loss (HE loss), which is capable of using more\nthan one hard sample to represent a large number of samples. Our experiments\ndemonstrate that a large number of negative samples provided by MoReID\nframework can be utilized at full capacity only with the HE loss, achieving the\nstate-of-the-art accuracy on three re-ID benchmarks, VeRi-776, Market-1501, and\nVeRi-Wild.\n", "rewritten_text": "We introduce a Momentum Re-identification (MoReID) framework that utilizes a vast number of negative samples during training for general re-identification tasks. Inspired by Momentum Contrast (MoCo), our framework employs a dictionary to store current and past batches, creating a large set of encoded samples. Unlike using past positive samples, which may not align well with current positive samples, MoReID focuses on utilizing a large number of negative samples stored in the dictionary. When training with the commonly used Triplet loss, which represents a set of positive/negative samples with only one sample, it becomes challenging to effectively leverage the expanded set of negative samples obtained by MoReID. To fully exploit the benefits of the increased negative sample set, we introduce the Hard-distance Elastic loss (HE loss), which can utilize multiple hard samples to represent numerous samples. Our experiments show that the MoReID framework's abundance of negative samples can be maximized with the HE loss, leading to state-of-the-art accuracy on three re-ID benchmarks: VeRi-776, Market-1501, and VeRi-Wild."}, "2410.12158": {"original_text": "  Foundation models have significantly enhanced 2D task performance, and recent\nworks like Bridge3D have successfully applied these models to improve 3D scene\nunderstanding through knowledge distillation, marking considerable\nadvancements. Nonetheless, challenges such as the misalignment between 2D and\n3D representations and the persistent long-tail distribution in 3D datasets\nstill restrict the effectiveness of knowledge distillation from 2D to 3D using\nfoundation models. To tackle these issues, we introduce a novel SAM-guided\ntokenization method that seamlessly aligns 3D transformer structures with\nregion-level knowledge distillation, replacing the traditional KNN-based\ntokenization techniques. Additionally, we implement a group-balanced\nre-weighting strategy to effectively address the long-tail problem in knowledge\ndistillation. Furthermore, inspired by the recent success of masked feature\nprediction, our framework incorporates a two-stage masked token prediction\nprocess in which the student model predicts both the global embeddings and the\ntoken-wise local embeddings derived from the teacher models trained in the\nfirst stage. Our methodology has been validated across multiple datasets,\nincluding SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and\nsemantic segmentation. The results demonstrate significant improvements over\ncurrent State-of-the-art self-supervised methods, establishing new benchmarks\nin this field.\n", "rewritten_text": "Foundation models have greatly improved performance in 2D tasks, and recent research such as Bridge3D has effectively utilized these models to enhance 3D scene understanding through knowledge distillation, representing significant progress. However, challenges persist, such as the discrepancy between 2D and 3D representations and the prevalent long-tail distribution in 3D datasets, which hinder the efficacy of knowledge distillation from 2D to 3D using foundation models. To address these issues, we propose a new SAM-guided tokenization approach that aligns 3D transformer structures with region-level knowledge distillation, replacing traditional KNN-based tokenization methods. Additionally, we introduce a group-balanced re-weighting strategy to tackle the long-tail problem in knowledge distillation. Moreover, drawing inspiration from successful masked feature prediction techniques, our framework incorporates a two-stage masked token prediction process where the student model predicts global embeddings and token-wise local embeddings derived from teacher models trained in the initial stage. Our methodology has been tested on various datasets, including SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and semantic segmentation. The results show significant enhancements compared to current state-of-the-art self-supervised methods, setting new benchmarks in this field."}, "2307.09892": {"original_text": "  We propose 3Deformer, a general-purpose framework for interactive 3D shape\nediting. Given a source 3D mesh with semantic materials, and a user-specified\nsemantic image, 3Deformer can accurately edit the source mesh following the\nshape guidance of the semantic image, while preserving the source topology as\nrigid as possible. Recent studies of 3D shape editing mostly focus on learning\nneural networks to predict 3D shapes, which requires high-cost 3D training\ndatasets and is limited to handling objects involved in the datasets. Unlike\nthese studies, our 3Deformer is a non-training and common framework, which only\nrequires supervision of readily-available semantic images, and is compatible\nwith editing various objects unlimited by datasets. In 3Deformer, the source\nmesh is deformed utilizing the differentiable renderer technique, according to\nthe correspondences between semantic images and mesh materials. However,\nguiding complex 3D shapes with a simple 2D image incurs extra challenges, that\nis, the deform accuracy, surface smoothness, geometric rigidity, and global\nsynchronization of the edited mesh should be guaranteed. To address these\nchallenges, we propose a hierarchical optimization architecture to balance the\nglobal and local shape features, and propose further various strategies and\nlosses to improve properties of accuracy, smoothness, rigidity, and so on.\nExtensive experiments show that our 3Deformer is able to produce impressive\nresults and reaches the state-of-the-art level.\n", "rewritten_text": "We introduce 3Deformer, a versatile framework designed for interactive editing of 3D shapes. By utilizing a source 3D mesh with semantic materials and a user-defined semantic image, 3Deformer can accurately modify the source mesh based on the guidance provided by the semantic image while maintaining the original mesh's structure as rigid as possible. Unlike existing approaches that rely on neural networks trained on costly 3D datasets limited to specific objects, 3Deformer is a non-training, universal framework that only needs supervision from easily accessible semantic images and can be applied to a wide range of objects without being constrained by datasets. Within 3Deformer, the deformation of the source mesh is achieved through a differentiable rendering technique that leverages the relationships between semantic images and mesh materials. However, guiding complex 3D shapes using a simple 2D image presents additional challenges such as ensuring deformation accuracy, surface smoothness, geometric rigidity, and overall consistency of the edited mesh. To tackle these challenges, we propose a hierarchical optimization framework that balances global and local shape features, along with various strategies and loss functions to enhance properties like accuracy, smoothness, and rigidity. Extensive experiments demonstrate that our 3Deformer framework can deliver impressive results and achieve a state-of-the-art performance level."}, "2105.03571": {"original_text": "  Dialogue state tracking (DST) plays a key role in task-oriented dialogue\nsystems to monitor the user's goal. In general, there are two strategies to\ntrack a dialogue state: predicting it from scratch and updating it from\nprevious state. The scratch-based strategy obtains each slot value by inquiring\nall the dialogue history, and the previous-based strategy relies on the current\nturn dialogue to update the previous dialogue state. However, it is hard for\nthe scratch-based strategy to correctly track short-dependency dialogue state\nbecause of noise; meanwhile, the previous-based strategy is not very useful for\nlong-dependency dialogue state tracking. Obviously, it plays different roles\nfor the context information of different granularity to track different kinds\nof dialogue states. Thus, in this paper, we will study and discuss how the\ncontext information of different granularity affects dialogue state tracking.\nFirst, we explore how greatly different granularities affect dialogue state\ntracking. Then, we further discuss how to combine multiple granularities for\ndialogue state tracking. Finally, we apply the findings about context\ngranularity to few-shot learning scenario. Besides, we have publicly released\nall codes.\n", "rewritten_text": "Dialogue state tracking (DST) is crucial in task-oriented dialogue systems for monitoring the user's goal. There are generally two approaches to tracking dialogue state: predicting it from scratch and updating it from the previous state. The scratch-based strategy involves retrieving each slot value by examining the entire dialogue history, while the previous-based strategy relies on the current turn dialogue to update the previous state. However, the scratch-based strategy may struggle to accurately track short-dependency dialogue states due to noise, whereas the previous-based strategy may not be as effective for tracking long-dependency dialogue states. Different roles are played by context information of varying granularity in tracking different types of dialogue states. This paper aims to investigate how context information of different granularity impacts dialogue state tracking. Initially, we explore the impact of different granularities on dialogue state tracking. Subsequently, we discuss how to integrate multiple granularities for improved dialogue state tracking. Finally, we apply the insights on context granularity to a few-shot learning scenario. Additionally, all codes have been made publicly available."}, "2405.10474": {"original_text": "  Over the last decade, a wide range of training and deployment strategies for\nLarge Language Models (LLMs) have emerged. Among these, the prompting paradigms\nof Auto-regressive LLMs (AR-LLMs) have catalyzed a significant surge in\nArtificial Intelligence (AI). This paper aims to emphasize the significance of\nutilizing free-form modalities (forms of input and output) and verbal free-form\ncontexts as user-directed channels (methods for transforming modalities) for\ndownstream deployment. Specifically, we analyze the structure of modalities\nwithin both two types of LLMs and six task-specific channels during deployment.\nFrom the perspective of users, our analysis introduces and applies the\nanalytical metrics of task customizability, transparency, and complexity to\ngauge their usability, highlighting the superior nature of AR-LLMs' prompting\nparadigms. Moreover, we examine the stimulation of diverse cognitive behaviors\nin LLMs through the adoption of free-form text and verbal contexts, mirroring\nhuman linguistic expressions of such behaviors. We then detail four common\ncognitive behaviors to underscore how AR-LLMs' prompting successfully imitate\nhuman-like behaviors using this free-form modality and channel. Lastly, the\npotential for improving LLM deployment, both as autonomous agents and within\nmulti-agent systems, is identified via cognitive behavior concepts and\nprinciples.\n", "rewritten_text": "In the past decade, various training and deployment strategies have emerged for Large Language Models (LLMs). The prompting paradigms of Auto-regressive LLMs (AR-LLMs) have played a key role in driving advancements in Artificial Intelligence (AI). This paper focuses on the importance of using free-form modalities and verbal contexts as user-directed channels for downstream deployment. It examines the modalities in two types of LLMs and six task-specific channels during deployment. The analysis introduces metrics such as task customizability, transparency, and complexity to assess usability from the user's perspective, highlighting the effectiveness of AR-LLMs' prompting paradigms. The study also explores how free-form text and verbal contexts can stimulate various cognitive behaviors in LLMs, resembling human linguistic expressions. It identifies four common cognitive behaviors that AR-LLMs successfully replicate using free-form modalities and channels. Lastly, the paper discusses the potential for enhancing LLM deployment, both as standalone agents and within multi-agent systems, by leveraging cognitive behavior concepts and principles."}, "2212.04214": {"original_text": "  In a citation graph, adjacent paper nodes share related scientific terms and\ntopics. The graph thus conveys unique structure information of document-level\nrelatedness that can be utilized in the paper summarization task, for exploring\nbeyond the intra-document information. In this work, we focus on leveraging\ncitation graphs to improve scientific paper extractive summarization under\ndifferent settings. We first propose a Multi-granularity Unsupervised\nSummarization model (MUS) as a simple and low-cost solution to the task. MUS\nfinetunes a pre-trained encoder model on the citation graph by link prediction\ntasks. Then, the abstract sentences are extracted from the corresponding paper\nconsidering multi-granularity information. Preliminary results demonstrate that\ncitation graph is helpful even in a simple unsupervised framework. Motivated by\nthis, we next propose a Graph-based Supervised Summarization model (GSS) to\nachieve more accurate results on the task when large-scale labeled data are\navailable. Apart from employing the link prediction as an auxiliary task, GSS\nintroduces a gated sentence encoder and a graph information fusion module to\ntake advantage of the graph information to polish the sentence representation.\nExperiments on a public benchmark dataset show that MUS and GSS bring\nsubstantial improvements over the prior state-of-the-art model.\n", "rewritten_text": "In a citation graph, neighboring paper nodes share related scientific terms and topics, conveying unique structural information about document-level relatedness. This information can be utilized in the task of summarizing papers, going beyond just the information within each document. This study focuses on using citation graphs to enhance extractive summarization of scientific papers in various scenarios. The first approach presented is a Multi-granularity Unsupervised Summarization model (MUS), which refines a pre-trained encoder model on the citation graph through link prediction tasks. Abstract sentences are then extracted from the respective papers, considering information at multiple levels of granularity. Initial findings suggest that even in a basic unsupervised framework, citation graphs can be beneficial. Building on this, a Graph-based Supervised Summarization model (GSS) is proposed to achieve more precise results when ample labeled data is available. In addition to using link prediction as an auxiliary task, GSS incorporates a gated sentence encoder and a graph information fusion module to leverage the graph information for enhancing sentence representation. Experiments conducted on a standard benchmark dataset demonstrate that both MUS and GSS outperform the previous state-of-the-art model significantly."}, "2310.14561": {"original_text": "  Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by\nwell-designed perturbations. This could lead to disastrous results on critical\napplications such as self-driving cars, surveillance security, and medical\ndiagnosis. At present, adversarial training is one of the most effective\ndefenses against adversarial examples. However, traditional adversarial\ntraining makes it difficult to achieve a good trade-off between clean accuracy\nand robustness since spurious features are still learned by DNNs. The intrinsic\nreason is that traditional adversarial training makes it difficult to fully\nlearn core features from adversarial examples when adversarial noise and clean\nexamples cannot be disentangled. In this paper, we disentangle the adversarial\nexamples into natural and perturbed patterns by bit-plane slicing. We assume\nthe higher bit-planes represent natural patterns and the lower bit-planes\nrepresent perturbed patterns, respectively. We propose a Feature-Focusing\nAdversarial Training (F$^2$AT), which differs from previous work in that it\nenforces the model to focus on the core features from natural patterns and\nreduce the impact of spurious features from perturbed patterns. The\nexperimental results demonstrated that F$^2$AT outperforms state-of-the-art\nmethods in clean accuracy and adversarial robustness.\n", "rewritten_text": "Deep neural networks (DNNs) are susceptible to adversarial examples created through carefully crafted perturbations. This vulnerability poses significant risks in critical applications like autonomous driving, surveillance systems, and medical diagnosis. While adversarial training is currently a key defense against such examples, traditional methods struggle to strike a balance between accuracy and robustness due to DNNs still learning irrelevant features. This challenge arises from the difficulty in distinguishing between adversarial noise and clean examples, hindering the full learning of essential features. In this study, we separate adversarial examples into natural and perturbed patterns using bit-plane slicing. We assign higher bit-planes to natural patterns and lower bit-planes to perturbed patterns. Introducing Feature-Focusing Adversarial Training (F$^2$AT), our approach differs by directing the model to prioritize core features from natural patterns while minimizing the influence of irrelevant features from perturbed patterns. Experimental results show that F$^2$AT surpasses existing methods in both clean accuracy and adversarial resilience."}, "2308.04782": {"original_text": "  Point cloud registration is a task to estimate the rigid transformation\nbetween two unaligned scans, which plays an important role in many computer\nvision applications. Previous learning-based works commonly focus on supervised\nregistration, which have limitations in practice. Recently, with the advance of\ninexpensive RGB-D sensors, several learning-based works utilize RGB-D data to\nachieve unsupervised registration. However, most of existing unsupervised\nmethods follow a cascaded design or fuse RGB-D data in a unidirectional manner,\nwhich do not fully exploit the complementary information in the RGB-D data. To\nleverage the complementary information more effectively, we propose a network\nimplementing multi-scale bidirectional fusion between RGB images and point\nclouds generated from depth images. By bidirectionally fusing visual and\ngeometric features in multi-scales, more distinctive deep features for\ncorrespondence estimation can be obtained, making our registration more\naccurate. Extensive experiments on ScanNet and 3DMatch demonstrate that our\nmethod achieves new state-of-the-art performance. Code will be released at\nhttps://github.com/phdymz/PointMBF\n", "rewritten_text": "Point cloud registration involves determining the rigid transformation between two scans that are not aligned, a crucial task in various computer vision applications. While previous learning-based approaches have mainly focused on supervised registration, they have practical limitations. Recently, with the availability of affordable RGB-D sensors, some learning-based methods have started using RGB-D data for unsupervised registration. However, many existing unsupervised techniques either follow a cascaded design or fuse RGB-D data in a one-way manner, failing to fully exploit the complementary information in the RGB-D data. To address this, we propose a network that incorporates multi-scale bidirectional fusion between RGB images and point clouds derived from depth images. By integrating visual and geometric features bidirectionally at multiple scales, our method generates more distinct deep features for correspondence estimation, leading to improved registration accuracy. Extensive experiments conducted on ScanNet and 3DMatch datasets demonstrate that our approach achieves state-of-the-art performance. The code will be made available at https://github.com/phdymz/PointMBF."}, "2404.02573": {"original_text": "  Knowledge distillation (KD) is a promising yet challenging model compression\ntechnique that transfers rich learning representations from a well-performing\nbut cumbersome teacher model to a compact student model. Previous methods for\nimage super-resolution (SR) mostly compare the feature maps directly or after\nstandardizing the dimensions with basic algebraic operations (e.g. average,\ndot-product). However, the intrinsic semantic differences among feature maps\nare overlooked, which are caused by the disparate expressive capacity between\nthe networks. This work presents MiPKD, a multi-granularity mixture of prior KD\nframework, to facilitate efficient SR model through the feature mixture in a\nunified latent space and stochastic network block mixture. Extensive\nexperiments demonstrate the effectiveness of the proposed MiPKD method.\n", "rewritten_text": "\"Knowledge distillation (KD) is a challenging yet promising technique for compressing models, where a compact student model learns rich representations from a well-performing but bulky teacher model. Previous methods for image super-resolution (SR) often compare feature maps directly or after standardizing dimensions using basic algebraic operations like averaging or dot products. However, these methods overlook the semantic differences among feature maps, which stem from the varying expressive capacities of the networks. This study introduces MiPKD, a multi-granularity mixture of prior KD framework, to enhance the efficiency of SR models by blending features in a unified latent space and stochastic network block mixture. Extensive experiments confirm the effectiveness of the MiPKD approach.\""}, "2307.08198": {"original_text": "  We introduce the notion of point affiliation into feature upsampling. By\nabstracting a feature map into non-overlapped semantic clusters formed by\npoints of identical semantic meaning, feature upsampling can be viewed as point\naffiliation -- designating a semantic cluster for each upsampled point. In the\nframework of kernel-based dynamic upsampling, we show that an upsampled point\ncan resort to its low-res decoder neighbors and high-res encoder point to\nreason the affiliation, conditioned on the mutual similarity between them. We\ntherefore present a generic formulation for generating similarity-aware\nupsampling kernels and prove that such kernels encourage not only semantic\nsmoothness but also boundary sharpness. This formulation constitutes a novel,\nlightweight, and universal upsampling solution, Similarity-Aware Point\nAffiliation (SAPA). We show its working mechanism via our preliminary designs\nwith window-shape kernel. After probing the limitations of the designs on\nobject detection, we reveal additional insights for upsampling, leading to SAPA\nwith the dynamic kernel shape. Extensive experiments demonstrate that SAPA\noutperforms prior upsamplers and invites consistent performance improvements on\na number of dense prediction tasks, including semantic segmentation, object\ndetection, instance segmentation, panoptic segmentation, image matting, and\ndepth estimation. Code is made available at: https://github.com/tiny-smart/sapa\n", "rewritten_text": "We propose a new concept called point affiliation in the context of feature upsampling. This involves organizing a feature map into distinct semantic clusters based on points with similar semantic meanings, allowing feature upsampling to be seen as assigning each upsampled point to a specific semantic cluster. In the context of kernel-based dynamic upsampling, we demonstrate that an upsampled point can determine its affiliation by considering its neighboring points in the low-resolution decoder and the corresponding point in the high-resolution encoder, taking into account their similarity. We present a general method for creating similarity-aware upsampling kernels, showing that these kernels promote both semantic consistency and sharp boundaries. This approach, known as Similarity-Aware Point Affiliation (SAPA), offers a novel, efficient, and versatile solution for upsampling. We illustrate how SAPA operates using initial designs with window-shaped kernels. By identifying the limitations of these designs in object detection, we propose an enhanced version of SAPA with dynamically shaped kernels. Extensive experiments demonstrate that SAPA surpasses existing upsampling methods and consistently enhances performance across various dense prediction tasks such as semantic segmentation, object detection, instance segmentation, panoptic segmentation, image matting, and depth estimation. The code for SAPA is accessible at: https://github.com/tiny-smart/sapa"}, "2410.16646": {"original_text": "  Diffusion models excel at creating visually impressive images but often\nstruggle to generate images with a specified topology. The Betti number, which\nrepresents the number of structures in an image, is a fundamental measure in\ntopology. Yet, diffusion models fail to satisfy even this basic constraint.\nThis limitation restricts their utility in applications requiring exact\ncontrol, like robotics and environmental modeling. To address this, we propose\nTopoDiffusionNet (TDN), a novel approach that enforces diffusion models to\nmaintain the desired topology. We leverage tools from topological data\nanalysis, particularly persistent homology, to extract the topological\nstructures within an image. We then design a topology-based objective function\nto guide the denoising process, preserving intended structures while\nsuppressing noisy ones. Our experiments across four datasets demonstrate\nsignificant improvements in topological accuracy. TDN is the first to integrate\ntopology with diffusion models, opening new avenues of research in this area.\n", "rewritten_text": "Diffusion models are known for their ability to produce visually stunning images, but they often struggle to generate images with specific topological features. The Betti number, which indicates the number of structures in an image, is a key measure in topology. However, diffusion models typically do not meet this basic requirement, limiting their usefulness in applications that demand precise control, such as robotics and environmental modeling. To overcome this limitation, we introduce TopoDiffusionNet (TDN), a new approach that enforces diffusion models to adhere to the desired topology. By utilizing tools from topological data analysis, particularly persistent homology, we identify the topological structures within an image. We then create an objective function based on topology to guide the denoising process, preserving intended structures while eliminating noise. Our experiments on four datasets show significant enhancements in topological accuracy. TDN is the first model to combine topology with diffusion models, paving the way for further research in this field."}, "1603.01684": {"original_text": "  In this paper, we propose an improved mechanism for saliency detection.\nFirstly,based on a neoteric background prior selecting four corners of an image\nas background,we use color and spatial contrast with each superpixel to obtain\na salinecy map(CBP). Inspired by reverse-measurement methods to improve the\naccuracy of measurement in Engineering,we employ the Objectness labels as\nforeground prior based on part of information of CBP to construct a\nmap(OFP).Further,an original energy function is applied to optimize both of\nthem respectively and a single-layer saliency map(SLP)is formed by merging the\nabove twos.Finally,to deal with the scale problem,we obtain our multi-layer\nmap(MLP) by presenting an integration algorithm to take advantage of multiple\nsaliency maps. Quantitative and qualitative experiments on three datasets\ndemonstrate that our method performs favorably against the state-of-the-art\nalgorithm.\n", "rewritten_text": "\"In this paper, we introduce an enhanced approach for detecting saliency. Initially, we utilize a modern background estimation technique to identify four corners of an image as the background. By incorporating color and spatial contrast with each superpixel, we generate a saliency map (CBP). Drawing inspiration from reverse-measurement methods in Engineering, we use Objectness labels as foreground information based on a portion of the CBP data to create another map (OFP). Subsequently, we apply an original energy function to optimize both maps individually and merge them to form a single-layer saliency map (SLP). To address scale issues, we develop a multi-layer map (MLP) by integrating multiple saliency maps using an algorithm. Our method outperforms the current state-of-the-art algorithm, as demonstrated through quantitative and qualitative experiments on three datasets.\""}, "2403.16998": {"original_text": "  Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly.\n", "rewritten_text": "\"Recent advancements in Large Language Models (LLMs) have led to impressive results in long-video comprehension tasks. Our study delves into how the broad knowledge base and robust reasoning abilities of LLMs contribute to this success. Surprisingly, we find that LLM-based methods can achieve high accuracy in long-video tasks even with limited or no video-specific information. To further enhance performance, we explore integrating video-specific details into an LLM-based framework. By extracting object-centric information from videos using standard vision tools and combining it with natural language, our Multimodal Video Understanding (MVU) framework achieves top-tier results on various video comprehension benchmarks. Its success extends to robotics tasks, showcasing its versatility. We plan to make our code publicly available.\""}, "2006.01372": {"original_text": "  In general, the labels used in sequence labeling consist of different types\nof elements. For example, IOB-format entity labels, such as B-Person and\nI-Person, can be decomposed into span (B and I) and type information (Person).\nHowever, while most sequence labeling models do not consider such label\ncomponents, the shared components across labels, such as Person, can be\nbeneficial for label prediction. In this work, we propose to integrate label\ncomponent information as embeddings into models. Through experiments on English\nand Japanese fine-grained named entity recognition, we demonstrate that the\nproposed method improves performance, especially for instances with\nlow-frequency labels.\n", "rewritten_text": "Generally, sequence labeling involves using various types of elements for labels. For instance, IOB-format entity labels like B-Person and I-Person can be broken down into span (B and I) and type (Person) components. While many sequence labeling models do not take into account these label components, incorporating shared components like Person can enhance label prediction. In this study, we suggest integrating label component information as embeddings into models. Our experiments on English and Japanese fine-grained named entity recognition show that this approach enhances performance, particularly for labels with low frequency."}, "2410.09140": {"original_text": "  The remarkable development of text-to-image generation models has raised\nnotable security concerns, such as the infringement of portrait rights and the\ngeneration of inappropriate content. Concept erasure has been proposed to\nremove the model's knowledge about protected and inappropriate concepts.\nAlthough many methods have tried to balance the efficacy (erasing target\nconcepts) and specificity (retaining irrelevant concepts), they can still\ngenerate abundant erasure concepts under the steering of semantically related\ninputs. In this work, we propose RealEra to address this \"concept residue\"\nissue. Specifically, we first introduce the mechanism of neighbor-concept\nmining, digging out the associated concepts by adding random perturbation into\nthe embedding of erasure concept, thus expanding the erasing range and\neliminating the generations even through associated concept inputs.\nFurthermore, to mitigate the negative impact on the generation of irrelevant\nconcepts caused by the expansion of erasure scope, RealEra preserves the\nspecificity through the beyond-concept regularization. This makes irrelevant\nconcepts maintain their corresponding spatial position, thereby preserving\ntheir normal generation performance. We also employ the closed-form solution to\noptimize weights of U-Net for the cross-attention alignment, as well as the\nprediction noise alignment with the LoRA module. Extensive experiments on\nmultiple benchmarks demonstrate that RealEra outperforms previous concept\nerasing methods in terms of superior erasing efficacy, specificity, and\ngenerality. More details are available on our project page\nhttps://realerasing.github.io/RealEra/ .\n", "rewritten_text": "The significant advancement of models that generate images from text has raised concerns regarding security, such as potential violations of portrait rights and the creation of inappropriate content. To address these issues, the concept of erasure has been introduced to eliminate the model's knowledge of protected and inappropriate concepts. While previous methods have attempted to balance effectiveness in erasing target concepts with specificity in retaining irrelevant concepts, they often still produce a large number of erasure concepts influenced by semantically related inputs. In this study, we present RealEra as a solution to the problem of \"concept residue.\" RealEra incorporates neighbor-concept mining to identify associated concepts by introducing random perturbations into the embedding of the erasure concept, thereby expanding the erasure range and preventing the generation of associated concepts. Additionally, RealEra maintains specificity by implementing beyond-concept regularization to preserve the spatial positions of irrelevant concepts and ensure their normal generation performance. We also utilize a closed-form solution to optimize the weights of U-Net for cross-attention alignment and incorporate prediction noise alignment with the LoRA module. Extensive experiments on various benchmarks demonstrate that RealEra surpasses previous concept erasing methods in terms of erasing effectiveness, specificity, and generality. For more information, please visit our project page at https://realerasing.github.io/RealEra/."}, "2210.02843": {"original_text": "  Focusing on the issue of how to effectively capture and utilize\ncross-modality information in RGB-D salient object detection (SOD) task, we\npresent a convolutional neural network (CNN) model, named CIR-Net, based on the\nnovel cross-modality interaction and refinement. For the cross-modality\ninteraction, 1) a progressive attention guided integration unit is proposed to\nsufficiently integrate RGB-D feature representations in the encoder stage, and\n2) a convergence aggregation structure is proposed, which flows the RGB and\ndepth decoding features into the corresponding RGB-D decoding streams via an\nimportance gated fusion unit in the decoder stage. For the cross-modality\nrefinement, we insert a refinement middleware structure between the encoder and\nthe decoder, in which the RGB, depth, and RGB-D encoder features are further\nrefined by successively using a self-modality attention refinement unit and a\ncross-modality weighting refinement unit. At last, with the gradually refined\nfeatures, we predict the saliency map in the decoder stage. Extensive\nexperiments on six popular RGB-D SOD benchmarks demonstrate that our network\noutperforms the state-of-the-art saliency detectors both qualitatively and\nquantitatively.\n", "rewritten_text": "In this study, we introduce a convolutional neural network model called CIR-Net, designed to enhance RGB-D salient object detection by effectively capturing and utilizing cross-modality information. Our approach includes a novel cross-modality interaction and refinement strategy. The model features a progressive attention guided integration unit to combine RGB-D feature representations in the encoder stage, as well as a convergence aggregation structure that merges RGB and depth decoding features using an importance gated fusion unit in the decoder stage. Additionally, a refinement middleware structure is inserted between the encoder and decoder, where RGB, depth, and RGB-D encoder features are further refined through self-modality attention and cross-modality weighting units. The refined features are then used to predict the saliency map in the decoder stage. Experimental results on six RGB-D salient object detection benchmarks demonstrate that our network surpasses existing saliency detectors in terms of both quality and performance."}, "1703.01515": {"original_text": "  Temporal action localization is an important yet challenging problem. Given a\nlong, untrimmed video consisting of multiple action instances and complex\nbackground contents, we need not only to recognize their action categories, but\nalso to localize the start time and end time of each instance. Many\nstate-of-the-art systems use segment-level classifiers to select and rank\nproposal segments of pre-determined boundaries. However, a desirable model\nshould move beyond segment-level and make dense predictions at a fine\ngranularity in time to determine precise temporal boundaries. To this end, we\ndesign a novel Convolutional-De-Convolutional (CDC) network that places CDC\nfilters on top of 3D ConvNets, which have been shown to be effective for\nabstracting action semantics but reduce the temporal length of the input data.\nThe proposed CDC filter performs the required temporal upsampling and spatial\ndownsampling operations simultaneously to predict actions at the frame-level\ngranularity. It is unique in jointly modeling action semantics in space-time\nand fine-grained temporal dynamics. We train the CDC network in an end-to-end\nmanner efficiently. Our model not only achieves superior performance in\ndetecting actions in every frame, but also significantly boosts the precision\nof localizing temporal boundaries. Finally, the CDC network demonstrates a very\nhigh efficiency with the ability to process 500 frames per second on a single\nGPU server. We will update the camera-ready version and publish the source\ncodes online soon.\n", "rewritten_text": "Temporal action localization is a crucial yet complex task involving identifying action categories and pinpointing the start and end times of each action instance within a long, untrimmed video. While many current systems rely on segment-level classifiers to select and rank proposal segments with predetermined boundaries, an ideal model should go beyond this approach to make precise temporal boundary predictions at a fine time granularity. In this study, we introduce a novel Convolutional-De-Convolutional (CDC) network that integrates CDC filters on top of 3D ConvNets to address this challenge. The CDC filter simultaneously performs temporal upsampling and spatial downsampling operations to predict actions at the frame-level granularity, effectively capturing action semantics in both space-time and fine-grained temporal dynamics. Training the CDC network end-to-end yields superior performance in action detection and significantly improves the accuracy of temporal boundary localization. Notably, the CDC network demonstrates high efficiency, processing 500 frames per second on a single GPU server. Stay tuned for updates on the finalized version and the release of the source codes."}, "2305.0541": {"original_text": "  The medical conversational question answering (CQA) system aims at providing\na series of professional medical services to improve the efficiency of medical\ncare. Despite the success of large language models (LLMs) in complex reasoning\ntasks in various fields, such as mathematics, logic, and commonsense QA, they\nstill need to improve with the increased complexity and specialization of the\nmedical field. This is because medical CQA tasks require not only strong\nmedical reasoning, but also the ability to think broadly and deeply. In this\npaper, to address these challenges in medical CQA tasks that need to be\nconsidered and understood in many aspects, we propose the Holistically Thought\n(HoT) method, which is designed to guide the LLMs to perform the diffused and\nfocused thinking for generating high-quality medical responses. The proposed\nHoT method has been evaluated through automated and manual assessments in three\ndifferent medical CQA datasets containing the English and Chinese languages.\nThe extensive experimental results show that our method can produce more\ncorrectness, professional, and considerate answers than several\nstate-of-the-art (SOTA) methods, manifesting its effectiveness. Our code in\nhttps://github.com/WENGSYX/HoT.\n", "rewritten_text": "The medical conversational question answering (CQA) system is developed to offer a range of professional medical services to enhance the efficiency of medical care. While large language models (LLMs) have shown success in complex reasoning tasks across various fields like mathematics, logic, and commonsense QA, they still require improvement to handle the increasing complexity and specialization of the medical domain. Medical CQA tasks demand not only strong medical reasoning but also the ability to think broadly and deeply. To tackle these challenges in medical CQA tasks, we introduce the Holistically Thought (HoT) method, which guides LLMs to engage in diffused and focused thinking to generate high-quality medical responses. The HoT method is evaluated through automated and manual assessments on three different medical CQA datasets in English and Chinese languages. Extensive experimental results demonstrate that our method outperforms several state-of-the-art (SOTA) methods in producing more accurate, professional, and thoughtful answers, showcasing its effectiveness. Access our code at https://github.com/WENGSYX/HoT."}, "2110.02204": {"original_text": "  Contextualised word embeddings generated from Neural Language Models (NLMs),\nsuch as BERT, represent a word with a vector that considers the semantics of\nthe target word as well its context. On the other hand, static word embeddings\nsuch as GloVe represent words by relatively low-dimensional, memory- and\ncompute-efficient vectors but are not sensitive to the different senses of the\nword. We propose Context Derived Embeddings of Senses (CDES), a method that\nextracts sense related information from contextualised embeddings and injects\nit into static embeddings to create sense-specific static embeddings.\nExperimental results on multiple benchmarks for word sense disambiguation and\nsense discrimination tasks show that CDES can accurately learn sense-specific\nstatic embeddings reporting comparable performance to the current\nstate-of-the-art sense embeddings.\n", "rewritten_text": "Contextualized word embeddings, like those produced by Neural Language Models such as BERT, capture both the semantics of a word and its context through vectors. In contrast, static word embeddings like GloVe are efficient in terms of memory and computation but do not account for different senses of a word. Our proposed method, Context Derived Embeddings of Senses (CDES), combines sense-related information from contextualized embeddings with static embeddings to create sense-specific static embeddings. Results from experiments on various word sense disambiguation and sense discrimination tasks demonstrate that CDES effectively learns sense-specific static embeddings, achieving performance comparable to current state-of-the-art sense embeddings."}, "1804.00247": {"original_text": "  This article describes our experiments in neural machine translation using\nthe recent Tensor2Tensor framework and the Transformer sequence-to-sequence\nmodel (Vaswani et al., 2017). We examine some of the critical parameters that\naffect the final translation quality, memory usage, training stability and\ntraining time, concluding each experiment with a set of recommendations for\nfellow researchers. In addition to confirming the general mantra \"more data and\nlarger models\", we address scaling to multiple GPUs and provide practical tips\nfor improved training regarding batch size, learning rate, warmup steps,\nmaximum sentence length and checkpoint averaging. We hope that our observations\nwill allow others to get better results given their particular hardware and\ndata constraints.\n", "rewritten_text": "This article discusses our neural machine translation experiments using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We explore key parameters impacting translation quality, memory usage, training stability, and time, concluding each experiment with recommendations for fellow researchers. Alongside emphasizing the importance of \"more data and larger models,\" we delve into scaling to multiple GPUs and offer practical training tips on batch size, learning rate, warmup steps, maximum sentence length, and checkpoint averaging. Our aim is to help others achieve improved results based on their specific hardware and data limitations."}, "2402.08874": {"original_text": "  While large language models (LLMs) excel at understanding and generating\nplain text, they are not tailored to handle hierarchical text structures or\ndirectly predict task-specific properties such as text rating. In fact,\nselectively and repeatedly grasping the hierarchical structure of large-scale\ntext is pivotal for deciphering its essence. To this end, we propose a novel\nframework for hierarchical text rating utilizing LLMs, which incorporates\nRecurrent Alignment with Hard Attention (RAHA). Particularly, hard attention\nmechanism prompts a frozen LLM to selectively focus on pertinent leaf texts\nassociated with the root text and generate symbolic representations of their\nrelationships. Inspired by the gradual stabilization of the Markov Chain,\nrecurrent alignment strategy involves feeding predicted ratings iteratively\nback into the prompts of another trainable LLM, aligning it to progressively\napproximate the desired target. Experimental results demonstrate that RAHA\noutperforms existing state-of-the-art methods on three hierarchical text rating\ndatasets. Theoretical and empirical analysis confirms RAHA's ability to\ngradually converge towards the underlying target through multiple inferences.\nAdditional experiments on plain text rating datasets verify the effectiveness\nof this Markov-like alignment. Our data and code can be available in\nhttps://github.com/ECNU-Text-Computing/Markov-LLM.\n", "rewritten_text": "\"While large language models (LLMs) are proficient in comprehending and generating plain text, they are not specifically designed to handle hierarchical text structures or directly predict task-specific properties like text rating. Understanding the hierarchical structure of large-scale text is crucial for interpreting its meaning. Therefore, we introduce a new framework for hierarchical text rating using LLMs, which incorporates Recurrent Alignment with Hard Attention (RAHA). The hard attention mechanism allows a fixed LLM to focus on relevant leaf texts connected to the root text and create symbolic representations of their relationships. The recurrent alignment strategy involves iteratively feeding predicted ratings back into the prompts of another trainable LLM, aligning it to progressively approach the desired target. Experimental results show that RAHA surpasses current state-of-the-art methods on three hierarchical text rating datasets. Theoretical and empirical analysis confirms RAHA's ability to gradually converge towards the target through multiple inferences. Further experiments on plain text rating datasets validate the effectiveness of this Markov-like alignment approach. Our data and code are accessible at https://github.com/ECNU-Text-Computing/Markov-LLM.\""}, "2410.10227": {"original_text": "  Few-Shot Learning (FSL) aims to recognize new classes with limited labeled\ndata. Recent studies have attempted to address the challenge of rare samples\nwith textual prompts to modulate visual features. However, they usually\nstruggle to capture complex semantic relationships between textual and visual\nfeatures. Moreover, vanilla self-attention is heavily affected by useless\ninformation in images, severely constraining the potential of semantic priors\nin FSL due to the confusion of numerous irrelevant tokens during interaction.\nTo address these aforementioned issues, a K-NN Transformer with Pyramid Prompts\n(KTPP) is proposed to select discriminative information with K-NN Context\nAttention (KCA) and adaptively modulate visual features with Pyramid\nCross-modal Prompts (PCP). First, for each token, the KCA only selects the K\nmost relevant tokens to compute the self-attention matrix and incorporates the\nmean of all tokens as the context prompt to provide the global context in three\ncascaded stages. As a result, irrelevant tokens can be progressively\nsuppressed. Secondly, pyramid prompts are introduced in the PCP to emphasize\nvisual features via interactions between text-based class-aware prompts and\nmulti-scale visual features. This allows the ViT to dynamically adjust the\nimportance weights of visual features based on rich semantic information at\ndifferent scales, making models robust to spatial variations. Finally,\naugmented visual features and class-aware prompts are interacted via the KCA to\nextract class-specific features. Consequently, our model further enhances\nnoise-free visual representations via deep cross-modal interactions, extracting\ngeneralized visual representation in scenarios with few labeled samples.\nExtensive experiments on four benchmark datasets demonstrate the effectiveness\nof our method.\n", "rewritten_text": "\"Few-Shot Learning (FSL) is the task of recognizing new classes with limited labeled data. Recent research has explored using textual prompts to modulate visual features in order to address the challenge of rare samples. However, existing methods often struggle to capture complex semantic relationships between textual and visual features. Additionally, traditional self-attention methods can be influenced by irrelevant information in images, limiting the potential of semantic priors in FSL due to confusion caused by irrelevant tokens. To tackle these issues, a novel approach called K-NN Transformer with Pyramid Prompts (KTPP) is proposed. This method leverages K-NN Context Attention (KCA) to select discriminative information and adaptively modulate visual features using Pyramid Cross-modal Prompts (PCP). The KCA selects the most relevant tokens for each token, progressively suppressing irrelevant tokens. The PCP introduces pyramid prompts to enhance visual features through interactions between text-based class-aware prompts and multi-scale visual features. This allows for dynamic adjustment of importance weights based on semantic information at different scales, making models robust to spatial variations. By interacting augmented visual features and class-aware prompts via the KCA, class-specific features are extracted, leading to noise-free visual representations and generalized visual representation in scenarios with few labeled samples. Extensive experiments on four benchmark datasets validate the effectiveness of the proposed method.\""}, "2005.00692": {"original_text": "  Cross-lingual Entity Linking (XEL), the problem of grounding mentions of\nentities in a foreign language text into an English knowledge base such as\nWikipedia, has seen a lot of research in recent years, with a range of\npromising techniques. However, current techniques do not rise to the challenges\nintroduced by text in low-resource languages (LRL) and, surprisingly, fail to\ngeneralize to text not taken from Wikipedia, on which they are usually trained.\n  This paper provides a thorough analysis of low-resource XEL techniques,\nfocusing on the key step of identifying candidate English Wikipedia titles that\ncorrespond to a given foreign language mention. Our analysis indicates that\ncurrent methods are limited by their reliance on Wikipedia's interlanguage\nlinks and thus suffer when the foreign language's Wikipedia is small. We\nconclude that the LRL setting requires the use of outside-Wikipedia\ncross-lingual resources and present a simple yet effective zero-shot XEL\nsystem, QuEL, that utilizes search engines query logs. With experiments on 25\nlanguages, QuEL~shows an average increase of 25\\% in gold candidate recall and\nof 13\\% in end-to-end linking accuracy over state-of-the-art baselines.\n", "rewritten_text": "The issue of Cross-lingual Entity Linking (XEL) involves connecting references to entities in a foreign language text to an English knowledge base like Wikipedia. Recent years have seen significant research in this area, exploring various promising techniques. However, existing methods struggle with texts in low-resource languages (LRL) and have difficulty generalizing to non-Wikipedia sources, despite being primarily trained on such data.\nThis study conducts a detailed examination of low-resource XEL methods, focusing on the crucial task of identifying potential English Wikipedia titles for a given foreign language reference. The analysis reveals that current approaches are hindered by their dependence on Wikipedia's interlanguage links, particularly when dealing with small Wikipedia editions in foreign languages. The study suggests that addressing the challenges of LRL scenarios necessitates leveraging cross-lingual resources beyond Wikipedia. As a solution, the paper introduces QuEL, a straightforward yet efficient zero-shot XEL system that utilizes search engine query logs. Experimental results across 25 languages demonstrate that QuEL outperforms state-of-the-art techniques, achieving an average increase of 25% in gold candidate recall and 13% in end-to-end linking accuracy."}, "2103.12462": {"original_text": "  Person ReID methods always learn through a stationary domain that is fixed by\nthe choice of a given dataset. In many contexts (e.g., lifelong learning),\nthose methods are ineffective because the domain is continually changing in\nwhich case incremental learning over multiple domains is required potentially.\nIn this work we explore a new and challenging ReID task, namely lifelong person\nre-identification (LReID), which enables to learn continuously across multiple\ndomains and even generalise on new and unseen domains. Following the cognitive\nprocesses in the human brain, we design an Adaptive Knowledge Accumulation\n(AKA) framework that is endowed with two crucial abilities: knowledge\nrepresentation and knowledge operation. Our method alleviates catastrophic\nforgetting on seen domains and demonstrates the ability to generalize to unseen\ndomains. Correspondingly, we also provide a new and large-scale benchmark for\nLReID. Extensive experiments demonstrate our method outperforms other\ncompetitors by a margin of 5.8% mAP in generalising evaluation.\n", "rewritten_text": "Person ReID methods typically learn within a fixed domain determined by the dataset used. However, in scenarios such as lifelong learning, where the domain is constantly evolving, these methods may prove ineffective. In such cases, incremental learning across multiple domains becomes necessary. This study introduces a novel and challenging ReID task called lifelong person re-identification (LReID), which allows for continuous learning across various domains and the ability to generalize to new and unseen domains. Inspired by human cognitive processes, we propose an Adaptive Knowledge Accumulation (AKA) framework with key capabilities in knowledge representation and operation. Our approach mitigates catastrophic forgetting in familiar domains and showcases the capacity to generalize to unfamiliar domains. Additionally, we present a new large-scale benchmark for LReID. Through extensive experiments, our method demonstrates superior performance compared to other competitors, achieving a 5.8% higher mean average precision (mAP) in generalization evaluation."}, "1705.07426": {"original_text": "  While the research community appears to have developed a consensus on the\nmethods of acquiring annotated data, design and training of CNNs, many\nquestions still remain to be answered. In this paper, we explore the following\nquestions that are critical to face recognition research: (i) Can we train on\nstill images and expect the systems to work on videos? (ii) Are deeper datasets\nbetter than wider datasets? (iii) Does adding label noise lead to improvement\nin performance of deep networks? (iv) Is alignment needed for face recognition?\nWe address these questions by training CNNs using CASIA-WebFace, UMDFaces, and\na new video dataset and testing on YouTube- Faces, IJB-A and a disjoint portion\nof UMDFaces datasets. Our new data set, which will be made publicly available,\nhas 22,075 videos and 3,735,476 human annotated frames extracted from them.\n", "rewritten_text": "\"While there is a general agreement within the research community regarding the methods for obtaining annotated data and training Convolutional Neural Networks (CNNs), there are still numerous unanswered questions. This paper delves into key inquiries crucial to face recognition research, including: (i) Can training on static images result in effective performance on videos? (ii) Are deeper datasets more effective than wider datasets? (iii) Does introducing label noise enhance the performance of deep networks? (iv) Is alignment necessary for successful face recognition? To address these questions, we train CNNs using CASIA-WebFace, UMDFaces, and a new video dataset, and evaluate their performance on YouTube-Faces, IJB-A, and a separate section of UMDFaces datasets. Our new dataset, comprising 22,075 videos and 3,735,476 human annotated frames, will be publicly accessible.\""}, "2010.06363": {"original_text": "  Lip motion reflects behavior characteristics of speakers, and thus can be\nused as a new kind of biometrics in speaker recognition. In the literature,\nlots of works used two-dimensional (2D) lip images to recognize speaker in a\ntextdependent context. However, 2D lip easily suffers from various face\norientations. To this end, in this work, we present a novel end-to-end 3D lip\nmotion Network (3LMNet) by utilizing the sentence-level 3D lip motion (S3DLM)\nto recognize speakers in both the text-independent and text-dependent contexts.\nA new regional feedback module (RFM) is proposed to obtain attentions in\ndifferent lip regions. Besides, prior knowledge of lip motion is investigated\nto complement RFM, where landmark-level and frame-level features are merged to\nform a better feature representation. Moreover, we present two methods, i.e.,\ncoordinate transformation and face posture correction to pre-process the LSD-AV\ndataset, which contains 68 speakers and 146 sentences per speaker. The\nevaluation results on this dataset demonstrate that our proposed 3LMNet is\nsuperior to the baseline models, i.e., LSTM, VGG-16 and ResNet-34, and\noutperforms the state-of-the-art using 2D lip image as well as the 3D face. The\ncode of this work is released at\nhttps://github.com/wutong18/Three-Dimensional-Lip-\nMotion-Network-for-Text-Independent-Speaker-Recognition.\n", "rewritten_text": "The movement of lips can reveal unique characteristics of speakers, making it a potential biometric for speaker recognition. Previous studies have focused on using two-dimensional (2D) lip images for speaker recognition in specific speech contexts. However, 2D lip images are limited by variations in face orientation. In this study, we introduce a novel end-to-end 3D lip motion Network (3LMNet) that leverages sentence-level 3D lip motion (S3DLM) for speaker recognition in various speech contexts. A new regional feedback module (RFM) is proposed to capture attention in different lip regions, and prior knowledge of lip motion is incorporated to enhance feature representation. Additionally, we introduce methods for dataset pre-processing, including coordinate transformation and face posture correction, on the LSD-AV dataset. Evaluation results on this dataset show that our 3LMNet outperforms baseline models such as LSTM, VGG-16, and ResNet-34, as well as state-of-the-art approaches using 2D lip images or 3D face data. The code for this work is available at the following link: https://github.com/wutong18/Three-Dimensional-Lip-Motion-Network-for-Text-Independent-Speaker-Recognition."}, "2210.09345": {"original_text": "  Relation Extraction (RE) has attracted increasing attention, but current RE\nevaluation is limited to in-domain evaluation setups. Little is known on how\nwell a RE system fares in challenging, but realistic out-of-distribution\nevaluation setups. To address this gap, we propose CrossRE, a new,\nfreely-available cross-domain benchmark for RE, which comprises six distinct\ntext domains and includes multi-label annotations. An additional innovation is\nthat we release meta-data collected during annotation, to include explanations\nand flags of difficult instances. We provide an empirical evaluation with a\nstate-of-the-art model for relation classification. As the meta-data enables us\nto shed new light on the state-of-the-art model, we provide a comprehensive\nanalysis on the impact of difficult cases and find correlations between model\nand human annotations. Overall, our empirical investigation highlights the\ndifficulty of cross-domain RE. We release our dataset, to spur more research in\nthis direction.\n", "rewritten_text": "\"Relation Extraction (RE) has been receiving increasing attention, but current evaluation methods are limited to assessing performance within specific domains. There is a lack of understanding regarding how well RE systems perform in challenging out-of-domain scenarios that reflect real-world conditions. To bridge this gap, we introduce CrossRE, a novel cross-domain benchmark for RE that includes six diverse text domains and multi-label annotations. A key feature of CrossRE is the release of meta-data gathered during annotation, providing insights and flags for challenging instances. We conduct an empirical evaluation using a cutting-edge model for relation classification, leveraging the meta-data to offer new perspectives on the model's performance. Our analysis reveals the impact of difficult cases and uncovers correlations between model predictions and human annotations. This study underscores the challenges of cross-domain RE and aims to stimulate further research in this area by making our dataset publicly available.\""}, "1006.2734": {"original_text": "  A difficult problem in clustering is how to handle data with a manifold\nstructure, i.e. data that is not shaped in the form of compact clouds of\npoints, forming arbitrary shapes or paths embedded in a high-dimensional space.\nIn this work we introduce the Penalized k-Nearest-Neighbor-Graph (PKNNG) based\nmetric, a new tool for evaluating distances in such cases. The new metric can\nbe used in combination with most clustering algorithms. The PKNNG metric is\nbased on a two-step procedure: first it constructs the k-Nearest-Neighbor-Graph\nof the dataset of interest using a low k-value and then it adds edges with an\nexponentially penalized weight for connecting the sub-graphs produced by the\nfirst step. We discuss several possible schemes for connecting the different\nsub-graphs. We use three artificial datasets in four different embedding\nsituations to evaluate the behavior of the new metric, including a comparison\namong different clustering methods. We also evaluate the new metric in a real\nworld application, clustering the MNIST digits dataset. In all cases the PKNNG\nmetric shows promising clustering results.\n", "rewritten_text": "In clustering, a challenging issue is how to handle data that has a manifold structure, meaning data that does not conform to compact clusters but instead forms various shapes or paths within a high-dimensional space. This study introduces the Penalized k-Nearest-Neighbor-Graph (PKNNG) based metric as a novel tool for assessing distances in such scenarios. The PKNNG metric can be utilized alongside most clustering algorithms. It operates through a two-step process: initially constructing the k-Nearest-Neighbor-Graph with a low k-value, followed by adding edges with exponentially penalized weights to connect the sub-graphs generated in the first step. Various approaches for linking the sub-graphs are discussed. The effectiveness of the new metric is evaluated using three synthetic datasets in four different embedding scenarios, comparing its performance across different clustering methods. Additionally, the PKNNG metric is tested in a practical application by clustering the MNIST digits dataset, demonstrating promising results in all cases."}, "1804.01422": {"original_text": "  In this paper, we propose a simple but effective semantic-based aggregation\n(SBA) method. The proposed SBA utilizes the discriminative filters of deep\nconvolutional layers as semantic detectors. Moreover, we propose the effective\nunsupervised strategy to select some semantic detectors to generate the\n\"probabilistic proposals\", which highlight certain discriminative pattern of\nobjects and suppress the noise of background. The final global SBA\nrepresentation could then be acquired by aggregating the regional\nrepresentations weighted by the selected \"probabilistic proposals\"\ncorresponding to various semantic content. Our unsupervised SBA is easy to\ngeneralize and achieves excellent performance on various tasks. We conduct\ncomprehensive experiments and show that our unsupervised SBA outperforms the\nstate-of-the-art unsupervised and supervised aggregation methods on image\nretrieval, place recognition and cloud classification.\n", "rewritten_text": "In this paper, we introduce a straightforward yet powerful semantic-based aggregation method known as SBA. The method leverages the discriminative filters found in deep convolutional layers as semantic detectors. Additionally, we present an effective unsupervised approach for selecting specific semantic detectors to create \"probabilistic proposals.\" These proposals emphasize distinctive object patterns while reducing background noise. The overall global SBA representation is then obtained by combining regional representations weighted by the chosen \"probabilistic proposals\" associated with different semantic content. Our unsupervised SBA is easily adaptable and demonstrates outstanding performance across various tasks. Through extensive experiments, we demonstrate that our unsupervised SBA surpasses both unsupervised and supervised aggregation methods in image retrieval, place recognition, and cloud classification."}, "2109.00881": {"original_text": "  There is a growing interest in product aesthetics analytics and design.\nHowever, the lack of available large-scale data that covers various variables\nand information is one of the biggest challenges faced by analysts and\nresearchers. In this paper, we present our multidisciplinary initiative of\ndeveloping a comprehensive automotive dataset from different online sources and\nformats. Specifically, the created dataset contains 1.4 million images from 899\ncar models and their corresponding model specifications and sales information\nover more than ten years in the UK market. Our work makes significant\ncontributions to: (i) research and applications in the automotive industry;\n(ii) big data creation and sharing; (iii) database design; and (iv) data\nfusion. Apart from our motivation, technical details and data structure, we\nfurther present three simple examples to demonstrate how our data can be used\nin business research and applications.\n", "rewritten_text": "The interest in analyzing and designing product aesthetics is increasing. However, a major challenge for analysts and researchers is the limited availability of large-scale data covering various variables. This paper introduces a multidisciplinary effort to create a comprehensive automotive dataset sourced from different online platforms. The dataset includes 1.4 million images of 899 car models, along with their specifications and sales data spanning over a decade in the UK market. Our work contributes significantly to research and applications in the automotive industry, big data creation and sharing, database design, and data fusion. In addition to discussing our motivation, technical aspects, and data structure, we provide three examples to illustrate how this data can be utilized in business research and applications."}, "1611.09007": {"original_text": "  Hyperspectral imaging sensors are becoming increasingly popular in robotics\napplications such as agriculture and mining, and allow per-pixel thematic\nclassification of materials in a scene based on their unique spectral\nsignatures. Recently, convolutional neural networks have shown remarkable\nperformance for classification tasks, but require substantial amounts of\nlabelled training data. This data must sufficiently cover the variability\nexpected to be encountered in the environment. For hyperspectral data, one of\nthe main variations encountered outdoors is due to incident illumination, which\ncan change in spectral shape and intensity depending on the scene geometry. For\nexample, regions occluded from the sun have a lower intensity and their\nincident irradiance skewed towards shorter wavelengths.\n  In this work, a data augmentation strategy based on relighting is used during\ntraining of a hyperspectral convolutional neural network. It allows training to\noccur in the outdoor environment given only a small labelled region, which does\nnot need to sufficiently represent the geometric variability of the entire\nscene. This is important for applications where obtaining large amounts of\ntraining data is labourious, hazardous or difficult, such as labelling pixels\nwithin shadows. Radiometric normalisation approaches for pre-processing the\nhyperspectral data are analysed and it is shown that methods based on the raw\npixel data are sufficient to be used as input for the classifier. This removes\nthe need for external hardware such as calibration boards, which can restrict\nthe application of hyperspectral sensors in robotics applications. Experiments\nto evaluate the classification system are carried out on two datasets captured\nfrom a field-based platform.\n", "rewritten_text": "\"Hyperspectral imaging sensors are gaining popularity in robotics applications like agriculture and mining. They enable detailed material classification in a scene based on unique spectral signatures per pixel. Convolutional neural networks have shown impressive performance for classification tasks, but require ample labeled training data covering expected environmental variability. In outdoor settings, incident illumination variations due to scene geometry, such as changes in spectral shape and intensity, are common. To address this, a data augmentation strategy using relighting is employed in training a hyperspectral convolutional neural network. This approach allows training in outdoor environments with minimal labeled data, reducing the need to represent full scene variability. This is particularly beneficial for applications where collecting large amounts of training data is challenging, such as labeling shadowed pixels. The study also examines radiometric normalization methods for preprocessing hyperspectral data, demonstrating that raw pixel data-based approaches are sufficient for classifier input, eliminating the need for external hardware like calibration boards. Experiments evaluating the classification system are conducted on two datasets captured from a field-based platform.\""}, "1905.00996": {"original_text": "  In this work, we propose a novel framework named Region-Aware Network\n(RANet), which learns the ability of anti-confusing in case of heavy occlusion,\nnearby person and symmetric appearance, for human pose estimation.\nSpecifically, the proposed method addresses three key aspects, i.e., data\naugmentation, feature learning and prediction fusion, respectively. First, we\npropose Parsing-based Data Augmentation (PDA) to generate abundant data that\nsynthesizes confusing textures. Second, we not only propose a Feature Pyramid\nStem (FPS) to learn stronger low-level features in lower stage; but also\nincorporate an Effective Region Extraction (ERE) module to excavate better\ntarget-specific features. Third, we introduce Cascade Voting Fusion (CVF) to\nexplicitly exclude the inferior predictions and fuse the rest effective\npredictions for the final pose estimation. Extensive experimental results on\ntwo popular benchmarks, i.e. MPII and LSP, demonstrate the effectiveness of our\nmethod against the state-of-the-art competitors. Especially on\neasily-confusable joints, our method makes significant improvement.\n", "rewritten_text": "In this study, we introduce a new framework called Region-Aware Network (RANet) that focuses on improving human pose estimation by addressing challenges such as heavy occlusion, nearby individuals, and similar appearances. Our approach tackles three main aspects: data augmentation, feature learning, and prediction fusion. Firstly, we propose Parsing-based Data Augmentation (PDA) to create diverse data with complex textures. Secondly, we introduce the Feature Pyramid Stem (FPS) to enhance low-level features and the Effective Region Extraction (ERE) module to extract target-specific features. Lastly, we present Cascade Voting Fusion (CVF) to combine accurate predictions and exclude inferior ones for the final pose estimation. Our method outperforms existing techniques on popular benchmarks like MPII and LSP, particularly showing significant enhancements in easily-confused joints."}, "2304.00025": {"original_text": "  After the pandemic, artificial intelligence (AI) powered support for mental\nhealth care has become increasingly important. The breadth and complexity of\nsignificant challenges required to provide adequate care involve: (a)\nPersonalized patient understanding, (b) Safety-constrained and medically\nvalidated chatbot patient interactions, and (c) Support for continued\nfeedback-based refinements in design using chatbot-patient interactions. We\npropose Alleviate, a chatbot designed to assist patients suffering from mental\nhealth challenges with personalized care and assist clinicians with\nunderstanding their patients better. Alleviate draws from an array of publicly\navailable clinically valid mental-health texts and databases, allowing\nAlleviate to make medically sound and informed decisions. In addition,\nAlleviate's modular design and explainable decision-making lends itself to\nrobust and continued feedback-based refinements to its design. In this paper,\nwe explain the different modules of Alleviate and submit a short video\ndemonstrating Alleviate's capabilities to help patients and clinicians\nunderstand each other better to facilitate optimal care strategies.\n", "rewritten_text": "Following the pandemic, the importance of utilizing artificial intelligence (AI) in mental health care support has grown significantly. Addressing the various challenges involved in providing effective care includes: (a) Tailoring patient understanding, (b) Ensuring safe and medically validated interactions with chatbot patients, and (c) Supporting ongoing improvements in design through feedback obtained from chatbot-patient interactions. Our solution, Alleviate, is a chatbot specifically designed to aid individuals facing mental health issues by offering personalized care and assisting healthcare providers in gaining deeper insights into their patients. Alleviate leverages a range of publicly accessible, clinically validated mental health resources to ensure its decisions are medically sound and well-informed. Furthermore, Alleviate's adaptable structure and transparent decision-making process allow for continuous enhancements based on feedback. This paper outlines the various components of Alleviate and includes a brief video showcasing how Alleviate can enhance communication between patients and clinicians to facilitate the development of optimal care strategies."}, "1204.2294": {"original_text": "  This paper present our new intensity chromaticity space-based feature\ndetection and matching algorithm. This approach utilizes hybridization of\nwireless local area network and camera internal sensor which to receive signal\nstrength from a access point and the same time retrieve interest point\ninformation from hallways. This information is combined by model fitting\napproach in order to find the absolute of user target position. No conventional\nsearching algorithm is required, thus it is expected reducing the computational\ncomplexity. Finally we present pre-experimental results to illustrate the\nperformance of the localization system for an indoor environment set-up.\n", "rewritten_text": "This paper introduces a novel algorithm for detecting and matching features based on intensity chromaticity space. The method combines a wireless local area network and camera internal sensor to capture signal strength from an access point and extract interest point information from hallways simultaneously. By employing a model fitting approach, the information is integrated to determine the precise position of the user target without the need for traditional search algorithms, thereby reducing computational complexity. Additionally, preliminary experimental results are presented to demonstrate the effectiveness of the localization system in an indoor environment."}, "2410.09595": {"original_text": "  Deformable image registration remains a fundamental task in clinical\npractice, yet solving registration problems involving complex deformations\nremains challenging. Current deep learning-based registration methods employ\ncontinuous deformation to model large deformations, which often suffer from\naccumulated registration errors and interpolation inaccuracies. Moreover,\nachieving satisfactory results with these frameworks typically requires a large\nnumber of cascade stages, demanding substantial computational resources.\nTherefore, we propose a novel approach, the field refinement framework\n(FiRework), tailored for unsupervised deformable registration, aiming to\naddress these challenges. In FiRework, we redesign the continuous deformation\nframework to mitigate the aforementioned errors. Notably, our FiRework requires\nonly one level of recursion during training and supports continuous inference,\noffering improved efficacy compared to continuous deformation frameworks. We\nconducted experiments on two brain MRI datasets, enhancing two existing\ndeformable registration networks with FiRework. The experimental results\ndemonstrate the superior performance of our proposed framework in deformable\nregistration. The code is publicly available at\nhttps://github.com/ZAX130/FiRework.\n", "rewritten_text": "Deformable image registration is a crucial task in clinical settings, but addressing registration challenges involving complex deformations remains difficult. Current deep learning-based registration methods use continuous deformation to handle large deformations, but they often encounter issues such as accumulated errors and interpolation inaccuracies. These methods also typically require multiple cascade stages and significant computational resources to achieve satisfactory results. To tackle these challenges, we introduce a new approach called the field refinement framework (FiRework) for unsupervised deformable registration. In FiRework, we redesign the continuous deformation framework to reduce errors, requiring only one level of recursion during training and supporting continuous inference. Our experiments on two brain MRI datasets show that enhancing existing deformable registration networks with FiRework leads to superior performance. The code for FiRework is available at https://github.com/ZAX130/FiRework."}, "2401.07770": {"original_text": "  Computer vision tasks typically involve describing what is present in an\nimage (e.g. classification, detection, segmentation, and captioning). We study\na visual common sense task that requires understanding what is not present.\nSpecifically, given an image (e.g. of a living room) and name of an object\n(\"cushion\"), a vision system is asked to predict semantically-meaningful\nregions (masks or bounding boxes) in the image where that object could be\nplaced or is likely be placed by humans (e.g. on the sofa). We call this task:\nSemantic Placement (SP) and believe that such common-sense visual understanding\nis critical for assitive robots (tidying a house), and AR devices\n(automatically rendering an object in the user's space). Studying the invisible\nis hard. Datasets for image description are typically constructed by curating\nrelevant images and asking humans to annotate the contents of the image;\nneither of those two steps are straightforward for objects not present in the\nimage. We overcome this challenge by operating in the opposite direction: we\nstart with an image of an object in context from web, and then remove that\nobject from the image via inpainting. This automated pipeline converts\nunstructured web data into a dataset comprising pairs of images with/without\nthe object. Using this, we collect a novel dataset, with ${\\sim}1.3$M images\nacross $9$ object categories, and train a SP prediction model called CLIP-UNet.\nCLIP-UNet outperforms existing VLMs and baselines that combine semantic priors\nwith object detectors on real-world and simulated images. In our user studies,\nwe find that the SP masks predicted by CLIP-UNet are favored $43.7\\%$ and\n$31.3\\%$ times when comparing against the $4$ SP baselines on real and\nsimulated images. In addition, we demonstrate leveraging SP mask predictions\nfrom CLIP-UNet enables downstream applications like building tidying robots in\nindoor environments.\n", "rewritten_text": "Computer vision tasks typically involve identifying objects in an image through tasks such as classification, detection, segmentation, and captioning. In this study, we focus on a unique visual common sense task that requires understanding what is absent in an image. Specifically, given an image, such as a living room, and the name of an object, like a \"cushion,\" a vision system is tasked with predicting regions in the image where that object could be placed or is likely to be placed by humans, such as on a sofa. This task, known as Semantic Placement (SP), is crucial for assistive robots and augmented reality devices. Creating datasets for image description usually involves curating relevant images and having humans annotate the contents, which becomes challenging when dealing with objects not present in the image. To address this, we take a different approach by starting with an image containing the object in context, then removing the object through inpainting. This method allows us to generate a dataset of image pairs with and without the object, resulting in a novel dataset with approximately 1.3 million images across nine object categories. We train a SP prediction model called CLIP-UNet using this dataset, which outperforms existing models on both real-world and simulated images. User studies show that the SP masks predicted by CLIP-UNet are preferred over other baselines in both real and simulated scenarios. Furthermore, leveraging the SP mask predictions from CLIP-UNet enables applications like developing tidying robots for indoor environments."}, "2112.11454": {"original_text": "  Generating digital humans that move realistically has many applications and\nis widely studied, but existing methods focus on the major limbs of the body,\nignoring the hands and head. Hands have been separately studied, but the focus\nhas been on generating realistic static grasps of objects. To synthesize\nvirtual characters that interact with the world, we need to generate full-body\nmotions and realistic hand grasps simultaneously. Both sub-problems are\nchallenging on their own and, together, the state-space of poses is\nsignificantly larger, the scales of hand and body motions differ, and the\nwhole-body posture and the hand grasp must agree, satisfy physical constraints,\nand be plausible. Additionally, the head is involved because the avatar must\nlook at the object to interact with it. For the first time, we address the\nproblem of generating full-body, hand and head motions of an avatar grasping an\nunknown object. As input, our method, called GOAL, takes a 3D object, its\nposition, and a starting 3D body pose and shape. GOAL outputs a sequence of\nwhole-body poses using two novel networks. First, GNet generates a goal\nwhole-body grasp with a realistic body, head, arm, and hand pose, as well as\nhand-object contact. Second, MNet generates the motion between the starting and\ngoal pose. This is challenging, as it requires the avatar to walk towards the\nobject with foot-ground contact, orient the head towards it, reach out, and\ngrasp it with a realistic hand pose and hand-object contact. To achieve this,\nthe networks exploit a representation that combines SMPL-X body parameters and\n3D vertex offsets. We train and evaluate GOAL, both qualitatively and\nquantitatively, on the GRAB dataset. Results show that GOAL generalizes well to\nunseen objects, outperforming baselines. GOAL takes a step towards synthesizing\nrealistic full-body object grasping.\n", "rewritten_text": "Creating lifelike digital human movements has numerous practical uses and is extensively researched. However, current methods typically focus on animating the main body parts while neglecting the hands and head. While hands have been studied independently, the emphasis has been on creating realistic static grips on objects. To generate virtual characters that can interact realistically with their environment, it is essential to simultaneously animate full-body motions and lifelike hand grips. Both of these tasks present unique challenges, especially when considering the vast range of possible poses, the differing scales of hand and body movements, and the need for coordination between body posture and hand grip while adhering to physical constraints. Moreover, the head plays a crucial role as the avatar must visually engage with objects during interactions. In a groundbreaking approach, we tackle the complex task of animating a full-body avatar, including hand and head movements, grasping an unfamiliar object. Our method, named GOAL, takes as input a 3D object, its location, and an initial 3D body pose and shape, and produces a sequence of realistic whole-body poses using two innovative networks. The first network, GNet, generates a comprehensive grasp pose incorporating realistic body, head, arm, and hand positions, as well as hand-object contact. The second network, MNet, generates the motion between the initial and goal poses, requiring the avatar to walk towards the object with proper foot-ground contact, orient the head towards the object, reach out, and grasp it with a realistic hand pose and hand-object contact. To accomplish this, the networks leverage a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and assess GOAL on the GRAB dataset both qualitatively and quantitatively, demonstrating its ability to generalize to unseen objects and outperform baseline methods. GOAL represents a significant advancement in the synthesis of realistic full-body object grasping animations."}, "2103.05861": {"original_text": "  Neural network pruning is an essential approach for reducing the\ncomputational complexity of deep models so that they can be well deployed on\nresource-limited devices. Compared with conventional methods, the recently\ndeveloped dynamic pruning methods determine redundant filters variant to each\ninput instance which achieves higher acceleration. Most of the existing methods\ndiscover effective sub-networks for each instance independently and do not\nutilize the relationship between different inputs. To maximally excavate\nredundancy in the given network architecture, this paper proposes a new\nparadigm that dynamically removes redundant filters by embedding the manifold\ninformation of all instances into the space of pruned networks (dubbed as\nManiDP). We first investigate the recognition complexity and feature similarity\nbetween images in the training set. Then, the manifold relationship between\ninstances and the pruned sub-networks will be aligned in the training\nprocedure. The effectiveness of the proposed method is verified on several\nbenchmarks, which shows better performance in terms of both accuracy and\ncomputational cost compared to the state-of-the-art methods. For example, our\nmethod can reduce 55.3% FLOPs of ResNet-34 with only 0.57% top-1 accuracy\ndegradation on ImageNet.\n", "rewritten_text": "Neural network pruning is a crucial technique for reducing the computational complexity of deep models, making them suitable for deployment on devices with limited resources. Unlike traditional methods, dynamic pruning methods identify redundant filters specific to each input instance, resulting in greater acceleration. While most current methods identify effective sub-networks for individual instances without considering relationships between different inputs, this paper introduces a new approach called ManiDP. This method dynamically removes redundant filters by incorporating manifold information from all instances into pruned networks. By analyzing recognition complexity and feature similarity among images in the training set, the manifold relationship between instances and pruned sub-networks is aligned during training. Experimental results on various benchmarks demonstrate that the proposed method outperforms state-of-the-art techniques in terms of accuracy and computational cost. For instance, our method achieves a 55.3% reduction in FLOPs for ResNet-34 with only a 0.57% decrease in top-1 accuracy on ImageNet."}, "2002.01127": {"original_text": "  How to generate descriptions from structured data organized in tables?\nExisting approaches using neural encoder-decoder models often suffer from\nlacking diversity. We claim that an open set of templates is crucial for\nenriching the phrase constructions and realizing varied generations. Learning\nsuch templates is prohibitive since it often requires a large paired <table,\ndescription> corpus, which is seldom available. This paper explores the problem\nof automatically learning reusable \"templates\" from paired and non-paired data.\nWe propose the variational template machine (VTM), a novel method to generate\ntext descriptions from data tables. Our contributions include: a) we carefully\ndevise a specific model architecture and losses to explicitly disentangle text\ntemplate and semantic content information, in the latent spaces, and b)we\nutilize both small parallel data and large raw text without aligned tables to\nenrich the template learning. Experiments on datasets from a variety of\ndifferent domains show that VTM is able to generate more diversely while\nkeeping a good fluency and quality.\n", "rewritten_text": "\"How can descriptions be generated from structured data presented in tables? Current methods using neural encoder-decoder models often lack diversity. We argue that having a diverse set of templates is essential for enhancing the variety of generated phrases. However, learning such templates can be challenging due to the need for a large dataset of paired <table, description> examples, which is often not readily available. This study addresses the issue of automatically learning reusable templates from both paired and unpaired data. We introduce the variational template machine (VTM), a new approach for generating text descriptions from data tables. Our key contributions are: a) designing a specific model architecture and loss functions to separate text templates and semantic content information in the latent spaces, and b) leveraging both limited paired data and extensive unaligned text to improve template learning. Experiments conducted on datasets from various domains demonstrate that VTM can produce more diverse descriptions while maintaining high fluency and quality.\""}, "2010.03725": {"original_text": "  We present a new benchmark dataset called PARADE for paraphrase\nidentification that requires specialized domain knowledge. PARADE contains\nparaphrases that overlap very little at the lexical and syntactic level but are\nsemantically equivalent based on computer science domain knowledge, as well as\nnon-paraphrases that overlap greatly at the lexical and syntactic level but are\nnot semantically equivalent based on this domain knowledge. Experiments show\nthat both state-of-the-art neural models and non-expert human annotators have\npoor performance on PARADE. For example, BERT after fine-tuning achieves an F1\nscore of 0.709, which is much lower than its performance on other paraphrase\nidentification datasets. PARADE can serve as a resource for researchers\ninterested in testing models that incorporate domain knowledge. We make our\ndata and code freely available.\n", "rewritten_text": "We introduce a new benchmark dataset named PARADE for identifying paraphrases, which necessitates specialized domain expertise. PARADE comprises paraphrases that exhibit minimal overlap at the lexical and syntactic levels but are semantically equivalent within the field of computer science, as well as non-paraphrases that display significant overlap at the lexical and syntactic levels but lack semantic equivalence within this domain. Experimental results demonstrate that both cutting-edge neural models and non-expert human annotators struggle with PARADE. For instance, even BERT, following fine-tuning, achieves an F1 score of 0.709, significantly lower than its performance on other paraphrase identification datasets. PARADE can be utilized as a valuable resource for researchers looking to evaluate models that integrate domain knowledge. Our data and code are openly accessible for use."}, "2307.12067": {"original_text": "  We introduce Replay, a collection of multi-view, multi-modal videos of humans\ninteracting socially. Each scene is filmed in high production quality, from\ndifferent viewpoints with several static cameras, as well as wearable action\ncameras, and recorded with a large array of microphones at different positions\nin the room. Overall, the dataset contains over 4000 minutes of footage and\nover 7 million timestamped high-resolution frames annotated with camera poses\nand partially with foreground masks. The Replay dataset has many potential\napplications, such as novel-view synthesis, 3D reconstruction, novel-view\nacoustic synthesis, human body and face analysis, and training generative\nmodels. We provide a benchmark for training and evaluating novel-view\nsynthesis, with two scenarios of different difficulty. Finally, we evaluate\nseveral baseline state-of-the-art methods on the new benchmark.\n", "rewritten_text": "We present Replay, a compilation of multi-view, multi-modal videos showcasing human social interactions. Each scene is captured in high-quality production, utilizing various static cameras, wearable action cameras, and an extensive array of microphones placed at different locations within the room. The dataset comprises over 4000 minutes of video footage and more than 7 million timestamped high-resolution frames, annotated with camera poses and partially with foreground masks. The Replay dataset offers diverse applications, including novel-view synthesis, 3D reconstruction, novel-view acoustic synthesis, human body and face analysis, and generative model training. We offer a benchmark for training and assessing novel-view synthesis, featuring two difficulty scenarios. Additionally, we assess multiple cutting-edge methods against this new benchmark."}, "1703.10645": {"original_text": "  We propose a novel method called the Relevance Subject Machine (RSM) to solve\nthe person re-identification (re-id) problem. RSM falls under the category of\nBayesian sparse recovery algorithms and uses the sparse representation of the\ninput video under a pre-defined dictionary to identify the subject in the\nvideo. Our approach focuses on the multi-shot re-id problem, which is the\nprevalent problem in many video analytics applications. RSM captures the\nessence of the multi-shot re-id problem by constraining the support of the\nsparse codes for each input video frame to be the same. Our proposed approach\nis also robust enough to deal with time varying outliers and occlusions by\nintroducing a sparse, non-stationary noise term in the model error. We provide\na novel Variational Bayesian based inference procedure along with an intuitive\ninterpretation of the proposed update rules. We evaluate our approach over\nseveral commonly used re-id datasets and show superior performance over current\nstate-of-the-art algorithms. Specifically, for ILIDS-VID, a recent large scale\nre-id dataset, RSM shows significant improvement over all published approaches,\nachieving an 11.5% (absolute) improvement in rank 1 accuracy over the closest\ncompeting algorithm considered.\n", "rewritten_text": "We introduce a new method known as the Relevance Subject Machine (RSM) for addressing the person re-identification (re-id) challenge. RSM is classified as a Bayesian sparse recovery algorithm that leverages the sparse representation of the input video using a predefined dictionary to recognize the subject in the video. Our method targets the multi-shot re-id issue, which is common in various video analysis applications. RSM effectively tackles the essence of the multi-shot re-id problem by enforcing consistency in the sparse codes' support for each frame of the input video. Additionally, our approach is resilient to time-varying outliers and occlusions by incorporating a sparse, non-stationary noise term in the model error. We present a novel Variational Bayesian-based inference process and offer a straightforward explanation of the proposed update rules. Through evaluations on multiple widely used re-id datasets, we demonstrate superior performance compared to existing state-of-the-art algorithms. Notably, on the ILIDS-VID dataset, a recent extensive re-id dataset, RSM exhibits a substantial improvement over all previously published methods, achieving an 11.5% increase in rank 1 accuracy compared to the nearest competing algorithm."}, "1908.08498": {"original_text": "  We focus on multi-modal fusion for egocentric action recognition, and propose\na novel architecture for multi-modal temporal-binding, i.e. the combination of\nmodalities within a range of temporal offsets. We train the architecture with\nthree modalities -- RGB, Flow and Audio -- and combine them with mid-level\nfusion alongside sparse temporal sampling of fused representations. In contrast\nwith previous works, modalities are fused before temporal aggregation, with\nshared modality and fusion weights over time. Our proposed architecture is\ntrained end-to-end, outperforming individual modalities as well as late-fusion\nof modalities.\n  We demonstrate the importance of audio in egocentric vision, on per-class\nbasis, for identifying actions as well as interacting objects. Our method\nachieves state of the art results on both the seen and unseen test sets of the\nlargest egocentric dataset: EPIC-Kitchens, on all metrics using the public\nleaderboard.\n", "rewritten_text": "Our research focuses on integrating multiple sources of information for recognizing actions in a first-person perspective. We introduce a new approach for combining different modalities at various time intervals, training the model with RGB, Flow, and Audio data. Our method involves merging modalities before aggregating temporal information, using shared weights for fusion over time. By training the architecture end-to-end, we surpass the performance of individual modalities and late fusion techniques. We highlight the significance of audio in first-person vision for action recognition and object interaction. Our approach achieves state-of-the-art results on both known and unknown test sets of the EPIC-Kitchens dataset, leading in all evaluation metrics on the public leaderboard."}, "1611.09813": {"original_text": "  We propose a CNN-based approach for 3D human body pose estimation from single\nRGB images that addresses the issue of limited generalizability of models\ntrained solely on the starkly limited publicly available 3D pose data. Using\nonly the existing 3D pose data and 2D pose data, we show state-of-the-art\nperformance on established benchmarks through transfer of learned features,\nwhile also generalizing to in-the-wild scenes. We further introduce a new\ntraining set for human body pose estimation from monocular images of real\nhumans that has the ground truth captured with a multi-camera marker-less\nmotion capture system. It complements existing corpora with greater diversity\nin pose, human appearance, clothing, occlusion, and viewpoints, and enables an\nincreased scope of augmentation. We also contribute a new benchmark that covers\noutdoor and indoor scenes, and demonstrate that our 3D pose dataset shows\nbetter in-the-wild performance than existing annotated data, which is further\nimproved in conjunction with transfer learning from 2D pose data. All in all,\nwe argue that the use of transfer learning of representations in tandem with\nalgorithmic and data contributions is crucial for general 3D body pose\nestimation.\n", "rewritten_text": "We present a CNN-based method for estimating 3D human body poses from single RGB images. Our approach tackles the challenge of limited model generalizability when trained only on the scarce publicly available 3D pose data. By leveraging both existing 3D and 2D pose data, we achieve top-notch performance on standard benchmarks by transferring learned features, while also adapting well to real-world scenarios. Additionally, we introduce a new training dataset for estimating human body poses from monocular images, capturing ground truth with a multi-camera marker-less motion capture system. This dataset enhances existing collections by offering greater diversity in pose, appearance, clothing, occlusion, and viewpoints, allowing for more extensive augmentation. We also establish a new benchmark encompassing outdoor and indoor scenes, demonstrating superior performance in real-world settings compared to existing annotated data, especially when combined with transfer learning from 2D pose data. In summary, we emphasize the importance of utilizing transfer learning alongside algorithmic and data enhancements for accurate 3D body pose estimation."}, "2109.12338": {"original_text": "  Model binarization is an effective method of compressing neural networks and\naccelerating their inference process. However, a significant performance gap\nstill exists between the 1-bit model and the 32-bit one. The empirical study\nshows that binarization causes a great loss of information in the forward and\nbackward propagation. We present a novel Distribution-sensitive Information\nRetention Network (DIR-Net) that retains the information in the forward and\nbackward propagation by improving internal propagation and introducing external\nrepresentations. The DIR-Net mainly relies on three technical contributions:\n(1) Information Maximized Binarization (IMB): minimizing the information loss\nand the binarization error of weights/activations simultaneously by weight\nbalance and standardization; (2) Distribution-sensitive Two-stage Estimator\n(DTE): retaining the information of gradients by distribution-sensitive soft\napproximation by jointly considering the updating capability and accurate\ngradient; (3) Representation-align Binarization-aware Distillation (RBD):\nretaining the representation information by distilling the representations\nbetween full-precision and binarized networks. The DIR-Net investigates both\nforward and backward processes of BNNs from the unified information\nperspective, thereby providing new insight into the mechanism of network\nbinarization. The three techniques in our DIR-Net are versatile and effective\nand can be applied in various structures to improve BNNs. Comprehensive\nexperiments on the image classification and objective detection tasks show that\nour DIR-Net consistently outperforms the state-of-the-art binarization\napproaches under mainstream and compact architectures, such as ResNet, VGG,\nEfficientNet, DARTS, and MobileNet. Additionally, we conduct our DIR-Net on\nreal-world resource-limited devices which achieves 11.1x storage saving and\n5.4x speedup.\n", "rewritten_text": "Model binarization is a useful technique for compressing neural networks and speeding up their inference process. However, there is still a significant performance gap between 1-bit and 32-bit models. Research indicates that binarization leads to a substantial loss of information during both forward and backward propagation. To address this issue, we introduce the Distribution-sensitive Information Retention Network (DIR-Net), which focuses on preserving information in both propagation directions through enhanced internal propagation and external representations. The DIR-Net is built on three key technical advancements: Information Maximized Binarization (IMB) for minimizing information loss and binarization errors, Distribution-sensitive Two-stage Estimator (DTE) for retaining gradient information, and Representation-align Binarization-aware Distillation (RBD) for maintaining representation information. By examining the forward and backward processes of Binary Neural Networks (BNNs) holistically, the DIR-Net offers new insights into network binarization mechanisms. These techniques are versatile and effective, enhancing BNN performance across various architectures like ResNet, VGG, EfficientNet, DARTS, and MobileNet. Experimental results on image classification and object detection tasks demonstrate that DIR-Net consistently outperforms existing binarization methods, achieving significant storage savings and speed improvements on resource-limited devices."}, "2310.07343": {"original_text": "  Although large language models (LLMs) are impressive in solving various\ntasks, they can quickly be outdated after deployment. Maintaining their\nup-to-date status is a pressing concern in the current era. This paper provides\na comprehensive review of recent advances in aligning LLMs with the\never-changing world knowledge without re-training from scratch. We categorize\nresearch works systemically and provide in-depth comparisons and discussion. We\nalso discuss existing challenges and highlight future directions to facilitate\nresearch in this field. We release the paper list at\nhttps://github.com/hyintell/awesome-refreshing-llms\n", "rewritten_text": "\"While large language models (LLMs) demonstrate impressive capabilities in tackling various tasks, they face the challenge of quickly becoming outdated post-deployment. Ensuring their relevance in the rapidly evolving landscape is a critical issue today. This paper offers a thorough examination of recent advancements in keeping LLMs aligned with current knowledge without requiring complete re-training. We systematically categorize research efforts, offer detailed comparisons and discussions, address existing obstacles, and outline future research directions in this area. For a list of referenced papers, please visit https://github.com/hyintell/awesome-refreshing-llms.\""}, "2301.05792": {"original_text": "  Class-Incremental Learning (CIL) [40] trains classifiers under a strict\nmemory budget: in each incremental phase, learning is done for new data, most\nof which is abandoned to free space for the next phase. The preserved data are\nexemplars used for replaying. However, existing methods use a static and ad hoc\nstrategy for memory allocation, which is often sub-optimal. In this work, we\npropose a dynamic memory management strategy that is optimized for the\nincremental phases and different object classes. We call our method reinforced\nmemory management (RMM), leveraging reinforcement learning. RMM training is not\nnaturally compatible with CIL as the past, and future data are strictly\nnon-accessible during the incremental phases. We solve this by training the\npolicy function of RMM on pseudo CIL tasks, e.g., the tasks built on the data\nof the 0-th phase, and then applying it to target tasks. RMM propagates two\nlevels of actions: Level-1 determines how to split the memory between old and\nnew classes, and Level-2 allocates memory for each specific class. In essence,\nit is an optimizable and general method for memory management that can be used\nin any replaying-based CIL method. For evaluation, we plug RMM into two\ntop-performing baselines (LUCIR+AANets and POD+AANets [30]) and conduct\nexperiments on three benchmarks (CIFAR-100, ImageNet-Subset, and\nImageNet-Full). Our results show clear improvements, e.g., boosting POD+AANets\nby 3.6%, 4.4%, and 1.9% in the 25-Phase settings of the above benchmarks,\nrespectively.\n", "rewritten_text": "Class-Incremental Learning (CIL) trains classifiers within a limited memory capacity by conducting learning for new data in each incremental phase, discarding most of it to make room for the next phase. The retained data are exemplars used for replaying. However, current methods employ a static and arbitrary approach to memory allocation, which is often suboptimal. This study introduces a dynamic memory management strategy optimized for incremental phases and different object classes, known as reinforced memory management (RMM), utilizing reinforcement learning. RMM training is not inherently compatible with CIL due to the strict inaccessibility of past and future data during incremental phases. This challenge is addressed by training the policy function of RMM on pseudo CIL tasks, such as those constructed using data from the initial phase, and then applying it to target tasks. RMM involves two levels of actions: Level-1 determines the memory allocation between old and new classes, while Level-2 allocates memory for each specific class. Essentially, it is an adaptable and versatile method for memory management applicable to any replaying-based CIL approach. To evaluate its performance, RMM is integrated into two leading baselines (LUCIR+AANets and POD+AANets) and tested on three benchmarks (CIFAR-100, ImageNet-Subset, and ImageNet-Full). The results demonstrate significant enhancements, including a 3.6%, 4.4%, and 1.9% improvement in the 25-Phase settings of the aforementioned benchmarks for POD+AANets."}, "1711.01362": {"original_text": "  An Unreliable news is any piece of information which is false or misleading,\ndeliberately spread to promote political, ideological and financial agendas.\nRecently the problem of unreliable news has got a lot of attention as the\nnumber instances of using news and social media outlets for propaganda have\nincreased rapidly. This poses a serious threat to society, which calls for\ntechnology to automatically and reliably identify unreliable news sources. This\npaper is an effort made in this direction to build systems for detecting\nunreliable news articles. In this paper, various NLP algorithms were built and\nevaluated on Unreliable News Data 2017 dataset. Variants of hierarchical\nattention networks (HAN) are presented for encoding and classifying news\narticles which achieve the best results of 0.944 ROC-AUC. Finally, Attention\nlayer weights are visualized to understand and give insight into the decisions\nmade by HANs. The results obtained are very promising and encouraging to deploy\nand use these systems in the real world to mitigate the problem of unreliable\nnews.\n", "rewritten_text": "Unreliable news refers to false or misleading information intentionally spread to serve political, ideological, or financial interests. The rise in propaganda through news and social media platforms has drawn significant attention to this issue. Detecting and combating unreliable news sources is crucial to safeguard society. This study focuses on developing systems to identify unreliable news articles using NLP algorithms. The research evaluates various hierarchical attention networks (HAN) on the Unreliable News Data 2017 dataset, achieving a high ROC-AUC score of 0.944. Visualizing the attention layer weights provides insights into the decision-making process of HANs. The promising results suggest the potential for deploying these systems to address the challenge of unreliable news effectively."}, "1803.10464": {"original_text": "  The deficiency of segmentation labels is one of the main obstacles to\nsemantic segmentation in the wild. To alleviate this issue, we present a novel\nframework that generates segmentation labels of images given their image-level\nclass labels. In this weakly supervised setting, trained models have been known\nto segment local discriminative parts rather than the entire object area. Our\nsolution is to propagate such local responses to nearby areas which belong to\nthe same semantic entity. To this end, we propose a Deep Neural Network (DNN)\ncalled AffinityNet that predicts semantic affinity between a pair of adjacent\nimage coordinates. The semantic propagation is then realized by random walk\nwith the affinities predicted by AffinityNet. More importantly, the supervision\nemployed to train AffinityNet is given by the initial discriminative part\nsegmentation, which is incomplete as a segmentation annotation but sufficient\nfor learning semantic affinities within small image areas. Thus the entire\nframework relies only on image-level class labels and does not require any\nextra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with\nsegmentation labels generated by our method outperforms previous models trained\nwith the same level of supervision, and is even as competitive as those relying\non stronger supervision.\n", "rewritten_text": "The lack of segmentation labels poses a significant challenge for semantic segmentation in real-world scenarios. To address this issue, we introduce a new approach that produces segmentation labels for images based on their class labels at the image level. In a weakly supervised setup, models trained in this manner tend to focus on segmenting specific parts of objects rather than the entire object area. Our solution involves extending these local segmentations to neighboring regions that belong to the same semantic category. We introduce a Deep Neural Network (DNN) named AffinityNet, which predicts the semantic affinity between adjacent image coordinates. The propagation of semantic information is achieved through a random walk using the predicted affinities from AffinityNet. Importantly, AffinityNet is trained using supervision from initial partial segmentations of discriminative parts, which may be incomplete but are adequate for learning semantic relationships within small image regions. This framework relies solely on image-level class labels and does not necessitate additional data or annotations. On the PASCAL VOC 2012 dataset, a DNN trained with segmentation labels generated by our method surpasses previous models trained under similar levels of supervision and performs comparably to models trained with stronger supervision."}, "1902.07938": {"original_text": "  Named entity recognition (NER) is an important task in NLP, which is all the\nmore challenging in conversational domain with their noisy facets. Moreover,\nconversational texts are often available in limited amount, making supervised\ntasks infeasible. To learn from small data, strong inductive biases are\nrequired. Previous work relied on hand-crafted features to encode these biases\nuntil transfer learning emerges. Here, we explore a transfer learning method,\nnamely language model pretraining, on NER task in Indonesian conversational\ntexts. We utilize large unlabeled data (generic domain) to be transferred to\nconversational texts, enabling supervised training on limited in-domain data.\nWe report two transfer learning variants, namely supervised model fine-tuning\nand unsupervised pretrained LM fine-tuning. Our experiments show that both\nvariants outperform baseline neural models when trained on small data (100\nsentences), yielding an absolute improvement of 32 points of test F1 score.\nFurthermore, we find that the pretrained LM encodes part-of-speech information\nwhich is a strong predictor for NER.\n", "rewritten_text": "Named entity recognition (NER) is a crucial task in natural language processing (NLP), especially challenging in conversational settings due to their noisy nature. Additionally, conversational texts are often limited in quantity, making traditional supervised approaches impractical. To address this issue, strong inductive biases are necessary to learn effectively from small datasets. Previous methods relied on hand-crafted features to incorporate these biases until the introduction of transfer learning. In this study, we investigate the use of transfer learning, specifically language model pretraining, for NER in Indonesian conversational texts. By leveraging large amounts of unlabeled data from a generic domain, we transfer knowledge to conversational texts, enabling supervised training on a limited in-domain dataset. We present two transfer learning approaches: supervised model fine-tuning and unsupervised pretrained language model fine-tuning. Our experiments demonstrate that both approaches outperform baseline neural models when trained on a small dataset of 100 sentences, achieving a significant 32-point increase in test F1 score. Additionally, we observe that the pretrained language model captures part-of-speech information, which serves as a strong predictor for NER."}, "1504.0234": {"original_text": "  In this paper, we focus on the two key aspects of multiple target tracking\nproblem: 1) designing an accurate affinity measure to associate detections and\n2) implementing an efficient and accurate (near) online multiple target\ntracking algorithm. As the first contribution, we introduce a novel Aggregated\nLocal Flow Descriptor (ALFD) that encodes the relative motion pattern between a\npair of temporally distant detections using long term interest point\ntrajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust\naffinity measure for estimating the likelihood of matching detections\nregardless of the application scenarios. As another contribution, we present a\nNear-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is\nformulated as a data-association between targets and detections in a temporal\nwindow, that is performed repeatedly at every frame. While being efficient,\nNOMT achieves robustness via integrating multiple cues including ALFD metric,\ntarget dynamics, appearance similarity, and long term trajectory regularization\ninto the model. Our ablative analysis verifies the superiority of the ALFD\nmetric over the other conventional affinity metrics. We run a comprehensive\nexperimental evaluation on two challenging tracking datasets, KITTI and MOT\ndatasets. The NOMT method combined with ALFD metric achieves the best accuracy\nin both datasets with significant margins (about 10% higher MOTA) over the\nstate-of-the-arts.\n", "rewritten_text": "This paper focuses on two main aspects of the multiple target tracking problem: 1) developing a precise affinity measure for linking detections, and 2) creating an efficient and accurate (near) online multiple target tracking algorithm. The first contribution introduces a new Aggregated Local Flow Descriptor (ALFD) that captures the relative motion pattern between temporally distant detections using long term interest point trajectories (IPTs). The ALFD offers a robust affinity measure for assessing the likelihood of matching detections across various scenarios. The second contribution presents a Near-Online Multi-target Tracking (NOMT) algorithm, which formulates the tracking problem as a continuous data-association process between targets and detections within a temporal window at each frame. NOMT integrates multiple cues, including the ALFD metric, target dynamics, appearance similarity, and long term trajectory regularization, to achieve efficiency and robustness. Ablative analysis confirms the superiority of the ALFD metric compared to traditional affinity metrics. Experimental evaluations on the KITTI and MOT datasets demonstrate that the NOMT method, combined with the ALFD metric, outperforms state-of-the-art methods with significantly higher accuracy (approximately 10% higher MOTA) on both datasets."}, "2405.07399": {"original_text": "  Weeds present a significant challenge in agriculture, causing yield loss and\nrequiring expensive control measures. Automatic weed detection using computer\nvision and deep learning offers a promising solution. However, conventional\ndeep learning methods often require large amounts of labelled training data,\nwhich can be costly and time-consuming to acquire. This paper introduces a\nnovel method for semi-supervised weed detection, comprising two main\ncomponents. Firstly, a multi-scale feature representation technique is employed\nto capture distinctive weed features across different scales. Secondly, we\npropose an adaptive pseudo-label assignment strategy, leveraging a small set of\nlabelled images during training. This strategy dynamically assigns confidence\nscores to pseudo-labels generated from unlabeled data. Additionally, our\napproach integrates epoch-corresponding and mixed pseudo-labels to further\nenhance the learning process. Experimental results on the COCO dataset and five\nprominent weed datasets -- CottonWeedDet12, CropAndWeed, Palmer amaranth,\nRadishWheat, and RoboWeedMap -- illustrate that our method achieves\nstate-of-the-art performance in weed detection, even with significantly less\nlabelled data compared to existing techniques. This approach holds the\npotential to alleviate the labelling burden and enhance the feasibility and\ndeployment speed of deep learning for weed detection in real-world agricultural\nscenarios.\n", "rewritten_text": "Weeds pose a significant challenge in agriculture by causing yield loss and necessitating costly control measures. Utilizing computer vision and deep learning for automatic weed detection shows promise as a solution. However, traditional deep learning methods often demand large amounts of labeled training data, which can be expensive and time-consuming to obtain. This study introduces a new approach for semi-supervised weed detection, consisting of two main components. Firstly, a multi-scale feature representation technique is utilized to capture distinct weed characteristics across various scales. Secondly, an adaptive pseudo-label assignment strategy is proposed, utilizing a small set of labeled images during training. This strategy dynamically assigns confidence scores to pseudo-labels generated from unlabeled data. Furthermore, the method integrates epoch-corresponding and mixed pseudo-labels to enhance the learning process. Experimental results on the COCO dataset and five prominent weed datasets -- CottonWeedDet12, CropAndWeed, Palmer amaranth, RadishWheat, and RoboWeedMap -- demonstrate that the approach achieves state-of-the-art performance in weed detection, even with significantly less labeled data compared to existing techniques. This method has the potential to reduce the labeling workload and improve the practicality and deployment speed of deep learning for weed detection in real-world agricultural settings."}, "2204.12785": {"original_text": "  Language models (LMs) have shown great potential as implicit knowledge bases\n(KBs). And for their practical use, knowledge in LMs need to be updated\nperiodically. However, existing tasks to assess LMs' efficacy as KBs do not\nadequately consider multiple large-scale updates. To this end, we first propose\na novel task--Continuously-updated QA (CuQA)--in which multiple large-scale\nupdates are made to LMs, and the performance is measured with respect to the\nsuccess in adding and updating knowledge while retaining existing knowledge. We\nthen present LMs with plug-in modules that effectively handle the updates.\nExperiments conducted on zsRE QA and NQ datasets show that our method\noutperforms existing approaches. We find that our method is 4x more effective\nin terms of updates/forgets ratio, compared to a fine-tuning baseline.\n", "rewritten_text": "Language models (LMs) have demonstrated significant potential as implicit knowledge bases (KBs). In order to be practically useful, the knowledge within LMs must be regularly updated. However, current methods for evaluating the effectiveness of LMs as KBs do not adequately account for multiple large-scale updates. To address this, we introduce a new task called Continuously-updated QA (CuQA), where LMs undergo multiple large-scale updates and their performance is assessed based on their ability to incorporate new knowledge while retaining existing knowledge. We also introduce plug-in modules to help LMs effectively manage these updates. Our experiments on zsRE QA and NQ datasets show that our approach outperforms existing methods, with our method being 4 times more effective in terms of updates/forgets ratio compared to a fine-tuning baseline."}, "2406.08215": {"original_text": "  Extractive summarization is a task of highlighting the most important parts\nof the text. We introduce a new approach to extractive summarization task using\nhidden clustering structure of the text. Experimental results on CNN/DailyMail\ndemonstrate that our approach generates more accurate summaries than both\nextractive and abstractive methods, achieving state-of-the-art results in terms\nof ROUGE-2 metric exceeding the previous approaches by 10%. Additionally, we\nshow that hidden structure of the text could be interpreted as aspects.\n", "rewritten_text": "\"Extractive summarization involves identifying the key elements of a text. Our novel approach to this task leverages the hidden clustering structure within the text. Results from experiments on CNN/DailyMail show that our method produces more precise summaries compared to both extractive and abstractive techniques, outperforming previous approaches by 10% in terms of the ROUGE-2 metric. Furthermore, we demonstrate that the hidden text structure can be interpreted as aspects.\""}, "1205.3183": {"original_text": "  Existing probabilistic scanners and parsers impose hard constraints on the\nway lexical and syntactic ambiguities can be resolved. Furthermore, traditional\ngrammar-based parsing tools are limited in the mechanisms they allow for taking\ncontext into account. In this paper, we propose a model-driven tool that allows\nfor statistical language models with arbitrary probability estimators. Our work\non model-driven probabilistic parsing is built on top of ModelCC, a model-based\nparser generator, and enables the probabilistic interpretation and resolution\nof anaphoric, cataphoric, and recursive references in the disambiguation of\nabstract syntax graphs. In order to prove the expression power of ModelCC, we\ndescribe the design of a general-purpose natural language parser.\n", "rewritten_text": "The current probabilistic scanners and parsers have strict limitations on how lexical and syntactic ambiguities are resolved. Additionally, traditional grammar-based parsing tools have constraints on incorporating context. This paper introduces a model-driven tool that supports statistical language models with various probability estimators. Our approach to model-driven probabilistic parsing is based on ModelCC, a model-based parser generator, which facilitates the probabilistic interpretation and resolution of anaphoric, cataphoric, and recursive references in disambiguating abstract syntax graphs. To demonstrate the capabilities of ModelCC, we outline the design of a versatile natural language parser."}, "1607.06408": {"original_text": "  Action recognition has received increasing attention from the computer vision\nand machine learning communities in the last decade. To enable the study of\nthis problem, there exist a vast number of action datasets, which are recorded\nunder controlled laboratory settings, real-world surveillance environments, or\ncrawled from the Internet. Apart from the \"in-the-wild\" datasets, the training\nand test split of conventional datasets often possess similar environments\nconditions, which leads to close to perfect performance on constrained\ndatasets. In this paper, we introduce a new dataset, namely Multi-Camera Action\nDataset (MCAD), which is designed to evaluate the open view classification\nproblem under the surveillance environment. In total, MCAD contains 14,298\naction samples from 18 action categories, which are performed by 20 subjects\nand independently recorded with 5 cameras. Inspired by the well received\nevaluation approach on the LFW dataset, we designed a standard evaluation\nprotocol and benchmarked MCAD under several scenarios. The benchmark shows that\nwhile an average of 85% accuracy is achieved under the closed-view scenario,\nthe performance suffers from a significant drop under the cross-view scenario.\nIn the worst case scenario, the performance of 10-fold cross validation drops\nfrom 87.0% to 47.4%.\n", "rewritten_text": "Over the past decade, action recognition has garnered increased interest from the computer vision and machine learning communities. Various action datasets have been created for studying this problem, sourced from controlled laboratory settings, real-world surveillance environments, or the Internet. While traditional datasets typically exhibit similar environmental conditions in their training and test splits, leading to high performance, there is a need for more diverse datasets. This paper introduces the Multi-Camera Action Dataset (MCAD), which focuses on open view classification in surveillance settings. MCAD comprises 14,298 action samples across 18 categories, performed by 20 subjects and recorded by 5 cameras. Following the evaluation approach of the LFW dataset, a standard evaluation protocol was designed for MCAD, revealing that while closed-view accuracy reaches 85%, performance significantly drops in cross-view scenarios. In the worst-case scenario, 10-fold cross validation accuracy decreases from 87.0% to 47.4%."}, "2007.06233": {"original_text": "  In the majority of object detection frameworks, the confidence of instance\nclassification is used as the quality criterion of predicted bounding boxes,\nlike the confidence-based ranking in non-maximum suppression (NMS). However,\nthe quality of bounding boxes, indicating the spatial relations, is not only\ncorrelated with the classification scores. Compared with the region proposal\nnetwork (RPN) based detectors, single-shot object detectors suffer the box\nquality as there is a lack of pre-selection of box proposals. In this paper, we\naim at single-shot object detectors and propose a location-aware anchor-based\nreasoning (LAAR) for the bounding boxes. LAAR takes both the location and\nclassification confidences into consideration for the quality evaluation of\nbounding boxes. We introduce a novel network block to learn the relative\nlocation between the anchors and the ground truths, denoted as a localization\nscore, which acts as a location reference during the inference stage. The\nproposed localization score leads to an independent regression branch and\ncalibrates the bounding box quality by scoring the predicted localization score\nso that the best-qualified bounding boxes can be picked up in NMS. Experiments\non MS COCO and PASCAL VOC benchmarks demonstrate that the proposed\nlocation-aware framework enhances the performances of current anchor-based\nsingle-shot object detection frameworks and yields consistent and robust\ndetection results.\n", "rewritten_text": "\"In many object detection frameworks, the confidence of instance classification is typically used as the primary measure of predicted bounding box quality, such as in the confidence-based ranking used in non-maximum suppression (NMS). However, the quality of bounding boxes, which reflects spatial relationships, is not solely dependent on classification scores. Unlike region proposal network (RPN) based detectors, single-shot object detectors face challenges in box quality due to the lack of pre-selection of box proposals. This study focuses on single-shot object detectors and introduces a method called location-aware anchor-based reasoning (LAAR) for evaluating bounding box quality. LAAR considers both location and classification confidences in assessing bounding box quality. A new network block is proposed to learn the relative location between anchors and ground truths, referred to as a localization score, which serves as a location reference during inference. The introduced localization score leads to an independent regression branch and adjusts bounding box quality by scoring the predicted localization score to select the best-qualified bounding boxes in NMS. Experiments conducted on MS COCO and PASCAL VOC benchmarks show that the proposed location-aware framework improves the performance of current anchor-based single-shot object detection frameworks, producing consistent and reliable detection results.\""}, "2308.09475": {"original_text": "  Robot-assisted surgery has made significant progress, with instrument\nsegmentation being a critical factor in surgical intervention quality. It\nserves as the building block to facilitate surgical robot navigation and\nsurgical education for the next generation of operating intelligence. Although\nexisting methods have achieved accurate instrument segmentation results, they\nsimultaneously generate segmentation masks for all instruments, without the\ncapability to specify a target object and allow an interactive experience. This\nwork explores a new task of Referring Surgical Video Instrument Segmentation\n(RSVIS), which aims to automatically identify and segment the corresponding\nsurgical instruments based on the given language expression. To achieve this,\nwe devise a novel Video-Instrument Synergistic Network (VIS-Net) to learn both\nvideo-level and instrument-level knowledge to boost performance, while previous\nwork only used video-level information. Meanwhile, we design a Graph-based\nRelation-aware Module (GRM) to model the correlation between multi-modal\ninformation (i.e., textual description and video frame) to facilitate the\nextraction of instrument-level information. We are also the first to produce\ntwo RSVIS datasets to promote related research. Our method is verified on these\ndatasets, and experimental results exhibit that the VIS-Net can significantly\noutperform existing state-of-the-art referring segmentation methods. Our code\nand our datasets will be released upon the publication of this work.\n", "rewritten_text": "Robot-assisted surgery has made significant advancements, with instrument segmentation playing a crucial role in ensuring high-quality surgical interventions. This segmentation is essential for enhancing surgical robot navigation and educating the next generation of surgical practitioners. While current methods have successfully achieved accurate instrument segmentation results, they lack the ability to target specific objects and provide an interactive experience. This study introduces a new task called Referring Surgical Video Instrument Segmentation (RSVIS), which aims to automatically identify and segment surgical instruments based on language expressions. To accomplish this, a novel Video-Instrument Synergistic Network (VIS-Net) is developed to leverage both video-level and instrument-level knowledge for improved performance. Additionally, a Graph-based Relation-aware Module (GRM) is designed to model the correlation between different types of information (textual descriptions and video frames) to enhance instrument-level information extraction. Two RSVIS datasets are created to support further research in this area. Experimental results demonstrate that VIS-Net outperforms existing referring segmentation methods. The code and datasets will be made available upon publication of this work."}, "1401.119": {"original_text": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n", "rewritten_text": "The extraction and identification of Bangla text from video frame images present challenges such as complex color backgrounds and low resolution. This paper introduces an algorithm designed to address these challenges by extracting and recognizing Bangla text from video frames with complex backgrounds. The proposed method involves a two-step approach. Initially, the text line is divided into words based on line contours, utilizing the first-order gradient value of text blocks to determine word gaps. Subsequently, a local binarization technique is applied to each word, and the text line is reconstructed using these words. Finally, the binarized text block is processed by OCR for recognition purposes."}, "1502.06807": {"original_text": "  We introduce and evaluate several architectures for Convolutional Neural\nNetworks to predict the 3D joint locations of a hand given a depth map. We\nfirst show that a prior on the 3D pose can be easily introduced and\nsignificantly improves the accuracy and reliability of the predictions. We also\nshow how to use context efficiently to deal with ambiguities between fingers.\nThese two contributions allow us to significantly outperform the\nstate-of-the-art on several challenging benchmarks, both in terms of accuracy\nand computation times.\n", "rewritten_text": "\"We present and assess multiple designs for Convolutional Neural Networks aimed at forecasting the 3D positions of hand joints based on depth maps. Initially, we demonstrate that incorporating a prior on the 3D pose can be done effortlessly and greatly enhances prediction accuracy and reliability. Additionally, we illustrate an effective method for leveraging context to address finger ambiguity. These advancements enable us to surpass the current best performance on various demanding benchmarks, excelling in both accuracy and computational efficiency.\""}, "1812.01855": {"original_text": "  We aim to dismantle the prevalent black-box neural architectures used in\ncomplex visual reasoning tasks, into the proposed eXplainable and eXplicit\nNeural Modules (XNMs), which advance beyond existing neural module networks\ntowards using scene graphs --- objects as nodes and the pairwise relationships\nas edges --- for explainable and explicit reasoning with structured knowledge.\nXNMs allow us to pay more attention to teach machines how to \"think\",\nregardless of what they \"look\". As we will show in the paper, by using scene\ngraphs as an inductive bias, 1) we can design XNMs in a concise and flexible\nfashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce\nthe number of parameters by 10 to 100 times, and 2) we can explicitly trace the\nreasoning-flow in terms of graph attentions. XNMs are so generic that they\nsupport a wide range of scene graph implementations with various qualities. For\nexample, when the graphs are detected perfectly, XNMs achieve 100% accuracy on\nboth CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound\nfor visual reasoning; when the graphs are noisily detected from real-world\nimages, XNMs are still robust to achieve a competitive 67.5% accuracy on\nVQAv2.0, surpassing the popular bag-of-objects attention models without graph\nstructures.\n", "rewritten_text": "Our goal is to break down the commonly used black-box neural architectures in complex visual reasoning tasks and introduce the eXplainable and eXplicit Neural Modules (XNMs). These modules move beyond traditional neural networks by utilizing scene graphs, where objects are represented as nodes and their relationships as edges, to enable explainable and explicit reasoning with structured knowledge. XNMs help us focus on teaching machines how to \"think\" rather than just how things \"look\". By incorporating scene graphs as an inductive bias, we can create XNMs in a concise and flexible manner, reducing the number of parameters significantly and allowing us to trace the reasoning flow through graph attentions. XNMs are versatile and can support various scene graph implementations with different qualities. In experiments, XNMs achieve 100% accuracy on CLEVR and CLEVR CoGenT when perfect graphs are provided, setting a performance benchmark for visual reasoning. Even when dealing with noisy real-world images, XNMs demonstrate robustness by achieving a competitive 67.5% accuracy on VQAv2.0, outperforming traditional bag-of-objects attention models without graph structures."}, "2401.03340": {"original_text": "  This paper introduces the CowStallNumbers dataset, a collection of images\nextracted from videos focusing on cow teats, designed to advance the field of\ncow stall number detection. The dataset comprises 1042 training images and 261\ntest images, featuring stall numbers ranging from 0 to 60. To enhance the\ndataset, we performed fine-tuning on a YOLO model and applied data augmentation\ntechniques, including random crop, center crop, and random rotation. The\nexperimental outcomes demonstrate a notable 95.4\\% accuracy in recognizing\nstall numbers.\n", "rewritten_text": "This paper presents the CowStallNumbers dataset, which consists of images taken from videos that highlight cow teats. The dataset aims to improve the detection of cow stall numbers. It includes 1042 training images and 261 test images, showcasing stall numbers from 0 to 60. To improve the dataset, we fine-tuned a YOLO model and utilized data augmentation methods such as random crop, center crop, and random rotation. The results of the experiments show an impressive 95.4% accuracy in identifying stall numbers."}, "2307.05158": {"original_text": "  Predicting where a person is looking is a complex task, requiring to\nunderstand not only the person's gaze and scene content, but also the 3D scene\nstructure and the person's situation (are they manipulating? interacting or\nobserving others? attentive?) to detect obstructions in the line of sight or\napply attention priors that humans typically have when observing others. In\nthis paper, we hypothesize that identifying and leveraging such priors can be\nbetter achieved through the exploitation of explicitly derived multimodal cues\nsuch as depth and pose. We thus propose a modular multimodal architecture\nallowing to combine these cues using an attention mechanism. The architecture\ncan naturally be exploited in privacy-sensitive situations such as surveillance\nand health, where personally identifiable information cannot be released. We\nperform extensive experiments on the GazeFollow and VideoAttentionTarget public\ndatasets, obtaining state-of-the-art performance and demonstrating very\ncompetitive results in the privacy setting case.\n", "rewritten_text": "Predicting a person's gaze direction involves understanding various factors such as their gaze, surroundings, 3D scene structure, and current activity. This task also requires detecting obstacles in the line of sight and considering attention patterns typically observed in human behavior. In this study, we suggest that leveraging multimodal cues like depth and pose can enhance the accuracy of predicting gaze direction. To achieve this, we propose a modular multimodal architecture that combines these cues using an attention mechanism. This architecture is particularly useful in privacy-sensitive scenarios like surveillance and healthcare, where protecting personal information is crucial. Our experiments on the GazeFollow and VideoAttentionTarget datasets show that our approach achieves state-of-the-art performance and competitive results in privacy-preserving settings."}, "2302.07027": {"original_text": "  Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.\n", "rewritten_text": "Pretrained language models (PLMs) are initially trained on large datasets, but often require specialization for specific domains. A method for efficient parameter adaptation involves training domain-specific adapters for language modeling tasks. While this approach yields high performance within specific domains, it may not be practical in scenarios with limited resources or domain constraints. To address this issue, a related-domain adapter can be utilized for new domains during testing. This paper introduces AdapterSoup, a technique that involves averaging the weights of adapters trained on various domains. The process is parallel and involves training domain-specific adapters first, followed by determining which adapters to combine for each new domain at test time. Extensive experiments demonstrate that AdapterSoup consistently enhances performance on new domains without requiring additional training. Additionally, the method explores weight averaging of adapters trained on the same domain but with different hyperparameters, showing that it maintains PLM performance on new domains while achieving strong results within specific domains. Various approaches for selecting which adapters to combine, such as text clustering and semantic similarity, are explored, with clustering proving to be the most effective method for achieving competitive results on new domains."}, "1706.09147": {"original_text": "  We address the task of Named Entity Disambiguation (NED) for noisy text. We\npresent WikilinksNED, a large-scale NED dataset of text fragments from the web,\nwhich is significantly noisier and more challenging than existing news-based\ndatasets. To capture the limited and noisy local context surrounding each\nmention, we design a neural model and train it with a novel method for sampling\ninformative negative examples. We also describe a new way of initializing word\nand entity embeddings that significantly improves performance. Our model\nsignificantly outperforms existing state-of-the-art methods on WikilinksNED\nwhile achieving comparable performance on a smaller newswire dataset.\n", "rewritten_text": "We focus on Named Entity Disambiguation (NED) in noisy text. Our work introduces WikilinksNED, a large-scale dataset of text snippets from the web that is more challenging and noisy compared to traditional news datasets. To handle the limited and noisy context around each entity mention, we develop a neural model and train it using a unique method for selecting negative examples. Additionally, we propose a novel approach for initializing word and entity embeddings, leading to improved performance. Our model surpasses current state-of-the-art methods on WikilinksNED and performs similarly on a smaller newswire dataset."}, "2312.16274": {"original_text": "  Recent progress in multi-modal conditioned face synthesis has enabled the\ncreation of visually striking and accurately aligned facial images. Yet,\ncurrent methods still face issues with scalability, limited flexibility, and a\none-size-fits-all approach to control strength, not accounting for the\ndiffering levels of conditional entropy, a measure of unpredictability in data\ngiven some condition, across modalities. To address these challenges, we\nintroduce a novel uni-modal training approach with modal surrogates, coupled\nwith an entropy-aware modal-adaptive modulation, to support flexible, scalable,\nand scalable multi-modal conditioned face synthesis network. Our uni-modal\ntraining with modal surrogate that only leverage uni-modal data, use modal\nsurrogate to decorate condition with modal-specific characteristic and serve as\nlinker for inter-modal collaboration , fully learns each modality control in\nface synthesis process as well as inter-modal collaboration. The entropy-aware\nmodal-adaptive modulation finely adjust diffusion noise according to\nmodal-specific characteristics and given conditions, enabling well-informed\nstep along denoising trajectory and ultimately leading to synthesis results of\nhigh fidelity and quality. Our framework improves multi-modal face synthesis\nunder various conditions, surpassing current methods in image quality and\nfidelity, as demonstrated by our thorough experimental results.\n", "rewritten_text": "Recent advancements in multi-modal conditioned face synthesis have allowed for the generation of visually appealing and accurately aligned facial images. However, existing techniques still encounter challenges related to scalability, limited flexibility, and a uniform approach to controlling strength that does not consider the varying levels of conditional entropy across different modalities. To tackle these obstacles, we propose a new approach to training using modal surrogates in a uni-modal setting, combined with an entropy-aware modal-adaptive modulation. This method aims to support a more adaptable, scalable, and effective multi-modal conditioned face synthesis network. Our uni-modal training approach utilizes modal surrogates to enhance condition with modal-specific characteristics and facilitate inter-modal collaboration, enabling comprehensive learning of each modality's control in the face synthesis process. The entropy-aware modal-adaptive modulation adjusts diffusion noise based on modal-specific features and conditions, guiding the denoising process towards high-quality synthesis outcomes. Our framework enhances multi-modal face synthesis across diverse conditions, outperforming current methods in terms of image quality and fidelity, as evidenced by our comprehensive experimental findings."}, "2407.06938": {"original_text": "  We present RodinHD, which can generate high-fidelity 3D avatars from a\nportrait image. Existing methods fail to capture intricate details such as\nhairstyles which we tackle in this paper. We first identify an overlooked\nproblem of catastrophic forgetting that arises when fitting triplanes\nsequentially on many avatars, caused by the MLP decoder sharing scheme. To\novercome this issue, we raise a novel data scheduling strategy and a weight\nconsolidation regularization term, which improves the decoder's capability of\nrendering sharper details. Additionally, we optimize the guiding effect of the\nportrait image by computing a finer-grained hierarchical representation that\ncaptures rich 2D texture cues, and injecting them to the 3D diffusion model at\nmultiple layers via cross-attention. When trained on 46K avatars with a noise\nschedule optimized for triplanes, the resulting model can generate 3D avatars\nwith notably better details than previous methods and can generalize to\nin-the-wild portrait input.\n", "rewritten_text": "We introduce RodinHD, a system that creates highly detailed 3D avatars from a single portrait image. Unlike existing approaches, we address the challenge of capturing intricate features like hairstyles. We identify a previously overlooked issue of catastrophic forgetting when fitting triplanes to multiple avatars in sequence, caused by the MLP decoder sharing scheme. To resolve this problem, we propose a new data scheduling strategy and a regularization term for weight consolidation, enhancing the decoder's ability to produce sharper details. Furthermore, we enhance the guidance provided by the portrait image by creating a more detailed hierarchical representation that incorporates rich 2D texture information. This information is then integrated into the 3D model at various levels using cross-attention mechanisms. By training on a dataset of 46,000 avatars with a noise schedule optimized for triplanes, our model generates 3D avatars with significantly improved details compared to previous methods and can handle diverse portrait inputs."}, "2311.08107": {"original_text": "  Large Language Models (LLMs) can justify or critique their predictions\nthrough discussions with other models or humans, thereby enriching their\nintrinsic understanding of instances. While proactive discussions in the\ninference phase have been shown to boost performance, such interactions have\nnot been extensively explored during the training phase. We hypothesize that\nincorporating interactive discussions into the training process can enhance the\nmodels' understanding and improve their reasoning and verbal expression\nabilities during inference. This work introduces the SAIE framework, which\nfacilitates supportive and adversarial discussions between learner and partner\nmodels. The learner model receives responses from the partner, and its\nparameters are then updated based on this discussion. This dynamic adjustment\nprocess continues throughout the training phase, responding to the evolving\noutputs of the learner model. Our empirical evaluation across various tasks,\nincluding math problems, commonsense reasoning, and multi-domain knowledge,\ndemonstrates that models fine-tuned with the SAIE framework outperform those\ntrained with conventional fine-tuning approaches. Furthermore, our method\nenhances the models' reasoning capabilities, improving both individual and\nmulti-agent inference performance.\n", "rewritten_text": "\"Large Language Models (LLMs) have the ability to justify or critique their predictions by engaging in discussions with other models or humans, which helps them gain a deeper understanding of specific instances. While proactive discussions during the inference phase have been proven to enhance performance, the potential benefits of such interactions during the training phase have not been thoroughly explored. We propose that integrating interactive discussions into the training process can improve the models' comprehension and enhance their reasoning and verbal expression skills for better performance during inference. This study introduces the SAIE framework, which enables supportive and adversarial discussions between a learner model and its partner models. The learner model receives feedback from the partner model, and its parameters are adjusted based on these discussions. This iterative process continues throughout the training phase, adapting to the learner model's evolving outputs. Our experimental results across various tasks, such as math problems, commonsense reasoning, and multi-domain knowledge, show that models fine-tuned using the SAIE framework outperform those trained using traditional fine-tuning methods. Additionally, our approach enhances the models' reasoning abilities, leading to improved performance in both individual and multi-agent inference tasks.\""}, "1704.05831": {"original_text": "  We propose a hierarchical approach for making long-term predictions of future\nframes. To avoid inherent compounding errors in recursive pixel-level\nprediction, we propose to first estimate high-level structure in the input\nframes, then predict how that structure evolves in the future, and finally by\nobserving a single frame from the past and the predicted high-level structure,\nwe construct the future frames without having to observe any of the pixel-level\npredictions. Long-term video prediction is difficult to perform by recurrently\nobserving the predicted frames because the small errors in pixel space\nexponentially amplify as predictions are made deeper into the future. Our\napproach prevents pixel-level error propagation from happening by removing the\nneed to observe the predicted frames. Our model is built with a combination of\nLSTM and analogy based encoder-decoder convolutional neural networks, which\nindependently predict the video structure and generate the future frames,\nrespectively. In experiments, our model is evaluated on the Human3.6M and Penn\nAction datasets on the task of long-term pixel-level video prediction of humans\nperforming actions and demonstrate significantly better results than the\nstate-of-the-art.\n", "rewritten_text": "We suggest a hierarchical method for making long-term predictions of future frames. To prevent compounding errors in recursive pixel-level prediction, we propose first estimating the high-level structure in the input frames, then predicting how that structure evolves in the future. By observing a single frame from the past and the predicted high-level structure, we can construct future frames without needing to observe any pixel-level predictions. Long-term video prediction is challenging when relying on recurrently observing predicted frames, as small errors in pixel space can exponentially amplify with deeper future predictions. Our approach avoids pixel-level error propagation by eliminating the need to observe predicted frames. Our model combines LSTM and analogy-based encoder-decoder convolutional neural networks to independently predict video structure and generate future frames. In experiments, our model is tested on the Human3.6M and Penn Action datasets for long-term pixel-level video prediction of human actions, showing significantly improved results compared to the current state-of-the-art methods."}, "1606.05706": {"original_text": "  We study the problem of agreement and disagreement detection in online\ndiscussions. An isotonic Conditional Random Fields (isotonic CRF) based\nsequential model is proposed to make predictions on sentence- or segment-level.\nWe automatically construct a socially-tuned lexicon that is bootstrapped from\nexisting general-purpose sentiment lexicons to further improve the performance.\nWe evaluate our agreement and disagreement tagging model on two disparate\nonline discussion corpora -- Wikipedia Talk pages and online debates. Our model\nis shown to outperform the state-of-the-art approaches in both datasets. For\nexample, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for\nagreement and disagreement detection, when a linear chain CRF obtains 0.58 and\n0.56 for the discussions on Wikipedia Talk pages.\n", "rewritten_text": "We are researching how to detect agreement and disagreement in online discussions. We propose a sequential model based on isotonic Conditional Random Fields (isotonic CRF) to predict at the sentence- or segment-level. Additionally, we create a socially-tuned lexicon derived from existing sentiment lexicons to enhance performance. Our agreement and disagreement tagging model is tested on two different online discussion datasets -- Wikipedia Talk pages and online debates. Our model surpasses existing methods in both datasets. For instance, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for agreement and disagreement detection, outperforming a linear chain CRF which scores 0.58 and 0.56 on Wikipedia Talk pages discussions."}, "2401.13011": {"original_text": "  This paper presents a novel generative model, Collaborative Competitive\nAgents (CCA), which leverages the capabilities of multiple Large Language\nModels (LLMs) based agents to execute complex tasks. Drawing inspiration from\nGenerative Adversarial Networks (GANs), the CCA system employs two equal-status\ngenerator agents and a discriminator agent. The generators independently\nprocess user instructions and generate results, while the discriminator\nevaluates the outputs, and provides feedback for the generator agents to\nfurther reflect and improve the generation results. Unlike the previous\ngenerative model, our system can obtain the intermediate steps of generation.\nThis allows each generator agent to learn from other successful executions due\nto its transparency, enabling a collaborative competition that enhances the\nquality and robustness of the system's results. The primary focus of this study\nis image editing, demonstrating the CCA's ability to handle intricate\ninstructions robustly. The paper's main contributions include the introduction\nof a multi-agent-based generative model with controllable intermediate steps\nand iterative optimization, a detailed examination of agent relationships, and\ncomprehensive experiments on image editing. Code is available at\n\\href{https://github.com/TiankaiHang/CCA}{https://github.com/TiankaiHang/CCA}.\n", "rewritten_text": "This paper introduces a new generative model called Collaborative Competitive Agents (CCA), which utilizes multiple Large Language Models (LLMs) to perform complex tasks. Inspired by Generative Adversarial Networks (GANs), the CCA system consists of two generator agents and a discriminator agent of equal status. The generators process user instructions independently to generate results, while the discriminator evaluates the outputs and provides feedback to help the generators improve. Unlike previous generative models, our system allows for the observation of intermediate generation steps, enabling collaborative competition among the agents to enhance the quality and robustness of results. The study focuses on image editing, showcasing the CCA's ability to handle detailed instructions effectively. Key contributions of the paper include introducing a multi-agent generative model with controllable intermediate steps and iterative optimization, analyzing agent relationships in detail, and conducting comprehensive experiments on image editing. The code can be accessed at \\href{https://github.com/TiankaiHang/CCA}{https://github.com/TiankaiHang/CCA}."}, "2309.08644": {"original_text": "  3D pose estimation is an invaluable task in computer vision with various\npractical applications. Especially, 3D pose estimation for multi-person from a\nmonocular video (3DMPPE) is particularly challenging and is still largely\nuncharted, far from applying to in-the-wild scenarios yet. We pose three\nunresolved issues with the existing methods: lack of robustness on unseen views\nduring training, vulnerability to occlusion, and severe jittering in the\noutput. As a remedy, we propose POTR-3D, the first realization of a\nsequence-to-sequence 2D-to-3D lifting model for 3DMPPE, powered by a novel\ngeometry-aware data augmentation strategy, capable of generating unbounded data\nwith a variety of views while caring about the ground plane and occlusions.\nThrough extensive experiments, we verify that the proposed model and data\naugmentation robustly generalizes to diverse unseen views, robustly recovers\nthe poses against heavy occlusions, and reliably generates more natural and\nsmoother outputs. The effectiveness of our approach is verified not only by\nachieving the state-of-the-art performance on public benchmarks, but also by\nqualitative results on more challenging in-the-wild videos. Demo videos are\navailable at https://www.youtube.com/@potr3d.\n", "rewritten_text": "3D pose estimation is a crucial task in computer vision with practical applications. Specifically, estimating 3D poses for multiple individuals from a single video (3DMPPE) is highly challenging and not yet widely applicable in real-world scenarios. We identify three key issues with current methods: lack of robustness when faced with unseen views during training, susceptibility to occlusion, and significant jittering in the output. To address these challenges, we introduce POTR-3D, a novel sequence-to-sequence 2D-to-3D lifting model for 3DMPPE. This model is enhanced by a unique geometry-aware data augmentation technique that generates diverse data with various views, considering ground plane and occlusions. Extensive experiments demonstrate that our proposed model and data augmentation approach effectively generalize to unseen views, handle occlusions well, and produce more natural and smoother outputs. Our method achieves state-of-the-art performance on public benchmarks and shows promising results on challenging real-world videos. Demo videos can be viewed at https://www.youtube.com/@potr3d."}, "2008.08735": {"original_text": "  Computer vision (CV) has achieved great success in interpreting semantic\nmeanings from images, yet CV algorithms can be brittle for tasks with adverse\nvision conditions and the ones suffering from data/label pair limitation. One\nof this tasks is in-bed human pose estimation, which has significant values in\nmany healthcare applications. In-bed pose monitoring in natural settings could\ninvolve complete darkness or full occlusion. Furthermore, the lack of publicly\navailable in-bed pose datasets hinders the use of many successful pose\nestimation algorithms for this task. In this paper, we introduce our\nSimultaneously-collected multimodal Lying Pose (SLP) dataset, which includes\nin-bed pose images from 109 participants captured using multiple imaging\nmodalities including RGB, long wave infrared, depth, and pressure map. We also\npresent a physical hyper parameter tuning strategy for ground truth pose label\ngeneration under extreme conditions such as lights off and being fully covered\nby a sheet/blanket. SLP design is compatible with the mainstream human pose\ndatasets, therefore, the state-of-the-art 2D pose estimation models can be\ntrained effectively with SLP data with promising performance as high as 95% at\nPCKh@0.5 on a single modality. The pose estimation performance can be further\nimproved by including additional modalities through collaboration.\n", "rewritten_text": "Computer vision has made significant progress in extracting semantic meanings from images. However, existing CV algorithms may struggle with tasks that involve challenging visual conditions or limited data and labels. One such task is estimating human poses in bed, which is crucial for various healthcare applications. Monitoring poses in bed under natural conditions, such as darkness or occlusion, can be particularly difficult. Additionally, the lack of publicly available datasets for in-bed poses hampers the application of successful pose estimation algorithms. This paper introduces the Simultaneously-collected multimodal Lying Pose (SLP) dataset, which features in-bed pose images of 109 participants captured using various imaging modalities like RGB, long wave infrared, depth, and pressure map. A method for generating accurate pose labels under extreme conditions, like darkness and full coverage by a sheet, is also presented. The SLP dataset is compatible with mainstream human pose datasets, enabling effective training of state-of-the-art 2D pose estimation models with up to 95% performance at PCKh@0.5 using a single modality. Collaborating with additional modalities can further enhance pose estimation performance."}, "2206.04846": {"original_text": "  Deep neural networks are capable of learning powerful representations to\ntackle complex vision tasks but expose undesirable properties like the\nover-fitting issue. To this end, regularization techniques like image\naugmentation are necessary for deep neural networks to generalize well.\nNevertheless, most prevalent image augmentation recipes confine themselves to\noff-the-shelf linear transformations like scale, flip, and colorjitter. Due to\ntheir hand-crafted property, these augmentations are insufficient to generate\ntruly hard augmented examples. In this paper, we propose a novel perspective of\naugmentation to regularize the training process. Inspired by the recent success\nof applying masked image modeling to self-supervised learning, we adopt the\nself-supervised masked autoencoder to generate the distorted view of the input\nimages. We show that utilizing such model-based nonlinear transformation as\ndata augmentation can improve high-level recognition tasks. We term the\nproposed method as \\textbf{M}ask-\\textbf{R}econstruct \\textbf{A}ugmentation\n(MRA). The extensive experiments on various image classification benchmarks\nverify the effectiveness of the proposed augmentation. Specifically, MRA\nconsistently enhances the performance on supervised, semi-supervised as well as\nfew-shot classification. The code will be available at\n\\url{https://github.com/haohang96/MRA}.\n", "rewritten_text": "\"Although deep neural networks can learn powerful representations for complex vision tasks, they often suffer from issues such as overfitting. In order to address this, regularization techniques like image augmentation are essential for ensuring that deep neural networks can generalize effectively. However, many existing image augmentation methods are limited to basic linear transformations like scaling, flipping, and color adjustments. These hand-crafted augmentations may not be sufficient for generating truly challenging augmented examples. In this study, we introduce a new approach to augmentation that aims to improve the training process. Drawing inspiration from the success of masked image modeling in self-supervised learning, we propose using a self-supervised masked autoencoder to create distorted versions of input images. Our experiments demonstrate that incorporating such model-based nonlinear transformations as data augmentation can enhance performance in high-level recognition tasks. We refer to this method as Mask-Reconstruct Augmentation (MRA). Extensive experiments on various image classification benchmarks confirm the effectiveness of our proposed augmentation technique, showing consistent improvements in supervised, semi-supervised, and few-shot classification tasks. The code for implementing MRA will be available at https://github.com/haohang96/MRA.\""}, "1803.10547": {"original_text": "  Text articles with false claims, especially news, have recently become\naggravating for the Internet users. These articles are in wide circulation and\nreaders face difficulty discerning fact from fiction. Previous work on\ncredibility assessment has focused on factual analysis and linguistic features.\nThe task's main challenge is the distinction between the features of true and\nfalse articles. In this paper, we propose a novel approach called Credibility\nOutcome (CREDO) which aims at scoring the credibility of an article in an open\ndomain setting.\n  CREDO consists of different modules for capturing various features\nresponsible for the credibility of an article. These features includes\ncredibility of the article's source and author, semantic similarity between the\narticle and related credible articles retrieved from a knowledge base, and\nsentiments conveyed by the article. A neural network architecture learns the\ncontribution of each of these modules to the overall credibility of an article.\nExperiments on Snopes dataset reveals that CREDO outperforms the\nstate-of-the-art approaches based on linguistic features.\n", "rewritten_text": "Recently, there has been a rise in text articles containing false claims, particularly in news, which has been frustrating for Internet users. These articles are widely circulated, making it challenging for readers to distinguish between fact and fiction. Previous research on credibility assessment has focused on analyzing facts and linguistic characteristics. The main difficulty lies in differentiating between true and false articles. This paper introduces a new method called Credibility Outcome (CREDO) that aims to evaluate the credibility of articles in an open domain. CREDO comprises various modules that capture different aspects influencing an article's credibility, such as the credibility of the source and author, semantic similarity to credible articles from a knowledge base, and the sentiments expressed in the article. A neural network architecture is used to learn how each module contributes to the overall credibility assessment. Experiments conducted on the Snopes dataset demonstrate that CREDO surpasses current approaches based on linguistic features."}, "1704.01926": {"original_text": "  This paper tackles the problem of semi-supervised video object segmentation,\nthat is, segmenting an object in a sequence given its mask in the first frame.\nOne of the main challenges in this scenario is the change of appearance of the\nobjects of interest. Their semantics, on the other hand, do not vary. This\npaper investigates how to take advantage of such invariance via the\nintroduction of a semantic prior that guides the appearance model.\nSpecifically, given the segmentation mask of the first frame of a sequence, we\nestimate the semantics of the object of interest, and propagate that knowledge\nthroughout the sequence to improve the results based on an appearance model. We\npresent Semantically-Guided Video Object Segmentation (SGV), which improves\nresults over previous state of the art on two different datasets using a\nvariety of evaluation metrics, while running in half a second per frame.\n", "rewritten_text": "This study addresses the issue of semi-supervised video object segmentation, which involves segmenting an object in a sequence based on its mask in the initial frame. A key challenge in this context is the changing appearance of the objects, while their semantics remain consistent. The research explores leveraging this semantic consistency by introducing a semantic prior to inform the appearance model. The proposed approach, Semantically-Guided Video Object Segmentation (SGV), utilizes the segmentation mask from the first frame to estimate the object's semantics and propagate this information throughout the sequence to enhance results based on the appearance model. SGV demonstrates improved performance compared to previous state-of-the-art methods on two datasets across various evaluation metrics, achieving real-time processing speed of half a second per frame."}, "1902.06557": {"original_text": "  We propose a novel biophysical and dichromatic reflectance model that\nefficiently characterises spectral skin reflectance. We show how to fit the\nmodel to multispectral face images enabling high quality estimation of diffuse\nand specular shading as well as biophysical parameter maps (melanin and\nhaemoglobin). Our method works from a single image without requiring complex\ncontrolled lighting setups yet provides quantitatively accurate reconstructions\nand qualitatively convincing decomposition and editing.\n", "rewritten_text": "We introduce a new biophysical and dichromatic reflectance model that effectively describes the spectral reflectance of skin. Our approach demonstrates how to apply this model to multispectral face images, allowing for accurate estimation of diffuse and specular shading, as well as maps of biophysical parameters such as melanin and hemoglobin. Our method operates using just a single image, eliminating the need for elaborate lighting setups, while still producing precise reconstructions and visually appealing decomposition and editing results."}, "2211.09379": {"original_text": "  In dialogue state tracking (DST), labeling the dataset involves considerable\nhuman labor. We propose a new self-training framework for few-shot generative\nDST that utilize unlabeled data. Our self-training method iteratively improves\nthe model by pseudo labeling and employs Purpose Preserving Augmentation\n(PPAug) to prevent overfitting. We increaese the few-shot 10% performance by\napproximately 4% on MultiWOZ 2.1 and enhances the slot-recall 8.34% for unseen\nvalues compared to baseline.\n", "rewritten_text": "\"In the context of dialogue state tracking (DST), annotating the dataset requires significant human effort. We introduce a novel self-training approach for few-shot generative DST that leverages unlabeled data. Our self-training technique enhances the model through iterative refinement using pseudo labeling and incorporates Purpose Preserving Augmentation (PPAug) to avoid overfitting. We achieve a 4% improvement in few-shot performance on MultiWOZ 2.1 and enhance slot-recall by 8.34% for unseen values compared to the baseline.\""}, "1902.11268": {"original_text": "  Deep neural networks (DNNs), especially deep convolutional neural networks\n(CNNs), have emerged as the powerful technique in various machine learning\napplications. However, the large model sizes of DNNs yield high demands on\ncomputation resource and weight storage, thereby limiting the practical\ndeployment of DNNs. To overcome these limitations, this paper proposes to\nimpose the circulant structure to the construction of convolutional layers, and\nhence leads to circulant convolutional layers (CircConvs) and circulant CNNs.\nThe circulant structure and models can be either trained from scratch or\nre-trained from a pre-trained non-circulant model, thereby making it very\nflexible for different training environments. Through extensive experiments,\nsuch strong structure-imposing approach is proved to be able to substantially\nreduce the number of parameters of convolutional layers and enable significant\nsaving of computational cost by using fast multiplication of the circulant\ntensor.\n", "rewritten_text": "\"Deep neural networks, particularly deep convolutional neural networks (CNNs), have become a powerful tool in various machine learning applications. However, the large sizes of these models create significant demands on computational resources and storage, which can hinder their practical deployment. To address these challenges, this study suggests incorporating a circulant structure into the design of convolutional layers, resulting in circulant convolutional layers (CircConvs) and circulant CNNs. These circulant structures and models can be trained from scratch or re-trained from a pre-existing non-circulant model, offering flexibility for different training scenarios. Through extensive experiments, this structured approach has been shown to significantly reduce the number of parameters in convolutional layers and achieve substantial savings in computational costs through efficient circulant tensor multiplication.\""}, "1704.00447": {"original_text": "  Purpose: To allow fast and high-quality reconstruction of clinical\naccelerated multi-coil MR data by learning a variational network that combines\nthe mathematical structure of variational models with deep learning.\n  Theory and Methods: Generalized compressed sensing reconstruction formulated\nas a variational model is embedded in an unrolled gradient descent scheme. All\nparameters of this formulation, including the prior model defined by filter\nkernels and activation functions as well as the data term weights, are learned\nduring an offline training procedure. The learned model can then be applied\nonline to previously unseen data.\n  Results: The variational network approach is evaluated on a clinical knee\nimaging protocol. The variational network reconstructions outperform standard\nreconstruction algorithms in terms of image quality and residual artifacts for\nall tested acceleration factors and sampling patterns.\n  Conclusion: Variational network reconstructions preserve the natural\nappearance of MR images as well as pathologies that were not included in the\ntraining data set. Due to its high computational performance, i.e.,\nreconstruction time of 193 ms on a single graphics card, and the omission of\nparameter tuning once the network is trained, this new approach to image\nreconstruction can easily be integrated into clinical workflow.\n", "rewritten_text": "Objective: The aim is to efficiently and effectively reconstruct clinical accelerated multi-coil MR data using a variational network that merges variational models with deep learning techniques.\nMethodology: A generalized compressed sensing reconstruction is framed as a variational model within an unrolled gradient descent process. All parameters, such as filter kernels, activation functions, and data term weights, are learned through offline training. The trained model can then be applied to new data in real-time.\nFindings: The variational network method is assessed using a clinical knee imaging protocol. Results show that the variational network reconstructions surpass traditional algorithms in terms of image quality and artifact reduction across various acceleration factors and sampling patterns.\nConclusion: Variational network reconstructions maintain the natural appearance of MR images and can accurately depict pathologies not present in the training dataset. With rapid computational performance (193 ms on a single graphics card) and no need for parameter adjustments post-training, this novel reconstruction approach can seamlessly integrate into clinical workflows."}, "2104.1269": {"original_text": "  Data is the engine of modern computer vision, which necessitates collecting\nlarge-scale datasets. This is expensive, and guaranteeing the quality of the\nlabels is a major challenge. In this paper, we investigate efficient annotation\nstrategies for collecting multi-class classification labels for a large\ncollection of images. While methods that exploit learnt models for labeling\nexist, a surprisingly prevalent approach is to query humans for a fixed number\nof labels per datum and aggregate them, which is expensive. Building on prior\nwork on online joint probabilistic modeling of human annotations and\nmachine-generated beliefs, we propose modifications and best practices aimed at\nminimizing human labeling effort. Specifically, we make use of advances in\nself-supervised learning, view annotation as a semi-supervised learning\nproblem, identify and mitigate pitfalls and ablate several key design choices\nto propose effective guidelines for labeling. Our analysis is done in a more\nrealistic simulation that involves querying human labelers, which uncovers\nissues with evaluation using existing worker simulation methods. Simulated\nexperiments on a 125k image subset of the ImageNet100 show that it can be\nannotated to 80% top-1 accuracy with 0.35 annotations per image on average, a\n2.7x and 6.7x improvement over prior work and manual annotation, respectively.\nProject page: https://fidler-lab.github.io/efficient-annotation-cookbook\n", "rewritten_text": "The text discusses the importance of data in modern computer vision and the challenges of collecting high-quality datasets for multi-class classification labels. The paper explores efficient annotation strategies to minimize human labeling effort by leveraging self-supervised learning and semi-supervised learning techniques. The study proposes modifications and best practices based on joint probabilistic modeling of human annotations and machine-generated beliefs. Through simulated experiments on a subset of ImageNet100, the research demonstrates significant improvements in annotation accuracy with fewer annotations per image compared to previous methods. For more information, visit the project page at https://fidler-lab.github.io/efficient-annotation-cookbook."}, "2206.00923": {"original_text": "  We present a method that achieves state-of-the-art results on challenging\n(few-shot) layout-to-image generation tasks by accurately modeling textures,\nstructures and relationships contained in a complex scene. After compressing\nRGB images into patch tokens, we propose the Transformer with Focal Attention\n(TwFA) for exploring dependencies of object-to-object, object-to-patch and\npatch-to-patch. Compared to existing CNN-based and Transformer-based generation\nmodels that entangled modeling on pixel-level&patch-level and\nobject-level&patch-level respectively, the proposed focal attention predicts\nthe current patch token by only focusing on its highly-related tokens that\nspecified by the spatial layout, thereby achieving disambiguation during\ntraining. Furthermore, the proposed TwFA largely increases the data efficiency\nduring training, therefore we propose the first few-shot complex scene\ngeneration strategy based on the well-trained TwFA. Comprehensive experiments\nshow the superiority of our method, which significantly increases both\nquantitative metrics and qualitative visual realism with respect to\nstate-of-the-art CNN-based and transformer-based methods. Code is available at\nhttps://github.com/JohnDreamer/TwFA.\n", "rewritten_text": "We introduce a novel approach that outperforms existing methods in generating images from layouts, especially in challenging few-shot scenarios. Our method accurately captures textures, structures, and relationships within complex scenes by representing RGB images as patch tokens and utilizing the Transformer with Focal Attention (TwFA). Unlike previous models that mix pixel-level and patch-level modeling, or object-level and patch-level modeling, TwFA focuses on dependencies between objects, patches, and spatial layouts to predict each patch token. This targeted attention mechanism enhances training by clarifying relationships between tokens. TwFA also improves data efficiency, enabling the development of a novel few-shot complex scene generation strategy. Our method demonstrates superior performance in both quantitative metrics and visual realism compared to state-of-the-art CNN-based and transformer-based approaches. The code is available at https://github.com/JohnDreamer/TwFA."}, "2204.09841": {"original_text": "  Information from an image occurs over multiple and distinct spatial scales.\nImage pyramid multiresolution representations are a useful data structure for\nimage analysis and manipulation over a spectrum of spatial scales. This paper\nemploys the Gaussian-Laplacian pyramid to treat different spatial frequency\nbands of a texture separately. First, we generate three images corresponding to\nthree levels of the Gaussian-Laplacian pyramid for an input image to capture\nintrinsic details. Then we aggregate features extracted from gray and color\ntexture images using bio-inspired texture descriptors, information-theoretic\nmeasures, gray-level co-occurrence matrix features, and Haralick statistical\nfeatures into a single feature vector. Such an aggregation aims at producing\nfeatures that characterize textures to their maximum extent, unlike employing\neach descriptor separately, which may lose some relevant textural information\nand reduce the classification performance. The experimental results on texture\nand histopathologic image datasets have shown the advantages of the proposed\nmethod compared to state-of-the-art approaches. Such findings emphasize the\nimportance of multiscale image analysis and corroborate that the descriptors\nmentioned above are complementary.\n", "rewritten_text": "The information in an image is present at various spatial scales. Using image pyramids with multiple resolutions is beneficial for analyzing and manipulating images across different scales. This study utilizes the Gaussian-Laplacian pyramid to process different spatial frequencies of textures individually. Initially, three images are created at different levels of the Gaussian-Laplacian pyramid to capture specific details of an input image. Features extracted from grayscale and color texture images are combined using bio-inspired texture descriptors, information-theoretic measures, gray-level co-occurrence matrix features, and Haralick statistical features to create a comprehensive feature vector. This approach aims to fully characterize textures, unlike using each descriptor separately, which may lead to the loss of relevant textural information and reduced classification performance. Experimental results on texture and histopathologic image datasets demonstrate the advantages of the proposed method over existing techniques. These findings underscore the importance of multiscale image analysis and highlight the complementary nature of the mentioned descriptors."}, "2207.00744": {"original_text": "  Spatial-Temporal Video Grounding (STVG) is a challenging task which aims to\nlocalize the spatio-temporal tube of the interested object semantically\naccording to a natural language query. Most previous works not only severely\nrely on the anchor boxes extracted by Faster R-CNN, but also simply regard the\nvideo as a series of individual frames, thus lacking their temporal modeling.\nInstead, in this paper, we are the first to propose an anchor-free framework\nfor STVG, called Gaussian Kernel-based Cross Modal Network (GKCMN).\nSpecifically, we utilize the learned Gaussian Kernel-based heatmaps of each\nvideo frame to locate the query-related object. A mixed serial and parallel\nconnection network is further developed to leverage both spatial and temporal\nrelations among frames for better grounding. Experimental results on VidSTG\ndataset demonstrate the effectiveness of our proposed GKCMN.\n", "rewritten_text": "The task of Spatial-Temporal Video Grounding (STVG) is a complex challenge that involves semantically localizing the spatio-temporal tube of a specific object based on a natural language query. Previous approaches heavily relied on anchor boxes from Faster R-CNN and treated the video as a sequence of individual frames, neglecting temporal modeling. This paper introduces a novel anchor-free framework for STVG, named Gaussian Kernel-based Cross Modal Network (GKCMN). The GKCMN utilizes Gaussian Kernel-based heatmaps learned from each video frame to pinpoint the object related to the query. A mixed serial and parallel connection network is then developed to incorporate spatial and temporal relationships among frames for improved grounding. Experimental results on the VidSTG dataset validate the effectiveness of the proposed GKCMN."}, "2209.00232": {"original_text": "  Capsule networks (CapsNets) aim to parse images into a hierarchy of objects,\nparts, and their relations using a two-step process involving part-whole\ntransformation and hierarchical component routing. However, this hierarchical\nrelationship modeling is computationally expensive, which has limited the wider\nuse of CapsNet despite its potential advantages. The current state of CapsNet\nmodels primarily focuses on comparing their performance with capsule baselines,\nfalling short of achieving the same level of proficiency as deep CNN variants\nin intricate tasks. To address this limitation, we present an efficient\napproach for learning capsules that surpasses canonical baseline models and\neven demonstrates superior performance compared to high-performing convolution\nmodels. Our contribution can be outlined in two aspects: firstly, we introduce\na group of subcapsules onto which an input vector is projected. Subsequently,\nwe present the Hybrid Gromov-Wasserstein framework, which initially quantifies\nthe dissimilarity between the input and the components modeled by the\nsubcapsules, followed by determining their alignment degree through optimal\ntransport. This innovative mechanism capitalizes on new insights into defining\nalignment between the input and subcapsules, based on the similarity of their\nrespective component distributions. This approach enhances CapsNets' capacity\nto learn from intricate, high-dimensional data while retaining their\ninterpretability and hierarchical structure. Our proposed model offers two\ndistinct advantages: (i) its lightweight nature facilitates the application of\ncapsules to more intricate vision tasks, including object detection; (ii) it\noutperforms baseline approaches in these demanding tasks.\n", "rewritten_text": "Capsule networks, also known as CapsNets, are designed to break down images into a hierarchy of objects, parts, and their relationships through a two-step process involving part-whole transformation and hierarchical component routing. However, the computational complexity of this hierarchical relationship modeling has hindered the widespread adoption of CapsNets despite their potential benefits. Current CapsNet models mainly focus on comparing their performance with capsule baselines, falling short of achieving the same level of proficiency as deep CNN variants in complex tasks. To overcome this limitation, we propose an efficient approach for learning capsules that outperforms traditional baseline models and even shows superior performance compared to high-performing convolution models. Our approach involves introducing a group of subcapsules onto which an input vector is projected. We then introduce the Hybrid Gromov-Wasserstein framework, which quantifies the dissimilarity between the input and the components represented by the subcapsules, and determines their alignment degree through optimal transport. This innovative mechanism leverages new insights into defining alignment between the input and subcapsules based on the similarity of their respective component distributions. This method enhances CapsNets' ability to learn from complex, high-dimensional data while maintaining interpretability and a hierarchical structure. Our proposed model offers two key advantages: (i) its lightweight nature enables the application of capsules to more complex vision tasks, such as object detection; (ii) it outperforms baseline approaches in these challenging tasks."}, "2311.12076": {"original_text": "  Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof open-world intelligent systems. Despite the notable advancements in existing\nOOD detection methodologies, our study identifies a significant performance\ndrop under the scarcity of training samples. In this context, we introduce a\nnovel few-shot OOD detection benchmark, carefully constructed to address this\ngap. Our empirical analysis reveals the superiority of ParameterEfficient\nFine-Tuning (PEFT) strategies, such as visual prompt tuning and visual adapter\ntuning, over conventional techniques, including fully fine-tuning and linear\nprobing tuning in the few-shot OOD detection task. Recognizing some crucial\ninformation from the pre-trained model, which is pivotal for OOD detection, may\nbe lost during the fine-tuning process, we propose a method termed\nDomainSpecific and General Knowledge Fusion (DSGF). This approach is designed\nto be compatible with diverse fine-tuning frameworks. Our experiments show that\nthe integration of DSGF significantly enhances the few-shot OOD detection\ncapabilities across various methods and fine-tuning methodologies, including\nfully fine-tuning, visual adapter tuning, and visual prompt tuning. The code\nwill be released.\n", "rewritten_text": "\"Detecting out-of-distribution (OOD) instances is crucial for ensuring the reliability of intelligent systems in open-world scenarios. While existing OOD detection methods have made significant progress, our research highlights a drop in performance when training data is limited. To address this issue, we introduce a new few-shot OOD detection benchmark designed to overcome this challenge. Our study demonstrates the effectiveness of ParameterEfficient Fine-Tuning (PEFT) techniques, such as visual prompt tuning and visual adapter tuning, compared to traditional methods like fully fine-tuning and linear probing tuning in few-shot OOD detection tasks. Recognizing the importance of retaining key information from pre-trained models during fine-tuning, we propose a method called Domain-Specific and General Knowledge Fusion (DSGF) that can be integrated into various fine-tuning frameworks. Our experiments show that incorporating DSGF significantly improves few-shot OOD detection performance across different methods and fine-tuning approaches, including fully fine-tuning, visual adapter tuning, and visual prompt tuning. The code for our work will be made available.\""}, "2104.10117": {"original_text": "  We present a novel deep learning-based framework to generate embedding\nrepresentations of fine-grained emotions that can be used to computationally\ndescribe psychological models of emotions. Our framework integrates a\ncontextualized embedding encoder with a multi-head probing model that enables\nto interpret dynamically learned representations optimized for an emotion\nclassification task. Our model is evaluated on the Empathetic Dialogue dataset\nand shows the state-of-the-art result for classifying 32 emotions. Our layer\nanalysis can derive an emotion graph to depict hierarchical relations among the\nemotions. Our emotion representations can be used to generate an emotion wheel\ndirectly comparable to the one from Plutchik's\\LN model, and also augment the\nvalues of missing emotions in the PAD emotional state model.\n", "rewritten_text": "We introduce a new deep learning framework for creating detailed emotional embeddings that can be applied in computational descriptions of emotional models. Our framework combines a contextualized embedding encoder with a multi-head probing model to interpret dynamically learned representations tailored for emotion classification. We assess our model using the Empathetic Dialogue dataset and achieve cutting-edge results in classifying 32 emotions. Through layer analysis, we construct an emotion graph illustrating hierarchical relationships among emotions. Our emotional representations can generate an emotion wheel similar to Plutchik's model and enhance missing emotions in the PAD emotional state model."}, "1912.06842": {"original_text": "  The main requisite for fine-grained recognition task is to focus on subtle\ndiscriminative details that make the subordinate classes different from each\nother. We note that existing methods implicitly address this requirement and\nleave it to a data-driven pipeline to figure out what makes a subordinate class\ndifferent from the others. This results in two major limitations: First, the\nnetwork focuses on the most obvious distinctions between classes and overlooks\nmore subtle inter-class variations. Second, the chance of misclassifying a\ngiven sample in any of the negative classes is considered equal, while in fact,\nconfusions generally occur among only the most similar classes. Here, we\npropose to explicitly force the network to find the subtle differences among\nclosely related classes. In this pursuit, we introduce two key novelties that\ncan be easily plugged into existing end-to-end deep learning pipelines. On one\nhand, we introduce diversification block which masks the most salient features\nfor an input to force the network to use more subtle cues for its correct\nclassification. Concurrently, we introduce a gradient-boosting loss function\nthat focuses only on the confusing classes for each sample and therefore moves\nswiftly along the direction on the loss surface that seeks to resolve these\nambiguities. The synergy between these two blocks helps the network to learn\nmore effective feature representations. Comprehensive experiments are performed\non five challenging datasets. Our approach outperforms existing methods using\nsimilar experimental setting on all five datasets.\n", "rewritten_text": "The key requirement for achieving accurate recognition in fine-grained tasks is to pay attention to the subtle distinguishing details that differentiate subordinate classes. Current methods tend to rely on a data-driven approach to identify these differences, which can lead to two main drawbacks. Firstly, the network may focus on obvious class distinctions and overlook more nuanced variations between classes. Secondly, misclassifications often occur between closely related classes, yet existing methods treat all negative classes equally. To address these issues, we propose a method that explicitly guides the network to identify subtle differences among closely related classes. Our approach introduces two innovative components that can be seamlessly integrated into existing deep learning pipelines. Firstly, a diversification block masks prominent features to encourage the network to rely on more subtle cues for classification. Secondly, a gradient-boosting loss function prioritizes confusing classes for each sample, guiding the network towards resolving ambiguities efficiently. The combination of these components enhances the network's ability to learn effective feature representations. Extensive experiments conducted on five challenging datasets demonstrate that our approach surpasses existing methods under similar experimental conditions across all five datasets."}, "1310.7447": {"original_text": "  A new method for removing impulse noise from speech in the wavelet transform\ndomain is proposed. The method utilizes the multiresolution property of the\nwavelet transform, which provides finer time resolution at the higher\nfrequencies than the short-time Fourier transform (STFT), to effectively\nidentify and remove impulse noise. It uses two features of speech to\ndiscriminate speech from impulse noise: one is the slow time-varying nature of\nspeech and the other is the Lipschitz regularity of the speech components. On\nthe basis of these features, an algorithm has been developed to identify and\nsuppress wavelet coefficients that correspond to impulse noise. Experiment\nresults show that the new method is able to significantly reduce impulse noise\nwithout degrading the quality of the speech signal or introducing any audible\nartifacts.\n", "rewritten_text": "A novel approach is proposed for eliminating impulse noise from speech using the wavelet transform domain. This method leverages the multiresolution characteristic of the wavelet transform, which offers enhanced time resolution at higher frequencies compared to the short-time Fourier transform (STFT). By exploiting the slow time-varying nature of speech and the Lipschitz regularity of speech components, the method effectively distinguishes speech from impulse noise. An algorithm has been developed based on these characteristics to detect and eliminate wavelet coefficients associated with impulse noise. Experimental results demonstrate that this new method significantly reduces impulse noise without compromising the quality of the speech signal or introducing any noticeable artifacts."}, "2108.06771": {"original_text": "  Quantification of uncertainty in deep-neural-networks (DNN) based image\nregistration algorithms plays a critical role in the deployment of image\nregistration algorithms for clinical applications such as surgical planning,\nintraoperative guidance, and longitudinal monitoring of disease progression or\ntreatment efficacy as well as in research-oriented processing pipelines.\nCurrently available approaches for uncertainty estimation in DNN-based image\nregistration algorithms may result in sub-optimal clinical decision making due\nto potentially inaccurate estimation of the uncertainty of the registration\nstems for the assumed parametric distribution of the registration latent space.\nWe introduce NPBDREG, a fully non-parametric Bayesian framework for uncertainty\nestimation in DNN-based deformable image registration by combining an Adam\noptimizer with stochastic gradient Langevin dynamics (SGLD) to characterize the\nunderlying posterior distribution through posterior sampling. Thus, it has the\npotential to provide uncertainty estimates that are highly correlated with the\npresence of out of distribution data. We demonstrated the added-value of\nNPBDREG, compared to the baseline probabilistic VoxelMorph model (PrVXM), on\nbrain MRI image registration using $390$ image pairs from four publicly\navailable databases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a\nbetter correlation of the predicted uncertainty with out-of-distribution data\n($r>0.95$ vs. $r<0.5$) as well as a 7.3%improvement in the registration\naccuracy (Dice score, $0.74$ vs. $0.69$, $p \\ll 0.01$), and 18% improvement in\nregistration smoothness (percentage of folds in the deformation field, 0.014\nvs. 0.017, $p \\ll 0.01$). Finally, NPBDREG demonstrated a better generalization\ncapability for data corrupted by a mixed structure noise (Dice score of $0.73$\nvs. $0.69$, $p \\ll 0.01$) compared to the baseline PrVXM approach.\n", "rewritten_text": "The quantification of uncertainty in deep neural network (DNN) based image registration algorithms is crucial for their application in clinical settings such as surgical planning, intraoperative guidance, disease monitoring, and research pipelines. Existing methods for estimating uncertainty in DNN-based image registration algorithms may lead to suboptimal clinical decisions due to potentially inaccurate estimation of registration uncertainty based on assumed parametric distributions. We introduce NPBDREG, a non-parametric Bayesian framework for uncertainty estimation in DNN-based deformable image registration. NPBDREG combines an Adam optimizer with stochastic gradient Langevin dynamics (SGLD) to sample the posterior distribution, offering potential for uncertainty estimates that align closely with out-of-distribution data. In a study comparing NPBDREG to the baseline probabilistic VoxelMorph model (PrVXM) using brain MRI image registration with 390 image pairs from various databases, NPBDREG demonstrated superior correlation of predicted uncertainty with out-of-distribution data (r>0.95 vs. r<0.5), 7.3% improvement in registration accuracy (Dice score 0.74 vs. 0.69, p < 0.01), and 18% improvement in registration smoothness (percentage of folds in the deformation field 0.014 vs. 0.017, p < 0.01). NPBDREG also exhibited better generalization for data affected by mixed structural noise (Dice score 0.73 vs. 0.69, p < 0.01) compared to the PrVXM approach."}, "2308.13678": {"original_text": "  Reconstructing and tracking deformable surface with little or no texture has\nposed long-standing challenges. Fundamentally, the challenges stem from\ntextureless surfaces lacking features for establishing cross-image\ncorrespondences. In this work, we present a novel type of markers to\nproactively enrich the object's surface features, and thereby ease the 3D\nsurface reconstruction and correspondence tracking. Our markers are made of\nfluorescent dyes, visible only under the ultraviolet (UV) light and invisible\nunder regular lighting condition. Leveraging the markers, we design a\nmulti-camera system that captures surface deformation under the UV light and\nthe visible light in a time multiplexing fashion. Under the UV light, markers\non the object emerge to enrich its surface texture, allowing high-quality 3D\nshape reconstruction and tracking. Under the visible light, markers become\ninvisible, allowing us to capture the object's original untouched appearance.\nWe perform experiments on various challenging scenes, including hand gestures,\nfacial expressions, waving cloth, and hand-object interaction. In all these\ncases, we demonstrate that our system is able to produce robust, high-quality\n3D reconstruction and tracking.\n", "rewritten_text": "\"Addressing the longstanding challenges of reconstructing and tracking deformable surfaces with minimal or no texture has been difficult. The main issue arises from the lack of distinctive features on textureless surfaces, making it hard to establish connections between images. This study introduces a new type of markers to enhance the surface features of objects, making 3D surface reconstruction and tracking easier. These markers are created using fluorescent dyes that are visible only under ultraviolet (UV) light and invisible under normal lighting conditions. By utilizing these markers, a multi-camera system is developed to capture surface deformations under both UV light and visible light in a time-sharing manner. The markers enhance the surface texture under UV light, enabling accurate 3D shape reconstruction and tracking, while remaining invisible under visible light to preserve the object's original appearance. Experimental tests on various challenging scenarios, such as hand gestures, facial expressions, waving cloth, and hand-object interactions, demonstrate the system's ability to achieve reliable and high-quality 3D reconstruction and tracking.\""}, "2408.12100": {"original_text": "  In recent years Plug-and-Play (PnP) methods have achieved state-of-the-art\nperformance in inverse imaging problems by replacing proximal operators with\ndenoisers. Based on the proximal gradient method, some theoretical results of\nPnP have appeared, where appropriate step size is crucial for convergence\nanalysis. However, in practical applications, applying PnP methods with\ntheoretically guaranteed step sizes is difficult, and these algorithms are\nlimited to Gaussian noise. In this paper,from a perspective of split convex\nfeasibility problems (SCFP), an adaptive PnP algorithm with Projected Landweber\nOperator (PnP-PLO) is proposed to address these issues. Numerical experiments\non image deblurring, super-resolution, and compressed sensing MRI experiments\nillustrate that PnP-PLO with theoretical guarantees outperforms\nstate-of-the-art methods such as RED and RED-PRO.\n", "rewritten_text": "\"In recent years, Plug-and-Play (PnP) techniques have shown cutting-edge performance in solving inverse imaging problems by substituting proximal operators with denoisers. Some theoretical advancements in PnP, based on the proximal gradient method, have emphasized the importance of selecting an appropriate step size for convergence analysis. However, in practical scenarios, implementing PnP methods with theoretically optimal step sizes can be challenging, and these algorithms are typically limited to handling Gaussian noise. This paper introduces an adaptive PnP algorithm utilizing the Projected Landweber Operator (PnP-PLO) from the perspective of split convex feasibility problems (SCFP) to tackle these challenges. Experimental results in image deblurring, super-resolution, and compressed sensing MRI demonstrate that PnP-PLO, with its theoretical guarantees, outperforms existing methods like RED and RED-PRO.\""}, "2210.13077": {"original_text": "  Novel-view synthesis (NVS) can be tackled through different approaches,\ndepending on the general setting: a single source image to a short video\nsequence, exact or noisy camera pose information, 3D-based information such as\npoint clouds etc. The most challenging scenario, the one where we stand in this\nwork, only considers a unique source image to generate a novel one from another\nviewpoint. However, in such a tricky situation, the latest learning-based\nsolutions often struggle to integrate the camera viewpoint transformation.\nIndeed, the extrinsic information is often passed as-is, through a\nlow-dimensional vector. It might even occur that such a camera pose, when\nparametrized as Euler angles, is quantized through a one-hot representation.\nThis vanilla encoding choice prevents the learnt architecture from inferring\nnovel views on a continuous basis (from a camera pose perspective). We claim it\nexists an elegant way to better encode relative camera pose, by leveraging\n3D-related concepts such as the epipolar constraint. We, therefore, introduce\nan innovative method that encodes the viewpoint transformation as a 2D feature\nimage. Such a camera encoding strategy gives meaningful insights to the network\nregarding how the camera has moved in space between the two views. By encoding\nthe camera pose information as a finite number of coloured epipolar lines, we\ndemonstrate through our experiments that our strategy outperforms vanilla\nencoding.\n", "rewritten_text": "The synthesis of novel views can be approached in various ways, depending on the specific circumstances. This includes scenarios involving a single source image transforming into a short video sequence, precise or noisy camera pose data, and 3D information like point clouds. Our focus in this study is on the most challenging scenario, where only one source image is used to create a new view from a different perspective. Existing learning-based solutions often struggle to incorporate the camera viewpoint transformation effectively in such situations. Typically, extrinsic information is passed in a simplistic manner, potentially leading to limitations in generating continuous novel views based on camera pose. To address this, we propose a novel method that encodes the viewpoint transformation as a 2D feature image, leveraging concepts like the epipolar constraint. This innovative camera encoding approach provides valuable insights to the network on how the camera has moved between views. By representing camera pose information as colored epipolar lines, our experiments demonstrate that our strategy surpasses traditional encoding methods."}, "2106.00184": {"original_text": "  Encouraging progress in few-shot semantic segmentation has been made by\nleveraging features learned upon base classes with sufficient training data to\nrepresent novel classes with few-shot examples. However, this feature sharing\nmechanism inevitably causes semantic aliasing between novel classes when they\nhave similar compositions of semantic concepts. In this paper, we reformulate\nfew-shot segmentation as a semantic reconstruction problem, and convert base\nclass features into a series of basis vectors which span a class-level semantic\nspace for novel class reconstruction. By introducing contrastive loss, we\nmaximize the orthogonality of basis vectors while minimizing semantic aliasing\nbetween classes. Within the reconstructed representation space, we further\nsuppress interference from other classes by projecting query features to the\nsupport vector for precise semantic activation. Our proposed approach, referred\nto as anti-aliasing semantic reconstruction (ASR), provides a systematic yet\ninterpretable solution for few-shot learning problems. Extensive experiments on\nPASCAL VOC and MS COCO datasets show that ASR achieves strong results compared\nwith the prior works.\n", "rewritten_text": "Progress in few-shot semantic segmentation has been advanced by utilizing features learned from base classes with ample training data to represent new classes with only a few examples. However, this sharing of features can lead to semantic confusion between new classes that share similar semantic concepts. In this study, we redefine few-shot segmentation as a semantic reconstruction challenge, transforming base class features into a set of basis vectors that cover a class-level semantic space for reconstructing new classes. By incorporating a contrastive loss, we aim to maximize the independence of basis vectors while minimizing semantic confusion between classes. In the reconstructed space, we further reduce interference from other classes by mapping query features to the support vector for precise semantic activation. Our proposed method, known as anti-aliasing semantic reconstruction (ASR), offers a structured and understandable solution for few-shot learning tasks. Extensive experiments on PASCAL VOC and MS COCO datasets demonstrate that ASR outperforms previous approaches."}, "2310.00274": {"original_text": "  Africa has a very low doctor-to-patient ratio. At very busy clinics, doctors\ncould see 30+ patients per day -- a heavy patient burden compared with\ndeveloped countries -- but productivity tools such as clinical automatic speech\nrecognition (ASR) are lacking for these overworked clinicians. However,\nclinical ASR is mature, even ubiquitous, in developed nations, and\nclinician-reported performance of commercial clinical ASR systems is generally\nsatisfactory. Furthermore, the recent performance of general domain ASR is\napproaching human accuracy. However, several gaps exist. Several publications\nhave highlighted racial bias with speech-to-text algorithms and performance on\nminority accents lags significantly. To our knowledge, there is no publicly\navailable research or benchmark on accented African clinical ASR, and speech\ndata is non-existent for the majority of African accents. We release\nAfriSpeech, 200hrs of Pan-African English speech, 67,577 clips from 2,463\nunique speakers across 120 indigenous accents from 13 countries for clinical\nand general domain ASR, a benchmark test set, with publicly available\npre-trained models with SOTA performance on the AfriSpeech benchmark.\n", "rewritten_text": "Africa has a notably low doctor-to-patient ratio, with doctors in busy clinics often seeing over 30 patients per day. This patient load is much higher compared to developed countries, yet these overworked clinicians lack productivity tools like clinical automatic speech recognition (ASR). In contrast, clinical ASR is well-established in developed nations, with satisfactory performance reported by clinicians using commercial systems. While general domain ASR is nearing human accuracy, there are still gaps to address, including racial bias in speech-to-text algorithms and challenges with minority accents. Currently, there is a lack of research and data on accented African clinical ASR. To address this gap, AfriSpeech has released 200 hours of Pan-African English speech data, representing 120 indigenous accents from 13 countries. This dataset serves as a benchmark for clinical and general domain ASR, with publicly available pre-trained models that demonstrate state-of-the-art performance on the AfriSpeech benchmark."}, "2306.02351": {"original_text": "  We present the RSSOD-Bench dataset for salient object detection (SOD) in\noptical remote sensing imagery. While SOD has achieved success in natural scene\nimages with deep learning, research in SOD for remote sensing imagery (RSSOD)\nis still in its early stages. Existing RSSOD datasets have limitations in terms\nof scale, and scene categories, which make them misaligned with real-world\napplications. To address these shortcomings, we construct the RSSOD-Bench\ndataset, which contains images from four different cities in the USA. The\ndataset provides annotations for various salient object categories, such as\nbuildings, lakes, rivers, highways, bridges, aircraft, ships, athletic fields,\nand more. The salient objects in RSSOD-Bench exhibit large-scale variations,\ncluttered backgrounds, and different seasons. Unlike existing datasets,\nRSSOD-Bench offers uniform distribution across scene categories. We benchmark\n23 different state-of-the-art approaches from both the computer vision and\nremote sensing communities. Experimental results demonstrate that more research\nefforts are required for the RSSOD task.\n", "rewritten_text": "We introduce the RSSOD-Bench dataset for salient object detection (SOD) in optical remote sensing imagery. While SOD has been successful in natural scene images using deep learning, research in SOD for remote sensing imagery (RSSOD) is still in its early stages. Current RSSOD datasets have limitations in terms of scale and scene categories, which do not align well with real-world applications. To address these issues, we have created the RSSOD-Bench dataset, featuring images from four different cities in the USA. This dataset includes annotations for various salient object categories such as buildings, lakes, rivers, highways, bridges, aircraft, ships, athletic fields, and more. The salient objects in RSSOD-Bench exhibit diverse characteristics including large-scale variations, cluttered backgrounds, and seasonal changes. Unlike existing datasets, RSSOD-Bench provides a balanced distribution across scene categories. We evaluate 23 different cutting-edge approaches from both computer vision and remote sensing fields. Our experimental results highlight the need for further research efforts in the RSSOD domain."}, "2303.12946": {"original_text": "  We have made a dataset of camouflage object detection mainly for complex\nseabed scenes, and named it UnderWater RGB&Sonar,or UW-RS for short. The UW-RS\ndataset contains a total of 1972 image data. The dataset mainly consists of two\nparts, namely underwater optical data part (UW-R dataset) and underwater sonar\ndata part (UW-S dataset).\n", "rewritten_text": "We have created a dataset focused on detecting camouflaged objects in intricate seabed environments, which we named UnderWater RGB&Sonar (UW-RS). The UW-RS dataset comprises 1972 image data entries and is divided into two main sections: the underwater optical data part (UW-R dataset) and the underwater sonar data part (UW-S dataset)."}, "2308.02738": {"original_text": "  Recently, visual-language learning has shown great potential in enhancing\nvisual-based person re-identification (ReID). Existing visual-language\nlearning-based ReID methods often focus on whole-body scale image-text feature\nalignment, while neglecting supervisions on fine-grained part features. This\nchoice simplifies the learning process but cannot guarantee within-part feature\nsemantic consistency thus hindering the final performance. Therefore, we\npropose to enhance fine-grained visual features with part-informed language\nsupervision for ReID tasks. The proposed method, named Part-Informed\nVisual-language Learning ($\\pi$-VL), suggests that (i) a human parsing-guided\nprompt tuning strategy and (ii) a hierarchical fusion-based visual-language\nalignment paradigm play essential roles in ensuring within-part feature\nsemantic consistency. Specifically, we combine both identity labels and parsing\nmaps to constitute pixel-level text prompts and fuse multi-stage visual\nfeatures with a light-weight auxiliary head to perform fine-grained image-text\nalignment. As a plug-and-play and inference-free solution, our $\\pi$-VL\nachieves substantial improvements over previous state-of-the-arts on four\ncommon-used ReID benchmarks, especially reporting 90.3% Rank-1 and 76.5% mAP\nfor the most challenging MSMT17 database without bells and whistles.\n", "rewritten_text": "Recently, there has been significant progress in utilizing visual-language learning to enhance visual-based person re-identification (ReID). While existing methods in this area typically focus on aligning image-text features at a whole-body scale, they often overlook supervising fine-grained part features. This simplification in the learning process can lead to inconsistencies in within-part feature semantics, ultimately impacting performance. To address this issue, we propose a method called Part-Informed Visual-language Learning ($\\pi$-VL) that leverages part-informed language supervision to enhance fine-grained visual features for ReID tasks. Our approach emphasizes a human parsing-guided prompt tuning strategy and a hierarchical fusion-based visual-language alignment paradigm to ensure semantic consistency within part features. By combining identity labels and parsing maps to create pixel-level text prompts and fusing multi-stage visual features with a lightweight auxiliary head for fine-grained image-text alignment, our $\\pi$-VL method outperforms previous state-of-the-art approaches on four commonly used ReID benchmarks. Notably, it achieves impressive results, including 90.3% Rank-1 accuracy and 76.5% mean Average Precision (mAP) on the challenging MSMT17 database without additional complexities."}, "1812.05785": {"original_text": "  It is prohibitively expensive to annotate a large-scale video-based person\nre-identification (re-ID) dataset, which makes fully supervised methods\ninapplicable to real-world deployment. How to maximally reduce the annotation\ncost while retaining the re-ID performance becomes an interesting problem. In\nthis paper, we address this problem by integrating an active learning scheme\ninto a deep learning framework. Noticing that the truly matched tracklet-pairs,\nalso denoted as true positives (TP), are the most informative samples for our\nre-ID model, we propose a sampling criterion to choose the most TP-likely\ntracklet-pairs for annotation. A view-aware sampling strategy considering\nview-specific biases is designed to facilitate candidate selection, followed by\nan adaptive resampling step to leave out the selected candidates that are\nunnecessary to annotate. Our method learns the re-ID model and updates the\nannotation set iteratively. The re-ID model is supervised by the tracklets'\npesudo labels that are initialized by treating each tracklet as a distinct\nclass. With the gained annotations of the actively selected candidates, the\ntracklets' pesudo labels are updated by label merging and further used to\nre-train our re-ID model. While being simple, the proposed method demonstrates\nits effectiveness on three video-based person re-ID datasets. Experimental\nresults show that less than 3\\% pairwise annotations are needed for our method\nto reach comparable performance with the fully-supervised setting.\n", "rewritten_text": "Annotating a large-scale video-based person re-identification (re-ID) dataset is extremely costly, rendering fully supervised methods impractical for real-world deployment. Addressing this challenge, we propose integrating an active learning scheme into a deep learning framework to minimize annotation expenses while maintaining re-ID performance. By prioritizing truly matched tracklet-pairs (true positives, TP) as the most informative samples, we introduce a sampling criterion to select the most TP-likely tracklet-pairs for annotation. Our approach includes a view-aware sampling strategy to account for view-specific biases and an adaptive resampling step to exclude unnecessary candidates. The re-ID model is trained iteratively using pseudo labels assigned to tracklets, with annotations from actively selected candidates updating these labels through merging and retraining. Despite its simplicity, our method proves effective on three video-based person re-ID datasets, achieving comparable performance to fully supervised methods with less than 3% of pairwise annotations required."}, "2202.05457": {"original_text": "  Sentiment Analysis typically refers to using natural language processing,\ntext analysis and computational linguistics to extract affect and emotion based\ninformation from text data. Our work explores how we can effectively use deep\nneural networks in transfer learning and joint dual input learning settings to\neffectively classify sentiments and detect hate speech in Hindi and Bengali\ndata. We start by training Word2Vec word embeddings for Hindi \\textbf{HASOC\ndataset} and Bengali hate speech and then train LSTM and subsequently, employ\nparameter sharing based transfer learning to Bengali sentiment classifiers by\nreusing and fine-tuning the trained weights of Hindi classifiers with both\nclassifier being used as baseline in our study. Finally, we use BiLSTM with\nself attention in joint dual input learning setting where we train a single\nneural network on Hindi and Bengali dataset simultaneously using their\nrespective embeddings.\n", "rewritten_text": "Sentiment Analysis involves utilizing natural language processing, text analysis, and computational linguistics to extract emotional information from text data. Our research focuses on the effective use of deep neural networks in transfer learning and joint dual input learning scenarios to classify sentiments and identify hate speech in Hindi and Bengali text. We begin by training Word2Vec word embeddings for the Hindi HASOC dataset and Bengali hate speech. We then train LSTM models and apply parameter sharing-based transfer learning to Bengali sentiment classifiers by fine-tuning the weights of Hindi classifiers. Additionally, we employ BiLSTM with self-attention in a joint dual input learning setup to train a single neural network on both Hindi and Bengali datasets simultaneously using their respective embeddings."}, "1210.0115": {"original_text": "  A framework of demosaicing and superresolution for color filter array (CFA)\nvia residual image reconstruction and sparse representation is presented.Given\nthe intermediate image produced by certain demosaicing and interpolation\ntechnique, a residual image between the final reconstruction image and the\nintermediate image is reconstructed using sparse representation.The final\nreconstruction image has richer edges and details than that of the intermediate\nimage. Specifically, a generic dictionary is learned from a large set of\ncomposite training data composed of intermediate data and residual data. The\nlearned dictionary implies a mapping between the two data. A specific\ndictionary adaptive to the input CFA is learned thereafter. Using the adaptive\ndictionary, the sparse coefficients of intermediate data are computed and\ntransformed to predict residual image. The residual image is added back into\nthe intermediate image to obtain the final reconstruction image. Experimental\nresults demonstrate the state-of-the-art performance in terms of PSNR and\nsubjective visual perception.\n", "rewritten_text": "The text introduces a method for enhancing image quality in color filter array (CFA) processing through demosaicing and superresolution. It involves reconstructing a residual image from an intermediate image using sparse representation techniques. By learning dictionaries from training data, the method enhances edges and details in the final reconstruction image compared to the intermediate image. Experimental results show superior performance in terms of both PSNR and visual perception."}, "2406.01300": {"original_text": "  Text-guided image generation enables the creation of visual content from\ntextual descriptions. However, certain visual concepts cannot be effectively\nconveyed through language alone. This has sparked a renewed interest in\nutilizing the CLIP image embedding space for more visually-oriented tasks\nthrough methods such as IP-Adapter. Interestingly, the CLIP image embedding\nspace has been shown to be semantically meaningful, where linear operations\nwithin this space yield semantically meaningful results. Yet, the specific\nmeaning of these operations can vary unpredictably across different images. To\nharness this potential, we introduce pOps, a framework that trains specific\nsemantic operators directly on CLIP image embeddings. Each pOps operator is\nbuilt upon a pretrained Diffusion Prior model. While the Diffusion Prior model\nwas originally trained to map between text embeddings and image embeddings, we\ndemonstrate that it can be tuned to accommodate new input conditions, resulting\nin a diffusion operator. Working directly over image embeddings not only\nimproves our ability to learn semantic operations but also allows us to\ndirectly use a textual CLIP loss as an additional supervision when needed. We\nshow that pOps can be used to learn a variety of photo-inspired operators with\ndistinct semantic meanings, highlighting the semantic diversity and potential\nof our proposed approach.\n", "rewritten_text": "The generation of visual content from text descriptions, known as text-guided image generation, has limitations in effectively conveying certain visual concepts through language alone. This has led to a growing interest in leveraging the CLIP image embedding space for tasks focused on visual content, such as with the IP-Adapter method. The CLIP image embedding space has been found to have semantic meaning, with linear operations within it producing meaningful results. However, the interpretation of these operations can vary unpredictably across different images. To leverage this potential, a framework called pOps is introduced to train specific semantic operators directly on CLIP image embeddings. Each pOps operator is based on a pretrained Diffusion Prior model, originally designed to map between text and image embeddings but adaptable to new input conditions to create a diffusion operator. By working directly with image embeddings, pOps enhances the ability to learn semantic operations and allows for the use of a textual CLIP loss for additional supervision when necessary. The study demonstrates that pOps can be used to learn various photo-inspired operators with distinct semantic meanings, showcasing the semantic diversity and potential of this approach."}, "1805.07477": {"original_text": "  Augmenting neural networks with skip connections, as introduced in the\nso-called ResNet architecture, surprised the community by enabling the training\nof networks of more than 1,000 layers with significant performance gains. This\npaper deciphers ResNet by analyzing the effect of skip connections, and puts\nforward new theoretical results on the advantages of identity skip connections\nin neural networks. We prove that the skip connections in the residual blocks\nfacilitate preserving the norm of the gradient, and lead to stable\nback-propagation, which is desirable from optimization perspective. We also\nshow that, perhaps surprisingly, as more residual blocks are stacked, the\nnorm-preservation of the network is enhanced. Our theoretical arguments are\nsupported by extensive empirical evidence. Can we push for extra\nnorm-preservation? We answer this question by proposing an efficient method to\nregularize the singular values of the convolution operator and making the\nResNet's transition layers extra norm-preserving. Our numerical investigations\ndemonstrate that the learning dynamics and the classification performance of\nResNet can be improved by making it even more norm preserving. Our results and\nthe introduced modification for ResNet, referred to as Procrustes ResNets, can\nbe used as a guide for training deeper networks and can also inspire new deeper\narchitectures.\n", "rewritten_text": "The ResNet architecture, known for its skip connections, revolutionized neural networks by allowing the training of networks with over 1,000 layers and achieving significant performance improvements. This study delves into ResNet's skip connections, revealing the benefits of identity skip connections in neural networks. It demonstrates that these connections in the residual blocks help maintain gradient norms, ensuring stable back-propagation for better optimization. Surprisingly, adding more residual blocks enhances norm preservation in the network. The study is supported by empirical evidence and proposes a method to further enhance norm preservation by regularizing the singular values of the convolution operator in ResNet's transition layers. The resulting Procrustes ResNets show improved learning dynamics and classification performance, offering insights for training deeper networks and inspiring new architectures."}, "2104.08668": {"original_text": "  Communicating new research ideas involves highlighting similarities and\ndifferences with past work. Authors write fluent, often long sections to survey\nthe distinction of a new paper with related work. In this work we model\ngenerating related work sections while being cognisant of the motivation behind\nciting papers. Our content planning model generates a tree of cited papers\nbefore a surface realization model lexicalizes this skeleton. Our model\noutperforms several strong state-of-the-art summarization and multi-document\nsummarization models on generating related work on an ACL Anthology (AA) based\ndataset which we contribute.\n", "rewritten_text": "\"Presenting fresh research concepts requires emphasizing similarities and contrasts with previous studies. Authors typically write detailed sections to compare a new paper with existing work. In this study, we develop a method for creating sections on related work while considering the reasons for citing papers. Our approach involves generating a citation tree using a content planning model, followed by a surface realization model to flesh out the details. Our model surpasses various advanced summarization models in producing related work summaries on a dataset from the ACL Anthology (AA) that we have provided.\""}, "1608.08021": {"original_text": "  This paper presents how we can achieve the state-of-the-art accuracy in\nmulti-category object detection task while minimizing the computational cost by\nadapting and combining recent technical innovations. Following the common\npipeline of \"CNN feature extraction + region proposal + RoI classification\", we\nmainly redesign the feature extraction part, since region proposal part is not\ncomputationally expensive and classification part can be efficiently compressed\nwith common techniques like truncated SVD. Our design principle is \"less\nchannels with more layers\" and adoption of some building blocks including\nconcatenated ReLU, Inception, and HyperNet. The designed network is deep and\nthin and trained with the help of batch normalization, residual connections,\nand learning rate scheduling based on plateau detection. We obtained solid\nresults on well-known object detection benchmarks: 83.8% mAP (mean average\nprecision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only\n750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA\nTitan X GPU. Theoretically, our network requires only 12.3% of the\ncomputational cost compared to ResNet-101, the winner on VOC2012.\n", "rewritten_text": "This paper demonstrates how to achieve cutting-edge accuracy in multi-category object detection tasks while minimizing computational expenses through the integration and optimization of recent technological advancements. By focusing on redesigning the feature extraction component within the established pipeline of \"CNN feature extraction + region proposal + RoI classification,\" we aim to reduce computational overhead. Our approach emphasizes \"less channels with more layers\" and incorporates various building blocks such as concatenated ReLU, Inception, and HyperNet. The resulting network is characterized by its depth and slimness, trained using batch normalization, residual connections, and learning rate scheduling based on plateau detection. Our method has yielded impressive results on prominent object detection benchmarks, achieving 83.8% mAP on VOC2007 and 82.5% mAP on VOC2012 (2nd place), with processing times of 750ms/image on an Intel i7-6700K CPU and 46ms/image on an NVIDIA Titan X GPU. Notably, our network boasts a computational cost that is only 12.3% of ResNet-101, the VOC2012 champion."}, "2210.05738": {"original_text": "  In this work, we propose to explicitly use the landmarks of prostate to guide\nthe MR-TRUS image registration. We first train a deep neural network to\nautomatically localize a set of meaningful landmarks, and then directly\ngenerate the affine registration matrix from the location of these landmarks.\nFor landmark localization, instead of directly training a network to predict\nthe landmark coordinates, we propose to regress a full-resolution distance map\nof the landmark, which is demonstrated effective in avoiding statistical bias\nto unsatisfactory performance and thus improving performance. We then use the\npredicted landmarks to generate the affine transformation matrix, which\noutperforms the clinicians' manual rigid registration by a significant margin\nin terms of TRE.\n", "rewritten_text": "\"In this study, we suggest utilizing prostate landmarks to assist in MR-TRUS image registration. Our approach involves training a deep neural network to identify key landmarks, and then using this information to create an affine registration matrix. Instead of directly predicting landmark coordinates, we propose regressing a high-resolution distance map for more accurate results. This method helps avoid statistical bias and enhances performance compared to manual registration by clinicians, as demonstrated by improved TRE outcomes.\""}, "2006.08844": {"original_text": "  We tackle the problem of establishing dense pixel-wise correspondences\nbetween a pair of images. In this work, we introduce Dual-Resolution\nCorrespondence Networks (DualRC-Net), to obtain pixel-wise correspondences in a\ncoarse-to-fine manner. DualRC-Net extracts both coarse- and fine- resolution\nfeature maps. The coarse maps are used to produce a full but coarse 4D\ncorrelation tensor, which is then refined by a learnable neighbourhood\nconsensus module. The fine-resolution feature maps are used to obtain the final\ndense correspondences guided by the refined coarse 4D correlation tensor. The\nselected coarse-resolution matching scores allow the fine-resolution features\nto focus only on a limited number of possible matches with high confidence. In\nthis way, DualRC-Net dramatically increases matching reliability and\nlocalisation accuracy, while avoiding to apply the expensive 4D convolution\nkernels on fine-resolution feature maps. We comprehensively evaluate our method\non large-scale public benchmarks including HPatches, InLoc, and Aachen\nDay-Night. It achieves the state-of-the-art results on all of them.\n", "rewritten_text": "We address the challenge of establishing dense pixel-wise correspondences between a pair of images. In this study, we present Dual-Resolution Correspondence Networks (DualRC-Net), which aims to achieve pixel-wise correspondences in a step-by-step manner from coarse to fine. DualRC-Net generates both coarse and fine-resolution feature maps. The coarse maps are utilized to create a complete but rough 4D correlation tensor, which is then enhanced by a trainable neighborhood consensus module. The fine-resolution feature maps are employed to derive the final dense correspondences based on the refined coarse 4D correlation tensor. By leveraging the selected coarse-resolution matching scores, the fine-resolution features concentrate on a limited number of potential matches with high confidence. This approach significantly enhances matching reliability and localization accuracy of DualRC-Net, without the need for costly 4D convolution kernels on fine-resolution feature maps. Our method is extensively evaluated on prominent public datasets such as HPatches, InLoc, and Aachen Day-Night, achieving state-of-the-art performance across all benchmarks."}, "1704.00405": {"original_text": "  As for semantic role labeling (SRL) task, when it comes to utilizing parsing\ninformation, both traditional methods and recent recurrent neural network (RNN)\nbased methods use the feature engineering way. In this paper, we propose Syntax\nAware Long Short Time Memory(SA-LSTM). The structure of SA-LSTM modifies\naccording to dependency parsing information in order to model parsing\ninformation directly in an architecture engineering way instead of feature\nengineering way. We experimentally demonstrate that SA-LSTM gains more\nimprovement from the model architecture. Furthermore, SA-LSTM outperforms the\nstate-of-the-art on CPB 1.0 significantly according to Student t-test\n($p<0.05$).\n", "rewritten_text": "In the context of semantic role labeling (SRL), both traditional methods and recent recurrent neural network (RNN) approaches typically rely on feature engineering when incorporating parsing information. This paper introduces Syntax Aware Long Short Time Memory (SA-LSTM), which adapts its structure based on dependency parsing details to directly integrate parsing information into the architecture rather than through feature engineering. Our experimental results show that SA-LSTM benefits significantly from this architectural approach, surpassing the current state-of-the-art performance on CPB 1.0 with statistical significance ($p<0.05$) as confirmed by Student t-test."}, "2304.08481": {"original_text": "  High-definition (HD) semantic maps are crucial in enabling autonomous\nvehicles to navigate urban environments. The traditional method of creating\noffline HD maps involves labor-intensive manual annotation processes, which are\nnot only costly but also insufficient for timely updates. Recent studies have\nproposed an alternative approach that generates local maps using online sensor\nobservations. However, this approach is limited by the sensor's perception\nrange and its susceptibility to occlusions. In this study, we propose Neural\nMap Prior (NMP), a neural representation of global maps. This representation\nautomatically updates itself and improves the performance of local map\ninference. Specifically, we utilize two approaches to achieve this. Firstly, to\nintegrate a strong map prior into local map inference, we apply\ncross-attention, a mechanism that dynamically identifies correlations between\ncurrent and prior features. Secondly, to update the global neural map prior, we\nutilize a learning-based fusion module that guides the network in fusing\nfeatures from previous traversals. Our experimental results, based on the\nnuScenes dataset, demonstrate that our framework is highly compatible with\nvarious map segmentation and detection architectures. It significantly improves\nmap prediction performance, even in challenging weather conditions and\nsituations with a longer perception range. To the best of our knowledge, this\nis the first learning-based system for creating a global map prior.\n", "rewritten_text": "\"High-definition semantic maps in HD are essential for autonomous vehicles to navigate urban areas. The traditional method of creating offline HD maps involves manual annotation processes that are labor-intensive and costly, making timely updates challenging. Recent studies suggest an alternative approach of generating local maps using online sensor observations. However, this method has limitations due to the sensor's range and susceptibility to occlusions. This study introduces Neural Map Prior (NMP), a neural representation of global maps that automatically updates itself and enhances local map inference performance. Two key approaches are utilized: integrating a strong map prior into local map inference through cross-attention, which identifies correlations between current and prior features dynamically, and updating the global neural map prior using a learning-based fusion module that guides the network in fusing features from previous traversals. Experimental results on the nuScenes dataset show that this framework is compatible with various map segmentation and detection architectures, significantly improving map prediction performance even in challenging weather conditions and situations with an extended perception range. This is believed to be the first learning-based system for creating a global map prior.\""}, "2202.10108": {"original_text": "  Vision transformers have shown great potential in various computer vision\ntasks owing to their strong capability to model long-range dependency using the\nself-attention mechanism. Nevertheless, they treat an image as a 1D sequence of\nvisual tokens, lacking an intrinsic inductive bias (IB) in modeling local\nvisual structures and dealing with scale variance, which is instead learned\nimplicitly from large-scale training data with longer training schedules. In\nthis paper, we propose a Vision Transformer Advanced by Exploring intrinsic IB\nfrom convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid\nreduction modules to downsample and embed the input image into tokens with rich\nmulti-scale context using multiple convolutions with different dilation rates.\nIn this way, it acquires an intrinsic scale invariance IB and can learn robust\nfeature representation for objects at various scales. Moreover, in each\ntransformer layer, ViTAE has a convolution block parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. The proposed two kinds\nof cells are stacked in both isotropic and multi-stage manners to formulate two\nfamilies of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on\nthe ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and\nAP10K datasets validate the superiority of our models over the baseline\ntransformer models and concurrent works. Besides, we scale up our ViTAE model\nto 644M parameters and obtain the state-of-the-art classification performance,\ni.e., 88.5% Top-1 classification accuracy on ImageNet validation set and the\nbest 91.2% Top-1 accuracy on ImageNet real validation set, without using extra\nprivate data.\n", "rewritten_text": "Vision transformers have demonstrated significant potential in a variety of computer vision tasks due to their ability to model long-range dependencies effectively using the self-attention mechanism. However, they view an image as a 1D sequence of visual tokens, lacking an inherent inductive bias (IB) in capturing local visual structures and addressing scale variance, which is instead learned implicitly from extensive training data with extended training periods. In this study, we introduce a Vision Transformer Advanced by Exploring intrinsic IB from convolutions, known as ViTAE. Technically, ViTAE incorporates several spatial pyramid reduction modules to downsample and encode the input image into tokens with diverse multi-scale context using multiple convolutions with varying dilation rates. This approach enables ViTAE to acquire an inherent scale invariance IB and develop robust feature representations for objects at different scales. Additionally, within each transformer layer, ViTAE includes a convolution block running in parallel with the multi-head self-attention module, with their features combined and passed to the feed-forward network. As a result, ViTAE possesses an intrinsic locality IB and can effectively learn both local features and global dependencies in a collaborative manner. The proposed two types of cells are stacked in isotropic and multi-stage configurations to create two ViTAE model families: the vanilla ViTAE and ViTAEv2. Experimental results on the ImageNet dataset and downstream tasks on the MS COCO, ADE20K, and AP10K datasets confirm the superior performance of our models compared to baseline transformer models and other contemporary approaches. Furthermore, we scale up our ViTAE model to 644M parameters and achieve state-of-the-art classification results, specifically 88.5% Top-1 classification accuracy on the ImageNet validation set and the highest 91.2% Top-1 accuracy on the ImageNet real validation set, without the use of additional private data."}, "1809.08371": {"original_text": "  Human pose estimation is an essential yet challenging task in computer\nvision. One of the reasons for this difficulty is that there are many redundant\nregions in the images. In this work, we proposed a convolutional network\narchitecture combined with the novel attention model. We named it attention\nconvolutional neural network (ACNN). ACNN learns to focus on specific regions\nof different input features. It's a multi-stage architecture. Early stages\nfiltrate the \"nothing-regions\", such as background and redundant body parts.\nAnd then, they submit the important regions which contain the joints of the\nhuman body to the following stages to get a more accurate result. What's more,\nit does not require extra manual annotations and self-learning is one of our\nintentions. We separately trained the network because the attention learning\ntask and the pose estimation task are not independent. State-of-the-art\nperformance is obtained on the MPII benchmarks.\n", "rewritten_text": "Human pose estimation is a crucial yet challenging task in computer vision due to the presence of numerous redundant regions in images. In this study, we introduced a novel approach combining a convolutional network architecture with an attention model, known as the attention convolutional neural network (ACNN). ACNN is designed to focus on specific regions of input features through a multi-stage process. The initial stages filter out irrelevant regions, such as background and redundant body parts, while highlighting important regions containing human body joints for more accurate results in subsequent stages. Notably, ACNN does not rely on additional manual annotations and emphasizes self-learning. The network was trained separately to address the interdependence between attention learning and pose estimation tasks, resulting in state-of-the-art performance on the MPII benchmarks."}, "1310.0302": {"original_text": "  Surface registration is a technique that is used in various areas such as\nobject recognition and 3D model reconstruction. Problem of surface registration\ncan be analyzed as an optimization problem of seeking a rigid motion between\ntwo different views. Genetic algorithms can be used for solving this\noptimization problem, both for obtaining the robust parameter estimation and\nfor its fine-tuning. The main drawback of genetic algorithms is that they are\ntime consuming which makes them unsuitable for online applications. Modern\nacquisition systems enable the implementation of the solutions that would\nimmediately give the information on the rotational angles between the different\nviews, thus reducing the dimension of the optimization problem. The paper gives\nan analysis of the genetic algorithm implemented in the conditions when the\nrotation matrix is known and a comparison of these results with results when\nthis information is not available.\n", "rewritten_text": "Surface registration is a method utilized in various fields such as object recognition and 3D model reconstruction. It involves optimizing the alignment between two different views by finding a rigid motion. Genetic algorithms can be employed to solve this optimization problem, providing robust parameter estimation and refinement. However, genetic algorithms are time-consuming and may not be suitable for real-time applications. With advancements in acquisition systems, it is now possible to quickly determine rotational angles between views, simplifying the optimization process. This paper examines the use of genetic algorithms in scenarios where the rotation matrix is known, comparing the results to cases where this information is not provided."}, "2008.0194": {"original_text": "  While the progress of machine translation of written text has come far in the\npast several years thanks to the increasing availability of parallel corpora\nand corpora-based training technologies, automatic translation of spoken text\nand dialogues remains challenging even for modern systems. In this paper, we\naim to boost the machine translation quality of conversational texts by\nintroducing a newly constructed Japanese-English business conversation parallel\ncorpus. A detailed analysis of the corpus is provided along with challenging\nexamples for automatic translation. We also experiment with adding the corpus\nin a machine translation training scenario and show how the resulting system\nbenefits from its use.\n", "rewritten_text": "In recent years, advancements in machine translation for written text have been significant due to the increased availability of parallel corpora and training technologies. However, translating spoken text and dialogues remains a challenge for modern systems. This paper focuses on enhancing the quality of machine translation for conversational texts by introducing a new Japanese-English business conversation parallel corpus. The paper includes a thorough analysis of the corpus, presents difficult translation examples, and explores the impact of incorporating the corpus into machine translation training to improve system performance."}, "2204.08308": {"original_text": "  With the rapid development of multimedia technology, Augmented Reality (AR)\nhas become a promising next-generation mobile platform. The primary theory\nunderlying AR is human visual confusion, which allows users to perceive the\nreal-world scenes and augmented contents (virtual-world scenes) simultaneously\nby superimposing them together. To achieve good Quality of Experience (QoE), it\nis important to understand the interaction between two scenarios, and\nharmoniously display AR contents. However, studies on how this superimposition\nwill influence the human visual attention are lacking. Therefore, in this\npaper, we mainly analyze the interaction effect between background (BG) scenes\nand AR contents, and study the saliency prediction problem in AR. Specifically,\nwe first construct a Saliency in AR Dataset (SARD), which contains 450 BG\nimages, 450 AR images, as well as 1350 superimposed images generated by\nsuperimposing BG and AR images in pair with three mixing levels. A large-scale\neye-tracking experiment among 60 subjects is conducted to collect eye movement\ndata. To better predict the saliency in AR, we propose a vector quantized\nsaliency prediction method and generalize it for AR saliency prediction. For\ncomparison, three benchmark methods are proposed and evaluated together with\nour proposed method on our SARD. Experimental results demonstrate the\nsuperiority of our proposed method on both of the common saliency prediction\nproblem and the AR saliency prediction problem over benchmark methods. Our\ndataset and code are available at: https://github.com/DuanHuiyu/ARSaliency.\n", "rewritten_text": "Due to the rapid advancement of multimedia technology, Augmented Reality (AR) has emerged as a promising mobile platform for the future. The fundamental principle behind AR is the manipulation of human visual perception, enabling users to simultaneously view real-world scenes and virtual content by overlaying them. To ensure a high Quality of Experience (QoE), it is crucial to comprehend the interaction between these two scenarios and seamlessly present AR content. However, there is a lack of research on how this overlay affects human visual attention. Therefore, this study focuses on analyzing the interaction between background (BG) scenes and AR content, investigating the saliency prediction issue in AR. The study introduces a Saliency in AR Dataset (SARD) comprising 450 BG images, 450 AR images, and 1350 superimposed images created by combining BG and AR images at three mixing levels. An extensive eye-tracking experiment involving 60 participants was conducted to gather eye movement data. To enhance saliency prediction in AR, a vector quantized saliency prediction method is proposed and adapted for AR saliency prediction. Three benchmark methods are also introduced and evaluated alongside the proposed method using the SARD. Results from experiments demonstrate the effectiveness of the proposed method in addressing both general saliency prediction and AR saliency prediction challenges compared to benchmark methods. The dataset and code are accessible at: https://github.com/DuanHuiyu/ARSaliency."}, "2210.16865": {"original_text": "  Explicit decomposition modeling, which involves breaking down complex tasks\ninto more straightforward and often more interpretable sub-tasks, has long been\na central theme in developing robust and interpretable NLU systems. However,\ndespite the many datasets and resources built as part of this effort, the\nmajority have small-scale annotations and limited scope, which is insufficient\nto solve general decomposition tasks. In this paper, we look at large-scale\nintermediate pre-training of decomposition-based transformers using distant\nsupervision from comparable texts, particularly large-scale parallel news. We\nshow that with such intermediate pre-training, developing robust\ndecomposition-based models for a diverse range of tasks becomes more feasible.\nFor example, on semantic parsing, our model, DecompT5, improves 20% to 30% on\ntwo datasets, Overnight and TORQUE, over the baseline language model. We\nfurther use DecompT5 to build a novel decomposition-based QA system named\nDecompEntail, improving over state-of-the-art models, including GPT-3, on both\nHotpotQA and StrategyQA by 8% and 4%, respectively.\n", "rewritten_text": "The practice of explicit decomposition modeling, which involves breaking down complex tasks into simpler sub-tasks, has been a key focus in the development of robust and interpretable NLU systems. Despite the creation of numerous datasets and resources for this purpose, many have limited annotations and scope, making them inadequate for solving general decomposition tasks. This paper explores the use of large-scale intermediate pre-training for decomposition-based transformers, utilizing distant supervision from comparable texts such as large-scale parallel news. The study demonstrates that with this intermediate pre-training, it becomes more feasible to develop robust decomposition-based models for a variety of tasks. For instance, our model, DecompT5, shows significant improvements of 20% to 30% on semantic parsing datasets Overnight and TORQUE compared to the baseline language model. Additionally, we introduce a novel decomposition-based QA system called DecompEntail, which outperforms state-of-the-art models, including GPT-3, by 8% and 4% on HotpotQA and StrategyQA, respectively."}, "2103.16364": {"original_text": "  Unsupervised person re-identification (ReID) aims at learning discriminative\nidentity features without annotations. Recently, self-supervised contrastive\nlearning has gained increasing attention for its effectiveness in unsupervised\nrepresentation learning. The main idea of instance contrastive learning is to\nmatch a same instance in different augmented views. However, the relationship\nbetween different instances has not been fully explored in previous contrastive\nmethods, especially for instance-level contrastive loss. To address this issue,\nwe propose Inter-instance Contrastive Encoding (ICE) that leverages\ninter-instance pairwise similarity scores to boost previous class-level\ncontrastive ReID methods. We first use pairwise similarity ranking as one-hot\nhard pseudo labels for hard instance contrast, which aims at reducing\nintra-class variance. Then, we use similarity scores as soft pseudo labels to\nenhance the consistency between augmented and original views, which makes our\nmodel more robust to augmentation perturbations. Experiments on several\nlarge-scale person ReID datasets validate the effectiveness of our proposed\nunsupervised method ICE, which is competitive with even supervised methods.\nCode is made available at https://github.com/chenhao2345/ICE.\n", "rewritten_text": "Unsupervised person re-identification (ReID) involves learning distinctive identity features without annotations. Recently, self-supervised contrastive learning has garnered attention for its efficacy in unsupervised representation learning. The core concept of instance contrastive learning is to match the same instance in different augmented views. However, the relationship between different instances has not been thoroughly explored in previous contrastive methods, particularly for instance-level contrastive loss. To tackle this issue, we introduce Inter-instance Contrastive Encoding (ICE), which utilizes inter-instance pairwise similarity scores to enhance existing class-level contrastive ReID approaches. Initially, we employ pairwise similarity ranking as one-hot hard pseudo labels for challenging instance contrast to reduce intra-class variance. Subsequently, we utilize similarity scores as soft pseudo labels to improve consistency between augmented and original views, enhancing the model's resilience to augmentation perturbations. Experiments on various large-scale person ReID datasets confirm the effectiveness of our proposed unsupervised method ICE, which competes with even supervised methods. The code can be accessed at https://github.com/chenhao2345/ICE."}, "1607.07604": {"original_text": "  The interpretation and analysis of the wireless capsule endoscopy recording\nis a complex task which requires sophisticated computer aided decision (CAD)\nsystems in order to help physicians with the video screening and, finally, with\nthe diagnosis. Most of the CAD systems in the capsule endoscopy share a common\nsystem design, but use very different image and video representations. As a\nresult, each time a new clinical application of WCE appears, new CAD system has\nto be designed from scratch. This characteristic makes the design of new CAD\nsystems a very time consuming. Therefore, in this paper we introduce a system\nfor small intestine motility characterization, based on Deep Convolutional\nNeural Networks, which avoids the laborious step of designing specific features\nfor individual motility events. Experimental results show the superiority of\nthe learned features over alternative classifiers constructed by using state of\nthe art hand-crafted features. In particular, it reaches a mean classification\naccuracy of 96% for six intestinal motility events, outperforming the other\nclassifiers by a large margin (a 14% relative performance increase).\n", "rewritten_text": "The analysis of wireless capsule endoscopy recordings is a complex task that relies on sophisticated computer-aided decision systems to assist physicians in video screening and diagnosis. While most CAD systems for capsule endoscopy follow a similar system design, they utilize different image and video representations. This results in the need to create new CAD systems from scratch for each new clinical application of wireless capsule endoscopy, making the process time-consuming. In this study, we present a system for characterizing small intestine motility using Deep Convolutional Neural Networks, which eliminates the need to design specific features for individual motility events. Experimental results demonstrate the superior performance of the learned features compared to classifiers using traditional hand-crafted features, achieving a mean classification accuracy of 96% for six intestinal motility events and outperforming other classifiers by a significant margin (a 14% relative performance increase)."}, "2206.12262": {"original_text": "  Microblogs have become a social platform for people to express their emotions\nin real-time, and it is a trend to analyze user emotional tendencies from the\ninformation on Microblogs. The dynamic features of emojis can affect the\nsentiment polarity of microblog texts. Since existing models seldom consider\nthe diversity of emoji sentiment polarity,the paper propose a microblog\nsentiment classification model based on ALBERT-FAET. We obtain text embedding\nvia ALBERT pretraining model and learn the inter-emoji embedding with an\nattention-based LSTM network. In addition, a fine-grained attention mechanism\nis proposed to capture the word-level interactions between plain text and\nemoji. Finally, we concatenate these features and feed them into a CNN\nclassifier to predict the sentiment labels of the microblogs. To verify the\neffectiveness of the model and the fine-grained attention network, we conduct\ncomparison experiments and ablation experiments. The comparison experiments\nshow that the model outperforms previous methods in three evaluation indicators\n(accuracy, precision, and recall) and the model can significantly improve\nsentiment classification. The ablation experiments show that compared with\nALBERT-AET, the proposed model ALBERT-FAET is better in the metrics, indicating\nthat the fine-grained attention network can understand the diversified\ninformation of emoticons.\n", "rewritten_text": "Microblogs have evolved into a popular platform for individuals to share their emotions in real-time, leading to a growing interest in analyzing user emotional patterns through Microblog data. The use of emojis in Microblogs can impact the sentiment expressed in the texts. However, existing models often overlook the varying sentiment polarities of emojis. This study introduces a microblog sentiment classification model called ALBERT-FAET, which leverages text embedding from the ALBERT pretraining model and incorporates inter-emoji embedding using an attention-based LSTM network. Furthermore, a detailed attention mechanism is proposed to capture interactions between plain text and emojis at the word level. These features are combined and input into a CNN classifier to predict sentiment labels in microblogs. Experimental results demonstrate the effectiveness of the model and the fine-grained attention network, outperforming previous methods in accuracy, precision, and recall. Ablation experiments reveal that ALBERT-FAET surpasses ALBERT-AET, highlighting the superior performance of the fine-grained attention network in understanding diverse emoticon information."}, "1910.07481": {"original_text": "  In Machine Translation, considering the document as a whole can help to\nresolve ambiguities and inconsistencies. In this paper, we propose a simple yet\npromising approach to add contextual information in Neural Machine Translation.\nWe present a method to add source context that capture the whole document with\naccurate boundaries, taking every word into account. We provide this additional\ninformation to a Transformer model and study the impact of our method on three\nlanguage pairs. The proposed approach obtains promising results in the\nEnglish-German, English-French and French-English document-level translation\ntasks. We observe interesting cross-sentential behaviors where the model learns\nto use document-level information to improve translation coherence.\n", "rewritten_text": "\"In the field of Machine Translation, considering the entire document can be beneficial in resolving ambiguities and inconsistencies. This paper introduces a straightforward yet effective method for incorporating contextual information into Neural Machine Translation. Our approach involves integrating source context that encompasses the entire document with precise boundaries, taking into consideration every word. We feed this additional information into a Transformer model and evaluate its impact on translation quality across three language pairs. The proposed method shows promising results in document-level translation tasks for English-German, English-French, and French-English pairs. We observe intriguing cross-sentential behaviors where the model leverages document-level information to enhance translation coherence.\""}, "2007.02424": {"original_text": "  Recent advances in unsupervised domain adaptation for semantic segmentation\nhave shown great potentials to relieve the demand of expensive per-pixel\nannotations. However, most existing works address the domain discrepancy by\naligning the data distributions of two domains at a global image level whereas\nthe local consistencies are largely neglected. This paper presents an\ninnovative local contextual-relation consistent domain adaptation (CrCDA)\ntechnique that aims to achieve local-level consistencies during the\nglobal-level alignment. The idea is to take a closer look at region-wise\nfeature representations and align them for local-level consistencies.\nSpecifically, CrCDA learns and enforces the prototypical local\ncontextual-relations explicitly in the feature space of a labelled source\ndomain while transferring them to an unlabelled target domain via\nbackpropagation-based adversarial learning. An adaptive entropy max-min\nadversarial learning scheme is designed to optimally align these hundreds of\nlocal contextual-relations across domain without requiring discriminator or\nextra computation overhead. The proposed CrCDA has been evaluated extensively\nover two challenging domain adaptive segmentation tasks (e.g., GTA5 to\nCityscapes and SYNTHIA to Cityscapes), and experiments demonstrate its superior\nsegmentation performance as compared with state-of-the-art methods.\n", "rewritten_text": "Recent advancements in unsupervised domain adaptation for semantic segmentation have demonstrated significant potential in reducing the need for costly per-pixel annotations. However, most existing studies focus on aligning the data distributions of two domains at a global image level, overlooking local consistencies. This paper introduces a novel technique called contextual-relation consistent domain adaptation (CrCDA) that aims to achieve local-level consistencies while aligning at the global level. The approach involves examining region-wise feature representations and aligning them for local-level consistencies. Specifically, CrCDA learns and enforces prototypical local contextual-relations in the feature space of a labeled source domain and transfers them to an unlabeled target domain through backpropagation-based adversarial learning. An adaptive entropy max-min adversarial learning scheme is developed to effectively align these numerous local contextual-relations across domains without the need for a discriminator or additional computational overhead. The proposed CrCDA is extensively evaluated on two challenging domain adaptive segmentation tasks (GTA5 to Cityscapes and SYNTHIA to Cityscapes), with experiments showcasing its superior segmentation performance compared to state-of-the-art methods."}, "1805.10047": {"original_text": "  Neural machine translation (NMT) has a drawback in that can generate only\nhigh-frequency words owing to the computational costs of the softmax function\nin the output layer.\n  In Japanese-English NMT, Japanese predicate conjugation causes an increase in\nvocabulary size. For example, one verb can have as many as 19 surface\nvarieties. In this research, we focus on predicate conjugation for compressing\nthe vocabulary size in Japanese. The vocabulary list is filled with the various\nforms of verbs. We propose methods using predicate conjugation information\nwithout discarding linguistic information. The proposed methods can generate\nlow-frequency words and deal with unknown words. Two methods were considered to\nintroduce conjugation information: the first considers it as a token\n(conjugation token) and the second considers it as an embedded vector\n(conjugation feature).\n  The results using these methods demonstrate that the vocabulary size can be\ncompressed by approximately 86.1% (Tanaka corpus) and the NMT models can output\nthe words not in the training data set. Furthermore, BLEU scores improved by\n0.91 points in Japanese-to-English translation, and 0.32 points in\nEnglish-to-Japanese translation with ASPEC.\n", "rewritten_text": "Neural machine translation (NMT) faces a limitation in generating primarily high-frequency words due to the computational demands of the softmax function in the output layer. In Japanese-English NMT, the complexity of Japanese predicate conjugation leads to an expansion in vocabulary size, with a single verb potentially having up to 19 different surface forms. This study focuses on utilizing predicate conjugation to reduce the vocabulary size in Japanese without losing linguistic information. Our proposed methods leverage conjugation details to handle low-frequency and unknown words. Two approaches were explored: treating conjugation as a token (conjugation token) and as an embedded vector (conjugation feature). Results show that these methods can reduce vocabulary size by around 86.1% (Tanaka corpus) and enable NMT models to generate words not present in the training data. Additionally, BLEU scores improved by 0.91 points for Japanese-to-English translation and 0.32 points for English-to-Japanese translation using ASPEC."}, "1703.01028": {"original_text": "  Outlier detection and cluster number estimation is an important issue for\nclustering real data. This paper focuses on spectral clustering, a time-tested\nclustering method, and reveals its important properties related to outliers.\nThe highlights of this paper are the following two mathematical observations:\nfirst, spectral clustering's intrinsic property of an outlier cluster\nformation, and second, the singularity of an outlier cluster with a valid\ncluster number. Based on these observations, we designed a function that\nevaluates clustering and outlier detection results. In experiments, we prepared\ntwo scenarios, face clustering in photo album and person re-identification in a\ncamera network. We confirmed that the proposed method detects outliers and\nestimates the number of clusters properly in both problems. Our method\noutperforms state-of-the-art methods in both the 128-dimensional sparse space\nfor face clustering and the 4,096-dimensional non-sparse space for person\nre-identification.\n", "rewritten_text": "The focus of this paper is on spectral clustering, a well-established method for clustering real data, with a particular emphasis on outlier detection and estimating the number of clusters. The paper highlights two key mathematical observations: the inherent tendency of spectral clustering to form outlier clusters, and the unique nature of outlier clusters when a valid cluster number is present. A function was developed based on these observations to evaluate clustering and outlier detection results. Experimental scenarios involving face clustering in a photo album and person re-identification in a camera network were conducted, demonstrating that the proposed method effectively detects outliers and accurately estimates cluster numbers in both scenarios. The method showed superior performance compared to existing methods in both 128-dimensional sparse space for face clustering and 4,096-dimensional non-sparse space for person re-identification."}, "2402.19122": {"original_text": "  Gait recognition stands as one of the most pivotal remote identification\ntechnologies and progressively expands across research and industry\ncommunities. However, existing gait recognition methods heavily rely on\ntask-specific upstream driven by supervised learning to provide explicit gait\nrepresentations like silhouette sequences, which inevitably introduce expensive\nannotation costs and potential error accumulation. Escaping from this trend,\nthis work explores effective gait representations based on the all-purpose\nknowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a\nsimple yet efficient gait framework, termed BigGait. Specifically, the Gait\nRepresentation Extractor (GRE) within BigGait draws upon design principles from\nestablished gait representations, effectively transforming all-purpose\nknowledge into implicit gait representations without requiring third-party\nsupervision signals. Experiments on CCPG, CAISA-B* and SUSTech1K indicate that\nBigGait significantly outperforms the previous methods in both within-domain\nand cross-domain tasks in most cases, and provides a more practical paradigm\nfor learning the next-generation gait representation. Finally, we delve into\nprospective challenges and promising directions in LVMs-based gait recognition,\naiming to inspire future work in this emerging topic. The source code is\navailable at https://github.com/ShiqiYu/OpenGait.\n", "rewritten_text": "\"Gait recognition is a crucial technology for remote identification that is gaining traction in research and industry. However, current methods heavily rely on supervised learning for explicit gait representations, leading to high annotation costs and potential errors. In contrast, this study explores effective gait representations using task-agnostic Large Vision Models (LVMs) to propose a simple yet efficient framework called BigGait. The Gait Representation Extractor (GRE) in BigGait leverages principles from established gait representations to transform all-purpose knowledge into implicit gait representations without external supervision. Experimental results on CCPG, CAISA-B*, and SUSTech1K datasets show that BigGait outperforms previous methods in both within-domain and cross-domain tasks, offering a practical approach for next-generation gait representation learning. The study also discusses future challenges and directions in LVMs-based gait recognition to inspire further research in this field. The source code can be found at https://github.com/ShiqiYu/OpenGait.\""}, "1808.03959": {"original_text": "  Deep Learning based stereo matching methods have shown great successes and\nachieved top scores across different benchmarks. However, like most data-driven\nmethods, existing deep stereo matching networks suffer from some well-known\ndrawbacks such as requiring large amount of labeled training data, and that\ntheir performances are fundamentally limited by the generalization ability. In\nthis paper, we propose a novel Recurrent Neural Network (RNN) that takes a\ncontinuous (possibly previously unseen) stereo video as input, and directly\npredicts a depth-map at each frame without a pre-training process, and without\nthe need of ground-truth depth-maps as supervision. Thanks to the recurrent\nnature (provided by two convolutional-LSTM blocks), our network is able to\nmemorize and learn from its past experiences, and modify its inner parameters\n(network weights) to adapt to previously unseen or unfamiliar environments.\nThis suggests a remarkable generalization ability of the net, making it\napplicable in an {\\em open world} setting. Our method works robustly with\nchanges in scene content, image statistics, and lighting and season conditions\n{\\em etc}. By extensive experiments, we demonstrate that the proposed method\nseamlessly adapts between different scenarios. Equally important, in terms of\nthe stereo matching accuracy, it outperforms state-of-the-art deep stereo\napproaches on standard benchmark datasets such as KITTI and Middlebury stereo.\n", "rewritten_text": "\"Recent advancements in stereo matching using Deep Learning have been highly successful, consistently achieving top scores in various benchmarks. However, like many data-driven approaches, current deep stereo matching networks face common challenges such as the need for extensive labeled training data and limitations in generalization capabilities. In this study, we introduce a new Recurrent Neural Network (RNN) that can process continuous stereo video input, predicting depth-maps for each frame without requiring pre-training or ground-truth depth-maps for supervision. Leveraging the recurrent nature of the network through convolutional-LSTM blocks, our model can learn from past experiences, adapt to new environments, and demonstrate strong generalization abilities in diverse settings. Our method exhibits robust performance across changes in scene content, image characteristics, lighting conditions, and seasonal variations. Through comprehensive experiments, we showcase the adaptability of our approach across different scenarios and highlight its superior stereo matching accuracy compared to existing deep stereo methods on benchmark datasets like KITTI and Middlebury stereo.\""}, "2206.12571": {"original_text": "  This competition focus on Urban-Sense Segmentation based on the vehicle\ncamera view. Class highly unbalanced Urban-Sense images dataset challenge the\nexisting solutions and further studies. Deep Conventional neural network-based\nsemantic segmentation methods such as encoder-decoder architecture and\nmulti-scale and pyramid-based approaches become flexible solutions applicable\nto real-world applications. In this competition, we mainly review the\nliterature and conduct experiments on transformer-driven methods especially\nSegFormer, to achieve an optimal trade-off between performance and efficiency.\nFor example, SegFormer-B0 achieved 74.6% mIoU with the smallest FLOPS, 15.6G,\nand the largest model, SegFormer- B5 archived 80.2% mIoU. According to multiple\nfactors, including individual case failure analysis, individual class\nperformance, training pressure and efficiency estimation, the final candidate\nmodel for the competition is SegFormer- B2 with 50.6 GFLOPS and 78.5% mIoU\nevaluated on the testing set. Checkout our code implementation at\nhttps://vmv.re/cv3315.\n", "rewritten_text": "This competition focuses on Urban-Sense Segmentation using vehicle camera views. The highly unbalanced Urban-Sense image dataset poses a challenge to existing solutions and ongoing research. Deep neural network-based semantic segmentation methods, such as encoder-decoder architecture and multi-scale and pyramid-based approaches, offer flexible solutions for real-world applications. In this competition, we primarily examine the literature and conduct experiments on transformer-driven methods, particularly SegFormer, to find the optimal balance between performance and efficiency. For instance, SegFormer-B0 achieved a mean Intersection over Union (mIoU) of 74.6% with the lowest FLOPS at 15.6G, while the largest model, SegFormer-B5, reached 80.2% mIoU. After considering various factors like individual case failure analysis, class performance, training efficiency, and model size, the selected model for the competition is SegFormer-B2, which achieved 78.5% mIoU with 50.6 GFLOPS on the testing set. You can find our code implementation at https://vmv.re/cv3315."}, "2204.11994": {"original_text": "  Digital pathological analysis is run as the main examination used for cancer\ndiagnosis. Recently, deep learning-driven feature extraction from pathology\nimages is able to detect genetic variations and tumor environment, but few\nstudies focus on differential gene expression in tumor cells. In this paper, we\npropose a self-supervised contrastive learning framework, HistCode, to infer\ndifferential gene expressions from whole slide images (WSIs). We leveraged\ncontrastive learning on large-scale unannotated WSIs to derive slide-level\nhistopathological feature in latent space, and then transfer it to tumor\ndiagnosis and prediction of differentially expressed cancer driver genes. Our\nextensive experiments showed that our method outperformed other\nstate-of-the-art models in tumor diagnosis tasks, and also effectively\npredicted differential gene expressions. Interestingly, we found the higher\nfold-changed genes can be more precisely predicted. To intuitively illustrate\nthe ability to extract informative features from pathological images, we\nspatially visualized the WSIs colored by the attentive scores of image tiles.\nWe found that the tumor and necrosis areas were highly consistent with the\nannotations of experienced pathologists. Moreover, the spatial heatmap\ngenerated by lymphocyte-specific gene expression patterns was also consistent\nwith the manually labeled WSI.\n", "rewritten_text": "Digital pathology analysis is the primary method used for diagnosing cancer. Recent advancements in deep learning have enabled the detection of genetic variations and tumor environments from pathology images. However, there is limited research on identifying differential gene expression in tumor cells. This paper introduces HistCode, a self-supervised contrastive learning framework, to infer gene expression differences from whole slide images (WSIs). By utilizing contrastive learning on unannotated WSIs, we extract histopathological features and apply them to tumor diagnosis and predicting cancer driver genes. Our experiments demonstrate that HistCode outperforms other models in tumor diagnosis and accurately predicts gene expression variances, particularly for highly changed genes. Visualizations of WSIs colored by image tile scores show alignment with pathologists' annotations, highlighting the model's ability to extract informative features. Additionally, spatial heatmaps based on lymphocyte-specific gene expression patterns align with manual annotations on WSIs."}, "2309.07509": {"original_text": "  Generating realistic talking faces is a complex and widely discussed task\nwith numerous applications. In this paper, we present DiffTalker, a novel model\ndesigned to generate lifelike talking faces through audio and landmark\nco-driving. DiffTalker addresses the challenges associated with directly\napplying diffusion models to audio control, which are traditionally trained on\ntext-image pairs. DiffTalker consists of two agent networks: a\ntransformer-based landmarks completion network for geometric accuracy and a\ndiffusion-based face generation network for texture details. Landmarks play a\npivotal role in establishing a seamless connection between the audio and image\ndomains, facilitating the incorporation of knowledge from pre-trained diffusion\nmodels. This innovative approach efficiently produces articulate-speaking\nfaces. Experimental results showcase DiffTalker's superior performance in\nproducing clear and geometrically accurate talking faces, all without the need\nfor additional alignment between audio and image features.\n", "rewritten_text": "In this paper, we introduce DiffTalker, a new model that aims to create realistic talking faces by combining audio and landmark information. DiffTalker overcomes the challenges of using diffusion models for audio control, which are typically trained on text-image pairs. It consists of two networks: a transformer-based landmarks completion network for precise geometry and a diffusion-based face generation network for detailed textures. Landmarks play a crucial role in bridging the gap between audio and image domains, allowing for the integration of knowledge from pre-trained diffusion models. This innovative approach efficiently generates expressive talking faces. Experimental results demonstrate DiffTalker's exceptional performance in producing clear and accurate talking faces, eliminating the need for additional alignment between audio and image features."}, "2210.09071": {"original_text": "  Monocular Depth Estimation (MDE) aims to predict pixel-wise depth given a\nsingle RGB image. For both, the convolutional as well as the recent\nattention-based models, encoder-decoder-based architectures have been found to\nbe useful due to the simultaneous requirement of global context and pixel-level\nresolution. Typically, a skip connection module is used to fuse the encoder and\ndecoder features, which comprises of feature map concatenation followed by a\nconvolution operation. Inspired by the demonstrated benefits of attention in a\nmultitude of computer vision problems, we propose an attention-based fusion of\nencoder and decoder features. We pose MDE as a pixel query refinement problem,\nwhere coarsest-level encoder features are used to initialize pixel-level\nqueries, which are then refined to higher resolutions by the proposed Skip\nAttention Module (SAM). We formulate the prediction problem as ordinal\nregression over the bin centers that discretize the continuous depth range and\nintroduce a Bin Center Predictor (BCP) module that predicts bins at the\ncoarsest level using pixel queries. Apart from the benefit of image adaptive\ndepth binning, the proposed design helps learn improved depth embedding in\ninitial pixel queries via direct supervision from the ground truth. Extensive\nexperiments on the two canonical datasets, NYUV2 and KITTI, show that our\narchitecture outperforms the state-of-the-art by 5.3% and 3.9%, respectively,\nalong with an improved generalization performance by 9.4% on the SUNRGBD\ndataset. Code is available at https://github.com/ashutosh1807/PixelFormer.git.\n", "rewritten_text": "Monocular Depth Estimation (MDE) is the task of predicting depth for each pixel in a single RGB image. Both convolutional and attention-based models have found encoder-decoder architectures to be effective, as they require both global context and pixel-level detail. Typically, a skip connection module is used to combine encoder and decoder features through feature map concatenation and convolution. Drawing inspiration from the benefits of attention in various computer vision tasks, we propose an attention-based fusion of encoder and decoder features. We approach MDE as a problem of refining pixel queries, starting with coarse encoder features to initialize queries and then refining them to higher resolutions using the Skip Attention Module (SAM). We frame the prediction task as ordinal regression over bin centers that discretize the depth range, introducing a Bin Center Predictor (BCP) module to predict bins at the coarsest level based on pixel queries. This design not only benefits from adaptive depth binning but also helps improve depth embedding in initial pixel queries through direct supervision from ground truth. Extensive experiments on NYUV2 and KITTI datasets demonstrate that our architecture outperforms the state-of-the-art by 5.3% and 3.9%, respectively, and shows improved generalization performance by 9.4% on the SUNRGBD dataset. The code is available at https://github.com/ashutosh1807/PixelFormer.git."}, "2312.11716": {"original_text": "  Demand for efficient onboard object detection is increasing due to its key\nrole in autonomous navigation. However, deploying object detection models such\nas YOLO on resource constrained edge devices is challenging due to the high\ncomputational requirements of such models. In this paper, an compressed object\ndetection model named Squeezed Edge YOLO is examined. This model is compressed\nand optimized to kilobytes of parameters in order to fit onboard such edge\ndevices. To evaluate Squeezed Edge YOLO, two use cases - human and shape\ndetection - are used to show the model accuracy and performance. Moreover, the\nmodel is deployed onboard a GAP8 processor with 8 RISC-V cores and an NVIDIA\nJetson Nano with 4GB of memory. Experimental results show Squeezed Edge YOLO\nmodel size is optimized by a factor of 8x which leads to 76% improvements in\nenergy efficiency and 3.3x faster throughout.\n", "rewritten_text": "The demand for efficient onboard object detection is growing as it plays a crucial role in autonomous navigation. However, deploying object detection models like YOLO on edge devices with limited resources is challenging due to their high computational requirements. This paper examines a compressed object detection model called Squeezed Edge YOLO, which has been optimized to have kilobytes of parameters to be suitable for edge devices. The accuracy and performance of Squeezed Edge YOLO are evaluated using two use cases - human and shape detection. The model is tested on a GAP8 processor with 8 RISC-V cores and an NVIDIA Jetson Nano with 4GB of memory. Experimental results demonstrate that the Squeezed Edge YOLO model size has been optimized by a factor of 8x, resulting in a 76% improvement in energy efficiency and 3.3x faster throughput."}, "2409.15939": {"original_text": "  3D shape completion is traditionally solved using supervised training or by\ndistribution learning on complete shape examples. Recently self-supervised\nlearning approaches that do not require any complete 3D shape examples have\ngained more interests. In this paper, we propose a non-adversarial\nself-supervised approach for the shape completion task. Our first finding is\nthat completion problems can be formulated as an involutory function trivially,\nwhich implies a special constraint on the completion function G, such that\nG(G(X)) = X. Our second constraint on self-supervised shape completion relies\non the fact that shape completion becomes easier to solve with correspondences\nand similarly, completion can simplify the correspondences problem. We\nformulate a consistency measure in the canonical space in order to supervise\nthe completion function. We efficiently optimize the completion and\ncorrespondence modules using \"freeze and alternate\" strategy. The overall\napproach performs well for rigid shapes in a category as well as dynamic\nnon-rigid shapes. We ablate our design choices and compare our solution against\nstate-of-the-art methods, showing remarkable accuracy approaching supervised\naccuracy in some cases.\n", "rewritten_text": "The completion of 3D shapes has traditionally been addressed through supervised training or distribution learning using complete shape examples. Recently, there has been a growing interest in self-supervised learning methods that do not rely on complete 3D shape examples. This paper introduces a non-adversarial self-supervised approach for shape completion. The key insight is that completion problems can be seen as involutory functions, leading to a special constraint on the completion function G, where G(G(X)) = X. Another constraint leverages correspondences to simplify shape completion and vice versa. A consistency measure in the canonical space is used to supervise the completion function. The completion and correspondence modules are efficiently optimized using a \"freeze and alternate\" strategy. The proposed approach performs well for both rigid and non-rigid shapes, demonstrating competitive accuracy compared to state-of-the-art methods, approaching supervised accuracy in some scenarios."}, "1807.10552": {"original_text": "  Histology imaging is an essential diagnosis method to finalize the grade and\nstage of cancer of different tissues, especially for breast cancer diagnosis.\nSpecialists often disagree on the final diagnosis on biopsy tissue due to the\ncomplex morphological variety. Although convolutional neural networks (CNN)\nhave advantages in extracting discriminative features in image classification,\ndirectly training a CNN on high resolution histology images is computationally\ninfeasible currently. Besides, inconsistent discriminative features often\ndistribute over the whole histology image, which incurs challenges in\npatch-based CNN classification method. In this paper, we propose a novel\narchitecture for automatic classification of high resolution histology images.\nFirst, an adapted residual network is employed to explore hierarchical features\nwithout attenuation. Second, we develop a robust deep fusion network to utilize\nthe spatial relationship between patches and learn to correct the prediction\nbias generated from inconsistent discriminative feature distribution. The\nproposed method is evaluated using 10-fold cross-validation on 400 high\nresolution breast histology images with balanced labels and reports 95%\naccuracy on 4-class classification and 98.5% accuracy, 99.6% AUC on 2-class\nclassification (carcinoma and non-carcinoma), which substantially outperforms\nprevious methods and close to pathologist performance.\n", "rewritten_text": "Histology imaging plays a crucial role in diagnosing cancer grades and stages, particularly in breast cancer cases. Due to the complex morphological variations in biopsy tissues, specialists often struggle to reach a consensus on the final diagnosis. While convolutional neural networks (CNNs) excel in extracting distinctive features for image classification, training them directly on high-resolution histology images is currently impractical due to computational constraints. Additionally, the scattered distribution of discriminative features across histology images poses challenges for patch-based CNN classification methods. This paper introduces a novel architecture for automatically classifying high-resolution histology images. The approach involves using an adapted residual network to capture hierarchical features effectively and a deep fusion network to leverage spatial relationships between patches and address prediction biases caused by inconsistent feature distribution. The proposed method achieves impressive results, with 95% accuracy in 4-class classification and 98.5% accuracy, 99.6% AUC in 2-class classification (carcinoma and non-carcinoma) based on evaluation using 10-fold cross-validation on 400 balanced-label breast histology images. These results significantly outperform previous methods and approach the performance of pathologists."}, "2205.11395": {"original_text": "  Hyperspectral anomalous change detection has been a challenging task for its\nemphasis on the dynamics of small and rare objects against the prevalent\nchanges. In this paper, we have proposed a Multi-Temporal spatial-spectral\nComparison Network for hyperspectral anomalous change detection (MTC-NET). The\nwhole model is a deep siamese network, aiming at learning the prevalent\nspectral difference resulting from the complex imaging conditions from the\nhyperspectral images by contrastive learning. A three-dimensional spatial\nspectral attention module is designed to effectively extract the spatial\nsemantic information and the key spectral differences. Then the gaps between\nthe multi-temporal features are minimized, boosting the alignment of the\nsemantic and spectral features and the suppression of the multi-temporal\nbackground spectral difference. The experiments on the \"Viareggio 2013\"\ndatasets demonstrate the effectiveness of proposed MTC-NET.\n", "rewritten_text": "The task of detecting anomalous changes in hyperspectral imagery has been difficult due to the focus on identifying small and rare objects amidst common changes. This paper introduces a novel approach called the Multi-Temporal spatial-spectral Comparison Network (MTC-NET) for hyperspectral anomalous change detection. The MTC-NET is a deep siamese network that aims to learn the spectral differences in hyperspectral images under complex imaging conditions through contrastive learning. It includes a three-dimensional spatial spectral attention module to extract spatial semantic information and key spectral differences effectively. By minimizing the gaps between multi-temporal features, the model enhances the alignment of semantic and spectral features while suppressing background spectral differences over time. Experimental results on the \"Viareggio 2013\" datasets validate the effectiveness of the proposed MTC-NET."}, "2109.01211": {"original_text": "  Reproducibility has become an intensely debated topic in NLP and ML over\nrecent years, but no commonly accepted way of assessing reproducibility, let\nalone quantifying it, has so far emerged. The assumption has been that wider\nscientific reproducibility terminology and definitions are not applicable to\nNLP/ML, with the result that many different terms and definitions have been\nproposed, some diametrically opposed. In this paper, we test this assumption,\nby taking the standard terminology and definitions from metrology and applying\nthem directly to NLP/ML. We find that we are able to straightforwardly derive a\npractical framework for assessing reproducibility which has the desirable\nproperty of yielding a quantified degree of reproducibility that is comparable\nacross different reproduction studies.\n", "rewritten_text": "In recent years, there has been much discussion surrounding reproducibility in the fields of NLP and ML. However, a universally accepted method for evaluating reproducibility, let alone measuring it, has yet to be established. It has been assumed that traditional scientific reproducibility concepts do not apply to NLP/ML, leading to a variety of conflicting terms and definitions being proposed. This study challenges that assumption by adopting standard terminology and definitions from metrology and applying them directly to NLP/ML. The results show that this approach allows for the development of a practical framework for assessing reproducibility, which provides a quantifiable measure that can be compared across different replication studies."}, "2101.1065": {"original_text": "  In machine translation field, in both academia and industry, there is a\ngrowing interest in increasingly powerful systems, using corpora of several\nhundred million to several billion examples. These systems represent the\nstate-of-the-art. Here we defend the idea of developing in parallel <<frugal>>\nbilingual translation systems, trained with relatively small corpora. Based on\nthe observation of a standard human professional translator, we estimate that\nthe corpora should be composed at maximum of a monolingual sub-corpus of 75\nmillion examples for the source language, a second monolingual sub-corpus of 6\nmillion examples for the target language, and an aligned bilingual sub-corpus\nof 6 million bi-examples. A less desirable alternative would be an aligned\nbilingual corpus of 47.5 million bi-examples.\n", "rewritten_text": "In the field of machine translation, there is a growing interest in developing powerful systems that utilize large corpora containing hundreds of millions to billions of examples. These advanced systems are considered state-of-the-art. In this context, we propose the concept of creating \"frugal\" bilingual translation systems in parallel, which can be trained using relatively small corpora. Drawing inspiration from the practices of professional human translators, we suggest that an optimal setup would involve a monolingual sub-corpus of up to 75 million examples for the source language, another monolingual sub-corpus of 6 million examples for the target language, and an aligned bilingual sub-corpus of 6 million bi-examples. A less ideal option would be to use an aligned bilingual corpus containing 47.5 million bi-examples."}, "2211.10104": {"original_text": "  Stereo images, containing left and right view images with disparity, are\nutilized in solving low-vision tasks recently, e.g., rain removal and\nsuper-resolution. Stereo image restoration methods usually obtain better\nperformance than monocular methods by learning the disparity between dual views\neither implicitly or explicitly. However, existing stereo rain removal methods\nstill cannot make full use of the complementary information between two views,\nand we find it is because: 1) the rain streaks have more complex distributions\nin directions and densities, which severely damage the complementary\ninformation and pose greater challenges; 2) the disparity estimation is not\naccurate enough due to the imperfect fusion mechanism for the features between\ntwo views. To overcome such limitations, we propose a new \\underline{Stereo}\n\\underline{I}mage \\underline{R}ain \\underline{R}emoval method (StereoIRR) via\nsufficient interaction between two views, which incorporates: 1) a new\nDual-view Mutual Attention (DMA) mechanism which generates mutual attention\nmaps by taking left and right views as key information for each other to\nfacilitate cross-view feature fusion; 2) a long-range and cross-view\ninteraction, which is constructed with basic blocks and dual-view mutual\nattention, can alleviate the adverse effect of rain on complementary\ninformation to help the features of stereo images to get long-range and\ncross-view interaction and fusion. Notably, StereoIRR outperforms other related\nmonocular and stereo image rain removal methods on several datasets. Our codes\nand datasets will be released.\n", "rewritten_text": "Recently, stereo images, which consist of left and right view images with disparity, have been used for various low-vision tasks such as rain removal and super-resolution. Stereo image restoration techniques typically achieve superior results compared to monocular methods by learning the disparity between the dual views either implicitly or explicitly. However, current stereo rain removal methods struggle to fully leverage the complementary information between the two views due to two main reasons: 1) the complex distributions of rain streaks in terms of directions and densities, which can severely disrupt the complementary information and present significant challenges; and 2) inaccurate disparity estimation resulting from imperfect fusion mechanisms for features between the two views. To address these limitations, we introduce a novel Stereo Image Rain Removal method (StereoIRR) that emphasizes interaction between the two views. This method incorporates: 1) a Dual-view Mutual Attention (DMA) mechanism that generates mutual attention maps by utilizing the left and right views as key information for each other to enhance cross-view feature fusion; and 2) a long-range and cross-view interaction approach, which combines basic blocks and dual-view mutual attention to mitigate the negative impact of rain on complementary information, enabling stereo images to benefit from long-range and cross-view interaction and fusion. Notably, StereoIRR surpasses other monocular and stereo image rain removal methods on multiple datasets. Our codes and datasets will be made available for further research."}, "1704.07293": {"original_text": "  The accuracy of object detectors and trackers is most commonly evaluated by\nthe Intersection over Union (IoU) criterion. To date, most approaches are\nrestricted to axis-aligned or oriented boxes and, as a consequence, many\ndatasets are only labeled with boxes. Nevertheless, axis-aligned or oriented\nboxes cannot accurately capture an object's shape. To address this, a number of\ndensely segmented datasets has started to emerge in both the object detection\nand the object tracking communities. However, evaluating the accuracy of object\ndetectors and trackers that are restricted to boxes on densely segmented data\nis not straightforward. To close this gap, we introduce the relative\nIntersection over Union (rIoU) accuracy measure. The measure normalizes the IoU\nwith the optimal box for the segmentation to generate an accuracy measure that\nranges between 0 and 1 and allows a more precise measurement of accuracies.\nFurthermore, it enables an efficient and easy way to understand scenes and the\nstrengths and weaknesses of an object detection or tracking approach. We\ndisplay how the new measure can be efficiently calculated and present an\neasy-to-use evaluation framework. The framework is tested on the DAVIS and the\nVOT2016 segmentations and has been made available to the community.\n", "rewritten_text": "Object detectors and trackers are typically assessed using the Intersection over Union (IoU) criterion. However, current methods often rely on axis-aligned or oriented boxes for labeling, which may not accurately represent an object's shape. To address this limitation, there is a growing trend towards densely segmented datasets in both object detection and tracking fields. Evaluating the accuracy of detectors and trackers on such datasets can be challenging. To bridge this gap, we propose the relative Intersection over Union (rIoU) accuracy measure. This measure normalizes IoU with the optimal box for segmentation, providing a more precise accuracy assessment ranging from 0 to 1. It also offers insights into the strengths and weaknesses of detection and tracking methods. We demonstrate the efficient calculation of this measure and provide an accessible evaluation framework tested on DAVIS and VOT2016 segmentations, now available for the community."}, "2306.02819": {"original_text": "  Natural language understanding (NLU) is an essential branch of natural\nlanguage processing, which relies on representations generated by pre-trained\nlanguage models (PLMs). However, PLMs primarily focus on acquiring\nlexico-semantic information, while they may be unable to adequately handle the\nmeaning of constructions. To address this issue, we introduce construction\ngrammar (CxG), which highlights the pairings of form and meaning, to enrich\nlanguage representation. We adopt usage-based construction grammar as the basis\nof our work, which is highly compatible with statistical models such as PLMs.\nThen a HyCxG framework is proposed to enhance language representation through a\nthree-stage solution. First, all constructions are extracted from sentences via\na slot-constraints approach. As constructions can overlap with each other,\nbringing redundancy and imbalance, we formulate the conditional max coverage\nproblem for selecting the discriminative constructions. Finally, we propose a\nrelational hypergraph attention network to acquire representation from\nconstructional information by capturing high-order word interactions among\nconstructions. Extensive experiments demonstrate the superiority of the\nproposed model on a variety of NLU tasks.\n", "rewritten_text": "Natural language understanding (NLU) is a crucial aspect of natural language processing that relies on representations produced by pre-trained language models (PLMs). While PLMs mainly focus on acquiring lexico-semantic information, they may struggle to fully grasp the meaning of linguistic structures. To tackle this challenge, we introduce construction grammar (CxG), which emphasizes the connection between form and meaning to enhance language representation. Our approach is based on usage-based construction grammar, which aligns well with statistical models like PLMs. We propose a HyCxG framework to improve language representation through a three-stage process. Firstly, constructions are extracted from sentences using a slot-constraints method. To address redundancy and imbalance caused by overlapping constructions, we introduce the conditional max coverage problem to select the most informative constructions. Finally, we introduce a relational hypergraph attention network to capture high-order word interactions among constructions and derive representations from constructional information. Extensive experiments demonstrate the effectiveness of our model across various NLU tasks."}, "1309.4573": {"original_text": "  This paper is based on an application of smoothing of 3D face images followed\nby feature detection i.e. detecting the nose tip. The present method uses a\nweighted mesh median filtering technique for smoothing. In this present\nsmoothing technique we have built the neighborhood surrounding a particular\npoint in 3D face and replaced that with the weighted value of the surrounding\npoints in 3D face image. After applying the smoothing technique to the 3D face\nimages our experimental results show that we have obtained considerable\nimprovement as compared to the algorithm without smoothing. We have used here\nthe maximum intensity algorithm for detecting the nose-tip and this method\ncorrectly detects the nose-tip in case of any pose i.e. along X, Y, and Z axes.\nThe present technique gave us worked successfully on 535 out of 542 3D face\nimages as compared to the method without smoothing which worked only on 521 3D\nface images out of 542 face images. Thus we have obtained a 98.70% performance\nrate over 96.12% performance rate of the algorithm without smoothing. All the\nexperiments have been performed on the FRAV3D database.\n", "rewritten_text": "This study focuses on enhancing 3D face images by applying a smoothing technique followed by nose tip detection. The method utilizes weighted mesh median filtering for smoothing, where the neighborhood around a specific point in the 3D face is replaced with a weighted value from surrounding points. Results indicate significant improvement compared to the unsmoothed algorithm. Nose tip detection using the maximum intensity algorithm is successful across various poses. The technique achieved successful results on 535 out of 542 3D face images, outperforming the unsmoothed method which worked on 521 images. This represents a performance rate of 98.70% compared to 96.12% without smoothing. Experiments were conducted using the FRAV3D database."}, "2306.16682": {"original_text": "  Egocentric action anticipation aims to predict the future actions the camera\nwearer will perform from the observation of the past. While predictions about\nthe future should be available before the predicted events take place, most\napproaches do not pay attention to the computational time required to make such\npredictions. As a result, current evaluation schemes assume that predictions\nare available right after the input video is observed, i.e., presuming a\nnegligible runtime, which may lead to overly optimistic evaluations. We propose\na streaming egocentric action evaluation scheme which assumes that predictions\nare performed online and made available only after the model has processed the\ncurrent input segment, which depends on its runtime. To evaluate all models\nconsidering the same prediction horizon, we hence propose that slower models\nshould base their predictions on temporal segments sampled ahead of time. Based\non the observation that model runtime can affect performance in the considered\nstreaming evaluation scenario, we further propose a lightweight action\nanticipation model based on feed-forward 3D CNNs which is optimized using\nknowledge distillation techniques with a novel past-to-future distillation\nloss. Experiments on the three popular datasets EPIC-KITCHENS-55,\nEPIC-KITCHENS-100 and EGTEA Gaze+ show that (i) the proposed evaluation scheme\ninduces a different ranking on state-of-the-art methods as compared to classic\nevaluations, (ii) lightweight approaches tend to outmatch more computationally\nexpensive ones, and (iii) the proposed model based on feed-forward 3D CNNs and\nknowledge distillation outperforms current art in the streaming egocentric\naction anticipation scenario.\n", "rewritten_text": "The goal of egocentric action anticipation is to predict the future actions of the camera wearer based on past observations. Many existing approaches do not consider the computational time needed to make these predictions, leading to potentially overly optimistic evaluations. We suggest a new evaluation scheme where predictions are made online and only available after processing the current input segment, taking into account the model's runtime. Slower models should base their predictions on pre-sampled temporal segments to ensure fair comparison. We introduce a lightweight action anticipation model using feed-forward 3D CNNs optimized with knowledge distillation techniques. Experiments on popular datasets show that our proposed evaluation scheme results in a different ranking of methods compared to traditional evaluations, with lightweight models often outperforming more complex ones. Our model, based on feed-forward 3D CNNs and knowledge distillation, surpasses current state-of-the-art methods in egocentric action anticipation."}, "1508.04458": {"original_text": "  Three-dimensional x-ray CT image reconstruction in baggage scanning in\nsecurity applications is an important research field. The variety of materials\nto be reconstructed is broader than medical x-ray imaging. Presence of high\nattenuating materials such as metal may cause artifacts if analytical\nreconstruction methods are used. Statistical modeling and the resultant\niterative algorithms are known to reduce these artifacts and present good\nquantitative accuracy in estimates of linear attenuation coefficients. However,\niterative algorithms may require computations in order to achieve\nquantitatively accurate results. For the case of baggage scanning, in order to\nprovide fast accurate inspection throughput, they must be accelerated\ndrastically. There are many approaches proposed in the literature to increase\nspeed of convergence. This paper presents a new method that estimates the\nwavelet coefficients of the images in the discrete wavelet transform domain\ninstead of the image space itself. Initially, surrogate functions are created\naround approximation coefficients only. As the iterations proceed, the wavelet\ntree on which the updates are made is expanded based on a criterion and detail\ncoefficients at each level are updated and the tree is expanded this way. For\nexample, in the smooth regions of the image the detail coefficients are not\nupdated while the coefficients that represent the high-frequency component\naround edges are being updated, thus saving time by focusing computations where\nthey are needed. This approach is implemented on real data from a SureScan (TM)\nx1000 Explosive Detection System and compared to straightforward implementation\nof the unregularized alternating minimization of O'Sullivan and Benac [1].\n", "rewritten_text": "The field of three-dimensional x-ray CT image reconstruction for baggage scanning in security applications is a crucial area of research. The range of materials to be reconstructed is wider compared to medical x-ray imaging, with the presence of high attenuating materials like metal potentially causing artifacts when using analytical reconstruction methods. To address this issue, statistical modeling and iterative algorithms have been developed to reduce artifacts and provide accurate estimates of linear attenuation coefficients. However, these iterative algorithms often require significant computational resources to achieve precise results. In the context of baggage scanning, it is essential to accelerate these algorithms to ensure fast and accurate inspection throughput. Various methods have been proposed in the literature to enhance the convergence speed. This paper introduces a novel approach that estimates wavelet coefficients in the discrete wavelet transform domain instead of directly in the image space. Initially, surrogate functions are created around the approximation coefficients, and as iterations progress, the wavelet tree is expanded based on a criterion, updating detail coefficients at each level. This targeted updating strategy focuses computations on areas where they are most needed, such as high-frequency components around edges, while avoiding unnecessary updates in smooth regions of the image. The effectiveness of this approach is demonstrated using real data from a SureScan (TM) x1000 Explosive Detection System and compared to a standard implementation of the unregularized alternating minimization method proposed by O'Sullivan and Benac [1]."}, "2301.00184": {"original_text": "  Most existing text-video retrieval methods focus on cross-modal matching\nbetween the visual content of videos and textual query sentences. However, in\nreal-world scenarios, online videos are often accompanied by relevant text\ninformation such as titles, tags, and even subtitles, which can be utilized to\nmatch textual queries. This insight has motivated us to propose a novel\napproach to text-video retrieval, where we directly generate associated\ncaptions from videos using zero-shot video captioning with knowledge from\nweb-scale pre-trained models (e.g., CLIP and GPT-2). Given the generated\ncaptions, a natural question arises: what benefits do they bring to text-video\nretrieval? To answer this, we introduce Cap4Video, a new framework that\nleverages captions in three ways: i) Input data: video-caption pairs can\naugment the training data. ii) Intermediate feature interaction: we perform\ncross-modal feature interaction between the video and caption to produce\nenhanced video representations. iii) Output score: the Query-Caption matching\nbranch can complement the original Query-Video matching branch for text-video\nretrieval. We conduct comprehensive ablation studies to demonstrate the\neffectiveness of our approach. Without any post-processing, Cap4Video achieves\nstate-of-the-art performance on four standard text-video retrieval benchmarks:\nMSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%). The code is\navailable at https://github.com/whwu95/Cap4Video .\n", "rewritten_text": "Many current methods for text-video retrieval focus on matching visual content in videos with textual queries. However, in real-world situations, online videos often come with accompanying text information like titles, tags, and subtitles. This additional text can be used to match queries. This realization has inspired us to propose a new approach to text-video retrieval. In our method, we generate captions directly from videos using zero-shot video captioning with the help of large pre-trained models like CLIP and GPT-2. This raises the question: how do these generated captions benefit text-video retrieval? To address this, we introduce Cap4Video, a framework that utilizes captions in three ways: 1) as additional training data, 2) for enhancing video representations through cross-modal feature interaction, and 3) to complement the original Query-Video matching branch for retrieval. We conduct thorough studies to show the effectiveness of our approach. Cap4Video achieves top performance on four standard text-video retrieval benchmarks: MSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%) without any additional processing. The code can be found at https://github.com/whwu95/Cap4Video."}, "1907.08854": {"original_text": "  Document Grounded Conversations is a task to generate dialogue responses when\nchatting about the content of a given document. Obviously, document knowledge\nplays a critical role in Document Grounded Conversations, while existing\ndialogue models do not exploit this kind of knowledge effectively enough. In\nthis paper, we propose a novel Transformer-based architecture for multi-turn\ndocument grounded conversations. In particular, we devise an Incremental\nTransformer to encode multi-turn utterances along with knowledge in related\ndocuments. Motivated by the human cognitive process, we design a two-pass\ndecoder (Deliberation Decoder) to improve context coherence and knowledge\ncorrectness. Our empirical study on a real-world Document Grounded Dataset\nproves that responses generated by our model significantly outperform\ncompetitive baselines on both context coherence and knowledge relevance.\n", "rewritten_text": "The task of Document Grounded Conversations involves generating dialogue responses based on the content of a given document. While document knowledge is crucial in these conversations, current dialogue models do not effectively utilize this knowledge. This paper introduces a new Transformer-based architecture for multi-turn document grounded conversations. The Incremental Transformer is designed to encode multi-turn utterances and related document knowledge. Inspired by human cognitive processes, a two-pass decoder (Deliberation Decoder) is created to enhance context coherence and knowledge accuracy. Empirical findings on a real-world Document Grounded Dataset demonstrate that our model produces responses that outperform other models in terms of context coherence and knowledge relevance."}, "2406.05023": {"original_text": "  Generative adversarial networks (GANs) are machine learning models that are\nused to estimate the underlying statistical structure of a given dataset and as\na result can be used for a variety of tasks such as image generation or anomaly\ndetection. Despite their initial simplicity, designing an effective loss\nfunction for training GANs remains challenging, and various loss functions have\nbeen proposed aiming to improve the performance and stability of the generative\nmodels. In this study, loss function design for GANs is presented as an\noptimization problem solved using the genetic programming (GP) approach.\nInitial experiments were carried out using small Deep Convolutional GAN (DCGAN)\nmodel and the MNIST dataset, in order to search experimentally for an improved\nloss function. The functions found were evaluated on CIFAR10, with the best\nfunction, named GANetic loss, showing exceptionally better performance and\nstability compared to the losses commonly used for GAN training. To further\nevalute its general applicability on more challenging problems, GANetic loss\nwas applied for two medical applications: image generation and anomaly\ndetection. Experiments were performed with histopathological, gastrointestinal\nor glaucoma images to evaluate the GANetic loss in medical image generation,\nresulting in improved image quality compared to the baseline models. The\nGANetic Loss used for polyp and glaucoma images showed a strong improvement in\nthe detection of anomalies. In summary, the GANetic loss function was evaluated\non multiple datasets and applications where it consistently outperforms\nalternative loss functions. Moreover, GANetic loss leads to stable training and\nreproducible results, a known weak spot of GANs.\n", "rewritten_text": "Generative adversarial networks (GANs) are machine learning models utilized to uncover the underlying statistical structure of a dataset, enabling tasks like image generation and anomaly detection. Despite their apparent simplicity, creating an effective loss function for training GANs remains a challenge, prompting the proposal of various loss functions to enhance generative model performance and stability. This study introduces loss function design for GANs as an optimization problem tackled through genetic programming (GP). Initial experiments with a small Deep Convolutional GAN (DCGAN) model and the MNIST dataset were conducted to empirically search for an enhanced loss function. The identified functions were then assessed on CIFAR10, with the top-performing function, dubbed GANetic loss, demonstrating significantly improved performance and stability compared to commonly used loss functions in GAN training. To assess its broader applicability in more complex scenarios, GANetic loss was applied to two medical applications: image generation and anomaly detection. Trials were conducted using histopathological, gastrointestinal, and glaucoma images, showcasing enhanced image quality compared to baseline models. Notably, GANetic loss exhibited notable improvements in anomaly detection for polyp and glaucoma images. In conclusion, GANetic loss was evaluated across multiple datasets and applications, consistently outperforming alternative loss functions. Furthermore, GANetic loss facilitated stable training and reproducible results, addressing a known weakness of GANs."}, "2208.01633": {"original_text": "  We present UnrealEgo, i.e., a new large-scale naturalistic dataset for\negocentric 3D human pose estimation. UnrealEgo is based on an advanced concept\nof eyeglasses equipped with two fisheye cameras that can be used in\nunconstrained environments. We design their virtual prototype and attach them\nto 3D human models for stereo view capture. We next generate a large corpus of\nhuman motions. As a consequence, UnrealEgo is the first dataset to provide\nin-the-wild stereo images with the largest variety of motions among existing\negocentric datasets. Furthermore, we propose a new benchmark method with a\nsimple but effective idea of devising a 2D keypoint estimation module for\nstereo inputs to improve 3D human pose estimation. The extensive experiments\nshow that our approach outperforms the previous state-of-the-art methods\nqualitatively and quantitatively. UnrealEgo and our source codes are available\non our project web page.\n", "rewritten_text": "We introduce UnrealEgo, a novel dataset for egocentric 3D human pose estimation that features a large-scale collection of naturalistic data. UnrealEgo utilizes innovative eyeglasses with dual fisheye cameras, allowing for usage in diverse environments. By creating a virtual prototype of these glasses and attaching them to 3D human models for stereo view capture, we generate a diverse set of human motions. This makes UnrealEgo the first dataset to offer stereo images captured in real-world settings with a wide range of motions compared to existing egocentric datasets. Additionally, we propose a new benchmark approach that involves a 2D keypoint estimation module for stereo inputs to enhance 3D human pose estimation. Through extensive experiments, we demonstrate that our method surpasses previous state-of-the-art techniques both qualitatively and quantitatively. The UnrealEgo dataset and our source codes can be accessed on our project website."}, "2011.131": {"original_text": "  To assess the knowledge proficiency of a learner, multiple choice question is\nan efficient and widespread form in standard tests. However, the composition of\nthe multiple choice question, especially the construction of distractors is\nquite challenging. The distractors are required to both incorrect and plausible\nenough to confuse the learners who did not master the knowledge. Currently, the\ndistractors are generated by domain experts which are both expensive and\ntime-consuming. This urges the emergence of automatic distractor generation,\nwhich can benefit various standard tests in a wide range of domains. In this\npaper, we propose a question and answer guided distractor generation (EDGE)\nframework to automate distractor generation. EDGE consists of three major\nmodules: (1) the Reforming Question Module and the Reforming Passage Module\napply gate layers to guarantee the inherent incorrectness of the generated\ndistractors; (2) the Distractor Generator Module applies attention mechanism to\ncontrol the level of plausibility. Experimental results on a large-scale public\ndataset demonstrate that our model significantly outperforms existing models\nand achieves a new state-of-the-art.\n", "rewritten_text": "\"To evaluate a learner's knowledge proficiency, multiple choice questions are commonly used in standard tests. However, creating effective distractors within these questions can be challenging. Distractors must be both incorrect and plausible enough to confuse learners who have not fully grasped the material. Currently, distractors are typically created by domain experts, which can be costly and time-consuming. This has led to the need for automated distractor generation, which can benefit a variety of tests across different subject areas. In this study, we introduce the Question and Answer Guided Distractor Generation (EDGE) framework, which aims to automate the process of generating distractors. EDGE consists of three main modules: the Reforming Question Module and the Reforming Passage Module, which use gate layers to ensure the generated distractors are inherently incorrect; and the Distractor Generator Module, which utilizes an attention mechanism to control the plausibility of the distractors. Results from experiments conducted on a large public dataset show that our model outperforms existing models and achieves a new state-of-the-art performance level.\""}, "2308.14082": {"original_text": "  Reconstructing interacting hands from monocular images is indispensable in\nAR/VR applications. Most existing solutions rely on the accurate localization\nof each skeleton joint. However, these methods tend to be unreliable due to the\nsevere occlusion and confusing similarity among adjacent hand parts. This also\ndefies human perception because humans can quickly imitate an interaction\npattern without localizing all joints. Our key idea is to first construct a\ntwo-hand interaction prior and recast the interaction reconstruction task as\nthe conditional sampling from the prior. To expand more interaction states, a\nlarge-scale multimodal dataset with physical plausibility is proposed. Then a\nVAE is trained to further condense these interaction patterns as latent codes\nin a prior distribution. When looking for image cues that contribute to\ninteraction prior sampling, we propose the interaction adjacency heatmap (IAH).\nCompared with a joint-wise heatmap for localization, IAH assigns denser visible\nfeatures to those invisible joints. Compared with an all-in-one visible\nheatmap, it provides more fine-grained local interaction information in each\ninteraction region. Finally, the correlations between the extracted features\nand corresponding interaction codes are linked by the ViT module. Comprehensive\nevaluations on benchmark datasets have verified the effectiveness of this\nframework. The code and dataset are publicly available at\nhttps://github.com/binghui-z/InterPrior_pytorch\n", "rewritten_text": "Recreating hand interactions from single images is crucial for AR/VR applications. Many current methods rely on accurately pinpointing each joint in the skeleton. However, these approaches can be unreliable due to occlusion and similarities between adjacent hand parts. This contrasts with human ability to mimic interactions without needing to locate every joint. Our main concept involves establishing a prior for two-hand interactions and transforming the reconstruction task into conditional sampling from this prior. To enhance the variety of interaction possibilities, we introduce a large-scale multimodal dataset with realistic scenarios. A VAE is then trained to condense these interaction patterns into latent codes within a prior distribution. To identify image cues that influence interaction sampling from the prior, we introduce the interaction adjacency heatmap (IAH). Compared to a heatmap based on individual joints for localization, the IAH highlights features more prominently for joints that are not visible. In contrast to a single visible heatmap, the IAH provides detailed local interaction information within each interaction region. The ViT module is used to establish connections between the extracted features and corresponding interaction codes. Extensive evaluations on standard datasets have confirmed the efficacy of this framework. The code and dataset can be accessed publicly at https://github.com/binghui-z/InterPrior_pytorch."}, "1910.10223": {"original_text": "  Deep generative models come with the promise to learn an explainable\nrepresentation for visual objects that allows image sampling, synthesis, and\nselective modification. The main challenge is to learn to properly model the\nindependent latent characteristics of an object, especially its appearance and\npose. We present a novel approach that learns disentangled representations of\nthese characteristics and explains them individually. Training requires only\npairs of images depicting the same object appearance, but no pose annotations.\nWe propose an additional classifier that estimates the minimal amount of\nregularization required to enforce disentanglement. Thus both representations\ntogether can completely explain an image while being independent of each other.\nPrevious methods based on adversarial approaches fail to enforce this\nindependence, while methods based on variational approaches lead to\nuninformative representations. In experiments on diverse object categories, the\napproach successfully recombines pose and appearance to reconstruct and\nretarget novel synthesized images. We achieve significant improvements over\nstate-of-the-art methods which utilize the same level of supervision, and reach\nperformances comparable to those of pose-supervised approaches. However, we can\nhandle the vast body of articulated object classes for which no pose\nmodels/annotations are available.\n", "rewritten_text": "\"Deep generative models offer the potential to acquire an interpretable representation of visual objects, enabling tasks such as image sampling, synthesis, and selective modification. The primary obstacle lies in effectively capturing the distinct latent features of an object, particularly its appearance and pose. Our novel approach aims to learn disentangled representations of these features, allowing for individual explanation. Training only requires pairs of images showing the same object appearance, without the need for pose annotations. We introduce an additional classifier to determine the minimal regularization needed to ensure disentanglement. By combining these representations, an image can be fully explained while maintaining independence. Unlike adversarial methods that struggle to enforce this independence, and variational methods that produce uninformative representations, our approach successfully reconstructs and retargets novel synthesized images by recombining pose and appearance. In experiments across various object categories, our method outperforms existing techniques with similar levels of supervision, achieving results comparable to pose-supervised approaches. Importantly, our approach can handle a wide range of articulated object classes for which pose models or annotations are not available.\""}, "2405.17765": {"original_text": "  Video quality assessment (VQA) is a challenging problem due to the numerous\nfactors that can affect the perceptual quality of a video, \\eg, content\nattractiveness, distortion type, motion pattern, and level. However, annotating\nthe Mean opinion score (MOS) for videos is expensive and time-consuming, which\nlimits the scale of VQA datasets, and poses a significant obstacle for deep\nlearning-based methods. In this paper, we propose a VQA method named PTM-VQA,\nwhich leverages PreTrained Models to transfer knowledge from models pretrained\non various pre-tasks, enabling benefits for VQA from different aspects.\n  Specifically, we extract features of videos from different pretrained models\nwith frozen weights and integrate them to generate representation. Since these\nmodels possess various fields of knowledge and are often trained with labels\nirrelevant to quality, we propose an Intra-Consistency and Inter-Divisibility\n(ICID) loss to impose constraints on features extracted by multiple pretrained\nmodels. The intra-consistency constraint ensures that features extracted by\ndifferent pretrained models are in the same unified quality-aware latent space,\nwhile the inter-divisibility introduces pseudo clusters based on the annotation\nof samples and tries to separate features of samples from different clusters.\nFurthermore, with a constantly growing number of pretrained models, it is\ncrucial to determine which models to use and how to use them. To address this\nproblem, we propose an efficient scheme to select suitable candidates. Models\nwith better clustering performance on VQA datasets are chosen to be our\ncandidates. Extensive experiments demonstrate the effectiveness of the proposed\nmethod.\n", "rewritten_text": "The assessment of video quality is a complex task due to various factors influencing how viewers perceive videos, such as content appeal, distortion, motion, and more. However, assigning Mean Opinion Scores (MOS) to videos is costly and time-consuming, limiting the size of datasets for video quality assessment and creating challenges for deep learning approaches. In this study, we introduce a method called PTM-VQA that utilizes PreTrained Models to transfer knowledge from models trained on different tasks, offering advantages for video quality assessment. We extract video features from various pretrained models with fixed weights and combine them to create a representation. To ensure consistency among features from different models and enhance quality-awareness, we introduce an Intra-Consistency and Inter-Divisibility (ICID) loss. This loss enforces that features from different models align in a unified quality-aware space and introduces pseudo clusters based on sample annotations to separate features from distinct quality levels. To select the most suitable pretrained models, we propose an efficient selection scheme based on clustering performance on VQA datasets. Our method is validated through extensive experiments, demonstrating its effectiveness."}, "2306.00246": {"original_text": "  The monetary value of a given piece of real estate, a parcel, is often\nreadily available from a geographic information system. However, for many\napplications, such as insurance and urban planning, it is useful to have\nestimates of property value at much higher spatial resolutions. We propose a\nmethod to estimate the distribution over property value at the pixel level from\nremote sensing imagery. We evaluate on a real-world dataset of a major urban\narea. Our results show that the proposed approaches are capable of generating\nfine-level estimates of property values, significantly improving upon a diverse\ncollection of baseline approaches.\n", "rewritten_text": "The financial worth of a specific piece of land, such as a parcel, can typically be obtained from a geographic information system. Yet, for various purposes like insurance and city planning, it is beneficial to have property value estimates at more detailed spatial levels. Our suggestion is a technique to predict the distribution of property values at the pixel level using remote sensing images. We test this on a dataset from a large urban region and find that our method can produce precise estimates of property values, surpassing various other basic methods."}, "1606.02147": {"original_text": "  The ability to perform pixel-wise semantic segmentation in real-time is of\nparamount importance in mobile applications. Recent deep neural networks aimed\nat this task have the disadvantage of requiring a large number of floating\npoint operations and have long run-times that hinder their usability. In this\npaper, we propose a novel deep neural network architecture named ENet\n(efficient neural network), created specifically for tasks requiring low\nlatency operation. ENet is up to 18$\\times$ faster, requires 75$\\times$ less\nFLOPs, has 79$\\times$ less parameters, and provides similar or better accuracy\nto existing models. We have tested it on CamVid, Cityscapes and SUN datasets\nand report on comparisons with existing state-of-the-art methods, and the\ntrade-offs between accuracy and processing time of a network. We present\nperformance measurements of the proposed architecture on embedded systems and\nsuggest possible software improvements that could make ENet even faster.\n", "rewritten_text": "The ability to conduct real-time pixel-wise semantic segmentation is crucial for mobile applications. Existing deep neural networks designed for this task often require a high number of floating point operations and have long run-times, limiting their practicality. In this study, we introduce a new deep neural network architecture called ENet (efficient neural network) specifically optimized for low latency tasks. ENet is significantly faster, with 75 times fewer FLOPs, 79 times fewer parameters, and comparable or better accuracy compared to current models. Our evaluation on CamVid, Cityscapes, and SUN datasets includes comparisons with state-of-the-art methods, highlighting the balance between accuracy and processing speed. We also present performance results of ENet on embedded systems and suggest potential software enhancements to further improve its speed."}, "2201.11374": {"original_text": "  In this work, we focus on low-resource dependency parsing for multiple\nlanguages. Several strategies are tailored to enhance performance in\nlow-resource scenarios. While these are well-known to the community, it is not\ntrivial to select the best-performing combination of these strategies for a\nlow-resource language that we are interested in, and not much attention has\nbeen given to measuring the efficacy of these strategies. We experiment with 5\nlow-resource strategies for our ensembled approach on 7 Universal Dependency\n(UD) low-resource languages. Our exhaustive experimentation on these languages\nsupports the effective improvements for languages not covered in pretrained\nmodels. We show a successful application of the ensembled system on a truly\nlow-resource language Sanskrit. The code and data are available at:\nhttps://github.com/Jivnesh/SanDP\n", "rewritten_text": "In this study, our focus is on low-resource dependency parsing for multiple languages. We have developed various strategies to improve performance in low-resource settings. While these strategies are known within the research community, selecting the most effective combination for a specific low-resource language can be challenging, and there has been limited investigation into their effectiveness. We conducted experiments using five low-resource strategies in our ensemble approach across seven Universal Dependency (UD) low-resource languages. Our comprehensive testing on these languages demonstrates significant enhancements for languages not included in pre-trained models. We present a successful implementation of the ensemble system on the truly low-resource language Sanskrit. The code and data can be accessed at: https://github.com/Jivnesh/SanDP"}, "2407.03841": {"original_text": "  Large Language Models (LLMs) have showcased remarkable capabilities in\nvarious Natural Language Processing tasks. For automatic open-domain dialogue\nevaluation in particular, LLMs have been seamlessly integrated into evaluation\nframeworks, and together with human evaluation, compose the backbone of most\nevaluations. However, existing evaluation benchmarks often rely on outdated\ndatasets and evaluate aspects like Fluency and Relevance, which fail to\nadequately capture the capabilities and limitations of state-of-the-art chatbot\nmodels.\n  This paper critically examines current evaluation benchmarks, highlighting\nthat the use of older response generators and quality aspects fail to\naccurately reflect modern chatbot capabilities. A small annotation experiment\non a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as\nGPT-4 struggle to detect actual deficiencies in dialogues generated by current\nLLM chatbots.\n", "rewritten_text": "\"Large Language Models (LLMs) have demonstrated impressive abilities across various Natural Language Processing tasks. In the context of automatic open-domain dialogue evaluation, LLMs have been smoothly integrated into evaluation frameworks, working alongside human evaluation to form the foundation of most assessments. However, current evaluation benchmarks often rely on outdated datasets and assess factors like Fluency and Relevance, which do not fully capture the strengths and weaknesses of cutting-edge chatbot models.\nThis paper critically analyzes existing evaluation benchmarks, pointing out that the use of older response generators and quality metrics does not accurately represent the capabilities of modern chatbots. A small annotation experiment conducted on a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as GPT-4 struggle to identify real shortcomings in dialogues produced by current LLM chatbots.\""}, "1502.00377": {"original_text": "  In order to track the moving objects in long range against occlusion,\ninterruption, and background clutter, this paper proposes a unified approach\nfor global trajectory analysis. Instead of the traditional frame-by-frame\ntracking, our method recovers target trajectories based on a short sequence of\nvideo frames, e.g. $15$ frames. We initially calculate a foreground map at each\nframe, as obtained from a state-of-the-art background model. An attribute graph\nis then extracted from the foreground map, where the graph vertices are image\nprimitives represented by the composite features. With this graph\nrepresentation, we pose trajectory analysis as a joint task of spatial graph\npartitioning and temporal graph matching. The task can be formulated by\nmaximizing a posteriori under the Bayesian framework, in which we integrate the\nspatio-temporal contexts and the appearance models. The probabilistic inference\nis achieved by a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given a\nperoid of observed frames, the algorithm simulates a ergodic and aperiodic\nMarkov Chain, and it visits a sequence of solution states in the joint space of\nspatial graph partitioning and temporal graph matching. In the experiments, our\nmethod is tested on several challenging videos from the public datasets of\nvisual surveillance, and it outperforms the state-of-the-art methods.\n", "rewritten_text": "This paper introduces a novel approach for tracking moving objects over long distances while dealing with occlusion, interruptions, and background clutter. Instead of the traditional frame-by-frame tracking, our method analyzes global trajectories by reconstructing target paths using a short sequence of video frames, typically 15 frames. Initially, a foreground map is generated for each frame using a cutting-edge background model. From this foreground map, an attribute graph is extracted, with graph vertices representing image primitives characterized by composite features. This graph-based representation allows us to tackle trajectory analysis as a combined task of spatial graph partitioning and temporal graph matching. We formulate this task by maximizing a posteriori probability within a Bayesian framework, integrating spatial-temporal contexts and appearance models. Probabilistic inference is carried out using a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Over a period of observed frames, the algorithm simulates an ergodic and aperiodic Markov Chain, exploring a sequence of solution states in the joint space of spatial graph partitioning and temporal graph matching. Experimental evaluations demonstrate the effectiveness of our method on challenging videos from public visual surveillance datasets, showcasing superior performance compared to existing state-of-the-art techniques."}, "1607.02204": {"original_text": "  In this paper we introduce a method to overcome one of the main challenges of\nperson re-identification in multi-camera networks, namely cross-view appearance\nchanges. The proposed solution addresses the extreme variability of person\nappearance in different camera views by exploiting multiple feature\nrepresentations. For each feature, Kernel Canonical Correlation Analysis (KCCA)\nwith different kernels is exploited to learn several projection spaces in which\nthe appearance correlation between samples of the same person observed from\ndifferent cameras is maximized. An iterative logistic regression is finally\nused to select and weigh the contributions of each feature projections and\nperform the matching between the two views. Experimental evaluation shows that\nthe proposed solution obtains comparable performance on VIPeR and PRID 450s\ndatasets and improves on PRID and CUHK01 datasets with respect to the state of\nthe art.\n", "rewritten_text": "This paper presents a method to address a key challenge in person re-identification across multiple camera networks: the issue of cross-view appearance changes. The proposed solution tackles the significant variations in person appearance across different camera views by leveraging multiple feature representations. Kernel Canonical Correlation Analysis (KCCA) is utilized with various kernels to learn multiple projection spaces that maximize the appearance correlation between samples of the same individual captured from different cameras. An iterative logistic regression is then employed to choose and assign weights to the contributions of each feature projection, facilitating the matching between the different views. Experimental results demonstrate that the proposed approach achieves comparable performance on the VIPeR and PRID 450s datasets, and outperforms existing methods on the PRID and CUHK01 datasets."}, "2208.03207": {"original_text": "  Learning with noisy labels (LNL) aims at designing strategies to improve\nmodel performance and generalization by mitigating the effects of model\noverfitting to noisy labels. The key success of LNL lies in identifying as many\nclean samples as possible from massive noisy data, while rectifying the wrongly\nassigned noisy labels. Recent advances employ the predicted label distributions\nof individual samples to perform noise verification and noisy label correction,\neasily giving rise to confirmation bias. To mitigate this issue, we propose\nNeighborhood Collective Estimation, in which the predictive reliability of a\ncandidate sample is re-estimated by contrasting it against its feature-space\nnearest neighbors. Specifically, our method is divided into two steps: 1)\nNeighborhood Collective Noise Verification to separate all training samples\ninto a clean or noisy subset, 2) Neighborhood Collective Label Correction to\nrelabel noisy samples, and then auxiliary techniques are used to assist further\nmodel optimization. Extensive experiments on four commonly used benchmark\ndatasets, i.e., CIFAR-10, CIFAR-100, Clothing-1M and Webvision-1.0, demonstrate\nthat our proposed method considerably outperforms state-of-the-art methods.\n", "rewritten_text": "The goal of Learning with Noisy Labels (LNL) is to enhance model performance and generalization by addressing overfitting to noisy labels. The success of LNL hinges on identifying clean samples within noisy data and correcting mislabeled instances. Recent advancements utilize predicted label distributions to verify noise and correct labels, but this can lead to confirmation bias. To address this issue, we introduce Neighborhood Collective Estimation. This method re-evaluates a candidate sample's predictive reliability by comparing it to its nearest neighbors in feature space. Our approach consists of two main steps: 1) Neighborhood Collective Noise Verification to categorize training samples as clean or noisy, and 2) Neighborhood Collective Label Correction to rectify mislabeled samples. Additional techniques are then employed to further optimize the model. Extensive experiments on popular datasets like CIFAR-10, CIFAR-100, Clothing-1M, and Webvision-1.0 demonstrate that our method significantly outperforms existing approaches."}, "2305.03112": {"original_text": "  A surge of interest has emerged in weakly supervised semantic segmentation\ndue to its remarkable efficiency in recent years. Existing approaches based on\ntransformers mainly focus on exploring the affinity matrix to boost CAMs with\nglobal relationships. While in this work, we first perform a scrupulous\nexamination towards the impact of successive affinity matrices and discover\nthat they possess an inclination toward sparsification as the network\napproaches convergence, hence disclosing a manifestation of over-smoothing.\nBesides, it has been observed that enhanced attention maps tend to evince a\nsubstantial amount of extraneous background noise in deeper layers. Drawing\nupon this, we posit a daring conjecture that the undisciplined over-smoothing\nphenomenon introduces a noteworthy quantity of semantically irrelevant\nbackground noise, causing performance degradation. To alleviate this issue, we\npropose a novel perspective that highlights the objects of interest by\ninvestigating the regions of the trait, thereby fostering an extensive\ncomprehension of the successive affinity matrix. Consequently, we suggest an\nadaptive re-activation mechanism (AReAM) that alleviates the issue of\nincomplete attention within the object and the unbounded background noise.\nAReAM accomplishes this by supervising high-level attention with shallow\naffinity matrices, yielding promising results. Exhaustive experiments conducted\non the commonly used dataset manifest that segmentation results can be greatly\nimproved through our proposed AReAM, which imposes restrictions on each\naffinity matrix in deep layers to make it attentive to semantic regions.\n", "rewritten_text": "Recently, there has been a growing interest in weakly supervised semantic segmentation due to its notable efficiency. While previous transformer-based approaches have focused on enhancing CAMs with global relationships through exploring affinity matrices, our work takes a closer look at the impact of successive affinity matrices. We have found that these matrices tend to become more sparse as the network converges, indicating an issue of over-smoothing. Additionally, we have observed that enhanced attention maps in deeper layers often contain excessive background noise. Building on these findings, we propose that the over-smoothing phenomenon introduces semantically irrelevant background noise, leading to performance degradation. To address this, we introduce a novel perspective that emphasizes objects of interest by analyzing trait regions, thereby enhancing our understanding of successive affinity matrices. Our solution, Adaptive Re-Activation Mechanism (AReAM), tackles incomplete attention within objects and uncontrolled background noise by supervising high-level attention with shallow affinity matrices, resulting in promising outcomes. Extensive experiments on a standard dataset demonstrate that our AReAM approach significantly enhances segmentation results by guiding each affinity matrix in deep layers to focus on semantic regions."}, "2010.10894": {"original_text": "  This paper aims to enhance the few-shot relation classification especially\nfor sentences that jointly describe multiple relations. Due to the fact that\nsome relations usually keep high co-occurrence in the same context, previous\nfew-shot relation classifiers struggle to distinguish them with few annotated\ninstances. To alleviate the above relation confusion problem, we propose CTEG,\na model equipped with two mechanisms to learn to decouple these easily-confused\nrelations. On the one hand, an Entity-Guided Attention (EGA) mechanism, which\nleverages the syntactic relations and relative positions between each word and\nthe specified entity pair, is introduced to guide the attention to filter out\ninformation causing confusion. On the other hand, a Confusion-Aware Training\n(CAT) method is proposed to explicitly learn to distinguish relations by\nplaying a pushing-away game between classifying a sentence into a true relation\nand its confusing relation. Extensive experiments are conducted on the FewRel\ndataset, and the results show that our proposed model achieves comparable and\neven much better results to strong baselines in terms of accuracy. Furthermore,\nthe ablation test and case study verify the effectiveness of our proposed EGA\nand CAT, especially in addressing the relation confusion problem.\n", "rewritten_text": "The objective of this study is to improve few-shot relation classification, particularly for sentences that describe multiple relations simultaneously. Previous few-shot relation classifiers struggle to differentiate between closely related relations due to their high co-occurrence in the same context and limited annotated instances. To address this issue, we introduce CTEG, a model with two mechanisms designed to untangle these easily-confused relations. The first mechanism, Entity-Guided Attention (EGA), utilizes syntactic relations and relative positions between words and specified entity pairs to focus attention and filter out confusing information. The second mechanism, Confusion-Aware Training (CAT), involves explicitly learning to distinguish relations by engaging in a game that involves classifying a sentence into its true relation while pushing away from its confusing relation. Extensive experiments on the FewRel dataset demonstrate that our proposed model performs comparably or even better than strong baselines in terms of accuracy. Ablation tests and case studies confirm the effectiveness of EGA and CAT, particularly in addressing the challenge of relation confusion."}, "1903.05396": {"original_text": "  This paper introduces improved methods for sub-event detection in social\nmedia streams, by applying neural sequence models not only on the level of\nindividual posts, but also directly on the stream level. Current approaches to\nidentify sub-events within a given event, such as a goal during a soccer match,\nessentially do not exploit the sequential nature of social media streams. We\naddress this shortcoming by framing the sub-event detection problem in social\nmedia streams as a sequence labeling task and adopt a neural sequence\narchitecture that explicitly accounts for the chronological order of posts.\nSpecifically, we (i) establish a neural baseline that outperforms a graph-based\nstate-of-the-art method for binary sub-event detection (2.7% micro-F1\nimprovement), as well as (ii) demonstrate superiority of a recurrent neural\nnetwork model on the posts sequence level for labeled sub-events (2.4%\nbin-level F1 improvement over non-sequential models).\n", "rewritten_text": "This paper presents enhanced techniques for detecting sub-events in social media streams. It utilizes neural sequence models not only at the post level but also directly at the stream level. Current methods for identifying sub-events within events like goals in a soccer match often overlook the sequential nature of social media streams. To address this, we approach sub-event detection in social media streams as a sequence labeling task and employ a neural sequence architecture that considers the chronological order of posts. Our study establishes a neural baseline that outperforms a graph-based method for binary sub-event detection (with a 2.7% micro-F1 improvement) and demonstrates the superiority of a recurrent neural network model at the post sequence level for labeled sub-events (with a 2.4% bin-level F1 improvement over non-sequential models)."}, "2212.03496": {"original_text": "  Script event prediction aims to predict the subsequent event given the\ncontext. This requires the capability to infer the correlations between events.\nRecent works have attempted to improve event correlation reasoning by using\npretrained language models and incorporating external knowledge~(e.g.,\ndiscourse relations). Though promising results have been achieved, some\nchallenges still remain. First, the pretrained language models adopted by\ncurrent works ignore event-level knowledge, resulting in an inability to\ncapture the correlations between events well. Second, modeling correlations\nbetween events with discourse relations is limited because it can only capture\nexplicit correlations between events with discourse markers, and cannot capture\nmany implicit correlations. To this end, we propose a novel generative approach\nfor this task, in which a pretrained language model is fine-tuned with an\nevent-centric pretraining objective and predicts the next event within a\ngenerative paradigm. Specifically, we first introduce a novel event-level blank\ninfilling strategy as the learning objective to inject event-level knowledge\ninto the pretrained language model, and then design a likelihood-based\ncontrastive loss for fine-tuning the generative model. Instead of using an\nadditional prediction layer, we perform prediction by using sequence\nlikelihoods generated by the generative model. Our approach models correlations\nbetween events in a soft way without any external knowledge. The\nlikelihood-based prediction eliminates the need to use additional networks to\nmake predictions and is somewhat interpretable since it scores each word in the\nevent. Experimental results on the multi-choice narrative cloze~(MCNC) task\ndemonstrate that our approach achieves better results than other\nstate-of-the-art baselines. Our code will be available at\nhttps://github.com/zhufq00/mcnc.\n", "rewritten_text": "The goal of script event prediction is to forecast the next event based on the context, requiring the ability to deduce connections between events. Recent studies have sought to enhance event correlation reasoning by leveraging pretrained language models and integrating external knowledge, such as discourse relations. While these efforts have shown promise, certain challenges persist. Firstly, existing works using pretrained language models overlook event-specific knowledge, leading to a limited ability to capture event correlations effectively. Secondly, relying on discourse relations to model event correlations has limitations as it can only capture explicit connections marked by discourse indicators, missing many implicit correlations. In response, we propose a new generative approach for this task. Our method involves fine-tuning a pretrained language model with an event-centric pretraining objective to predict the subsequent event within a generative framework. We introduce a unique event-level blank filling strategy as the learning objective to incorporate event-specific knowledge into the pretrained language model. Additionally, we devise a likelihood-based contrastive loss for fine-tuning the generative model. Instead of using an extra prediction layer, we predict by utilizing sequence likelihoods generated by the generative model. Our approach soft models event correlations without external knowledge. The likelihood-based prediction eliminates the need for additional networks to make predictions and offers some interpretability by scoring each word in the event. Experimental results on the multi-choice narrative cloze (MCNC) task show that our approach outperforms other state-of-the-art baselines. Our code will be accessible at https://github.com/zhufq00/mcnc."}, "1911.12982": {"original_text": "  Recently, Chinese word segmentation (CWS) methods using neural networks have\nmade impressive progress. Most of them regard the CWS as a sequence labeling\nproblem which construct models based on local features rather than considering\nglobal information of input sequence. In this paper, we cast the CWS as a\nsequence translation problem and propose a novel sequence-to-sequence CWS model\nwith an attention-based encoder-decoder framework. The model captures the\nglobal information from the input and directly outputs the segmented sequence.\nIt can also tackle other NLP tasks with CWS jointly in an end-to-end mode.\nExperiments on Weibo, PKU and MSRA benchmark datasets show that our approach\nhas achieved competitive performances compared with state-of-the-art methods.\nMeanwhile, we successfully applied our proposed model to jointly learning CWS\nand Chinese spelling correction, which demonstrates its applicability of\nmulti-task fusion.\n", "rewritten_text": "Lately, there have been significant advancements in Chinese word segmentation (CWS) techniques utilizing neural networks. Many of these methods treat CWS as a sequence labeling issue, focusing on constructing models using local features rather than considering the overall input sequence information. This study presents a new approach by framing CWS as a sequence translation problem and introducing a novel sequence-to-sequence CWS model with an attention-based encoder-decoder framework. The model captures global input information and directly produces the segmented sequence, offering the ability to address other NLP tasks alongside CWS in an end-to-end manner. Experimental results on Weibo, PKU, and MSRA benchmark datasets demonstrate that our method achieves competitive performance compared to state-of-the-art techniques. Additionally, we successfully applied our model to simultaneously learn CWS and Chinese spelling correction, showcasing its versatility in multi-task integration."}, "2308.14018": {"original_text": "  Few-shot font generation is challenging, as it needs to capture the\nfine-grained stroke styles from a limited set of reference glyphs, and then\ntransfer to other characters, which are expected to have similar styles.\nHowever, due to the diversity and complexity of Chinese font styles, the\nsynthesized glyphs of existing methods usually exhibit visible artifacts, such\nas missing details and distorted strokes. In this paper, we propose a\nVQGAN-based framework (i.e., VQ-Font) to enhance glyph fidelity through token\nprior refinement and structure-aware enhancement. Specifically, we pre-train a\nVQGAN to encapsulate font token prior within a codebook. Subsequently, VQ-Font\nrefines the synthesized glyphs with the codebook to eliminate the domain gap\nbetween synthesized and real-world strokes. Furthermore, our VQ-Font leverages\nthe inherent design of Chinese characters, where structure components such as\nradicals and character components are combined in specific arrangements, to\nrecalibrate fine-grained styles based on references. This process improves the\nmatching and fusion of styles at the structure level. Both modules collaborate\nto enhance the fidelity of the generated fonts. Experiments on a collected font\ndataset show that our VQ-Font outperforms the competing methods both\nquantitatively and qualitatively, especially in generating challenging styles.\n", "rewritten_text": "Generating fonts with few reference samples is a difficult task, as it requires capturing intricate stroke styles from a limited set of glyphs and transferring them to other characters with similar styles. Existing methods often produce synthesized glyphs with noticeable flaws like missing details and distorted strokes due to the diverse and complex nature of Chinese font styles. In this study, we introduce a VQGAN-based framework called VQ-Font to improve glyph fidelity through token prior refinement and structure-aware enhancement. Initially, a VQGAN is pre-trained to encode font token priors in a codebook. Subsequently, VQ-Font refines the synthesized glyphs using this codebook to bridge the gap between synthesized and real-world strokes. Additionally, VQ-Font utilizes the inherent structure of Chinese characters, such as radicals and character components arranged in specific ways, to adjust stroke styles based on references, enhancing style matching and fusion at the structural level. These modules work together to enhance the fidelity of generated fonts. Experimental results on a font dataset demonstrate that VQ-Font surpasses other methods both quantitatively and qualitatively, particularly in generating challenging styles."}, "2403.19836": {"original_text": "  Identifying the targets of hate speech is a crucial step in grasping the\nnature of such speech and, ultimately, in improving the detection of offensive\nposts on online forums. Much harmful content on online platforms uses implicit\nlanguage especially when targeting vulnerable and protected groups such as\nusing stereotypical characteristics instead of explicit target names, making it\nharder to detect and mitigate the language. In this study, we focus on\nidentifying implied targets of hate speech, essential for recognizing subtler\nhate speech and enhancing the detection of harmful content on digital\nplatforms. We define a new task aimed at identifying the targets even when they\nare not explicitly stated. To address that task, we collect and annotate target\nspans in three prominent implicit hate speech datasets: SBIC, DynaHate, and\nIHC. We call the resulting merged collection Implicit-Target-Span. The\ncollection is achieved using an innovative pooling method with matching scores\nbased on human annotations and Large Language Models (LLMs). Our experiments\nindicate that Implicit-Target-Span provides a challenging test bed for target\nspan detection methods.\n", "rewritten_text": "Identifying the recipients of hate speech is a crucial step in understanding the nature of such speech and, ultimately, in improving the identification of offensive posts on online forums. Much harmful content on online platforms utilizes implicit language, particularly when targeting vulnerable and protected groups by using stereotypical characteristics instead of explicit names. This makes it more difficult to detect and address the harmful language. This study focuses on identifying implied targets of hate speech, which is essential for recognizing more subtle forms of hate speech and improving the detection of harmful content on digital platforms. A new task is defined to identify targets even when they are not explicitly mentioned. To tackle this task, target spans are collected and annotated in three prominent implicit hate speech datasets: SBIC, DynaHate, and IHC. The resulting merged collection, named Implicit-Target-Span, is created using an innovative pooling method that considers matching scores based on human annotations and Large Language Models (LLMs). Experiments show that Implicit-Target-Span serves as a challenging test environment for target span detection methods."}, "2303.05955": {"original_text": "  Remote photoplethysmography (rPPG) technology has drawn increasing attention\nin recent years. It can extract Blood Volume Pulse (BVP) from facial videos,\nmaking many applications like health monitoring and emotional analysis more\naccessible. However, as the BVP signal is easily affected by environmental\nchanges, existing methods struggle to generalize well for unseen domains. In\nthis paper, we systematically address the domain shift problem in the rPPG\nmeasurement task. We show that most domain generalization methods do not work\nwell in this problem, as domain labels are ambiguous in complicated\nenvironmental changes. In light of this, we propose a domain-label-free\napproach called NEuron STructure modeling (NEST). NEST improves the\ngeneralization capacity by maximizing the coverage of feature space during\ntraining, which reduces the chance for under-optimized feature activation\nduring inference. Besides, NEST can also enrich and enhance domain invariant\nfeatures across multi-domain. We create and benchmark a large-scale domain\ngeneralization protocol for the rPPG measurement task. Extensive experiments\nshow that our approach outperforms the state-of-the-art methods on both\ncross-dataset and intra-dataset settings.\n", "rewritten_text": "In recent years, there has been a growing interest in Remote Photoplethysmography (rPPG) technology. This technology can analyze Blood Volume Pulse (BVP) from facial videos, enabling various applications such as health monitoring and emotional analysis to be more accessible. However, the BVP signal is susceptible to environmental influences, posing a challenge for existing methods to perform well across different domains. This paper addresses the domain shift issue in rPPG measurement systematically. It is noted that conventional domain generalization methods struggle due to the ambiguity of domain labels in complex environmental conditions. To tackle this, a domain-label-free approach called NEuron STructure modeling (NEST) is proposed. NEST enhances generalization by maximizing feature space coverage during training, reducing the likelihood of suboptimal feature activation during inference. Additionally, NEST can enhance domain invariant features across multiple domains. A comprehensive domain generalization protocol for the rPPG measurement task is developed and evaluated. Results from extensive experiments demonstrate that the proposed approach surpasses current state-of-the-art methods in both cross-dataset and intra-dataset scenarios."}, "2401.06957": {"original_text": "  As virtual environments continue to advance, the demand for immersive and\nemotionally engaging experiences has grown. Addressing this demand, we\nintroduce Emotion enabled Virtual avatar mapping using Optimized KnowledgE\ndistillation (EVOKE), a lightweight emotion recognition framework designed for\nthe seamless integration of emotion recognition into 3D avatars within virtual\nenvironments. Our approach leverages knowledge distillation involving\nmulti-label classification on the publicly available DEAP dataset, which covers\nvalence, arousal, and dominance as primary emotional classes. Remarkably, our\ndistilled model, a CNN with only two convolutional layers and 18 times fewer\nparameters than the teacher model, achieves competitive results, boasting an\naccuracy of 87% while demanding far less computational resources. This\nequilibrium between performance and deployability positions our framework as an\nideal choice for virtual environment systems. Furthermore, the multi-label\nclassification outcomes are utilized to map emotions onto custom-designed 3D\navatars.\n", "rewritten_text": "\"As virtual environments progress, there is a growing need for immersive and emotionally captivating experiences. To meet this demand, we present EVOKE (Emotion enabled Virtual avatar mapping using Optimized KnowledgE distillation), a lightweight emotion recognition framework tailored for integrating emotion recognition into 3D avatars in virtual settings. Our method utilizes knowledge distillation through multi-label classification on the DEAP dataset, which includes valence, arousal, and dominance as key emotional categories. Notably, our distilled model, a CNN with just two convolutional layers and significantly fewer parameters than the teacher model, achieves competitive results with an accuracy of 87% while requiring fewer computational resources. This balance between performance and usability makes our framework an excellent choice for virtual environment systems. Additionally, the multi-label classification results are used to assign emotions to custom-designed 3D avatars.\""}, "2108.07638": {"original_text": "  Affective Computing is the study of how computers can recognize, interpret\nand simulate human affects. Sentiment Analysis is a common task inNLP related\nto this topic, but it focuses only on emotion valence (positive, negative,\nneutral). An emerging approach in NLP is Emotion Recognition, which relies on\nfined-grained classification. This research describes an approach to create a\nlexical-based weakly supervised corpus for fine-grained emotion in Portuguese.\nWe evaluated our dataset by fine-tuning a transformer-based language model\n(BERT) and validating it on a Gold Standard annotated validation set. Our\nresults (F1-score=.64) suggest lexical-based weak supervision as an appropriate\nstrategy for initial work in low resourced environment.\n", "rewritten_text": "Affective Computing involves exploring how computers can understand and replicate human emotions. While Sentiment Analysis in NLP focuses on emotion valence (positive, negative, neutral), a newer approach called Emotion Recognition delves into more detailed emotion classification. This study presents a method for creating a lexicon-based weakly supervised corpus for fine-grained emotion analysis in Portuguese. The dataset was evaluated by fine-tuning a transformer-based language model (BERT) and testing it on a Gold Standard annotated validation set. The results (F1-score=.64) indicate that lexicon-based weak supervision is a suitable strategy for initial work in resource-limited settings."}, "2408.01653": {"original_text": "  We introduce Multi-Cylindrical Panoramic Depth Estimation (MCPDepth), a\ntwo-stage framework for omnidirectional depth estimation via stereo matching\nbetween multiple cylindrical panoramas. MCPDepth uses cylindrical panoramas for\ninitial stereo matching and then fuses the resulting depth maps across views. A\ncircular attention module is employed to overcome the distortion along the\nvertical axis. MCPDepth exclusively utilizes standard network components,\nsimplifying deployment to embedded devices and outperforming previous methods\nthat require custom kernels. We theoretically and experimentally compare\nspherical and cylindrical projections for stereo matching, highlighting the\nadvantages of the cylindrical projection. MCPDepth achieves state-of-the-art\nperformance with an 18.8% reduction in mean absolute error (MAE) for depth on\nthe outdoor synthetic dataset Deep360 and a 19.9% reduction on the indoor\nreal-scene dataset 3D60.\n", "rewritten_text": "We present MCPDepth, a two-stage framework for omnidirectional depth estimation using stereo matching on multiple cylindrical panoramas. The method first performs stereo matching on cylindrical panoramas and then combines the resulting depth maps. A circular attention module is used to address distortion along the vertical axis. MCPDepth relies on standard network components, making it easy to deploy on embedded devices and surpassing methods that require custom kernels. We compare spherical and cylindrical projections for stereo matching both theoretically and experimentally, demonstrating the benefits of cylindrical projection. MCPDepth achieves state-of-the-art performance with an 18.8% reduction in mean absolute error (MAE) on the Deep360 outdoor synthetic dataset and a 19.9% reduction on the 3D60 indoor real-scene dataset."}, "2111.09091": {"original_text": "  Monitoring behaviour in smart homes using sensors can offer insights into\nchanges in the independent ability and long-term health of residents. Passive\nInfrared motion sensors (PIRs) are standard, however may not accurately track\nthe full duration of movement. They also require line-of-sight to detect motion\nwhich can restrict performance and ensures they must be visible to residents.\nChannel State Information (CSI) is a low cost, unintrusive form of radio\nsensing which can monitor movement but also offers opportunities to generate\nrich data. We have developed a novel, self-calibrating motion detection system\nwhich uses CSI data collected and processed on a stock Raspberry Pi 4. This\nsystem exploits the correlation between CSI frames, on which we perform\nvariance analysis using our algorithm to accurately measure the full period of\na resident's movement. We demonstrate the effectiveness of this approach in\nseveral real-world environments. Experiments conducted demonstrate that\nactivity start and end time can be accurately detected for motion examples of\ndifferent intensities at different locations.\n", "rewritten_text": "\"Utilizing sensors in smart homes to monitor resident behavior can provide valuable insights into changes in their independence and long-term health. While Passive Infrared motion sensors (PIRs) are commonly used, they may not capture the complete duration of movement accurately and require a direct line of sight, limiting their performance and visibility to residents. Channel State Information (CSI) presents a cost-effective and non-intrusive radio sensing method that not only tracks movement but also generates detailed data. Our innovative motion detection system, developed using CSI data processed on a Raspberry Pi 4, leverages the correlation between CSI frames. Through variance analysis with our algorithm, we can precisely measure the entire duration of a resident's movement. Our approach has been successfully tested in various real-world settings, demonstrating accurate detection of activity start and end times for movements of different intensities and locations.\""}, "2405.02296": {"original_text": "  Perspective distortion (PD) causes unprecedented changes in shape, size,\norientation, angles, and other spatial relationships of visual concepts in\nimages. Precisely estimating camera intrinsic and extrinsic parameters is a\nchallenging task that prevents synthesizing perspective distortion.\nNon-availability of dedicated training data poses a critical barrier to\ndeveloping robust computer vision methods. Additionally, distortion correction\nmethods make other computer vision tasks a multi-step approach and lack\nperformance. In this work, we propose mitigating perspective distortion (MPD)\nby employing a fine-grained parameter control on a specific family of M\\\"obius\ntransform to model real-world distortion without estimating camera intrinsic\nand extrinsic parameters and without the need for actual distorted data. Also,\nwe present a dedicated perspectively distorted benchmark dataset, ImageNet-PD,\nto benchmark the robustness of deep learning models against this new dataset.\nThe proposed method outperforms existing benchmarks, ImageNet-E and ImageNet-X.\nAdditionally, it significantly improves performance on ImageNet-PD while\nconsistently performing on standard data distribution. Notably, our method\nshows improved performance on three PD-affected real-world applications crowd\ncounting, fisheye image recognition, and person re-identification and one\nPD-affected challenging CV task: object detection. The source code, dataset,\nand models are available on the project webpage at\nhttps://prakashchhipa.github.io/projects/mpd.\n", "rewritten_text": "Perspective distortion (PD) results in significant alterations to the shape, size, orientation, angles, and spatial relationships of visual elements in images. Accurately determining camera intrinsic and extrinsic parameters is a difficult task that hinders the synthesis of perspective distortion. The lack of specific training data presents a major obstacle to the development of reliable computer vision techniques. Moreover, existing distortion correction methods complicate other computer vision tasks and exhibit subpar performance. In this study, we introduce a method called mitigating perspective distortion (MPD) that utilizes precise parameter control on a particular family of M\\\"obius transforms to simulate real-world distortion without the need to estimate camera parameters or use distorted data. We also introduce a new benchmark dataset, ImageNet-PD, to evaluate the resilience of deep learning models against this novel dataset. Our proposed approach surpasses current benchmarks, ImageNet-E and ImageNet-X, and notably enhances performance on ImageNet-PD while maintaining consistency on standard data distributions. Importantly, our method demonstrates improved performance on various real-world applications affected by perspective distortion, including crowd counting, fisheye image recognition, person re-identification, and the challenging computer vision task of object detection. The project webpage at https://prakashchhipa.github.io/projects/mpd provides access to the source code, dataset, and models."}, "1705.08479": {"original_text": "  This paper introduces a new architectural framework, known as input\nfast-forwarding, that can enhance the performance of deep networks. The main\nidea is to incorporate a parallel path that sends representations of input\nvalues forward to deeper network layers. This scheme is substantially different\nfrom \"deep supervision\" in which the loss layer is re-introduced to earlier\nlayers. The parallel path provided by fast-forwarding enhances the training\nprocess in two ways. First, it enables the individual layers to combine\nhigher-level information (from the standard processing path) with lower-level\ninformation (from the fast-forward path). Second, this new architecture reduces\nthe problem of vanishing gradients substantially because the fast-forwarding\npath provides a shorter route for gradient backpropagation. In order to\nevaluate the utility of the proposed technique, a Fast-Forward Network (FFNet),\nwith 20 convolutional layers along with parallel fast-forward paths, has been\ncreated and tested. The paper presents empirical results that demonstrate\nimproved learning capacity of FFNet due to fast-forwarding, as compared to\nGoogLeNet (with deep supervision) and CaffeNet, which are 4x and 18x larger in\nsize, respectively. All of the source code and deep learning models described\nin this paper will be made available to the entire research community\n", "rewritten_text": "This paper introduces a novel architectural framework called input fast-forwarding, aimed at boosting the performance of deep networks. The key concept involves integrating a parallel path that forwards representations of input values to deeper network layers. This approach differs significantly from \"deep supervision,\" where the loss layer is reintroduced to earlier layers. The parallel path introduced by fast-forwarding enhances the training process in two key ways. Firstly, it allows individual layers to merge higher-level information from the standard processing path with lower-level information from the fast-forward path. Secondly, this new architecture addresses the issue of vanishing gradients by providing a shorter route for gradient backpropagation through the fast-forwarding path. To assess the effectiveness of this technique, a Fast-Forward Network (FFNet) comprising 20 convolutional layers with parallel fast-forward paths was developed and tested. The paper presents empirical results demonstrating the enhanced learning capacity of FFNet due to fast-forwarding, compared to GoogLeNet (with deep supervision) and CaffeNet, which are 4x and 18x larger in size, respectively. The source code and deep learning models described in this paper will be made available to the research community."}, "2002.06515": {"original_text": "  Automatic analysis of highly crowded people has attracted extensive attention\nfrom computer vision research. Previous approaches for crowd counting have\nalready achieved promising performance across various benchmarks. However, to\ndeal with the real situation, we hope the model run as fast as possible while\nkeeping accuracy. In this paper, we propose a compact convolutional neural\nnetwork for crowd counting which learns a more efficient model with a small\nnumber of parameters. With three parallel filters executing the convolutional\noperation on the input image simultaneously at the front of the network, our\nmodel could achieve nearly real-time speed and save more computing resources.\nExperiments on two benchmarks show that our proposed method not only takes a\nbalance between performance and efficiency which is more suitable for actual\nscenes but also is superior to existing light-weight models in speed.\n", "rewritten_text": "The automatic analysis of densely populated areas has garnered significant attention in computer vision research. While previous methods for counting crowds have shown promising results on various benchmarks, there is a need for models that can operate quickly without sacrificing accuracy in real-world scenarios. In this study, we introduce a compact convolutional neural network for crowd counting that prioritizes efficiency by utilizing a small number of parameters. By employing three parallel filters to perform convolution operations simultaneously at the beginning of the network, our model achieves nearly real-time processing speeds and conserves computational resources. Experimental results on two benchmarks demonstrate that our approach strikes a balance between performance and efficiency, making it well-suited for practical applications and outperforming existing lightweight models in terms of speed."}, "1711.03278": {"original_text": "  Convolution Neural Networks (CNN), known as ConvNets are widely used in many\nvisual imagery application, object classification, speech recognition. After\nthe implementation and demonstration of the deep convolution neural network in\nImagenet classification in 2012 by krizhevsky, the architecture of deep\nConvolution Neural Network is attracted many researchers. This has led to the\nmajor development in Deep learning frameworks such as Tensorflow, caffe, keras,\ntheno. Though the implementation of deep learning is quite possible by\nemploying deep learning frameworks, mathematical theory and concepts are harder\nto understand for new learners and practitioners. This article is intended to\nprovide an overview of ConvNets architecture and to explain the mathematical\ntheory behind it including activation function, loss function, feedforward and\nbackward propagation. In this article, grey scale image is taken as input\ninformation image, ReLU and Sigmoid activation function are considered for\ndeveloping the architecture and cross-entropy loss function are used for\ncomputing the difference between predicted value and actual value. The\narchitecture is developed in such a way that it can contain one convolution\nlayer, one pooling layer, and multiple dense layers\n", "rewritten_text": "Convolutional Neural Networks (CNNs), also known as ConvNets, are widely utilized in various visual applications such as object classification and speech recognition. Since the introduction of deep Convolutional Neural Networks in the Imagenet classification by Krizhevsky in 2012, the architecture has garnered significant attention from researchers. This has resulted in substantial advancements in Deep Learning frameworks like TensorFlow, Caffe, Keras, and Theano. While implementing deep learning using these frameworks is feasible, understanding the underlying mathematical theories and concepts can be challenging for newcomers and practitioners. This article aims to provide an overview of ConvNets' architecture and elucidate the mathematical principles behind it, including activation functions, loss functions, feedforward, and backpropagation. The article uses grayscale images as input data, employs ReLU and Sigmoid activation functions in the architecture, and utilizes cross-entropy loss functions to measure the variance between predicted and actual values. The architecture is designed to include a convolutional layer, a pooling layer, and multiple dense layers."}, "1610.07995": {"original_text": "  In this paper, the task of recognizing signs made by hearing impaired people\nat sentence level has been addressed. A novel method of extracting spatial\nfeatures to capture hand movements of a signer has been proposed. Frames of a\ngiven video of a sign are preprocessed to extract face and hand components of a\nsigner. The local centroids of the extracted components along with the global\ncentroid are exploited to extract spatial features. The concept of interval\nvalued type symbolic data has been explored to capture variations in the same\nsign made by the different signers at different instances of time. A suitable\nsymbolic similarity measure is studied to establish matching between test and\nreference signs and a simple nearest neighbour classifier is used to recognize\nan unknown sign as one among the known signs by specifying a desired level of\nthreshold. An extensive experimentation is conducted on a considerably large\ndatabase of signs created by us during the course of research work in order to\nevaluate the performance of the proposed system\n", "rewritten_text": "This paper addresses the task of recognizing gestures made by individuals with hearing impairments at the sentence level. A new method is proposed for extracting spatial features to capture the hand movements of a signer. The frames of a given sign video are preprocessed to isolate the face and hand components of the signer. The local centroids of these components, along with the global centroid, are used to extract spatial features. The study explores the use of interval-valued symbolic data to capture variations in the same sign made by different signers at different times. A symbolic similarity measure is examined to establish a match between test and reference signs, and a simple nearest neighbor classifier is employed to recognize an unknown sign as one of the known signs by setting a desired threshold level. Extensive experiments are conducted on a large database of signs created during the research to evaluate the performance of the proposed system."}, "1908.04501": {"original_text": "  When a deep neural network is trained on data with only image-level labeling,\nthe regions activated in each image tend to identify only a small region of the\ntarget object. We propose a method of using videos automatically harvested from\nthe web to identify a larger region of the target object by using temporal\ninformation, which is not present in the static image. The temporal variations\nin a video allow different regions of the target object to be activated. We\nobtain an activated region in each frame of a video, and then aggregate the\nregions from successive frames into a single image, using a warping technique\nbased on optical flow. The resulting localization maps cover more of the target\nobject, and can then be used as proxy ground-truth to train a segmentation\nnetwork. This simple approach outperforms existing methods under the same level\nof supervision, and even approaches relying on extra annotations. Based on\nVGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4,\nrespectively, on PASCAL VOC 2012 test images, which represents a new\nstate-of-the-art.\n", "rewritten_text": "\"When training a deep neural network using data labeled only at the image level, the activated regions in each image typically only identify a small portion of the target object. Our proposed method involves utilizing videos obtained from the web to capture a larger region of the target object by leveraging temporal information, which is absent in static images. The temporal variations in videos enable different parts of the target object to be activated. By extracting activated regions in each frame of a video and combining them using a warping technique based on optical flow, we generate localization maps that cover more of the target object. These maps can serve as approximate ground-truth data for training a segmentation network. This straightforward approach surpasses existing methods with the same level of supervision and even rivals approaches that rely on additional annotations. Using VGG-16 and ResNet 101 backbones, our method achieves mIoU scores of 65.0 and 67.4, respectively, on PASCAL VOC 2012 test images, establishing a new state-of-the-art.\""}, "2203.01532": {"original_text": "  Recently, contrastive learning-based image translation methods have been\nproposed, which contrasts different spatial locations to enhance the spatial\ncorrespondence. However, the methods often ignore the diverse semantic relation\nwithin the images. To address this, here we propose a novel semantic relation\nconsistency (SRC) regularization along with the decoupled contrastive learning,\nwhich utilize the diverse semantics by focusing on the heterogeneous semantics\nbetween the image patches of a single image. To further improve the\nperformance, we present a hard negative mining by exploiting the semantic\nrelation. We verified our method for three tasks: single-modal and multi-modal\nimage translations, and GAN compression task for image translation.\nExperimental results confirmed the state-of-art performance of our method in\nall the three tasks.\n", "rewritten_text": "Recently, new methods for image translation based on contrastive learning have emerged. These methods aim to enhance spatial correspondence by contrasting different spatial locations. However, they often overlook the varied semantic relationships present within images. To tackle this issue, we introduce a novel approach called semantic relation consistency (SRC) regularization in conjunction with decoupled contrastive learning. This approach leverages diverse semantics by focusing on the heterogeneous semantic connections between patches within a single image. Additionally, we incorporate hard negative mining to further enhance performance by exploiting semantic relationships. Our method was evaluated across three tasks: single-modal and multi-modal image translations, as well as GAN compression for image translation. Experimental results demonstrate the superior performance of our method in all three tasks."}, "2310.03414": {"original_text": "  Multi-document summarization is a challenging task due to its inherent\nsubjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4\namong DUC-2004 reference summaries. In this work, we aim to enhance the\nobjectivity of news summarization by focusing on the main event of a group of\nrelated news documents and presenting it coherently with sufficient context.\nOur primary objective is to succinctly report the main event, ensuring that the\nsummary remains objective and informative. To achieve this, we employ an\nextract-rewrite approach that incorporates a main-event biased\nmonotone-submodular function for content selection. This enables us to extract\nthe most crucial information related to the main event from the document\ncluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for\nrewriting the extracted content into a coherent text. The evaluation using\nobjective metrics and human evaluators confirms the effectiveness of our\napproach, as it surpasses potential baselines, demonstrating excellence in both\ncontent coverage, coherence, and informativeness.\n", "rewritten_text": "Multi-document summarization presents a challenge due to its subjective bias, as shown by a low inter-annotator ROUGE-1 score of 0.4 in DUC-2004 reference summaries. This study aims to improve the objectivity of news summarization by focusing on the main event in a group of related news documents and presenting it cohesively with ample context. The main goal is to concisely report the main event while maintaining objectivity and informativeness. To achieve this, an extract-rewrite approach is utilized, incorporating a main-event biased monotone-submodular function for content selection. This method allows for extracting crucial information related to the main event from the document cluster. To ensure coherence, a fine-tuned Language Model (LLM) is employed to rewrite the extracted content into a coherent text. Evaluation using objective metrics and human evaluators validates the effectiveness of this approach, surpassing potential baselines and excelling in content coverage, coherence, and informativeness."}, "1608.04314": {"original_text": "  We present a technique for weakly supervised object localization (WSOL),\nbuilding on the observation that WSOL algorithms usually work better on images\nwith bigger objects. Instead of training the object detector on the entire\ntraining set at the same time, we propose a curriculum learning strategy to\nfeed training images into the WSOL learning loop in an order from images\ncontaining bigger objects down to smaller ones. To automatically determine the\norder, we train a regressor to estimate the size of the object given the whole\nimage as input. Furthermore, we use these size estimates to further improve the\nre-localization step of WSOL by assigning weights to object proposals according\nto how close their size matches the estimated object size. We demonstrate the\neffectiveness of using size order and size weighting on the challenging PASCAL\nVOC 2007 dataset, where we achieve a significant improvement over existing\nstate-of-the-art WSOL techniques.\n", "rewritten_text": "We introduce a method for weakly supervised object localization (WSOL) that capitalizes on the fact that WSOL algorithms tend to perform better on images featuring larger objects. Instead of training the object detector on the entire training set simultaneously, we suggest a curriculum learning approach where training images are fed into the WSOL learning loop in a sequence starting from images with larger objects and progressing to smaller ones. To determine this sequence automatically, we train a regressor to predict the object size based on the entire image. Additionally, we utilize these size predictions to enhance the re-localization step of WSOL by assigning weights to object proposals based on how closely their size matches the estimated object size. Our experiments on the challenging PASCAL VOC 2007 dataset demonstrate the effectiveness of incorporating size order and size weighting, resulting in a significant performance boost compared to existing state-of-the-art WSOL techniques."}, "2402.00160": {"original_text": "  In this work, we introduce the Multiple Embedding Model for EHR (MEME), an\napproach that serializes multimodal EHR tabular data into text using\npseudo-notes, mimicking clinical text generation. This conversion not only\npreserves better representations of categorical data and learns contexts but\nalso enables the effective employment of pretrained foundation models for rich\nfeature representation. To address potential issues with context length, our\nframework encodes embeddings for each EHR modality separately. We demonstrate\nthe effectiveness of MEME by applying it to several decision support tasks\nwithin the Emergency Department across multiple hospital systems. Our findings\nindicate that MEME outperforms traditional machine learning, EHR-specific\nfoundation models, and general LLMs, highlighting its potential as a general\nand extendible EHR representation strategy.\n", "rewritten_text": "\"In this study, we present the Multiple Embedding Model for Electronic Health Records (MEME), a method that converts multimodal EHR tabular data into text format using pseudo-notes to simulate clinical text generation. This conversion not only maintains more accurate representations of categorical data and captures contextual information, but also allows for the effective utilization of pretrained foundation models for comprehensive feature representation. To overcome potential challenges related to context length, our framework encodes embeddings for each EHR modality separately. We showcase the effectiveness of MEME by applying it to various decision support tasks in Emergency Departments across multiple hospital systems. Our results demonstrate that MEME performs better than traditional machine learning approaches, EHR-specific foundation models, and general Large Language Models (LLMs), underscoring its potential as a versatile and scalable EHR representation approach.\""}, "2305.11853": {"original_text": "  Large language models (LLMs) with in-context learning have demonstrated\nremarkable capability in the text-to-SQL task. Previous research has prompted\nLLMs with various demonstration-retrieval strategies and intermediate reasoning\nsteps to enhance the performance of LLMs. However, those works often employ\nvaried strategies when constructing the prompt text for text-to-SQL inputs,\nsuch as databases and demonstration examples. This leads to a lack of\ncomparability in both the prompt constructions and their primary contributions.\nFurthermore, selecting an effective prompt construction has emerged as a\npersistent problem for future research. To address this limitation, we\ncomprehensively investigate the impact of prompt constructions across various\nsettings and provide insights into prompt constructions for future text-to-SQL\nstudies.\n", "rewritten_text": "\"Recent advancements in large language models (LLMs) utilizing in-context learning have shown impressive performance in the text-to-SQL task. Previous studies have explored different strategies, including demonstration-retrieval methods and intermediate reasoning steps, to improve LLM performance. However, these studies often use diverse approaches in constructing prompt texts for text-to-SQL inputs, such as databases and demonstration examples, leading to challenges in comparability and understanding their key contributions. The selection of an effective prompt construction remains a persistent issue for future research. To address this challenge, we conduct a comprehensive investigation into the impact of prompt constructions across various scenarios and offer insights for future text-to-SQL studies.\""}, "1007.0621": {"original_text": "  In this paper fusion of visual and thermal images in wavelet transformed\ndomain has been presented. Here, Daubechies wavelet transform, called as D2,\ncoefficients from visual and corresponding coefficients computed in the same\nmanner from thermal images are combined to get fused coefficients. After\ndecomposition up to fifth level (Level 5) fusion of coefficients is done.\nInverse Daubechies wavelet transform of those coefficients gives us fused face\nimages. The main advantage of using wavelet transform is that it is well-suited\nto manage different image resolution and allows the image decomposition in\ndifferent kinds of coefficients, while preserving the image information. Fused\nimages thus found are passed through Principal Component Analysis (PCA) for\nreduction of dimensions and then those reduced fused images are classified\nusing a multi-layer perceptron. For experiments IRIS Thermal/Visual Face\nDatabase was used. Experimental results show that the performance of the\napproach presented here achieves maximum success rate of 100% in many cases.\n", "rewritten_text": "This paper introduces a method for combining visual and thermal images using wavelet transformation. The Daubechies wavelet transform (D2) is applied to the visual and thermal images to obtain coefficients, which are then fused. The fused coefficients are obtained by combining the corresponding coefficients from both types of images after decomposition up to the fifth level. The inverse Daubechies wavelet transform is then applied to generate fused face images. The advantage of using wavelet transform is its ability to handle different image resolutions and decompose images into various coefficients while preserving image information. The fused images are further processed using Principal Component Analysis (PCA) to reduce dimensions, and then classified using a multi-layer perceptron. The experimental evaluation was conducted using the IRIS Thermal/Visual Face Database, showing that the proposed approach achieved a maximum success rate of 100% in many cases."}, "2210.11725": {"original_text": "  We present AROS, a one-shot learning approach that uses an explicit\nrepresentation of interactions between highly-articulated human poses and 3D\nscenes. The approach is one-shot as the method does not require re-training to\nadd new affordance instances. Furthermore, only one or a small handful of\nexamples of the target pose are needed to describe the interaction. Given a 3D\nmesh of a previously unseen scene, we can predict affordance locations that\nsupport the interactions and generate corresponding articulated 3D human bodies\naround them. We evaluate on three public datasets of scans of real environments\nwith varied degrees of noise. Via rigorous statistical analysis of crowdsourced\nevaluations, results show that our one-shot approach outperforms data-intensive\nbaselines by up to 80\\%.\n", "rewritten_text": "We introduce AROS, a one-shot learning method that utilizes a clear representation of interactions between detailed human poses and 3D environments. This approach is considered one-shot because it does not necessitate re-training for adding new affordance instances. Additionally, only one or a few examples of the target pose are required to describe the interaction. By providing a 3D mesh of an unfamiliar scene, we can anticipate affordance locations that facilitate interactions and create corresponding detailed 3D human figures around them. Our evaluation is conducted on three public datasets containing scans of real environments with varying levels of noise. Through thorough statistical analysis of evaluations from a crowd, the results demonstrate that our one-shot method surpasses data-heavy baselines by up to 80%."}, "1604.06646": {"original_text": "  In this paper we introduce a new method for text detection in natural images.\nThe method comprises two contributions: First, a fast and scalable engine to\ngenerate synthetic images of text in clutter. This engine overlays synthetic\ntext to existing background images in a natural way, accounting for the local\n3D scene geometry. Second, we use the synthetic images to train a\nFully-Convolutional Regression Network (FCRN) which efficiently performs text\ndetection and bounding-box regression at all locations and multiple scales in\nan image. We discuss the relation of FCRN to the recently-introduced YOLO\ndetector, as well as other end-to-end object detection systems based on deep\nlearning. The resulting detection network significantly out performs current\nmethods for text detection in natural images, achieving an F-measure of 84.2%\non the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per\nsecond on a GPU.\n", "rewritten_text": "In this paper, we present a novel approach for detecting text in natural images. Our method consists of two main components. Firstly, we have developed a rapid and scalable engine that creates synthetic images of text within cluttered backgrounds. This engine seamlessly integrates synthetic text into existing background images, taking into consideration the local 3D scene geometry. Secondly, we utilize these synthetic images to train a Fully-Convolutional Regression Network (FCRN) that efficiently detects text and performs bounding-box regression across various locations and scales within an image. We explore the relationship between FCRN and other deep learning-based object detection systems, such as the YOLO detector. Our detection network surpasses current methods for text detection in natural images, achieving an impressive F-measure of 84.2% on the ICDAR 2013 benchmark. Additionally, it can process 15 images per second on a GPU."}, "2204.06028": {"original_text": "  In this paper, we describe our submission to the Simultaneous Speech\nTranslation at IWSLT 2022. We explore strategies to utilize an offline model in\na simultaneous setting without the need to modify the original model. In our\nexperiments, we show that our onlinization algorithm is almost on par with the\noffline setting while being $3\\times$ faster than offline in terms of latency\non the test set. We also show that the onlinized offline model outperforms the\nbest IWSLT2021 simultaneous system in medium and high latency regimes and is\nalmost on par in the low latency regime. We make our system publicly available.\n", "rewritten_text": "In this paper, we present our entry for the Simultaneous Speech Translation at IWSLT 2022. We investigate methods for using an offline model in a simultaneous context without requiring modifications to the original model. Through our experiments, we demonstrate that our onlinization technique performs nearly as well as the offline setup but is three times faster in terms of latency on the test dataset. Furthermore, we illustrate that the onlinized offline model surpasses the top IWSLT2021 simultaneous system in medium and high latency scenarios and is comparable in low latency situations. Our system is now accessible to the public."}, "2304.02715": {"original_text": "  This paper presents an unsupervised approach that leverages raw aerial videos\nto learn to estimate planar homographic transformation between consecutive\nvideo frames. Previous learning-based estimators work on pairs of images to\nestimate their planar homographic transformations but suffer from severe\nover-fitting issues, especially when applying over aerial videos. To address\nthis concern, we develop a sequential estimator that directly processes a\nsequence of video frames and estimates their pairwise planar homographic\ntransformations in batches. We also incorporate a set of spatial-temporal\nknowledge to regularize the learning of such a sequence-to-sequence model. We\ncollect a set of challenging aerial videos and compare the proposed method to\nthe alternative algorithms. Empirical studies suggest that our sequential model\nachieves significant improvement over alternative image-based methods and the\nknowledge-rich regularization further boosts our system performance. Our codes\nand dataset could be found at https://github.com/Paul-LiPu/DeepVideoHomography\n", "rewritten_text": "This paper introduces a novel unsupervised approach that utilizes raw aerial videos to learn how to estimate planar homographic transformations between consecutive frames. Unlike previous learning-based methods that focus on pairs of images, our approach processes a sequence of video frames to estimate their pairwise planar homographic transformations in batches. We also incorporate spatial-temporal knowledge to enhance the learning process. By testing our method on challenging aerial videos and comparing it to alternative algorithms, we demonstrate significant improvements in performance. The code and dataset can be accessed at https://github.com/Paul-LiPu/DeepVideoHomography."}, "2306.16925": {"original_text": "  Pretraining with large-scale 3D volumes has a potential for improving the\nsegmentation performance on a target medical image dataset where the training\nimages and annotations are limited. Due to the high cost of acquiring\npixel-level segmentation annotations on the large-scale pretraining dataset,\npretraining with unannotated images is highly desirable. In this work, we\npropose a novel self-supervised learning strategy named Volume Fusion (VF) for\npretraining 3D segmentation models. It fuses several random patches from a\nforeground sub-volume to a background sub-volume based on a predefined set of\ndiscrete fusion coefficients, and forces the model to predict the fusion\ncoefficient of each voxel, which is formulated as a self-supervised\nsegmentation task without manual annotations. Additionally, we propose a novel\nnetwork architecture based on parallel convolution and transformer blocks that\nis suitable to be transferred to different downstream segmentation tasks with\nvarious scales of organs and lesions. The proposed model was pretrained with\n110k unannotated 3D CT volumes, and experiments with different downstream\nsegmentation targets including head and neck organs, thoracic/abdominal organs\nshowed that our pretrained model largely outperformed training from scratch and\nseveral state-of-the-art self-supervised training methods and segmentation\nmodels. The code and pretrained model are available at\nhttps://github.com/openmedlab/MIS-FM.\n", "rewritten_text": "\"Utilizing large-scale 3D volumes for pretraining can enhance segmentation performance on limited medical image datasets. Given the challenges of acquiring detailed annotations, pretraining with unannotated images is preferred. This study introduces a new self-supervised learning approach called Volume Fusion (VF) for pretraining 3D segmentation models. VF combines random patches from foreground and background sub-volumes using fusion coefficients, prompting the model to predict voxel fusion coefficients without manual annotations. A novel network architecture incorporating parallel convolution and transformer blocks is proposed for transferability to diverse segmentation tasks. Pretraining the model with 110k unannotated 3D CT volumes resulted in superior performance compared to training from scratch and other self-supervised methods and models on various segmentation targets. The code and pretrained model can be accessed at https://github.com/openmedlab/MIS-FM.\""}, "2102.0932": {"original_text": "  Event cameras are novel vision sensors that report per-pixel brightness\nchanges as a stream of asynchronous \"events\". They offer significant advantages\ncompared to standard cameras due to their high temporal resolution, high\ndynamic range and lack of motion blur. However, events only measure the varying\ncomponent of the visual signal, which limits their ability to encode scene\ncontext. By contrast, standard cameras measure absolute intensity frames, which\ncapture a much richer representation of the scene. Both sensors are thus\ncomplementary. However, due to the asynchronous nature of events, combining\nthem with synchronous images remains challenging, especially for learning-based\nmethods. This is because traditional recurrent neural networks (RNNs) are not\ndesigned for asynchronous and irregular data from additional sensors. To\naddress this challenge, we introduce Recurrent Asynchronous Multimodal (RAM)\nnetworks, which generalize traditional RNNs to handle asynchronous and\nirregular data from multiple sensors. Inspired by traditional RNNs, RAM\nnetworks maintain a hidden state that is updated asynchronously and can be\nqueried at any time to generate a prediction. We apply this novel architecture\nto monocular depth estimation with events and frames where we show an\nimprovement over state-of-the-art methods by up to 30% in terms of mean\nabsolute depth error. To enable further research on multimodal learning with\nevents, we release EventScape, a new dataset with events, intensity frames,\nsemantic labels, and depth maps recorded in the CARLA simulator.\n", "rewritten_text": "Event cameras are innovative vision sensors that detect changes in brightness on a per-pixel basis, generating a continuous stream of asynchronous \"events\". They offer notable advantages over traditional cameras, such as high temporal resolution, wide dynamic range, and absence of motion blur. However, events only capture the changing aspect of the visual signal, limiting their ability to convey scene context. In contrast, standard cameras capture absolute intensity frames, providing a more comprehensive representation of the scene. Both types of sensors complement each other, but integrating asynchronous events with synchronous images poses challenges, particularly for learning-based methods. This is because conventional recurrent neural networks (RNNs) are not optimized for handling asynchronous and irregular data from multiple sensors. To tackle this issue, we introduce Recurrent Asynchronous Multimodal (RAM) networks, which extend traditional RNNs to process asynchronous and irregular data from various sensors. Drawing inspiration from RNNs, RAM networks maintain a hidden state that is updated asynchronously and can be accessed at any time to make predictions. We apply this innovative architecture to monocular depth estimation using events and frames, achieving up to a 30% improvement in mean absolute depth error compared to current methods. To facilitate further research on multimodal learning with events, we introduce EventScape, a new dataset containing events, intensity frames, semantic labels, and depth maps captured in the CARLA simulator."}, "2410.05963": {"original_text": "  Existing perception models achieve great success by learning from large\namounts of labeled data, but they still struggle with open-world scenarios. To\nalleviate this issue, researchers introduce open-set perception tasks to detect\nor segment unseen objects in the training set. However, these models require\npredefined object categories as inputs during inference, which are not\navailable in real-world scenarios. Recently, researchers pose a new and more\npractical problem, \\textit{i.e.}, open-ended object detection, which discovers\nunseen objects without any object categories as inputs. In this paper, we\npresent VL-SAM, a training-free framework that combines the generalized object\nrecognition model (\\textit{i.e.,} Vision-Language Model) with the generalized\nobject localization model (\\textit{i.e.,} Segment-Anything Model), to address\nthe open-ended object detection and segmentation task. Without additional\ntraining, we connect these two generalized models with attention maps as the\nprompts. Specifically, we design an attention map generation module by\nemploying head aggregation and a regularized attention flow to aggregate and\npropagate attention maps across all heads and layers in VLM, yielding\nhigh-quality attention maps. Then, we iteratively sample positive and negative\npoints from the attention maps with a prompt generation module and send the\nsampled points to SAM to segment corresponding objects. Experimental results on\nthe long-tail instance segmentation dataset (LVIS) show that our method\nsurpasses the previous open-ended method on the object detection task and can\nprovide additional instance segmentation masks. Besides, VL-SAM achieves\nfavorable performance on the corner case object detection dataset (CODA),\ndemonstrating the effectiveness of VL-SAM in real-world applications. Moreover,\nVL-SAM exhibits good model generalization that can incorporate various VLMs and\nSAMs.\n", "rewritten_text": "The current perception models have been successful in learning from large amounts of labeled data, but they face challenges in open-world scenarios. To address this issue, researchers have introduced open-set perception tasks to identify unseen objects in the training set. However, these models rely on predefined object categories during inference, which are not always available in real-world situations. A new practical problem called open-ended object detection has been proposed, aiming to detect unseen objects without predefined categories. This paper introduces VL-SAM, a framework that combines a generalized object recognition model (Vision-Language Model) with a generalized object localization model (Segment-Anything Model) to tackle open-ended object detection and segmentation tasks. By connecting these two models using attention maps as prompts, VL-SAM generates high-quality attention maps through an attention map generation module. The method then samples points from these attention maps and sends them to SAM for object segmentation. Experimental results on the LVIS dataset demonstrate that VL-SAM outperforms previous methods in object detection and provides additional instance segmentation masks. Additionally, VL-SAM shows promising results on the CODA dataset, showcasing its effectiveness in real-world applications and its ability to generalize across various VLMs and SAMs."}, "1805.05421": {"original_text": "  Deep learning has made significant improvements at many image processing\ntasks in recent years, such as image classification, object recognition and\nobject detection. Convolutional neural networks (CNN), which is a popular deep\nlearning architecture designed to process data in multiple array form, show\ngreat success to almost all detection \\& recognition problems and computer\nvision tasks. However, the number of parameters in a CNN is too high such that\nthe computers require more energy and larger memory size. In order to solve\nthis problem, we propose a novel energy efficient model Binary Weight and\nHadamard-transformed Image Network (BWHIN), which is a combination of Binary\nWeight Network (BWN) and Hadamard-transformed Image Network (HIN). It is\nobserved that energy efficiency is achieved with a slight sacrifice at\nclassification accuracy. Among all energy efficient networks, our novel\nensemble model outperforms other energy efficient models.\n", "rewritten_text": "In recent years, deep learning has shown significant advancements in various image processing tasks like image classification, object recognition, and object detection. Convolutional neural networks (CNN) have been particularly successful in addressing detection and recognition challenges in computer vision by processing data in array form. However, CNNs often have a high number of parameters, leading to increased energy consumption and memory requirements. To address this issue, we introduce a new energy-efficient model called Binary Weight and Hadamard-transformed Image Network (BWHIN), which combines Binary Weight Network (BWN) and Hadamard-transformed Image Network (HIN). Our research demonstrates that BWHIN achieves energy efficiency with minimal impact on classification accuracy. When compared to other energy-efficient models, our novel ensemble model stands out as the top performer."}, "2001.11561": {"original_text": "  We consider referring image segmentation. It is a problem at the intersection\nof computer vision and natural language understanding. Given an input image and\na referring expression in the form of a natural language sentence, the goal is\nto segment the object of interest in the image referred by the linguistic\nquery. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to\ntackle this problem. Our model consists of an encoder network and a decoder\nnetwork, where ConvLSTM is used in both encoder and decoder networks to capture\nspatial and sequential information. The encoder network extracts visual and\nlinguistic features for each word in the expression sentence, and adopts an\nattention mechanism to focus on words that are more informative in the\nmultimodal interaction. The decoder network integrates the features generated\nby the encoder network at multiple levels as its input and produces the final\nprecise segmentation mask. Experimental results on four challenging datasets\ndemonstrate that the proposed network achieves superior segmentation\nperformance compared with other state-of-the-art methods.\n", "rewritten_text": "We are discussing image segmentation with reference to natural language. This involves using a natural language sentence to identify and segment a specific object in an image. Our approach involves a dual convolutional LSTM (ConvLSTM) network, which includes an encoder and a decoder network. The ConvLSTM is utilized in both networks to capture spatial and sequential information. The encoder network extracts visual and linguistic features from the input sentence, using an attention mechanism to focus on key words. The decoder network combines these features from the encoder network to generate the final segmentation mask. Results from experiments on challenging datasets show that our network outperforms other current methods in segmentation accuracy."}, "1903.07224": {"original_text": "  This work develops a novel end-to-end deep unsupervised learning method based\non convolutional neural network (CNN) with pseudo-classes for remote sensing\nscene representation. First, we introduce center points as the centers of the\npseudo classes and the training samples can be allocated with pseudo labels\nbased on the center points. Therefore, the CNN model, which is used to extract\nfeatures from the scenes, can be trained supervised with the pseudo labels.\nMoreover, a pseudo-center loss is developed to decrease the variance between\nthe samples and the corresponding pseudo center point. The pseudo-center loss\nis important since it can update both the center points with the training\nsamples and the CNN model with the center points in the training process\nsimultaneously. Finally, joint learning of the pseudo-center loss and the\npseudo softmax loss which is formulated with the samples and the pseudo labels\nis developed for unsupervised remote sensing scene representation to obtain\ndiscriminative representations from the scenes. Experiments are conducted over\ntwo commonly used remote sensing scene datasets to validate the effectiveness\nof the proposed method and the experimental results show the superiority of the\nproposed method when compared with other state-of-the-art methods.\n", "rewritten_text": "This study introduces a new deep unsupervised learning approach using convolutional neural networks (CNN) with pseudo-classes for representing remote sensing scenes. The method involves assigning pseudo labels to training samples based on center points, enabling supervised training of the CNN model for feature extraction. Additionally, a pseudo-center loss is implemented to minimize the difference between samples and their corresponding center points. This loss function updates both the center points and the CNN model simultaneously during training. The study also combines the pseudo-center loss with a pseudo softmax loss, formulated using samples and pseudo labels, to achieve discriminative scene representations in an unsupervised manner. Experimental results on two remote sensing datasets demonstrate the effectiveness and superiority of the proposed method compared to existing state-of-the-art techniques."}, "2203.073": {"original_text": "  Current mobile user authentication systems based on PIN codes, fingerprint,\nand face recognition have several shortcomings. Such limitations have been\naddressed in the literature by exploring the feasibility of passive\nauthentication on mobile devices through behavioral biometrics. In this line of\nresearch, this work carries out a comparative analysis of unimodal and\nmultimodal behavioral biometric traits acquired while the subjects perform\ndifferent activities on the phone such as typing, scrolling, drawing a number,\nand tapping on the screen, considering the touchscreen and the simultaneous\nbackground sensor data (accelerometer, gravity sensor, gyroscope, linear\naccelerometer, and magnetometer). Our experiments are performed over HuMIdb,\none of the largest and most comprehensive freely available mobile user\ninteraction databases to date. A separate Recurrent Neural Network (RNN) with\ntriplet loss is implemented for each single modality. Then, the weighted fusion\nof the different modalities is carried out at score level. In our experiments,\nthe most discriminative background sensor is the magnetometer, whereas among\ntouch tasks the best results are achieved with keystroke in a fixed-text\nscenario. In all cases, the fusion of modalities is very beneficial, leading to\nEqual Error Rates (EER) ranging from 4% to 9% depending on the modality\ncombination in a 3-second interval.\n", "rewritten_text": "The current methods of authenticating mobile users, such as PIN codes, fingerprint, and face recognition, have limitations. To address these shortcomings, researchers have explored passive authentication using behavioral biometrics on mobile devices. This study conducts a comparative analysis of unimodal and multimodal behavioral biometric traits collected while users engage in various phone activities like typing, scrolling, drawing, and tapping on the screen. The analysis includes touchscreen data and background sensor data (accelerometer, gravity sensor, gyroscope, linear accelerometer, and magnetometer). Experiments are conducted using HuMIdb, a comprehensive mobile user interaction database. Separate Recurrent Neural Networks (RNNs) with triplet loss are implemented for each modality, followed by weighted fusion at the score level. Results show that the magnetometer is the most discriminative background sensor, and keystroke in a fixed-text scenario yields the best touch task results. Combining modalities proves beneficial, resulting in Equal Error Rates (EER) ranging from 4% to 9% depending on the modality combination within a 3-second interval."}, "2305.14722": {"original_text": "  Most contemporary supervised Remote Sensing (RS) image Change Detection (CD)\napproaches are customized for equal-resolution bitemporal images. Real-world\napplications raise the need for cross-resolution change detection, aka, CD\nbased on bitemporal images with different spatial resolutions. Given training\nsamples of a fixed bitemporal resolution difference (ratio) between the\nhigh-resolution (HR) image and the low-resolution (LR) one, current\ncross-resolution methods may fit a certain ratio but lack adaptation to other\nresolution differences. Toward continuous cross-resolution CD, we propose\nscale-invariant learning to enforce the model consistently predicting HR\nresults given synthesized samples of varying resolution differences.\nConcretely, we synthesize blurred versions of the HR image by random\ndownsampled reconstructions to reduce the gap between HR and LR images. We\nintroduce coordinate-based representations to decode per-pixel predictions by\nfeeding the coordinate query and corresponding multi-level embedding features\ninto an MLP that implicitly learns the shape of land cover changes, therefore\nbenefiting recognizing blurred objects in the LR image. Moreover, considering\nthat spatial resolution mainly affects the local textures, we apply\nlocal-window self-attention to align bitemporal features during the early\nstages of the encoder. Extensive experiments on two synthesized and one\nreal-world different-resolution CD datasets verify the effectiveness of the\nproposed method. Our method significantly outperforms several vanilla CD\nmethods and two cross-resolution CD methods on the three datasets both in\nin-distribution and out-of-distribution settings. The empirical results suggest\nthat our method could yield relatively consistent HR change predictions\nregardless of varying bitemporal resolution ratios. Our code is available at\n\\url{https://github.com/justchenhao/SILI_CD}.\n", "rewritten_text": "Many current methods for detecting changes in Remote Sensing (RS) images are designed for analyzing bitemporal images with the same resolution. However, real-world applications require the ability to detect changes in images with different spatial resolutions, known as cross-resolution change detection. Existing cross-resolution methods are limited in their adaptability to varying resolution differences. To address this, we propose a scale-invariant learning approach that consistently predicts high-resolution results using synthesized samples with different resolution gaps. By generating blurred versions of high-resolution images and incorporating coordinate-based representations, our method improves the recognition of changes in low-resolution images. Additionally, we utilize local-window self-attention to align features from different resolutions early in the process. Experimental results on multiple datasets demonstrate the effectiveness of our method compared to traditional and other cross-resolution change detection methods. Our approach shows promise in producing accurate high-resolution change predictions across various resolution ratios. The code for our method can be found at \\url{https://github.com/justchenhao/SILI_CD}."}, "1704.08763": {"original_text": "  We present GazeDirector, a new approach for eye gaze redirection that uses\nmodel-fitting. Our method first tracks the eyes by fitting a multi-part eye\nregion model to video frames using analysis-by-synthesis, thereby recovering\neye region shape, texture, pose, and gaze simultaneously. It then redirects\ngaze by 1) warping the eyelids from the original image using a model-derived\nflow field, and 2) rendering and compositing synthesized 3D eyeballs onto the\noutput image in a photorealistic manner. GazeDirector allows us to change where\npeople are looking without person-specific training data, and with full\narticulation, i.e. we can precisely specify new gaze directions in 3D.\nQuantitatively, we evaluate both model-fitting and gaze synthesis, with\nexperiments for gaze estimation and redirection on the Columbia gaze dataset.\nQualitatively, we compare GazeDirector against recent work on gaze redirection,\nshowing better results especially for large redirection angles. Finally, we\ndemonstrate gaze redirection on YouTube videos by introducing new 3D gaze\ntargets and by manipulating visual behavior.\n", "rewritten_text": "We introduce GazeDirector, a novel method for redirecting eye gaze using model-fitting. Our approach involves tracking the eyes by fitting a multi-part eye region model to video frames through analysis-by-synthesis. This process allows us to simultaneously recover eye region shape, texture, pose, and gaze. Gaze redirection is achieved by warping the eyelids from the original image using a model-derived flow field and rendering synthesized 3D eyeballs onto the output image in a realistic manner. GazeDirector enables us to alter where individuals are looking without the need for person-specific training data, offering precise control over new gaze directions in 3D. We evaluate our method through experiments on gaze estimation and redirection using the Columbia gaze dataset, comparing it favorably to recent work on gaze redirection, particularly for large redirection angles. Additionally, we showcase gaze redirection on YouTube videos by introducing new 3D gaze targets and manipulating visual behavior."}, "2110.13385": {"original_text": "  Recently, Transformer-based networks have shown great promise on\nskeleton-based action recognition tasks. The ability to capture global and\nlocal dependencies is the key to success while it also brings quadratic\ncomputation and memory cost. Another problem is that previous studies mainly\nfocus on the relationships among individual joints, which often suffers from\nthe noisy skeleton joints introduced by the noisy inputs of sensors or\ninaccurate estimations. To address the above issues, we propose a novel\nTransformer-based network (IIP-Transformer). Instead of exploiting interactions\namong individual joints, our IIP-Transformer incorporates body joints and parts\ninteractions simultaneously and thus can capture both joint-level (intra-part)\nand part-level (inter-part) dependencies efficiently and effectively. From the\ndata aspect, we introduce a part-level skeleton data encoding that\nsignificantly reduces the computational complexity and is more robust to\njoint-level skeleton noise. Besides, a new part-level data augmentation is\nproposed to improve the performance of the model. On two large-scale datasets,\nNTU-RGB+D 60 and NTU RGB+D 120, the proposed IIP-Transformer achieves\nthe-state-of-art performance with more than 8x less computational complexity\nthan DSTA-Net, which is the SOTA Transformer-based method.\n", "rewritten_text": "Recently, Transformer-based networks have demonstrated significant potential in skeleton-based action recognition tasks. The key to their success lies in their ability to capture both global and local dependencies, albeit at the cost of quadratic computation and memory usage. A common challenge in previous studies has been the focus on relationships among individual joints, which can be affected by noisy inputs from sensors or inaccurate estimations. To address these challenges, we introduce a novel Transformer-based network called IIP-Transformer. Unlike previous approaches that only consider interactions among individual joints, our IIP-Transformer simultaneously incorporates interactions among body joints and parts. This allows for efficient and effective capture of both joint-level (intra-part) and part-level (inter-part) dependencies. We also propose a part-level skeleton data encoding method to reduce computational complexity and enhance robustness against joint-level noise. Additionally, a new part-level data augmentation technique is introduced to enhance model performance. On the NTU-RGB+D 60 and NTU RGB+D 120 datasets, our proposed IIP-Transformer achieves state-of-the-art performance with over 8 times less computational complexity compared to DSTA-Net, the current state-of-the-art Transformer-based method."}, "2307.01200": {"original_text": "  Learning-based approaches to monocular motion capture have recently shown\npromising results by learning to regress in a data-driven manner. However, due\nto the challenges in data collection and network designs, it remains\nchallenging for existing solutions to achieve real-time full-body capture while\nbeing accurate in world space. In this work, we introduce ProxyCap, a\nhuman-centric proxy-to-motion learning scheme to learn world-space motions from\na proxy dataset of 2D skeleton sequences and 3D rotational motions. Such proxy\ndata enables us to build a learning-based network with accurate world-space\nsupervision while also mitigating the generalization issues. For more accurate\nand physically plausible predictions in world space, our network is designed to\nlearn human motions from a human-centric perspective, which enables the\nunderstanding of the same motion captured with different camera trajectories.\nMoreover, a contact-aware neural motion descent module is proposed in our\nnetwork so that it can be aware of foot-ground contact and motion misalignment\nwith the proxy observations. With the proposed learning-based solution, we\ndemonstrate the first real-time monocular full-body capture system with\nplausible foot-ground contact in world space even using hand-held moving\ncameras. Our project page is https://zhangyux15.github.io/ProxyCapV2.\n", "rewritten_text": "Recently, there have been promising advancements in monocular motion capture using learning-based methods that regress data-driven results. However, challenges in data collection and network designs have made it difficult for existing solutions to achieve real-time full-body capture with accuracy in world space. In this study, we present ProxyCap, a human-centric proxy-to-motion learning approach that learns world-space motions from a dataset of 2D skeleton sequences and 3D rotational motions. This proxy data allows us to develop a learning-based network with precise world-space supervision while addressing generalization issues. To improve accuracy and realism in world space predictions, our network is designed to learn human motions from a human-centric viewpoint, enabling understanding of the same motion captured from different camera angles. Additionally, we introduce a contact-aware neural motion descent module in our network to account for foot-ground contact and motion misalignment based on proxy observations. Through our learning-based solution, we showcase the first real-time monocular full-body capture system with realistic foot-ground contact in world space, even with handheld moving cameras. More information can be found on our project page at https://zhangyux15.github.io/ProxyCapV2."}, "1410.7484": {"original_text": "  Stochastic sampling based trackers have shown good performance for abrupt\nmotion tracking so that they have gained popularity in recent years. However,\nconventional methods tend to use a two-stage sampling paradigm, in which the\nsearch space needs to be uniformly explored with an inefficient preliminary\nsampling phase. In this paper, we propose a novel sampling-based method in the\nBayesian filtering framework to address the problem. Within the framework,\nnearest neighbor field estimation is utilized to compute the importance\nproposal probabilities, which guide the Markov chain search towards promising\nregions and thus enhance the sampling efficiency; given the motion priors, a\nsmoothing stochastic sampling Monte Carlo algorithm is proposed to approximate\nthe posterior distribution through a smoothing weight-updating scheme.\nMoreover, to track the abrupt and the smooth motions simultaneously, we develop\nan abrupt-motion detection scheme which can discover the presence of abrupt\nmotions during online tracking. Extensive experiments on challenging image\nsequences demonstrate the effectiveness and the robustness of our algorithm in\nhandling the abrupt motions.\n", "rewritten_text": "\"Stochastic sampling-based trackers have demonstrated strong performance in tracking abrupt motion, leading to increased popularity in recent years. However, traditional methods typically employ a two-stage sampling approach, requiring inefficient preliminary sampling to explore the search space uniformly. This paper introduces a new sampling method within the Bayesian filtering framework to address this issue. The proposed method utilizes nearest neighbor field estimation to calculate importance proposal probabilities, guiding the Markov chain search towards promising regions and improving sampling efficiency. By incorporating motion priors, a smoothing stochastic sampling Monte Carlo algorithm is introduced to approximate the posterior distribution through a weight-updating scheme. Additionally, to track both abrupt and smooth motions simultaneously, an abrupt-motion detection scheme is developed to identify abrupt motions during online tracking. Extensive experiments on challenging image sequences demonstrate the effectiveness and robustness of our algorithm in handling abrupt motions.\""}, "2010.06944": {"original_text": "  We present a formulation of the relative depth estimation from a single image\nproblem, as a ranking problem. By reformulating the problem this way, we were\nable to utilize literature on the ranking problem, and apply the existing\nknowledge to achieve better results. To this end, we have introduced a listwise\nranking loss borrowed from ranking literature, weighted ListMLE, to the\nrelative depth estimation problem. We have also brought a new metric which\nconsiders pixel depth ranking accuracy, on which our method is stronger.\n", "rewritten_text": "Here is a rephrased version of the text:\n\n\"We have developed an approach to estimate relative depth from a single image by framing it as a ranking problem. This novel perspective allowed us to leverage existing research on ranking problems, leading to improved outcomes. In this regard, we introduced a listwise ranking loss, known as weighted ListMLE, borrowed from ranking literature, to address the relative depth estimation challenge. Additionally, we introduced a new evaluation metric that focuses on the accuracy of pixel depth ranking, showcasing the strength of our method.\""}, "2012.02951": {"original_text": "  Visual scene understanding is the core task in making any crucial decision in\nany computer vision system. Although popular computer vision datasets like\nCityscapes, MS-COCO, PASCAL provide good benchmarks for several tasks (e.g.\nimage classification, segmentation, object detection), these datasets are\nhardly suitable for post disaster damage assessments. On the other hand,\nexisting natural disaster datasets include mainly satellite imagery which have\nlow spatial resolution and a high revisit period. Therefore, they do not have a\nscope to provide quick and efficient damage assessment tasks. Unmanned Aerial\nVehicle(UAV) can effortlessly access difficult places during any disaster and\ncollect high resolution imagery that is required for aforementioned tasks of\ncomputer vision. To address these issues we present a high resolution UAV\nimagery, FloodNet, captured after the hurricane Harvey. This dataset\ndemonstrates the post flooded damages of the affected areas. The images are\nlabeled pixel-wise for semantic segmentation task and questions are produced\nfor the task of visual question answering. FloodNet poses several challenges\nincluding detection of flooded roads and buildings and distinguishing between\nnatural water and flooded water. With the advancement of deep learning\nalgorithms, we can analyze the impact of any disaster which can make a precise\nunderstanding of the affected areas. In this paper, we compare and contrast the\nperformances of baseline methods for image classification, semantic\nsegmentation, and visual question answering on our dataset.\n", "rewritten_text": "Understanding visual scenes is essential for making critical decisions in computer vision systems. While popular datasets like Cityscapes, MS-COCO, and PASCAL serve as benchmarks for tasks such as image classification, segmentation, and object detection, they are not ideal for assessing post-disaster damage. Existing natural disaster datasets primarily consist of low-resolution satellite imagery with infrequent revisits, limiting their effectiveness for rapid and efficient damage assessment. Unmanned Aerial Vehicles (UAVs) can access challenging disaster areas and capture high-resolution imagery necessary for computer vision tasks. To address these challenges, we introduce FloodNet, a high-resolution UAV dataset showcasing post-flood damages after Hurricane Harvey. The dataset includes pixel-wise labels for semantic segmentation and questions for visual question answering. FloodNet presents challenges like identifying flooded roads and buildings and distinguishing between natural and flooded water. Leveraging deep learning algorithms enables precise analysis of disaster impacts on affected areas. This study compares and contrasts baseline methods for image classification, semantic segmentation, and visual question answering using our dataset."}, "1905.1062": {"original_text": "  Large-scale face recognition in-the-wild has been recently achieved matured\nperformance in many real work applications. However, such systems are built on\nGPU platforms and mostly deploy heavy deep network architectures. Given a\nhigh-performance heavy network as a teacher, this work presents a simple and\nelegant teacher-student learning paradigm, namely ShrinkTeaNet, to train a\nportable student network that has significantly fewer parameters and\ncompetitive accuracy against the teacher network. Far apart from prior\nteacher-student frameworks mainly focusing on accuracy and compression ratios\nin closed-set problems, our proposed teacher-student network is proved to be\nmore robust against open-set problem, i.e. large-scale face recognition. In\naddition, this work introduces a novel Angular Distillation Loss for distilling\nthe feature direction and the sample distributions of the teacher's hypersphere\nto its student. Then ShrinkTeaNet framework can efficiently guide the student's\nlearning process with the teacher's knowledge presented in both intermediate\nand last stages of the feature embedding. Evaluations on LFW, CFP-FP, AgeDB,\nIJB-B and IJB-C Janus, and MegaFace with one million distractors have\ndemonstrated the efficiency of the proposed approach to learn robust student\nnetworks which have satisfying accuracy and compact sizes. Our ShrinkTeaNet is\nable to support the light-weight architecture achieving high performance with\n99.77% on LFW and 95.64% on large-scale Megaface protocols.\n", "rewritten_text": "Recently, large-scale face recognition in real-world scenarios has shown significant progress, particularly in applications. However, these systems typically rely on GPU platforms and complex deep network architectures. This study introduces a new approach called ShrinkTeaNet, which leverages a high-performance deep network as a teacher to train a more compact student network with fewer parameters while maintaining competitive accuracy. Unlike previous teacher-student frameworks that focus on accuracy and compression ratios in closed-set scenarios, our method demonstrates robustness in open-set problems like large-scale face recognition. We propose a novel Angular Distillation Loss to transfer the teacher's knowledge to the student network efficiently. Evaluation on various datasets confirms the effectiveness of ShrinkTeaNet in producing accurate and compact student networks, achieving impressive results such as 99.77% accuracy on LFW and 95.64% on MegaFace protocols."}, "1601.0763": {"original_text": "  We propose a method that integrates two widely available data sources,\nbuilding footprints from 2D maps and street level images, to derive valuable\ninformation that is generally difficult to acquire -- building heights and\nbuilding facade masks in images. Building footprints are elevated in world\ncoordinates and projected onto images. Building heights are estimated by\nscoring projected footprints based on their alignment with building features in\nimages. Building footprints with estimated heights can be converted to simple\n3D building models, which are projected back to images to identify buildings.\nIn this procedure, accurate camera projections are critical. However, camera\nposition errors inherited from external sensors commonly exist, which adversely\naffect results. We derive a solution to precisely locate cameras on maps using\ncorrespondence between image features and building footprints. Experiments on\nreal-world datasets show the promise of our method.\n", "rewritten_text": "We present a method that combines building footprints from 2D maps and street level images to extract valuable information that is typically hard to obtain - building heights and building facade masks in images. The building footprints are geolocated and overlaid onto images, allowing us to estimate building heights by analyzing the alignment of footprints with building features in the images. These estimated heights enable us to create basic 3D building models, which are then projected back onto the images for building identification. Accurate camera projections are crucial in this process, but errors in camera positioning from external sensors can impact the results. To address this, we propose a solution to accurately position cameras on maps by matching image features with building footprints. Our experiments on real-world datasets demonstrate the potential of our approach."}, "2112.12141": {"original_text": "  3D human pose estimation (HPE) in autonomous vehicles (AV) differs from other\nuse cases in many factors, including the 3D resolution and range of data,\nabsence of dense depth maps, failure modes for LiDAR, relative location between\nthe camera and LiDAR, and a high bar for estimation accuracy. Data collected\nfor other use cases (such as virtual reality, gaming, and animation) may\ntherefore not be usable for AV applications. This necessitates the collection\nand annotation of a large amount of 3D data for HPE in AV, which is\ntime-consuming and expensive. In this paper, we propose one of the first\napproaches to alleviate this problem in the AV setting. Specifically, we\npropose a multi-modal approach which uses 2D labels on RGB images as weak\nsupervision to perform 3D HPE. The proposed multi-modal architecture\nincorporates LiDAR and camera inputs with an auxiliary segmentation branch. On\nthe Waymo Open Dataset, our approach achieves a 22% relative improvement over\ncamera-only 2D HPE baseline, and 6% improvement over LiDAR-only model. Finally,\ncareful ablation studies and parts based analysis illustrate the advantages of\neach of our contributions.\n", "rewritten_text": "The 3D human pose estimation (HPE) in autonomous vehicles (AV) presents unique challenges compared to other applications due to factors such as varying 3D resolution and data range, lack of dense depth maps, LiDAR failure modes, camera-LiDAR positioning, and high accuracy requirements. Data from other domains like virtual reality, gaming, and animation may not be suitable for AV use, necessitating the costly and time-consuming collection and annotation of specific 3D data for HPE in AV. This paper introduces a novel approach to address this issue in the AV context by proposing a multi-modal method that leverages 2D labels on RGB images for weak supervision in 3D HPE. The proposed architecture integrates LiDAR and camera inputs with an additional segmentation branch. Evaluation on the Waymo Open Dataset demonstrates a 22% relative enhancement over a camera-only 2D HPE baseline and a 6% improvement over a LiDAR-only model. Detailed ablation studies and parts-based analysis highlight the benefits of each component of our approach."}, "2306.02182": {"original_text": "  Indian court legal texts and processes are essential towards the integrity of\nthe judicial system and towards maintaining the social and political order of\nthe nation. Due to the increase in number of pending court cases, there is an\nurgent need to develop tools to automate many of the legal processes with the\nknowledge of artificial intelligence. In this paper, we employ knowledge\nextraction techniques, specially the named entity extraction of legal entities\nwithin court case judgements. We evaluate several state of the art\narchitectures in the realm of sequence labeling using models trained on a\ncurated dataset of legal texts. We observe that a Bi-LSTM model trained on\nFlair Embeddings achieves the best results, and we also publish the BIO\nformatted dataset as part of this paper.\n", "rewritten_text": "The legal texts and procedures within Indian courts play a crucial role in upholding the integrity of the judicial system and preserving the social and political order of the nation. With a rising number of pending court cases, there is an urgent necessity to implement tools that can automate various legal processes using artificial intelligence. This study focuses on utilizing knowledge extraction techniques, particularly named entity extraction of legal entities found in court case judgments. We assess different cutting-edge architectures in the field of sequence labeling by training models on a carefully curated dataset of legal texts. Our findings show that a Bi-LSTM model trained on Flair Embeddings yields the most favorable outcomes, and we also provide the BIO formatted dataset as part of this research."}, "2011.09563": {"original_text": "  Unsupervised domain adaptation (UDA) is widely used to transfer knowledge\nfrom a labeled source domain to an unlabeled target domain with different data\ndistribution. While extensive studies attested that deep learning models are\nvulnerable to adversarial attacks, the adversarial robustness of models in\ndomain adaptation application has largely been overlooked. This paper points\nout that the inevitable domain distribution deviation in UDA is a critical\nbarrier to model robustness on the target domain. To address the problem, we\npropose a novel Class-consistent Unsupervised Robust Domain Adaptation (CURDA)\nframework for training robust UDA models. With the introduced contrastive\nrobust training and source anchored adversarial contrastive losses, our\nproposed CURDA framework can effectively robustify UDA models by simultaneously\nminimizing the data distribution deviation and the distance between target\ndomain clean-adversarial pairs without creating classification confusion.\nExperiments on several public benchmarks show that CURDA can significantly\nimprove model robustness in the target domain with only minor cost of accuracy\non the clean samples.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) is commonly used to transfer knowledge from a labeled source domain to an unlabeled target domain with different data distribution. While research has shown that deep learning models are susceptible to adversarial attacks, the robustness of models in domain adaptation applications has not received much attention. This study highlights that the inherent differences in domain distributions in UDA pose a significant challenge to model robustness on the target domain. To tackle this issue, we introduce a new framework called Class-consistent Unsupervised Robust Domain Adaptation (CURDA) for training robust UDA models. By incorporating contrastive robust training and source anchored adversarial contrastive losses, our CURDA framework enhances the robustness of UDA models by minimizing both data distribution differences and the distance between target domain clean-adversarial pairs without causing classification confusion. Experiments conducted on various public benchmarks demonstrate that CURDA can notably enhance model robustness in the target domain with only a slight decrease in accuracy on clean samples."}, "1907.00184": {"original_text": "  Since Bahdanau et al. [1] first introduced attention for neural machine\ntranslation, most sequence-to-sequence models made use of attention mechanisms\n[2, 3, 4]. While they produce soft-alignment matrices that could be interpreted\nas alignment between target and source languages, we lack metrics to quantify\ntheir quality, being unclear which approach produces the best alignments. This\npaper presents an empirical evaluation of 3 main sequence-to-sequence models\n(CNN, RNN and Transformer-based) for word discovery from unsegmented phoneme\nsequences. This task consists in aligning word sequences in a source language\nwith phoneme sequences in a target language, inferring from it word\nsegmentation on the target side [5]. Evaluating word segmentation quality can\nbe seen as an extrinsic evaluation of the soft-alignment matrices produced\nduring training. Our experiments in a low-resource scenario on Mboshi and\nEnglish languages (both aligned to French) show that RNNs surprisingly\noutperform CNNs and Transformer for this task. Our results are confirmed by an\nintrinsic evaluation of alignment quality through the use of Average Normalized\nEntropy (ANE). Lastly, we improve our best word discovery model by using an\nalignment entropy confidence measure that accumulates ANE over all the\noccurrences of a given alignment pair in the collection.\n", "rewritten_text": "Since the introduction of attention for neural machine translation by Bahdanau et al. [1], most sequence-to-sequence models have incorporated attention mechanisms [2, 3, 4]. These models generate soft-alignment matrices that represent the alignment between target and source languages. However, there is a lack of metrics to assess the quality of these alignments, making it unclear which approach yields the best results. This study conducts an empirical evaluation of three main sequence-to-sequence models (CNN, RNN, and Transformer-based) for word discovery from unsegmented phoneme sequences. The task involves aligning word sequences in a source language with phoneme sequences in a target language to infer word segmentation on the target side [5]. Assessing word segmentation quality serves as an extrinsic evaluation of the soft-alignment matrices produced during training. Surprisingly, in low-resource scenarios with Mboshi and English languages (both aligned to French), RNNs outperform CNNs and Transformers for this task. These findings are supported by an intrinsic evaluation of alignment quality using Average Normalized Entropy (ANE). Additionally, the best word discovery model is enhanced by incorporating an alignment entropy confidence measure that aggregates ANE across all occurrences of a specific alignment pair in the dataset."}, "1812.09877": {"original_text": "  In multimodal unsupervised image-to-image translation tasks, the goal is to\ntranslate an image from the source domain to many images in the target domain.\nWe present a simple method that produces higher quality images than current\nstate-of-the-art while maintaining the same amount of multimodal diversity.\nPrevious methods follow the unconditional approach of trying to map the latent\ncode directly to a full-size image. This leads to complicated network\narchitectures with several introduced hyperparameters to tune. By treating the\nlatent code as a modifier of the convolutional filters, we produce multimodal\noutput while maintaining the traditional Generative Adversarial Network (GAN)\nloss and without additional hyperparameters. The only tuning required by our\nmethod controls the tradeoff between variability and quality of generated\nimages. Furthermore, we achieve disentanglement between source domain content\nand target domain style for free as a by-product of our formulation. We perform\nqualitative and quantitative experiments showing the advantages of our method\ncompared with the state-of-the art on multiple benchmark image-to-image\ntranslation datasets.\n", "rewritten_text": "\"In tasks involving multimodal unsupervised image-to-image translation, the objective is to convert an image from one domain to multiple images in another domain. Our approach offers a straightforward technique that generates higher-quality images than current leading methods, while maintaining the same level of multimodal diversity. Unlike previous techniques that directly map latent codes to full-size images, our method treats the latent code as a modifier of convolutional filters. This results in multimodal output without the need for complex network architectures or additional hyperparameters. Our method only requires tuning to balance the variability and quality of generated images. Additionally, our formulation naturally achieves disentanglement between source domain content and target domain style. Through qualitative and quantitative experiments, we demonstrate the superiority of our approach over existing methods on various benchmark image-to-image translation datasets.\""}, "1807.04047": {"original_text": "  Document image quality assessment (DIQA) is an important and challenging\nproblem in real applications. In order to predict the quality scores of\ndocument images, this paper proposes a novel no-reference DIQA method based on\ncharacter gradient, where the OCR accuracy is used as a ground-truth quality\nmetric. Character gradient is computed on character patches detected with the\nmaximally stable extremal regions (MSER) based method. Character patches are\nessentially significant to character recognition and therefore suitable for use\nin estimating document image quality. Experiments on a benchmark dataset show\nthat the proposed method outperforms the state-of-the-art methods in estimating\nthe quality score of document images.\n", "rewritten_text": "The assessment of document image quality (DIQA) is a significant and complex issue in practical scenarios. This study introduces a new no-reference DIQA approach that utilizes character gradient to predict quality scores of document images. The method relies on OCR accuracy as a quality metric. Character gradient is calculated on character patches identified using the maximally stable extremal regions (MSER) method. These character patches are crucial for character recognition and are therefore suitable for evaluating document image quality. Results from experiments on a standard dataset demonstrate that the proposed method surpasses current methods in assessing the quality of document images."}, "2406.01171": {"original_text": "  The concept of persona, originally adopted in dialogue literature, has\nre-surged as a promising framework for tailoring large language models (LLMs)\nto specific context (e.g., personalized search, LLM-as-a-judge). However, the\ngrowing research on leveraging persona in LLMs is relatively disorganized and\nlacks a systematic taxonomy. To close the gap, we present a comprehensive\nsurvey to categorize the current state of the field. We identify two lines of\nresearch, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and\n(2) LLM Personalization, where LLMs take care of user personas. Additionally,\nwe introduce existing methods for LLM personality evaluation. To the best of\nour knowledge, we present the first survey for role-playing and personalization\nin LLMs under the unified view of persona. We continuously maintain a paper\ncollection to foster future endeavors:\nhttps://github.com/MiuLab/PersonaLLM-Survey\n", "rewritten_text": "The idea of persona, originally used in dialogue literature, has resurfaced as a promising framework for customizing large language models (LLMs) for specific contexts such as personalized search and LLM-as-a-judge. However, the current research on incorporating persona into LLMs is somewhat disorganized and lacks a systematic classification. To address this gap, we have conducted a comprehensive survey to categorize the existing state of the field. We have identified two main research directions: (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs cater to user personas. Additionally, we have introduced various methods for evaluating LLM personality. This survey is believed to be the first of its kind to explore role-playing and personalization in LLMs from a unified persona perspective. We will continue to update our paper collection to support future research efforts: https://github.com/MiuLab/PersonaLLM-Survey"}, "1804.02745": {"original_text": "  Dynamic contrast-enhanced (DCE) MRI is an evolving imaging technique that\nprovides a quantitative measure of pharmacokinetic (PK) parameters in body\ntissues, in which series of T1-weighted images are collected following the\nadministration of a paramagnetic contrast agent. Unfortunately, in many\napplications, conventional clinical DCE-MRI suffers from low spatiotemporal\nresolution and insufficient volume coverage. In this paper, we propose a novel\ndeep learning based approach to directly estimate the PK parameters from\nundersampled DCE-MRI data. Specifically, we design a custom loss function where\nwe incorporate a forward physical model that relates the PK parameters to\ncorrupted image-time series obtained due to subsampling in k-space. This allows\nthe network to directly exploit the knowledge of true contrast agent kinetics\nin the training phase, and hence provide more accurate restoration of PK\nparameters. Experiments on clinical brain DCE datasets demonstrate the efficacy\nof our approach in terms of fidelity of PK parameter reconstruction and\nsignificantly faster parameter inference compared to a model-based iterative\nreconstruction method.\n", "rewritten_text": "\"DCE-MRI is a developing imaging technique that quantitatively measures pharmacokinetic parameters in body tissues by collecting a series of T1-weighted images after administering a contrast agent. However, traditional clinical DCE-MRI often lacks resolution and coverage. In this study, we introduce a new deep learning method to estimate PK parameters directly from undersampled DCE-MRI data. Our approach incorporates a custom loss function with a physical model linking PK parameters to corrupted image-time series resulting from k-space subsampling. This enables the network to leverage true contrast agent kinetics during training, leading to more accurate PK parameter restoration. Experiments on clinical brain DCE datasets show that our method achieves high fidelity in PK parameter reconstruction and faster parameter inference compared to iterative reconstruction.\""}, "2207.01164": {"original_text": "  Neural Radiance Field (NeRF) regresses a neural parameterized scene by\ndifferentially rendering multi-view images with ground-truth supervision.\nHowever, when interpolating novel views, NeRF often yields inconsistent and\nvisually non-smooth geometric results, which we consider as a generalization\ngap between seen and unseen views. Recent advances in convolutional neural\nnetworks have demonstrated the promise of advanced robust data augmentations,\neither random or learned, in enhancing both in-distribution and\nout-of-distribution generalization. Inspired by that, we propose Augmented NeRF\n(Aug-NeRF), which for the first time brings the power of robust data\naugmentations into regularizing the NeRF training. Particularly, our proposal\nlearns to seamlessly blend worst-case perturbations into three distinct levels\nof the NeRF pipeline with physical grounds, including (1) the input\ncoordinates, to simulate imprecise camera parameters at image capture; (2)\nintermediate features, to smoothen the intrinsic feature manifold; and (3)\npre-rendering output, to account for the potential degradation factors in the\nmulti-view image supervision. Extensive results demonstrate that Aug-NeRF\neffectively boosts NeRF performance in both novel view synthesis (up to 1.5dB\nPSNR gain) and underlying geometry reconstruction. Furthermore, thanks to the\nimplicit smooth prior injected by the triple-level augmentations, Aug-NeRF can\neven recover scenes from heavily corrupted images, a highly challenging setting\nuntackled before. Our codes are available in\nhttps://github.com/VITA-Group/Aug-NeRF.\n", "rewritten_text": "The Neural Radiance Field (NeRF) method uses neural networks to model scenes by rendering multiple views with ground-truth supervision. However, NeRF can produce inconsistent and visually non-smooth results when generating new views, indicating a gap between seen and unseen views. Recent advancements in convolutional neural networks have shown the benefits of robust data augmentations for improving generalization. Building on this, we introduce Augmented NeRF (Aug-NeRF), which incorporates robust data augmentations into NeRF training to enhance performance. Aug-NeRF blends perturbations at three levels of the NeRF pipeline - input coordinates, intermediate features, and pre-rendering output - to simulate camera parameter inaccuracies, smooth feature representations, and address potential image degradation factors. Experimental results show that Aug-NeRF improves novel view synthesis and geometry reconstruction compared to NeRF, achieving up to a 1.5dB PSNR gain. Additionally, Aug-NeRF can recover scenes from heavily corrupted images, a previously challenging task. Our code is available at https://github.com/VITA-Group/Aug-NeRF."}, "1511.05296": {"original_text": "  In this paper, we propose a method for ranking fashion images to find the\nones which might be liked by more people. We collect two new datasets from\nimage sharing websites (Pinterest and Polyvore). We represent fashion images\nbased on attributes: semantic attributes and data-driven attributes. To learn\nsemantic attributes from limited training data, we use an algorithm on\nmulti-task convolutional neural networks to share visual knowledge among\ndifferent semantic attribute categories. To discover data-driven attributes\nunsupervisedly, we propose an algorithm to simultaneously discover visual\nclusters and learn fashion-specific feature representations. Given attributes\nas representations, we propose to learn a ranking SPN (sum product networks) to\nrank pairs of fashion images. The proposed ranking SPN can capture the\nhigh-order correlations of the attributes. We show the effectiveness of our\nmethod on our two newly collected datasets.\n", "rewritten_text": "\"In this paper, we present a technique for sorting fashion images to identify those that may appeal to a larger audience. We have gathered two fresh datasets from image-sharing platforms (Pinterest and Polyvore). Our approach involves categorizing fashion images based on semantic and data-driven attributes. To extract semantic attributes from limited training data, we utilize a multi-task convolutional neural network algorithm to share visual knowledge across various semantic attribute categories. For unsupervised discovery of data-driven attributes, we introduce an algorithm that simultaneously identifies visual clusters and learns fashion-specific feature representations. Using these attributes as representations, we propose training a ranking SPN (sum product networks) to rank pairs of fashion images. This ranking SPN is designed to capture the complex correlations among attributes. Our method's effectiveness is demonstrated on the newly collected datasets.\""}, "1901.00049": {"original_text": "  We introduce a new silhouette-based representation for modeling clothed human\nbodies using deep generative models. Our method can reconstruct a complete and\ntextured 3D model of a person wearing clothes from a single input picture.\nInspired by the visual hull algorithm, our implicit representation uses 2D\nsilhouettes and 3D joints of a body pose to describe the immense shape\ncomplexity and variations of clothed people. Given a segmented 2D silhouette of\na person and its inferred 3D joints from the input picture, we first synthesize\nconsistent silhouettes from novel view points around the subject. The\nsynthesized silhouettes which are the most consistent with the input\nsegmentation are fed into a deep visual hull algorithm for robust 3D shape\nprediction. We then infer the texture of the subject's back view using the\nfrontal image and segmentation mask as input to a conditional generative\nadversarial network. Our experiments demonstrate that our silhouette-based\nmodel is an effective representation and the appearance of the back view can be\npredicted reliably using an image-to-image translation network. While classic\nmethods based on parametric models often fail for single-view images of\nsubjects with challenging clothing, our approach can still produce successful\nresults, which are comparable to those obtained from multi-view input.\n", "rewritten_text": "We present a novel approach for modeling clothed human bodies using deep generative models based on silhouettes. Our method can generate a detailed 3D model of a person in clothing from just one input image. Drawing inspiration from the visual hull algorithm, our implicit representation leverages 2D silhouettes and 3D joints of a body pose to capture the diverse shapes and variations of clothed individuals. By utilizing a segmented 2D silhouette and inferred 3D joints, we synthesize consistent silhouettes from different viewpoints around the subject. These synthesized silhouettes, which closely match the input segmentation, are then processed through a deep visual hull algorithm to predict a robust 3D shape. Additionally, we predict the texture of the subject's back view by using the frontal image and segmentation mask as inputs to a conditional generative adversarial network. Our experiments demonstrate the effectiveness of our silhouette-based model and the reliable prediction of the back view appearance through an image-to-image translation network. Unlike traditional methods relying on parametric models that struggle with single-view images of subjects in complex clothing, our approach yields successful results comparable to those achieved with multi-view inputs."}, "2308.07837": {"original_text": "  In this paper, we present a novel shape reconstruction method leveraging\ndiffusion model to generate 3D sparse point cloud for the object captured in a\nsingle RGB image. Recent methods typically leverage global embedding or local\nprojection-based features as the condition to guide the diffusion model.\nHowever, such strategies fail to consistently align the denoised point cloud\nwith the given image, leading to unstable conditioning and inferior\nperformance. In this paper, we present CCD-3DR, which exploits a novel centered\ndiffusion probabilistic model for consistent local feature conditioning. We\nconstrain the noise and sampled point cloud from the diffusion model into a\nsubspace where the point cloud center remains unchanged during the forward\ndiffusion process and reverse process. The stable point cloud center further\nserves as an anchor to align each point with its corresponding local\nprojection-based features. Extensive experiments on synthetic benchmark\nShapeNet-R2N2 demonstrate that CCD-3DR outperforms all competitors by a large\nmargin, with over 40% improvement. We also provide results on real-world\ndataset Pix3D to thoroughly demonstrate the potential of CCD-3DR in real-world\napplications. Codes will be released soon\n", "rewritten_text": "This paper introduces a new method for reconstructing shapes using a diffusion model to create a 3D sparse point cloud from a single RGB image of an object. Existing methods often rely on global embedding or local projection-based features to guide the diffusion model, but these approaches struggle to consistently align the denoised point cloud with the input image, resulting in unstable conditioning and subpar performance. Our proposed method, CCD-3DR, utilizes a novel centered diffusion probabilistic model for reliable local feature conditioning. By confining noise and sampled point cloud data within a subspace where the point cloud center remains constant throughout the diffusion process, we establish a stable anchor point for aligning each point with its corresponding local projection-based features. Extensive experiments on the ShapeNet-R2N2 benchmark dataset show that CCD-3DR significantly outperforms existing methods, achieving over a 40% improvement. Results on the Pix3D real-world dataset further highlight the potential of CCD-3DR in practical applications. The code for CCD-3DR will be made available soon."}, "2312.10610": {"original_text": "  A number of tasks have been proposed recently to facilitate easy access to\ncharts such as chart QA and summarization. The dominant paradigm to solve these\ntasks has been to fine-tune a pretrained model on the task data. However, this\napproach is not only expensive but also not generalizable to unseen tasks. On\nthe other hand, large language models (LLMs) have shown impressive\ngeneralization capabilities to unseen tasks with zero- or few-shot prompting.\nHowever, their application to chart-related tasks is not trivial as these tasks\ntypically involve considering not only the underlying data but also the visual\nfeatures in the chart image. We propose PromptChart, a multimodal few-shot\nprompting framework with LLMs for chart-related applications. By analyzing the\ntasks carefully, we have come up with a set of prompting guidelines for each\ntask to elicit the best few-shot performance from LLMs. We further propose a\nstrategy to inject visual information into the prompts. Our experiments on\nthree different chart-related information consumption tasks show that with\nproperly designed prompts LLMs can excel on the benchmarks, achieving\nstate-of-the-art.\n", "rewritten_text": "Recently, several tasks have been suggested to make it easier to access charts, such as chart quality assurance and summarization. The common approach to addressing these tasks has been to fine-tune a pre-trained model using task-specific data. However, this method is costly and lacks generalizability to new tasks. Conversely, large language models (LLMs) have demonstrated impressive ability to generalize to new tasks with minimal prompting. Yet, applying LLMs to chart-related tasks is challenging due to the need to consider both data and visual features in chart images. To address this, we introduce PromptChart, a multimodal few-shot prompting framework utilizing LLMs for chart-related applications. Through careful task analysis, we have developed prompting guidelines for each task to optimize LLM performance. Additionally, we propose a method to incorporate visual information into the prompts. Our experiments on three chart-related information tasks indicate that well-crafted prompts enable LLMs to outperform existing benchmarks, achieving state-of-the-art results."}, "1906.03657": {"original_text": "  Group convolution works well with many deep convolutional neural networks\n(CNNs) that can effectively compress the model by reducing the number of\nparameters and computational cost. Using this operation, feature maps of\ndifferent group cannot communicate, which restricts their representation\ncapability. To address this issue, in this work, we propose a novel operation\nnamed Hierarchical Group Convolution (HGC) for creating computationally\nefficient neural networks. Different from standard group convolution which\nblocks the inter-group information exchange and induces the severe performance\ndegradation, HGC can hierarchically fuse the feature maps from each group and\nleverage the inter-group information effectively. Taking advantage of the\nproposed method, we introduce a family of compact networks called HGCNets.\nCompared to networks using standard group convolution, HGCNets have a huge\nimprovement in accuracy at the same model size and complexity level. Extensive\nexperimental results on the CIFAR dataset demonstrate that HGCNets obtain\nsignificant reduction of parameters and computational cost to achieve\ncomparable performance over the prior CNN architectures designed for mobile\ndevices such as MobileNet and ShuffleNet.\n", "rewritten_text": "Group convolution is effective in reducing the number of parameters and computational cost in deep convolutional neural networks (CNNs). However, it limits the communication between feature maps of different groups, which hinders their representation capability. To address this limitation, we propose a new operation called Hierarchical Group Convolution (HGC) in this study. Unlike standard group convolution, which restricts inter-group information exchange and leads to performance degradation, HGC can hierarchically merge feature maps from each group and utilize inter-group information efficiently. By leveraging this method, we introduce a series of compact networks known as HGCNets. Compared to networks using standard group convolution, HGCNets demonstrate significant improvements in accuracy at the same model size and complexity level. Extensive experiments on the CIFAR dataset show that HGCNets achieve notable reductions in parameters and computational cost while maintaining comparable performance to previous CNN architectures designed for mobile devices like MobileNet and ShuffleNet."}, "2304.01534": {"original_text": "  Bird's eye view (BEV) perception is becoming increasingly important in the\nfield of autonomous driving. It uses multi-view camera data to learn a\ntransformer model that directly projects the perception of the road environment\nonto the BEV perspective. However, training a transformer model often requires\na large amount of data, and as camera data for road traffic are often private,\nthey are typically not shared. Federated learning offers a solution that\nenables clients to collaborate and train models without exchanging data but\nmodel parameters. In this paper, we introduce FedBEVT, a federated transformer\nlearning approach for BEV perception. In order to address two common data\nheterogeneity issues in FedBEVT: (i) diverse sensor poses, and (ii) varying\nsensor numbers in perception systems, we propose two approaches -- Federated\nLearning with Camera-Attentive Personalization (FedCaP) and Adaptive\nMulti-Camera Masking (AMCM), respectively. To evaluate our method in real-world\nsettings, we create a dataset consisting of four typical federated use cases.\nOur findings suggest that FedBEVT outperforms the baseline approaches in all\nfour use cases, demonstrating the potential of our approach for improving BEV\nperception in autonomous driving.\n", "rewritten_text": "The importance of Bird's Eye View (BEV) perception is growing in the autonomous driving field. It involves using data from multiple cameras to train a transformer model that directly maps the road environment onto a BEV perspective. However, training such a model typically requires a large amount of data, which can be challenging as road traffic camera data is often private and not shared. Federated learning provides a solution by allowing clients to collaborate and train models without sharing data, only model parameters. This paper introduces FedBEVT, a federated transformer learning method for BEV perception. To address common data heterogeneity issues in FedBEVT, such as diverse sensor poses and varying sensor numbers in perception systems, two approaches are proposed: Federated Learning with Camera-Attentive Personalization (FedCaP) and Adaptive Multi-Camera Masking (AMCM). The method is evaluated using a dataset containing four typical federated use cases, with results showing that FedBEVT outperforms baseline approaches in all scenarios, highlighting its potential for enhancing BEV perception in autonomous driving."}, "1911.10248": {"original_text": "  The rapid development of inexpensive commodity depth sensors has made\nkeypoint detection and matching in the depth image modality an important\nproblem in computer vision. Despite great improvements in recent RGB local\nfeature learning methods, adapting them directly in the depth modality leads to\nunsatisfactory performance. Most of these methods do not explicitly reason\nbeyond the visible pixels in the images. To address the limitations of these\nmethods, we propose a framework ViewSynth, to jointly learn: (1) viewpoint\ninvariant keypoint-descriptor from depth images using a proposed Contrastive\nMatching Loss, and (2) view synthesis of depth images from different viewpoints\nusing the proposed View Synthesis Module and View Synthesis Loss. By learning\nview synthesis, we explicitly encourage the feature extractor to encode\ninformation about not only the visible, but also the occluded parts of the\nscene. We demonstrate that in the depth modality, ViewSynth outperforms the\nstate-of-the-art depth and RGB local feature extraction techniques in the 3D\nkeypoint matching and camera localization tasks on the RGB-D datasets 7-Scenes,\nTUM RGBD and CoRBS in most scenarios. We also show the generalizability of\nViewSynth in 3D keypoint matching across different datasets.\n", "rewritten_text": "The rapid advancement of affordable commodity depth sensors has made detecting and matching keypoints in depth images a significant challenge in computer vision. While there have been notable improvements in RGB local feature learning methods, directly applying them to depth data often results in subpar performance. Many existing methods do not consider information beyond what is visible in the images. To overcome these limitations, we introduce a framework called ViewSynth, which aims to simultaneously learn: (1) viewpoint-invariant keypoint descriptors from depth images using a novel Contrastive Matching Loss, and (2) generate depth image views from various perspectives through the proposed View Synthesis Module and View Synthesis Loss. By incorporating view synthesis, we encourage the feature extractor to encode details not only from visible areas but also from occluded parts of the scene. Our experiments demonstrate that ViewSynth surpasses current depth and RGB local feature extraction techniques in 3D keypoint matching and camera localization tasks on datasets such as 7-Scenes, TUM RGBD, and CoRBS in most scenarios within the depth modality. Furthermore, we showcase the versatility of ViewSynth in 3D keypoint matching across diverse datasets."}, "2211.10018": {"original_text": "  Relation extraction has the potential for large-scale knowledge graph\nconstruction, but current methods do not consider the qualifier attributes for\neach relation triplet, such as time, quantity or location. The qualifiers form\nhyper-relational facts which better capture the rich and complex knowledge\ngraph structure. For example, the relation triplet (Leonard Parker, Educated\nAt, Harvard University) can be factually enriched by including the qualifier\n(End Time, 1967). Hence, we propose the task of hyper-relational extraction to\nextract more specific and complete facts from text. To support the task, we\nconstruct HyperRED, a large-scale and general-purpose dataset. Existing models\ncannot perform hyper-relational extraction as it requires a model to consider\nthe interaction between three entities. Hence, we propose CubeRE, a\ncube-filling model inspired by table-filling approaches and explicitly\nconsiders the interaction between relation triplets and qualifiers. To improve\nmodel scalability and reduce negative class imbalance, we further propose a\ncube-pruning method. Our experiments show that CubeRE outperforms strong\nbaselines and reveal possible directions for future research. Our code and data\nare available at github.com/declare-lab/HyperRED.\n", "rewritten_text": "The extraction of relations has the potential to build extensive knowledge graphs, but current methods overlook the qualifier attributes associated with each relation triplet, such as time, quantity, or location. These qualifiers represent hyper-relational facts that can more accurately capture the intricate structure of knowledge graphs. For instance, enhancing the relation triplet (Leonard Parker, Educated At, Harvard University) with the qualifier (End Time, 1967) enriches the factual information. Therefore, we introduce the concept of hyper-relational extraction to extract more detailed and comprehensive facts from text. To facilitate this task, we introduce HyperRED, a comprehensive dataset. Existing models struggle with hyper-relational extraction due to the need to consider interactions between three entities. To address this, we propose CubeRE, a model inspired by table-filling techniques that explicitly accounts for the interplay between relation triplets and qualifiers. Additionally, we introduce a cube-pruning method to enhance model scalability and address negative class imbalance. Our experiments demonstrate that CubeRE surpasses strong baseline models and suggest potential avenues for future research. The code and data can be accessed at github.com/declare-lab/HyperRED."}, "2210.06246": {"original_text": "  Recently, the community has achieved substantial progress on many commonsense\nreasoning benchmarks. However, it is still unclear what is learned from the\ntraining process: the knowledge, inference capability, or both? We argue that\ndue to the large scale of commonsense knowledge, it is infeasible to annotate a\nlarge enough training set for each task to cover all commonsense for learning.\nThus we should separate the commonsense knowledge acquisition and inference\nover commonsense knowledge as two separate tasks. In this work, we focus on\ninvestigating models' commonsense inference capabilities from two perspectives:\n(1) Whether models can know if the knowledge they have is enough to solve the\ntask; (2) Whether models can develop commonsense inference capabilities that\ngeneralize across commonsense tasks. We first align commonsense tasks with\nrelevant knowledge from commonsense knowledge bases and ask humans to annotate\nwhether the knowledge is enough or not. Then, we convert different commonsense\ntasks into a unified question answering format to evaluate models'\ngeneralization capabilities. We name the benchmark as Commonsense Inference\nwith Knowledge-in-the-loop Question Answering (CIKQA).\n", "rewritten_text": "Recently, significant progress has been made in the community on various benchmarks related to common sense reasoning. However, there remains uncertainty regarding what is actually learned during the training process: is it knowledge, inference ability, or both? We propose that due to the vast amount of common sense knowledge, it is impractical to create a sufficiently large training dataset for each task that encompasses all common sense aspects for learning. Therefore, we suggest separating the acquisition of common sense knowledge from the inference over common sense knowledge as two distinct tasks. In this study, we concentrate on examining models' abilities to infer common sense from two angles: (1) determining if models can recognize when they possess enough knowledge to solve a task, and (2) assessing whether models can develop generalizable common sense inference capabilities across various tasks. We align common sense tasks with relevant information from common sense knowledge bases and have humans annotate whether the knowledge is adequate. Subsequently, we transform diverse common sense tasks into a standardized question-answering format to evaluate models' generalization abilities. This benchmark is referred to as Commonsense Inference with Knowledge-in-the-loop Question Answering (CIKQA)."}, "2409.11819": {"original_text": "  6D object pose estimation is the problem of identifying the position and\norientation of an object relative to a chosen coordinate system, which is a\ncore technology for modern XR applications. State-of-the-art 6D object pose\nestimators directly predict an object pose given an object observation. Due to\nthe ill-posed nature of the pose estimation problem, where multiple different\nposes can correspond to a single observation, generating additional plausible\nestimates per observation can be valuable. To address this, we reformulate the\nstate-of-the-art algorithm GDRNPP and introduce EPRO-GDR (End-to-End\nProbabilistic Geometry-Guided Regression). Instead of predicting a single pose\nper detection, we estimate a probability density distribution of the pose.\nUsing the evaluation procedure defined by the BOP (Benchmark for 6D Object Pose\nEstimation) Challenge, we test our approach on four of its core datasets and\ndemonstrate superior quantitative results for EPRO-GDR on LM-O, YCB-V, and\nITODD. Our probabilistic solution shows that predicting a pose distribution\ninstead of a single pose can improve state-of-the-art single-view pose\nestimation while providing the additional benefit of being able to sample\nmultiple meaningful pose candidates.\n", "rewritten_text": "The task of 6D object pose estimation involves determining the location and orientation of an object in relation to a specific coordinate system, which is crucial for modern XR applications. Leading 6D object pose estimation methods predict the object's pose directly from its observation. Given the ambiguous nature of pose estimation, where one observation can correspond to multiple poses, generating multiple plausible estimates per observation can be advantageous. To tackle this challenge, we enhance the existing GDRNPP algorithm with EPRO-GDR (End-to-End Probabilistic Geometry-Guided Regression). Instead of predicting a single pose for each detection, we estimate a probability density distribution of the pose. Through evaluation using the BOP (Benchmark for 6D Object Pose Estimation) Challenge's defined procedure, we evaluate our approach on key datasets and demonstrate superior quantitative results for EPRO-GDR on LM-O, YCB-V, and ITODD. Our probabilistic approach illustrates that predicting a distribution of poses rather than a single pose can enhance state-of-the-art single-view pose estimation and enable the sampling of multiple meaningful pose options."}, "2405.18111": {"original_text": "  Large language models (LLMs) are proven to benefit a lot from\nretrieval-augmented generation (RAG) in alleviating hallucinations confronted\nwith knowledge-intensive questions. RAG adopts information retrieval techniques\nto inject external knowledge from semantic-relevant documents as input\ncontexts. However, since today's Internet is flooded with numerous noisy and\nfabricating content, it is inevitable that RAG systems are vulnerable to these\nnoises and prone to respond incorrectly. To this end, we propose to optimize\nthe retrieval-augmented Generator with an Adversarial Tuning Multi-agent system\n(ATM). The ATM steers the Generator to have a robust perspective of useful\ndocuments for question answering with the help of an auxiliary Attacker agent\nthrough adversarially tuning the agents for several iterations. After rounds of\nmulti-agent iterative tuning, the Generator can eventually better discriminate\nuseful documents amongst fabrications. The experimental results verify the\neffectiveness of ATM and we also observe that the Generator can achieve better\nperformance compared to the state-of-the-art baselines.\n", "rewritten_text": "Large language models (LLMs) have been shown to greatly benefit from retrieval-augmented generation (RAG) in addressing hallucinations when faced with knowledge-intensive questions. RAG utilizes information retrieval techniques to incorporate external knowledge from semantically relevant documents as input contexts. However, given the abundance of noisy and misleading content on the internet today, RAG systems are susceptible to inaccuracies. To address this issue, we propose optimizing the retrieval-augmented Generator using an Adversarial Tuning Multi-agent system (ATM). The ATM guides the Generator to develop a robust understanding of relevant documents for question answering with the assistance of an auxiliary Attacker agent, by adversarially adjusting the agents through multiple iterations. Following rounds of multi-agent iterative tuning, the Generator becomes better equipped to distinguish between useful documents and fabrications. Experimental results confirm the effectiveness of ATM, demonstrating that the Generator can outperform current state-of-the-art approaches."}, "2309.00468": {"original_text": "  Dietary assessment is essential to maintaining a healthy lifestyle. Automatic\nimage-based dietary assessment is a growing field of research due to the\nincreasing prevalence of image capturing devices (e.g. mobile phones). In this\nwork, we estimate food energy from a single monocular image, a difficult task\ndue to the limited hard-to-extract amount of energy information present in an\nimage. To do so, we employ an improved encoder-decoder framework for energy\nestimation; the encoder transforms the image into a representation embedded\nwith food energy information in an easier-to-extract format, which the decoder\nthen extracts the energy information from. To implement our method, we compile\na high-quality food image dataset verified by registered dietitians containing\neating scene images, food-item segmentation masks, and ground truth calorie\nvalues. Our method improves upon previous caloric estimation methods by over\n10\\% and 30 kCal in terms of MAPE and MAE respectively.\n", "rewritten_text": "Assessing one's diet is crucial for maintaining a healthy lifestyle. The field of automatic image-based dietary assessment is gaining traction as the use of image-capturing devices like mobile phones becomes more common. In this study, we aim to estimate food energy from a single image, which is challenging due to the limited energy information that can be extracted from an image. Our approach involves a refined encoder-decoder framework for energy estimation. The encoder converts the image into a representation that includes food energy information in a more accessible format, which the decoder then extracts. To implement our method, we have curated a high-quality food image dataset verified by registered dietitians, comprising images of eating scenes, food-item segmentation masks, and accurate calorie values. Our method outperforms previous calorie estimation techniques by more than 10% in terms of MAPE and 30 kCal in terms of MAE."}, "2411.08196": {"original_text": "  Diffusion Transformers (DiTs) have recently achieved remarkable success in\ntext-guided image generation. In image editing, DiTs project text and image\ninputs to a joint latent space, from which they decode and synthesize new\nimages. However, it remains largely unexplored how multimodal information\ncollectively forms this joint space and how they guide the semantics of the\nsynthesized images. In this paper, we investigate the latent space of DiT\nmodels and uncover two key properties: First, DiT's latent space is inherently\nsemantically disentangled, where different semantic attributes can be\ncontrolled by specific editing directions. Second, consistent semantic editing\nrequires utilizing the entire joint latent space, as neither encoded image nor\ntext alone contains enough semantic information. We show that these editing\ndirections can be obtained directly from text prompts, enabling precise\nsemantic control without additional training or mask annotations. Based on\nthese insights, we propose a simple yet effective Encode-Identify-Manipulate\n(EIM) framework for zero-shot fine-grained image editing. Specifically, we\nfirst encode both the given source image and the text prompt that describes the\nimage, to obtain the joint latent embedding. Then, using our proposed Hessian\nScore Distillation Sampling (HSDS) method, we identify editing directions that\ncontrol specific target attributes while preserving other image features. These\ndirections are guided by text prompts and used to manipulate the latent\nembeddings. Moreover, we propose a new metric to quantify the disentanglement\ndegree of the latent space of diffusion models. Extensive experiment results on\nour new curated benchmark dataset and analysis demonstrate DiT's\ndisentanglement properties and effectiveness of the EIM framework.\n", "rewritten_text": "Recently, Diffusion Transformers (DiTs) have made significant strides in generating images guided by text. In the realm of image editing, DiTs align text and image inputs in a shared latent space to create new images. However, there is still much to explore regarding how different types of information come together in this shared space and influence the meaning of the generated images. This study delves into the latent space of DiT models and uncovers two key characteristics: Firstly, the latent space of DiTs naturally separates different semantic attributes, allowing for precise control over specific aspects through editing. Secondly, achieving consistent semantic edits necessitates leveraging the entire shared latent space, as neither the encoded image nor text alone provides sufficient semantic detail. The study demonstrates that these editing controls can be derived directly from text prompts, enabling accurate semantic manipulation without additional training or annotations. Building on these findings, the study introduces the Encode-Identify-Manipulate (EIM) framework for fine-grained image editing without prior training. The process involves encoding the source image and accompanying text prompt to create a joint latent embedding, identifying editing directions using the Hessian Score Distillation Sampling (HSDS) method, and manipulating the latent embeddings based on these directions guided by the text prompts. Additionally, a new metric is proposed to measure the degree of disentanglement in the latent space of diffusion models. Extensive experiments on a new benchmark dataset and analysis showcase the disentanglement capabilities of DiTs and the effectiveness of the EIM framework."}, "2403.06444": {"original_text": "  Estimating reliable geometric model parameters from the data with severe\noutliers is a fundamental and important task in computer vision. This paper\nattempts to sample high-quality subsets and select model instances to estimate\nparameters in the multi-structural data. To address this, we propose an\neffective method called Latent Semantic Consensus (LSC). The principle of LSC\nis to preserve the latent semantic consensus in both data points and model\nhypotheses. Specifically, LSC formulates the model fitting problem into two\nlatent semantic spaces based on data points and model hypotheses, respectively.\nThen, LSC explores the distributions of points in the two latent semantic\nspaces, to remove outliers, generate high-quality model hypotheses, and\neffectively estimate model instances. Finally, LSC is able to provide\nconsistent and reliable solutions within only a few milliseconds for general\nmulti-structural model fitting, due to its deterministic fitting nature and\nefficiency. Compared with several state-of-the-art model fitting methods, our\nLSC achieves significant superiority for the performance of both accuracy and\nspeed on synthetic data and real images. The code will be available at\nhttps://github.com/guobaoxiao/LSC.\n", "rewritten_text": "This paper focuses on the challenge of accurately estimating geometric model parameters from data that contains outliers, which is crucial in computer vision. The proposed method, called Latent Semantic Consensus (LSC), aims to select high-quality subsets and model instances to estimate parameters in multi-structural data. LSC works by preserving latent semantic consensus in both data points and model hypotheses. By formulating the model fitting problem in two latent semantic spaces based on data points and model hypotheses, LSC identifies outliers, generates high-quality model hypotheses, and effectively estimates model instances. LSC provides consistent and reliable solutions quickly, making it superior in accuracy and speed compared to other model fitting methods. The code for LSC can be found at https://github.com/guobaoxiao/LSC."}, "1708.0923": {"original_text": "  Named Entity Recognition and Disambiguation (NERD) systems have recently been\nwidely researched to deal with the significant growth of the Web. NERD systems\nare crucial for several Natural Language Processing (NLP) tasks such as\nsummarization, understanding, and machine translation. However, there is no\nstandard interface specification, i.e. these systems may vary significantly\neither for exporting their outputs or for processing the inputs. Thus, when a\ngiven company desires to implement more than one NERD system, the process is\nquite exhaustive and prone to failure. In addition, industrial solutions demand\ncritical requirements, e.g., large-scale processing, completeness, versatility,\nand licenses. Commonly, these requirements impose a limitation, making good\nNERD models to be ignored by companies. This paper presents TANKER, a\ndistributed architecture which aims to overcome scalability, reliability and\nfailure tolerance limitations related to industrial needs by combining NERD\nsystems. To this end, TANKER relies on a micro-services oriented architecture,\nwhich enables agile development and delivery of complex enterprise\napplications. In addition, TANKER provides a standardized API which makes\npossible to combine several NERD systems at once.\n", "rewritten_text": "Recently, there has been extensive research on Named Entity Recognition and Disambiguation (NERD) systems to address the rapid growth of the Web. These systems play a crucial role in various Natural Language Processing (NLP) tasks such as summarization, understanding, and machine translation. However, the lack of a standard interface specification means that NERD systems can vary significantly in how they export outputs or process inputs. This variability makes it challenging for companies to implement multiple NERD systems efficiently, leading to potential failures. Industrial solutions have specific requirements like large-scale processing, completeness, versatility, and licensing, which can limit the adoption of effective NERD models by companies. This paper introduces TANKER, a distributed architecture designed to address scalability, reliability, and failure tolerance issues related to industrial needs by integrating multiple NERD systems. TANKER utilizes a micro-services oriented architecture for agile development and delivery of complex enterprise applications. Additionally, TANKER offers a standardized API that enables the combination of multiple NERD systems simultaneously."}, "1708.01846": {"original_text": "  Low-rank decomposition (LRD) is a state-of-the-art method for visual data\nreconstruction and modelling. However, it is a very challenging problem when\nthe image data contains significant occlusion, noise, illumination variation,\nand misalignment from rotation or viewpoint changes. We leverage the specific\nstructure of data in order to improve the performance of LRD when the data are\nnot ideal. To this end, we propose a new framework that embeds manifold priors\ninto LRD. To implement the framework, we design an alternating direction method\nof multipliers (ADMM) method which efficiently integrates the manifold\nconstraints during the optimization process. The proposed approach is\nsuccessfully used to calculate low-rank models from face images, hand-written\ndigits and planar surface images. The results show a consistent increase of\nperformance when compared to the state-of-the-art over a wide range of\nrealistic image misalignments and corruptions.\n", "rewritten_text": "Low-rank decomposition (LRD) is a cutting-edge technique used for reconstructing and modeling visual data. However, it becomes particularly challenging when dealing with image data that includes significant occlusion, noise, varying illumination, and misalignment due to rotation or changes in viewpoint. In order to enhance the performance of LRD under less-than-ideal conditions, we exploit the inherent structure of the data. Our solution involves introducing a novel framework that incorporates manifold priors into LRD. This framework is implemented using an alternating direction method of multipliers (ADMM) that effectively integrates manifold constraints into the optimization process. Our proposed approach has been successfully applied to generating low-rank models from face images, hand-written digits, and planar surface images. The results demonstrate a consistent performance improvement compared to existing methods across a wide range of realistic image misalignments and corruptions."}, "2001.0732": {"original_text": "  It's natural these days for people to know the local events from massive\ndocuments. Many texts contain location information, such as city name or road\nname, which is always incomplete or latent. It's significant to extract the\nadministrative area of the text and organize the hierarchy of area, called\nlocation normalization. Existing detecting location systems either exclude\nhierarchical normalization or present only a few specific regions. We propose a\nsystem named ROIBase that normalizes the text by the Chinese hierarchical\nadministrative divisions. ROIBase adopts a co-occurrence constraint as the\nbasic framework to score the hit of the administrative area, achieves the\ninference by special embeddings, and expands the recall by the ROI (region of\ninterest). It has high efficiency and interpretability because it mainly\nestablishes on the definite knowledge and has less complex logic than the\nsupervised models. We demonstrate that ROIBase achieves better performance\nagainst feasible solutions and is useful as a strong support system for\nlocation normalization.\n", "rewritten_text": "These days, it is common for people to gather information about local events from large documents. Many texts include location details, such as city or road names, which are often incomplete or ambiguous. It is important to identify the administrative area mentioned in the text and organize the hierarchy of locations, a process known as location normalization. Existing location detection systems may not prioritize hierarchical normalization or may focus on only a few specific regions. Our proposed system, ROIBase, aims to normalize text based on the hierarchical administrative divisions in China. ROIBase utilizes a co-occurrence constraint framework to assess the relevance of the administrative area, employs special embeddings for inference, and enhances recall through the region of interest (ROI). This system is efficient and easy to interpret as it relies on established knowledge and has simpler logic compared to supervised models. Our study shows that ROIBase outperforms other solutions and serves as a valuable tool for location normalization."}, "1908.02505": {"original_text": "  This paper presents a scoring system that has shown the top result on the\ntext subset of CALL v3 shared task. The presented system is based on text\nembeddings, namely NNLM~\\cite{nnlm} and BERT~\\cite{Bert}. The distinguishing\nfeature of the given approach is that it does not rely on the reference grammar\nfile for scoring. The model is compared against approaches that use the grammar\nfile and proves the possibility to achieve similar and even higher results\nwithout a predefined set of correct answers.\n  The paper describes the model itself and the data preparation process that\nplayed a crucial role in the model training.\n", "rewritten_text": "This paper introduces a scoring system that has achieved the highest performance in the text subset of the CALL v3 shared task. The system utilizes text embeddings, specifically NNLM~\\cite{nnlm} and BERT~\\cite{Bert}. A key aspect of this approach is its independence from a reference grammar file for scoring. By comparing the model to approaches that rely on the grammar file, it demonstrates the potential to achieve comparable or superior results without a predefined set of correct answers. The paper details the model and the essential data preparation process that significantly influenced the model training."}, "1604.0801": {"original_text": "  The purpose of this paper is the detection of salient areas in natural video\nby using the new deep learning techniques. Salient patches in video frames are\npredicted first. Then the predicted visual fixation maps are built upon them.\nWe design the deep architecture on the basis of CaffeNet implemented with Caffe\ntoolkit. We show that changing the way of data selection for optimisation of\nnetwork parameters, we can save computation cost up to 12 times. We extend deep\nlearning approaches for saliency prediction in still images with RGB values to\nspecificity of video using the sensitivity of the human visual system to\nresidual motion. Furthermore, we complete primary colour pixel values by\ncontrast features proposed in classical visual attention prediction models. The\nexperiments are conducted on two publicly available datasets. The first is\nIRCCYN video database containing 31 videos with an overall amount of 7300\nframes and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided\n2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, the\naccuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction of\nsaliency of patches show the improvement up to 2% with regard to RGB use only.\nThe resulting accuracy of 76, 6% is obtained. The AUC metric in comparison of\npredicted saliency maps with visual fixation maps shows the increase up to 16%\non a sample of video clips from this dataset.\n", "rewritten_text": "The aim of this paper is to identify important areas in natural videos using advanced deep learning techniques. Initially, salient patches in video frames are predicted, followed by the creation of visual fixation maps based on these predictions. The deep architecture is developed using CaffeNet and the Caffe toolkit. By optimizing network parameters through a different data selection approach, a computation cost reduction of up to 12 times is achieved. The study expands deep learning methods for predicting saliency in video by incorporating human visual system sensitivity to residual motion, and enhancing primary color pixel values with contrast features from traditional visual attention models. Experiments are carried out on two publicly available datasets: the IRCCYN video database with 31 videos and 7300 frames, and the HOLLYWOOD2 dataset with 2517 movie clips. Results show an accuracy of 89.51% on the IRCCYN dataset and a 2% improvement in saliency prediction on HOLLYWOOD2 compared to using only RGB values, resulting in an accuracy of 76.6%. The AUC metric demonstrates a 16% increase in predicted saliency maps compared to visual fixation maps on a subset of video clips from the HOLLYWOOD2 dataset."}, "2301.03182": {"original_text": "  Existing deep learning-based shadow removal methods still produce images with\nshadow remnants. These shadow remnants typically exist in homogeneous regions\nwith low-intensity values, making them untraceable in the existing\nimage-to-image mapping paradigm. We observe that shadows mainly degrade images\nat the image-structure level (in which humans perceive object shapes and\ncontinuous colors). Hence, in this paper, we propose to remove shadows at the\nimage structure level. Based on this idea, we propose a novel\nstructure-informed shadow removal network (StructNet) to leverage the\nimage-structure information to address the shadow remnant problem.\nSpecifically, StructNet first reconstructs the structure information of the\ninput image without shadows and then uses the restored shadow-free structure\nprior to guiding the image-level shadow removal. StructNet contains two main\nnovel modules: (1) a mask-guided shadow-free extraction (MSFE) module to\nextract image structural features in a non-shadow-to-shadow directional manner,\nand (2) a multi-scale feature & residual aggregation (MFRA) module to leverage\nthe shadow-free structure information to regularize feature consistency. In\naddition, we also propose to extend StructNet to exploit multi-level structure\ninformation (MStructNet), to further boost the shadow removal performance with\nminimum computational overheads. Extensive experiments on three shadow removal\nbenchmarks demonstrate that our method outperforms existing shadow removal\nmethods, and our StructNet can be integrated with existing methods to improve\nthem further.\n", "rewritten_text": "The current deep learning-based techniques for removing shadows still leave remnants in the resulting images. These remnants are often found in uniform areas with low intensity, making them difficult to detect using the current image-to-image mapping approach. We have noticed that shadows primarily affect images at the level of image structure, where object shapes and color continuity are perceived by humans. Therefore, in this study, we suggest eliminating shadows at the image structure level. To achieve this, we introduce a new network called StructNet, which utilizes image structure information to address the issue of shadow remnants. StructNet works by first reconstructing the structure information of the input image without shadows, then using this shadow-free structure to guide the removal of shadows at the image level. The network consists of two key components: a mask-guided shadow-free extraction (MSFE) module for extracting image structural features in a direction from non-shadow to shadow, and a multi-scale feature & residual aggregation (MFRA) module for utilizing the shadow-free structure information to maintain feature consistency. Additionally, we propose extending StructNet to incorporate multi-level structure information (MStructNet) to enhance shadow removal performance with minimal computational overhead. Through extensive experiments on three shadow removal benchmarks, we demonstrate that our approach surpasses existing methods, and that StructNet can be combined with existing techniques to further enhance their effectiveness."}, "1709.0586": {"original_text": "  We present a novel method for cell segmentation in microscopy images which is\ninspired by the Generative Adversarial Neural Network (GAN) approach. Our\nframework is built on a pair of two competitive artificial neural networks,\nwith a unique architecture, termed Rib Cage, which are trained simultaneously\nand together define a min-max game resulting in an accurate segmentation of a\ngiven image. Our approach has two main strengths, similar to the GAN, the\nmethod does not require a formulation of a loss function for the optimization\nprocess. This allows training on a limited amount of annotated data in a weakly\nsupervised manner. Promising segmentation results on real fluorescent\nmicroscopy data are presented. The code is freely available at:\nhttps://github.com/arbellea/DeepCellSeg.git\n", "rewritten_text": "We introduce an innovative technique for segmenting cells in microscopy images, drawing inspiration from the Generative Adversarial Neural Network (GAN) methodology. Our system consists of a pair of competitive artificial neural networks, known as Rib Cage, with a unique architecture. These networks are trained simultaneously in a min-max game setup, leading to precise image segmentation. Similar to GAN, our method boasts two key advantages: it does not necessitate the formulation of a loss function for optimization, enabling training with minimal annotated data in a weakly supervised manner. We demonstrate promising segmentation outcomes on actual fluorescent microscopy data. The code can be accessed freely at: https://github.com/arbellea/DeepCellSeg.git"}, "2203.14565": {"original_text": "  Domain adaptation (DA) or domain generalization (DG) for face presentation\nattack detection (PAD) has attracted attention recently with its robustness\nagainst unseen attack scenarios. Existing DA/DG-based PAD methods, however,\nhave not yet fully explored the domain-specific style information that can\nprovide knowledge regarding attack styles (e.g., materials, background,\nillumination and resolution). In this paper, we introduce a novel Style-Guided\nDomain Adaptation (SGDA) framework for inference-time adaptive PAD.\nSpecifically, Style-Selective Normalization (SSN) is proposed to explore the\ndomain-specific style information within the high-order feature statistics. The\nproposed SSN enables the adaptation of the model to the target domain by\nreducing the style difference between the target and the source domains.\nMoreover, we carefully design Style-Aware Meta-Learning (SAML) to boost the\nadaptation ability, which simulates the inference-time adaptation with style\nselection process on virtual test domain. In contrast to previous domain\nadaptation approaches, our method does not require either additional auxiliary\nmodels (e.g., domain adaptors) or the unlabeled target domain during training,\nwhich makes our method more practical to PAD task. To verify our experiments,\nwe utilize the public datasets: MSU-MFSD, CASIA-FASD, OULU-NPU and Idiap\nREPLAYATTACK. In most assessments, the result demonstrates a notable gap of\nperformance compared to the conventional DA/DG-based PAD methods.\n", "rewritten_text": "Recently, there has been a growing interest in domain adaptation (DA) and domain generalization (DG) for detecting face presentation attacks (PAD) due to their ability to remain robust against unforeseen attack scenarios. However, current methods based on DA/DG for PAD have not fully tapped into the domain-specific style information that could offer insights into attack styles such as materials, background, illumination, and resolution. This paper introduces a new framework called Style-Guided Domain Adaptation (SGDA) for adaptive PAD at inference time. The framework proposes Style-Selective Normalization (SSN) to leverage domain-specific style information in high-order feature statistics, facilitating model adaptation to the target domain by minimizing style discrepancies between the target and source domains. Additionally, Style-Aware Meta-Learning (SAML) is carefully designed to enhance adaptation capabilities by simulating inference-time adaptation through a style selection process on a virtual test domain. Unlike previous domain adaptation methods, our approach does not require additional auxiliary models (e.g., domain adaptors) or unlabeled target domain data during training, making it more practical for PAD tasks. Experimental validation is conducted using public datasets such as MSU-MFSD, CASIA-FASD, OULU-NPU, and Idiap REPLAYATTACK. The results show a significant performance improvement compared to traditional DA/DG-based PAD methods in most evaluations."}, "1905.08511": {"original_text": "  Question answering (QA) using textual sources for purposes such as reading\ncomprehension (RC) has attracted much attention. This study focuses on the task\nof explainable multi-hop QA, which requires the system to return the answer\nwith evidence sentences by reasoning and gathering disjoint pieces of the\nreference texts. It proposes the Query Focused Extractor (QFE) model for\nevidence extraction and uses multi-task learning with the QA model. QFE is\ninspired by extractive summarization models; compared with the existing method,\nwhich extracts each evidence sentence independently, it sequentially extracts\nevidence sentences by using an RNN with an attention mechanism on the question\nsentence. It enables QFE to consider the dependency among the evidence\nsentences and cover important information in the question sentence.\nExperimental results show that QFE with a simple RC baseline model achieves a\nstate-of-the-art evidence extraction score on HotpotQA. Although designed for\nRC, it also achieves a state-of-the-art evidence extraction score on FEVER,\nwhich is a recognizing textual entailment task on a large textual database.\n", "rewritten_text": "The study focuses on explainable multi-hop question answering (QA) using textual sources, such as reading comprehension (RC). It introduces the Query Focused Extractor (QFE) model for evidence extraction, which aims to return answers with supporting sentences by reasoning and gathering information from reference texts. Unlike existing methods that extract evidence sentences independently, QFE sequentially extracts evidence sentences using an RNN with an attention mechanism on the question sentence. This approach allows QFE to consider the relationship among evidence sentences and capture important information from the question. Experimental results demonstrate that QFE, when combined with a simple RC baseline model, achieves a state-of-the-art evidence extraction score on HotpotQA and FEVER, a recognizing textual entailment task on a large textual database."}, "2310.18205": {"original_text": "  Claim span identification (CSI) is an important step in fact-checking\npipelines, aiming to identify text segments that contain a checkworthy claim or\nassertion in a social media post. Despite its importance to journalists and\nhuman fact-checkers, it remains a severely understudied problem, and the scarce\nresearch on this topic so far has only focused on English. Here we aim to\nbridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K\nreal-world claims collected from numerous social media platforms in five Indian\nlanguages and English. We report strong baselines with state-of-the-art\nencoder-only language models (e.g., XLM-R) and we demonstrate the benefits of\ntraining on multiple languages over alternative cross-lingual transfer methods\nsuch as zero-shot transfer, or training on translated data, from a\nhigh-resource language such as English. We evaluate generative large language\nmodels from the GPT series using prompting methods on the X-CLAIM dataset and\nwe find that they underperform the smaller encoder-only language models for\nlow-resource languages.\n", "rewritten_text": "\"Identifying claim spans (CSI) is a crucial step in fact-checking processes, with the goal of pinpointing text segments containing claims or assertions worth verifying in social media posts. Despite its significance for journalists and fact-checkers, this area has received limited research attention, primarily focusing on English. To address this gap, we introduce a new dataset called X-CLAIM, comprising 7,000 real-world claims gathered from various social media platforms in five Indian languages and English. We present strong baseline results using advanced encoder-only language models like XLM-R, highlighting the advantages of training on multiple languages compared to other cross-lingual transfer methods such as zero-shot transfer or training on translated data from a high-resource language like English. Our evaluation of generative large language models from the GPT series on the X-CLAIM dataset reveals that they perform less effectively than smaller encoder-only models for low-resource languages when using prompting techniques.\""}, "2306.08075": {"original_text": "  Current knowledge distillation approaches in semantic segmentation tend to\nadopt a holistic approach that treats all spatial locations equally. However,\nfor dense prediction, students' predictions on edge regions are highly\nuncertain due to contextual information leakage, requiring higher spatial\nsensitivity knowledge than the body regions. To address this challenge, this\npaper proposes a novel approach called boundary-privileged knowledge\ndistillation (BPKD). BPKD distills the knowledge of the teacher model's body\nand edges separately to the compact student model. Specifically, we employ two\ndistinct loss functions: (i) edge loss, which aims to distinguish between\nambiguous classes at the pixel level in edge regions; (ii) body loss, which\nutilizes shape constraints and selectively attends to the inner-semantic\nregions. Our experiments demonstrate that the proposed BPKD method provides\nextensive refinements and aggregation for edge and body regions. Additionally,\nthe method achieves state-of-the-art distillation performance for semantic\nsegmentation on three popular benchmark datasets, highlighting its\neffectiveness and generalization ability. BPKD shows consistent improvements\nacross a diverse array of lightweight segmentation structures, including both\nCNNs and transformers, underscoring its architecture-agnostic adaptability. The\ncode is available at \\url{https://github.com/AkideLiu/BPKD}.\n", "rewritten_text": "The current approaches to knowledge distillation in semantic segmentation typically treat all spatial locations equally, but this may lead to uncertainty in students' predictions on edge regions. To address this issue, this paper introduces a new method called boundary-privileged knowledge distillation (BPKD). BPKD separates the teacher model's knowledge of body and edges and transfers it to a compact student model. This is achieved through two loss functions: edge loss, which focuses on pixel-level distinctions in edge regions, and body loss, which emphasizes inner-semantic regions. Experimental results show that BPKD significantly improves the segmentation of edge and body regions, outperforming existing methods on popular benchmark datasets. BPKD is versatile, demonstrating superior performance across various lightweight segmentation structures, including CNNs and transformers. The code for BPKD is available at \\url{https://github.com/AkideLiu/BPKD}."}, "2010.05762": {"original_text": "  Scene flow represents the 3D motion of every point in the dynamic\nenvironments. Like the optical flow that represents the motion of pixels in 2D\nimages, 3D motion representation of scene flow benefits many applications, such\nas autonomous driving and service robot. This paper studies the problem of\nscene flow estimation from two consecutive 3D point clouds. In this paper, a\nnovel hierarchical neural network with double attention is proposed for\nlearning the correlation of point features in adjacent frames and refining\nscene flow from coarse to fine layer by layer. The proposed network has a new\nmore-for-less hierarchical architecture. The more-for-less means that the\nnumber of input points is greater than the number of output points for scene\nflow estimation, which brings more input information and balances the precision\nand resource consumption. In this hierarchical architecture, scene flow of\ndifferent levels is generated and supervised respectively. A novel attentive\nembedding module is introduced to aggregate the features of adjacent points\nusing a double attention method in a patch-to-patch manner. The proper layers\nfor flow embedding and flow supervision are carefully considered in our network\ndesignment. Experiments show that the proposed network outperforms the\nstate-of-the-art performance of 3D scene flow estimation on the FlyingThings3D\nand KITTI Scene Flow 2015 datasets. We also apply the proposed network to\nrealistic LiDAR odometry task, which is an key problem in autonomous driving.\nThe experiment results demonstrate that our proposed network can outperform the\nICP-based method and shows the good practical application ability.\n", "rewritten_text": "The concept of scene flow involves capturing the 3D motion of all points within dynamic environments. Similar to optical flow, which tracks pixel movement in 2D images, scene flow in 3D has various practical applications, such as in autonomous driving and service robots. This study focuses on estimating scene flow from consecutive 3D point clouds. A novel hierarchical neural network with dual attention mechanisms is proposed to learn the relationships between point features in adjacent frames and refine scene flow progressively from coarse to fine layers. The network's architecture follows a more-for-less approach, where the number of input points exceeds the output points for scene flow estimation, enhancing input information while balancing precision and resource usage. Different levels of scene flow are generated and supervised within this hierarchical structure. An innovative attentive embedding module is introduced to combine features of neighboring points using a dual attention method in a patch-to-patch manner. The network's design carefully considers the appropriate layers for flow embedding and supervision. Experimental results demonstrate that the proposed network surpasses current methods in 3D scene flow estimation on datasets like FlyingThings3D and KITTI Scene Flow 2015. Additionally, the network is applied to a practical LiDAR odometry task crucial for autonomous driving, outperforming the traditional ICP-based method and showcasing strong practical utility."}, "2303.14829": {"original_text": "  Generating grammatically and semantically correct captions in video\ncaptioning is a challenging task. The captions generated from the existing\nmethods are either word-by-word that do not align with grammatical structure or\nmiss key information from the input videos. To address these issues, we\nintroduce a novel global-local fusion network, with a Global-Local Fusion Block\n(GLFB) that encodes and fuses features from different parts of speech (POS)\ncomponents with visual-spatial features. We use novel combinations of different\nPOS components - 'determinant + subject', 'auxiliary verb', 'verb', and\n'determinant + object' for supervision of the POS blocks - Det + Subject, Aux\nVerb, Verb, and Det + Object respectively. The novel global-local fusion\nnetwork together with POS blocks helps align the visual features with language\ndescription to generate grammatically and semantically correct captions.\nExtensive qualitative and quantitative experiments on benchmark MSVD and MSRVTT\ndatasets demonstrate that the proposed approach generates more grammatically\nand semantically correct captions compared to the existing methods, achieving\nthe new state-of-the-art. Ablations on the POS blocks and the GLFB demonstrate\nthe impact of the contributions on the proposed method.\n", "rewritten_text": "Creating accurate captions that are both grammatically and semantically correct in video captioning poses a significant challenge. Current methods often produce captions that lack proper grammatical structure or fail to include essential information from the input videos. To overcome these shortcomings, we present a new approach called the global-local fusion network, which incorporates a Global-Local Fusion Block (GLFB) to combine and process features from various parts of speech (POS) components alongside visual-spatial features. Our method utilizes unique combinations of POS components such as 'determinant + subject', 'auxiliary verb', 'verb', and 'determinant + object' to supervise the corresponding POS blocks - Det + Subject, Aux Verb, Verb, and Det + Object. By integrating the global-local fusion network with POS blocks, we effectively align visual features with language descriptions to generate captions that are both grammatically and semantically accurate. Through extensive qualitative and quantitative experiments on standard MSVD and MSRVTT datasets, we demonstrate that our proposed approach outperforms existing methods, establishing a new state-of-the-art. Furthermore, ablation studies on the POS blocks and the GLFB highlight the significant contributions of these components to the overall effectiveness of our method."}, "1805.11788": {"original_text": "  Traditional Convolutional Neural Networks (CNNs) typically use the same\nactivation function (usually ReLU) for all neurons with non-linear mapping\noperations. For example, the deep convolutional architecture Inception-v4 uses\nReLU. To improve the classification performance of traditional CNNs, a new\n\"Multi-function Convolutional Neural Network\" (MCNN) is created by using\ndifferent activation functions for different neurons. For $n$ neurons and $m$\ndifferent activation functions, there are a total of $m^n-m$ MCNNs and only $m$\ntraditional CNNs. Therefore, the best model is very likely to be chosen from\nMCNNs because there are $m^n-2m$ more MCNNs than traditional CNNs. For\nperformance analysis, two different datasets for two applications (classifying\nhandwritten digits from the MNIST database and classifying brain MRI images\ninto one of the four stages of Alzheimer's disease (AD)) are used. For both\napplications, an activation function is randomly selected for each layer of a\nMCNN. For the AD diagnosis application, MCNNs using a newly created\nmulti-function Inception-v4 architecture are constructed. Overall, simulations\nshow that MCNNs can outperform traditional CNNs in terms of multi-class\nclassification accuracy for both applications. An important future research\nwork will be to efficiently select the best MCNN from $m^n-m$ candidate MCNNs.\nCurrent CNN software only provides users with partial functionality of MCNNs\nsince different layers can use different activation functions but not\nindividual neurons in the same layer. Thus, modifying current CNN software\nsystems such as ResNets, DenseNets, and Dual Path Networks by using multiple\nactivation functions and developing more effective and faster MCNN software\nsystems and tools would be very useful to solve difficult practical image\nclassification problems.\n", "rewritten_text": "Traditional Convolutional Neural Networks (CNNs) typically utilize a uniform activation function, often ReLU, across all neurons for non-linear mapping operations. In contrast, a new approach called \"Multi-function Convolutional Neural Network\" (MCNN) has been developed to enhance classification performance by employing various activation functions for different neurons. With $n$ neurons and $m$ distinct activation functions, there exist a total of $m^n-m$ MCNNs compared to only $m$ traditional CNNs. Consequently, the likelihood of selecting the best model from MCNNs is high due to the significantly larger number of MCNN options. Performance evaluation is conducted using two datasets for classifying handwritten digits from the MNIST database and categorizing brain MRI images into four stages of Alzheimer's disease (AD). For the AD diagnosis task, MCNNs are built using a novel multi-function Inception-v4 architecture. Simulation results demonstrate that MCNNs can surpass traditional CNNs in multi-class classification accuracy for both applications. Future research should focus on efficiently selecting the optimal MCNN from the pool of $m^n-m$ candidate MCNNs. Current CNN software offers limited functionality for MCNNs, as it allows different layers to have distinct activation functions but not individual neurons within the same layer. Therefore, enhancing existing CNN software systems like ResNets, DenseNets, and Dual Path Networks to incorporate multiple activation functions and developing more efficient and faster MCNN software tools would be beneficial for addressing challenging image classification tasks."}, "2108.024": {"original_text": "  Gait data captured by inertial sensors have demonstrated promising results on\nuser authentication. However, most existing approaches stored the enrolled gait\npattern insecurely for matching with the validating pattern, thus, posed\ncritical security and privacy issues. In this study, we present a gait\ncryptosystem that generates from gait data the random key for user\nauthentication, meanwhile, secures the gait pattern. First, we propose a\nrevocable and random binary string extraction method using a deep neural\nnetwork followed by feature-wise binarization. A novel loss function for\nnetwork optimization is also designed, to tackle not only the intrauser\nstability but also the inter-user randomness. Second, we propose a new\nbiometric key generation scheme, namely Irreversible Error Correct and\nObfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme,\nto securely generate from the binary string the random and irreversible key.\nThe model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. We\nshowed that our model could generate the key of 139 bits from 5-second data\nsequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR)\nsmaller than 5.441%. In addition, the security and user privacy analyses showed\nthat our model was secure against existing attacks on biometric template\nprotection, and fulfilled irreversibility and unlinkability.\n", "rewritten_text": "The use of gait data collected through inertial sensors has shown promising results in user authentication. However, many current methods store the registered gait pattern in an insecure manner for comparison with the validating pattern, leading to significant security and privacy concerns. This study introduces a gait cryptosystem that generates a random key for user authentication from gait data while also ensuring the security of the gait pattern. Initially, a method is proposed for extracting a revocable and random binary string using a deep neural network and feature-wise binarization. A unique loss function is designed for network optimization to address both the stability within users and the randomness between users. Subsequently, a new biometric key generation scheme called Irreversible Error Correct and Obfuscate (IECO) is introduced, building upon the Error Correct and Obfuscate (ECO) scheme to securely create a random and irreversible key from the binary string. The model is evaluated using two benchmark datasets, OU-ISIR and whuGAIT, demonstrating the ability to generate a 139-bit key from a 5-second data sequence with a zero False Acceptance Rate (FAR) and a False Rejection Rate (FRR) below 5.441%. Furthermore, security and user privacy analyses confirm that the model is resilient against existing attacks on biometric template protection, meeting requirements for irreversibility and unlinkability."}, "2404.03443": {"original_text": "  The goal of occluded person re-identification (ReID) is to retrieve specific\npedestrians in occluded situations. However, occluded person ReID still suffers\nfrom background clutter and low-quality local feature representations, which\nlimits model performance. In our research, we introduce a new framework called\nPAB-ReID, which is a novel ReID model incorporating part-attention mechanisms\nto tackle the aforementioned issues effectively. Firstly, we introduce the\nhuman parsing label to guide the generation of more accurate human part\nattention maps. In addition, we propose a fine-grained feature focuser for\ngenerating fine-grained human local feature representations while suppressing\nbackground interference. Moreover, We also design a part triplet loss to\nsupervise the learning of human local features, which optimizes\nintra/inter-class distance. We conducted extensive experiments on specialized\nocclusion and regular ReID datasets, showcasing that our approach outperforms\nthe existing state-of-the-art methods.\n", "rewritten_text": "The objective of occluded person re-identification (ReID) is to identify specific individuals in obstructed scenarios. However, occluded person ReID faces challenges such as background clutter and subpar local feature representations, which hinder model performance. In our study, we present a new framework named PAB-ReID, which is an innovative ReID model that incorporates part-attention mechanisms to effectively address these issues. To begin with, we utilize human parsing labels to enhance the accuracy of human part attention maps. Additionally, we introduce a fine-grained feature focuser to generate detailed human local feature representations while minimizing background interference. Furthermore, we introduce a part triplet loss to supervise the learning of human local features, optimizing intra/inter-class distance. Through extensive experiments on specialized occlusion and standard ReID datasets, we demonstrate that our approach surpasses current state-of-the-art methods."}, "2310.05620": {"original_text": "  General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic.\n", "rewritten_text": "LLMs specializing in general and legal domains have shown strong performance in various LegalAI tasks. However, current evaluations of these LLMs in LegalAI are conducted by computer science experts, which may not align with the logic of legal practice, making it challenging to assess their practical capabilities. To address this issue, we have developed the Chinese legal LLMs benchmark LAiW, which is based on the logic of legal practice. To better reflect the thinking process of legal experts and legal practice, we have categorized the legal capabilities of LLMs into three levels: basic information retrieval, legal foundation inference, and complex legal application. Each level includes multiple tasks for a comprehensive evaluation. Our automated evaluation of existing general and legal domain LLMs on this benchmark suggests that these models may not fully align with the logic of legal practice. While LLMs show potential in acquiring complex legal application capabilities, they struggle with basic tasks, which could hinder their practical use and acceptance by legal experts. To further validate the complex legal application capabilities of current LLMs in legal scenarios, we have also included human evaluation with legal experts. The results indicate that although LLMs demonstrate strong performance, they still need to strengthen their understanding of legal logic."}, "2408.12677": {"original_text": "  Traditional volumetric fusion algorithms preserve the spatial structure of 3D\nscenes, which is beneficial for many tasks in computer vision and robotics.\nHowever, they often lack realism in terms of visualization. Emerging 3D\nGaussian splatting bridges this gap, but existing Gaussian-based reconstruction\nmethods often suffer from artifacts and inconsistencies with the underlying 3D\nstructure, and struggle with real-time optimization, unable to provide users\nwith immediate feedback in high quality. One of the bottlenecks arises from the\nmassive amount of Gaussian parameters that need to be updated during\noptimization. Instead of using 3D Gaussian as a standalone map representation,\nwe incorporate it into a volumetric mapping system to take advantage of\ngeometric information and propose to use a quadtree data structure on images to\ndrastically reduce the number of splats initialized. In this way, we\nsimultaneously generate a compact 3D Gaussian map with fewer artifacts and a\nvolumetric map on the fly. Our method, GSFusion, significantly enhances\ncomputational efficiency without sacrificing rendering quality, as demonstrated\non both synthetic and real datasets. Code will be available at\nhttps://github.com/goldoak/GSFusion.\n", "rewritten_text": "\"Traditional volumetric fusion techniques maintain the spatial structure of 3D scenes, which is advantageous for various applications in computer vision and robotics. However, they often lack realism in terms of visualization. The emerging approach of 3D Gaussian splatting addresses this issue, but current Gaussian-based reconstruction methods can suffer from artifacts and inconsistencies with the underlying 3D structure, making real-time optimization challenging and hindering the ability to provide users with immediate high-quality feedback. One of the main challenges is the large number of Gaussian parameters that require updating during optimization. Instead of using 3D Gaussian as a standalone map representation, we integrate it into a volumetric mapping system to leverage geometric information and propose the use of a quadtree data structure on images to significantly reduce the number of initialized splats. This approach allows for the simultaneous generation of a compact 3D Gaussian map with fewer artifacts and a volumetric map in real-time. Our method, GSFusion, greatly improves computational efficiency without compromising rendering quality, as demonstrated on both synthetic and real datasets. The code will be accessible at https://github.com/goldoak/GSFusion.\""}, "2309.14962": {"original_text": "  All tables can be represented as grids. Based on this observation, we propose\nGridFormer, a novel approach for interpreting unconstrained table structures by\npredicting the vertex and edge of a grid. First, we propose a flexible table\nrepresentation in the form of an MXN grid. In this representation, the vertexes\nand edges of the grid store the localization and adjacency information of the\ntable. Then, we introduce a DETR-style table structure recognizer to\nefficiently predict this multi-objective information of the grid in a single\nshot. Specifically, given a set of learned row and column queries, the\nrecognizer directly outputs the vertexes and edges information of the\ncorresponding rows and columns. Extensive experiments on five challenging\nbenchmarks which include wired, wireless, multi-merge-cell, oriented, and\ndistorted tables demonstrate the competitive performance of our model over\nother methods.\n", "rewritten_text": "\"All tables can be visualized as grids. With this in mind, we introduce GridFormer, a new method for interpreting diverse table structures by predicting the vertices and edges of a grid. Initially, we propose a versatile table representation in the shape of an MXN grid. This representation captures the positioning and connectivity details of the table within its vertices and edges. Subsequently, we present a DETR-style table structure recognizer to efficiently forecast this comprehensive grid information in a single step. By utilizing a set of trained row and column queries, the recognizer promptly generates the vertexes and edges data for the corresponding rows and columns. Through extensive experiments on five challenging benchmarks encompassing various table types, our model showcases superior performance compared to alternative approaches.\""}, "2303.17658": {"original_text": "  Machine learning models deployed in the open world may encounter observations\nthat they were not trained to recognize, and they risk misclassifying such\nobservations with high confidence. Therefore, it is essential that these models\nare able to ascertain what is in-distribution (ID) and out-of-distribution\n(OOD), to avoid this misclassification. In recent years, huge strides have been\nmade in creating models that are robust to this distinction. As a result, the\ncurrent state-of-the-art has reached near perfect performance on relatively\ncoarse-grained OOD detection tasks, such as distinguishing horses from trucks,\nwhile struggling with finer-grained classification, like differentiating models\nof commercial aircraft. In this paper, we describe a new theoretical framework\nfor understanding fine- and coarse-grained OOD detection, we re-conceptualize\nfine grained classification into a three part problem, and we propose a new\nbaseline task for OOD models on two fine-grained hierarchical data sets, two\nnew evaluation methods to differentiate fine- and coarse-grained OOD\nperformance, along with a new loss function for models in this task.\n", "rewritten_text": "Machine learning models deployed in real-world scenarios may encounter unfamiliar observations, leading to potential misclassification with high confidence. It is crucial for these models to distinguish between in-distribution (ID) and out-of-distribution (OOD) data to prevent such errors. Recent advancements have improved the robustness of models in this regard, achieving near-perfect performance in coarse-grained OOD detection tasks like distinguishing between horses and trucks. However, challenges persist in finer-grained classification tasks such as identifying different models of commercial aircraft. This paper introduces a new theoretical framework for understanding fine- and coarse-grained OOD detection, redefines fine-grained classification as a three-part problem, and proposes a new baseline task for OOD models using two fine-grained hierarchical datasets. Additionally, new evaluation methods are presented to assess fine- and coarse-grained OOD performance, along with a novel loss function tailored for models in this context."}, "2109.00698": {"original_text": "  While conventional wisdom suggests that more aggressively filtering data from\nlow-quality sources like Common Crawl always monotonically improves the quality\nof training data, we find that aggressive filtering can in fact lead to a\ndecrease in model quality on a wide array of downstream tasks for a GPT-like\nlanguage model. We speculate that this is because optimizing sufficiently\nstrongly for a proxy metric harms performance on the true objective, suggesting\na need for more robust filtering objectives when attempting to filter more\naggressively. We hope this work leads to detailed analysis of the effects of\ndataset filtering design choices on downstream model performance in future\nwork.\n", "rewritten_text": "The common belief that rigorously filtering data from low-quality sources such as Common Crawl consistently enhances the quality of training data is challenged in our study. We discovered that overly aggressive filtering can actually result in a decline in model performance across various tasks for a language model similar to GPT. We propose that this may occur because focusing too heavily on a proxy metric can hinder performance on the actual objective, indicating a necessity for more resilient filtering criteria when pursuing aggressive filtering. We anticipate that our findings will prompt further examination of the impact of dataset filtering strategies on the performance of subsequent models in future research."}, "1501.03952": {"original_text": "  Domain adaptation techniques aim at adapting a classifier learnt on a source\ndomain to work on the target domain. Exploiting the subspaces spanned by\nfeatures of the source and target domains respectively is one approach that has\nbeen investigated towards solving this problem. These techniques normally\nassume the existence of a single subspace for the entire source / target\ndomain. In this work, we consider the hierarchical organization of the data and\nconsider multiple subspaces for the source and target domain based on the\nhierarchy. We evaluate different subspace based domain adaptation techniques\nunder this setting and observe that using different subspaces based on the\nhierarchy yields consistent improvement over a non-hierarchical baseline\n", "rewritten_text": "Domain adaptation techniques focus on adjusting a classifier trained on one domain to function effectively on another domain. One approach involves leveraging the subspaces formed by the features of the source and target domains. Typically, these methods assume a single subspace for the entire source/target domain. However, in this study, we explore the hierarchical structure of the data and consider multiple subspaces for both the source and target domains based on this hierarchy. By evaluating various subspace-based domain adaptation techniques within this framework, we find that utilizing different subspaces according to the hierarchy consistently enhances performance compared to a non-hierarchical approach."}, "1707.00569": {"original_text": "  This survey presents a deep analysis of the learning and inference\ncapabilities in nine popular trackers. It is neither intended to study the\nwhole literature nor is it an attempt to review all kinds of neural networks\nproposed for visual tracking. We focus instead on Siamese neural networks which\nare a promising starting point for studying the challenging problem of\ntracking. These networks integrate efficiently feature learning and the\ntemporal matching and have so far shown state-of-the-art performance. In\nparticular, the branches of Siamese networks, their layers connecting these\nbranches, specific aspects of training and the embedding of these networks into\nthe tracker are highlighted. Quantitative results from existing papers are\ncompared with the conclusion that the current evaluation methodology shows\nproblems with the reproducibility and the comparability of results. The paper\nproposes a novel Lisp-like formalism for a better comparison of trackers. This\nassumes a certain functional design and functional decomposition of trackers.\nThe paper tries to give foundation for tracker design by a formulation of the\nproblem based on the theory of machine learning and by the interpretation of a\ntracker as a decision function. The work concludes with promising lines of\nresearch and suggests future work.\n", "rewritten_text": "This survey provides a detailed analysis of the learning and inference capabilities in nine popular trackers. It does not aim to cover all existing literature or review every type of neural network used for visual tracking. The focus is on Siamese neural networks, which offer a promising approach to tackling the challenging task of tracking. These networks effectively combine feature learning and temporal matching, demonstrating top-notch performance. The survey highlights various aspects of Siamese networks, including their branches, connecting layers, training specifics, and integration into tracking systems. It compares quantitative results from previous studies, noting issues with reproducibility and result comparability in current evaluation methods. The paper introduces a new Lisp-like formalism to enhance tracker comparisons, emphasizing a functional design and decomposition approach. It aims to lay a foundation for tracker design by framing the problem within machine learning theory and viewing a tracker as a decision function. The survey concludes by outlining potential research directions and suggesting future work."}, "2202.04897": {"original_text": "  Knowledge graph embedding (KGE) models learn the representation of entities\nand relations in knowledge graphs. Distance-based methods show promising\nperformance on link prediction task, which predicts the result by the distance\nbetween two entity representations. However, most of these methods represent\nthe head entity and tail entity separately, which limits the model capacity. We\npropose two novel distance-based methods named InterHT and InterHT+ that allow\nthe head and tail entities to interact better and get better entity\nrepresentation. Experimental results show that our proposed method achieves the\nbest results on ogbl-wikikg2 dataset.\n", "rewritten_text": "Knowledge graph embedding (KGE) models aim to understand the relationships between entities in knowledge graphs by learning their representations. Distance-based approaches have shown promise in predicting links by measuring the distance between entity representations. However, many existing methods treat head and tail entities separately, which can restrict the model's capabilities. To address this limitation, we introduce two new distance-based methods called InterHT and InterHT+, which enable improved interaction between head and tail entities for enhanced entity representation. Our experimental findings demonstrate that our proposed approach outperforms others on the ogbl-wikikg2 dataset."}, "1907.0203": {"original_text": "  Factchecking has always been a part of the journalistic process. However with\nnewsroom budgets shrinking it is coming under increasing pressure just as the\namount of false information circulating is on the rise. We therefore propose a\nmethod to increase the efficiency of the factchecking process, using the latest\ndevelopments in Natural Language Processing (NLP). This method allows us to\ncompare incoming claims to an existing corpus and return similar, factchecked,\nclaims in a live system-allowing factcheckers to work simultaneously without\nduplicating their work.\n", "rewritten_text": "\"Fact-checking has long been a crucial aspect of journalism. However, as newsroom budgets dwindle and the prevalence of false information grows, the practice is facing greater challenges. To address this, we suggest a method to enhance the effectiveness of fact-checking by leveraging advancements in Natural Language Processing (NLP). This approach enables us to compare new claims with an established database and provide verified claims in real-time, allowing fact-checkers to collaborate efficiently without redundancy.\""}, "2311.06377": {"original_text": "  Heaps' law is an empirical relation in text analysis that predicts vocabulary\ngrowth as a function of corpus size. While this law has been validated in\ndiverse human-authored text corpora, its applicability to large language model\ngenerated text remains unexplored. This study addresses this gap, focusing on\nthe emulation of corpora using the suite of GPT-Neo large language models. To\nconduct our investigation, we emulated corpora of PubMed abstracts using three\ndifferent parameter sizes of the GPT-Neo model. Our emulation strategy involved\nusing the initial five words of each PubMed abstract as a prompt and\ninstructing the model to expand the content up to the original abstract's\nlength. Our findings indicate that the generated corpora adhere to Heaps' law.\nInterestingly, as the GPT-Neo model size grows, its generated vocabulary\nincreasingly adheres to Heaps' law as as observed in human-authored text. To\nfurther improve the richness and authenticity of GPT-Neo outputs, future\niterations could emphasize enhancing model size or refining the model\narchitecture to curtail vocabulary repetition.\n", "rewritten_text": "\"Heaps' law is a relationship observed in text analysis that predicts vocabulary growth based on the size of the text collection. While this law has been confirmed in various human-written text collections, its relevance to text generated by large language models has not been thoroughly investigated. This study aims to fill this gap by focusing on simulating text collections using the GPT-Neo large language models. In our research, we simulated collections of PubMed abstracts using three different sizes of the GPT-Neo model. Our simulation approach involved using the first five words of each PubMed abstract as a prompt and instructing the model to expand the content to match the original abstract's length. Our results show that the simulated collections follow Heaps' law. Interestingly, as the size of the GPT-Neo model increases, the generated vocabulary increasingly aligns with Heaps' law, similar to what is observed in human-written text. To enhance the quality and authenticity of GPT-Neo outputs in the future, efforts could focus on increasing model size or refining the model architecture to reduce vocabulary repetition.\""}, "1809.07099": {"original_text": "  Deep neural networks have exhibited promising performance in image\nsuper-resolution (SR) due to the power in learning the non-linear mapping from\nlow-resolution (LR) images to high-resolution (HR) images. However, most deep\nlearning methods employ feed-forward architectures, and thus the dependencies\nbetween LR and HR images are not fully exploited, leading to limited learning\nperformance. Moreover, most deep learning based SR methods apply the pixel-wise\nreconstruction error as the loss, which, however, may fail to capture\nhigh-frequency information and produce perceptually unsatisfying results,\nwhilst the recent perceptual loss relies on some pre-trained deep model and\nthey may not generalize well. In this paper, we introduce a mask to separate\nthe image into low- and high-frequency parts based on image gradient magnitude,\nand then devise a gradient sensitive loss to well capture the structures in the\nimage without sacrificing the recovery of low-frequency content. Moreover, by\ninvestigating the duality in SR, we develop a dual reconstruction network (DRN)\nto improve the SR performance. We provide theoretical analysis on the\ngeneralization performance of our method and demonstrate its effectiveness and\nsuperiority with thorough experiments.\n", "rewritten_text": "\"Deep neural networks have shown promise in enhancing image resolution through their ability to learn complex mappings from low-resolution to high-resolution images. However, many existing deep learning approaches utilize feed-forward architectures, which do not fully leverage the relationships between low-resolution and high-resolution images, resulting in limited performance. Additionally, traditional deep learning methods for image super-resolution often rely on pixel-wise reconstruction error as the loss function, which may overlook high-frequency details and lead to visually unsatisfactory outcomes. Recent perceptual loss methods, which utilize pre-trained deep models, may also lack generalizability. \n\nIn this study, we propose a novel approach that involves segmenting images into low- and high-frequency components based on gradient magnitude, and introduce a gradient-sensitive loss function to better capture image structures while preserving low-frequency content. Furthermore, by exploring the duality in super-resolution, we introduce a dual reconstruction network (DRN) to enhance performance. The effectiveness and superiority of our method are demonstrated through theoretical analysis and comprehensive experiments.\""}, "2310.15052": {"original_text": "  Dataset distillation plays a crucial role in creating compact datasets with\nsimilar training performance compared with original large-scale ones. This is\nessential for addressing the challenges of data storage and training costs.\nPrevalent methods facilitate knowledge transfer by matching the gradients,\nembedding distributions, or training trajectories of synthetic images with\nthose of the sampled original images. Although there are various matching\nobjectives, currently the strategy for selecting original images is limited to\nnaive random sampling. We argue that random sampling overlooks the evenness of\nthe selected sample distribution, which may result in noisy or biased matching\ntargets. Besides, the sample diversity is also not constrained by random\nsampling. Additionally, current methods predominantly focus on\nsingle-dimensional matching, where information is not fully utilized. To\naddress these challenges, we propose a novel matching strategy called Dataset\nDistillation by Bidirectional REpresentAtive Matching (DREAM+), which selects\nrepresentative original images for bidirectional matching. DREAM+ is applicable\nto a variety of mainstream dataset distillation frameworks and significantly\nreduces the number of distillation iterations by more than 15 times without\naffecting performance. Given sufficient training time, DREAM+ can further\nimprove the performance and achieve state-of-the-art results. We have released\nthe code at github.com/NUS-HPC-AI-Lab/DREAM+.\n", "rewritten_text": "Dataset distillation is crucial for creating smaller datasets that perform similarly to larger ones, addressing challenges like data storage and training costs. Current methods focus on matching gradients, distributions, or trajectories of synthetic and original images, but random sampling may lead to uneven or biased selections. To overcome this, we propose DREAM+, a bidirectional matching strategy that selects representative original images. DREAM+ reduces distillation iterations significantly without sacrificing performance and can enhance results with sufficient training time. The code is available at github.com/NUS-HPC-AI-Lab/DREAM+."}, "2207.09625": {"original_text": "  Given an image and a reference caption, the image caption editing task aims\nto correct the misalignment errors and generate a refined caption. However, all\nexisting caption editing works are implicit models, ie, they directly produce\nthe refined captions without explicit connections to the reference captions. In\nthis paper, we introduce a new task: Explicit Caption Editing (ECE). ECE models\nexplicitly generate a sequence of edit operations, and this edit operation\nsequence can translate the reference caption into a refined one. Compared to\nthe implicit editing, ECE has multiple advantages: 1) Explainable: it can trace\nthe whole editing path. 2) Editing Efficient: it only needs to modify a few\nwords. 3) Human-like: it resembles the way that humans perform caption editing,\nand tries to keep original sentence structures. To solve this new task, we\npropose the first ECE model: TIger. TIger is a non-autoregressive\ntransformer-based model, consisting of three modules: Tagger_del, Tagger_add,\nand Inserter. Specifically, Tagger_del decides whether each word should be\npreserved or not, Tagger_add decides where to add new words, and Inserter\npredicts the specific word for adding. To further facilitate ECE research, we\npropose two new ECE benchmarks by re-organizing two existing datasets, dubbed\nCOCO-EE and Flickr30K-EE, respectively. Extensive ablations on both two\nbenchmarks have demonstrated the effectiveness of TIger.\n", "rewritten_text": "The task of image caption editing involves refining a caption by correcting errors in alignment between the image and the reference caption. Existing caption editing methods typically generate refined captions without explicit connections to the reference captions. This paper introduces a new task called Explicit Caption Editing (ECE), where models explicitly generate a sequence of edit operations to translate the reference caption into a refined one. ECE offers several advantages over implicit editing, including being explainable, editing efficiently by modifying only a few words, and resembling human-like caption editing behavior while preserving original sentence structures. To address this new task, the first ECE model, TIger, is proposed as a non-autoregressive transformer-based model with three modules: Tagger_del, Tagger_add, and Inserter. Tagger_del determines whether each word should be preserved, Tagger_add identifies where new words should be added, and Inserter predicts the specific word to add. Additionally, two new ECE benchmarks, COCO-EE and Flickr30K-EE, are introduced by re-organizing existing datasets to facilitate further research in ECE. Extensive experiments on both benchmarks demonstrate the effectiveness of TIger."}, "1805.05503": {"original_text": "  Human faces are one interesting object class with numerous applications.\nWhile significant progress has been made in the generic deblurring problem,\nexisting methods are less effective for blurry face images. The success of the\nstate-of-the-art image deblurring algorithms stems mainly from implicit or\nexplicit restoration of salient edges for kernel estimation. However, existing\nmethods are less effective as only few edges can be restored from blurry face\nimages for kernel estimation. In this paper, we address the problem of\ndeblurring face images by exploiting facial structures. We propose a deblurring\nalgorithm based on an exemplar dataset without using coarse-to-fine strategies\nor heuristic edge selections. In addition, we develop a convolutional neural\nnetwork to restore sharp edges from blurry images for deblurring. Extensive\nexperiments against the state-of-the-art methods demonstrate the effectiveness\nof the proposed algorithms for deblurring face images. In addition, we show the\nproposed algorithms can be applied to image deblurring for other object\nclasses.\n", "rewritten_text": "\"Human faces are a fascinating object category with a wide range of applications. While there has been significant progress in addressing the general deblurring issue, current methods are not as effective when it comes to blurry face images. The success of cutting-edge image deblurring algorithms largely relies on restoring prominent edges for kernel estimation, but this approach falls short for blurry face images due to the limited restoration of edges. In this study, we tackle the challenge of deblurring face images by leveraging facial structures. We introduce a deblurring technique based on a specific dataset that does not rely on coarse-to-fine strategies or arbitrary edge selections. Furthermore, we introduce a convolutional neural network to enhance sharp edges in blurry images for deblurring purposes. Through extensive experiments comparing our methods to existing ones, we demonstrate the efficacy of our proposed algorithms in deblurring face images. Moreover, we illustrate that our techniques can also be applied to deblurring images of other object categories.\""}, "2205.09299": {"original_text": "  Convolutional Neural Networks (CNNs) have achieved promising results in\nmedical image segmentation. However, CNNs require lots of training data and are\nincapable of handling pose and deformation of objects. Furthermore, their\npooling layers tend to discard important information such as positions as well\nas CNNs are sensitive to rotation and affine transformation. Capsule network is\na recent new architecture that has achieved better robustness in part-whole\nrepresentation learning by replacing pooling layers with dynamic routing and\nconvolutional strides, which has shown potential results on popular tasks such\nas digit classification and object segmentation. In this paper, we propose a 3D\nencoder-decoder network with Convolutional Capsule Encoder (called 3DConvCaps)\nto learn lower-level features (short-range attention) with convolutional layers\nwhile modeling the higher-level features (long-range dependence) with capsule\nlayers. Our experiments on multiple datasets including iSeg-2017, Hippocampus,\nand Cardiac demonstrate that our 3D 3DConvCaps network considerably outperforms\nprevious capsule networks and 3D-UNets. We further conduct ablation studies of\nnetwork efficiency and segmentation performance under various configurations of\nconvolution layers and capsule layers at both contracting and expanding paths.\n", "rewritten_text": "Convolutional Neural Networks (CNNs) have shown promise in medical image segmentation, but they require a large amount of training data and struggle with object pose and deformation. Additionally, their pooling layers may discard important information like object positions, and they are sensitive to rotation and affine transformations. Capsule networks offer a new architecture that improves robustness in part-whole representation learning by using dynamic routing and convolutional strides instead of pooling layers. This approach has demonstrated potential in tasks such as digit classification and object segmentation. In this study, we introduce a 3D encoder-decoder network called 3DConvCaps, which combines Convolutional Capsule Encoder to learn short-range attention features with convolutional layers and model long-range dependence features with capsule layers. Our experiments on various datasets, including iSeg-2017, Hippocampus, and Cardiac, show that the 3DConvCaps network outperforms previous capsule networks and 3D-UNets. We also analyze the network's efficiency and segmentation performance by testing different configurations of convolution and capsule layers in both contracting and expanding paths."}, "2407.08290": {"original_text": "  Recent advances in mobile mapping systems have greatly enhanced the\nefficiency and convenience of acquiring urban 3D data. These systems utilize\nLiDAR sensors mounted on vehicles to capture vast cityscapes. However, a\nsignificant challenge arises due to occlusions caused by roadside parked\nvehicles, leading to the loss of scene information, particularly on the roads,\nsidewalks, curbs, and the lower sections of buildings. In this study, we\npresent a novel approach that leverages deep neural networks to learn a model\ncapable of filling gaps in urban scenes that are obscured by vehicle occlusion.\nWe have developed an innovative technique where we place virtual vehicle models\nalong road boundaries in the gap-free scene and utilize a ray-casting algorithm\nto create a new scene with occluded gaps. This allows us to generate diverse\nand realistic urban point cloud scenes with and without vehicle occlusion,\nsurpassing the limitations of real-world training data collection and\nannotation. Furthermore, we introduce the Scene Gap Completion Network\n(SGC-Net), an end-to-end model that can generate well-defined shape boundaries\nand smooth surfaces within occluded gaps. The experiment results reveal that\n97.66% of the filled points fall within a range of 5 centimeters relative to\nthe high-density ground truth point cloud scene. These findings underscore the\nefficacy of our proposed model in gap completion and reconstructing urban\nscenes affected by vehicle occlusions.\n", "rewritten_text": "Recent advancements in mobile mapping technology have significantly improved the efficiency and convenience of obtaining 3D urban data. These systems use LiDAR sensors mounted on vehicles to capture extensive city landscapes. However, a notable challenge arises from obstructions caused by parked vehicles along roadsides, resulting in the loss of scene details such as roads, sidewalks, curbs, and lower building sections. In this research, we introduce an innovative method that employs deep neural networks to create a model capable of filling in gaps in urban scenes obscured by vehicle obstructions. Our approach involves placing virtual vehicle models along road boundaries in the gap-free scene and utilizing a ray-casting algorithm to generate a new scene with previously obscured areas filled in. This technique enables the generation of diverse and realistic urban point cloud scenes with and without vehicle obstructions, surpassing the constraints of real-world data collection and annotation. Additionally, we present the Scene Gap Completion Network (SGC-Net), an end-to-end model that can produce well-defined shape boundaries and smooth surfaces within obstructed areas. Experimental results demonstrate that 97.66% of the filled points align within a 5-centimeter range compared to the high-density ground truth point cloud scene. These outcomes highlight the effectiveness of our proposed model in completing gaps and reconstructing urban scenes impacted by vehicle obstructions."}, "2004.04699": {"original_text": "  Deep Neural Networks trained in a fully supervised fashion are the dominant\ntechnology in perception-based autonomous driving systems. While collecting\nlarge amounts of unlabeled data is already a major undertaking, only a subset\nof it can be labeled by humans due to the effort needed for high-quality\nannotation. Therefore, finding the right data to label has become a key\nchallenge. Active learning is a powerful technique to improve data efficiency\nfor supervised learning methods, as it aims at selecting the smallest possible\ntraining set to reach a required performance. We have built a scalable\nproduction system for active learning in the domain of autonomous driving. In\nthis paper, we describe the resulting high-level design, sketch some of the\nchallenges and their solutions, present our current results at scale, and\nbriefly describe the open problems and future directions.\n", "rewritten_text": "\"Deep Neural Networks trained through full supervision are the primary technology used in perception-driven autonomous driving systems. While gathering large amounts of unlabeled data is a significant task, only a portion of it can be manually labeled due to the labor-intensive nature of high-quality annotation. Consequently, identifying the most suitable data for labeling has become a critical issue. Active learning is a potent method for enhancing data efficiency in supervised learning by selecting the smallest training set necessary to achieve desired performance levels. We have developed a scalable production system for active learning in the field of autonomous driving. This paper outlines the resulting high-level design, discusses some challenges and their solutions, presents our current large-scale results, and briefly touches on open issues and future directions.\""}, "1808.01491": {"original_text": "  Single image rain streaks removal has recently witnessed substantial progress\ndue to the development of deep convolutional neural networks. However, existing\ndeep learning based methods either focus on the entrance and exit of the\nnetwork by decomposing the input image into high and low frequency information\nand employing residual learning to reduce the mapping range, or focus on the\nintroduction of cascaded learning scheme to decompose the task of rain streaks\nremoval into multi-stages. These methods treat the convolutional neural network\nas an encapsulated end-to-end mapping module without deepening into the\nrationality and superiority of neural network design. In this paper, we delve\ninto an effective end-to-end neural network structure for stronger feature\nexpression and spatial correlation learning. Specifically, we propose a\nnon-locally enhanced encoder-decoder network framework, which consists of a\npooling indices embedded encoder-decoder network to efficiently learn\nincreasingly abstract feature representation for more accurate rain streaks\nmodeling while perfectly preserving the image detail. The proposed\nencoder-decoder framework is composed of a series of non-locally enhanced dense\nblocks that are designed to not only fully exploit hierarchical features from\nall the convolutional layers but also well capture the long-distance\ndependencies and structural information. Extensive experiments on synthetic and\nreal datasets demonstrate that the proposed method can effectively remove\nrain-streaks on rainy image of various densities while well preserving the\nimage details, which achieves significant improvements over the recent\nstate-of-the-art methods.\n", "rewritten_text": "Recent advancements in the field of single-image rain streak removal have been driven by the emergence of deep convolutional neural networks. Existing deep learning approaches have primarily focused on either decomposing input images into high and low frequency components and utilizing residual learning to narrow the mapping range, or implementing a cascaded learning scheme to break down rain streak removal into multiple stages. However, these methods have treated the convolutional neural network as a black box without delving into the rationale behind its design. This study proposes an end-to-end neural network structure that enhances feature expression and spatial correlation learning. The proposed framework, a non-locally enhanced encoder-decoder network, incorporates pooling indices to efficiently learn abstract feature representations for accurate rain streak modeling while preserving image details. The encoder-decoder architecture includes non-locally enhanced dense blocks to leverage hierarchical features from all convolutional layers and capture long-distance dependencies and structural information. Experimental results on synthetic and real datasets demonstrate the effectiveness of the proposed method in removing rain streaks from images of varying densities while maintaining image details, outperforming recent state-of-the-art techniques."}, "2303.15698": {"original_text": "  Standard deep learning models such as convolutional neural networks (CNNs)\nlack the ability of generalizing to domains which have not been seen during\ntraining. This problem is mainly due to the common but often wrong assumption\nof such models that the source and target data come from the same i.i.d.\ndistribution. Recently, Vision Transformers (ViTs) have shown outstanding\nperformance for a broad range of computer vision tasks. However, very few\nstudies have investigated their ability to generalize to new domains. This\npaper presents a first Token-level Feature Stylization (TFS-ViT) approach for\ndomain generalization, which improves the performance of ViTs to unseen data by\nsynthesizing new domains. Our approach transforms token features by mixing the\nnormalization statistics of images from different domains. We further improve\nthis approach with a novel strategy for attention-aware stylization, which uses\nthe attention maps of class (CLS) tokens to compute and mix normalization\nstatistics of tokens corresponding to different image regions. The proposed\nmethod is flexible to the choice of backbone model and can be easily applied to\nany ViT-based architecture with a negligible increase in computational\ncomplexity. Comprehensive experiments show that our approach is able to achieve\nstate-of-the-art performance on five challenging benchmarks for domain\ngeneralization, and demonstrate its ability to deal with different types of\ndomain shifts. The implementation is available at:\nhttps://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.\n", "rewritten_text": "Traditional deep learning models like convolutional neural networks (CNNs) struggle to generalize to unseen domains due to the assumption that source and target data come from the same distribution. In contrast, Vision Transformers (ViTs) have excelled in various computer vision tasks, but their ability to generalize to new domains remains underexplored. This paper introduces Token-level Feature Stylization (TFS-ViT), a novel approach that enhances ViTs' performance on unseen data by synthesizing new domains. TFS-ViT transforms token features by blending normalization statistics from different domain images. Additionally, it incorporates attention-aware stylization, leveraging class token attention maps to mix normalization statistics of tokens corresponding to various image regions. The method is adaptable to different backbone models and ViT-based architectures with minimal computational overhead. Extensive experiments demonstrate that TFS-ViT achieves state-of-the-art results on five challenging domain generalization benchmarks, showcasing its effectiveness in handling diverse domain shifts. The implementation can be accessed at: https://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization."}, "1804.09555": {"original_text": "  Outdoor vision-based systems suffer from atmospheric turbulences, and rain is\none of the worst factors for vision degradation. Current rain removal methods\nshow limitations either for complex dynamic scenes, or under torrential rain\nwith opaque occlusions. We propose a novel derain framework which applies\nsuperpixel (SP) segmentation to decompose the scene into depth consistent\nunits. Alignment of scene contents are done at the SP level, which proves to be\nrobust against rain occlusion interferences and fast camera motion. Two\nalignment output tensors, i.e., optimal temporal match tensor and sorted\nspatial-temporal match tensor, provide informative clues for the location of\nrain streaks and the occluded background contents. Different classical and\nnovel methods such as Robust Principle Component Analysis and Convolutional\nNeural Networks are applied and compared for their respective advantages in\nefficiently exploiting the rich spatial-temporal features provided by the two\ntensors. Extensive evaluations show that advantage of up to 5dB is achieved on\nthe scene restoration PSNR over state-of-the-art methods, and the advantage is\nespecially obvious with highly complex and dynamic scenes. Visual evaluations\nshow that the proposed framework is not only able to suppress heavy and opaque\noccluding rain streaks, but also large semi-transparent regional fluctuations\nand distortions.\n", "rewritten_text": "Outdoor vision-based systems are affected by atmospheric turbulences, with rain being a significant factor in degrading vision quality. Existing methods for removing rain have limitations in handling complex dynamic scenes or heavy rain with opaque obstructions. To address this, we introduce a new derain framework that utilizes superpixel segmentation to break down the scene into depth-consistent units. By aligning scene elements at the superpixel level, our approach proves resilient against rain obstructions and rapid camera movements. Two output tensors, the optimal temporal match tensor and the sorted spatial-temporal match tensor, offer valuable insights into rain streak locations and obscured background details. Various traditional and innovative techniques, such as Robust Principle Component Analysis and Convolutional Neural Networks, are employed and compared for their effectiveness in leveraging the spatial-temporal features provided by the tensors. Extensive evaluations demonstrate a restoration PSNR advantage of up to 5dB over current methods, particularly benefiting highly complex and dynamic scenes. Visual assessments confirm the framework's ability to effectively suppress heavy, opaque rain streaks as well as large semi-transparent regional distortions."}, "2005.00975": {"original_text": "  In the deep learning (DL) era, parsing models are extremely simplified with\nlittle hurt on performance, thanks to the remarkable capability of multi-layer\nBiLSTMs in context representation. As the most popular graph-based dependency\nparser due to its high efficiency and performance, the biaffine parser directly\nscores single dependencies under the arc-factorization assumption, and adopts a\nvery simple local token-wise cross-entropy training loss. This paper for the\nfirst time presents a second-order TreeCRF extension to the biaffine parser.\nFor a long time, the complexity and inefficiency of the inside-outside\nalgorithm hinder the popularity of TreeCRF. To address this issue, we propose\nan effective way to batchify the inside and Viterbi algorithms for direct large\nmatrix operation on GPUs, and to avoid the complex outside algorithm via\nefficient back-propagation. Experiments and analysis on 27 datasets from 13\nlanguages clearly show that techniques developed before the DL era, such as\nstructural learning (global TreeCRF loss) and high-order modeling are still\nuseful, and can further boost parsing performance over the state-of-the-art\nbiaffine parser, especially for partially annotated training data. We release\nour code at https://github.com/yzhangcs/crfpar.\n", "rewritten_text": "In the era of deep learning (DL), parsing models have been greatly simplified without sacrificing performance, thanks to the powerful multi-layer BiLSTMs for context representation. The biaffine parser, known for its efficiency and performance as a popular graph-based dependency parser, directly evaluates individual dependencies based on the arc-factorization assumption and utilizes a straightforward local token-wise cross-entropy training loss. This study introduces a novel second-order TreeCRF extension to the biaffine parser. Traditionally, the complexity and inefficiency of the inside-outside algorithm have limited the adoption of TreeCRF. To overcome this challenge, we propose a method to batch process the inside and Viterbi algorithms for efficient large matrix operations on GPUs, eliminating the need for the intricate outside algorithm through streamlined back-propagation. Experiments conducted on 27 datasets across 13 languages demonstrate that pre-DL techniques like structural learning (global TreeCRF loss) and high-order modeling remain valuable and can significantly enhance parsing performance compared to the current state-of-the-art biaffine parser, particularly for partially annotated training data. The code for this research is available at https://github.com/yzhangcs/crfpar."}, "1812.00108": {"original_text": "  With vast amounts of video content being uploaded to the Internet every\nminute, video summarization becomes critical for efficient browsing, searching,\nand indexing of visual content. Nonetheless, the spread of social and\negocentric cameras creates an abundance of sparse scenarios captured by several\ndevices, and ultimately required to be jointly summarized. In this paper, we\ndiscuss the problem of summarizing videos recorded independently by several\ndynamic cameras that intermittently share the field of view. We present a\nrobust framework that (a) identifies a diverse set of important events among\nmoving cameras that often are not capturing the same scene, and (b) selects the\nmost representative view(s) at each event to be included in a universal\nsummary. Due to the lack of an applicable alternative, we collected a new\nmulti-view egocentric dataset, Multi-Ego. Our dataset is recorded\nsimultaneously by three cameras, covering a wide variety of real-life\nscenarios. The footage is annotated by multiple individuals under various\nsummarization configurations, with a consensus analysis ensuring a reliable\nground truth. We conduct extensive experiments on the compiled dataset in\naddition to three other standard benchmarks that show the robustness and the\nadvantage of our approach in both supervised and unsupervised settings.\nAdditionally, we show that our approach learns collectively from data of varied\nnumber-of-views and orthogonal to other summarization methods, deeming it\nscalable and generic.\n", "rewritten_text": "Given the continuous influx of video content on the Internet, the need for video summarization is crucial for efficient browsing, searching, and indexing of visual material. The proliferation of social and egocentric cameras has led to a surplus of diverse scenarios captured by multiple devices, necessitating a collaborative approach to summarization. This study addresses the challenge of summarizing videos recorded independently by dynamic cameras that occasionally overlap in their field of view. A robust framework is proposed to identify key events across different camera perspectives and select the most representative views for a comprehensive summary. To address the lack of suitable alternatives, a new multi-view egocentric dataset, Multi-Ego, was created, featuring footage from three cameras capturing various real-life scenarios. The dataset is annotated by multiple individuals under different summarization configurations, ensuring a reliable ground truth through consensus analysis. Extensive experiments conducted on this dataset and three standard benchmarks demonstrate the effectiveness and superiority of the proposed approach in both supervised and unsupervised settings. Furthermore, the study highlights the scalability and versatility of the approach, which can learn from diverse data sources and complement existing summarization methods."}, "2012.07791": {"original_text": "  We propose real-time, six degrees of freedom (6DoF), 3D face pose estimation\nwithout face detection or landmark localization. We observe that estimating the\n6DoF rigid transformation of a face is a simpler problem than facial landmark\ndetection, often used for 3D face alignment. In addition, 6DoF offers more\ninformation than face bounding box labels. We leverage these observations to\nmake multiple contributions: (a) We describe an easily trained, efficient,\nFaster R-CNN--based model which regresses 6DoF pose for all faces in the photo,\nwithout preliminary face detection. (b) We explain how pose is converted and\nkept consistent between the input photo and arbitrary crops created while\ntraining and evaluating our model. (c) Finally, we show how face poses can\nreplace detection bounding box training labels. Tests on AFLW2000-3D and BIWI\nshow that our method runs at real-time and outperforms state of the art (SotA)\nface pose estimators. Remarkably, our method also surpasses SotA models of\ncomparable complexity on the WIDER FACE detection benchmark, despite not been\noptimized on bounding box labels.\n", "rewritten_text": "We present a method for real-time estimation of 3D face pose in six degrees of freedom (6DoF) without the need for face detection or landmark localization. Our approach focuses on estimating the rigid transformation of a face, which is a simpler task compared to traditional facial landmark detection. By leveraging the additional information provided by 6DoF pose estimation, we introduce several key contributions: \n(a) We introduce an efficient model based on Faster R-CNN that can accurately regress 6DoF pose for all faces in an image without requiring prior face detection. \n(b) We describe how we maintain consistency in pose estimation between the original image and any cropped versions used during training and evaluation. \n(c) We demonstrate how face poses can be used as a replacement for traditional bounding box labels in training. \nOur method, tested on datasets like AFLW2000-3D and BIWI, achieves real-time performance and outperforms current state-of-the-art face pose estimators. Notably, our approach also surpasses models of similar complexity on the WIDER FACE detection benchmark, despite not being specifically optimized for bounding box labels."}, "2208.05909": {"original_text": "  Preservation of domain knowledge from the source to target is crucial in any\ntranslation workflow. It is common in the translation industry to receive\nhighly specialized projects, where there is hardly any parallel in-domain data.\nIn such scenarios where there is insufficient in-domain data to fine-tune\nMachine Translation (MT) models, producing translations that are consistent\nwith the relevant context is challenging. In this work, we propose a novel\napproach to domain adaptation leveraging state-of-the-art pretrained language\nmodels (LMs) for domain-specific data augmentation for MT, simulating the\ndomain characteristics of either (a) a small bilingual dataset, or (b) the\nmonolingual source text to be translated. Combining this idea with\nback-translation, we can generate huge amounts of synthetic bilingual in-domain\ndata for both use cases. For our investigation, we use the state-of-the-art\nTransformer architecture. We employ mixed fine-tuning to train models that\nsignificantly improve translation of in-domain texts. More specifically, in\nboth scenarios, our proposed methods achieve improvements of approximately 5-6\nBLEU and 2-3 BLEU, respectively, on the Arabic-to-English and English-to-Arabic\nlanguage pairs. Furthermore, the outcome of human evaluation corroborates the\nautomatic evaluation results.\n", "rewritten_text": "Preserving domain knowledge throughout the translation process is essential. In the translation industry, specialized projects often lack relevant in-domain data, making it difficult to produce translations that align with the context. To address this challenge, we introduce a new method for domain adaptation using advanced pretrained language models. This approach involves augmenting domain-specific data for machine translation by simulating domain characteristics from either a small bilingual dataset or the monolingual source text. By combining this technique with back-translation, we can generate large amounts of synthetic bilingual in-domain data. Our study focuses on the Transformer architecture and utilizes mixed fine-tuning to enhance translation quality for in-domain texts. Our methods demonstrate significant improvements in translation accuracy, achieving around 5-6 BLEU and 2-3 BLEU enhancements for Arabic-to-English and English-to-Arabic language pairs, respectively. Human evaluation results support the findings of automatic evaluations."}, "1811.09745": {"original_text": "  We propose a new geometric regularization principle for reconstructing vector\nfields based on prior knowledge about their divergence. As one important\nexample of this general idea, we focus on vector fields modelling blood flow\npattern that should be divergent in arteries and convergent in veins. We show\nthat this previously ignored regularization constraint can significantly\nimprove the quality of vessel tree reconstruction particularly around\nbifurcations where non-zero divergence is concentrated. Our divergence prior is\ncritical for resolving (binary) sign ambiguity in flow orientations produced by\nstandard vessel filters, e.g. Frangi. Our vessel tree centerline reconstruction\ncombines divergence constraints with robust curvature regularization. Our\nunsupervised method can reconstruct complete vessel trees with near-capillary\ndetails on synthetic and real 3D volumes.\n", "rewritten_text": "We introduce a novel geometric regularization principle for reconstructing vector fields by leveraging prior knowledge of their divergence. A key application of this concept involves modeling blood flow patterns, where arteries exhibit divergence and veins exhibit convergence. By incorporating this overlooked regularization constraint, we demonstrate a significant enhancement in vessel tree reconstruction quality, particularly at bifurcations where non-zero divergence is prominent. Our divergence prior plays a crucial role in resolving sign ambiguity in flow orientations generated by standard vessel filters like Frangi. The reconstruction of vessel tree centerlines integrates divergence constraints with robust curvature regularization. Our unsupervised approach enables the reconstruction of complete vessel trees with intricate details down to capillary level in both synthetic and real 3D volumes."}, "2104.08575": {"original_text": "  Super-resolution (SR) is an ill-posed problem, which means that infinitely\nmany high-resolution (HR) images can be degraded to the same low-resolution\n(LR) image. To study the one-to-many stochastic SR mapping, we implicitly\nrepresent the non-local self-similarity of natural images and develop a\nVariational Sparse framework for Super-Resolution (VSpSR) via neural networks.\nSince every small patch of a HR image can be well approximated by the sparse\nrepresentation of atoms in an over-complete dictionary, we design a two-branch\nmodule, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM\nextracts patch-level basis from the LR input, and the other branch infers\npixel-wise variational distributions with respect to the sparse coefficients.\nBy repeatedly sampling coefficients, we could obtain infinite sparse\nrepresentations, and thus generate diverse HR images. According to the\npreliminary results of NTIRE 2021 challenge on learning SR space, our team\n(FudanZmic21) ranks 7-th in terms of released scores. The implementation of\nVSpSR is released at https://zmiclab.github.io/.\n", "rewritten_text": "Super-resolution (SR) presents a challenging problem due to its ill-posed nature, where numerous high-resolution (HR) images can be transformed into the same low-resolution (LR) image. To address this issue, we investigate the stochastic SR mapping, leveraging the inherent non-local self-similarity found in natural images. Through the development of a Variational Sparse framework for Super-Resolution (VSpSR) using neural networks, we aim to capture the diverse possibilities in the SR process. Our approach involves a two-branch module, VSpM, which extracts patch-level basis from the LR input and infers pixel-wise variational distributions based on sparse coefficients. By sampling coefficients iteratively, we can generate a wide range of HR images. In the recent NTIRE 2021 challenge on learning SR space, our team (FudanZmic21) achieved the 7th position in terms of released scores. The implementation of VSpSR can be accessed at https://zmiclab.github.io/."}, "2206.12505": {"original_text": "  We propose a novel semi-supervised learning approach for classification of\nhistopathology images. We employ strong supervision with patch-level\nannotations combined with a novel co-training loss to create a semi-supervised\nlearning framework. Co-training relies on multiple conditionally independent\nand sufficient views of the data. We separate the hematoxylin and eosin\nchannels in pathology images using color deconvolution to create two views of\neach slide that can partially fulfill these requirements. Two separate CNNs are\nused to embed the two views into a joint feature space. We use a contrastive\nloss between the views in this feature space to implement co-training. We\nevaluate our approach in clear cell renal cell and prostate carcinomas, and\ndemonstrate improvement over state-of-the-art semi-supervised learning methods.\n", "rewritten_text": "We introduce a new method for classifying histopathology images using a semi-supervised learning approach. Our method combines strong supervision with patch-level annotations and a unique co-training loss to establish a semi-supervised learning framework. Co-training leverages multiple independent views of the data, achieved by separating the hematoxylin and eosin channels in pathology images through color deconvolution. Two separate convolutional neural networks (CNNs) are employed to map these views into a shared feature space, with a contrastive loss used to facilitate co-training. We assess the effectiveness of our approach on clear cell renal cell and prostate carcinomas, showing enhancements compared to existing semi-supervised learning techniques."}, "2303.08714": {"original_text": "  Adapting the Diffusion Probabilistic Model (DPM) for direct image\nsuper-resolution is wasteful, given that a simple Convolutional Neural Network\n(CNN) can recover the main low-frequency content. Therefore, we present\nResDiff, a novel Diffusion Probabilistic Model based on Residual structure for\nSingle Image Super-Resolution (SISR). ResDiff utilizes a combination of a CNN,\nwhich restores primary low-frequency components, and a DPM, which predicts the\nresidual between the ground-truth image and the CNN predicted image. In\ncontrast to the common diffusion-based methods that directly use LR images to\nguide the noise towards HR space, ResDiff utilizes the CNN's initial prediction\nto direct the noise towards the residual space between HR space and\nCNN-predicted space, which not only accelerates the generation process but also\nacquires superior sample quality. Additionally, a frequency-domain-based loss\nfunction for CNN is introduced to facilitate its restoration, and a\nfrequency-domain guided diffusion is designed for DPM on behalf of predicting\nhigh-frequency details. The extensive experiments on multiple benchmark\ndatasets demonstrate that ResDiff outperforms previous diffusion based methods\nin terms of shorter model convergence time, superior generation quality, and\nmore diverse samples.\n", "rewritten_text": "Adapting the Diffusion Probabilistic Model (DPM) for direct image super-resolution may not be the most efficient approach, as a simple Convolutional Neural Network (CNN) can already recover the main low-frequency content. Therefore, we introduce ResDiff, a new model for Single Image Super-Resolution (SISR) that combines a CNN to restore primary low-frequency components and a DPM to predict the residual between the ground-truth image and the CNN's prediction. Unlike traditional diffusion-based methods that use low-resolution (LR) images to guide noise towards high-resolution (HR) space, ResDiff uses the CNN's initial prediction to direct noise towards the residual space between HR and CNN-predicted space. This not only speeds up the generation process but also improves sample quality. We also introduce a frequency-domain-based loss function for the CNN to aid in restoration and a frequency-domain guided diffusion for the DPM to predict high-frequency details. Extensive experiments on various benchmark datasets show that ResDiff surpasses previous diffusion-based methods in terms of faster model convergence, better generation quality, and more diverse samples."}, "1607.06797": {"original_text": "  In this paper we proposed an ordered patch based method using Conditional\nRandom Field (CRF) in order to encode local properties and their spatial\nrelationship in images to address texture classification, face recognition, and\nscene classification problems. Typical image classification approaches work\nwithout considering spatial causality among distinctive properties of an image\nfor image representation in feature space. In this method first, each image is\nencoded as a sequence of ordered patches, including local properties. Second,\nthe sequence of these ordered patches is modeled as a probabilistic feature\nvector by CRF to model spatial relationship of these local properties. And\nfinally, image classification is performed on such probabilistic image\nrepresentation. Experimental results on several standard image datasets\nindicate that proposed method outperforms some of existing image classification\nmethods.\n", "rewritten_text": "\"In this paper, we introduce a method that utilizes Conditional Random Field (CRF) to encode local properties and their spatial relationships in images for texture classification, face recognition, and scene classification. Traditional image classification techniques do not take into account the spatial causality among different properties of an image when representing it in feature space. Our method involves encoding each image as a sequence of ordered patches containing local properties, followed by modeling this sequence as a probabilistic feature vector using CRF to capture the spatial relationships among these properties. Finally, image classification is carried out based on this probabilistic image representation. Experimental results on various standard image datasets demonstrate that our proposed method outperforms some existing image classification approaches.\""}, "2305.15929": {"original_text": "  Current large language models, such as OpenAI's ChatGPT, have captured the\npublic's attention because how remarkable they are in the use of language.\nHere, I demonstrate that ChatGPT displays phonological biases that are a\nhallmark of human language processing. More concretely, just like humans,\nChatGPT has a consonant bias. That is, the chatbot has a tendency to use\nconsonants over vowels to identify words. This is observed across languages\nthat differ in their relative distribution of consonants and vowels such as\nEnglish and Spanish. Despite the differences in how current artificial\nintelligence language models are trained to process linguistic stimuli and how\nhuman infants acquire language, such training seems to be enough for the\nemergence of a phonological bias in ChatGPT\n", "rewritten_text": "The current large language models, like OpenAI's ChatGPT, have garnered public attention due to their impressive language capabilities. In this study, I show that ChatGPT exhibits phonological biases similar to those found in human language processing. Specifically, like humans, ChatGPT shows a preference for using consonants over vowels in word identification. This tendency is consistent across languages with varying distributions of consonants and vowels, such as English and Spanish. Despite differences in training methods between artificial intelligence language models and human language acquisition, it appears that the training is sufficient for ChatGPT to develop a phonological bias."}, "2008.13196": {"original_text": "  The task of spatial-temporal action detection has attracted increasing\nattention among researchers. Existing dominant methods solve this problem by\nrelying on short-term information and dense serial-wise detection on each\nindividual frames or clips. Despite their effectiveness, these methods showed\ninadequate use of long-term information and are prone to inefficiency. In this\npaper, we propose for the first time, an efficient framework that generates\naction tube proposals from video streams with a single forward pass in a\nsparse-to-dense manner. There are two key characteristics in this framework:\n(1) Both long-term and short-term sampled information are explicitly utilized\nin our spatiotemporal network, (2) A new dynamic feature sampling module (DTS)\nis designed to effectively approximate the tube output while keeping the system\ntractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 and\nUCFSports benchmark datasets, achieving promising results that are competitive\nto state-of-the-art methods. The proposed sparse-to-dense strategy rendered our\nframework about 7.6 times more efficient than the nearest competitor.\n", "rewritten_text": "The field of spatial-temporal action detection has garnered increasing interest from researchers. Current leading approaches address this challenge by focusing on short-term data and conducting dense detection on individual frames or clips. While effective, these methods often underutilize long-term information and can be inefficient. In this study, we introduce a novel framework that efficiently generates action tube proposals from video streams in a sparse-to-dense manner with a single forward pass. Our framework stands out for two main reasons: (1) it leverages both long-term and short-term sampled data in our spatiotemporal network, and (2) it incorporates a new dynamic feature sampling module (DTS) to approximate tube output effectively while maintaining system manageability. We assess the performance of our model on benchmark datasets like UCF101-24, JHMDB-21, and UCFSports, achieving competitive results compared to state-of-the-art methods. Our sparse-to-dense approach makes our framework approximately 7.6 times more efficient than the closest competitor."}, "2010.07574": {"original_text": "  Evaluation of grammatical error correction (GEC) systems has primarily\nfocused on essays written by non-native learners of English, which however is\nonly part of the full spectrum of GEC applications. We aim to broaden the\ntarget domain of GEC and release CWEB, a new benchmark for GEC consisting of\nwebsite text generated by English speakers of varying levels of proficiency.\nWebsite data is a common and important domain that contains far fewer\ngrammatical errors than learner essays, which we show presents a challenge to\nstate-of-the-art GEC systems. We demonstrate that a factor behind this is the\ninability of systems to rely on a strong internal language model in low error\ndensity domains. We hope this work shall facilitate the development of\nopen-domain GEC models that generalize to different topics and genres.\n", "rewritten_text": "The evaluation of grammatical error correction (GEC) systems has mainly focused on essays written by non-native English learners, which represents only a portion of the full range of GEC applications. Our goal is to expand the scope of GEC evaluation by introducing CWEB, a new benchmark that includes website text created by English speakers with varying levels of proficiency. Website content is a common and significant domain that typically contains fewer grammatical errors compared to learner essays, posing a challenge to current state-of-the-art GEC systems. We illustrate that one reason for this challenge is the difficulty for systems to rely on a robust internal language model in domains with low error density. We anticipate that this research will support the development of open-domain GEC models that can adapt to diverse topics and genres."}, "2306.0045": {"original_text": "  Semantic segmentation is a crucial task in computer vision that involves\nsegmenting images into semantically meaningful regions at the pixel level.\nHowever, existing approaches often rely on expensive human annotations as\nsupervision for model training, limiting their scalability to large, unlabeled\ndatasets. To address this challenge, we present ZeroSeg, a novel method that\nleverages the existing pretrained vision-language (VL) model (e.g. CLIP) to\ntrain open-vocabulary zero-shot semantic segmentation models. Although acquired\nextensive knowledge of visual concepts, it is non-trivial to exploit knowledge\nfrom these VL models to the task of semantic segmentation, as they are usually\ntrained at an image level. ZeroSeg overcomes this by distilling the visual\nconcepts learned by VL models into a set of segment tokens, each summarizing a\nlocalized region of the target image. We evaluate ZeroSeg on multiple popular\nsegmentation benchmarks, including PASCAL VOC 2012, PASCAL Context, and COCO,\nin a zero-shot manner (i.e., no training or adaption on target segmentation\ndatasets). Our approach achieves state-of-the-art performance when compared to\nother zero-shot segmentation methods under the same training data, while also\nperforming competitively compared to strongly supervised methods. Finally, we\nalso demonstrated the effectiveness of ZeroSeg on open-vocabulary segmentation,\nthrough both human studies and qualitative visualizations.\n", "rewritten_text": "Semantic segmentation is a critical task in computer vision that involves dividing images into meaningful regions at the pixel level. However, current methods often require costly human annotations for model training, which limits their scalability to large, unlabeled datasets. To tackle this issue, we introduce ZeroSeg, a new approach that utilizes pretrained vision-language models (such as CLIP) to train semantic segmentation models without the need for specific annotations. Despite the vast visual knowledge acquired by these models, leveraging this knowledge for semantic segmentation is challenging as they are typically trained at the image level. ZeroSeg addresses this by distilling visual concepts from vision-language models into segment tokens that summarize localized regions in the image. We evaluate ZeroSeg on various segmentation benchmarks, including PASCAL VOC 2012, PASCAL Context, and COCO, in a zero-shot manner (without training on target datasets). Our method achieves state-of-the-art performance compared to other zero-shot segmentation techniques, while also performing well against strongly supervised methods. Additionally, we demonstrate the effectiveness of ZeroSeg for open-vocabulary segmentation through human studies and visualizations."}, "2401.05166": {"original_text": "  In dyadic interactions, humans communicate their intentions and state of mind\nusing verbal and non-verbal cues, where multiple different facial reactions\nmight be appropriate in response to a specific speaker behaviour. Then, how to\ndevelop a machine learning (ML) model that can automatically generate multiple\nappropriate, diverse, realistic and synchronised human facial reactions from an\npreviously unseen speaker behaviour is a challenging task. Following the\nsuccessful organisation of the first REACT challenge (REACT 2023), this edition\nof the challenge (REACT 2024) employs a subset used by the previous challenge,\nwhich contains segmented 30-secs dyadic interaction clips originally recorded\nas part of the NOXI and RECOLA datasets, encouraging participants to develop\nand benchmark Machine Learning (ML) models that can generate multiple\nappropriate facial reactions (including facial image sequences and their\nattributes) given an input conversational partner's stimulus under various\ndyadic video conference scenarios. This paper presents: (i) the guidelines of\nthe REACT 2024 challenge; (ii) the dataset utilized in the challenge; and (iii)\nthe performance of the baseline systems on the two proposed sub-challenges:\nOffline Multiple Appropriate Facial Reaction Generation and Online Multiple\nAppropriate Facial Reaction Generation, respectively. The challenge baseline\ncode is publicly available at\nhttps://github.com/reactmultimodalchallenge/baseline_react2024.\n", "rewritten_text": "\"In human interactions, individuals convey their intentions and mental state through both spoken words and non-verbal cues. Different facial expressions may be suitable responses to specific behaviors exhibited by a speaker. Developing a machine learning model that can generate multiple diverse, realistic, and synchronized human facial reactions based on unseen speaker behavior poses a significant challenge. Building on the success of the previous REACT challenge (REACT 2023), the current edition (REACT 2024) focuses on a subset of dyadic interaction clips from the NOXI and RECOLA datasets. Participants are tasked with creating and evaluating machine learning models capable of producing various appropriate facial reactions in response to a conversational partner's stimuli in different video conference scenarios. This paper outlines the guidelines for the REACT 2024 challenge, describes the dataset used, and evaluates the performance of baseline systems in two sub-challenges: Offline Multiple Appropriate Facial Reaction Generation and Online Multiple Appropriate Facial Reaction Generation. The code for the challenge baseline is publicly accessible at https://github.com/reactmultimodalchallenge/baseline_react2024.\""}, "2406.10323": {"original_text": "  Most public instruction finetuning datasets are relatively small compared to\nthe closed source datasets used to train industry models. To study questions\nabout finetuning at scale, such as curricula and learning rate cooldown\nschedules, there is a need for industrial-scale datasets. However, this scale\nnecessitates a data generation process that is almost entirely automated. In\nthis work, we study methods for generating large instruction datasets from a\nsingle prompt. With little human oversight, we get LLMs to write diverse sets\nof instruction examples ranging from simple completion tasks to complex\nmulti-turn dialogs across a variety of subject areas. When finetuning a Llama-3\n8B base model, our dataset meets or exceeds both WizardLM and Ultrachat on both\nknowledge-intensive leaderboard tasks as well as conversational evaluations. We\nrelease our dataset, the \"generator\" prompts that created it, and our finetuned\nmodel checkpoints.\n", "rewritten_text": "The majority of public datasets for refining instruction are relatively small in comparison to the proprietary datasets used for training models in the industry. To explore questions related to large-scale instruction refinement, such as curriculum design and learning rate adjustments, there is a need for datasets of industrial scale. However, achieving this scale requires a data generation process that is predominantly automated. In this study, we investigate techniques for creating extensive instruction datasets from a single prompt. With minimal human intervention, we prompt large language models (LLMs) to generate a wide range of instruction examples, from simple tasks to complex multi-turn dialogues covering various subjects. When fine-tuning a Llama-3 8B base model using our dataset, we achieve performance that matches or surpasses both WizardLM and Ultrachat on tasks requiring substantial knowledge as well as in conversational assessments. We make our dataset, the \"generator\" prompts used to create it, and our fine-tuned model checkpoints publicly available."}, "2303.12776": {"original_text": "  One-to-one label assignment in object detection has successfully obviated the\nneed for non-maximum suppression (NMS) as postprocessing and makes the pipeline\nend-to-end. However, it triggers a new dilemma as the widely used sparse\nqueries cannot guarantee a high recall, while dense queries inevitably bring\nmore similar queries and encounter optimization difficulties. As both sparse\nand dense queries are problematic, then what are the expected queries in\nend-to-end object detection? This paper shows that the solution should be Dense\nDistinct Queries (DDQ). Concretely, we first lay dense queries like traditional\ndetectors and then select distinct ones for one-to-one assignments. DDQ blends\nthe advantages of traditional and recent end-to-end detectors and significantly\nimproves the performance of various detectors including FCN, R-CNN, and DETRs.\nMost impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12\nepochs using a ResNet-50 backbone, outperforming all existing detectors in the\nsame setting. DDQ also shares the benefit of end-to-end detectors in crowded\nscenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchers\nto consider the complementarity between traditional methods and end-to-end\ndetectors. The source code can be found at\n\\url{https://github.com/jshilong/DDQ}.\n", "rewritten_text": "The use of one-to-one label assignment in object detection has eliminated the need for non-maximum suppression (NMS) as a postprocessing step, creating an end-to-end pipeline. However, this approach presents a new challenge: sparse queries may not ensure high recall, while dense queries can lead to optimization difficulties due to the presence of similar queries. Given these issues with both sparse and dense queries, the question arises: what should the ideal queries be for end-to-end object detection? This study proposes Dense Distinct Queries (DDQ) as the solution. Specifically, dense queries are initially laid out like in traditional detectors, and then distinct queries are selected for one-to-one assignments. DDQ combines the strengths of traditional and modern end-to-end detectors, resulting in significant performance improvements for various detectors such as FCN, R-CNN, and DETRs. Notably, DDQ-DETR achieves an impressive 52.1 AP on the MS-COCO dataset in just 12 epochs using a ResNet-50 backbone, surpassing all existing detectors in the same conditions. DDQ also excels in crowded scenes, achieving a 93.8 AP on CrowdHuman. This work aims to encourage researchers to explore the synergy between traditional methods and end-to-end detectors. The source code is available at \\url{https://github.com/jshilong/DDQ}."}, "2303.1112": {"original_text": "  Positional reasoning is the process of ordering unsorted parts contained in a\nset into a consistent structure. We present Positional Diffusion, a\nplug-and-play graph formulation with Diffusion Probabilistic Models to address\npositional reasoning. We use the forward process to map elements' positions in\na set to random positions in a continuous space. Positional Diffusion learns to\nreverse the noising process and recover the original positions through an\nAttention-based Graph Neural Network. We conduct extensive experiments with\nbenchmark datasets including two puzzle datasets, three sentence ordering\ndatasets, and one visual storytelling dataset, demonstrating that our method\noutperforms long-lasting research on puzzle solving with up to +18% compared to\nthe second-best deep learning method, and performs on par against the\nstate-of-the-art methods on sentence ordering and visual storytelling. Our work\nhighlights the suitability of diffusion models for ordering problems and\nproposes a novel formulation and method for solving various ordering tasks.\nProject website at https://iit-pavis.github.io/Positional_Diffusion/\n", "rewritten_text": "Positional reasoning involves organizing unsorted parts within a set into a coherent structure. This paper introduces Positional Diffusion, a graph-based approach utilizing Diffusion Probabilistic Models to tackle positional reasoning. By utilizing a forward process to map elements' positions in a set to random positions in a continuous space, Positional Diffusion learns to reverse this process and accurately reconstruct the original positions using an Attention-based Graph Neural Network. Through extensive experiments on diverse datasets, including puzzle, sentence ordering, and visual storytelling datasets, our method surpasses existing research on puzzle solving by up to +18% compared to the next best deep learning method. Additionally, it performs competitively with state-of-the-art methods on sentence ordering and visual storytelling tasks. This study underscores the effectiveness of diffusion models for ordering problems and introduces a novel formulation and approach for addressing various ordering challenges. For more information, visit our project website at https://iit-pavis.github.io/Positional_Diffusion/"}, "2311.13735": {"original_text": "  Recent advances in large language models (LLMs) show potential for clinical\napplications, such as clinical decision support and trial recommendations.\nHowever, the GPT-4 LLM predicts an excessive number of ICD codes for medical\ncoding tasks, leading to high recall but low precision. To tackle this\nchallenge, we introduce LLM-codex, a two-stage approach to predict ICD codes\nthat first generates evidence proposals using an LLM and then employs an\nLSTM-based verification stage. The LSTM learns from both the LLM's high recall\nand human expert's high precision, using a custom loss function. Our model is\nthe only approach that simultaneously achieves state-of-the-art results in\nmedical coding accuracy, accuracy on rare codes, and sentence-level evidence\nidentification to support coding decisions without training on human-annotated\nevidence according to experiments on the MIMIC dataset.\n", "rewritten_text": "Recent advancements in large language models (LLMs) have shown promise in clinical settings, such as providing support for clinical decisions and recommending trials. However, the GPT-4 LLM has been found to generate an excessive number of ICD codes during medical coding tasks, resulting in high recall but low precision. To address this issue, we propose LLM-codex, a two-stage approach for predicting ICD codes. This method first generates evidence proposals using an LLM and then utilizes an LSTM-based verification stage. The LSTM leverages the high recall of the LLM and the high precision of human experts by incorporating a custom loss function. Our model is unique in that it achieves top-tier results in medical coding accuracy, accuracy for rare codes, and identification of sentence-level evidence to support coding decisions, all without the need for training on human-annotated evidence, as demonstrated through experiments on the MIMIC dataset."}, "2209.14575": {"original_text": "  In this paper, we investigate the problem of bit allocation in Neural Video\nCompression (NVC). First, we reveal that a recent bit allocation approach\nclaimed to be optimal is, in fact, sub-optimal due to its implementation.\nSpecifically, we find that its sub-optimality lies in the improper application\nof semi-amortized variational inference (SAVI) on latent with non-factorized\nvariational posterior. Then, we show that the corrected version of SAVI on\nnon-factorized latent requires recursively applying back-propagating through\ngradient ascent, based on which we derive the corrected optimal bit allocation\nalgorithm. Due to the computational in-feasibility of the corrected bit\nallocation, we design an efficient approximation to make it practical.\nEmpirical results show that our proposed correction significantly improves the\nincorrect bit allocation in terms of R-D performance and bitrate error, and\noutperforms all other bit allocation methods by a large margin. The source code\nis provided in the supplementary material.\n", "rewritten_text": "\"In this study, we examine the issue of bit allocation in Neural Video Compression (NVC). Initially, we discover that a recently proposed optimal bit allocation method is actually suboptimal due to its implementation. Specifically, we identify that the suboptimal aspect stems from the improper use of semi-amortized variational inference (SAVI) on latent variables with a non-factorized variational posterior. We then demonstrate that the corrected version of SAVI on non-factorized latent variables necessitates recursively applying back-propagation through gradient ascent. This leads us to develop an improved optimal bit allocation algorithm. However, due to the computational challenges of the corrected bit allocation, we devise an efficient approximation to make it feasible. Our experimental findings indicate that our proposed correction significantly enhances the inaccurate bit allocation in terms of R-D performance and bitrate error, surpassing all other bit allocation methods by a considerable margin. The supplementary material includes the source code.\""}, "2302.03744": {"original_text": "  The ability to perceive and understand 3D scenes is crucial for many\napplications in computer vision and robotics. Inverse graphics is an appealing\napproach to 3D scene understanding that aims to infer the 3D scene structure\nfrom 2D images. In this paper, we introduce probabilistic modeling to the\ninverse graphics framework to quantify uncertainty and achieve robustness in 6D\npose estimation tasks. Specifically, we propose 3D Neural Embedding Likelihood\n(3DNEL) as a unified probabilistic model over RGB-D images, and develop\nefficient inference procedures on 3D scene descriptions. 3DNEL effectively\ncombines learned neural embeddings from RGB with depth information to improve\nrobustness in sim-to-real 6D object pose estimation from RGB-D images.\nPerformance on the YCB-Video dataset is on par with state-of-the-art yet is\nmuch more robust in challenging regimes. In contrast to discriminative\napproaches, 3DNEL's probabilistic generative formulation jointly models\nmultiple objects in a scene, quantifies uncertainty in a principled way, and\nhandles object pose tracking under heavy occlusion. Finally, 3DNEL provides a\nprincipled framework for incorporating prior knowledge about the scene and\nobjects, which allows natural extension to additional tasks like camera pose\ntracking from video.\n", "rewritten_text": "The ability to understand three-dimensional scenes is essential for various applications in computer vision and robotics. Inverse graphics offers a promising approach to this by inferring the 3D structure of a scene from 2D images. This paper introduces probabilistic modeling to the inverse graphics framework to enhance robustness in estimating the six-dimensional pose. The proposed 3D Neural Embedding Likelihood (3DNEL) model combines neural embeddings from RGB and depth information to improve object pose estimation accuracy. The performance on the YCB-Video dataset matches state-of-the-art methods and excels in challenging scenarios. Unlike discriminative approaches, 3DNEL's generative formulation models multiple objects in a scene, quantifies uncertainty, and handles object pose tracking under occlusion. Additionally, 3DNEL allows for incorporating prior knowledge about scenes and objects, enabling extensions to tasks like camera pose tracking from video."}, "2005.04551": {"original_text": "  A common approach to localize 3D human joints in a synchronized and\ncalibrated multi-view setup consists of two-steps: (1) apply a 2D detector\nseparately on each view to localize joints in 2D, and (2) perform robust\ntriangulation on 2D detections from each view to acquire the 3D joint\nlocations. However, in step 1, the 2D detector is limited to solving\nchallenging cases which could potentially be better resolved in 3D, such as\nocclusions and oblique viewing angles, purely in 2D without leveraging any 3D\ninformation. Therefore, we propose the differentiable \"epipolar transformer\",\nwhich enables the 2D detector to leverage 3D-aware features to improve 2D pose\nestimation. The intuition is: given a 2D location p in the current view, we\nwould like to first find its corresponding point p' in a neighboring view, and\nthen combine the features at p' with the features at p, thus leading to a\n3D-aware feature at p. Inspired by stereo matching, the epipolar transformer\nleverages epipolar constraints and feature matching to approximate the features\nat p'. Experiments on InterHand and Human3.6M show that our approach has\nconsistent improvements over the baselines. Specifically, in the condition\nwhere no external data is used, our Human3.6M model trained with ResNet-50\nbackbone and image size 256 x 256 outperforms state-of-the-art by 4.23 mm and\nachieves MPJPE 26.9 mm.\n", "rewritten_text": "One common method for localizing 3D human joints in a synchronized and calibrated multi-view setup involves a two-step process: first, applying a 2D detector separately on each view to locate joints in 2D, and then performing robust triangulation on the 2D detections from each view to determine the 3D joint positions. However, the 2D detector in the first step is limited in addressing challenging scenarios, such as occlusions and oblique viewing angles, which could potentially be better resolved in 3D. This limitation arises from the detector operating solely in 2D without utilizing any 3D information. To address this issue, we introduce the differentiable \"epipolar transformer,\" which allows the 2D detector to incorporate 3D-aware features for enhanced 2D pose estimation. The concept is to identify the corresponding point p' in a neighboring view for a given 2D location p, and then merge the features at p' with those at p to generate a 3D-aware feature at p. Drawing inspiration from stereo matching, the epipolar transformer utilizes epipolar constraints and feature matching to estimate the features at p'. Experimental results on InterHand and Human3.6M datasets demonstrate that our approach consistently outperforms baseline methods. Specifically, our Human3.6M model, trained with a ResNet-50 backbone and image size of 256 x 256, achieves a Mean Per Joint Position Error (MPJPE) of 26.9 mm, surpassing the state-of-the-art by 4.23 mm in scenarios where no external data is utilized."}, "1912.0643": {"original_text": "  Annotating videos is cumbersome, expensive and not scalable. Yet, many strong\nvideo models still rely on manually annotated data. With the recent\nintroduction of the HowTo100M dataset, narrated videos now offer the\npossibility of learning video representations without manual supervision. In\nthis work we propose a new learning approach, MIL-NCE, capable of addressing\nmisalignments inherent to narrated videos. With this approach we are able to\nlearn strong video representations from scratch, without the need for any\nmanual annotation. We evaluate our representations on a wide range of four\ndownstream tasks over eight datasets: action recognition (HMDB-51, UCF-101,\nKinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization\n(YouTube-8M Segments, CrossTask) and action segmentation (COIN). Our method\noutperforms all published self-supervised approaches for these tasks as well as\nseveral fully supervised baselines.\n", "rewritten_text": "\"Although annotating videos is typically burdensome, costly, and not easily scalable, many advanced video models still rely on manually annotated data. The introduction of the HowTo100M dataset has opened up the possibility of learning video representations from narrated videos without the need for manual supervision. In this study, we present a novel learning approach called MIL-NCE, which effectively addresses the challenges associated with narrated videos. Using this approach, we successfully develop strong video representations from scratch, eliminating the requirement for manual annotation. Our representations are evaluated across a variety of downstream tasks on eight datasets, including action recognition (HMDB-51, UCF-101, Kinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization (YouTube-8M Segments, CrossTask), and action segmentation (COIN). Our method surpasses all previously published self-supervised approaches and several fully supervised baselines in performance on these tasks.\""}, "2211.09795": {"original_text": "  Diffusion models have become the go-to method for many generative tasks,\nparticularly for image-to-image generation tasks such as super-resolution and\ninpainting. Current diffusion-based methods do not provide statistical\nguarantees regarding the generated results, often preventing their use in\nhigh-stakes situations. To bridge this gap, we construct a confidence interval\naround each generated pixel such that the true value of the pixel is guaranteed\nto fall within the interval with a probability set by the user. Since diffusion\nmodels parametrize the data distribution, a straightforward way of constructing\nsuch intervals is by drawing multiple samples and calculating their bounds.\nHowever, this method has several drawbacks: i) slow sampling speeds ii)\nsuboptimal bounds iii) requires training a diffusion model per task. To\nmitigate these shortcomings we propose Conffusion, wherein we fine-tune a\npre-trained diffusion model to predict interval bounds in a single forward\npass. We show that Conffusion outperforms the baseline method while being three\norders of magnitude faster.\n", "rewritten_text": "Diffusion models are commonly used for various generative tasks, especially for tasks like super-resolution and inpainting in image-to-image generation. However, current diffusion-based methods lack statistical guarantees for the generated results, limiting their applicability in critical scenarios. To address this limitation, we introduce a method that establishes a confidence interval around each generated pixel, ensuring that the true pixel value falls within the interval with a user-defined probability. While traditional approaches involve drawing multiple samples and calculating bounds, this method is slow, provides suboptimal bounds, and requires training a diffusion model for each task. In contrast, our proposed approach, Conffusion, fine-tunes a pre-trained diffusion model to predict interval bounds in a single forward pass. Our results demonstrate that Conffusion surpasses the baseline method in performance and is significantly faster by three orders of magnitude."}, "2404.14248": {"original_text": "  This paper reviews the NTIRE 2024 low light image enhancement challenge,\nhighlighting the proposed solutions and results. The aim of this challenge is\nto discover an effective network design or solution capable of generating\nbrighter, clearer, and visually appealing results when dealing with a variety\nof conditions, including ultra-high resolution (4K and beyond), non-uniform\nillumination, backlighting, extreme darkness, and night scenes. A notable total\nof 428 participants registered for the challenge, with 22 teams ultimately\nmaking valid submissions. This paper meticulously evaluates the\nstate-of-the-art advancements in enhancing low-light images, reflecting the\nsignificant progress and creativity in this field.\n", "rewritten_text": "This document examines the NTIRE 2024 competition focused on improving low light images, showcasing the proposed approaches and outcomes. The goal of this competition is to identify an efficient network design or solution that can produce brighter, clearer, and visually appealing results across various challenging conditions, such as ultra-high resolution (4K and higher), uneven lighting, backlighting, extreme darkness, and nighttime settings. A total of 428 participants signed up for the competition, with 22 teams ultimately submitting valid entries. This document thoroughly assesses the latest developments in enhancing low-light images, demonstrating the notable advancements and innovation in this area."}, "2311.01907": {"original_text": "  Automatic simplification can help laypeople to comprehend complex scientific\ntext. Language models are frequently applied to this task by translating from\ncomplex to simple language. In this paper, we describe our system based on\nLlama 2, which ranked first in the PLABA shared task addressing the\nsimplification of biomedical text. We find that the large portion of shared\ntokens between input and output leads to weak training signals and\nconservatively editing models. To mitigate these issues, we propose\nsentence-level and token-level loss weights. They give higher weight to\nmodified tokens, indicated by edit distance and edit operations, respectively.\nWe conduct an empirical evaluation on the PLABA dataset and find that both\napproaches lead to simplifications closer to those created by human annotators\n(+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x /\n1.8x edit distance) compared to the same model fine-tuned with standard cross\nentropy. We furthermore show that the hyperparameter $\\lambda$ in token-level\nloss weights can be used to control the edit distance and the simplicity level\n(FKGL).\n", "rewritten_text": "\"Utilizing automatic simplification can assist non-experts in understanding complex scientific text. Language models are commonly used for this purpose by converting complex language into simpler terms. In this study, we present our system based on Llama 2, which achieved top ranking in the PLABA shared task focused on simplifying biomedical text. We observed that a significant overlap of words between the original text and the simplified version results in limited training signals and cautious editing by models. To address this issue, we suggest adjusting the weights of losses at the sentence and token levels. These weights prioritize modified tokens based on edit distance and edit operations. Through an empirical evaluation using the PLABA dataset, we demonstrate that both strategies lead to simplifications that closely resemble those made by human annotators (+1.8% / +3.5% SARI), use simpler language (-1 / -1.1 FKGL), and involve more edits (1.6x / 1.8x edit distance) compared to the same model fine-tuned with standard cross entropy. Additionally, we show that the hyperparameter $\\lambda$ in token-level loss weights can be manipulated to control the edit distance and the level of simplicity (FKGL).\""}, "2402.12923": {"original_text": "  In recent years, 3D point clouds (PCs) have gained significant attention due\nto their diverse applications across various fields, such as computer vision\n(CV), condition monitoring (CM), virtual reality, robotics, autonomous driving,\netc. Deep learning (DL) has proven effective in leveraging 3D PCs to address\nvarious challenges encountered in 2D vision. However, applying deep neural\nnetworks (DNNs) to process 3D PCs presents unique challenges. This paper\nprovides an in-depth review of recent advancements in DL-based industrial CM\nusing 3D PCs, with a specific focus on defect shape classification and\nsegmentation within industrial applications. Recognizing the crucial role of\nthese aspects in industrial maintenance, the paper offers insightful\nobservations on the strengths and limitations of the reviewed DL-based PC\nprocessing methods. This knowledge synthesis aims to contribute to\nunderstanding and enhancing CM processes, particularly within the framework of\nremaining useful life (RUL), in industrial systems.\n", "rewritten_text": "\"In recent years, there has been a growing interest in 3D point clouds (PCs) due to their wide range of applications in fields such as computer vision (CV), condition monitoring (CM), virtual reality, robotics, and autonomous driving. Deep learning (DL) has been successful in utilizing 3D PCs to tackle challenges that arise in 2D vision. However, applying deep neural networks (DNNs) to process 3D PCs comes with its own set of unique challenges. This paper presents a detailed overview of recent progress in DL-based industrial CM using 3D PCs, focusing specifically on defect shape classification and segmentation in industrial settings. Recognizing the importance of these aspects in industrial maintenance, the paper provides valuable insights into the strengths and limitations of the reviewed DL-based PC processing techniques. The goal of this knowledge synthesis is to contribute to the understanding and improvement of CM processes, particularly in the context of remaining useful life (RUL) in industrial systems.\""}, "2402.02082": {"original_text": "  Speculative decoding is a relatively new decoding framework that leverages\nsmall and efficient draft models to reduce the latency of LLMs. In this study,\nwe introduce GliDe and CaPE, two low-hassle modifications to vanilla\nspeculative decoding to further improve the decoding speed of a frozen LLM.\nSpecifically, GliDe is a modified draft model architecture that reuses the\ncached keys and values from the target LLM, while CaPE is a proposal expansion\nmethod that uses the draft model's confidence scores to help select additional\ncandidate tokens for verification. Extensive experiments on different\nbenchmarks demonstrate that our proposed GliDe draft model significantly\nreduces the expected decoding latency. Additional evaluation using walltime\nreveals that GliDe can accelerate Vicuna models up to 2.17x and further extend\nthe improvement to 2.61x with CaPE. We will release our code, data, and the\ntrained draft models.\n", "rewritten_text": "\"In this research, we present GliDe and CaPE, two enhancements to speculative decoding aimed at boosting the decoding speed of a pre-trained language model (LLM). GliDe involves a modified draft model architecture that utilizes cached keys and values from the target LLM, while CaPE is a method that leverages the draft model's confidence scores to select additional candidate tokens for verification. Our experiments across various benchmarks demonstrate that GliDe significantly reduces decoding latency. Furthermore, using walltime evaluation, we show that GliDe can accelerate Vicuna models by up to 2.17x, with an additional improvement of 2.61x when combined with CaPE. We plan to make our code, data, and trained draft models publicly available.\""}, "2302.14354": {"original_text": "  The cultural heritage buildings (CHB), which are part of mankind's history\nand identity, are in constant danger of damage or in extreme situations total\ndestruction. That being said, it's of utmost importance to preserve them by\nidentifying the existent, or presumptive, defects using novel methods so that\nrenovation processes can be done in a timely manner and with higher accuracy.\nThe main goal of this research is to use new deep learning (DL) methods in the\nprocess of preserving CHBs (situated in Iran); a goal that has been neglected\nespecially in developing countries such as Iran, as these countries still\npreserve their CHBs using manual, and even archaic, methods that need direct\nhuman supervision. Having proven their effectiveness and performance when it\ncomes to processing images, the convolutional neural networks (CNN) are a\nstaple in computer vision (CV) literacy and this paper is not exempt. When\nlacking enough CHB images, training a CNN from scratch would be very difficult\nand prone to overfitting; that's why we opted to use a technique called\ntransfer learning (TL) in which we used pre-trained ResNet, MobileNet, and\nInception networks, for classification. Even more, the Grad-CAM was utilized to\nlocalize the defects to some extent. The final results were very favorable\nbased on those of similar research. The final proposed model can pave the way\nfor moving from manual to unmanned CHB conservation, hence an increase in\naccuracy and a decrease in human-induced errors.\n", "rewritten_text": "Cultural heritage buildings (CHBs), integral to human history and identity, are constantly at risk of damage or even complete destruction. Preserving these structures is crucial, necessitating the identification of existing or potential defects through innovative methods to facilitate timely and accurate renovation processes. This study aims to leverage new deep learning (DL) techniques to safeguard CHBs in Iran, a neglected aspect in developing countries like Iran where manual and outdated preservation methods still prevail. Convolutional neural networks (CNNs), renowned for their image processing capabilities, play a key role in this research. Due to limited CHB images, transfer learning (TL) was employed using pre-trained ResNet, MobileNet, and Inception networks for classification, with Grad-CAM aiding in defect localization. The results were promising, suggesting a shift towards unmanned CHB conservation for enhanced accuracy and reduced human errors."}, "2303.0149": {"original_text": "  Language identification is an important first step in many IR and NLP\napplications. Most publicly available language identification datasets,\nhowever, are compiled under the assumption that the gold label of each instance\nis determined by where texts are retrieved from. Research has shown that this\nis a problematic assumption, particularly in the case of very similar languages\n(e.g., Croatian and Serbian) and national language varieties (e.g., Brazilian\nand European Portuguese), where texts may contain no distinctive marker of the\nparticular language or variety. To overcome this important limitation, this\npaper presents DSL True Labels (DSL-TL), the first human-annotated multilingual\ndataset for language variety identification. DSL-TL contains a total of 12,900\ninstances in Portuguese, split between European Portuguese and Brazilian\nPortuguese; Spanish, split between Argentine Spanish and Castilian Spanish; and\nEnglish, split between American English and British English. We trained\nmultiple models to discriminate between these language varieties, and we\npresent the results in detail. The data and models presented in this paper\nprovide a reliable benchmark toward the development of robust and fairer\nlanguage variety identification systems. We make DSL-TL freely available to the\nresearch community.\n", "rewritten_text": "Identifying language is a crucial initial step in various information retrieval (IR) and natural language processing (NLP) tasks. While existing language identification datasets often rely on the source of texts to determine the correct label for each instance, this approach can be problematic, especially for closely related languages (e.g., Croatian and Serbian) and national language variations (e.g., Brazilian and European Portuguese) where texts may lack distinct markers of a specific language or variety. To address this limitation, this paper introduces DSL True Labels (DSL-TL), a novel human-annotated multilingual dataset designed for identifying language varieties. DSL-TL comprises 12,900 instances in Portuguese, divided into European Portuguese and Brazilian Portuguese; Spanish, divided into Argentine Spanish and Castilian Spanish; and English, divided into American English and British English. By training multiple models to distinguish between these language variations, detailed results are presented. The data and models shared in this paper establish a reliable benchmark for the development of more robust and equitable language variety identification systems. DSL-TL is openly accessible to the research community."}, "2004.0827": {"original_text": "  In this paper, we tackle the task of automatically analyzing 3D volumetric\nscans obtained from computed tomography (CT) devices. In particular, we address\na particular task for which data is very limited: the segmentation of ancient\nEgyptian mummies CT scans. We aim at digitally unwrapping the mummy and\nidentify different segments such as body, bandages and jewelry. The problem is\ncomplex because of the lack of annotated data for the different semantic\nregions to segment, thus discouraging the use of strongly supervised\napproaches. We, therefore, propose a weakly supervised and efficient\ninteractive segmentation method to solve this challenging problem. After\nsegmenting the wrapped mummy from its exterior region using histogram analysis\nand template matching, we first design a voxel distance measure to find an\napproximate solution for the body and bandage segments. Here, we use geodesic\ndistances since voxel features as well as spatial relationship among voxels is\nincorporated in this measure. Next, we refine the solution using a GrabCut\nbased segmentation together with a tracking method on the slices of the scan\nthat assigns labels to different regions in the volume, using limited\nsupervision in the form of scribbles drawn by the user. The efficiency of the\nproposed method is demonstrated using visualizations and validated through\nquantitative measures and qualitative unwrapping of the mummy.\n", "rewritten_text": "This paper focuses on automatically analyzing 3D volumetric scans from CT devices, specifically addressing the segmentation of ancient Egyptian mummies' CT scans. The goal is to digitally unwrap the mummy and identify segments like body, bandages, and jewelry. Due to limited annotated data for segmentation, we propose a weakly supervised and interactive segmentation method. The process involves segmenting the mummy using histogram analysis and template matching, followed by a voxel distance measure to approximate body and bandage segments. Geodesic distances are utilized to incorporate voxel features and spatial relationships. Further refinement is achieved through GrabCut-based segmentation and tracking on scan slices, with user-drawn scribbles providing limited supervision. The method's efficiency is demonstrated through visualizations, quantitative measures, and qualitative unwrapping of the mummy."}, "1607.06871": {"original_text": "  The \"interpretation through synthesis\" approach to analyze face images,\nparticularly Active Appearance Models (AAMs) method, has become one of the most\nsuccessful face modeling approaches over the last two decades. AAM models have\nability to represent face images through synthesis using a controllable\nparameterized Principal Component Analysis (PCA) model. However, the accuracy\nand robustness of the synthesized faces of AAM are highly depended on the\ntraining sets and inherently on the generalizability of PCA subspaces. This\npaper presents a novel Deep Appearance Models (DAMs) approach, an efficient\nreplacement for AAMs, to accurately capture both shape and texture of face\nimages under large variations. In this approach, three crucial components\nrepresented in hierarchical layers are modeled using the Deep Boltzmann\nMachines (DBM) to robustly capture the variations of facial shapes and\nappearances. DAMs are therefore superior to AAMs in inferencing a\nrepresentation for new face images under various challenging conditions. The\nproposed approach is evaluated in various applications to demonstrate its\nrobustness and capabilities, i.e. facial super-resolution reconstruction,\nfacial off-angle reconstruction or face frontalization, facial occlusion\nremoval and age estimation using challenging face databases, i.e. Labeled Face\nParts in the Wild (LFPW), Helen and FG-NET. Comparing to AAMs and other deep\nlearning based approaches, the proposed DAMs achieve competitive results in\nthose applications, thus this showed their advantages in handling occlusions,\nfacial representation, and reconstruction.\n", "rewritten_text": "The approach known as \"interpretation through synthesis\" for analyzing face images, specifically the Active Appearance Models (AAMs) method, has emerged as a highly successful face modeling technique in the past two decades. AAM models can represent face images through synthesis by utilizing a controllable parameterized Principal Component Analysis (PCA) model. However, the accuracy and robustness of the synthesized faces produced by AAMs heavily rely on the quality of the training sets and the generalizability of PCA subspaces. This paper introduces a new method called Deep Appearance Models (DAMs) as a more efficient alternative to AAMs, capable of accurately capturing both the shape and texture of face images across a wide range of variations. In DAMs, three essential components organized in hierarchical layers are modeled using Deep Boltzmann Machines (DBM) to effectively capture the variations in facial shapes and appearances. DAMs outperform AAMs in generating representations for new face images under diverse challenging conditions. The proposed approach is assessed across various applications to showcase its robustness and capabilities, including facial super-resolution reconstruction, facial off-angle reconstruction or face frontalization, facial occlusion removal, and age estimation using challenging face databases such as Labeled Face Parts in the Wild (LFPW), Helen, and FG-NET. Compared to AAMs and other deep learning-based methods, DAMs demonstrate competitive results in these applications, highlighting their advantages in handling occlusions, facial representation, and reconstruction."}, "2105.11872": {"original_text": "  Despite recent advances, standard sequence labeling systems often fail when\nprocessing noisy user-generated text or consuming the output of an Optical\nCharacter Recognition (OCR) process. In this paper, we improve the noise-aware\ntraining method by proposing an empirical error generation approach that\nemploys a sequence-to-sequence model trained to perform translation from\nerror-free to erroneous text. Using an OCR engine, we generated a large\nparallel text corpus for training and produced several real-world noisy\nsequence labeling benchmarks for evaluation. Moreover, to overcome the data\nsparsity problem that exacerbates in the case of imperfect textual input, we\nlearned noisy language model-based embeddings. Our approach outperformed the\nbaseline noise generation and error correction techniques on the erroneous\nsequence labeling data sets. To facilitate future research on robustness, we\nmake our code, embeddings, and data conversion scripts publicly available.\n", "rewritten_text": "\"In this paper, we address the challenges faced by standard sequence labeling systems in handling noisy user-generated text and OCR output. We propose an enhanced noise-aware training method that utilizes an empirical error generation approach. This involves training a sequence-to-sequence model to translate error-free text into erroneous text. By creating a large parallel text corpus using an OCR engine, we developed real-world noisy sequence labeling benchmarks for evaluation. Additionally, we tackled the issue of data sparsity in imperfect textual input by incorporating noisy language model-based embeddings. Our approach surpassed traditional noise generation and error correction techniques on erroneous sequence labeling datasets. To support further research on robustness, we have made our code, embeddings, and data conversion scripts publicly accessible.\""}, "2102.00062": {"original_text": "  In this paper, we present a method of clothes retargeting; generating the\npotential poses and deformations of a given 3D clothing template model to fit\nonto a person in a single RGB image. The problem is fundamentally ill-posed as\nattaining the ground truth data is impossible, i.e., images of people wearing\nthe different 3D clothing template model at exact same pose. We address this\nchallenge by utilizing large-scale synthetic data generated from physical\nsimulation, allowing us to map 2D dense body pose to 3D clothing deformation.\nWith the simulated data, we propose a semi-supervised learning framework that\nvalidates the physical plausibility of the 3D deformation by matching with the\nprescribed body-to-cloth contact points and clothing silhouette to fit onto the\nunlabeled real images. A new neural clothes retargeting network (CRNet) is\ndesigned to integrate the semi-supervised retargeting task in an end-to-end\nfashion. In our evaluation, we show that our method can predict the realistic\n3D pose and deformation field needed for retargeting clothes models in\nreal-world examples.\n", "rewritten_text": "In this paper, we introduce a technique for adapting clothing to fit onto a person in a single RGB image by adjusting the poses and shapes of a 3D clothing template model. The challenge lies in the fact that obtaining accurate ground truth data is unfeasible, as it would require images of individuals wearing the same 3D clothing template model in identical poses. To overcome this obstacle, we leverage large-scale synthetic data generated through physical simulation to establish a connection between 2D body poses and 3D clothing deformations. Using this simulated data, we propose a semi-supervised learning framework that verifies the physical plausibility of the 3D deformations by aligning them with specified body-to-cloth contact points and clothing silhouettes to adapt to unlabeled real images. We introduce a novel neural network called CRNet, specifically designed to handle the semi-supervised retargeting task seamlessly. Through our evaluation, we demonstrate that our approach can accurately predict the realistic 3D poses and deformations necessary for adapting clothing models in real-world scenarios."}, "1810.0578": {"original_text": "  Pose estimation is a widely explored problem, enabling many robotic tasks\nsuch as grasping and manipulation. In this paper, we tackle the problem of pose\nestimation for objects that exhibit rotational symmetry, which are common in\nman-made and industrial environments. In particular, our aim is to infer poses\nfor objects not seen at training time, but for which their 3D CAD models are\navailable at test time. Previous work has tackled this problem by learning to\ncompare captured views of real objects with the rendered views of their 3D CAD\nmodels, by embedding them in a joint latent space using neural networks. We\nshow that sidestepping the issue of symmetry in this scenario during training\nleads to poor performance at test time. We propose a model that reasons about\nrotational symmetry during training by having access to only a small set of\nsymmetry-labeled objects, whereby exploiting a large collection of unlabeled\nCAD models. We demonstrate that our approach significantly outperforms a\nnaively trained neural network on a new pose dataset containing images of tools\nand hardware.\n", "rewritten_text": "In this study, we address the challenge of pose estimation for objects with rotational symmetry, commonly found in man-made and industrial settings. Our goal is to predict poses for objects not encountered during training, but for which 3D CAD models are available at testing. Previous approaches have compared real object views with rendered CAD model views using neural networks, but neglecting symmetry during training has resulted in poor performance at test time. To overcome this, we propose a model that considers rotational symmetry during training by utilizing a small set of symmetry-labeled objects and a large collection of unlabeled CAD models. Our method outperforms a standard neural network on a new pose dataset featuring images of tools and hardware."}, "2012.15832": {"original_text": "  Increasing the input length has been a driver of progress in language\nmodeling with transformers. We identify conditions where shorter inputs are not\nharmful, and achieve perplexity and efficiency improvements through two new\nmethods that decrease input length. First, we show that initially training a\nmodel on short subsequences before moving on to longer ones both reduces\noverall training time and, surprisingly, substantially improves perplexity.\nSecond, we show how to improve the efficiency of recurrence methods in\ntransformers, which let models condition on previously processed tokens when\ngenerating sequences that exceed the maximal length the transformer can handle\nat once. Existing methods require computationally expensive relative position\nembeddings; we introduce a simple alternative of adding absolute position\nembeddings to queries and keys instead of to word embeddings, which efficiently\nproduces superior results. We show that these recurrent models also benefit\nfrom short input lengths. Combining these techniques speeds up training by a\nfactor of 1.65, reduces memory usage, and substantially improves perplexity on\nWikiText-103, without adding any parameters.\n", "rewritten_text": "Increasing the length of input has driven progress in language modeling using transformers. We have identified scenarios where shorter inputs are not detrimental and have developed two new methods to decrease input length, resulting in improvements in perplexity and efficiency. Firstly, we demonstrate that training a model on short subsequences before longer ones reduces overall training time and significantly enhances perplexity. Secondly, we enhance the efficiency of recurrence methods in transformers, allowing models to condition on previously processed tokens when generating sequences longer than the transformer's maximum handling capacity. Our approach involves adding absolute position embeddings to queries and keys instead of word embeddings, eliminating the need for computationally expensive relative position embeddings. These techniques not only accelerate training by 1.65 times and reduce memory usage but also notably enhance perplexity on WikiText-103 without increasing parameters."}, "cs/0006021": {"original_text": "  Systems now exist which are able to compile unification grammars into\nlanguage models that can be included in a speech recognizer, but it is so far\nunclear whether non-trivial linguistically principled grammars can be used for\nthis purpose. We describe a series of experiments which investigate the\nquestion empirically, by incrementally constructing a grammar and discovering\nwhat problems emerge when successively larger versions are compiled into finite\nstate graph representations and used as language models for a medium-vocabulary\nrecognition task.\n", "rewritten_text": "There are currently systems available that can convert unification grammars into language models for speech recognition. However, it is uncertain whether complex linguistically principled grammars can be effectively utilized for this purpose. Our experiments aim to address this question by gradually building a grammar and examining the challenges that arise as larger versions are converted into finite state graph representations for use as language models in a medium-vocabulary recognition task."}, "2209.13351": {"original_text": "  Accurately and timely detecting multiscale small objects that contain tens of\npixels from remote sensing images (RSI) remains challenging. Most of the\nexisting solutions primarily design complex deep neural networks to learn\nstrong feature representations for objects separated from the background, which\noften results in a heavy computation burden. In this article, we propose an\naccurate yet fast object detection method for RSI, named SuperYOLO, which fuses\nmultimodal data and performs high-resolution (HR) object detection on\nmultiscale objects by utilizing the assisted super resolution (SR) learning and\nconsidering both the detection accuracy and computation cost. First, we utilize\na symmetric compact multimodal fusion (MF) to extract supplementary information\nfrom various data for improving small object detection in RSI. Furthermore, we\ndesign a simple and flexible SR branch to learn HR feature representations that\ncan discriminate small objects from vast backgrounds with low-resolution (LR)\ninput, thus further improving the detection accuracy. Moreover, to avoid\nintroducing additional computation, the SR branch is discarded in the inference\nstage, and the computation of the network model is reduced due to the LR input.\nExperimental results show that, on the widely used VEDAI RS dataset, SuperYOLO\nachieves an accuracy of 75.09% (in terms of mAP50 ), which is more than 10%\nhigher than the SOTA large models, such as YOLOv5l, YOLOv5x, and RS designed\nYOLOrs. Meanwhile, the parameter size and GFLOPs of SuperYOLO are about 18\ntimes and 3.8 times less than YOLOv5x. Our proposed model shows a favorable\naccuracy and speed tradeoff compared to the state-of-the-art models. The code\nwill be open-sourced at https://github.com/icey-zhang/SuperYOLO.\n", "rewritten_text": "Detecting small objects at multiple scales accurately and promptly from remote sensing images (RSI) remains a challenge. Many current approaches rely on complex deep neural networks to extract strong features for object detection, leading to high computational costs. In this study, we introduce SuperYOLO, a fast and precise object detection method for RSI. SuperYOLO leverages multimodal data fusion and high-resolution (HR) object detection techniques, incorporating assisted super resolution (SR) learning to balance detection accuracy and computational efficiency. By employing a compact multimodal fusion (MF) approach, we enhance small object detection in RSI by extracting additional information from diverse data sources. Additionally, a simple and adaptable SR branch is designed to learn HR feature representations that distinguish small objects from complex backgrounds with low-resolution (LR) input, further enhancing detection accuracy. During inference, the SR branch is omitted to reduce computational load, resulting in improved efficiency. Experimental results on the VEDAI RS dataset demonstrate that SuperYOLO achieves an accuracy of 75.09% (mAP50), surpassing leading models like YOLOv5l, YOLOv5x, and YOLOrs in accuracy while significantly reducing parameter size and computational complexity. Our model strikes a favorable balance between accuracy and speed compared to existing models. The code for SuperYOLO will be available on https://github.com/icey-zhang/SuperYOLO."}, "2109.01958": {"original_text": "  Transformer-based pre-trained language models boost the performance of\nopen-domain dialogue systems. Prior works leverage Transformer-based\npre-trained language models to generate texts with desired attributes in two\ngeneral approaches: (1) gradient-based methods: updating all latent\nrepresentations of pre-trained models with gradients from attribute models; (2)\nweighted-decoding methods: re-ranking beam candidates from pre-trained models\nwith attribute functions. However, gradient-based methods lead to high\ncomputation cost and can easily get overfitted on small training sets, while\nweighted-decoding methods are inherently constrained by the low-variance\nhigh-bias pre-trained model. In this work, we propose a novel approach to\ncontrol the generation of Transformer-based pre-trained language models: the\nSideControl framework, which leverages a novel control attributes loss to\nincorporate useful control signals, and is shown to perform well with very\nlimited training samples. We evaluate our proposed method on two benchmark\nopen-domain dialogue datasets, and results show that the SideControl framework\nhas better controllability, higher generation quality and better\nsample-efficiency than existing gradient-based and weighted-decoding baselines.\n", "rewritten_text": "Transformer-based pre-trained language models enhance the performance of open-domain dialogue systems. Previous studies have utilized these models in two main ways to generate text with specific attributes: (1) gradient-based methods involve updating all latent representations of pre-trained models using gradients from attribute models; (2) weighted-decoding methods involve re-ranking beam candidates from pre-trained models with attribute functions. However, gradient-based methods are computationally expensive and prone to overfitting on small training sets, while weighted-decoding methods are limited by the pre-trained model's bias. In this study, we introduce a new approach for controlling the generation of Transformer-based pre-trained language models called the SideControl framework. This framework incorporates a novel control attributes loss to integrate useful control signals and demonstrates strong performance even with minimal training data. Our method is evaluated on two standard open-domain dialogue datasets, showing that the SideControl framework offers improved controllability, higher generation quality, and better sample efficiency compared to existing gradient-based and weighted-decoding approaches."}, "2205.02022": {"original_text": "  Recent advances in the pre-training of language models leverage large-scale\ndatasets to create multilingual models. However, low-resource languages are\nmostly left out in these datasets. This is primarily because many widely spoken\nlanguages are not well represented on the web and therefore excluded from the\nlarge-scale crawls used to create datasets. Furthermore, downstream users of\nthese models are restricted to the selection of languages originally chosen for\npre-training. This work investigates how to optimally leverage existing\npre-trained models to create low-resource translation systems for 16 African\nlanguages. We focus on two questions: 1) How can pre-trained models be used for\nlanguages not included in the initial pre-training? and 2) How can the\nresulting translation models effectively transfer to new domains? To answer\nthese questions, we create a new African news corpus covering 16 languages, of\nwhich eight languages are not part of any existing evaluation dataset. We\ndemonstrate that the most effective strategy for transferring both to\nadditional languages and to additional domains is to fine-tune large\npre-trained models on small quantities of high-quality translation data.\n", "rewritten_text": "Recent advancements in language model pre-training involve utilizing extensive datasets to develop multilingual models. However, low-resource languages are often overlooked in these datasets due to their limited representation online. Consequently, these languages are excluded from the large-scale data crawls used to construct datasets. Additionally, users of these models are constrained to the languages initially chosen for pre-training. This study explores the optimal utilization of existing pre-trained models to build low-resource translation systems for 16 African languages. The research focuses on two key inquiries: 1) How can pre-trained models be applied to languages not included in the original pre-training? and 2) How can the resulting translation models effectively adapt to new domains? To address these questions, a new African news corpus encompassing 16 languages is developed, with eight languages not featured in any existing evaluation dataset. The study reveals that the most successful approach for extending to additional languages and domains involves fine-tuning large pre-trained models using small amounts of high-quality translation data."}, "2109.12028": {"original_text": "  Human knowledge is collectively encoded in the roughly 6500 languages spoken\naround the world, but it is not distributed equally across languages. Hence,\nfor information-seeking question answering (QA) systems to adequately serve\nspeakers of all languages, they need to operate cross-lingually. In this work\nwe investigate the capabilities of multilingually pre-trained language models\non cross-lingual QA. We find that explicitly aligning the representations\nacross languages with a post-hoc fine-tuning step generally leads to improved\nperformance. We additionally investigate the effect of data size as well as the\nlanguage choice in this fine-tuning step, also releasing a dataset for\nevaluating cross-lingual QA systems. Code and dataset are publicly available\nhere: https://github.com/ffaisal93/aligned_qa\n", "rewritten_text": "The knowledge of humanity is stored in approximately 6500 languages spoken worldwide, but it is unevenly distributed among them. To ensure that information-seeking question answering systems can effectively cater to speakers of all languages, they must be able to operate across different languages. In this study, we explore the effectiveness of multilingual pre-trained language models in cross-lingual question answering. Our findings show that aligning representations across languages through post-hoc fine-tuning generally improves performance. We also examine the impact of data size and language selection in this fine-tuning process, and provide a dataset for evaluating cross-lingual question answering systems. The code and dataset can be accessed publicly at: https://github.com/ffaisal93/aligned_qa"}, "2010.00363": {"original_text": "  Long Short-Term Memory recurrent neural network (LSTM) is widely used and\nknown to capture informative long-term syntactic dependencies. However, how\nsuch information are reflected in its internal vectors for natural text has not\nyet been sufficiently investigated. We analyze them by learning a language\nmodel where syntactic structures are implicitly given. We empirically show that\nthe context update vectors, i.e. outputs of internal gates, are approximately\nquantized to binary or ternary values to help the language model to count the\ndepth of nesting accurately, as Suzgun et al. (2019) recently show for\nsynthetic Dyck languages. For some dimensions in the context vector, we show\nthat their activations are highly correlated with the depth of phrase\nstructures, such as VP and NP. Moreover, with an $L_1$ regularization, we also\nfound that it can accurately predict whether a word is inside a phrase\nstructure or not from a small number of components of the context vector. Even\nfor the case of learning from raw text, context vectors are shown to still\ncorrelate well with the phrase structures. Finally, we show that natural\nclusters of the functional words and the part of speeches that trigger phrases\nare represented in a small but principal subspace of the context-update vector\nof LSTM.\n", "rewritten_text": "The Long Short-Term Memory recurrent neural network (LSTM) is commonly used for capturing long-term syntactic dependencies in natural text. However, the way this information is reflected in the internal vectors of the LSTM has not been thoroughly explored. In this study, we investigate this by training a language model with implicit syntactic structures. Our empirical findings reveal that the context update vectors, which are the outputs of internal gates, tend to be quantized to binary or ternary values. This quantization helps the language model accurately determine the depth of nesting, as demonstrated by Suzgun et al. (2019) in synthetic Dyck languages. We also observe a strong correlation between certain dimensions of the context vector and the depth of phrase structures like VP and NP. Additionally, through $L_1$ regularization, we are able to predict whether a word is part of a phrase structure using only a few components of the context vector. Even when learning from raw text, the context vectors continue to exhibit strong correlations with phrase structures. Lastly, we find that natural groupings of functional words and parts of speech that signal phrases are represented in a key subspace of the LSTM's context-update vector."}, "2005.02877": {"original_text": "  Task-oriented dialog systems rely on dialog state tracking (DST) to monitor\nthe user's goal during the course of an interaction. Multi-domain and\nopen-vocabulary settings complicate the task considerably and demand scalable\nsolutions. In this paper we present a new approach to DST which makes use of\nvarious copy mechanisms to fill slots with values. Our model has no need to\nmaintain a list of candidate values. Instead, all values are extracted from the\ndialog context on-the-fly. A slot is filled by one of three copy mechanisms:\n(1) Span prediction may extract values directly from the user input; (2) a\nvalue may be copied from a system inform memory that keeps track of the\nsystem's inform operations; (3) a value may be copied over from a different\nslot that is already contained in the dialog state to resolve coreferences\nwithin and across domains. Our approach combines the advantages of span-based\nslot filling methods with memory methods to avoid the use of value picklists\naltogether. We argue that our strategy simplifies the DST task while at the\nsame time achieving state of the art performance on various popular evaluation\nsets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55%.\n", "rewritten_text": "Task-oriented dialogue systems rely on dialog state tracking (DST) to monitor the user's goal throughout an interaction. Dealing with multi-domain and open-vocabulary scenarios adds complexity to the task and requires scalable solutions. This paper introduces a novel DST approach that utilizes different copy mechanisms to populate slots with values. Our model eliminates the need for maintaining a list of candidate values by dynamically extracting all values from the dialog context. A slot can be filled using one of three copy mechanisms: (1) Span prediction extracts values directly from user input; (2) a value can be copied from a system inform memory that records the system's inform actions; (3) a value can be copied from a different slot already present in the dialog state to handle coreferences within and across domains. Our approach combines the benefits of span-based slot filling techniques with memory methods to eliminate the use of value picklists. We argue that this strategy simplifies the DST task while achieving top-notch performance on various popular evaluation datasets, including Multiwoz 2.1, where we achieve a joint goal accuracy exceeding 55%."}, "2408.06899": {"original_text": "  We present a novel method for measuring the rate of periodic phenomena (e.g.,\nrotation, flicker, and vibration), by an event camera, a device asynchronously\nreporting brightness changes at independently operating pixels with high\ntemporal resolution. The approach assumes that for a periodic phenomenon, a\nhighly similar set of events is generated within a spatio-temporal window at a\ntime difference corresponding to its period. The sets of similar events are\ndetected by a correlation in the spatio-temporal event stream space. The\nproposed method, EEPPR, is evaluated on a dataset of 12 sequences of periodic\nphenomena, i.e. flashing light and vibration, and periodic motion, e.g.,\nrotation, ranging from 3.2 Hz to 2 kHz (equivalent to 192 - 120 000 RPM). EEPPR\nsignificantly outperforms published methods on this dataset, achieving a mean\nrelative error of 0.1%, setting new state-of-the-art. The dataset and codes are\npublicly available on GitHub.\n", "rewritten_text": "We introduce a new technique for measuring the frequency of cyclic events (such as rotation, flicker, and vibration) using an event camera. This camera captures changes in brightness at individual pixels with high time resolution. Our method relies on the assumption that for a cyclic event, a similar pattern of events occurs within a specific time frame corresponding to its frequency. These similar event patterns are identified through correlation analysis in the event stream. Our method, EEPPR, is tested on a dataset containing 12 sequences of cyclic events, including flashing lights, vibrations, and rotational motion ranging from 3.2 Hz to 2 kHz (equivalent to 192 - 120,000 RPM). EEPPR outperforms existing methods on this dataset, achieving a mean relative error of 0.1% and setting a new standard. The dataset and codes are publicly accessible on GitHub."}, "1703.08448": {"original_text": "  We investigate a principle way to progressively mine discriminative object\nregions using classification networks to address the weakly-supervised semantic\nsegmentation problems. Classification networks are only responsive to small and\nsparse discriminative regions from the object of interest, which deviates from\nthe requirement of the segmentation task that needs to localize dense, interior\nand integral regions for pixel-wise inference. To mitigate this gap, we propose\na new adversarial erasing approach for localizing and expanding object regions\nprogressively. Starting with a single small object region, our proposed\napproach drives the classification network to sequentially discover new and\ncomplement object regions by erasing the current mined regions in an\nadversarial manner. These localized regions eventually constitute a dense and\ncomplete object region for learning semantic segmentation. To further enhance\nthe quality of the discovered regions by adversarial erasing, an online\nprohibitive segmentation learning approach is developed to collaborate with\nadversarial erasing by providing auxiliary segmentation supervision modulated\nby the more reliable classification scores. Despite its apparent simplicity,\nthe proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union\n(mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new\nstate-of-the-arts.\n", "rewritten_text": "We explore a novel method for gradually identifying discriminative object regions using classification networks to tackle the challenges of weakly-supervised semantic segmentation. While classification networks typically focus on small and sparse discriminative regions within the object of interest, semantic segmentation requires dense, interior, and complete regions for accurate pixel-wise inference. To bridge this gap, we introduce an innovative adversarial erasing technique to progressively localize and expand object regions. Starting with a small initial region, our method guides the classification network to uncover new object regions by iteratively erasing previously identified regions in an adversarial manner. These localized regions eventually form a comprehensive object region suitable for semantic segmentation training. Additionally, we develop an online prohibitive segmentation learning strategy to enhance the quality of the identified regions through collaboration with adversarial erasing, leveraging more reliable classification scores for auxiliary segmentation supervision. Despite its simplicity, our proposed approach achieves state-of-the-art mean Intersection-over-Union (mIoU) scores of 55.0% and 55.7% on the PASCAL VOC 2012 validation and test datasets."}, "1708.00284": {"original_text": "  Future frame prediction in videos is a promising avenue for unsupervised\nvideo representation learning. Video frames are naturally generated by the\ninherent pixel flows from preceding frames based on the appearance and motion\ndynamics in the video. However, existing methods focus on directly\nhallucinating pixel values, resulting in blurry predictions. In this paper, we\ndevelop a dual motion Generative Adversarial Net (GAN) architecture, which\nlearns to explicitly enforce future-frame predictions to be consistent with the\npixel-wise flows in the video through a dual-learning mechanism. The primal\nfuture-frame prediction and dual future-flow prediction form a closed loop,\ngenerating informative feedback signals to each other for better video\nprediction. To make both synthesized future frames and flows indistinguishable\nfrom reality, a dual adversarial training method is proposed to ensure that the\nfuture-flow prediction is able to help infer realistic future-frames, while the\nfuture-frame prediction in turn leads to realistic optical flows. Our dual\nmotion GAN also handles natural motion uncertainty in different pixel locations\nwith a new probabilistic motion encoder, which is based on variational\nautoencoders. Extensive experiments demonstrate that the proposed dual motion\nGAN significantly outperforms state-of-the-art approaches on synthesizing new\nvideo frames and predicting future flows. Our model generalizes well across\ndiverse visual scenes and shows superiority in unsupervised video\nrepresentation learning.\n", "rewritten_text": "Predicting future frames in videos is a promising approach for unsupervised video representation learning. Video frames are naturally created by the movement of pixels from previous frames, based on the visual appearance and motion dynamics in the video. However, current methods mainly focus on generating pixel values directly, leading to blurry predictions. In this study, we introduce a dual motion Generative Adversarial Net (GAN) architecture that aims to ensure future-frame predictions align with pixel flows in the video through a dual-learning mechanism. The primary future-frame prediction and secondary future-flow prediction work together in a closed loop, providing feedback to improve video prediction. To enhance the realism of both synthesized future frames and flows, we propose a dual adversarial training method. This method ensures that future-flow prediction aids in generating realistic future-frames, while future-frame prediction contributes to realistic optical flows. Our dual motion GAN also addresses motion uncertainty in different pixel locations using a new probabilistic motion encoder based on variational autoencoders. Extensive experiments demonstrate that our dual motion GAN surpasses current approaches in generating new video frames and predicting future flows. Our model exhibits strong performance across various visual scenes and excels in unsupervised video representation learning."}, "1405.6103": {"original_text": "  The Swiss avalanche bulletin is produced twice a day in four languages. Due\nto the lack of time available for manual translation, a fully automated\ntranslation system is employed, based on a catalogue of predefined phrases and\npredetermined rules of how these phrases can be combined to produce sentences.\nThe system is able to automatically translate such sentences from German into\nthe target languages French, Italian and English without subsequent\nproofreading or correction. Our catalogue of phrases is limited to a small\nsublanguage. The reduction of daily translation costs is expected to offset the\ninitial development costs within a few years. After being operational for two\nwinter seasons, we assess here the quality of the produced texts based on an\nevaluation where participants rate real danger descriptions from both origins,\nthe catalogue of phrases versus the manually written and translated texts. With\na mean recognition rate of 55%, users can hardly distinguish between the two\ntypes of texts, and give similar ratings with respect to their language\nquality. Overall, the output from the catalogue system can be considered\nvirtually equivalent to a text written by avalanche forecasters and then\nmanually translated by professional translators. Furthermore, forecasters\ndeclared that all relevant situations were captured by the system with\nsufficient accuracy and within the limited time available.\n", "rewritten_text": "The Swiss avalanche bulletin is generated twice daily in four languages using a fully automated translation system. This system relies on a set of predefined phrases and rules for combining them to create sentences, allowing for automatic translation from German to French, Italian, and English without the need for subsequent proofreading or correction. The phrase catalogue is limited to a specific sublanguage, with the aim of reducing daily translation costs to offset the initial development expenses within a few years. After two winter seasons of operation, an evaluation was conducted to compare the quality of texts produced by the automated system with manually written and translated texts. Participants rated real danger descriptions from both sources, revealing a mean recognition rate of 55% and similar ratings for language quality. The output from the automated system was found to be nearly indistinguishable from texts written by avalanche forecasters and manually translated by professionals. Forecasters confirmed that the system accurately captured all relevant situations within the limited time available."}, "2408.15063": {"original_text": "  Although most existing multi-modal salient object detection (SOD) methods\ndemonstrate effectiveness through training models from scratch, the limited\nmulti-modal data hinders these methods from reaching optimality. In this paper,\nwe propose a novel framework to explore and exploit the powerful feature\nrepresentation and zero-shot generalization ability of the pre-trained Segment\nAnything Model (SAM) for multi-modal SOD. Despite serving as a recent vision\nfundamental model, driving the class-agnostic SAM to comprehend and detect\nsalient objects accurately is non-trivial, especially in challenging scenes. To\nthis end, we develop \\underline{SAM} with se\\underline{m}antic\nf\\underline{e}ature fu\\underline{s}ion guidanc\\underline{e} (Sammese), which\nincorporates multi-modal saliency-specific knowledge into SAM to adapt SAM to\nmulti-modal SOD tasks. However, it is difficult for SAM trained on single-modal\ndata to directly mine the complementary benefits of multi-modal inputs and\ncomprehensively utilize them to achieve accurate saliency prediction. To\naddress these issues, we first design a multi-modal complementary fusion module\nto extract robust multi-modal semantic features by integrating information from\nvisible and thermal or depth image pairs. Then, we feed the extracted\nmulti-modal semantic features into both the SAM image encoder and mask decoder\nfor fine-tuning and prompting, respectively. Specifically, in the image\nencoder, a multi-modal adapter is proposed to adapt the single-modal SAM to\nmulti-modal information. In the mask decoder, a semantic-geometric prompt\ngeneration strategy is proposed to produce corresponding embeddings with\nvarious saliency cues. Extensive experiments on both RGB-D and RGB-T SOD\nbenchmarks show the effectiveness of the proposed framework. The code will be\navailable at \\url{https://github.com/Angknpng/Sammese}.\n", "rewritten_text": "\"While many current methods for multi-modal salient object detection (SOD) show effectiveness by training models from scratch, the limited availability of multi-modal data hinders their optimization. This paper introduces a new approach that leverages the feature representation and zero-shot generalization capabilities of the pre-trained Segment Anything Model (SAM) for multi-modal SOD. Adapting SAM for accurate detection of salient objects in challenging scenes is a complex task. To address this challenge, we introduce SAM with semantic feature fusion guidance (Sammese) to incorporate saliency-specific knowledge and enhance SAM's performance in multi-modal SOD tasks. We develop a multi-modal complementary fusion module to extract robust semantic features from visible and thermal or depth image pairs, which are then utilized for fine-tuning and prompting in the SAM image encoder and mask decoder, respectively. Our framework includes a multi-modal adapter in the image encoder to handle multi-modal information and a semantic-geometric prompt generation strategy in the mask decoder to generate embeddings with saliency cues. Experimental results on RGB-D and RGB-T SOD benchmarks demonstrate the effectiveness of our approach. The code for this framework will be available at \\url{https://github.com/Angknpng/Sammese}.\""}, "2210.09563": {"original_text": "  With the continuous development of deep learning in the field of image\ngeneration models, a large number of vivid forged faces have been generated and\nspread on the Internet. These high-authenticity artifacts could grow into a\nthreat to society security. Existing face forgery detection methods directly\nutilize the obtained public shared or centralized data for training but ignore\nthe personal privacy and security issues when personal data couldn't be\ncentralizedly shared in real-world scenarios. Additionally, different\ndistributions caused by diverse artifact types would further bring adverse\ninfluences on the forgery detection task. To solve the mentioned problems, the\npaper proposes a novel generalized residual Federated learning for face Forgery\ndetection (FedForgery). The designed variational autoencoder aims to learn\nrobust discriminative residual feature maps to detect forgery faces (with\ndiverse or even unknown artifact types). Furthermore, the general federated\nlearning strategy is introduced to construct distributed detection model\ntrained collaboratively with multiple local decentralized devices, which could\nfurther boost the representation generalization. Experiments conducted on\npublicly available face forgery detection datasets prove the superior\nperformance of the proposed FedForgery. The designed novel generalized face\nforgery detection protocols and source code would be publicly available.\n", "rewritten_text": "The continuous advancement of deep learning in image generation models has led to the proliferation of realistic fake faces on the Internet, posing a potential threat to societal security. Current face forgery detection methods rely on public or centralized data for training, overlooking personal privacy and security concerns in real-world scenarios where data cannot be easily shared. Moreover, variations in artifact types can complicate forgery detection. To address these challenges, this study introduces a novel approach called Federated Forgery Detection (FedForgery). By utilizing a variational autoencoder to learn robust residual features for detecting forged faces with diverse or unknown artifact types, and implementing a federated learning strategy to collaboratively train a distributed detection model across multiple decentralized devices, the proposed FedForgery demonstrates superior performance in experiments using publicly available datasets. The developed face forgery detection protocols and source code will be made publicly accessible."}, "2304.05995": {"original_text": "  In recent years, the success of large-scale vision-language models (VLMs)\nsuch as CLIP has led to their increased usage in various computer vision tasks.\nThese models enable zero-shot inference through carefully crafted instructional\ntext prompts without task-specific supervision. However, the potential of VLMs\nfor generalization tasks in remote sensing (RS) has not been fully realized. To\naddress this research gap, we propose a novel image-conditioned prompt learning\nstrategy called the Visual Attention Parameterized Prompts Learning Network\n(APPLeNet). APPLeNet emphasizes the importance of multi-scale feature learning\nin RS scene classification and disentangles visual style and content primitives\nfor domain generalization tasks. To achieve this, APPLeNet combines visual\ncontent features obtained from different layers of the vision encoder and style\nproperties obtained from feature statistics of domain-specific batches. An\nattention-driven injection module is further introduced to generate visual\ntokens from this information. We also introduce an anti-correlation regularizer\nto ensure discrimination among the token embeddings, as this visual information\nis combined with the textual tokens. To validate APPLeNet, we curated four\navailable RS benchmarks and introduced experimental protocols and datasets for\nthree domain generalization tasks. Our results consistently outperform the\nrelevant literature and code is available at\nhttps://github.com/mainaksingha01/APPLeNet\n", "rewritten_text": "In recent years, the increased utilization of large-scale vision-language models (VLMs) like CLIP has resulted in their growing application across various computer vision tasks. These models allow for zero-shot inference using carefully constructed text prompts, eliminating the need for task-specific supervision. Despite this success, the potential of VLMs in remote sensing (RS) for generalization tasks remains untapped. To bridge this research gap, we introduce a new approach called the Visual Attention Parameterized Prompts Learning Network (APPLeNet). APPLeNet focuses on multi-scale feature learning in RS scene classification and separates visual style and content elements for domain generalization tasks. By combining visual content features from different encoder layers and style characteristics from domain-specific batch statistics, APPLeNet generates visual tokens using an attention-driven injection module. Additionally, an anti-correlation regularizer is introduced to ensure distinction among token embeddings when merging visual and textual information. To validate APPLeNet, we curated four RS benchmarks, established experimental protocols, and provided datasets for three domain generalization tasks. Our results consistently outperform existing literature, and the code is accessible at https://github.com/mainaksingha01/APPLeNet."}, "1804.03287": {"original_text": "  Despite the noticeable progress in perceptual tasks like detection, instance\nsegmentation and human parsing, computers still perform unsatisfactorily on\nvisually understanding humans in crowded scenes, such as group behavior\nanalysis, person re-identification and autonomous driving, etc. To this end,\nmodels need to comprehensively perceive the semantic information and the\ndifferences between instances in a multi-human image, which is recently defined\nas the multi-human parsing task. In this paper, we present a new large-scale\ndatabase \"Multi-Human Parsing (MHP)\" for algorithm development and evaluation,\nand advances the state-of-the-art in understanding humans in crowded scenes.\nMHP contains 25,403 elaborately annotated images with 58 fine-grained semantic\ncategory labels, involving 2-26 persons per image and captured in real-world\nscenes from various viewpoints, poses, occlusion, interactions and background.\nWe further propose a novel deep Nested Adversarial Network (NAN) model for\nmulti-human parsing. NAN consists of three Generative Adversarial Network\n(GAN)-like sub-nets, respectively performing semantic saliency prediction,\ninstance-agnostic parsing and instance-aware clustering. These sub-nets form a\nnested structure and are carefully designed to learn jointly in an end-to-end\nway. NAN consistently outperforms existing state-of-the-art solutions on our\nMHP and several other datasets, and serves as a strong baseline to drive the\nfuture research for multi-human parsing.\n", "rewritten_text": "Despite the significant advancements in tasks such as detection, instance segmentation, and human parsing, computers still struggle to effectively understand humans in crowded environments, such as analyzing group behavior, person re-identification, and autonomous driving. In order to address this challenge, models must be able to accurately interpret semantic information and distinguish between individuals in images containing multiple humans, a task referred to as multi-human parsing. This paper introduces a new extensive dataset called \"Multi-Human Parsing (MHP)\" to facilitate algorithm development and evaluation, pushing the boundaries of human understanding in crowded scenes. The MHP dataset comprises 25,403 meticulously annotated images with 58 detailed semantic category labels, featuring 2-26 individuals per image captured in real-world scenarios with varying viewpoints, poses, occlusions, interactions, and backgrounds. Additionally, a novel deep Nested Adversarial Network (NAN) model is proposed for multi-human parsing, which includes three Generative Adversarial Network (GAN)-like sub-networks for semantic saliency prediction, instance-agnostic parsing, and instance-aware clustering. These sub-networks are intricately designed to learn collaboratively in an end-to-end manner within a nested structure. The NAN model consistently outperforms existing state-of-the-art solutions on the MHP dataset and other datasets, establishing a strong foundation for future research in multi-human parsing."}, "2206.10779": {"original_text": "  We propose a large-scale dataset of real-world rainy and clean image pairs\nand a method to remove degradations, induced by rain streaks and rain\naccumulation, from the image. As there exists no real-world dataset for\nderaining, current state-of-the-art methods rely on synthetic data and thus are\nlimited by the sim2real domain gap; moreover, rigorous evaluation remains a\nchallenge due to the absence of a real paired dataset. We fill this gap by\ncollecting a real paired deraining dataset through meticulous control of\nnon-rain variations. Our dataset enables paired training and quantitative\nevaluation for diverse real-world rain phenomena (e.g. rain streaks and rain\naccumulation). To learn a representation robust to rain phenomena, we propose a\ndeep neural network that reconstructs the underlying scene by minimizing a\nrain-robust loss between rainy and clean images. Extensive experiments\ndemonstrate that our model outperforms the state-of-the-art deraining methods\non real rainy images under various conditions. Project website:\nhttps://visual.ee.ucla.edu/gt_rain.htm/.\n", "rewritten_text": "We present a comprehensive dataset containing pairs of real-world images captured in rainy and clean conditions, along with a technique to eliminate distortions caused by rain streaks and accumulation in the images. Current deraining methods rely on synthetic data due to the lack of a real-world dataset, leading to limitations in bridging the gap between simulation and reality. Additionally, the absence of a paired real dataset poses challenges in evaluating these methods effectively. To address these issues, we have meticulously curated a real paired deraining dataset that controls non-rain variations. This dataset facilitates paired training and quantitative assessment of various rain phenomena, such as rain streaks and accumulation, in real-world scenarios. Our approach involves a deep neural network that reconstructs the scene by minimizing a rain-robust loss between rainy and clean images to learn a robust representation against rain effects. Through extensive experiments, we demonstrate that our model surpasses existing deraining methods when applied to real rainy images in diverse conditions. For more information, please visit our project website at https://visual.ee.ucla.edu/gt_rain.htm/."}, "1812.08115": {"original_text": "  We introduce a model-based deep learning architecture termed MoDL-MUSSELS for\nthe correction of phase errors in multishot diffusion-weighted echo-planar MRI\nimages. The proposed algorithm is a generalization of existing MUSSELS\nalgorithm with similar performance but with significantly reduced computational\ncomplexity. In this work, we show that an iterative re-weighted least-squares\nimplementation of MUSSELS alternates between a multichannel filter bank and the\nenforcement of data consistency. The multichannel filter bank projects the data\nto the signal subspace thus exploiting the phase relations between shots. Due\nto the high computational complexity of self-learned filter bank, we propose to\nreplace it with a convolutional neural network (CNN) whose parameters are\nlearned from exemplary data. The proposed CNN is a hybrid model involving a\nmultichannel CNN in the k-space and another CNN in the image space. The k-space\nCNN exploits the phase relations between the shot images, while the image\ndomain network is used to project the data to an image manifold. The\nexperiments show that the proposed scheme can yield reconstructions that are\ncomparable to state of the art methods while offering several orders of\nmagnitude reduction in run-time.\n", "rewritten_text": "We present a model-based deep learning framework called MoDL-MUSSELS for addressing phase errors in multishot diffusion-weighted echo-planar MRI images. Our algorithm is an extension of the existing MUSSELS method, delivering similar performance with significantly lower computational complexity. Our approach involves an iterative re-weighted least-squares implementation of MUSSELS, which alternates between a multichannel filter bank and enforcing data consistency. The multichannel filter bank projects the data to the signal subspace to leverage phase relationships between shots. To reduce computational complexity, we replace the self-learned filter bank with a convolutional neural network (CNN) trained on sample data. Our CNN model combines a multichannel CNN in the k-space and another CNN in the image space. The k-space CNN exploits phase relationships between shot images, while the image domain network projects data onto an image manifold. Experimental results demonstrate that our proposed approach can achieve reconstructions comparable to state-of-the-art methods while significantly reducing runtime."}, "1912.10644": {"original_text": "  In spite of the recent progresses on classifying 3D point cloud with deep\nCNNs, large geometric transformations like rotation and translation remain\nchallenging problem and harm the final classification performance. To address\nthis challenge, we propose Geometry Sharing Network (GS-Net) which effectively\nlearns point descriptors with holistic context to enhance the robustness to\ngeometric transformations. Compared with previous 3D point CNNs which perform\nconvolution on nearby points, GS-Net can aggregate point features in a more\nglobal way. Specially, GS-Net consists of Geometry Similarity Connection (GSC)\nmodules which exploit Eigen-Graph to group distant points with similar and\nrelevant geometric information, and aggregate features from nearest neighbors\nin both Euclidean space and Eigenvalue space. This design allows GS-Net to\nefficiently capture both local and holistic geometric features such as\nsymmetry, curvature, convexity and connectivity. Theoretically, we show the\nnearest neighbors of each point in Eigenvalue space are invariant to rotation\nand translation. We conduct extensive experiments on public datasets,\nModelNet40, ShapeNet Part. Experiments demonstrate that GS-Net achieves the\nstate-of-the-art performances on major datasets, 93.3% on ModelNet40, and are\nmore robust to geometric transformations.\n", "rewritten_text": "Despite the recent advancements in using deep CNNs to classify 3D point clouds, significant geometric transformations such as rotation and translation continue to pose challenges and impact classification accuracy. To tackle this issue, we introduce the Geometry Sharing Network (GS-Net), which effectively learns point descriptors with a comprehensive context to improve resilience to geometric transformations. Unlike previous 3D point CNNs that focus on convolution among nearby points, GS-Net can aggregate point features on a more global scale. Specifically, GS-Net incorporates Geometry Similarity Connection (GSC) modules that utilize Eigen-Graph to group distant points sharing similar geometric information, and combine features from nearest neighbors in both Euclidean and Eigenvalue spaces. This approach enables GS-Net to capture local and holistic geometric features such as symmetry, curvature, convexity, and connectivity efficiently. We demonstrate theoretically that the nearest neighbors of each point in Eigenvalue space are invariant to rotation and translation. Through extensive experiments on public datasets like ModelNet40 and ShapeNet Part, we show that GS-Net achieves state-of-the-art performance, with a 93.3% accuracy on ModelNet40, and demonstrates increased robustness to geometric transformations."}, "2111.01515": {"original_text": "  The enormous amount of data being generated on the web and social media has\nincreased the demand for detecting online hate speech. Detecting hate speech\nwill reduce their negative impact and influence on others. A lot of effort in\nthe Natural Language Processing (NLP) domain aimed to detect hate speech in\ngeneral or detect specific hate speech such as religion, race, gender, or\nsexual orientation. Hate communities tend to use abbreviations, intentional\nspelling mistakes, and coded words in their communication to evade detection,\nadding more challenges to hate speech detection tasks. Thus, word\nrepresentation will play an increasingly pivotal role in detecting hate speech.\nThis paper investigates the feasibility of leveraging domain-specific word\nembedding in Bidirectional LSTM based deep model to automatically\ndetect/classify hate speech. Furthermore, we investigate the use of the\ntransfer learning language model (BERT) on hate speech problem as a binary\nclassification task. The experiments showed that domainspecific word embedding\nwith the Bidirectional LSTM based deep model achieved a 93% f1-score while BERT\nachieved up to 96% f1-score on a combined balanced dataset from available hate\nspeech datasets.\n", "rewritten_text": "The rise in data volume from the internet and social media has led to an increased need for identifying online hate speech. Detecting and addressing hate speech can help minimize its harmful effects on individuals. Efforts in the field of Natural Language Processing (NLP) have focused on detecting hate speech in general, as well as specific types related to religion, race, gender, and sexual orientation. Hate groups often use tactics like abbreviations, intentional misspellings, and coded language to avoid detection, posing additional challenges for identifying hate speech. Therefore, the use of word representation is crucial for effective hate speech detection. This study explores the potential of utilizing domain-specific word embeddings in a Bidirectional LSTM deep learning model for automated hate speech detection and classification. Additionally, the study examines the application of transfer learning with the BERT language model for hate speech detection as a binary classification task. Results from experiments indicate that the domain-specific word embeddings combined with the Bidirectional LSTM model achieved a 93% f1-score, while BERT achieved a maximum of 96% f1-score on a balanced dataset compiled from various hate speech datasets."}, "2409.17432": {"original_text": "  Reducing the atmospheric haze and enhancing image clarity is crucial for\ncomputer vision applications. The lack of real-life hazy ground truth images\nnecessitates synthetic datasets, which often lack diverse haze types, impeding\neffective haze type classification and dehazing algorithm selection. This\nresearch introduces the HazeSpace2M dataset, a collection of over 2 million\nimages designed to enhance dehazing through haze type classification.\nHazeSpace2M includes diverse scenes with 10 haze intensity levels, featuring\nFog, Cloud, and Environmental Haze (EH). Using the dataset, we introduce a\ntechnique of haze type classification followed by specialized dehazers to clear\nhazy images. Unlike conventional methods, our approach classifies haze types\nbefore applying type-specific dehazing, improving clarity in real-life hazy\nimages. Benchmarking with state-of-the-art (SOTA) models, ResNet50 and AlexNet\nachieve 92.75\\% and 92.50\\% accuracy, respectively, against existing synthetic\ndatasets. However, these models achieve only 80% and 70% accuracy,\nrespectively, against our Real Hazy Testset (RHT), highlighting the challenging\nnature of our HazeSpace2M dataset. Additional experiments show that haze type\nclassification followed by specialized dehazing improves results by 2.41% in\nPSNR, 17.14% in SSIM, and 10.2\\% in MSE over general dehazers. Moreover, when\ntesting with SOTA dehazing models, we found that applying our proposed\nframework significantly improves their performance. These results underscore\nthe significance of HazeSpace2M and our proposed framework in addressing\natmospheric haze in multimedia processing. Complete code and dataset is\navailable on \\href{https://github.com/tanvirnwu/HazeSpace2M}\n{\\textcolor{blue}{\\textbf{GitHub}}}.\n", "rewritten_text": "Enhancing image clarity and reducing atmospheric haze are essential for computer vision applications. Due to the lack of real-life hazy images, synthetic datasets are often used, but they may not cover diverse haze types, making haze type classification and dehazing algorithm selection challenging. This study presents the HazeSpace2M dataset, containing over 2 million images aimed at improving dehazing through haze type classification. HazeSpace2M includes various scenes with 10 levels of haze intensity, such as Fog, Cloud, and Environmental Haze (EH). By classifying haze types and applying specialized dehazing techniques to clear hazy images, our approach differs from traditional methods, resulting in enhanced clarity in real-life hazy images. When compared to existing synthetic datasets, state-of-the-art models like ResNet50 and AlexNet achieve high accuracy levels of 92.75% and 92.50%, respectively. However, when tested against our Real Hazy Testset (RHT), these models only achieve 80% and 70% accuracy, demonstrating the difficulty of our HazeSpace2M dataset. Further experiments show that our approach improves results by 2.41% in PSNR, 17.14% in SSIM, and 10.2% in MSE over general dehazing methods. Additionally, applying our framework to state-of-the-art dehazing models significantly enhances their performance. These findings emphasize the importance of HazeSpace2M and our proposed framework in addressing atmospheric haze in multimedia processing. The complete code and dataset are available on GitHub at https://github.com/tanvirnwu/HazeSpace2M."}, "1808.045": {"original_text": "  Medical images with specific pathologies are scarce, but a large amount of\ndata is usually required for a deep convolutional neural network (DCNN) to\nachieve good accuracy. We consider the problem of segmenting the left\nventricular (LV) myocardium on late gadolinium enhancement (LGE) cardiovascular\nmagnetic resonance (CMR) scans of which only some of the scans have scar\ntissue. We propose ScarGAN to simulate scar tissue on healthy myocardium using\nchained generative adversarial networks (GAN). Our novel approach factorizes\nthe simulation process into 3 steps: 1) a mask generator to simulate the shape\nof the scar tissue; 2) a domain-specific heuristic to produce the initial\nsimulated scar tissue from the simulated shape; 3) a refining generator to add\ndetails to the simulated scar tissue. Unlike other approaches that generate\nsamples from scratch, we simulate scar tissue on normal scans resulting in\nhighly realistic samples. We show that experienced radiologists are unable to\ndistinguish between real and simulated scar tissue. Training a U-Net with\nadditional scans with scar tissue simulated by ScarGAN increases the percentage\nof scar pixels correctly included in LV myocardium prediction from 75.9% to\n80.5%.\n", "rewritten_text": "\"Obtaining medical images featuring specific pathologies can be challenging due to their scarcity, yet a significant amount of data is typically necessary for a deep convolutional neural network (DCNN) to achieve high accuracy. In this study, we address the task of segmenting the left ventricular (LV) myocardium on late gadolinium enhancement (LGE) cardiovascular magnetic resonance (CMR) scans, where only a subset of scans exhibit scar tissue. To tackle this issue, we introduce ScarGAN, a novel method that leverages chained generative adversarial networks (GAN) to simulate scar tissue on healthy myocardium. Our innovative approach breaks down the simulation process into three key steps: 1) generating a mask to mimic the shape of the scar tissue, 2) employing a domain-specific heuristic to create the initial simulated scar tissue based on the generated shape, and 3) utilizing a refining generator to enhance the details of the simulated scar tissue. Unlike traditional methods that generate samples from scratch, our technique simulates scar tissue on normal scans, resulting in highly realistic samples. Our experiments demonstrate that even experienced radiologists struggle to differentiate between real and simulated scar tissue. By training a U-Net model with additional scans featuring scar tissue simulated by ScarGAN, we observe an improvement in the accuracy of predicting scar pixels within the LV myocardium, increasing from 75.9% to 80.5%.\""}, "2409.00045": {"original_text": "  Colonoscopy is the primary method for examination, detection, and removal of\npolyps. Regular screening helps detect and prevent colorectal cancer at an\nearly curable stage. However, challenges such as variation among the\nendoscopists' skills, bowel quality preparation, and complex nature of the\nlarge intestine which cause large number of polyp miss-rate. These missed\npolyps can develop into cancer later on, which underscores the importance of\nimproving the detection methods. A computer-aided diagnosis system can support\nphysicians by assisting in detecting overlooked polyps. However, one of the\nimportant challenges for developing novel deep learning models for automatic\npolyp detection and segmentation is the lack of publicly available,\nmulti-center large and diverse datasets. To address this gap, we introduce\nPolypDB, a large scale publicly available dataset that contains 3934 still\npolyp images and their corresponding ground truth from real colonoscopy videos\nto design efficient polyp detection and segmentation architectures. The dataset\nhas been developed and verified by a team of 10 gastroenterologists. PolypDB\ncomprises of images from five modalities: Blue Light Imaging (BLI), Flexible\nImaging Color Enhancement (FICE), Linked Color Imaging (LCI), Narrow Band\nImaging (NBI), and White Light Imaging (WLI) and three medical centers from\nNorway, Sweden and Vietnam. Thus, we split the dataset based on modality and\nmedical center for modality-wise and center-wise analysis. We provide a\nbenchmark on each modality using eight popular segmentation methods and six\nstandard benchmark polyp detection methods. Furthermore, we also provide\nbenchmark on center-wise under federated learning settings. Our dataset is\npublic and can be downloaded at \\url{https://osf.io/pr7ms/}.\n", "rewritten_text": "Colonoscopy is the main method used for examining, detecting, and removing polyps. Regular screening is crucial for early detection and prevention of colorectal cancer. However, challenges such as variations in endoscopists' skills, bowel preparation quality, and the complexity of the large intestine can lead to a high rate of missed polyps. These undetected polyps may develop into cancer later on, highlighting the need for improved detection methods. A computer-aided diagnosis system can assist physicians in identifying overlooked polyps. One significant challenge in developing advanced deep learning models for automatic polyp detection and segmentation is the lack of publicly available, diverse datasets from multiple centers. To address this gap, we present PolypDB, a large-scale dataset containing 3934 still polyp images and corresponding ground truth data from real colonoscopy videos. This dataset was created and validated by a team of 10 gastroenterologists. PolypDB includes images from five modalities (BLI, FICE, LCI, NBI, WLI) and three medical centers in Norway, Sweden, and Vietnam. The dataset is divided based on modality and medical center for detailed analysis. We offer benchmarks for each modality using eight segmentation methods and six polyp detection methods. Additionally, we provide benchmarks for center-wise analysis under federated learning settings. The dataset is publicly available for download at \\url{https://osf.io/pr7ms/}."}, "2201.09724": {"original_text": "  The task of hot-refresh model upgrades of image retrieval systems plays an\nessential role in the industry but has never been investigated in academia\nbefore. Conventional cold-refresh model upgrades can only deploy new models\nafter the gallery is overall backfilled, taking weeks or even months for\nmassive data. In contrast, hot-refresh model upgrades deploy the new model\nimmediately and then gradually improve the retrieval accuracy by backfilling\nthe gallery on-the-fly. Compatible training has made it possible, however, the\nproblem of model regression with negative flips poses a great challenge to the\nstable improvement of user experience. We argue that it is mainly due to the\nfact that new-to-old positive query-gallery pairs may show less similarity than\nnew-to-new negative pairs. To solve the problem, we introduce a\nRegression-Alleviating Compatible Training (RACT) method to properly constrain\nthe feature compatibility while reducing negative flips. The core is to\nencourage the new-to-old positive pairs to be more similar than both the\nnew-to-old negative pairs and the new-to-new negative pairs. An efficient\nuncertainty-based backfilling strategy is further introduced to fasten accuracy\nimprovements. Extensive experiments on large-scale retrieval benchmarks (e.g.,\nGoogle Landmark) demonstrate that our RACT effectively alleviates the model\nregression for one more step towards seamless model upgrades. The code will be\navailable at https://github.com/binjiezhang/RACT_ICLR2022.\n", "rewritten_text": "The upgrading of image retrieval systems through hot-refresh model updates is crucial in industry, yet has not been explored in academia until now. Traditional cold-refresh model upgrades require backfilling the entire gallery before deploying new models, which can take weeks or even months for large datasets. In contrast, hot-refresh model upgrades immediately deploy new models and gradually enhance retrieval accuracy by backfilling the gallery on-the-fly. While compatible training has enabled this approach, the challenge of model regression with negative flips hinders stable user experience improvement. This issue arises from the lower similarity between new-to-old positive query-gallery pairs compared to new-to-new negative pairs. To address this, we propose a Regression-Alleviating Compatible Training (RACT) method that focuses on enhancing feature compatibility while reducing negative flips. The key is to ensure that new-to-old positive pairs are more similar than both new-to-old negative pairs and new-to-new negative pairs. Additionally, we introduce an efficient uncertainty-based backfilling strategy to accelerate accuracy improvements. Extensive experiments on large-scale retrieval benchmarks, such as Google Landmark, demonstrate that our RACT method effectively mitigates model regression, bringing us closer to seamless model upgrades. The code will be accessible at https://github.com/binjiezhang/RACT_ICLR2022."}, "2012.14345": {"original_text": "  Deep Learning (DL) based methods for object detection achieve remarkable\nperformance at the cost of computationally expensive training and extensive\ndata labeling. Robots embodiment can be exploited to mitigate this burden by\nacquiring automatically annotated training data via a natural interaction with\na human showing the object of interest, handheld. However, learning solely from\nthis data may introduce biases (the so-called domain shift), and prevents\nadaptation to novel tasks. While Weakly-supervised Learning (WSL) offers a\nwell-established set of techniques to cope with these problems in\ngeneral-purpose Computer Vision, its adoption in challenging robotic domains is\nstill at a preliminary stage. In this work, we target the scenario of a robot\ntrained in a teacher-learner setting to detect handheld objects. The aim is to\nimprove detection performance in different settings by letting the robot\nexplore the environment with a limited human labeling budget. We compare\nseveral techniques for WSL in detection pipelines to reduce model re-training\ncosts without compromising accuracy, proposing solutions which target the\nconsidered robotic scenario. We show that the robot can improve adaptation to\nnovel domains, either by interacting with a human teacher (Active Learning) or\nwith an autonomous supervision (Semi-supervised Learning). We integrate our\nstrategies into an on-line detection method, achieving efficient model update\ncapabilities with few labels. We experimentally benchmark our method on\nchallenging robotic object detection tasks under domain shift.\n", "rewritten_text": "Deep Learning (DL) methods for object detection have shown impressive performance, but they require computationally expensive training and extensive data labeling. To address this challenge, robots can be utilized to automatically annotate training data through natural interactions with humans, who show the objects of interest handheld. However, relying solely on this data may introduce biases and hinder adaptation to new tasks, known as domain shift. While Weakly-supervised Learning (WSL) techniques are well-established in general Computer Vision, their application in complex robotic environments is still in its early stages. This study focuses on training a robot in a teacher-learner setup to detect handheld objects, aiming to enhance detection performance across various scenarios with limited human labeling resources. By exploring different WSL techniques in detection pipelines, we aim to reduce model re-training costs while maintaining accuracy, specifically tailored to robotic applications. Our approach enables the robot to better adapt to new environments through interactions with a human teacher (Active Learning) or autonomous supervision (Semi-supervised Learning). These strategies are integrated into an online detection method, allowing for efficient model updates with minimal labeling. Experimental evaluations demonstrate the effectiveness of our method in challenging robotic object detection tasks affected by domain shift."}, "cs/0310014": {"original_text": "  Spoken Language can be used to provide insights into organisational\nprocesses, unfortunately transcription and coding stages are very time\nconsuming and expensive. The concept of partial transcription and coding is\nproposed in which spoken language is indexed prior to any subsequent\nprocessing. The functional linguistic theory of texture is used to describe the\neffects of partial transcription on observational records. The standard used to\nencode transcript context and metadata is called CHAT, but a previous XML\nschema developed to implement it contains design assumptions that make it\ndifficult to support partial transcription for example. This paper describes a\nmore effective XML schema that overcomes many of these problems and is intended\nfor use in applications that support the rapid development of spoken language\ndeliverables.\n", "rewritten_text": "\"Utilizing spoken language for insights into organizational processes can be valuable, but the transcription and coding stages are often time-consuming and costly. To address this issue, the concept of partial transcription and coding is suggested, where spoken language is indexed before further processing. The functional linguistic theory of texture is employed to explain the impact of partial transcription on observational records. The CHAT standard is used for encoding transcript context and metadata, but an existing XML schema designed for its implementation presents challenges in supporting partial transcription. This paper introduces a more efficient XML schema that resolves many of these issues, intended for applications that facilitate the swift development of spoken language deliverables.\""}, "2007.12099": {"original_text": "  Object detection is one of the most important areas in computer vision, which\nplays a key role in various practical scenarios. Due to limitation of hardware,\nit is often necessary to sacrifice accuracy to ensure the infer speed of the\ndetector in practice. Therefore, the balance between effectiveness and\nefficiency of object detector must be considered. The goal of this paper is to\nimplement an object detector with relatively balanced effectiveness and\nefficiency that can be directly applied in actual application scenarios, rather\nthan propose a novel detection model. Considering that YOLOv3 has been widely\nused in practice, we develop a new object detector based on YOLOv3. We mainly\ntry to combine various existing tricks that almost not increase the number of\nmodel parameters and FLOPs, to achieve the goal of improving the accuracy of\ndetector as much as possible while ensuring that the speed is almost unchanged.\nSince all experiments in this paper are conducted based on PaddlePaddle, we\ncall it PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a better\nbalance between effectiveness (45.2% mAP) and efficiency (72.9 FPS), surpassing\nthe existing state-of-the-art detectors such as EfficientDet and YOLOv4.Source\ncode is at https://github.com/PaddlePaddle/PaddleDetection.\n", "rewritten_text": "\"Object detection holds significant importance in computer vision, playing a crucial role in practical applications. Due to hardware limitations, there is often a need to compromise on accuracy to ensure efficient inference speed of the detector. Therefore, it is essential to strike a balance between the effectiveness and efficiency of the object detector. This paper aims to develop an object detector that achieves a relatively balanced effectiveness and efficiency for direct application in real-world scenarios, rather than introducing a new detection model. Leveraging the widespread usage of YOLOv3, a new object detector is built based on this model. The focus is on integrating existing techniques that do not significantly increase model parameters and computational operations, aiming to enhance detector accuracy while maintaining speed. Referred to as PP-YOLO due to experiments conducted using PaddlePaddle, this detector achieves a better balance between effectiveness (45.2% mAP) and efficiency (72.9 FPS), outperforming state-of-the-art detectors like EfficientDet and YOLOv4. The source code can be found at https://github.com/PaddlePaddle/PaddleDetection.\""}, "2406.15823": {"original_text": "  Understanding the abilities of LLMs to reason about natural language plans,\nsuch as instructional text and recipes, is critical to reliably using them in\ndecision-making systems. A fundamental aspect of plans is the temporal order in\nwhich their steps needs to be executed, which reflects the underlying causal\ndependencies between them. We introduce CaT-Bench, a benchmark of Step Order\nPrediction questions, which test whether a step must necessarily occur before\nor after another in cooking recipe plans. We use this to evaluate how well\nfrontier LLMs understand causal and temporal dependencies. We find that SOTA\nLLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased\ntowards predicting dependence more often, perhaps relying on temporal order of\nsteps as a heuristic. While prompting for explanations and using few-shot\nexamples improve performance, the best F1 result is only 0.73. Further, human\nevaluation of explanations along with answer correctness show that, on average,\nhumans do not agree with model reasoning. Surprisingly, we also find that\nexplaining after answering leads to better performance than normal\nchain-of-thought prompting, and LLM answers are not consistent across questions\nabout the same step pairs. Overall, results show that LLMs' ability to detect\ndependence between steps has significant room for improvement.\n", "rewritten_text": "Understanding how well LLMs can reason about natural language plans, such as instructional text and recipes, is crucial for effectively utilizing them in decision-making systems. A key aspect of plans is the order in which their steps should be carried out, reflecting the causal relationships between them. We present CaT-Bench, a benchmark for Step Order Prediction questions that assess whether a step must occur before or after another in cooking recipe plans. This benchmark is used to evaluate the ability of state-of-the-art LLMs to comprehend causal and temporal dependencies. Our findings indicate that current LLMs fall short (with the best zero-shot performance at only 0.59 in F1 score) and tend to predict dependencies more frequently, possibly relying on the temporal order of steps as a heuristic. While soliciting explanations and providing few-shot examples improve performance, the highest F1 score achieved is only 0.73. Additionally, human evaluation of explanations and answer accuracy reveals that, on average, humans do not align with the model's reasoning. Surprisingly, explaining after answering yields better results than traditional chain-of-thought prompting, and LLM responses are inconsistent across questions about the same step pairs. Overall, the results highlight the significant room for improvement in LLMs' ability to identify dependencies between steps."}, "1907.06007": {"original_text": "  With the development of deep neural networks, the demand for a significant\namount of annotated training data becomes the performance bottlenecks in many\nfields of research and applications. Image synthesis can generate annotated\nimages automatically and freely, which gains increasing attention recently. In\nthis paper, we propose to synthesize scene text images from the 3D virtual\nworlds, where the precise descriptions of scenes, editable\nillumination/visibility, and realistic physics are provided. Different from the\nprevious methods which paste the rendered text on static 2D images, our method\ncan render the 3D virtual scene and text instances as an entirety. In this way,\nreal-world variations, including complex perspective transformations, various\nilluminations, and occlusions, can be realized in our synthesized scene text\nimages. Moreover, the same text instances with various viewpoints can be\nproduced by randomly moving and rotating the virtual camera, which acts as\nhuman eyes. The experiments on the standard scene text detection benchmarks\nusing the generated synthetic data demonstrate the effectiveness and\nsuperiority of the proposed method. The code and synthetic data is available\nat: https://github.com/MhLiao/SynthText3D\n", "rewritten_text": "With the advancement of deep neural networks, the need for a large amount of annotated training data has become a major bottleneck in various research fields and applications. Recently, there has been a growing interest in using image synthesis to automatically generate annotated images. This paper introduces a method for creating scene text images by synthesizing them from 3D virtual environments, which provide detailed scene descriptions, editable lighting and visibility, and realistic physics. Unlike previous approaches that simply overlay rendered text onto static 2D images, our method renders the entire 3D virtual scene along with the text instances. This allows for the incorporation of real-world variations such as complex perspective shifts, diverse lighting conditions, and occlusions in the synthesized scene text images. Additionally, by randomly adjusting the virtual camera to simulate different viewpoints, the same text instances can be generated with varying perspectives, mimicking human observation. Experiments conducted on standard scene text detection benchmarks using the generated synthetic data showcase the effectiveness and superiority of our proposed method. The code and synthetic data can be accessed at: https://github.com/MhLiao/SynthText3D"}, "2006.13017": {"original_text": "  Recently, 3D convolutional networks (3D ConvNets) yield good performance in\naction recognition. However, optical flow stream is still needed to ensure\nbetter performance, the cost of which is very high. In this paper, we propose a\nfast but effective way to extract motion features from videos utilizing\nresidual frames as the input data in 3D ConvNets. By replacing traditional\nstacked RGB frames with residual ones, 35.6% and 26.6% points improvements over\ntop-1 accuracy can be obtained on the UCF101 and HMDB51 datasets when ResNet-18\nmodels are trained from scratch. And we achieved the state-of-the-art results\nin this training mode. Analysis shows that better motion features can be\nextracted using residual frames compared to RGB counterpart. By combining with\na simple appearance path, our proposal can be even better than some methods\nusing optical flow streams.\n", "rewritten_text": "In recent studies, 3D convolutional networks (3D ConvNets) have shown promising results in action recognition. However, incorporating an optical flow stream remains crucial for achieving optimal performance, despite its high cost. This paper introduces a novel approach to efficiently extract motion features from videos by utilizing residual frames as input data in 3D ConvNets. By substituting traditional stacked RGB frames with residual frames, significant improvements of 35.6% and 26.6% in top-1 accuracy were observed on the UCF101 and HMDB51 datasets when training ResNet-18 models from scratch. This approach achieved state-of-the-art results in this training scenario. Comparative analysis indicates that residual frames enable the extraction of superior motion features compared to RGB frames. When combined with a simple appearance path, our proposed method outperforms some techniques that rely on optical flow streams."}, "2403.15615": {"original_text": "  Conversation is the subject of increasing interest in the social, cognitive,\nand computational sciences. And yet, as conversational datasets continue to\nincrease in size and complexity, researchers lack scalable methods to segment\nspeech-to-text transcripts into conversational turns--the basic building blocks\nof social interaction. We introduce \"NaturalTurn,\" a turn segmentation\nalgorithm designed to accurately capture the dynamics of naturalistic exchange.\nNaturalTurn operates by distinguishing speakers' primary conversational turns\nfrom listeners' secondary utterances, such as backchannels, brief\ninterjections, and other forms of parallel speech that characterize\nconversation. Using data from a large conversation corpus, we show how\nNaturalTurn-derived transcripts demonstrate favorable statistical and\ninferential characteristics compared to transcripts derived from existing\nmethods. The NaturalTurn algorithm represents an improvement in\nmachine-generated transcript processing methods, or \"turn models\" that will\nenable researchers to associate turn-taking dynamics with the broader outcomes\nthat result from social interaction, a central goal of conversation science.\n", "rewritten_text": "The topic of conversation is gaining interest in various fields such as social, cognitive, and computational sciences. However, as conversational datasets become larger and more complex, researchers face challenges in effectively segmenting speech-to-text transcripts into conversational turns, which are essential for understanding social interactions. To address this issue, we present \"NaturalTurn,\" an algorithm that accurately identifies conversational turns by distinguishing between primary speakers' turns and secondary utterances from listeners. By analyzing a significant conversation dataset, we demonstrate that transcripts generated by NaturalTurn exhibit superior statistical and inferential qualities compared to existing methods. This algorithm represents an advancement in machine-generated transcript processing, allowing researchers to better study turn-taking dynamics and their broader impacts on social interactions, a key objective in conversation science."}, "1807.00502": {"original_text": "  The use of deep learning for medical imaging has seen tremendous growth in\nthe research community. One reason for the slow uptake of these systems in the\nclinical setting is that they are complex, opaque and tend to fail silently.\nOutside of the medical imaging domain, the machine learning community has\nrecently proposed several techniques for quantifying model uncertainty (i.e.~a\nmodel knowing when it has failed). This is important in practical settings, as\nwe can refer such cases to manual inspection or correction by humans. In this\npaper, we aim to bring these recent results on estimating uncertainty to bear\non two important outputs in deep learning-based segmentation. The first is\nproducing spatial uncertainty maps, from which a clinician can observe where\nand why a system thinks it is failing. The second is quantifying an image-level\nprediction of failure, which is useful for isolating specific cases and\nremoving them from automated pipelines. We also show that reasoning about\nspatial uncertainty, the first output, is a useful intermediate representation\nfor generating segmentation quality predictions, the second output. We propose\na two-stage architecture for producing these measures of uncertainty, which can\naccommodate any deep learning-based medical segmentation pipeline.\n", "rewritten_text": "The use of deep learning in medical imaging has experienced significant growth within the research community. One challenge hindering the widespread adoption of these systems in clinical settings is their complexity, lack of transparency, and tendency to fail without clear indication. In other fields of machine learning, various methods have been proposed to quantify model uncertainty, allowing models to recognize when they are failing. This capability is crucial in practical applications, as it enables cases of uncertainty to be flagged for manual review or human intervention. This study aims to apply recent advancements in uncertainty estimation to enhance two key aspects of deep learning-based segmentation. The first aspect involves generating spatial uncertainty maps, which enable clinicians to identify where and why a system may be failing. The second aspect focuses on quantifying the likelihood of failure at the image level, aiding in the identification and removal of problematic cases from automated processes. Additionally, we demonstrate that leveraging spatial uncertainty as an intermediate representation can improve the accuracy of segmentation quality predictions. To achieve these goals, we propose a two-stage architecture capable of integrating uncertainty measures into any deep learning-based medical segmentation pipeline."}, "2112.15439": {"original_text": "  This paper aims to conduct a comprehensive study on facial-sketch synthesis\n(FSS). However, due to the high costs of obtaining hand-drawn sketch datasets,\nthere lacks a complete benchmark for assessing the development of FSS\nalgorithms over the last decade. We first introduce a high-quality dataset for\nFSS, named FS2K, which consists of 2,104 image-sketch pairs spanning three\ntypes of sketch styles, image backgrounds, lighting conditions, skin colors,\nand facial attributes. FS2K differs from previous FSS datasets in difficulty,\ndiversity, and scalability and should thus facilitate the progress of FSS\nresearch. Second, we present the largest-scale FSS investigation by reviewing\n89 classical methods, including 25 handcrafted feature-based facial-sketch\nsynthesis approaches, 29 general translation methods, and 35 image-to-sketch\napproaches. Besides, we elaborate comprehensive experiments on the existing 19\ncutting-edge models. Third, we present a simple baseline for FSS, named FSGAN.\nWith only two straightforward components, i.e., facial-aware masking and\nstyle-vector expansion, FSGAN surpasses the performance of all previous\nstate-of-the-art models on the proposed FS2K dataset by a large margin.\nFinally, we conclude with lessons learned over the past years and point out\nseveral unsolved challenges. Our code is available at\nhttps://github.com/DengPingFan/FSGAN.\n", "rewritten_text": "This paper seeks to conduct a thorough investigation into facial-sketch synthesis (FSS). However, due to the high expenses associated with acquiring hand-drawn sketch datasets, there is a lack of a comprehensive benchmark for evaluating the progress of FSS algorithms over the past decade. To address this gap, we introduce a high-quality dataset for FSS called FS2K, which comprises 2,104 image-sketch pairs covering three different sketch styles, image backgrounds, lighting conditions, skin tones, and facial features. FS2K sets itself apart from previous FSS datasets in terms of complexity, diversity, and scalability, thereby aiding in advancing FSS research. Additionally, we conduct the most extensive FSS study to date by examining 89 traditional methods, including 25 feature-based facial-sketch synthesis techniques, 29 general translation methods, and 35 image-to-sketch approaches. Furthermore, we perform comprehensive experiments on 19 state-of-the-art models. We also introduce a basic FSS model called FSGAN, which outperforms all previous models by a significant margin on the FS2K dataset with just two simple components: facial-aware masking and style-vector expansion. Finally, we share insights gained over the years and highlight remaining challenges. Our code can be accessed at https://github.com/DengPingFan/FSGAN."}, "1804.09803": {"original_text": "  The inference structures and computational complexity of existing deep neural\nnetworks, once trained, are fixed and remain the same for all test images.\nHowever, in practice, it is highly desirable to establish a progressive\nstructure for deep neural networks which is able to adapt its inference process\nand complexity for images with different visual recognition complexity. In this\nwork, we develop a multi-stage progressive structure with integrated confidence\nanalysis and decision policy learning for deep neural networks. This new\nframework consists of a set of network units to be activated in a sequential\nmanner with progressively increased complexity and visual recognition power.\nOur extensive experimental results on the CIFAR-10 and ImageNet datasets\ndemonstrate that the proposed progressive deep neural network is able to obtain\nmore than 10 fold complexity scalability while achieving the state-of-the-art\nperformance using a single network model satisfying different\ncomplexity-accuracy requirements.\n", "rewritten_text": "The current deep neural networks have fixed inference structures and computational complexity after training, which remain consistent across all test images. However, there is a strong need in practice to develop a progressive structure for deep neural networks that can adjust its inference process and complexity based on the visual recognition complexity of different images. In this study, we introduce a multi-stage progressive structure that incorporates confidence analysis and decision policy learning for deep neural networks. This innovative framework includes a series of network units that are activated sequentially, with increasing complexity and visual recognition capabilities. Our extensive experiments on the CIFAR-10 and ImageNet datasets show that the proposed progressive deep neural network can achieve over 10 times complexity scalability while maintaining top-tier performance with a single network model that meets various complexity-accuracy requirements."}, "2312.14988": {"original_text": "  Autoregressive and diffusion models drive the recent breakthroughs on\ntext-to-image generation. Despite their huge success of generating\nhigh-realistic images, a common shortcoming of these models is their high\ninference latency - autoregressive models run more than a thousand times\nsuccessively to produce image tokens and diffusion models convert Gaussian\nnoise into images with many hundreds of denoising steps. In this work, we\nexplore non-autoregressive text-to-image models that efficiently generate\nhundreds of image tokens in parallel. We develop many model variations with\ndifferent learning and inference strategies, initialized text encoders, etc.\nCompared with autoregressive baselines that needs to run one thousand times,\nour model only runs 16 times to generate images of competitive quality with an\norder of magnitude lower inference latency. Our non-autoregressive model with\n346M parameters generates an image of 256$\\times$256 with about one second on\none V100 GPU.\n", "rewritten_text": "The recent advancements in text-to-image generation have been primarily driven by autoregressive and diffusion models. While these models have been successful in creating highly realistic images, they suffer from a common drawback - their high inference latency. Autoregressive models require over a thousand successive runs to generate image tokens, while diffusion models involve numerous denoising steps to convert Gaussian noise into images. This study focuses on non-autoregressive text-to-image models that can efficiently produce hundreds of image tokens simultaneously. Various model variations with different learning and inference strategies, as well as initialized text encoders, are explored. In comparison to autoregressive models that require a thousand runs, our model only needs 16 runs to generate images of comparable quality, significantly reducing the inference latency. Our non-autoregressive model, with 346 million parameters, can generate a 256x256 image in approximately one second on a single V100 GPU."}, "2106.16038": {"original_text": "  Recent pretraining models in Chinese neglect two important aspects specific\nto the Chinese language: glyph and pinyin, which carry significant syntax and\nsemantic information for language understanding. In this work, we propose\nChineseBERT, which incorporates both the {\\it glyph} and {\\it pinyin}\ninformation of Chinese characters into language model pretraining. The glyph\nembedding is obtained based on different fonts of a Chinese character, being\nable to capture character semantics from the visual features, and the pinyin\nembedding characterizes the pronunciation of Chinese characters, which handles\nthe highly prevalent heteronym phenomenon in Chinese (the same character has\ndifferent pronunciations with different meanings). Pretrained on large-scale\nunlabeled Chinese corpus, the proposed ChineseBERT model yields significant\nperformance boost over baseline models with fewer training steps. The porpsoed\nmodel achieves new SOTA performances on a wide range of Chinese NLP tasks,\nincluding machine reading comprehension, natural language inference, text\nclassification, sentence pair matching, and competitive performances in named\nentity recognition. Code and pretrained models are publicly available at\nhttps://github.com/ShannonAI/ChineseBert.\n", "rewritten_text": "In this study, recent pretraining models in Chinese have overlooked two crucial aspects unique to the Chinese language: glyph and pinyin. These elements contain important syntax and semantic information essential for language comprehension. To address this gap, we introduce ChineseBERT, a model that integrates both glyph and pinyin information of Chinese characters during language model pretraining. The glyph embedding captures character semantics through various fonts, while the pinyin embedding represents the pronunciation of Chinese characters, effectively handling the heteronym phenomenon. Trained on a large-scale unlabeled Chinese corpus, ChineseBERT outperforms baseline models with fewer training iterations, achieving state-of-the-art results across various Chinese NLP tasks. The model excels in machine reading comprehension, natural language inference, text classification, sentence pair matching, and performs competitively in named entity recognition. The code and pretrained models for ChineseBERT are publicly accessible at https://github.com/ShannonAI/ChineseBert."}, "2103.05342": {"original_text": "  In this paper, we propose a novel data augmentation strategy named\nCut-Thumbnail, that aims to improve the shape bias of the network. We reduce an\nimage to a certain size and replace the random region of the original image\nwith the reduced image. The generated image not only retains most of the\noriginal image information but also has global information in the reduced\nimage. We call the reduced image as thumbnail. Furthermore, we find that the\nidea of thumbnail can be perfectly integrated with Mixed Sample Data\nAugmentation, so we put one image's thumbnail on another image while the ground\ntruth labels are also mixed, making great achievements on various computer\nvision tasks. Extensive experiments show that Cut-Thumbnail works better than\nstate-of-the-art augmentation strategies across classification, fine-grained\nimage classification, and object detection. On ImageNet classification,\nResNet-50 architecture with our method achieves 79.21\\% accuracy, which is more\nthan 2.8\\% improvement on the baseline.\n", "rewritten_text": "\"In this paper, we introduce a new data augmentation technique called Cut-Thumbnail, designed to enhance the network's shape bias. This method involves resizing an image to a specific size and replacing a random region of the original image with the resized image, creating a new image that preserves most of the original information while incorporating global details from the resized image, referred to as a thumbnail. We have discovered that integrating the concept of thumbnails with Mixed Sample Data Augmentation yields significant improvements in various computer vision tasks. Through extensive experiments, we have demonstrated that Cut-Thumbnail outperforms existing augmentation strategies in tasks such as classification, fine-grained image classification, and object detection. For instance, using the ResNet-50 architecture with our approach achieved an accuracy of 79.21% on ImageNet classification, representing a notable improvement of over 2.8% compared to the baseline.\""}, "2307.15257": {"original_text": "  The complexity of learning problems, such as Generative Adversarial Network\n(GAN) and its variants, multi-task and meta-learning, hyper-parameter learning,\nand a variety of real-world vision applications, demands a deeper understanding\nof their underlying coupling mechanisms. Existing approaches often address\nthese problems in isolation, lacking a unified perspective that can reveal\ncommonalities and enable effective solutions. Therefore, in this work, we\nproposed a new framework, named Learning with Constraint Learning (LwCL), that\ncan holistically examine challenges and provide a unified methodology to tackle\nall the above-mentioned complex learning and vision problems. Specifically,\nLwCL is designed as a general hierarchical optimization model that captures the\nessence of these diverse learning and vision problems. Furthermore, we develop\na gradient-response based fast solution strategy to overcome optimization\nchallenges of the LwCL framework. Our proposed framework efficiently addresses\na wide range of applications in learning and vision, encompassing three\ncategories and nine different problem types. Extensive experiments on synthetic\ntasks and real-world applications verify the effectiveness of our approach. The\nLwCL framework offers a comprehensive solution for tackling complex machine\nlearning and computer vision problems, bridging the gap between theory and\npractice.\n", "rewritten_text": "The complexity of learning challenges, such as Generative Adversarial Network (GAN) and its variations, multi-task and meta-learning, hyper-parameter learning, and various real-world vision applications, requires a deeper comprehension of their underlying connections. Current methods often tackle these issues separately, lacking a unified viewpoint that can uncover similarities and facilitate effective solutions. Therefore, we introduce a new framework called Learning with Constraint Learning (LwCL) in this study. LwCL offers a holistic approach to address the complexities of learning and vision problems by providing a unified methodology. It is designed as a general hierarchical optimization model that captures the essence of diverse learning and vision challenges. Additionally, we present a gradient-response based rapid solution strategy to overcome optimization hurdles within the LwCL framework. Our proposed framework efficiently handles a wide range of learning and vision applications, covering three categories and nine different problem types. Extensive experiments on both synthetic tasks and real-world scenarios confirm the efficacy of our approach. The LwCL framework presents a comprehensive solution for addressing intricate machine learning and computer vision problems, bridging the gap between theory and practical implementation."}, "2309.11119": {"original_text": "  A recent sensor fusion in a Bird's Eye View (BEV) space has shown its utility\nin various tasks such as 3D detection, map segmentation, etc. However, the\napproach struggles with inaccurate camera BEV estimation, and a perception of\ndistant areas due to the sparsity of LiDAR points. In this paper, we propose a\nbroad BEV fusion (BroadBEV) that addresses the problems with a spatial\nsynchronization approach of cross-modality. Our strategy aims to enhance camera\nBEV estimation for a broad-sighted perception while simultaneously improving\nthe completion of LiDAR's sparsity in the entire BEV space. Toward that end, we\ndevise Point-scattering that scatters LiDAR BEV distribution to camera depth\ndistribution. The method boosts the learning of depth estimation of the camera\nbranch and induces accurate location of dense camera features in BEV space. For\nan effective BEV fusion between the spatially synchronized features, we suggest\nColFusion that applies self-attention weights of LiDAR and camera BEV features\nto each other. Our extensive experiments demonstrate that BroadBEV provides a\nbroad-sighted BEV perception with remarkable performance gains.\n", "rewritten_text": "In a recent study, a sensor fusion technique in a Bird's Eye View (BEV) setting has proven to be useful for tasks like 3D detection and map segmentation. However, challenges arise from inaccurate camera BEV estimation and limited perception of distant areas due to sparse LiDAR points. This paper introduces a new approach called BroadBEV fusion, which addresses these issues through spatial synchronization across different sensor modalities. The goal is to improve camera BEV estimation for a wider field of view and enhance the completeness of LiDAR data in the BEV space. To achieve this, we introduce Point-scattering to transform LiDAR BEV distribution into camera depth distribution, improving depth estimation and locating dense camera features accurately in the BEV space. Additionally, we propose ColFusion for effective fusion of synchronized features by applying self-attention weights between LiDAR and camera BEV features. Our experiments show that BroadBEV fusion significantly enhances BEV perception and performance."}, "2403.17920": {"original_text": "  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.\n", "rewritten_text": "New methods have been developed for generating dynamic 3D scenes from text input, utilizing guidance from pre-trained text-to-video models. However, current motion representations, such as deformation models or time-dependent neural representations, have limitations in the extent of motion they can produce, particularly beyond the bounding box used for volume rendering. This lack of a versatile motion model contributes to the realism gap between 4D generation techniques and recent, highly realistic video generation models. In response, we introduce TC4D: trajectory-conditioned text-to-4D generation, which divides motion into global and local components. The global motion of a scene's bounding box is represented through rigid transformation along a trajectory defined by a spline. Local deformations that align with the global trajectory are learned with guidance from a text-to-video model. Our approach allows for the creation of scenes animated along diverse trajectories, facilitates compositional scene generation, and enhances the realism and quantity of generated motion. We assess these improvements through qualitative evaluation and a user study. For video demonstrations, please visit our website: https://sherwinbahmani.github.io/tc4d."}, "2407.11144": {"original_text": "  Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25.\n", "rewritten_text": "Even though American Sign Language (ASL) is a well-studied sign language, the lack of data is a major challenge for machine learning research. This issue is even more pronounced for the numerous other sign languages used by Deaf/Hard of Hearing communities worldwide. This paper introduces YouTube-SL-25, a vast multilingual collection of sign language videos with accurately aligned captions sourced from YouTube. With over 3000 hours of videos spanning more than 25 sign languages, YouTube-SL-25 surpasses YouTube-ASL in size, stands as the largest parallel sign language dataset to date, and serves as the first or largest parallel dataset for many of its included languages. The study establishes baseline performance for sign-to-text tasks using a unified multilingual multitask model based on T5, showcasing results on benchmarks for four sign languages. The findings illustrate the advantages of multilingual transfer learning for both high- and low-resource sign languages within YouTube-SL-25."}, "1702.01776": {"original_text": "  In aspect-based sentiment analysis, most existing methods either focus on\naspect/opinion terms extraction or aspect terms categorization. However, each\ntask by itself only provides partial information to end users. To generate more\ndetailed and structured opinion analysis, we propose a finer-grained problem,\nwhich we call category-specific aspect and opinion terms extraction. This\nproblem involves the identification of aspect and opinion terms within each\nsentence, as well as the categorization of the identified terms. To this end,\nwe propose an end-to-end multi-task attention model, where each task\ncorresponds to aspect/opinion terms extraction for a specific category. Our\nmodel benefits from exploring the commonalities and relationships among\ndifferent tasks to address the data sparsity issue. We demonstrate its\nstate-of-the-art performance on three benchmark datasets.\n", "rewritten_text": "\"In the field of aspect-based sentiment analysis, most current approaches focus on either extracting aspect/opinion terms or categorizing aspect terms. However, each individual task only offers partial information to users. To provide more detailed and organized opinion analysis, we introduce a more refined issue known as category-specific aspect and opinion terms extraction. This challenge involves identifying aspect and opinion terms within each sentence and categorizing these terms. To tackle this, we propose an end-to-end multi-task attention model where each task is dedicated to extracting aspect/opinion terms for a specific category. Our model leverages the exploration of similarities and connections between different tasks to address the issue of sparse data. We showcase its top-notch performance on three standard datasets.\""}, "2301.00399": {"original_text": "  In the present paper, semantic parsing challenges are briefly introduced and\nQDMR formalism in semantic parsing is implemented using sequence to sequence\nmodel with attention but uses only part of speech(POS) as a representation of\nwords of a sentence to make the training as simple and as fast as possible and\nalso avoiding curse of dimensionality as well as overfitting. It is shown how\nsemantic operator prediction could be augmented with other models like the\nCopyNet model or the recursive neural net model.\n", "rewritten_text": "The paper briefly introduces semantic parsing challenges and implements the QDMR formalism in semantic parsing using a sequence-to-sequence model with attention. The model utilizes only part of speech (POS) as a representation of words in a sentence to simplify and expedite training, while also preventing issues such as curse of dimensionality and overfitting. The paper explores how semantic operator prediction could be enhanced by incorporating other models like the CopyNet model or the recursive neural net model."}, "2411.07075": {"original_text": "  To predict upcoming text, language models must in some cases retrieve\nin-context information verbatim. In this report, we investigated how the\nability of language models to retrieve arbitrary in-context nouns developed\nduring training (across time) and as language models trained on the same\ndataset increase in size (across scale). We then asked whether learning of\nin-context retrieval correlates with learning of more challenging zero-shot\nbenchmarks. Furthermore, inspired by semantic effects in human short-term\nmemory, we evaluated the retrieval with respect to a major semantic component\nof target nouns, namely whether they denote a concrete or abstract entity, as\nrated by humans. We show that verbatim in-context retrieval developed in a\nsudden transition early in the training process, after about 1% of the training\ntokens. This was observed across model sizes (from 14M and up to 12B\nparameters), and the transition occurred slightly later for the two smallest\nmodels. We further found that the development of verbatim in-context retrieval\nis positively correlated with the learning of zero-shot benchmarks. Around the\ntransition point, all models showed the advantage of retrieving concrete nouns\nas opposed to abstract nouns. In all but two smallest models, the advantage\ndissipated away toward the end of training.\n", "rewritten_text": "\"In this study, we explored how language models improve their ability to retrieve specific in-context nouns as they undergo training and increase in size. We also examined whether this improvement in retrieval skills is linked to their performance on more challenging zero-shot benchmarks. Additionally, we assessed the retrieval of nouns based on whether they represent concrete or abstract concepts, similar to how human short-term memory operates. Our findings indicate that language models experience a sudden improvement in verbatim in-context retrieval early in the training process, after processing about 1% of the training data. This improvement was consistent across models of varying sizes, ranging from 14M to 12B parameters, with the smaller models showing a slightly delayed transition. We also observed a positive correlation between the development of in-context retrieval and performance on zero-shot benchmarks. Notably, all models demonstrated a preference for retrieving concrete nouns over abstract nouns around the transition point, although this advantage diminished in larger models towards the end of training.\""}, "1911.11351": {"original_text": "  Recently, Human Attribute Recognition (HAR) has become a hot topic due to its\nscientific challenges and application potentials, where localizing attributes\nis a crucial stage but not well handled. In this paper, we propose a novel deep\nlearning approach to HAR, namely Distraction-aware HAR (Da-HAR). It enhances\ndeep CNN feature learning by improving attribute localization through a\ncoarse-to-fine attention mechanism. At the coarse step, a self-mask block is\nbuilt to roughly discriminate and reduce distractions, while at the fine step,\na masked attention branch is applied to further eliminate irrelevant regions.\nThanks to this mechanism, feature learning is more accurate, especially when\nheavy occlusions and complex backgrounds exist. Extensive experiments are\nconducted on the WIDER-Attribute and RAP databases, and state-of-the-art\nresults are achieved, demonstrating the effectiveness of the proposed approach.\n", "rewritten_text": "Recently, there has been a surge of interest in Human Attribute Recognition (HAR) due to its scientific challenges and potential applications. One critical aspect of HAR is localizing attributes, which has not been well addressed. This paper introduces a new deep learning method for HAR called Distraction-aware HAR (Da-HAR). Da-HAR improves deep CNN feature learning by enhancing attribute localization using a coarse-to-fine attention mechanism. In the coarse step, a self-mask block is used to roughly identify and reduce distractions, while in the fine step, a masked attention branch is employed to further eliminate irrelevant regions. This approach leads to more accurate feature learning, particularly in scenarios with heavy occlusions and complex backgrounds. Extensive experiments on the WIDER-Attribute and RAP databases show that the proposed method achieves state-of-the-art results, highlighting its effectiveness."}, "2310.08487": {"original_text": "  While multi-modal models have successfully integrated information from image,\nvideo, and audio modalities, integrating graph modality into large language\nmodels (LLMs) remains unexplored. This discrepancy largely stems from the\ninherent divergence between structured graph data and unstructured text data.\nIncorporating graph knowledge provides a reliable source of information,\nenabling potential solutions to address issues in text generation, e.g.,\nhallucination, and lack of domain knowledge. To evaluate the integration of\ngraph knowledge into language models, a dedicated dataset is needed. However,\nthere is currently no benchmark dataset specifically designed for multimodal\ngraph-language models. To address this gap, we propose GraphextQA, a question\nanswering dataset with paired subgraphs, retrieved from Wikidata, to facilitate\nthe evaluation and future development of graph-language models. Additionally,\nwe introduce a baseline model called CrossGNN, which conditions answer\ngeneration on the paired graphs by cross-attending question-aware graph\nfeatures at decoding. The proposed dataset is designed to evaluate\ngraph-language models' ability to understand graphs and make use of it for\nanswer generation. We perform experiments with language-only models and the\nproposed graph-language model to validate the usefulness of the paired graphs\nand to demonstrate the difficulty of the task.\n", "rewritten_text": "\"While existing multi-modal models have effectively combined information from image, video, and audio modalities, the integration of graph modality into large language models (LLMs) remains unexplored. This lack of exploration is primarily due to the fundamental differences between structured graph data and unstructured text data. By incorporating graph knowledge, a reliable source of information is introduced, offering potential solutions to challenges in text generation such as hallucination and lack of domain knowledge. Evaluating the incorporation of graph knowledge into language models requires a dedicated dataset, yet there is currently no benchmark dataset specifically tailored for multimodal graph-language models. To bridge this gap, we propose GraphextQA, a question answering dataset featuring paired subgraphs sourced from Wikidata to facilitate the assessment and future advancement of graph-language models. Furthermore, we introduce a baseline model named CrossGNN, which leverages question-aware graph features during answer generation by cross-attending to paired graphs at decoding. The proposed dataset aims to assess the capability of graph-language models to comprehend graphs and utilize them for generating answers. Through experiments involving language-only models and the proposed graph-language model, we aim to validate the utility of paired graphs and illustrate the complexity of the task.\""}, "cs/0010012": {"original_text": "  We describe a new framework for distilling information from word lattices to\nimprove the accuracy of speech recognition and obtain a more perspicuous\nrepresentation of a set of alternative hypotheses. In the standard MAP decoding\napproach the recognizer outputs the string of words corresponding to the path\nwith the highest posterior probability given the acoustics and a language\nmodel. However, even given optimal models, the MAP decoder does not necessarily\nminimize the commonly used performance metric, word error rate (WER). We\ndescribe a method for explicitly minimizing WER by extracting word hypotheses\nwith the highest posterior probabilities from word lattices. We change the\nstandard problem formulation by replacing global search over a large set of\nsentence hypotheses with local search over a small set of word candidates. In\naddition to improving the accuracy of the recognizer, our method produces a new\nrepresentation of the set of candidate hypotheses that specifies the sequence\nof word-level confusions in a compact lattice format. We study the properties\nof confusion networks and examine their use for other tasks, such as lattice\ncompression, word spotting, confidence annotation, and reevaluation of\nrecognition hypotheses using higher-level knowledge sources.\n", "rewritten_text": "We present a novel approach to extracting information from word lattices in order to enhance speech recognition accuracy and achieve a clearer representation of potential hypotheses. In the traditional Maximum A Posteriori (MAP) decoding method, the recognizer generates the word sequence with the highest probability based on acoustic input and a language model. However, even with optimal models, the MAP decoder may not effectively minimize the widely used word error rate (WER). Our method focuses on minimizing WER by identifying word hypotheses with the highest posterior probabilities within word lattices. This involves shifting from a global search across numerous sentence hypotheses to a local search among a smaller set of word candidates. Apart from enhancing recognition accuracy, our approach offers a fresh perspective on candidate hypotheses by detailing word-level confusions in a concise lattice format. We explore the characteristics of confusion networks and their potential applications in tasks such as lattice compression, word identification, confidence assessment, and reassessment of recognition hypotheses using advanced knowledge sources."}, "2111.03993": {"original_text": "  Skeleton data is of low dimension. However, there is a trend of using very\ndeep and complicated feedforward neural networks to model the skeleton sequence\nwithout considering the complexity in recent year. In this paper, a simple yet\neffective multi-scale semantics-guided neural network (MS-SGN) is proposed for\nskeleton-based action recognition. We explicitly introduce the high level\nsemantics of joints (joint type and frame index) into the network to enhance\nthe feature representation capability of joints. Moreover, a multi-scale\nstrategy is proposed to be robust to the temporal scale variations. In\naddition, we exploit the relationship of joints hierarchically through two\nmodules, i.e., a joint-level module for modeling the correlations of joints in\nthe same frame and a frame-level module for modeling the temporal dependencies\nof frames. With an order of magnitude smaller model size than most previous\nmethods, MSSGN achieves the state-of-the-art performance on the NTU60, NTU120,\nand SYSU datasets.\n", "rewritten_text": "The dimension of skeleton data is typically low. Despite the recent trend of using complex deep feedforward neural networks to model skeleton sequences without considering the complexity, this paper introduces a simple yet effective approach called Multi-Scale Semantics-Guided Neural Network (MS-SGN) for skeleton-based action recognition. The network incorporates high-level semantics of joints (joint type and frame index) to improve feature representation. A multi-scale strategy is employed to handle temporal scale variations, and the relationships between joints are explored hierarchically through joint-level and frame-level modules. Despite having a significantly smaller model size compared to previous methods, MSSGN achieves state-of-the-art performance on the NTU60, NTU120, and SYSU datasets."}, "1808.06428": {"original_text": "  This paper presents an approach for automatic detection of Munro's\nMicroabscess in stratum corneum (SC) of human skin biopsy in order to realize a\nmachine assisted diagnosis of Psoriasis. The challenge of detecting neutrophils\nin presence of nucleated cells is solved using the recent advances of deep\nlearning algorithms. Separation of SC layer, extraction of patches from the\nlayer followed by classification of patches with respect to presence or absence\nof neutrophils form the basis of the overall approach which is effected through\nan integration of a U-Net based segmentation network and a capsule network for\nclassification. The novel design of the present capsule net leads to a drastic\nreduction in the number of parameters without any noticeable compromise in the\noverall performance. The research further addresses the challenge of dealing\nwith Mega-pixel images (in 10X) vis-a-vis Giga-pixel ones (in 40X). The\npromising result coming out of an experiment on a dataset consisting of 273\nreal-life images shows that a practical system is possible based on the present\nresearch. The implementation of our system is available at\nhttps://github.com/Anabik/CapsDeMM.\n", "rewritten_text": "This paper introduces a method for automatically detecting Munro's Microabscess in the stratum corneum (SC) of human skin biopsies to facilitate a machine-assisted diagnosis of Psoriasis. The challenge of identifying neutrophils among nucleated cells is addressed using advanced deep learning algorithms. The approach involves separating the SC layer, extracting patches, and classifying them based on the presence or absence of neutrophils. This is achieved through a combination of a U-Net segmentation network and a capsule network for classification. The unique design of the capsule network significantly reduces the number of parameters without compromising performance. The study also tackles the issue of handling high-resolution images, demonstrating promising results on a dataset of 273 real-life images. The system implementation can be accessed at https://github.com/Anabik/CapsDeMM."}, "1302.1422": {"original_text": "  The variation of word meaning according to the context leads us to enrich the\ntype system of our syntactical and semantic analyser of French based on\ncategorial grammars and Montague semantics (or lambda-DRT). The main advantage\nof a deep semantic analyse is too represent meaning by logical formulae that\ncan be easily used e.g. for inferences. Determiners and quantifiers play a\nfundamental role in the construction of those formulae. But in our rich type\nsystem the usual semantic terms do not work. We propose a solution ins- pired\nby the tau and epsilon operators of Hilbert, kinds of generic elements and\nchoice functions. This approach unifies the treatment of the different determi-\nners and quantifiers as well as the dynamic binding of pronouns. Above all,\nthis fully computational view fits in well within the wide coverage parser\nGrail, both from a theoretical and a practical viewpoint.\n", "rewritten_text": "The variability in word meanings based on context prompts us to enhance the type system of our syntactic and semantic analyzer for French, which is grounded in categorial grammars and Montague semantics (or lambda-DRT). A key benefit of conducting a thorough semantic analysis is the ability to represent meaning through logical formulas that can be easily utilized for making inferences. Determiners and quantifiers play a crucial role in constructing these formulas. However, the standard semantic terms do not align with our comprehensive type system. To address this, we suggest a solution inspired by Hilbert's tau and epsilon operators, which involve generic elements and choice functions. This approach harmonizes the treatment of various determiners and quantifiers, as well as the dynamic binding of pronouns. Importantly, this computational perspective seamlessly integrates with the extensive coverage parser Grail, both theoretically and practically."}, "2012.07315": {"original_text": "  The categorical distribution is a natural representation of uncertainty in\nmulti-class segmentations. In the two-class case the categorical distribution\nreduces to the Bernoulli distribution, for which grayscale morphology provides\na range of useful operations. In the general case, applying morphological\noperations on uncertain multi-class segmentations is not straightforward as an\nimage of categorical distributions is not a complete lattice. Although\nmorphology on color images has received wide attention, this is not so for\ncolor-coded or categorical images and even less so for images of categorical\ndistributions. In this work, we establish a set of requirements for morphology\non categorical distributions by combining classic morphology with a\nprobabilistic view. We then define operators respecting these requirements,\nintroduce protected operations on categorical distributions and illustrate the\nutility of these operators on two example tasks: modeling annotator bias in\nbrain tumor segmentations and segmenting vesicle instances from the predictions\nof a multi-class U-Net.\n", "rewritten_text": "The categorical distribution is a natural way to represent uncertainty in multi-class segmentations. In a two-class scenario, it simplifies to the Bernoulli distribution, where grayscale morphology offers various useful functions. However, applying morphological operations to uncertain multi-class segmentations is not straightforward due to the incomplete lattice nature of categorical distribution images. While color image morphology is well-studied, the same cannot be said for color-coded or categorical images, especially those representing categorical distributions. This study establishes a set of criteria for morphological operations on categorical distributions by blending traditional morphology with a probabilistic perspective. We then introduce operators that meet these criteria, present protected operations on categorical distributions, and demonstrate the effectiveness of these operators through two case studies: modeling annotator bias in brain tumor segmentations and segmenting vesicle instances using predictions from a multi-class U-Net."}, "2410.24219": {"original_text": "  Despite advancements in Text-to-Video (T2V) generation, producing videos with\nrealistic motion remains challenging. Current models often yield static or\nminimally dynamic outputs, failing to capture complex motions described by\ntext. This issue stems from the internal biases in text encoding, which\noverlooks motions, and inadequate conditioning mechanisms in T2V generation\nmodels. To address this, we propose a novel framework called DEcomposed MOtion\n(DEMO), which enhances motion synthesis in T2V generation by decomposing both\ntext encoding and conditioning into content and motion components. Our method\nincludes a content encoder for static elements and a motion encoder for\ntemporal dynamics, alongside separate content and motion conditioning\nmechanisms. Crucially, we introduce text-motion and video-motion supervision to\nimprove the model's understanding and generation of motion. Evaluations on\nbenchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench\ndemonstrate DEMO's superior ability to produce videos with enhanced motion\ndynamics while maintaining high visual quality. Our approach significantly\nadvances T2V generation by integrating comprehensive motion understanding\ndirectly from textual descriptions. Project page:\nhttps://PR-Ryan.github.io/DEMO-project/\n", "rewritten_text": "Despite progress in Text-to-Video (T2V) technology, creating videos with realistic motion remains a challenge. Current models often generate static or minimally dynamic outputs, failing to accurately depict the complex motions described in text. This issue is due to biases in text encoding that overlook motions, as well as insufficient conditioning mechanisms in T2V generation models. To tackle this problem, we introduce a new framework called DEcomposed MOtion (DEMO), which improves motion synthesis in T2V generation by breaking down text encoding and conditioning into content and motion components. Our method includes a content encoder for static elements and a motion encoder for temporal dynamics, along with separate content and motion conditioning mechanisms. Importantly, we incorporate text-motion and video-motion supervision to enhance the model's comprehension and generation of motion. Evaluations on various benchmarks like MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench show that DEMO excels in producing videos with enhanced motion dynamics while maintaining high visual quality. Our approach represents a significant advancement in T2V generation by integrating a thorough understanding of motion directly from textual descriptions. Visit our project page at https://PR-Ryan.github.io/DEMO-project/ for more information."}, "2406.09386": {"original_text": "  Controllable synthetic data generation can substantially lower the annotation\ncost of training data. Prior works use diffusion models to generate driving\nimages conditioned on the 3D object layout. However, those models are trained\non small-scale datasets like nuScenes, which lack appearance and layout\ndiversity. Moreover, overfitting often happens, where the trained models can\nonly generate images based on the layout data from the validation set of the\nsame dataset. In this work, we introduce a simulator-conditioned scene\ngeneration framework called SimGen that can learn to generate diverse driving\nscenes by mixing data from the simulator and the real world. It uses a novel\ncascade diffusion pipeline to address challenging sim-to-real gaps and\nmulti-condition conflicts. A driving video dataset DIVA is collected to enhance\nthe generative diversity of SimGen, which contains over 147.5 hours of\nreal-world driving videos from 73 locations worldwide and simulated driving\ndata from the MetaDrive simulator. SimGen achieves superior generation quality\nand diversity while preserving controllability based on the text prompt and the\nlayout pulled from a simulator. We further demonstrate the improvements brought\nby SimGen for synthetic data augmentation on the BEV detection and segmentation\ntask and showcase its capability in safety-critical data generation.\n", "rewritten_text": "\"Generating synthetic data in a controllable manner can significantly reduce the cost of annotating training data. Previous studies have utilized diffusion models to create driving images based on the 3D object layout. However, these models are typically trained on limited datasets such as nuScenes, which lack diversity in appearance and layout. This often leads to overfitting, where the models can only generate images based on the layout data from the validation set of the same dataset. In this research, we present a new framework called SimGen, which generates scenes conditioned on a simulator to produce a variety of driving scenarios by combining data from both simulated and real-world environments. SimGen utilizes a unique cascade diffusion pipeline to address challenges in bridging the gap between simulation and reality, as well as handling conflicting conditions. To enhance the diversity of generated scenes, we have curated a driving video dataset called DIVA, comprising over 147.5 hours of real-world driving videos from 73 global locations and simulated driving data from the MetaDrive simulator. SimGen demonstrates superior quality and diversity in generating scenes while maintaining control based on input text prompts and simulator-derived layouts. We also showcase the benefits of using SimGen for synthetic data augmentation in tasks such as BEV detection and segmentation, highlighting its effectiveness in generating safety-critical data.\""}, "2311.02313": {"original_text": "  Large-scale semantic mapping is crucial for outdoor autonomous agents to\nfulfill high-level tasks such as planning and navigation. This paper proposes a\nnovel method for large-scale 3D semantic reconstruction through implicit\nrepresentations from posed LiDAR measurements alone. We first leverage an\noctree-based and hierarchical structure to store implicit features, then these\nimplicit features are decoded to semantic information and signed distance value\nthrough shallow Multilayer Perceptrons (MLPs). We adopt off-the-shelf\nalgorithms to predict the semantic labels and instance IDs of point clouds. We\nthen jointly optimize the feature embeddings and MLPs parameters with a\nself-supervision paradigm for point cloud geometry and a pseudo-supervision\nparadigm for semantic and panoptic labels. Subsequently, categories and\ngeometric structures for novel points are regressed, and marching cubes are\nexploited to subdivide and visualize the scenes in the inferring stage. For\nscenarios with memory constraints, a map stitching strategy is also developed\nto merge sub-maps into a complete map. Experiments on two real-world datasets,\nSemanticKITTI and SemanticPOSS, demonstrate the superior segmentation\nefficiency and mapping effectiveness of our framework compared to current\nstate-of-the-art 3D LiDAR mapping methods.\n", "rewritten_text": "Semantic mapping at a large scale is essential for outdoor autonomous agents to carry out tasks like planning and navigation effectively. This study introduces a new approach for reconstructing 3D semantics on a large scale using implicit representations derived solely from LiDAR measurements. The method involves storing implicit features in an octree-based hierarchical structure, decoding these features into semantic information and signed distance values using shallow Multilayer Perceptrons (MLPs), and utilizing existing algorithms to predict semantic labels and instance IDs of point clouds. The optimization process involves jointly refining feature embeddings and MLP parameters through self-supervision for point cloud geometry and pseudo-supervision for semantic and panoptic labels. Additionally, the framework includes regressing categories and geometric structures for new points, utilizing marching cubes to visualize scenes during inference, and implementing a map stitching strategy to merge sub-maps into a complete map for scenarios with memory constraints. Experimental results on real-world datasets, SemanticKITTI and SemanticPOSS, showcase the superior segmentation efficiency and mapping effectiveness of this framework compared to current state-of-the-art 3D LiDAR mapping methods."}, "2203.09333": {"original_text": "  Perceiving the similarity between images has been a long-standing and\nfundamental problem underlying various visual generation tasks. Predominant\napproaches measure the inter-image distance by computing pointwise absolute\ndeviations, which tends to estimate the median of instance distributions and\nleads to blurs and artifacts in the generated images. This paper presents\nMoNCE, a versatile metric that introduces image contrast to learn a calibrated\nmetric for the perception of multifaceted inter-image distances. Unlike vanilla\ncontrast which indiscriminately pushes negative samples from the anchor\nregardless of their similarity, we propose to re-weight the pushing force of\nnegative samples adaptively according to their similarity to the anchor, which\nfacilitates the contrastive learning from informative negative samples. Since\nmultiple patch-level contrastive objectives are involved in image distance\nmeasurement, we introduce optimal transport in MoNCE to modulate the pushing\nforce of negative samples collaboratively across multiple contrastive\nobjectives. Extensive experiments over multiple image translation tasks show\nthat the proposed MoNCE outperforms various prevailing metrics substantially.\nThe code is available at https://github.com/fnzhan/MoNCE.\n", "rewritten_text": "Recognizing similarities between images has been a longstanding and essential challenge in visual generation tasks. Traditional methods typically calculate inter-image distance using pointwise absolute deviations, which often result in estimating the median of instance distributions and can cause blurriness and artifacts in generated images. This study introduces MoNCE, a flexible metric that incorporates image contrast to develop a calibrated metric for assessing diverse inter-image distances. Unlike basic contrast methods that push negative samples away from the anchor without considering their similarity, we propose adjusting the pushing force of negative samples based on their similarity to the anchor, enabling more effective contrastive learning from informative negative samples. As multiple patch-level contrastive objectives are utilized in measuring image distance, we incorporate optimal transport in MoNCE to collectively adjust the pushing force of negative samples across these objectives. Extensive experiments across various image translation tasks demonstrate that MoNCE surpasses existing metrics significantly. The code can be accessed at https://github.com/fnzhan/MoNCE."}, "2402.17292": {"original_text": "  Text-to-Avatar generation has recently made significant strides due to\nadvancements in diffusion models. However, most existing work remains\nconstrained by limited diversity, producing avatars with subtle differences in\nappearance for a given text prompt. We design DivAvatar, a novel framework that\ngenerates diverse avatars, empowering 3D creatives with a multitude of distinct\nand richly varied 3D avatars from a single text prompt. Different from most\nexisting work that exploits scene-specific 3D representations such as NeRF,\nDivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse\navatar generation from simply noise sampling in inference time. DivAvatar has\ntwo key designs that help achieve generation diversity and visual quality. The\nfirst is a noise sampling technique during training phase which is critical in\ngenerating diverse appearances. The second is a semantic-aware zoom mechanism\nand a novel depth loss, the former producing appearances of high textual\nfidelity by separate fine-tuning of specific body parts and the latter\nimproving geometry quality greatly by smoothing the generated mesh in the\nfeatures space. Extensive experiments show that DivAvatar is highly versatile\nin generating avatars of diverse appearances.\n", "rewritten_text": "Recently, there have been significant advancements in the field of Text-to-Avatar generation, particularly due to improvements in diffusion models. However, existing methods are often limited in diversity, resulting in avatars that have subtle variations in appearance based on a given text prompt. To address this issue, we introduce DivAvatar, a new framework that aims to produce a wide range of unique and varied 3D avatars from a single text input. Unlike previous approaches that rely on specific 3D representations like NeRF, DivAvatar utilizes a 3D generative model (EVA3D) that can be fine-tuned to generate diverse avatars through noise sampling during inference. DivAvatar incorporates two key features to enhance diversity and visual quality: a noise sampling technique during training to create varied appearances, and a semantic-aware zoom mechanism and depth loss to improve fidelity and geometry quality. Through extensive experiments, DivAvatar demonstrates its versatility in generating avatars with diverse appearances."}, "2208.13078": {"original_text": "  Owing to the lack of corpora for low-resource languages, current works on\ndialogue generation have mainly focused on English. In this paper, we present\nmDIA, the first large-scale multilingual benchmark for dialogue generation\nacross low- to high-resource languages. It covers real-life conversations in 46\nlanguages across 19 language families. We present baseline results obtained by\nfine-tuning the multilingual, non-dialogue-focused pre-trained model mT5 as\nwell as English-centric, dialogue-focused pre-trained chatbot DialoGPT. The\nresults show that mT5-based models perform better on sacreBLEU and BertScore\nbut worse on diversity. Even though promising results are found in few-shot and\nzero-shot scenarios, there is a large gap between the generation quality in\nEnglish and other languages. We hope that the release of mDIA could encourage\nmore works on multilingual dialogue generation to promote language diversity.\n", "rewritten_text": "Due to the limited availability of data for languages with few resources, current research on dialogue generation has primarily concentrated on English. This paper introduces mDIA, a comprehensive multilingual benchmark for dialogue generation spanning low- to high-resource languages. It encompasses authentic conversations in 46 languages from 19 language families. The study presents initial outcomes achieved by adapting the multilingual, general-purpose pre-trained model mT5 and the English-focused pre-trained chatbot DialoGPT for dialogue tasks. The findings indicate that mT5-based models outperform in sacreBLEU and BertScore metrics but lag in diversity. While some promising results are observed in scenarios with minimal or no training data, there remains a significant disparity in generation quality between English and other languages. The authors anticipate that the introduction of mDIA will inspire further research in multilingual dialogue generation to support linguistic diversity."}, "2310.08908": {"original_text": "  The large language model (LLM) has garnered significant attention due to its\nin-context learning mechanisms and emergent capabilities. The research\ncommunity has conducted several pilot studies to apply LLMs to machine\ntranslation tasks and evaluate their performance from diverse perspectives.\nHowever, previous research has primarily focused on the LLM itself and has not\nexplored human intervention in the inference process of LLM. The\ncharacteristics of LLM, such as in-context learning and prompt engineering,\nclosely mirror human cognitive abilities in language tasks, offering an\nintuitive solution for human-in-the-loop generation. In this study, we propose\na human-in-the-loop pipeline that guides LLMs to produce customized outputs\nwith revision instructions. The pipeline initiates by prompting the LLM to\nproduce a draft translation, followed by the utilization of automatic retrieval\nor human feedback as supervision signals to enhance the LLM's translation\nthrough in-context learning. The human-machine interactions generated in this\npipeline are also stored in an external database to expand the in-context\nretrieval database, enabling us to leverage human supervision in an offline\nsetting. We evaluate the proposed pipeline using GPT-3.5-turbo API on five\ndomain-specific benchmarks for German-English translation. The results\ndemonstrate the effectiveness of the pipeline in tailoring in-domain\ntranslations and improving translation performance compared to direct\ntranslation. Additionally, we discuss the results from the following\nperspectives: 1) the effectiveness of different in-context retrieval methods;\n2) the construction of a retrieval database under low-resource scenarios; 3)\nthe observed domains differences; 4) the quantitative analysis of linguistic\nstatistics; and 5) the qualitative analysis of translation cases. The code and\ndata are available at https://github.com/NLP2CT/HIL-MT/.\n", "rewritten_text": "The significant attention garnered by the large language model (LLM) is attributed to its in-context learning mechanisms and emergent capabilities. Several pilot studies within the research community have explored the application of LLMs in machine translation tasks, evaluating their performance from various angles. However, existing research has predominantly focused on the LLM itself, neglecting human intervention in the inference process of LLM. The features of LLM, such as in-context learning and prompt engineering, closely resemble human cognitive abilities in language tasks, providing an intuitive solution for human-in-the-loop generation. This study introduces a human-in-the-loop pipeline that directs LLMs to generate customized outputs with revision instructions. The pipeline begins by prompting the LLM to produce a draft translation, followed by utilizing automatic retrieval or human feedback as supervision signals to enhance the LLM's translation through in-context learning. The human-machine interactions in this pipeline are stored in an external database to expand the in-context retrieval database, enabling the utilization of human supervision in an offline setting. The proposed pipeline is evaluated using the GPT-3.5-turbo API on five domain-specific benchmarks for German-English translation. The results highlight the pipeline's effectiveness in tailoring in-domain translations and enhancing translation performance compared to direct translation. Furthermore, the study discusses the effectiveness of different in-context retrieval methods, constructing a retrieval database under low-resource scenarios, domain differences observed, quantitative analysis of linguistic statistics, and qualitative analysis of translation cases. The code and data can be accessed at https://github.com/NLP2CT/HIL-MT/."}, "1011.0519": {"original_text": "  It is usual to consider that standards generate mixed feelings among\nscientists. They are often seen as not really reflecting the state of the art\nin a given domain and a hindrance to scientific creativity. Still, scientists\nshould theoretically be at the best place to bring their expertise into\nstandard developments, being even more neutral on issues that may typically be\nrelated to competing industrial interests. Even if it could be thought of as\neven more complex to think about developping standards in the humanities, we\nwill show how this can be made feasible through the experience gained both\nwithin the Text Encoding Initiative consortium and the International\nOrganisation for Standardisation. By taking the specific case of lexical\nresources, we will try to show how this brings about new ideas for designing\nfuture research infrastructures in the human and social sciences.\n", "rewritten_text": "It is common to believe that scientists have mixed feelings about standards. They are often viewed as not accurately representing the latest advancements in a particular field and as a barrier to scientific innovation. However, scientists are ideally positioned to contribute their expertise to the development of standards, as they can offer a more impartial perspective on issues that may be influenced by competing industrial interests. While it may seem more challenging to establish standards in the humanities, we will demonstrate how this can be achieved based on the experiences of the Text Encoding Initiative consortium and the International Organization for Standardization. Focusing on lexical resources, we will explore how this can inspire new approaches to designing future research infrastructures in the human and social sciences."}, "2305.04451": {"original_text": "  Virtual try-on attracts increasing research attention as a promising way for\nenhancing the user experience for online cloth shopping. Though existing\nmethods can generate impressive results, users need to provide a well-designed\nreference image containing the target fashion clothes that often do not exist.\nTo support user-friendly fashion customization in full-body portraits, we\npropose a multi-modal interactive setting by combining the advantages of both\ntext and texture for multi-level fashion manipulation. With the carefully\ndesigned fashion editing module and loss functions, FashionTex framework can\nsemantically control cloth types and local texture patterns without annotated\npairwise training data. We further introduce an ID recovery module to maintain\nthe identity of input portrait. Extensive experiments have demonstrated the\neffectiveness of our proposed pipeline.\n", "rewritten_text": "\"Virtual try-on technology is gaining attention in research as a promising method to enhance the online clothing shopping experience. While current methods can produce impressive results, users often struggle to find suitable reference images of the desired fashion items. To address this challenge and enable user-friendly fashion customization in full-body portraits, we propose a multi-modal interactive approach that combines text and texture for advanced fashion manipulation. Our FashionTex framework incorporates a carefully designed fashion editing module and loss functions to allow users to control cloth types and texture patterns without the need for annotated training data. Additionally, we introduce an ID recovery module to preserve the identity of the input portrait. Extensive experiments have validated the effectiveness of our proposed pipeline.\""}, "2310.05989": {"original_text": "  3D object detection plays a pivotal role in autonomous driving and robotics,\ndemanding precise interpretation of Bird's Eye View (BEV) images. The dynamic\nnature of real-world environments necessitates the use of dynamic query\nmechanisms in 3D object detection to adaptively capture and process the complex\nspatio-temporal relationships present in these scenes. However, prior\nimplementations of dynamic queries have often faced difficulties in effectively\nleveraging these relationships, particularly when it comes to integrating\ntemporal information in a computationally efficient manner. Addressing this\nlimitation, we introduce a framework utilizing dynamic query evolution\nstrategy, harnesses K-means clustering and Top-K attention mechanisms for\nrefined spatio-temporal data processing. By dynamically segmenting the BEV\nspace and prioritizing key features through Top-K attention, our model achieves\na real-time, focused analysis of pertinent scene elements. Our extensive\nevaluation on the nuScenes and Waymo dataset showcases a marked improvement in\ndetection accuracy, setting a new benchmark in the domain of query-based BEV\nobject detection. Our dynamic query evolution strategy has the potential to\npush the boundaries of current BEV methods with enhanced adaptability and\ncomputational efficiency. Project page:\nhttps://github.com/Jiawei-Yao0812/QE-BEV\n", "rewritten_text": "3D object detection is crucial for autonomous driving and robotics, requiring accurate interpretation of Bird's Eye View (BEV) images. Real-world environments are dynamic, leading to the need for dynamic query mechanisms in 3D object detection to effectively capture and process complex spatio-temporal relationships in scenes. Previous implementations of dynamic queries have struggled to leverage these relationships efficiently, especially in integrating temporal information in a computationally effective way. To address this challenge, we propose a framework that utilizes a dynamic query evolution strategy, incorporating K-means clustering and Top-K attention mechanisms for enhanced spatio-temporal data processing. By segmenting the BEV space dynamically and focusing on key features through Top-K attention, our model enables real-time, targeted analysis of important scene elements. Extensive evaluation on the nuScenes and Waymo datasets demonstrates significant improvements in detection accuracy, establishing a new standard in query-based BEV object detection. Our dynamic query evolution strategy has the potential to advance current BEV methods by enhancing adaptability and computational efficiency. Project page: https://github.com/Jiawei-Yao0812/QE-BEV"}, "1907.03513": {"original_text": "  Keeping up to date on emerging entities that appear every day is\nindispensable for various applications, such as social-trend analysis and\nmarketing research. Previous studies have attempted to detect unseen entities\nthat are not registered in a particular knowledge base as emerging entities and\nconsequently find non-emerging entities since the absence of entities in\nknowledge bases does not guarantee their emergence. We therefore introduce a\nnovel task of discovering truly emerging entities when they have just been\nintroduced to the public through microblogs and propose an effective method\nbased on time-sensitive distant supervision, which exploits distinctive\nearly-stage contexts of emerging entities. Experimental results with a\nlarge-scale Twitter archive show that the proposed method achieves 83.2%\nprecision of the top 500 discovered emerging entities, which outperforms\nbaselines based on unseen entity recognition with burst detection. Besides\nnotable emerging entities, our method can discover massive long-tail and\nhomographic emerging entities. An evaluation of relative recall shows that the\nmethod detects 80.4% emerging entities newly registered in Wikipedia; 92.4% of\nthem are discovered earlier than their registration in Wikipedia, and the\naverage lead-time is more than one year (571 days).\n", "rewritten_text": "Staying informed about new entities that emerge daily is crucial for various purposes, such as analyzing social trends and conducting marketing research. Previous research has aimed to identify emerging entities not found in existing knowledge bases, as well as non-emerging entities, as the absence of entities in these bases does not guarantee their emergence. This study introduces a new task of identifying truly emerging entities as they are first introduced to the public through microblogs. A method utilizing time-sensitive distant supervision is proposed, leveraging unique early-stage contexts of emerging entities. Results from experiments using a large Twitter dataset demonstrate that the proposed method achieves 83.2% precision in identifying the top 500 emerging entities, surpassing baselines that rely on detecting unseen entities with burst detection. In addition to identifying significant emerging entities, the method can also uncover numerous lesser-known emerging entities. An evaluation of relative recall indicates that the method identifies 80.4% of emerging entities newly added to Wikipedia, with 92.4% of them being discovered before their Wikipedia registration, often more than a year in advance (571 days on average)."}, "2312.04567": {"original_text": "  Recent significant advances in text-to-image models unlock the possibility of\ntraining vision systems using synthetic images, potentially overcoming the\ndifficulty of collecting curated data at scale. It is unclear, however, how\nthese models behave at scale, as more synthetic data is added to the training\nset. In this paper we study the scaling laws of synthetic images generated by\nstate of the art text-to-image models, for the training of supervised models:\nimage classifiers with label supervision, and CLIP with language supervision.\nWe identify several factors, including text prompts, classifier-free guidance\nscale, and types of text-to-image models, that significantly affect scaling\nbehavior. After tuning these factors, we observe that synthetic images\ndemonstrate a scaling trend similar to, but slightly less effective than, real\nimages in CLIP training, while they significantly underperform in scaling when\ntraining supervised image classifiers. Our analysis indicates that the main\nreason for this underperformance is the inability of off-the-shelf\ntext-to-image models to generate certain concepts, a limitation that\nsignificantly impairs the training of image classifiers. Our findings also\nsuggest that scaling synthetic data can be particularly effective in scenarios\nsuch as: (1) when there is a limited supply of real images for a supervised\nproblem (e.g., fewer than 0.5 million images in ImageNet), (2) when the\nevaluation dataset diverges significantly from the training data, indicating\nthe out-of-distribution scenario, or (3) when synthetic data is used in\nconjunction with real images, as demonstrated in the training of CLIP models.\n", "rewritten_text": "Recent advancements in text-to-image models have opened up the possibility of training vision systems using synthetic images, which could help overcome the challenge of gathering large amounts of curated data. However, the behavior of these models when scaled up with more synthetic data for training is not well understood. This study examines how synthetic images generated by state-of-the-art text-to-image models impact the scaling of supervised models, such as image classifiers with label supervision and CLIP with language supervision. Various factors, including text prompts, guidance scale, and types of text-to-image models, are found to significantly influence scaling behavior. After adjusting these factors, it is observed that synthetic images show a scaling trend similar to real images in CLIP training, but are less effective in training supervised image classifiers. The analysis suggests that the inability of standard text-to-image models to generate certain concepts is a key reason for this performance gap, hindering the training of image classifiers. The study also highlights scenarios where scaling synthetic data can be beneficial, such as when there is a shortage of real images for a supervised problem, when the evaluation dataset differs greatly from the training data, or when synthetic data is combined with real images in CLIP model training."}, "1909.1244": {"original_text": "  Recently, pre-trained language models have achieved remarkable success in a\nbroad range of natural language processing tasks. However, in multilingual\nsetting, it is extremely resource-consuming to pre-train a deep language model\nover large-scale corpora for each language. Instead of exhaustively\npre-training monolingual language models independently, an alternative solution\nis to pre-train a powerful multilingual deep language model over large-scale\ncorpora in hundreds of languages. However, the vocabulary size for each\nlanguage in such a model is relatively small, especially for low-resource\nlanguages. This limitation inevitably hinders the performance of these\nmultilingual models on tasks such as sequence labeling, wherein in-depth\ntoken-level or sentence-level understanding is essential.\n  In this paper, inspired by previous methods designed for monolingual\nsettings, we investigate two approaches (i.e., joint mapping and mixture\nmapping) based on a pre-trained multilingual model BERT for addressing the\nout-of-vocabulary (OOV) problem on a variety of tasks, including part-of-speech\ntagging, named entity recognition, machine translation quality estimation, and\nmachine reading comprehension. Experimental results show that using mixture\nmapping is more promising. To the best of our knowledge, this is the first work\nthat attempts to address and discuss the OOV issue in multilingual settings.\n", "rewritten_text": "Lately, pre-trained language models have seen significant success across various natural language processing tasks. However, when dealing with multiple languages, the process of pre-training a deep language model on extensive datasets for each language can be highly resource-intensive. Instead of individually pre-training monolingual language models, an alternative approach is to pre-train a robust multilingual deep language model on large datasets spanning hundreds of languages. Nonetheless, the vocabulary size for each language in such a model tends to be relatively small, particularly for languages with limited resources. This constraint can impede the performance of these multilingual models on tasks like sequence labeling, where a thorough understanding at the token or sentence level is crucial.\n\nIn this study, drawing inspiration from methods tailored for monolingual scenarios, we explore two strategies (joint mapping and mixture mapping) utilizing a pre-trained multilingual model BERT to tackle the out-of-vocabulary (OOV) challenge across various tasks such as part-of-speech tagging, named entity recognition, machine translation quality estimation, and machine reading comprehension. Our experimental findings indicate that the use of mixture mapping shows more promise. To the best of our knowledge, this is the first work to address and delve into the OOV problem in multilingual contexts."}, "2311.05821": {"original_text": "  While recent advances have boosted LM proficiency in linguistic benchmarks,\nLMs consistently struggle to reason correctly on complex tasks like\nmathematics. We turn to Reinforcement Learning from Human Feedback (RLHF) as a\nmethod with which to shape model reasoning processes. In particular, we explore\ntwo reward schemes, outcome-supervised reward models (ORMs) and\nprocess-supervised reward models (PRMs), to optimize for logical reasoning. Our\nresults show that the fine-grained reward provided by PRM-based methods\nenhances accuracy on simple mathematical reasoning (GSM8K) while, unexpectedly,\nreducing performance in complex tasks (MATH). Furthermore, we show the critical\nrole reward aggregation functions play in model performance. Providing\npromising avenues for future research, our study underscores the need for\nfurther exploration into fine-grained reward modeling for more reliable\nlanguage models.\n", "rewritten_text": "Recent advancements have improved language model proficiency in linguistic benchmarks, but they still struggle with complex tasks like mathematics. To address this, we investigate Reinforcement Learning from Human Feedback (RLHF) as a method to improve model reasoning processes. Specifically, we examine two reward schemes - outcome-supervised reward models (ORMs) and process-supervised reward models (PRMs) - to enhance logical reasoning. Our findings reveal that PRM-based methods with detailed rewards improve accuracy in simple mathematical reasoning (GSM8K) but unexpectedly decrease performance in complex tasks (MATH). Additionally, we highlight the importance of reward aggregation functions in model performance. This study suggests the potential of fine-grained reward modeling for more reliable language models and calls for further research in this area."}, "1309.6379": {"original_text": "  We propose a large deformation diffeomorphic metric mapping algorithm to\nalign multiple b-value diffusion weighted imaging (mDWI) data, specifically\nacquired via hybrid diffusion imaging (HYDI), denoted as LDDMM-HYDI. We then\npropose a Bayesian model for estimating the white matter atlas from HYDIs. We\nadopt the work given in Hosseinbor et al. (2012) and represent the q-space\ndiffusion signal with the Bessel Fourier orientation reconstruction (BFOR)\nsignal basis. The BFOR framework provides the representation of mDWI in the\nq-space and thus reduces memory requirement. In addition, since the BFOR signal\nbasis is orthonormal, the L2 norm that quantifies the differences in the\nq-space signals of any two mDWI datasets can be easily computed as the sum of\nthe squared differences in the BFOR expansion coefficients. In this work, we\nshow that the reorientation of the $q$-space signal due to spatial\ntransformation can be easily defined on the BFOR signal basis. We incorporate\nthe BFOR signal basis into the LDDMM framework and derive the gradient descent\nalgorithm for LDDMM-HYDI with explicit orientation optimization. Additionally,\nwe extend the previous Bayesian atlas estimation framework for scalar-valued\nimages to HYDIs and derive the expectation-maximization algorithm for solving\nthe HYDI atlas estimation problem. Using real HYDI datasets, we show the\nBayesian model generates the white matter atlas with anatomical details.\nMoreover, we show that it is important to consider the variation of mDWI\nreorientation due to a small change in diffeomorphic transformation in the\nLDDMM-HYDI optimization and to incorporate the full information of HYDI for\naligning mDWI.\n", "rewritten_text": "We introduce a method called LDDMM-HYDI, which utilizes a large deformation diffeomorphic metric mapping algorithm to align multiple b-value diffusion weighted imaging (mDWI) data obtained through hybrid diffusion imaging (HYDI). A Bayesian model is then proposed for estimating the white matter atlas from HYDIs, building upon the approach outlined in Hosseinbor et al. (2012). The q-space diffusion signal is represented using the Bessel Fourier orientation reconstruction (BFOR) signal basis within the BFOR framework, which simplifies memory requirements by providing a representation of mDWI in the q-space. The orthonormal nature of the BFOR signal basis allows for easy computation of the L2 norm to quantify differences in q-space signals between mDWI datasets. This study demonstrates that reorientation of the q-space signal following spatial transformation can be effectively defined using the BFOR signal basis. By integrating the BFOR signal basis into the LDDMM framework, a gradient descent algorithm is derived for LDDMM-HYDI with explicit orientation optimization. Furthermore, the Bayesian atlas estimation framework for scalar-valued images is extended to HYDIs, and an expectation-maximization algorithm is developed to solve the HYDI atlas estimation problem. Through the analysis of real HYDI datasets, it is shown that the Bayesian model produces a white matter atlas with detailed anatomical information. The study emphasizes the importance of considering variations in mDWI reorientation resulting from small changes in diffeomorphic transformation during LDDMM-HYDI optimization, and stresses the significance of utilizing the full information from HYDI for aligning mDWI data."}, "2101.03848": {"original_text": "  Convolutional neural networks (CNNs) have been widely used in various vision\ntasks, e.g. image classification, semantic segmentation, etc. Unfortunately,\nstandard 2D CNNs are not well suited for spherical signals such as panorama\nimages or spherical projections, as the sphere is an unstructured grid. In this\npaper, we present Spherical Transformer which can transform spherical signals\ninto vectors that can be directly processed by standard CNNs such that many\nwell-designed CNNs architectures can be reused across tasks and datasets by\npretraining. To this end, the proposed method first uses local structured\nsampling methods such as HEALPix to construct a transformer grid by using the\ninformation of spherical points and its adjacent points, and then transforms\nthe spherical signals to the vectors through the grid. By building the\nSpherical Transformer module, we can use multiple CNN architectures directly.\nWe evaluate our approach on the tasks of spherical MNIST recognition, 3D object\nclassification and omnidirectional image semantic segmentation. For 3D object\nclassification, we further propose a rendering-based projection method to\nimprove the performance and a rotational-equivariant model to improve the\nanti-rotation ability. Experimental results on three tasks show that our\napproach achieves superior performance over state-of-the-art methods.\n", "rewritten_text": "Convolutional neural networks (CNNs) are commonly used in various visual tasks such as image classification and semantic segmentation. However, traditional 2D CNNs are not ideal for processing spherical signals like panorama images due to the unstructured nature of the sphere. In this study, we introduce the Spherical Transformer, a method that converts spherical signals into vectors compatible with standard CNNs. This allows for the reuse of well-designed CNN architectures across different tasks and datasets through pretraining. The proposed approach utilizes local structured sampling techniques like HEALPix to create a transformer grid based on spherical point information and neighboring points. By employing the Spherical Transformer module, multiple CNN architectures can be directly applied. Our method is evaluated on tasks including spherical MNIST recognition, 3D object classification, and omnidirectional image semantic segmentation. For 3D object classification, we introduce a rendering-based projection technique to enhance performance and a rotational-equivariant model to improve anti-rotation capabilities. Experimental results across these tasks demonstrate that our approach outperforms existing methods."}, "2209.02686": {"original_text": "  Image-to-image translation has played an important role in enabling synthetic\ndata for computer vision. However, if the source and target domains have a\nlarge semantic mismatch, existing techniques often suffer from source content\ncorruption aka semantic flipping. To address this problem, we propose a new\nparadigm for image-to-image translation using Vector Symbolic Architectures\n(VSA), a theoretical framework which defines algebraic operations in a\nhigh-dimensional vector (hypervector) space. We introduce VSA-based constraints\non adversarial learning for source-to-target translations by learning a\nhypervector mapping that inverts the translation to ensure consistency with\nsource content. We show both qualitatively and quantitatively that our method\nimproves over other state-of-the-art techniques.\n", "rewritten_text": "Image-to-image translation has been crucial in generating synthetic data for computer vision. However, when there is a significant semantic mismatch between the source and target domains, current methods often encounter issues such as source content corruption, also known as semantic flipping. To tackle this challenge, we propose a novel approach to image-to-image translation utilizing Vector Symbolic Architectures (VSA), a theoretical framework that defines algebraic operations in a high-dimensional hypervector space. Our method introduces VSA-based constraints on adversarial learning for translating from source to target by learning a hypervector mapping that reverses the translation to maintain consistency with the source content. Through qualitative and quantitative analysis, we demonstrate that our approach outperforms existing state-of-the-art techniques."}, "2305.14576": {"original_text": "  Pre-trained language models (PLMs) have ignited a surge in demand for\neffective fine-tuning techniques, particularly in low-resource domains and\nlanguages. Active learning (AL), a set of algorithms designed to decrease\nlabeling costs by minimizing label complexity, has shown promise in confronting\nthe labeling bottleneck. In parallel, adapter modules designed for\nparameter-efficient fine-tuning (PEFT) have demonstrated notable potential in\nlow-resource settings. However, the interplay between AL and adapter-based PEFT\nremains unexplored. We present an empirical study of PEFT behavior with AL in\nlow-resource settings for text classification tasks. Our findings affirm the\nsuperiority of PEFT over full-fine tuning (FFT) in low-resource settings and\ndemonstrate that this advantage persists in AL setups. We further examine the\nproperties of PEFT and FFT through the lens of forgetting dynamics and\ninstance-level representations, where we find that PEFT yields more stable\nrepresentations of early and middle layers compared to FFT. Our research\nunderscores the synergistic potential of AL and PEFT in low-resource settings,\npaving the way for advancements in efficient and effective fine-tuning.\n", "rewritten_text": "\"Pre-trained language models (PLMs) have sparked increased interest in effective fine-tuning methods, especially in low-resource domains and languages. Active learning (AL) algorithms, which aim to reduce labeling costs by simplifying label complexity, have shown promise in addressing the labeling bottleneck. Similarly, adapter modules designed for parameter-efficient fine-tuning (PEFT) have shown significant potential in low-resource scenarios. However, the interaction between AL and adapter-based PEFT has not been thoroughly explored. In this study, we investigate the behavior of PEFT when combined with AL in low-resource settings for text classification tasks. Our results confirm the superiority of PEFT over full-fine tuning (FFT) in low-resource environments, even in AL setups. We also analyze the characteristics of PEFT and FFT in terms of forgetting dynamics and instance-level representations, revealing that PEFT provides more stable representations in early and middle layers compared to FFT. This research highlights the synergistic benefits of combining AL and PEFT in low-resource contexts, paving the way for advancements in efficient and effective fine-tuning.\""}, "2308.15448": {"original_text": "  The negative effects of online bullying and harassment are increasing with\nInternet popularity, especially in social media. One solution is using natural\nlanguage processing (NLP) and machine learning (ML) methods for the automatic\ndetection of harmful remarks, but these methods are limited in low-resource\nlanguages like the Chittagonian dialect of Bangla.This study focuses on\ndetecting vulgar remarks in social media using supervised ML and deep learning\nalgorithms.Logistic Regression achieved promising accuracy (0.91) while simple\nRNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the\nissue that NN algorithms require more data.\n", "rewritten_text": "The rise in online bullying and harassment, particularly on social media, is a growing concern as the Internet becomes more popular. One potential solution is to utilize natural language processing (NLP) and machine learning (ML) techniques to automatically identify harmful comments. However, these methods face limitations when dealing with low-resource languages such as the Chittagonian dialect of Bangla. This research focuses on identifying offensive comments on social media by employing supervised ML and deep learning algorithms. While Logistic Regression showed promising accuracy at 0.91, simpler RNN models using Word2vec and fastText achieved lower accuracy ranging from 0.84 to 0.90. This underscores the challenge that neural network algorithms face in requiring larger datasets."}, "2305.06052": {"original_text": "  Quantization is a widely adopted technique for deep neural networks to reduce\nthe memory and computational resources required. However, when quantized, most\nmodels would need a suitable calibration process to keep their performance\nintact, which requires data from the target domain, such as a fraction of the\ndataset used in model training and model validation (i.e. calibration dataset).\n  In this study, we investigate the use of synthetic data as a substitute for\nthe calibration with real data for the quantization method. We propose a data\ngeneration method based on Generative Adversarial Networks that are trained\nprior to the model quantization step. We compare the performance of models\nquantized using data generated by StyleGAN2-ADA and our pre-trained DiStyleGAN,\nwith quantization using real data and an alternative data generation method\nbased on fractal images. Overall, the results of our experiments demonstrate\nthe potential of leveraging synthetic data for calibration during the\nquantization process. In our experiments, the percentage of accuracy\ndegradation of the selected models was less than 0.6%, with our best\nperformance achieved on MobileNetV2 (0.05%). The code is available at:\nhttps://github.com/ThanosM97/gsoc2022-openvino\n", "rewritten_text": "Quantization is a commonly used method in deep neural networks to reduce memory and computational resources. However, after quantization, most models require a calibration process to maintain their performance, which involves using data from the target domain, such as a subset of the dataset used during model training and validation (referred to as the calibration dataset). This study explores the possibility of using synthetic data as a replacement for real data in the quantization process. We propose a data generation approach based on Generative Adversarial Networks that are trained before quantizing the model. We compare the performance of models quantized using data generated by StyleGAN2-ADA and our pre-trained DiStyleGAN, with models quantized using real data and an alternative data generation method based on fractal images. Our experiments show the potential of using synthetic data for calibration during quantization, with the accuracy degradation of selected models being less than 0.6% overall, and the best performance achieved on MobileNetV2 (0.05%). The code can be found at: https://github.com/ThanosM97/gsoc2022-openvino"}, "2406.15719": {"original_text": "  Convolutional Neural Networks (CNNs) and vision transformers (ViTs) have\nshown excellent capability in complex hyperspectral image (HSI) classification.\nHowever, these models require a significant number of training data and are\ncomputational resources. On the other hand, modern Multi-Layer Perceptrons\n(MLPs) have demonstrated great classification capability. These modern\nMLP-based models require significantly less training data compared to CNNs and\nViTs, achieving the state-of-the-art classification accuracy. Recently,\nKolmogorov-Arnold Networks (KANs) were proposed as viable alternatives for\nMLPs. Because of their internal similarity to splines and their external\nsimilarity to MLPs, KANs are able to optimize learned features with remarkable\naccuracy in addition to being able to learn new features. Thus, in this study,\nwe assess the effectiveness of KANs for complex HSI data classification.\nMoreover, to enhance the HSI classification accuracy obtained by the KANs, we\ndevelop and propose a Hybrid architecture utilizing 1D, 2D, and 3D KANs. To\ndemonstrate the effectiveness of the proposed KAN architecture, we conducted\nextensive experiments on three newly created HSI benchmark datasets:\nQUH-Pingan, QUH-Tangdaowan, and QUH-Qingyun. The results underscored the\ncompetitive or better capability of the developed hybrid KAN-based model across\nthese benchmark datasets over several other CNN- and ViT-based algorithms,\nincluding 1D-CNN, 2DCNN, 3D CNN, VGG-16, ResNet-50, EfficientNet, RNN, and ViT.\nThe code are publicly available at (https://github.com/aj1365/HSIConvKAN)\n", "rewritten_text": "Convolutional Neural Networks (CNNs) and vision transformers (ViTs) have demonstrated strong performance in the classification of complex hyperspectral images (HSI). However, these models require a large amount of training data and computational resources. In contrast, modern Multi-Layer Perceptrons (MLPs) have shown impressive classification capabilities while needing significantly less training data compared to CNNs and ViTs, achieving state-of-the-art accuracy. Recently, Kolmogorov-Arnold Networks (KANs) have emerged as promising alternatives to MLPs. Due to their resemblance to splines internally and MLPs externally, KANs can optimize learned features with high accuracy and adapt to new features. This study evaluates the effectiveness of KANs for complex HSI classification and introduces a Hybrid architecture combining 1D, 2D, and 3D KANs to enhance classification accuracy. Experiments were conducted on three new HSI benchmark datasets (QUH-Pingan, QUH-Tangdaowan, and QUH-Qingyun), demonstrating the competitive performance of the hybrid KAN-based model compared to various CNN- and ViT-based algorithms such as 1D-CNN, 2D-CNN, 3D-CNN, VGG-16, ResNet-50, EfficientNet, RNN, and ViT. The code is publicly available at (https://github.com/aj1365/HSIConvKAN)."}, "1905.00413": {"original_text": "  We explore the problem of view synthesis from a narrow baseline pair of\nimages, and focus on generating high-quality view extrapolations with plausible\ndisocclusions. Our method builds upon prior work in predicting a multiplane\nimage (MPI), which represents scene content as a set of RGB$\\alpha$ planes\nwithin a reference view frustum and renders novel views by projecting this\ncontent into the target viewpoints. We present a theoretical analysis showing\nhow the range of views that can be rendered from an MPI increases linearly with\nthe MPI disparity sampling frequency, as well as a novel MPI prediction\nprocedure that theoretically enables view extrapolations of up to $4\\times$ the\nlateral viewpoint movement allowed by prior work. Our method ameliorates two\nspecific issues that limit the range of views renderable by prior methods: 1)\nWe expand the range of novel views that can be rendered without depth\ndiscretization artifacts by using a 3D convolutional network architecture along\nwith a randomized-resolution training procedure to allow our model to predict\nMPIs with increased disparity sampling frequency. 2) We reduce the repeated\ntexture artifacts seen in disocclusions by enforcing a constraint that the\nappearance of hidden content at any depth must be drawn from visible content at\nor behind that depth. Please see our results video at:\nhttps://www.youtube.com/watch?v=aJqAaMNL2m4.\n", "rewritten_text": "We investigate the issue of generating new views from a pair of closely spaced images, focusing on creating high-quality view extensions with realistic disocclusions. Our approach builds on previous research involving the prediction of a multiplane image (MPI), which represents the scene as a collection of RGB$\\alpha$ planes within a specific view frustum and generates new views by projecting this content into different viewpoints. We provide a theoretical analysis demonstrating how the range of views that can be produced from an MPI grows proportionally with the MPI's disparity sampling frequency. Additionally, we introduce a new MPI prediction method that theoretically allows for view extensions up to $4\\times$ the lateral viewpoint movement permitted by previous techniques. Our approach addresses two key challenges that restrict the range of renderable views in existing methods: 1) We broaden the scope of new views that can be generated without depth discretization artifacts by utilizing a 3D convolutional network design and a randomized-resolution training process to enable our model to predict MPIs with increased disparity sampling frequency. 2) We mitigate the repetitive texture artifacts observed in disocclusions by enforcing a rule that the appearance of hidden content at any depth must be derived from visible content at or behind that depth. For more details, please refer to our results video at: https://www.youtube.com/watch?v=aJqAaMNL2m4."}, "1909.10579": {"original_text": "  Neural language models (LMs) perform well on tasks that require sensitivity\nto syntactic structure. Drawing on the syntactic priming paradigm from\npsycholinguistics, we propose a novel technique to analyze the representations\nthat enable such success. By establishing a gradient similarity metric between\nstructures, this technique allows us to reconstruct the organization of the\nLMs' syntactic representational space. We use this technique to demonstrate\nthat LSTM LMs' representations of different types of sentences with relative\nclauses are organized hierarchically in a linguistically interpretable manner,\nsuggesting that the LMs track abstract properties of the sentence.\n", "rewritten_text": "Neural language models excel at tasks involving syntactic structure. In this study, we introduce a new method inspired by psycholinguistics to analyze the representations responsible for this success. By measuring similarity between structures, we can reconstruct the organization of the models' syntactic representation space. Our findings show that LSTM language models organize representations of sentences with relative clauses in a hierarchical and linguistically interpretable way, indicating that the models capture abstract sentence properties."}, "1804.04213": {"original_text": "  We study how to synthesize novel views of human body from a single image.\nThough recent deep learning based methods work well for rigid objects, they\noften fail on objects with large articulation, like human bodies. The core step\nof existing methods is to fit a map from the observable views to novel views by\nCNNs; however, the rich articulation modes of human body make it rather\nchallenging for CNNs to memorize and interpolate the data well. To address the\nproblem, we propose a novel deep learning based pipeline that explicitly\nestimates and leverages the geometry of the underlying human body. Our new\npipeline is a composition of a shape estimation network and an image generation\nnetwork, and at the interface a perspective transformation is applied to\ngenerate a forward flow for pixel value transportation. Our design is able to\nfactor out the space of data variation and makes learning at each step much\neasier. Empirically, we show that the performance for pose-varying objects can\nbe improved dramatically. Our method can also be applied on real data captured\nby 3D sensors, and the flow generated by our methods can be used for generating\nhigh quality results in higher resolution.\n", "rewritten_text": "We are researching how to create new perspectives of the human body using just one image. While current deep learning techniques are effective for solid objects, they struggle with objects like human bodies that have complex movements. Existing methods rely on CNNs to map observable views to new perspectives, but the intricate articulation of the human body poses a challenge for CNNs to accurately process and interpolate the data. To tackle this issue, we introduce a fresh deep learning approach that explicitly considers and utilizes the geometry of the human body. Our innovative pipeline consists of a network for estimating shape and another for generating images, with a perspective transformation applied at the interface to facilitate pixel value transfer. This design simplifies the learning process by isolating data variations, leading to significant improvements in handling objects with varying poses. Our method is also compatible with real data from 3D sensors, producing high-quality results in higher resolutions through the flow generated by our approach."}, "2401.04354": {"original_text": "  With the explosive growth of video data in real-world applications, a\ncomprehensive representation of videos becomes increasingly important. In this\npaper, we address the problem of video scene recognition, whose goal is to\nlearn a high-level video representation to classify scenes in videos. Due to\nthe diversity and complexity of video contents in realistic scenarios, this\ntask remains a challenge. Most existing works identify scenes for videos only\nfrom visual or textual information in a temporal perspective, ignoring the\nvaluable information hidden in single frames, while several earlier studies\nonly recognize scenes for separate images in a non-temporal perspective. We\nargue that these two perspectives are both meaningful for this task and\ncomplementary to each other, meanwhile, externally introduced knowledge can\nalso promote the comprehension of videos. We propose a novel two-stream\nframework to model video representations from multiple perspectives, i.e.\ntemporal and non-temporal perspectives, and integrate the two perspectives in\nan end-to-end manner by self-distillation. Besides, we design a\nknowledge-enhanced feature fusion and label prediction method that contributes\nto naturally introducing knowledge into the task of video scene recognition.\nExperiments conducted on a real-world dataset demonstrate the effectiveness of\nour proposed method.\n", "rewritten_text": "In this paper, we focus on the challenge of recognizing scenes in videos by developing a comprehensive video representation. Existing approaches typically classify scenes based on visual or textual information over time or on individual frames. We argue that both temporal and non-temporal perspectives are valuable and propose a two-stream framework that integrates these perspectives through self-distillation. Additionally, we introduce external knowledge to enhance video comprehension. Our method includes a knowledge-enhanced feature fusion and label prediction approach, which we demonstrate to be effective through experiments on a real-world dataset."}, "2211.08112": {"original_text": "  Active Learning (AL) is a powerful tool for learning with less labeled data,\nin particular, for specialized domains, like legal documents, where unlabeled\ndata is abundant, but the annotation requires domain expertise and is thus\nexpensive. Recent works have shown the effectiveness of AL strategies for\npre-trained language models. However, most AL strategies require a set of\nlabeled samples to start with, which is expensive to acquire. In addition,\npre-trained language models have been shown unstable during fine-tuning with\nsmall datasets, and their embeddings are not semantically meaningful. In this\nwork, we propose a pipeline for effectively using active learning with\npre-trained language models in the legal domain. To this end, we leverage the\navailable unlabeled data in three phases. First, we continue pre-training the\nmodel to adapt it to the downstream task. Second, we use knowledge distillation\nto guide the model's embeddings to a semantically meaningful space. Finally, we\npropose a simple, yet effective, strategy to find the initial set of labeled\nsamples with fewer actions compared to existing methods. Our experiments on\nContract-NLI, adapted to the classification task, and LEDGAR benchmarks show\nthat our approach outperforms standard AL strategies, and is more efficient.\nFurthermore, our pipeline reaches comparable results to the fully-supervised\napproach with a small performance gap, and dramatically reduced annotation\ncost. Code and the adapted data will be made available.\n", "rewritten_text": "Active Learning (AL) is a valuable tool for learning with limited labeled data, especially in specialized domains such as legal documents where unlabeled data is plentiful but annotation is costly due to the need for domain expertise. Recent studies have demonstrated the effectiveness of AL strategies for pre-trained language models. However, most AL strategies typically require an initial set of labeled samples, which can be expensive to obtain. Additionally, pre-trained language models have been found to be unstable when fine-tuned with small datasets, and their embeddings may lack semantic meaning. In this study, we present a method for effectively utilizing active learning with pre-trained language models in the legal domain. Our approach involves leveraging available unlabeled data in three phases. Firstly, we continue pre-training the model to tailor it to the specific task. Secondly, we employ knowledge distillation to guide the model's embeddings towards a semantically meaningful space. Finally, we introduce a simple yet efficient strategy for identifying the initial set of labeled samples with fewer steps compared to existing methods. Our experiments on Contract-NLI and LEDGAR benchmarks, adapted for classification tasks, demonstrate that our approach surpasses standard AL strategies in terms of performance and efficiency. Furthermore, our pipeline achieves results comparable to fully-supervised methods with a minimal performance gap and significantly reduced annotation costs. The code and adapted data will be made publicly available."}, "2102.03115": {"original_text": "  Object detection in natural scenes can be a challenging task. In many\nreal-life situations, the visible spectrum is not suitable for traditional\ncomputer vision tasks. Moving outside the visible spectrum range, such as the\nthermal spectrum or the near-infrared (NIR) images, is much more beneficial in\nlow visibility conditions, NIR images are very helpful for understanding the\nobject's material quality. In this work, we have taken images with both the\nThermal and NIR spectrum for the object detection task. As multi-spectral data\nwith both Thermal and NIR is not available for the detection task, we needed to\ncollect data ourselves. Data collection is a time-consuming process, and we\nfaced many obstacles that we had to overcome. We train the YOLO v3 network from\nscratch to detect an object from multi-spectral images. Also, to avoid\noverfitting, we have done data augmentation and tune hyperparameters.\n", "rewritten_text": "Detecting objects in natural scenes can be a difficult challenge. In many real-world scenarios, traditional computer vision tasks may not be suitable in the visible spectrum. Utilizing the thermal spectrum or near-infrared (NIR) images outside the visible spectrum range can be more advantageous in low visibility conditions. NIR images are particularly useful for assessing an object's material quality. In this study, we acquired images in both the Thermal and NIR spectrum for object detection. Since multi-spectral data combining Thermal and NIR was not readily available, we had to gather the data ourselves, which was a time-consuming process with various obstacles to overcome. We trained the YOLO v3 network from the ground up to detect objects in multi-spectral images. To prevent overfitting, we implemented data augmentation and fine-tuned hyperparameters."}, "2207.09157": {"original_text": "  Supervised deep learning-based approaches have been applied to task-oriented\ndialog and have proven to be effective for limited domain and language\napplications when a sufficient number of training examples are available. In\npractice, these approaches suffer from the drawbacks of domain-driven design\nand under-resourced languages. Domain and language models are supposed to grow\nand change as the problem space evolves. On one hand, research on transfer\nlearning has demonstrated the cross-lingual ability of multilingual\nTransformers-based models to learn semantically rich representations. On the\nother, in addition to the above approaches, meta-learning have enabled the\ndevelopment of task and language learning algorithms capable of far\ngeneralization. Through this context, this article proposes to investigate the\ncross-lingual transferability of using synergistically few-shot learning with\nprototypical neural networks and multilingual Transformers-based models.\nExperiments in natural language understanding tasks on MultiATIS++ corpus shows\nthat our approach substantially improves the observed transfer learning\nperformances between the low and the high resource languages. More generally\nour approach confirms that the meaningful latent space learned in a given\nlanguage can be can be generalized to unseen and under-resourced ones using\nmeta-learning.\n", "rewritten_text": "Supervised deep learning methods have been successfully utilized in task-oriented dialogues, particularly in limited domain and language scenarios with ample training data. However, these methods face challenges related to domain-specific design and lack of resources in certain languages. As problem spaces evolve, domain and language models need to adapt accordingly. Recent research on transfer learning has shown that multilingual Transformers-based models can learn rich semantic representations across languages. Additionally, meta-learning techniques have enabled the development of algorithms that can generalize well across tasks and languages. This article proposes exploring the cross-lingual transferability by combining few-shot learning with prototypical neural networks and multilingual Transformers-based models. Experiments conducted on the MultiATIS++ corpus demonstrate significant improvements in transfer learning performance between low and high resource languages. Overall, this approach highlights the potential of leveraging meta-learning to generalize meaningful latent spaces learned in one language to unseen and under-resourced languages."}, "2203.1521": {"original_text": "  To learn camera-view invariant features for person Re-IDentification (Re-ID),\nthe cross-camera image pairs of each person play an important role. However,\nsuch cross-view training samples could be unavailable under the ISolated Camera\nSupervised (ISCS) setting, e.g., a surveillance system deployed across distant\nscenes. To handle this challenging problem, a new pipeline is introduced by\nsynthesizing the cross-camera samples in the feature space for model training.\nSpecifically, the feature encoder and generator are end-to-end optimized under\na novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint\nlearning procedure raises concern on the stability of generative model\ntraining. Therefore, a new feature generator, $\\sigma$-Regularized Conditional\nVariational Autoencoder ($\\sigma$-Reg.~CVAE), is proposed with theoretical and\nexperimental analysis on its robustness. Extensive experiments on two ISCS\nperson Re-ID datasets demonstrate the superiority of our CCSFG to the\ncompetitors.\n", "rewritten_text": "\"In order to develop features that are invariant to camera views for person re-identification (Re-ID), the cross-camera image pairs of each individual are crucial. However, obtaining such cross-view training samples may be challenging in scenarios like a surveillance system covering distant locations, known as the ISolated Camera Supervised (ISCS) setting. To address this issue, a new approach is presented that involves synthesizing cross-camera samples in the feature space for model training. This method, called Camera-Conditioned Stable Feature Generation (CCSFG), optimizes the feature encoder and generator in an end-to-end manner using a novel technique. The training of the generative model is a key focus of this joint learning process. To ensure stability in generative model training, a new feature generator called $\\sigma$-Regularized Conditional Variational Autoencoder ($\\sigma$-Reg.~CVAE) is introduced, with theoretical and experimental analysis demonstrating its robustness. Extensive experiments conducted on two ISCS person Re-ID datasets show the superior performance of CCSFG compared to other methods.\""}, "1812.0957": {"original_text": "  In recent years, we have seen the performance of video-based person\nRe-Identification (ReID) methods have improved considerably. However, most of\nthe work in this area has dealt with videos acquired by fixed cameras with\nwider field of view. Recently, widespread use of wearable cameras and recording\ndevices such as cellphones have opened the door to interesting research in\nfirst-person Point-of-view (POV) videos (egocentric videos). Nonetheless,\nanalysis of such videos is challenging due to factors such as poor video\nquality due to ego-motion, blurriness, severe changes in lighting conditions\nand perspective distortions. To facilitate the research towards conquering\nthese challenges, this paper contributes a new dataset called EgoReID. The\ndataset is captured using 3 mobile cellphones with non-overlapping\nfield-of-view. It contains 900 IDs and around 10,200 tracks with a total of\n176,000 detections. The dataset also contains 12-sensor meta data e.g. camera\norientation pitch and rotation for each video.\n  In addition, we propose a new framework which takes advantage of both visual\nand sensor meta data to successfully perform Person ReID. We extend image-based\nre-ID method employing human body parsing trained on ten datasets to\nvideo-based re-ID. In our method, first frame level local features are\nextracted for each semantic region, then 3D convolutions are applied to encode\nthe temporal information in each sequence of semantic regions. Additionally, we\nemploy sensor meta data to predict targets' next camera and their estimated\ntime of arrival, which considerably improves our ReID performance as it\nsignificantly reduces our search space.\n", "rewritten_text": "Recently, there has been significant progress in video-based person Re-Identification (ReID) methods. While most research has focused on videos from fixed cameras with wide fields of view, the rise of wearable cameras and devices like cellphones has sparked interest in first-person Point-of-view (POV) videos (egocentric videos). Analyzing these videos poses challenges such as poor quality due to ego-motion, blurriness, lighting changes, and perspective distortions. To address these challenges, this paper introduces a new dataset called EgoReID, captured using three mobile cellphones with non-overlapping fields of view. The dataset includes 900 IDs, approximately 10,200 tracks, and a total of 176,000 detections, along with 12-sensor metadata like camera orientation pitch and rotation for each video.\n\nFurthermore, a novel framework is proposed that leverages both visual and sensor metadata for effective Person ReID. This framework extends image-based re-ID methods by incorporating human body parsing trained on ten datasets into video-based re-ID. The approach involves extracting frame-level local features for each semantic region, applying 3D convolutions to encode temporal information in sequences of semantic regions, and utilizing sensor metadata to predict targets' next camera and estimated time of arrival. This integration of visual and sensor data significantly enhances ReID performance by reducing the search space."}, "2112.07928": {"original_text": "  Real-world data often follows a long-tailed distribution, which makes the\nperformance of existing classification algorithms degrade heavily. A key issue\nis that samples in tail categories fail to depict their intra-class diversity.\nHumans can imagine a sample in new poses, scenes, and view angles with their\nprior knowledge even if it is the first time to see this category. Inspired by\nthis, we propose a novel reasoning-based implicit semantic data augmentation\nmethod to borrow transformation directions from other classes. Since the\ncovariance matrix of each category represents the feature transformation\ndirections, we can sample new directions from similar categories to generate\ndefinitely different instances. Specifically, the long-tailed distributed data\nis first adopted to train a backbone and a classifier. Then, a covariance\nmatrix for each category is estimated, and a knowledge graph is constructed to\nstore the relations of any two categories. Finally, tail samples are adaptively\nenhanced via propagating information from all the similar categories in the\nknowledge graph. Experimental results on CIFAR-100-LT, ImageNet-LT, and\niNaturalist 2018 have demonstrated the effectiveness of our proposed method\ncompared with the state-of-the-art methods.\n", "rewritten_text": "Real-world data often exhibits a long-tailed distribution, leading to a significant decline in the performance of current classification algorithms. A critical challenge is that samples in less common categories do not adequately capture the diversity within their class. Unlike machines, humans can envision new poses, scenes, and perspectives for a sample based on prior knowledge, even if encountering the category for the first time. Drawing inspiration from this human ability, we introduce a novel reasoning-based implicit semantic data augmentation approach that leverages transformation directions from other classes. By utilizing the covariance matrix of each category to represent feature transformation directions, we can extract new directions from similar categories to create distinct instances. The process involves training a backbone and a classifier on the long-tailed data, estimating a covariance matrix for each category, constructing a knowledge graph to capture relationships between categories, and enhancing tail samples by sharing information with similar categories in the knowledge graph. Our experimental results on CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018 showcase the superior effectiveness of our proposed method compared to state-of-the-art techniques."}, "2208.07291": {"original_text": "  One of the possible dangers that older people face in their daily lives is\nfalling. Occlusion is one of the biggest challenges of vision-based fall\ndetection systems and degrades their detection performance considerably. To\ntackle this problem, we synthesize specifically-designed occluded videos for\ntraining fall detection systems using existing datasets. Then, by defining a\nnew cost function, we introduce a framework for weighted training of fall\ndetection models using occluded and un-occluded videos, which can be applied to\nany learnable fall detection system. Finally, we use both a non-deep and deep\nmodel to evaluate the effect of the proposed weighted training method.\nExperiments show that the proposed method can improve the classification\naccuracy by 36% for a non-deep model and 55% for a deep model in occlusion\nconditions. Moreover, it is shown that the proposed training framework can also\nsignificantly improve the detection performance of a deep network on normal\nun-occluded samples.\n", "rewritten_text": "Older individuals are at risk of falling in their daily lives, with occlusion posing a significant challenge to vision-based fall detection systems. To address this issue, we create occluded videos for training these systems using existing datasets. By introducing a new cost function, we establish a framework for weighted training of fall detection models using both occluded and un-occluded videos, applicable to any learnable fall detection system. Our study evaluates the impact of this weighted training method using both non-deep and deep models. Results indicate a 36% improvement in classification accuracy for non-deep models and a 55% improvement for deep models under occlusion conditions. Additionally, the proposed training framework enhances the detection performance of deep networks on normal un-occluded samples."}, "2101.0219": {"original_text": "  Object detection and semantic segmentation are two of the most widely adopted\ndeep learning algorithms in agricultural applications. One of the major sources\nof variability in image quality acquired in the outdoors for such tasks is\nchanging lighting condition that can alter the appearance of the objects or the\ncontents of the entire image. While transfer learning and data augmentation to\nsome extent reduce the need for large amount of data to train deep neural\nnetworks, the large variety of cultivars and the lack of shared datasets in\nagriculture makes wide-scale field deployments difficult. In this paper, we\npresent a high throughput robust active lighting-based camera system that\ngenerates consistent images in all lighting conditions. We detail experiments\nthat show the consistency in images quality leading to relatively fewer images\nto train deep neural networks for the task of object detection. We further\npresent results from field experiment under extreme lighting conditions where\nimages without active lighting significantly lack to provide consistent\nresults. The experimental results show that on average, deep nets for object\ndetection trained on consistent data required nearly four times less data to\nachieve similar level of accuracy. This proposed work could potentially provide\npragmatic solutions to computer vision needs in agriculture.\n", "rewritten_text": "Object detection and semantic segmentation are two of the most commonly used deep learning algorithms in agricultural applications. One of the main challenges faced when working with images taken outdoors is the variability in lighting conditions, which can impact the appearance of objects and the overall image content. While techniques like transfer learning and data augmentation can help reduce the amount of data needed to train deep neural networks, the lack of shared datasets and the wide variety of cultivars in agriculture make it difficult to deploy these algorithms on a large scale.\n\nThis paper introduces a robust active lighting-based camera system that produces consistent images in all lighting conditions, aiming to address the issues related to image quality variability. The experiments conducted demonstrate the consistency in image quality, which in turn reduces the number of images required to train deep neural networks for object detection tasks. Field experiments conducted under extreme lighting conditions highlight the importance of active lighting in ensuring consistent results, as images taken without active lighting showed significant inconsistencies.\n\nThe results of the experiments indicate that deep neural networks trained on consistent data require nearly four times less data to achieve similar levels of accuracy. This proposed approach has the potential to offer practical solutions to the computer vision challenges in agriculture."}, "1911.03828": {"original_text": "  Wasserstein autoencoders are effective for text generation. They do not\nhowever provide any control over the style and topic of the generated sentences\nif the dataset has multiple classes and includes different topics. In this\nwork, we present a semi-supervised approach for generating stylized sentences.\nOur model is trained on a multi-class dataset and learns the latent\nrepresentation of the sentences using a mixture of Gaussian prior without any\nadversarial losses. This allows us to generate sentences in the style of a\nspecified class or multiple classes by sampling from their corresponding prior\ndistributions. Moreover, we can train our model on relatively small datasets\nand learn the latent representation of a specified class by adding external\ndata with other styles/classes to our dataset. While a simple WAE or VAE cannot\ngenerate diverse sentences in this case, generated sentences with our approach\nare diverse, fluent, and preserve the style and the content of the desired\nclasses.\n", "rewritten_text": "\"Wasserstein autoencoders are effective for text generation, but they lack control over the style and topic of generated sentences when the dataset contains multiple classes and topics. In this study, we introduce a semi-supervised method for generating stylized sentences. Our model is trained on a multi-class dataset and captures the latent representation of sentences using a mixture of Gaussian prior without adversarial losses. This enables us to generate sentences in the style of specific classes or multiple classes by sampling from their respective prior distributions. Additionally, we can enhance our model's training on small datasets by incorporating external data with different styles or classes. Unlike a basic WAE or VAE, our approach produces diverse, fluent sentences that maintain the style and content of the desired classes.\""}, "2106.01656": {"original_text": "  Many variants of unsupervised domain adaptation (UDA) problems have been\nproposed and solved individually. Its side effect is that a method that works\nfor one variant is often ineffective for or not even applicable to another,\nwhich has prevented practical applications. In this paper, we give a general\nrepresentation of UDA problems, named Generalized Domain Adaptation (GDA). GDA\ncovers the major variants as special cases, which allows us to organize them in\na comprehensive framework. Moreover, this generalization leads to a new\nchallenging setting where existing methods fail, such as when domain labels are\nunknown, and class labels are only partially given to each domain. We propose a\nnovel approach to the new setting. The key to our approach is self-supervised\nclass-destructive learning, which enables the learning of class-invariant\nrepresentations and domain-adversarial classifiers without using any domain\nlabels. Extensive experiments using three benchmark datasets demonstrate that\nour method outperforms the state-of-the-art UDA methods in the new setting and\nthat it is competitive in existing UDA variations as well.\n", "rewritten_text": "Numerous versions of unsupervised domain adaptation (UDA) problems have been proposed and addressed individually. This has resulted in a situation where a method effective for one variant may not work for or be applicable to another, hindering practical applications. In this paper, we introduce a broad representation of UDA problems called Generalized Domain Adaptation (GDA). GDA encompasses the main variants as specific instances, allowing for their organization within a comprehensive framework. Furthermore, this generalization presents a new challenging scenario where current methods struggle, such as when domain labels are unknown and class labels are only partially provided for each domain. We present a novel approach to address this new scenario, focusing on self-supervised class-destructive learning. This approach facilitates the learning of class-invariant representations and domain-adversarial classifiers without the need for domain labels. Extensive experiments conducted on three benchmark datasets demonstrate that our method surpasses the current state-of-the-art UDA methods in the new scenario and remains competitive in existing UDA variations."}, "2305.07152": {"original_text": "  The ability to automatically detect and track surgical instruments in\nendoscopic videos can enable transformational interventions. Assessing surgical\nperformance and efficiency, identifying skilled tool use and choreography, and\nplanning operational and logistical aspects of OR resources are just a few of\nthe applications that could benefit. Unfortunately, obtaining the annotations\nneeded to train machine learning models to identify and localize surgical tools\nis a difficult task. Annotating bounding boxes frame-by-frame is tedious and\ntime-consuming, yet large amounts of data with a wide variety of surgical tools\nand surgeries must be captured for robust training. Moreover, ongoing annotator\ntraining is needed to stay up to date with surgical instrument innovation. In\nrobotic-assisted surgery, however, potentially informative data like timestamps\nof instrument installation and removal can be programmatically harvested. The\nability to rely on tool installation data alone would significantly reduce the\nworkload to train robust tool-tracking models. With this motivation in mind we\ninvited the surgical data science community to participate in the challenge,\nSurgToolLoc 2022. The goal was to leverage tool presence data as weak labels\nfor machine learning models trained to detect tools and localize them in video\nframes with bounding boxes. We present the results of this challenge along with\nmany of the team's efforts. We conclude by discussing these results in the\nbroader context of machine learning and surgical data science. The training\ndata used for this challenge consisting of 24,695 video clips with tool\npresence labels is also being released publicly and can be accessed at\nhttps://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022.\n", "rewritten_text": "The ability to automatically detect and track surgical instruments in endoscopic videos has the potential to revolutionize medical interventions. This technology can be utilized for evaluating surgical performance, identifying skilled tool usage, planning OR resources, and more. However, obtaining the necessary annotations to train machine learning models for this task is challenging. Annotating bounding boxes frame by frame is laborious, and a large dataset with diverse surgical tools and procedures is required for effective training. Continuous training of annotators is also essential to keep up with advancements in surgical instruments. In robotic-assisted surgery, valuable data such as timestamps of instrument usage can be automatically collected. Leveraging this data can significantly reduce the workload of training tool-tracking models. To address these challenges, we launched the SurgToolLoc 2022 challenge, inviting the surgical data science community to participate. The goal was to use tool presence data as weak labels to train machine learning models for detecting and localizing tools in video frames. The results of this challenge, along with the team's efforts, are presented, and the broader implications for machine learning and surgical data science are discussed. The training data used in this challenge, comprising 24,695 video clips with tool presence labels, is now publicly available at https://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022."}, "2403.01858": {"original_text": "  We present TMMLU+, a new benchmark designed for Traditional Chinese language\nunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66\nsubjects from elementary to professional level. It is six times larger and\nboasts a more balanced subject distribution than its predecessor, Taiwan\nMassive Multitask Language Understanding (TMMLU). We also benchmark\nclosed-source models and 26 open-weight Chinese large language models (LLMs) of\nparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings reveal\nthat (1.) Traditional Chinese models still trail behind their Simplified\nChinese counterparts, highlighting a need for more focused advancements in LLMs\ncatering to Traditional Chinese. (2.) Current LLMs still fall short of human\nperformance in average scores, indicating a potential need for future research\nto delve deeper into social science and humanities subjects. (3.) Among all the\ntokenization compression metrics examined, we identify that only the fertility\nscore uniquely demonstrates strong correlations with our benchmark results. We\nforesee that TMMLU+ will pinpoint areas for future model improvement, thereby\nnarrowing the gap between machine and human linguistic capabilities and\nsupporting researchers in developing Traditional Chinese LLMs. Our dataset,\nalong with the benchmark source code, is accessible at\nhuggingface.co/datasets/ikala/tmmluplus.\n", "rewritten_text": "We introduce TMMLU+, a new evaluation standard created for understanding the Traditional Chinese language. TMMLU+ is a multiple-choice question-answering dataset featuring 66 subjects spanning from basic to advanced levels. It is six times larger and has a more evenly distributed range of subjects compared to its predecessor, Taiwan Massive Multitask Language Understanding (TMMLU). We evaluate both proprietary models and 26 publicly available Chinese large language models (LLMs) with parameters ranging from 1.8B to 72B using TMMLU+. Our analysis reveals several key findings: (1.) Traditional Chinese models are still lagging behind Simplified Chinese models, indicating a need for targeted advancements in LLMs tailored to Traditional Chinese. (2.) Current LLMs are not yet achieving human-level performance on average scores, suggesting a potential for further research in social sciences and humanities subjects. (3.) Among various tokenization compression metrics examined, only the fertility score shows strong correlations with our benchmark results. We anticipate that TMMLU+ will identify areas for enhancing future models, bridging the gap between machine and human language capabilities, and aiding researchers in developing Traditional Chinese LLMs. Our dataset, along with the benchmark source code, can be accessed at huggingface.co/datasets/ikala/tmmluplus."}, "2005.04621": {"original_text": "  Deep convolutional neural networks generally perform well in underwater\nobject recognition tasks on both optical and sonar images. Many such methods\nrequire hundreds, if not thousands, of images per class to generalize well to\nunseen examples. However, obtaining and labeling sufficiently large volumes of\ndata can be relatively costly and time-consuming, especially when observing\nrare objects or performing real-time operations. Few-Shot Learning (FSL)\nefforts have produced many promising methods to deal with low data\navailability. However, little attention has been given in the underwater\ndomain, where the style of images poses additional challenges for object\nrecognition algorithms. To the best of our knowledge, this is the first paper\nto evaluate and compare several supervised and semi-supervised Few-Shot\nLearning (FSL) methods using underwater optical and side-scan sonar imagery.\nOur results show that FSL methods offer a significant advantage over the\ntraditional transfer learning methods that fine-tune pre-trained models. We\nhope that our work will help apply FSL to autonomous underwater systems and\nexpand their learning capabilities.\n", "rewritten_text": "\"Convolutional neural networks, particularly deep ones, are effective in recognizing underwater objects in both optical and sonar images. Many existing methods require a large number of images per class to perform well on new examples. However, collecting and labeling such a large dataset can be costly and time-consuming, especially for rare objects or real-time applications. Few-Shot Learning (FSL) techniques have shown promise in addressing limited data availability, but there has been limited focus on applying them in the underwater domain, where image characteristics present unique challenges for object recognition algorithms. This study is the first to assess and compare various supervised and semi-supervised FSL methods using underwater optical and side-scan sonar images. Our findings demonstrate that FSL methods offer significant advantages over traditional transfer learning approaches that fine-tune pre-trained models. We believe that our research will facilitate the integration of FSL into autonomous underwater systems and enhance their learning capabilities.\""}, "1712.01821": {"original_text": "  Factored neural machine translation (FNMT) is founded on the idea of using\nthe morphological and grammatical decomposition of the words (factors) at the\noutput side of the neural network. This architecture addresses two well-known\nproblems occurring in MT, namely the size of target language vocabulary and the\nnumber of unknown tokens produced in the translation. FNMT system is designed\nto manage larger vocabulary and reduce the training time (for systems with\nequivalent target language vocabulary size). Moreover, we can produce\ngrammatically correct words that are not part of the vocabulary. FNMT model is\nevaluated on IWSLT'15 English to French task and compared to the baseline\nword-based and BPE-based NMT systems. Promising qualitative and quantitative\nresults (in terms of BLEU and METEOR) are reported.\n", "rewritten_text": "\"Factored neural machine translation (FNMT) is based on the concept of utilizing the morphological and grammatical breakdown of words (factors) at the output end of the neural network. This structure tackles two common issues in machine translation: the size of the target language vocabulary and the generation of unknown tokens during translation. The FNMT system is designed to handle larger vocabularies and reduce training time (compared to systems with similar target language vocabulary sizes). Additionally, it can generate grammatically correct words that are not part of the vocabulary. The FNMT model is assessed on the IWSLT'15 English to French task and compared against baseline word-based and BPE-based NMT systems, showing promising qualitative and quantitative results in terms of BLEU and METEOR scores.\""}, "2206.01988": {"original_text": "  Automatic generation of ophthalmic reports using data-driven neural networks\nhas great potential in clinical practice. When writing a report,\nophthalmologists make inferences with prior clinical knowledge. This knowledge\nhas been neglected in prior medical report generation methods. To endow models\nwith the capability of incorporating expert knowledge, we propose a Cross-modal\nclinical Graph Transformer (CGT) for ophthalmic report generation (ORG), in\nwhich clinical relation triples are injected into the visual features as prior\nknowledge to drive the decoding procedure. However, two major common Knowledge\nNoise (KN) issues may affect models' effectiveness. 1) Existing general\nbiomedical knowledge bases such as the UMLS may not align meaningfully to the\nspecific context and language of the report, limiting their utility for\nknowledge injection. 2) Incorporating too much knowledge may divert the visual\nfeatures from their correct meaning. To overcome these limitations, we design\nan automatic information extraction scheme based on natural language processing\nto obtain clinical entities and relations directly from in-domain training\nreports. Given a set of ophthalmic images, our CGT first restores a sub-graph\nfrom the clinical graph and injects the restored triples into visual features.\nThen visible matrix is employed during the encoding procedure to limit the\nimpact of knowledge. Finally, reports are predicted by the encoded cross-modal\nfeatures via a Transformer decoder. Extensive experiments on the large-scale\nFFA-IR benchmark demonstrate that the proposed CGT is able to outperform\nprevious benchmark methods and achieve state-of-the-art performances.\n", "rewritten_text": "The potential of using data-driven neural networks to automatically generate ophthalmic reports in clinical practice is significant. Traditionally, ophthalmologists rely on their clinical knowledge to write reports, but this expertise has not been fully utilized in previous report generation methods. To address this gap, we introduce a Cross-modal clinical Graph Transformer (CGT) for ophthalmic report generation (ORG). This model incorporates clinical relation triples as prior knowledge to guide the report writing process. However, there are two common Knowledge Noise (KN) issues that can impact the effectiveness of the models. Firstly, existing biomedical knowledge bases like UMLS may not align well with the specific context and language of the reports, limiting their usefulness for knowledge injection. Secondly, injecting too much knowledge may distort the visual features' intended meaning. To overcome these challenges, we develop an automatic information extraction method using natural language processing to extract clinical entities and relations directly from training reports in the same domain. The CGT then restores a sub-graph from the clinical graph and injects the relevant triples into visual features. A visible matrix is used during encoding to control the influence of knowledge. Finally, reports are generated using a Transformer decoder based on the encoded cross-modal features. Extensive experiments on the FFA-IR benchmark show that our proposed CGT outperforms previous methods and achieves state-of-the-art results."}, "1602.01125": {"original_text": "  We propose a fully automatic method for fitting a 3D morphable model to\nsingle face images in arbitrary pose and lighting. Our approach relies on\ngeometric features (edges and landmarks) and, inspired by the iterated closest\npoint algorithm, is based on computing hard correspondences between model\nvertices and edge pixels. We demonstrate that this is superior to previous work\nthat uses soft correspondences to form an edge-derived cost surface that is\nminimised by nonlinear optimisation.\n", "rewritten_text": "We suggest an entirely automated technique for adapting a 3D morphable model to individual face images regardless of their pose and lighting conditions. Our method utilizes geometric characteristics (such as edges and landmarks) and is influenced by the iterated closest point algorithm. It involves establishing precise connections between model vertices and edge pixels. Our research shows that this approach outperforms previous methods that rely on soft connections to create a cost surface derived from edges, which is then minimized through nonlinear optimization."}, "2405.11862": {"original_text": "  Table structure recognition (TSR) aims to parse the inherent structure of a\ntable from its input image. The `\"split-and-merge\" paradigm is a pivotal\napproach to parse table structure, where the table separation line detection is\ncrucial. However, challenges such as wireless and deformed tables make it\ndemanding. In this paper, we adhere to the \"split-and-merge\" paradigm and\npropose SEMv3 (SEM: Split, Embed and Merge), a method that is both fast and\nrobust for detecting table separation lines. During the split stage, we\nintroduce a Keypoint Offset Regression (KOR) module, which effectively detects\ntable separation lines by directly regressing the offset of each line relative\nto its keypoint proposals. Moreover, in the merge stage, we define a series of\nmerge actions to efficiently describe the table structure based on table grids.\nExtensive ablation studies demonstrate that our proposed KOR module can detect\ntable separation lines quickly and accurately. Furthermore, on public datasets\n(e.g. WTW, ICDAR-2019 cTDaR Historical and iFLYTAB), SEMv3 achieves\nstate-of-the-art (SOTA) performance. The code is available at\nhttps://github.com/Chunchunwumu/SEMv3.\n", "rewritten_text": "The goal of Table Structure Recognition (TSR) is to analyze the inherent structure of a table from its input image. The \"split-and-merge\" method is a key approach for parsing table structure, with a focus on detecting table separation lines. However, challenges like wireless connections and distorted tables make this task difficult. This paper introduces SEMv3 (SEM: Split, Embed and Merge), a fast and robust method that follows the \"split-and-merge\" approach for detecting table separation lines. In the split stage, a Keypoint Offset Regression (KOR) module is introduced to accurately detect table separation lines by regressing the offset of each line relative to its keypoint proposals. Additionally, in the merge stage, a series of merge actions are defined to efficiently describe the table structure based on table grids. Extensive studies show that the proposed KOR module can quickly and accurately detect table separation lines. SEMv3 achieves state-of-the-art performance on public datasets such as WTW, ICDAR-2019 cTDaR Historical, and iFLYTAB. The code can be found at https://github.com/Chunchunwumu/SEMv3."}, "1505.04424": {"original_text": "  In this work, we propose a novel microaneurysm (MA) detection for early\ndiabetic retinopathy screening using color fundus images. Since MA usually the\nfirst lesions to appear as an indicator of diabetic retinopathy, accurate\ndetection of MA is necessary for treatment. Each pixel of the image is\nclassified as either MA or non-MA using a deep neural network with dropout\ntraining procedure using maxout activation function. No preprocessing step or\nmanual feature extraction is required. Substantial improvements over standard\nMA detection method based on the pipeline of preprocessing, feature extraction,\nclassification followed by post processing is achieved. The presented method is\nevaluated in publicly available Retinopathy Online Challenge (ROC) and\nDiaretdb1v2 database and achieved state-of-the-art accuracy.\n", "rewritten_text": "\"In this study, we introduce a new method for detecting microaneurysms (MAs) in early diabetic retinopathy screening using color fundus images. MAs are typically the first signs of diabetic retinopathy, making their accurate detection crucial for effective treatment. Our approach involves classifying each pixel in the image as either an MA or non-MA using a deep neural network with dropout training and maxout activation function, eliminating the need for preprocessing or manual feature extraction. Our method outperforms traditional MA detection methods that involve preprocessing, feature extraction, classification, and post-processing steps. We evaluated our approach on the Retinopathy Online Challenge (ROC) and Diaretdb1v2 databases, achieving state-of-the-art accuracy.\""}, "2206.13156": {"original_text": "  Transformer has been widely used in histopathology whole slide image (WSI)\nclassification for the purpose of tumor grading, prognosis analysis, etc.\nHowever, the design of token-wise self-attention and positional embedding\nstrategy in the common Transformer limits the effectiveness and efficiency in\nthe application to gigapixel histopathology images. In this paper, we propose a\nkernel attention Transformer (KAT) for histopathology WSI classification. The\ninformation transmission of the tokens is achieved by cross-attention between\nthe tokens and a set of kernels related to a set of positional anchors on the\nWSI. Compared to the common Transformer structure, the proposed KAT can better\ndescribe the hierarchical context information of the local regions of the WSI\nand meanwhile maintains a lower computational complexity. The proposed method\nwas evaluated on a gastric dataset with 2040 WSIs and an endometrial dataset\nwith 2560 WSIs, and was compared with 6 state-of-the-art methods. The\nexperimental results have demonstrated the proposed KAT is effective and\nefficient in the task of histopathology WSI classification and is superior to\nthe state-of-the-art methods. The code is available at\nhttps://github.com/zhengyushan/kat.\n", "rewritten_text": "The use of Transformers in histopathology whole slide image (WSI) classification, such as for tumor grading and prognosis analysis, has been widespread. However, the traditional Transformer's token-wise self-attention and positional embedding approach may limit its effectiveness and efficiency when applied to gigapixel histopathology images. In this study, we introduce a kernel attention Transformer (KAT) specifically designed for histopathology WSI classification. The KAT facilitates information exchange among tokens through cross-attention with a set of kernels associated with positional anchors on the WSI. Compared to the standard Transformer architecture, the KAT can more effectively capture hierarchical context information of local WSI regions while maintaining lower computational complexity. We evaluated the proposed method on gastric and endometrial datasets comprising 2040 and 2560 WSIs, respectively, and compared it with six state-of-the-art methods. Our experimental results demonstrate that the KAT outperforms existing methods in terms of effectiveness and efficiency for histopathology WSI classification. The code for the proposed method is available at https://github.com/zhengyushan/kat."}, "1610.00291": {"original_text": "  We present a novel method for constructing Variational Autoencoder (VAE).\nInstead of using pixel-by-pixel loss, we enforce deep feature consistency\nbetween the input and the output of a VAE, which ensures the VAE's output to\npreserve the spatial correlation characteristics of the input, thus leading the\noutput to have a more natural visual appearance and better perceptual quality.\nBased on recent deep learning works such as style transfer, we employ a\npre-trained deep convolutional neural network (CNN) and use its hidden features\nto define a feature perceptual loss for VAE training. Evaluated on the CelebA\nface dataset, we show that our model produces better results than other methods\nin the literature. We also show that our method can produce latent vectors that\ncan capture the semantic information of face expressions and can be used to\nachieve state-of-the-art performance in facial attribute prediction.\n", "rewritten_text": "We introduce a new approach for building Variational Autoencoder (VAE) models. Instead of relying on pixel-level loss, we emphasize the consistency of deep features between the input and output of the VAE. This ensures that the VAE's output maintains the spatial correlation characteristics of the input, resulting in a more natural visual appearance and improved perceptual quality. Drawing inspiration from recent advancements in deep learning, such as style transfer, we utilize a pre-trained deep convolutional neural network (CNN) to establish a feature perceptual loss for VAE training. Through evaluation on the CelebA face dataset, we demonstrate that our model outperforms existing methods in the field. Furthermore, we illustrate that our technique can generate latent vectors capable of capturing the semantic details of facial expressions, leading to cutting-edge performance in facial attribute prediction."}, "2206.00227": {"original_text": "  A data augmentation module is utilized in contrastive learning to transform\nthe given data example into two views, which is considered essential and\nirreplaceable. However, the predetermined composition of multiple data\naugmentations brings two drawbacks. First, the artificial choice of\naugmentation types brings specific representational invariances to the model,\nwhich have different degrees of positive and negative effects on different\ndownstream tasks. Treating each type of augmentation equally during training\nmakes the model learn non-optimal representations for various downstream tasks\nand limits the flexibility to choose augmentation types beforehand. Second, the\nstrong data augmentations used in classic contrastive learning methods may\nbring too much invariance in some cases, and fine-grained information that is\nessential to some downstream tasks may be lost. This paper proposes a general\nmethod to alleviate these two problems by considering where and what to\ncontrast in a general contrastive learning framework. We first propose to learn\ndifferent augmentation invariances at different depths of the model according\nto the importance of each data augmentation instead of learning\nrepresentational invariances evenly in the backbone. We then propose to expand\nthe contrast content with augmentation embeddings to reduce the misleading\neffects of strong data augmentations. Experiments based on several baseline\nmethods demonstrate that we learn better representations for various benchmarks\non classification, detection, and segmentation downstream tasks.\n", "rewritten_text": "In contrastive learning, a data augmentation module is used to create two different views of a given data example, which is considered crucial. However, using a fixed combination of multiple data augmentations has two drawbacks. Firstly, the specific choice of augmentation types introduces certain representational invariances to the model, impacting downstream tasks differently. Treating all augmentation types equally during training can lead to suboptimal representations for various tasks and limits the ability to select augmentation types beforehand. Secondly, the strong data augmentations commonly used in traditional contrastive learning methods may introduce excessive invariance in some cases, potentially losing fine-grained information crucial for certain tasks. This paper proposes a solution to these issues by determining where and what to contrast within a general contrastive learning framework. The approach involves learning different augmentation invariances at various depths of the model based on the importance of each augmentation, rather than evenly distributing representational invariances in the backbone. Additionally, expanding the contrast content with augmentation embeddings aims to mitigate the distorting effects of strong data augmentations. Experimental results using multiple baseline methods show improved representations for classification, detection, and segmentation tasks."}, "1711.0719": {"original_text": "  We present a stochastic first-order optimization algorithm, named BCSC, that\nadds a cyclic constraint to stochastic block-coordinate descent. It uses\ndifferent subsets of the data to update different subsets of the parameters,\nthus limiting the detrimental effect of outliers in the training set. Empirical\ntests in benchmark datasets show that our algorithm outperforms\nstate-of-the-art optimization methods in both accuracy as well as convergence\nspeed. The improvements are consistent across different architectures, and can\nbe combined with other training techniques and regularization methods.\n", "rewritten_text": "Here is a rephrased version of the text:\n\n\"We introduce BCSC, a stochastic first-order optimization algorithm that incorporates a cyclic constraint into stochastic block-coordinate descent. By updating various parameter subsets with different data subsets, BCSC mitigates the impact of outliers in the training set. Experimental results on benchmark datasets demonstrate that our algorithm surpasses current optimization methods in terms of accuracy and convergence speed. These enhancements are observed consistently across various architectures and can be integrated with other training techniques and regularization methods.\""}, "2008.11872": {"original_text": "  In Viticulture, visual inspection of the plant is a necessary task for\nmeasuring relevant variables. In many cases, these visual inspections are\nsusceptible to automation through computer vision methods. Bud detection is one\nsuch visual task, central for the measurement of important variables such as:\nmeasurement of bud sunlight exposure, autonomous pruning, bud counting,\ntype-of-bud classification, bud geometric characterization, internode length,\nbud area, and bud development stage, among others. This paper presents a\ncomputer method for grapevine bud detection based on a Fully Convolutional\nNetworks MobileNet architecture (FCN-MN). To validate its performance, this\narchitecture was compared in the detection task with a strong method for bud\ndetection, Scanning Windows (SW) based on a patch classifier, showing\nimprovements over three aspects of detection: segmentation, correspondence\nidentification and localization. The best version of FCN-MN showed a detection\nF1-measure of $88.6\\%$ (for true positives defined as detected components whose\nintersection-over-union with the true bud is above $0.5$), and false positives\nthat are small and near the true bud. Splits -- false positives overlapping the\ntrue bud -- showed a mean segmentation precision of $89.3\\% (21.7)$, while\nfalse alarms -- false positives not overlapping the true bud -- showed a mean\npixel area of only $8\\%$ the area of a true bud, and a distance (between mass\ncenters) of $1.1$ true bud diameters. The paper concludes by discussing how\nthese results for FCN-MN would produce sufficiently accurate measurements of\nbud variables such as bud number, bud area, and internode length, suggesting a\ngood performance in a practical setup.\n", "rewritten_text": "In Viticulture, visually inspecting plants is essential for measuring important variables. Many of these inspections can be automated using computer vision techniques. Bud detection is a key visual task for various measurements, such as assessing bud sunlight exposure, autonomous pruning, bud counting, bud classification, bud shape analysis, internode length, bud area, and bud growth stage. This study introduces a computerized method for detecting grapevine buds using a Fully Convolutional Networks MobileNet architecture (FCN-MN). The performance of this architecture was compared to a robust bud detection method called Scanning Windows (SW) based on a patch classifier, demonstrating improvements in segmentation, correspondence identification, and localization. The optimal version of FCN-MN achieved an F1-measure of 88.6% for true positives (detected components with an intersection-over-union above 0.5) with small false positives close to the actual bud. False positives overlapping the true bud had a mean segmentation precision of 89.3% and false alarms not overlapping the bud had a mean pixel area only 8% of a true bud, with a distance of 1.1 true bud diameters between their mass centers. The study concludes by suggesting that the results from FCN-MN can provide accurate measurements of bud variables like bud number, area, and internode length, indicating good performance in practical applications."}, "1703.10798": {"original_text": "  We present a system for converting a fully panoramic ($360^\\circ$) video into\na normal field-of-view (NFOV) hyperlapse for an optimal viewing experience. Our\nsystem exploits visual saliency and semantics to non-uniformly sample in space\nand time for generating hyperlapses. In addition, users can optionally choose\nobjects of interest for customizing the hyperlapses. We first stabilize an\ninput $360^\\circ$ video by smoothing the rotation between adjacent frames and\nthen compute regions of interest and saliency scores. An initial hyperlapse is\ngenerated by optimizing the saliency and motion smoothness followed by the\nsaliency-aware frame selection. We further smooth the result using an efficient\n2D video stabilization approach that adaptively selects the motion model to\ngenerate the final hyperlapse. We validate the design of our system by showing\nresults for a variety of scenes and comparing against the state-of-the-art\nmethod through a user study.\n", "rewritten_text": "We introduce a system that transforms a fully panoramic ($360^\\circ$) video into a standard field-of-view (NFOV) hyperlapse to enhance the viewing experience. Our system leverages visual saliency and semantics to selectively sample in space and time for hyperlapse creation. Users also have the option to personalize hyperlapses by selecting specific objects of interest. Initially, we stabilize a $360^\\circ$ video by smoothing frame rotations and then identify regions of interest and saliency scores. A preliminary hyperlapse is produced by optimizing saliency and motion smoothness, followed by saliency-driven frame selection. We further enhance the result using a 2D video stabilization method that dynamically selects the motion model for the final hyperlapse generation. We validate our system's design by showcasing results across various scenes and comparing them with the current state-of-the-art method through a user study."}, "2001.03182": {"original_text": "  Unsupervised domain adaptation algorithms aim to transfer the knowledge\nlearned from one domain to another (e.g., synthetic to real images). The\nadapted representations often do not capture pixel-level domain shifts that are\ncrucial for dense prediction tasks (e.g., semantic segmentation). In this\npaper, we present a novel pixel-wise adversarial domain adaptation algorithm.\nBy leveraging image-to-image translation methods for data augmentation, our key\ninsight is that while the translated images between domains may differ in\nstyles, their predictions for the task should be consistent. We exploit this\nproperty and introduce a cross-domain consistency loss that enforces our\nadapted model to produce consistent predictions. Through extensive experimental\nresults, we show that our method compares favorably against the\nstate-of-the-art on a wide variety of unsupervised domain adaptation tasks.\n", "rewritten_text": "Unsupervised domain adaptation techniques are designed to transfer knowledge acquired in one domain to another, such as from synthetic to real images. However, the adapted representations often fail to account for pixel-level differences between domains, which are crucial for tasks like semantic segmentation. This paper introduces a new approach to domain adaptation using pixel-wise adversarial techniques. By utilizing image-to-image translation methods for data augmentation, we propose that while the style of translated images may vary between domains, their task predictions should remain consistent. We leverage this concept by introducing a cross-domain consistency loss to ensure that our adapted model produces reliable predictions. Extensive experiments demonstrate that our method outperforms current state-of-the-art techniques across a range of unsupervised domain adaptation tasks."}, "1802.08636": {"original_text": "  Single document summarization is the task of producing a shorter version of a\ndocument while preserving its principal information content. In this paper we\nconceptualize extractive summarization as a sentence ranking task and propose a\nnovel training algorithm which globally optimizes the ROUGE evaluation metric\nthrough a reinforcement learning objective. We use our algorithm to train a\nneural summarization model on the CNN and DailyMail datasets and demonstrate\nexperimentally that it outperforms state-of-the-art extractive and abstractive\nsystems when evaluated automatically and by humans.\n", "rewritten_text": "The task of single document summarization involves creating a condensed version of a document that retains its main information. This paper presents a new approach to extractive summarization, treating it as a sentence ranking task and introducing a unique training algorithm that optimizes the ROUGE evaluation metric using reinforcement learning. By applying this algorithm to train a neural summarization model on the CNN and DailyMail datasets, we show through experiments that it surpasses existing extractive and abstractive systems in both automated and human evaluations."}, "2406.11192": {"original_text": "  Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets faces issues due to inconsistent entity definitions and\nredundant data, limiting LLMs to dataset-specific learning and hindering\nout-of-domain generalization. To address this, we present B2NERD, a cohesive\nand efficient dataset for Open NER, normalized from 54 existing English or\nChinese datasets using a two-step approach. First, we detect inconsistent\nentity definitions across datasets and clarify them by distinguishable label\nnames to construct a universal taxonomy of 400+ entity types. Second, we\naddress redundancy using a data pruning strategy that selects fewer samples\nwith greater category and semantic diversity. Comprehensive evaluation shows\nthat B2NERD significantly improves LLMs' generalization on Open NER. Our B2NER\nmodels, trained on B2NERD, outperform GPT-4 by 6.8-12.0 F1 points and surpass\nprevious methods in 3 out-of-domain benchmarks across 15 datasets and 6\nlanguages.\n", "rewritten_text": "Named Entity Recognition (NER) is a challenging task for Large Language Models (LLMs) as it involves identifying various types of entities across different domains. Recent research indicates that fine-tuning LLMs on extensive NER data can enhance their performance. However, training directly on existing datasets presents challenges due to inconsistent entity definitions and redundant information, limiting the models to specific datasets and hindering their ability to generalize beyond those domains. To tackle this issue, we introduce B2NERD, a comprehensive dataset for Open NER created by consolidating 54 English and Chinese datasets through a two-step process. Firstly, we identify and resolve inconsistent entity definitions by assigning distinct label names to create a universal taxonomy of over 400 entity types. Secondly, we reduce redundancy by selecting a smaller set of samples with diverse categories and semantics. Extensive evaluations demonstrate that B2NERD significantly improves LLMs' generalization in Open NER tasks. Our B2NER models, trained on B2NERD, outperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in three out-of-domain benchmarks spanning 15 datasets and six languages."}, "2411.01465": {"original_text": "  Despite the outstanding performance in many individual tasks, deep neural\nnetworks suffer from catastrophic forgetting when learning from continuous data\nstreams in real-world scenarios. Current Non-Exemplar Class-Incremental\nLearning (NECIL) methods mitigate forgetting by storing a single prototype per\nclass, which serves to inject previous information when sequentially learning\nnew classes. However, these stored prototypes or their augmented variants often\nfail to simultaneously capture spatial distribution diversity and precision\nneeded for representing old classes. Moreover, as the model acquires new\nknowledge, these prototypes gradually become outdated, making them less\neffective. To overcome these limitations, we propose a more efficient NECIL\nmethod that replaces prototypes with synthesized retrospective features for old\nclasses. Specifically, we model each old class's feature space using a\nmultivariate Gaussian distribution and generate deep representations by\nsampling from high-likelihood regions. Additionally, we introduce a\nsimilarity-based feature compensation mechanism that integrates generated old\nclass features with similar new class features to synthesize robust\nretrospective representations. These retrospective features are then\nincorporated into our incremental learning framework to preserve the decision\nboundaries of previous classes while learning new ones. Extensive experiments\non CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate that our method\nsignificantly improves the efficiency of non-exemplar class-incremental\nlearning and achieves state-of-the-art performance.\n", "rewritten_text": "\"In spite of excelling in various individual tasks, deep neural networks face the challenge of catastrophic forgetting when learning from continuous data streams in real-world settings. Current methods for Non-Exemplar Class-Incremental Learning (NECIL) aim to address this issue by storing a single prototype per class to retain previous information while learning new classes sequentially. However, these stored prototypes often struggle to capture both the spatial distribution diversity and precision required to represent old classes effectively. Additionally, as the model acquires new knowledge, these prototypes become outdated and less effective over time. To address these limitations, we propose a more efficient NECIL method that replaces prototypes with synthesized retrospective features for old classes. This involves modeling each old class's feature space using a multivariate Gaussian distribution and generating deep representations by sampling from high-likelihood regions. Furthermore, we introduce a feature compensation mechanism based on similarity, which combines generated old class features with similar new class features to create robust retrospective representations. These retrospective features are then integrated into our incremental learning framework to maintain the decision boundaries of previous classes while learning new ones. Extensive experiments conducted on CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate that our method significantly enhances the efficiency of non-exemplar class-incremental learning and achieves state-of-the-art performance.\""}, "2404.03654": {"original_text": "  NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel\nview synthesis and 3D reconstruction, but its performance is sensitive to input\nimage quality, which struggles to achieve high-fidelity rendering when provided\nwith low-quality sparse input viewpoints. Previous methods for NeRF restoration\nare tailored for specific degradation type, ignoring the generality of\nrestoration. To overcome this limitation, we propose a generic radiance fields\nrestoration pipeline, named RaFE, which applies to various types of\ndegradations, such as low resolution, blurriness, noise, compression artifacts,\nor their combinations. Our approach leverages the success of off-the-shelf 2D\nrestoration methods to recover the multi-view images individually. Instead of\nreconstructing a blurred NeRF by averaging inconsistencies, we introduce a\nnovel approach using Generative Adversarial Networks (GANs) for NeRF generation\nto better accommodate the geometric and appearance inconsistencies present in\nthe multi-view images. Specifically, we adopt a two-level tri-plane\narchitecture, where the coarse level remains fixed to represent the low-quality\nNeRF, and a fine-level residual tri-plane to be added to the coarse level is\nmodeled as a distribution with GAN to capture potential variations in\nrestoration. We validate RaFE on both synthetic and real cases for various\nrestoration tasks, demonstrating superior performance in both quantitative and\nqualitative evaluations, surpassing other 3D restoration methods specific to\nsingle task. Please see our project website\nhttps://zkaiwu.github.io/RaFE-Project/.\n", "rewritten_text": "The NeRF (Neural Radiance Fields) model has shown great promise in creating new views and reconstructing 3D scenes. However, its effectiveness is impacted by the quality of the input images, struggling to produce high-quality results when given low-quality sparse viewpoints. Existing methods for restoring NeRF are designed for specific types of degradation, limiting their applicability. To address this issue, we introduce RaFE, a versatile radiance fields restoration pipeline capable of handling various types of image degradation, such as low resolution, blurriness, noise, compression artifacts, or a combination of these factors. Our approach utilizes established 2D restoration techniques to enhance individual multi-view images. Instead of averaging inconsistencies to reconstruct a blurred NeRF, we propose a novel method using Generative Adversarial Networks (GANs) for NeRF generation, better accommodating geometric and appearance discrepancies in the input images. Our two-level tri-plane architecture maintains a fixed coarse level to represent the low-quality NeRF, while a fine-level residual tri-plane, modeled as a distribution with GAN, captures potential variations in restoration. RaFE is evaluated on synthetic and real-world scenarios for various restoration tasks, demonstrating superior performance in both quantitative and qualitative assessments compared to other 3D restoration methods tailored to specific tasks. For more information, please visit our project website at https://zkaiwu.github.io/RaFE-Project/."}, "2107.04217": {"original_text": "  This paper studies joint models for selecting correct answer sentences among\nthe top $k$ provided by answer sentence selection (AS2) modules, which are core\ncomponents of retrieval-based Question Answering (QA) systems. Our work shows\nthat a critical step to effectively exploit an answer set regards modeling the\ninterrelated information between pair of answers. For this purpose, we build a\nthree-way multi-classifier, which decides if an answer supports, refutes, or is\nneutral with respect to another one. More specifically, our neural architecture\nintegrates a state-of-the-art AS2 model with the multi-classifier, and a joint\nlayer connecting all components. We tested our models on WikiQA, TREC-QA, and a\nreal-world dataset. The results show that our models obtain the new state of\nthe art in AS2.\n", "rewritten_text": "This study examines joint models for selecting the correct answer sentences from the top $k$ options provided by answer sentence selection (AS2) modules, which are essential components of retrieval-based Question Answering (QA) systems. Our research demonstrates that a crucial aspect of effectively utilizing an answer set involves capturing the interconnected information between pairs of answers. To address this, we develop a three-way multi-classifier that determines whether an answer supports, refutes, or is neutral towards another answer. Our neural architecture combines a cutting-edge AS2 model with the multi-classifier and a joint layer that connects all components. We evaluate our models on WikiQA, TREC-QA, and a real-world dataset, and the results indicate that our models achieve a new state-of-the-art performance in AS2."}, "2407.05271": {"original_text": "  Name-based gender prediction has traditionally categorized individuals as\neither female or male based on their names, using a binary classification\nsystem. That binary approach can be problematic in the cases of gender-neutral\nnames that do not align with any one gender, among other reasons. Relying\nsolely on binary gender categories without recognizing gender-neutral names can\nreduce the inclusiveness of gender prediction tasks. We introduce an additional\ngender category, i.e., \"neutral\", to study and address potential gender biases\nin Large Language Models (LLMs). We evaluate the performance of several\nfoundational and large language models in predicting gender based on first\nnames only. Additionally, we investigate the impact of adding birth years to\nenhance the accuracy of gender prediction, accounting for shifting associations\nbetween names and genders over time. Our findings indicate that most LLMs\nidentify male and female names with high accuracy (over 80%) but struggle with\ngender-neutral names (under 40%), and the accuracy of gender prediction is\nhigher for English-based first names than non-English names. The experimental\nresults show that incorporating the birth year does not improve the overall\naccuracy of gender prediction, especially for names with evolving gender\nassociations. We recommend using caution when applying LLMs for gender\nidentification in downstream tasks, particularly when dealing with non-binary\ngender labels.\n", "rewritten_text": "Gender prediction based on names has traditionally been limited to classifying individuals as either female or male using a binary system. However, this approach can be problematic when dealing with gender-neutral names that do not align with a specific gender. To address potential biases in Large Language Models (LLMs), we propose the inclusion of a \"neutral\" gender category. Our study evaluates the performance of various language models in predicting gender solely based on first names, exploring the impact of incorporating birth years to improve accuracy. While most models excel at identifying male and female names, they struggle with gender-neutral names, especially those of non-English origin. Our findings suggest that adding birth years does not significantly enhance gender prediction accuracy, particularly for names with evolving gender associations. Caution is advised when using LLMs for gender identification in tasks involving non-binary gender labels."}, "2306.09379": {"original_text": "  In this technical report, we briefly introduce the solution of our team\nVIELab-HUST for coded target restoration through atmospheric turbulence in CVPR\n2023 UG$^2$+ Track 2.2. In this task, we propose an efficient multi-stage\nframework to restore a high quality image from distorted frames. Specifically,\neach distorted frame is initially aligned using image registration to suppress\ngeometric distortion. We subsequently select the sharpest set of registered\nframes by employing a frame selection approach based on image sharpness, and\naverage them to produce an image that is largely free of geometric distortion,\nalbeit with blurriness. A learning-based deblurring method is then applied to\nremove the residual blur in the averaged image. Finally, post-processing\ntechniques are utilized to further enhance the quality of the output image. Our\nframework is capable of handling different kinds of coded target dataset\nprovided in the final testing phase, and ranked 1st on the final leaderboard.\nOur code will be available at https://github.com/xsqhust/Turbulence_Removal.\n", "rewritten_text": "In our technical report for CVPR 2023 UG$^2$+ Track 2.2, we present the solution developed by our team VIELab-HUST for restoring coded targets affected by atmospheric turbulence. Our approach involves a multi-stage framework that aims to enhance the quality of distorted frames. Initially, each frame is aligned using image registration to reduce geometric distortion. We then select the sharpest registered frames based on image sharpness and average them to create an image with reduced geometric distortion, though some blurriness may remain. A learning-based deblurring method is subsequently applied to further improve the image quality by reducing residual blur. Additional post-processing techniques are used to enhance the final output image. Our framework is versatile and performed exceptionally well on various coded target datasets during the final testing phase, securing the top rank on the leaderboard. The code for our solution can be accessed at https://github.com/xsqhust/Turbulence_Removal."}, "2108.03502": {"original_text": "  Automatic summarization techniques aim to shorten and generalize information\ngiven in the text while preserving its core message and the most relevant\nideas. This task can be approached and treated with a variety of methods,\nhowever, not many attempts have been made to produce solutions specifically for\nthe Russian language despite existing localizations of the state-of-the-art\nmodels. In this paper, we aim to showcase ruGPT3 ability to summarize texts,\nfine-tuning it on the corpora of Russian news with their corresponding\nhuman-generated summaries. Additionally, we employ hyperparameter tuning so\nthat the model's output becomes less random and more tied to the original text.\nWe evaluate the resulting texts with a set of metrics, showing that our\nsolution can surpass the state-of-the-art model's performance without\nadditional changes in architecture or loss function. Despite being able to\nproduce sensible summaries, our model still suffers from a number of flaws,\nnamely, it is prone to altering Named Entities present in the original text\n(such as surnames, places, dates), deviating from facts stated in the given\ndocument, and repeating the information in the summary.\n", "rewritten_text": "Automatic summarization techniques aim to condense and generalize information from text while maintaining its core message and key ideas. Despite the availability of advanced models for the Russian language, there have been limited efforts to develop specific solutions. This study demonstrates the ability of ruGPT3 to summarize texts by fine-tuning it on Russian news datasets with human-generated summaries. Hyperparameter tuning is utilized to enhance the model's coherence with the original text. Evaluation using various metrics indicates that our approach can outperform existing models without altering the architecture or loss function. However, the model still exhibits shortcomings, including changes to Named Entities, factual inaccuracies, and repetition in the summaries."}, "1901.01574": {"original_text": "  This work systematically analyzes the smoothing effect of vocabulary\nreduction for phrase translation models. We extensively compare various\nword-level vocabularies to show that the performance of smoothing is not\nsignificantly affected by the choice of vocabulary. This result provides\nempirical evidence that the standard phrase translation model is extremely\nsparse. Our experiments also reveal that vocabulary reduction is more effective\nfor smoothing large-scale phrase tables.\n", "rewritten_text": "This study thoroughly examines how reducing vocabulary impacts the smoothing process in phrase translation models. We conduct a detailed comparison of different word-level vocabularies and find that the choice of vocabulary does not have a significant impact on smoothing performance. These findings suggest that the standard phrase translation model is highly sparse. Additionally, our experiments demonstrate that reducing vocabulary is particularly beneficial for smoothing large-scale phrase tables."}, "2210.16579": {"original_text": "  Generating videos is a complex task that is accomplished by generating a set\nof temporally coherent images frame-by-frame. This limits the expressivity of\nvideos to only image-based operations on the individual video frames needing\nnetwork designs to obtain temporally coherent trajectories in the underlying\nimage space. We propose INR-V, a video representation network that learns a\ncontinuous space for video-based generative tasks. INR-V parameterizes videos\nusing implicit neural representations (INRs), a multi-layered perceptron that\npredicts an RGB value for each input pixel location of the video. The INR is\npredicted using a meta-network which is a hypernetwork trained on neural\nrepresentations of multiple video instances. Later, the meta-network can be\nsampled to generate diverse novel videos enabling many downstream video-based\ngenerative tasks. Interestingly, we find that conditional regularization and\nprogressive weight initialization play a crucial role in obtaining INR-V. The\nrepresentation space learned by INR-V is more expressive than an image space\nshowcasing many interesting properties not possible with the existing works.\nFor instance, INR-V can smoothly interpolate intermediate videos between known\nvideo instances (such as intermediate identities, expressions, and poses in\nface videos). It can also in-paint missing portions in videos to recover\ntemporally coherent full videos. In this work, we evaluate the space learned by\nINR-V on diverse generative tasks such as video interpolation, novel video\ngeneration, video inversion, and video inpainting against the existing\nbaselines. INR-V significantly outperforms the baselines on several of these\ndemonstrated tasks, clearly showcasing the potential of the proposed\nrepresentation space.\n", "rewritten_text": "Creating videos involves a complex process of generating a series of coherent images frame by frame. This limits the capabilities of videos to image-based operations on individual frames, requiring network designs to ensure consistency over time. Our solution, INR-V, is a video representation network that learns a continuous space for generative tasks. INR-V uses implicit neural representations (INRs) to parameterize videos, predicting RGB values for each pixel location through a meta-network trained on multiple video instances. By sampling the meta-network, diverse novel videos can be generated for various tasks. Conditional regularization and progressive weight initialization are crucial for the success of INR-V. The representation space learned by INR-V is more expressive than traditional image spaces, enabling unique capabilities such as smooth interpolation between video instances and in-painting missing portions for coherent video recovery. INR-V excels in tasks like video interpolation, novel video generation, inversion, and inpainting, outperforming existing baselines and demonstrating the potential of this innovative representation space."}, "2101.01843": {"original_text": "  Autonomous driving applications use two types of sensor systems to identify\nvehicles - depth sensing LiDAR and radiance sensing cameras. We compare the\nperformance (average precision) of a ResNet for vehicle detection in complex,\ndaytime, driving scenes when the input is a depth map (D = d(x,y)), a radiance\nimage (L = r(x,y)), or both [D,L]. (1) When the spatial sampling resolution of\nthe depth map and radiance image are equal to typical camera resolutions, a\nResNet detects vehicles at higher average precision from depth than radiance.\n(2) As the spatial sampling of the depth map declines to the range of current\nLiDAR devices, the ResNet average precision is higher for radiance than depth.\n(3) For a hybrid system that combines a depth map and radiance image, the\naverage precision is higher than using depth or radiance alone. We established\nthese observations in simulation and then confirmed them using realworld data.\nThe advantage of combining depth and radiance can be explained by noting that\nthe two type of information have complementary weaknesses. The radiance data\nare limited by dynamic range and motion blur. The LiDAR data have relatively\nlow spatial resolution. The ResNet combines the two data sources effectively to\nimprove overall vehicle detection.\n", "rewritten_text": "Autonomous driving applications utilize two sensor systems - LiDAR for depth sensing and cameras for radiance sensing - to detect vehicles. In this study, we evaluate the performance of a ResNet model for vehicle detection in complex daytime driving scenarios using either a depth map (D = d(x,y)), a radiance image (L = r(x,y)), or both [D,L]. Our findings are as follows: (1) When the spatial resolution of the depth map and radiance image matches typical camera resolutions, the ResNet achieves higher average precision in vehicle detection from depth compared to radiance. (2) When the spatial resolution of the depth map is reduced to that of current LiDAR devices, the ResNet performs better in vehicle detection from radiance than depth. (3) Combining a depth map and radiance image in a hybrid system results in higher average precision than using either depth or radiance alone. These observations were initially made in simulation and later validated with real-world data. The advantage of combining depth and radiance lies in their complementary weaknesses: radiance data are limited by dynamic range and motion blur, while LiDAR data have lower spatial resolution. By effectively integrating these two sources of information, the ResNet enhances overall vehicle detection performance."}, "1511.03328": {"original_text": "  Deep convolutional neural networks (CNNs) are the backbone of state-of-art\nsemantic image segmentation systems. Recent work has shown that complementing\nCNNs with fully-connected conditional random fields (CRFs) can significantly\nenhance their object localization accuracy, yet dense CRF inference is\ncomputationally expensive. We propose replacing the fully-connected CRF with\ndomain transform (DT), a modern edge-preserving filtering method in which the\namount of smoothing is controlled by a reference edge map. Domain transform\nfiltering is several times faster than dense CRF inference and we show that it\nyields comparable semantic segmentation results, accurately capturing object\nboundaries. Importantly, our formulation allows learning the reference edge map\nfrom intermediate CNN features instead of using the image gradient magnitude as\nin standard DT filtering. This produces task-specific edges in an end-to-end\ntrainable system optimizing the target semantic segmentation quality.\n", "rewritten_text": "\"State-of-the-art semantic image segmentation systems rely on deep convolutional neural networks (CNNs) as their foundation. Recent research has demonstrated that incorporating fully-connected conditional random fields (CRFs) alongside CNNs can greatly improve object localization accuracy. However, the computational cost of dense CRF inference is high. To address this issue, we suggest replacing the fully-connected CRF with domain transform (DT), a modern edge-preserving filtering technique that adjusts the level of smoothing based on a reference edge map. DT filtering is significantly faster than dense CRF inference and delivers comparable semantic segmentation results, effectively capturing object boundaries. Notably, our approach enables the learning of the reference edge map from intermediate CNN features instead of relying on the image gradient magnitude as in traditional DT filtering. This results in task-specific edges within an end-to-end trainable system that optimizes the quality of semantic segmentation.\""}, "1610.03155": {"original_text": "  Convolutional Neural Networks (CNN) have demon- strated its successful\napplications in computer vision, speech recognition, and natural language\nprocessing. For object recog- nition, CNNs might be limited by its strict label\nrequirement and an implicit assumption that images are supposed to be target-\nobject-dominated for optimal solutions. However, the labeling procedure,\nnecessitating laying out the locations of target ob- jects, is very tedious,\nmaking high-quality large-scale dataset prohibitively expensive. Data\naugmentation schemes are widely used when deep networks suffer the insufficient\ntraining data problem. All the images produced through data augmentation share\nthe same label, which may be problematic since not all data augmentation\nmethods are label-preserving. In this paper, we propose a weakly supervised CNN\nframework named Multiple Instance Learning Convolutional Neural Networks\n(MILCNN) to solve this problem. We apply MILCNN framework to object recognition\nand report state-of-the-art performance on three benchmark datasets: CIFAR10,\nCIFAR100 and ILSVRC2015 classification dataset.\n", "rewritten_text": "Convolutional Neural Networks (CNN) have shown success in various fields such as computer vision, speech recognition, and natural language processing. In object recognition, CNNs may face limitations due to strict label requirements and the assumption that images should primarily feature the target object for optimal results. However, the laborious process of labeling object locations can be costly, hindering the creation of high-quality large-scale datasets. To address the issue of insufficient training data, data augmentation techniques are commonly employed. Yet, challenges arise when not all augmentation methods preserve labels consistently. This paper introduces a weakly supervised CNN framework called Multiple Instance Learning Convolutional Neural Networks (MILCNN) to tackle this issue. The MILCNN framework is applied to object recognition tasks and achieves state-of-the-art performance on three benchmark datasets: CIFAR10, CIFAR100, and ILSVRC2015 classification dataset."}, "1711.06606": {"original_text": "  To realize the full potential of deep learning for medical imaging, large\nannotated datasets are required for training. Such datasets are difficult to\nacquire because labeled medical images are not usually available due to privacy\nissues, lack of experts available for annotation, underrepresentation of rare\nconditions and poor standardization. Lack of annotated data has been addressed\nin conventional vision applications using synthetic images refined via\nunsupervised adversarial training to look like real images. However, this\napproach is difficult to extend to general medical imaging because of the\ncomplex and diverse set of features found in real human tissues. We propose an\nalternative framework that uses a reverse flow, where adversarial training is\nused to make real medical images more like synthetic images, and hypothesize\nthat clinically-relevant features can be preserved via self-regularization.\nThese domain-adapted images can then be accurately interpreted by networks\ntrained on large datasets of synthetic medical images. We test this approach\nfor the notoriously difficult task of depth-estimation from endoscopy. We train\na depth estimator on a large dataset of synthetic images generated using an\naccurate forward model of an endoscope and an anatomically-realistic colon.\nThis network predicts significantly better depths when using synthetic-like\ndomain-adapted images compared to the real images, confirming that the\nclinically-relevant features of depth are preserved.\n", "rewritten_text": "In order to fully leverage the potential of deep learning in medical imaging, extensive annotated datasets are necessary for training. However, obtaining such datasets is challenging due to the scarcity of labeled medical images caused by privacy concerns, limited availability of experts for annotation, rarity of certain conditions, and lack of standardization. While traditional methods in computer vision have used synthetic images refined through unsupervised adversarial training to mimic real images, this approach is not easily applicable to medical imaging due to the complex and varied features present in human tissues. A new framework is proposed that employs reverse flow, utilizing adversarial training to make real medical images more closely resemble synthetic images, with the hypothesis that important clinical features can be preserved through self-regularization. These domain-adapted images can then be effectively interpreted by networks trained on large datasets of synthetic medical images. This methodology is tested in the challenging task of depth estimation from endoscopy, where a depth estimator is trained on a large dataset of synthetic images generated using a precise model of an endoscope and a realistic colon. Results show that the network performs significantly better in predicting depths when using domain-adapted images that resemble synthetic ones, indicating preservation of clinically-relevant depth features."}, "2402.17074": {"original_text": "  Digital Image Correlation (DIC) is an optical technique that measures\ndisplacement and strain by tracking pattern movement in a sequence of captured\nimages during testing. DIC has gained recognition in asphalt pavement\nengineering since the early 2000s. However, users often perceive the DIC\ntechnique as an out-of-box tool and lack a thorough understanding of its\noperational and measurement principles. This article presents a state-of-art\nreview of DIC as a crucial tool for laboratory testing of asphalt concrete\n(AC), primarily focusing on the widely utilized 2D-DIC and 3D-DIC techniques.\nTo address frequently asked questions from users, the review thoroughly\nexamines the optimal methods for preparing speckle patterns, configuring\nsingle-camera or dual-camera imaging systems, conducting DIC analyses, and\nexploring various applications. Furthermore, emerging DIC methodologies such as\nDigital Volume Correlation and deep-learning-based DIC are introduced,\nhighlighting their potential for future applications in pavement engineering.\nThe article also provides a comprehensive and reliable flowchart for\nimplementing DIC in AC characterization. Finally, critical directions for\nfuture research are presented.\n", "rewritten_text": "Digital Image Correlation (DIC) is an optical method used to measure displacement and strain by tracking pattern movement in a series of captured images during testing. Since the early 2000s, DIC has become increasingly popular in asphalt pavement engineering. However, many users view DIC as a ready-to-use tool without fully understanding its operational and measurement principles. This article offers an up-to-date review of DIC as a vital tool for laboratory testing of asphalt concrete (AC), with a focus on the commonly used 2D-DIC and 3D-DIC techniques. Addressing common user queries, the review covers optimal methods for preparing speckle patterns, setting up single-camera or dual-camera imaging systems, performing DIC analyses, and exploring various applications. Additionally, it introduces emerging DIC techniques like Digital Volume Correlation and deep-learning-based DIC, showcasing their potential for future use in pavement engineering. The article also includes a detailed flowchart for implementing DIC in AC characterization and outlines key areas for future research."}, "1805.06173": {"original_text": "  Existing deep convolutional neural networks have found major success in image\nderaining, but at the expense of an enormous number of parameters. This limits\ntheir potential application, for example in mobile devices. In this paper, we\npropose a lightweight pyramid of networks (LPNet) for single image deraining.\nInstead of designing a complex network structures, we use domain-specific\nknowledge to simplify the learning process. Specifically, we find that by\nintroducing the mature Gaussian-Laplacian image pyramid decomposition\ntechnology to the neural network, the learning problem at each pyramid level is\ngreatly simplified and can be handled by a relatively shallow network with few\nparameters. We adopt recursive and residual network structures to build the\nproposed LPNet, which has less than 8K parameters while still achieving\nstate-of-the-art performance on rain removal. We also discuss the potential\nvalue of LPNet for other low- and high-level vision tasks.\n", "rewritten_text": "In the realm of image deraining, current deep convolutional neural networks have seen significant success. However, they come with a drawback of having a large number of parameters, which hinders their practical use, especially on mobile devices. This paper introduces a solution called the lightweight pyramid of networks (LPNet) for single image deraining. Instead of complex network designs, we leverage domain-specific knowledge to simplify the learning process. By incorporating Gaussian-Laplacian image pyramid decomposition technology into the neural network, we simplify the learning task at each pyramid level. This allows us to use a relatively shallow network with minimal parameters. The LPNet, constructed using recursive and residual network structures, contains less than 8K parameters yet achieves top-notch performance in rain removal. Furthermore, we explore the potential applications of LPNet in various low- and high-level vision tasks."}, "2208.00919": {"original_text": "  Visual-inertial localization is a key problem in computer vision and robotics\napplications such as virtual reality, self-driving cars, and aerial vehicles.\nThe goal is to estimate an accurate pose of an object when either the\nenvironment or the dynamics are known. Absolute pose regression (APR)\ntechniques directly regress the absolute pose from an image input in a known\nscene using convolutional and spatio-temporal networks. Odometry methods\nperform relative pose regression (RPR) that predicts the relative pose from a\nknown object dynamic (visual or inertial inputs). The localization task can be\nimproved by retrieving information from both data sources for a cross-modal\nsetup, which is a challenging problem due to contradictory tasks. In this work,\nwe conduct a benchmark to evaluate deep multimodal fusion based on pose graph\noptimization and attention networks. Auxiliary and Bayesian learning are\nutilized for the APR task. We show accuracy improvements for the APR-RPR task\nand for the RPR-RPR task for aerial vehicles and hand-held devices. We conduct\nexperiments on the EuRoC MAV and PennCOSYVIO datasets and record and evaluate a\nnovel industry dataset.\n", "rewritten_text": "Visual-inertial localization is a crucial issue in the fields of computer vision and robotics, with applications in virtual reality, self-driving cars, and aerial vehicles. The objective is to accurately determine the position of an object in scenarios where either the surroundings or the movement patterns are known. Absolute pose regression (APR) methods aim to directly estimate the absolute pose from an image input within a familiar setting using convolutional and spatio-temporal networks. On the other hand, odometry techniques focus on predicting the relative pose from known object dynamics (visual or inertial inputs), known as relative pose regression (RPR). Enhancing localization performance involves leveraging information from both data sources in a cross-modal setup, which poses a challenge due to conflicting tasks. This study introduces a benchmark for evaluating deep multimodal fusion through pose graph optimization and attention networks. Auxiliary and Bayesian learning techniques are applied to improve the APR task. The results demonstrate enhanced accuracy for both APR-RPR and RPR-RPR tasks, particularly for aerial vehicles and handheld devices. Experiments are conducted on the EuRoC MAV and PennCOSYVIO datasets, as well as a new industry dataset, to assess the proposed approach."}, "2107.0984": {"original_text": "  Multilingual pre-trained contextual embedding models (Devlin et al., 2019)\nhave achieved impressive performance on zero-shot cross-lingual transfer tasks.\nFinding the most effective fine-tuning strategy to fine-tune these models on\nhigh-resource languages so that it transfers well to the zero-shot languages is\na non-trivial task. In this paper, we propose a novel meta-optimizer to\nsoft-select which layers of the pre-trained model to freeze during fine-tuning.\nWe train the meta-optimizer by simulating the zero-shot transfer scenario.\nResults on cross-lingual natural language inference show that our approach\nimproves over the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al.,\n2020).\n", "rewritten_text": "The use of advanced multilingual contextual embedding models has shown impressive results in transferring knowledge across languages without prior training. However, determining the most effective fine-tuning strategy for these models on widely spoken languages to ensure successful transfer to less commonly used languages is a complex challenge. This study introduces a new meta-optimizer that intelligently selects which layers of the pre-trained model to freeze during fine-tuning. By training the meta-optimizer through simulations of zero-shot transfer scenarios, we demonstrate improved performance in cross-lingual natural language inference compared to standard fine-tuning methods and X-MAML (Nooralahzadeh et al., 2020)."}, "2010.09298": {"original_text": "  Though deep learning has achieved advanced performance recently, it remains a\nchallenging task in the field of medical imaging, as obtaining reliable labeled\ntraining data is time-consuming and expensive. In this paper, we propose a\ndouble-uncertainty weighted method for semi-supervised segmentation based on\nthe teacher-student model. The teacher model provides guidance for the student\nmodel by penalizing their inconsistent prediction on both labeled and unlabeled\ndata. We train the teacher model using Bayesian deep learning to obtain\ndouble-uncertainty, i.e. segmentation uncertainty and feature uncertainty. It\nis the first to extend segmentation uncertainty estimation to feature\nuncertainty, which reveals the capability to capture information among\nchannels. A learnable uncertainty consistency loss is designed for the\nunsupervised learning process in an interactive manner between prediction and\nuncertainty. With no ground-truth for supervision, it can still incentivize\nmore accurate teacher's predictions and facilitate the model to reduce\nuncertain estimations. Furthermore, our proposed double-uncertainty serves as a\nweight on each inconsistency penalty to balance and harmonize supervised and\nunsupervised training processes. We validate the proposed feature uncertainty\nand loss function through qualitative and quantitative analyses. Experimental\nresults show that our method outperforms the state-of-the-art uncertainty-based\nsemi-supervised methods on two public medical datasets.\n", "rewritten_text": "\"While deep learning has made significant advancements in performance, it remains a complex task in the realm of medical imaging due to the challenges of acquiring reliable labeled training data, which is both time-consuming and costly. This study introduces a novel double-uncertainty weighted approach for semi-supervised segmentation utilizing a teacher-student model. The teacher model guides the student model by penalizing inconsistent predictions on both labeled and unlabeled data. The teacher model is trained using Bayesian deep learning to capture double-uncertainty, encompassing segmentation uncertainty and feature uncertainty. This work extends segmentation uncertainty estimation to feature uncertainty, enhancing the model's ability to extract information across channels. An adaptable uncertainty consistency loss is formulated for unsupervised learning, fostering an interactive relationship between prediction and uncertainty. Even without ground-truth supervision, this approach encourages more accurate teacher predictions and aids in reducing uncertain estimations. Additionally, the proposed double-uncertainty mechanism acts as a weight on each inconsistency penalty to balance and optimize supervised and unsupervised training. The efficacy of the feature uncertainty and loss function is demonstrated through qualitative and quantitative analyses, showing superior performance compared to existing uncertainty-based semi-supervised methods on two public medical datasets.\""}, "2108.11575": {"original_text": "  Spatio-temporal representational learning has been widely adopted in various\nfields such as action recognition, video object segmentation, and action\nanticipation. Previous spatio-temporal representational learning approaches\nprimarily employ ConvNets or sequential models,e.g., LSTM, to learn the\nintra-frame and inter-frame features. Recently, Transformer models have\nsuccessfully dominated the study of natural language processing (NLP), image\nclassification, etc. However, the pure-Transformer based spatio-temporal\nlearning can be prohibitively costly on memory and computation to extract\nfine-grained features from a tiny patch. To tackle the training difficulty and\nenhance the spatio-temporal learning, we construct a shifted chunk Transformer\nwith pure self-attention blocks. Leveraging the recent efficient Transformer\ndesign in NLP, this shifted chunk Transformer can learn hierarchical\nspatio-temporal features from a local tiny patch to a global video clip. Our\nshifted self-attention can also effectively model complicated inter-frame\nvariances. Furthermore, we build a clip encoder based on Transformer to model\nlong-term temporal dependencies. We conduct thorough ablation studies to\nvalidate each component and hyper-parameters in our shifted chunk Transformer,\nand it outperforms previous state-of-the-art approaches on Kinetics-400,\nKinetics-600, UCF101, and HMDB51.\n", "rewritten_text": "The concept of learning spatio-temporal representations has been widely embraced in various fields, including action recognition, video object segmentation, and action anticipation. Traditionally, methods for spatio-temporal representation learning have relied on ConvNets or sequential models like LSTM to capture features within and between frames. While Transformer models have excelled in natural language processing and image classification, using them solely for spatio-temporal learning can be resource-intensive for extracting detailed features from small patches. To address this challenge and improve spatio-temporal learning, we introduce a novel approach called shifted chunk Transformer, which utilizes self-attention blocks. Drawing inspiration from efficient Transformer designs in NLP, this shifted chunk Transformer can learn hierarchical spatio-temporal features ranging from local patches to global video clips. Our shifted self-attention mechanism effectively captures complex inter-frame variations. Additionally, we introduce a clip encoder based on Transformer to model long-term temporal dependencies. Through comprehensive ablation studies, we validate each component and hyper-parameter of our shifted chunk Transformer, demonstrating superior performance compared to existing state-of-the-art methods on datasets such as Kinetics-400, Kinetics-600, UCF101, and HMDB51."}, "2102.0208": {"original_text": "  We introduce a top-down approach to discourse parsing that is conceptually\nsimpler than its predecessors (Kobayashi et al., 2020; Zhang et al., 2020). By\nframing the task as a sequence labelling problem where the goal is to\niteratively segment a document into individual discourse units, we are able to\neliminate the decoder and reduce the search space for splitting points. We\nexplore both traditional recurrent models and modern pre-trained transformer\nmodels for the task, and additionally introduce a novel dynamic oracle for\ntop-down parsing. Based on the Full metric, our proposed LSTM model sets a new\nstate-of-the-art for RST parsing.\n", "rewritten_text": "We present a new top-down approach to discourse parsing that is simpler in concept compared to previous methods (Kobayashi et al., 2020; Zhang et al., 2020). By treating the task as a sequence labeling problem where the objective is to segment a document into discourse units iteratively, we can remove the decoder and narrow down the search space for splitting points. We investigate both traditional recurrent models and modern pre-trained transformer models for this task, and introduce a unique dynamic oracle for top-down parsing. Our LSTM model, according to the Full metric, achieves a new state-of-the-art performance in RST parsing."}, "1112.2386": {"original_text": "  This paper proposes a new procedure in order to improve the performance of\nblock matching and 3-D filtering (BM3D) image denoising algorithm. It is\ndemonstrated that it is possible to achieve a better performance than that of\nBM3D algorithm in a variety of noise levels. This method changes BM3D algorithm\nparameter values according to noise level, removes prefiltering, which is used\nin high noise level; therefore Peak Signal-to-Noise Ratio (PSNR) and visual\nquality get improved, and BM3D complexities and processing time are reduced.\nThis improved BM3D algorithm is extended and used to denoise satellite and\ncolor filter array (CFA) images. Output results show that the performance has\nupgraded in comparison with current methods of denoising satellite and CFA\nimages. In this regard this algorithm is compared with Adaptive PCA algorithm,\nthat has led to superior performance for denoising CFA images, on the subject\nof PSNR and visual quality. Also the processing time has decreased\nsignificantly.\n", "rewritten_text": "This paper introduces a novel approach to enhance the performance of the block matching and 3-D filtering (BM3D) image denoising algorithm. The study demonstrates the ability to achieve superior results compared to the BM3D algorithm across various levels of noise. By adjusting the parameter values of the BM3D algorithm based on the noise level, eliminating prefiltering at high noise levels, enhancing Peak Signal-to-Noise Ratio (PSNR) and visual quality, and reducing BM3D complexities and processing time, the proposed method shows significant improvements. The enhanced BM3D algorithm is further applied to denoise satellite and color filter array (CFA) images, showcasing enhanced performance compared to existing denoising methods. A comparison with the Adaptive PCA algorithm reveals superior results in terms of PSNR, visual quality, and processing time for denoising CFA images."}, "2103.06678": {"original_text": "  In this paper, we explore the effects of language variants, data sizes, and\nfine-tuning task types in Arabic pre-trained language models. To do so, we\nbuild three pre-trained language models across three variants of Arabic: Modern\nStandard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a\nfourth language model which is pre-trained on a mix of the three. We also\nexamine the importance of pre-training data size by building additional models\nthat are pre-trained on a scaled-down set of the MSA variant. We compare our\ndifferent models to each other, as well as to eight publicly available models\nby fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest\nthat the variant proximity of pre-training data to fine-tuning data is more\nimportant than the pre-training data size. We exploit this insight in defining\nan optimized system selection model for the studied tasks.\n", "rewritten_text": "\"In this study, we investigate how language variations, dataset sizes, and types of fine-tuning tasks impact Arabic pre-trained language models. We create three pre-trained language models for different Arabic variants: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, along with a fourth model pre-trained on a combination of the three. Additionally, we explore the significance of pre-training data size by developing models pre-trained on a reduced MSA dataset. Comparing our models to each other and to eight publicly available models, we fine-tune them on five NLP tasks across 12 datasets. Our findings indicate that the similarity between pre-training and fine-tuning data variants is more crucial than the size of the pre-training data. Leveraging this insight, we propose an optimized system selection model for the tasks under study.\""}, "2408.00331": {"original_text": "  Reliably detecting when a deployed machine learning model is likely to fail\non a given input is crucial for ensuring safe operation. In this work, we\npropose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel\napproach that leverages priors from large language models (LLMs) and\nvision-language models (VLMs) to detect failures in image classification\nmodels. DECIDER utilizes LLMs to specify task-relevant core attributes and\nconstructs a ``debiased'' version of the classifier by aligning its visual\nfeatures to these core attributes using a VLM, and detects potential failure by\nmeasuring disagreement between the original and debiased models. In addition to\nproactively identifying samples on which the model would fail, DECIDER also\nprovides human-interpretable explanations for failure through a novel\nattribute-ablation strategy. Through extensive experiments across diverse\nbenchmarks spanning subpopulation shifts (spurious correlations, class\nimbalance) and covariate shifts (synthetic corruptions, domain shifts), DECIDER\nconsistently achieves state-of-the-art failure detection performance,\nsignificantly outperforming baselines in terms of the overall Matthews\ncorrelation coefficient as well as failure and success recall. Our codes can be\naccessed at~\\url{https://github.com/kowshikthopalli/DECIDER/}\n", "rewritten_text": "Detecting potential failures in machine learning models is essential for ensuring safe operation. In this study, we introduce DECIDER (Debiasing Classifiers to Identify Errors Reliably), a new method that uses information from large language models (LLMs) and vision-language models (VLMs) to identify failures in image classification models. DECIDER leverages LLMs to define key attributes for the task and creates a \"debiased\" version of the classifier by aligning its visual features with these attributes using a VLM. It detects possible failures by comparing the original and debiased models. DECIDER not only identifies samples where the model may fail but also offers human-understandable explanations for failures through a unique attribute-ablation technique. Through extensive experiments on various benchmarks covering subpopulation shifts and covariate shifts, DECIDER consistently achieves top-notch failure detection performance, surpassing baseline methods in terms of the overall Matthews correlation coefficient, failure and success recall. Our code is available at the following link: https://github.com/kowshikthopalli/DECIDER/"}, "1708.06126": {"original_text": "  This paper presents a novel application to detect counterfeit identity\ndocuments forged by a scan-printing operation. Texture analysis approaches are\nproposed to extract validation features from security background that is\nusually printed in documents as IDs or banknotes. The main contribution of this\nwork is the end-to-end mobile-server architecture, which provides a service for\nnon-expert users and therefore can be used in several scenarios. The system\nalso provides a crowdsourcing mode so labeled images can be gathered,\ngenerating databases for incremental training of the algorithms.\n", "rewritten_text": "This paper introduces a new method for identifying fake identity documents created through scan-printing. It suggests using texture analysis techniques to extract validation features from the security background typically found on IDs and banknotes. The key innovation of this study is the mobile-server architecture, designed to offer a user-friendly service suitable for various situations. Additionally, the system includes a crowdsourcing feature to collect labeled images for building databases and continuously improving the algorithms."}, "2409.12421": {"original_text": "  Camouflaged object detection (COD) aims to segment camouflaged objects which\nexhibit very similar patterns with the surrounding environment. Recent research\nworks have shown that enhancing the feature representation via the frequency\ninformation can greatly alleviate the ambiguity problem between the foreground\nobjects and the background.With the emergence of vision foundation models, like\nInternImage, Segment Anything Model etc, adapting the pretrained model on COD\ntasks with a lightweight adapter module shows a novel and promising research\ndirection. Existing adapter modules mainly care about the feature adaptation in\nthe spatial domain. In this paper, we propose a novel frequency-guided spatial\nadaptation method for COD task. Specifically, we transform the input features\nof the adapter into frequency domain. By grouping and interacting with\nfrequency components located within non overlapping circles in the spectrogram,\ndifferent frequency components are dynamically enhanced or weakened, making the\nintensity of image details and contour features adaptively adjusted. At the\nsame time, the features that are conducive to distinguishing object and\nbackground are highlighted, indirectly implying the position and shape of\ncamouflaged object. We conduct extensive experiments on four widely adopted\nbenchmark datasets and the proposed method outperforms 26 state-of-the-art\nmethods with large margins. Code will be released.\n", "rewritten_text": "The goal of camouflaged object detection (COD) is to identify objects that blend in with their surroundings by utilizing enhanced feature representation through frequency information. Recent studies have demonstrated that this approach can effectively differentiate between foreground objects and the background. Leveraging vision foundation models such as InternImage and Segment Anything Model, adapting pretrained models for COD tasks using a lightweight adapter module presents a promising research avenue. While existing adapter modules focus on spatial feature adaptation, this paper introduces a novel frequency-guided spatial adaptation technique for COD. By transforming adapter input features into the frequency domain and manipulating frequency components within non-overlapping circles in the spectrogram, this method dynamically adjusts image details and contour features. This process enhances features that aid in distinguishing objects from the background, indirectly revealing the position and shape of camouflaged objects. Extensive experiments on four benchmark datasets demonstrate that the proposed method significantly outperforms 26 state-of-the-art approaches. Code for this method will be made available."}, "1712.03687": {"original_text": "  Because of affected by weather conditions, camera pose and range, etc.\nObjects are usually small, blur, occluded and diverse pose in the images\ngathered from outdoor surveillance cameras or access control system. It is\nchallenging and important to detect faces precisely for face recognition system\nin the field of public security. In this paper, we design a based on context\nmodeling structure named Feature Hierarchy Encoder-Decoder Network for face\ndetection(FHEDN), which can detect small, blur and occluded face with hierarchy\nby hierarchy from the end to the beginning likes encoder-decoder in a single\nnetwork. The proposed network is consist of multiple context modeling and\nprediction modules, which are in order to detect small, blur, occluded and\ndiverse pose faces. In addition, we analyse the influence of distribution of\ntraining set, scale of default box and receipt field size to detection\nperformance in implement stage. Demonstrated by experiments, Our network\nachieves promising performance on WIDER FACE and FDDB benchmarks.\n", "rewritten_text": "Due to various factors such as weather conditions, camera pose, and range, objects in images from outdoor surveillance cameras or access control systems are often small, blurry, occluded, and in diverse poses. Precisely detecting faces is crucial for face recognition systems in public security. This paper introduces a novel model called Feature Hierarchy Encoder-Decoder Network (FHEDN) for face detection, which can effectively detect small, blurry, and occluded faces with a hierarchical approach. The network consists of multiple context modeling and prediction modules designed to detect faces of different sizes and poses. The study also examines the impact of training set distribution, default box scale, and receptive field size on detection performance. Experimental results on WIDER FACE and FDDB benchmarks demonstrate the effectiveness of our network in achieving promising performance."}, "2010.01305": {"original_text": "  Street view images classification aiming at urban land use analysis is\ndifficult because the class labels (e.g., commercial area), are concepts with\nhigher abstract level compared to the ones of general visual tasks (e.g.,\npersons and cars). Therefore, classification models using only visual features\noften fail to achieve satisfactory performance. In this paper, a novel approach\nbased on a \"Detector-Encoder-Classifier\" framework is proposed. Instead of\nusing visual features of the whole image directly as common image-level models\nbased on convolutional neural networks (CNNs) do, the proposed framework\nfirstly obtains the bounding boxes of buildings in street view images from a\ndetector. Their contextual information such as the co-occurrence patterns of\nbuilding classes and their layout are then encoded into metadata by the\nproposed algorithm \"CODING\" (Context encOding of Detected buildINGs). Finally,\nthese bounding box metadata are classified by a recurrent neural network (RNN).\nIn addition, we made a dual-labeled dataset named \"BEAUTY\" (Building dEtection\nAnd Urban funcTional-zone portraYing) of 19,070 street view images and 38,857\nbuildings based on the existing BIC GSV [1]. The dataset can be used not only\nfor street view image classification, but also for multi-class building\ndetection. Experiments on \"BEAUTY\" show that the proposed approach achieves a\n12.65% performance improvement on macro-precision and 12% on macro-recall over\nimage-level CNN based models. Our code and dataset are available at\nhttps://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/\n", "rewritten_text": "Classifying street view images for urban land use analysis presents challenges due to the abstract nature of class labels, such as \"commercial area,\" compared to more concrete visual tasks like identifying people and cars. Traditional classification models relying solely on visual features often fall short in performance. This paper introduces a new approach using a \"Detector-Encoder-Classifier\" framework. Instead of directly analyzing the entire image like typical convolutional neural networks (CNNs), this framework first detects building bounding boxes in street view images, encodes contextual information using the \"CODING\" algorithm, and then classifies this metadata using a recurrent neural network (RNN). Additionally, a dataset called \"BEAUTY\" (Building dEtection And Urban funcTional-zone portraYing) containing 19,070 street view images and 38,857 buildings has been created for street view image classification and multi-class building detection. Experimental results on \"BEAUTY\" demonstrate a 12.65% improvement in macro-precision and 12% in macro-recall compared to traditional image-level CNN models. The code and dataset are accessible at https://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/"}, "2403.10737": {"original_text": "  Facial action unit (AU) detection is a fundamental block for objective facial\nexpression analysis. Supervised learning approaches require a large amount of\nmanual labeling which is costly. The limited labeled data are also not diverse\nin terms of gender which can affect model fairness. In this paper, we propose\nto use synthetically generated data and multi-source domain adaptation (MSDA)\nto address the problems of the scarcity of labeled data and the diversity of\nsubjects. Specifically, we propose to generate a diverse dataset through\nsynthetic facial expression re-targeting by transferring the expressions from\nreal faces to synthetic avatars. Then, we use MSDA to transfer the AU detection\nknowledge from a real dataset and the synthetic dataset to a target dataset.\nInstead of aligning the overall distributions of different domains, we propose\nPaired Moment Matching (PM2) to align the features of the paired real and\nsynthetic data with the same facial expression. To further improve gender\nfairness, PM2 matches the features of the real data with a female and a male\nsynthetic image. Our results indicate that synthetic data and the proposed\nmodel improve both AU detection performance and fairness across genders,\ndemonstrating its potential to solve AU detection in-the-wild.\n", "rewritten_text": "Facial action unit (AU) detection plays a crucial role in analyzing facial expressions objectively. Traditional supervised learning methods rely on extensive manual labeling, which can be expensive. Moreover, the limited labeled data often lack diversity in terms of gender, potentially impacting model fairness. This study introduces a novel approach that leverages synthetically generated data and multi-source domain adaptation (MSDA) to overcome the challenges posed by the scarcity of labeled data and the need for diverse subjects. The proposed method involves creating a varied dataset through synthetic facial expression re-targeting, transferring expressions from real faces to synthetic avatars. Subsequently, MSDA is utilized to transfer AU detection knowledge from both real and synthetic datasets to a target dataset. Instead of aligning overall domain distributions, the approach employs Paired Moment Matching (PM2) to align features of paired real and synthetic data exhibiting the same facial expression. To enhance gender fairness, PM2 aligns features of real data with both female and male synthetic images. The results demonstrate that the use of synthetic data and the proposed model not only enhance AU detection performance but also promote fairness across genders, showcasing the potential to address AU detection challenges in real-world scenarios."}, "2401.03129": {"original_text": "  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n", "rewritten_text": "Recent advancements in Large Language Models (LLMs) have shown impressive performance in a variety of tasks. Due to the wide range of applications for LLMs in different fields, there has been a significant increase in their development. One common approach in developing LLMs is to continuously pre-train them on previously fine-tuned models. However, this method can result in catastrophic forgetting. Our research focuses on exploring the forgetting phenomenon that occurs during continuous pre-training on an already fine-tuned LLM. We assess the effects of ongoing pre-training on the fine-tuned LLM in terms of output structure, knowledge retention, and reliability. Our experiments demonstrate the considerable challenge of mitigating catastrophic forgetting during continuous pre-training, particularly the issue of repetition."}, "2105.07197": {"original_text": "  Modern machine learning models for computer vision exceed humans in accuracy\non specific visual recognition tasks, notably on datasets like ImageNet.\nHowever, high accuracy can be achieved in many ways. The particular decision\nfunction found by a machine learning system is determined not only by the data\nto which the system is exposed, but also the inductive biases of the model,\nwhich are typically harder to characterize. In this work, we follow a recent\ntrend of in-depth behavioral analyses of neural network models that go beyond\naccuracy as an evaluation metric by looking at patterns of errors. Our focus is\non comparing a suite of standard Convolutional Neural Networks (CNNs) and a\nrecently-proposed attention-based network, the Vision Transformer (ViT), which\nrelaxes the translation-invariance constraint of CNNs and therefore represents\na model with a weaker set of inductive biases. Attention-based networks have\npreviously been shown to achieve higher accuracy than CNNs on vision tasks, and\nwe demonstrate, using new metrics for examining error consistency with more\ngranularity, that their errors are also more consistent with those of humans.\nThese results have implications both for building more human-like vision\nmodels, as well as for understanding visual object recognition in humans.\n", "rewritten_text": "Modern machine learning models in computer vision have surpassed humans in accuracy for specific visual recognition tasks, particularly on datasets such as ImageNet. Achieving high accuracy can be done in various ways. The decision function of a machine learning system is influenced not only by the data it is trained on but also by the model's inductive biases, which are often challenging to define. This study follows a recent trend of conducting detailed behavioral analyses of neural network models that extend beyond accuracy as the sole evaluation metric, focusing on analyzing error patterns. The comparison is made between a selection of standard Convolutional Neural Networks (CNNs) and a newly proposed attention-based network called the Vision Transformer (ViT). The ViT relaxes the translation-invariance constraint of CNNs, resulting in a model with less rigid inductive biases. Previous studies have shown that attention-based networks outperform CNNs in vision tasks, and this research introduces new metrics to examine error consistency in more detail, revealing that the errors made by attention-based networks align more closely with those made by humans. These findings have implications for developing vision models that mimic human-like behavior and for enhancing our understanding of visual object recognition in humans."}, "1902.01955": {"original_text": "  In conventional speech recognition, phoneme-based models outperform\ngrapheme-based models for non-phonetic languages such as English. The\nperformance gap between the two typically reduces as the amount of training\ndata is increased. In this work, we examine the impact of the choice of\nmodeling unit for attention-based encoder-decoder models. We conduct\nexperiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, using various\ntarget units (phoneme, grapheme, and word-piece); across all tasks, we find\nthat grapheme or word-piece models consistently outperform phoneme-based\nmodels, even though they are evaluated without a lexicon or an external\nlanguage model. We also investigate model complementarity: we find that we can\nimprove WERs by up to 9% relative by rescoring N-best lists generated from a\nstrong word-piece based baseline with either the phoneme or the grapheme model.\nRescoring an N-best list generated by the phonemic system, however, provides\nlimited improvements. Further analysis shows that the word-piece-based models\nproduce more diverse N-best hypotheses, and thus lower oracle WERs, than\nphonemic models.\n", "rewritten_text": "\"In traditional speech recognition, phoneme-based models tend to perform better than grapheme-based models for languages like English that are not phonetic. As more training data is used, the performance difference between the two types of models usually decreases. This study explores how the choice of modeling unit affects attention-based encoder-decoder models. Experiments were conducted on the LibriSpeech tasks with 100hr, 460hr, and 960hr datasets, using different target units (phoneme, grapheme, and word-piece). Across all tasks, it was consistently found that grapheme or word-piece models outperformed phoneme-based models, even without a lexicon or external language model. Additionally, the study looked into model complementarity and discovered that rescoring N-best lists from a strong word-piece model with either phoneme or grapheme models could improve Word Error Rates (WERs) by up to 9% relative. However, rescoring N-best lists from a phonemic system showed limited improvements. Further analysis revealed that word-piece models generated more diverse N-best hypotheses, resulting in lower oracle WERs compared to phonemic models.\""}, "2012.12482": {"original_text": "  We address the problem of crowd localization, i.e., the prediction of dots\ncorresponding to people in a crowded scene. Due to various challenges, a\nlocalization method is prone to spatial semantic errors, i.e., predicting\nmultiple dots within a same person or collapsing multiple dots in a cluttered\nregion. We propose a topological approach targeting these semantic errors. We\nintroduce a topological constraint that teaches the model to reason about the\nspatial arrangement of dots. To enforce this constraint, we define a\npersistence loss based on the theory of persistent homology. The loss compares\nthe topographic landscape of the likelihood map and the topology of the ground\ntruth. Topological reasoning improves the quality of the localization algorithm\nespecially near cluttered regions. On multiple public benchmarks, our method\noutperforms previous localization methods. Additionally, we demonstrate the\npotential of our method in improving the performance in the crowd counting\ntask.\n", "rewritten_text": "We focus on the issue of crowd localization, which involves predicting the locations of individuals in a crowded setting. One challenge faced by localization methods is the occurrence of spatial semantic errors, such as predicting multiple points within the same person or merging multiple points in a congested area. To address this, we propose a topological approach that specifically targets these semantic errors. Our method introduces a topological constraint to help the model understand the spatial distribution of points. This constraint is enforced through a persistence loss based on persistent homology theory, which compares the topographic features of the likelihood map with the topology of the ground truth. By incorporating topological reasoning, our approach enhances the accuracy of the localization algorithm, particularly in crowded areas. Across various public benchmarks, our method surpasses previous localization techniques. Furthermore, we showcase the potential of our approach in enhancing performance in crowd counting tasks."}, "2303.02982": {"original_text": "  Learning from large-scale contrastive language-image pre-training like CLIP\nhas shown remarkable success in a wide range of downstream tasks recently, but\nit is still under-explored on the challenging few-shot action recognition\n(FSAR) task. In this work, we aim to transfer the powerful multimodal knowledge\nof CLIP to alleviate the inaccurate prototype estimation issue due to data\nscarcity, which is a critical problem in low-shot regimes. To this end, we\npresent a CLIP-guided prototype modulating framework called CLIP-FSAR, which\nconsists of two key components: a video-text contrastive objective and a\nprototype modulation. Specifically, the former bridges the task discrepancy\nbetween CLIP and the few-shot video task by contrasting videos and\ncorresponding class text descriptions. The latter leverages the transferable\ntextual concepts from CLIP to adaptively refine visual prototypes with a\ntemporal Transformer. By this means, CLIP-FSAR can take full advantage of the\nrich semantic priors in CLIP to obtain reliable prototypes and achieve accurate\nfew-shot classification. Extensive experiments on five commonly used benchmarks\ndemonstrate the effectiveness of our proposed method, and CLIP-FSAR\nsignificantly outperforms existing state-of-the-art methods under various\nsettings. The source code and models will be publicly available at\nhttps://github.com/alibaba-mmai-research/CLIP-FSAR.\n", "rewritten_text": "Recently, there has been notable success in various tasks by leveraging large-scale contrastive language-image pre-training models like CLIP. However, the application of these models to the challenging few-shot action recognition (FSAR) task remains relatively unexplored. This study aims to transfer the comprehensive multimodal knowledge of CLIP to address the issue of inaccurate prototype estimation caused by limited data in low-shot scenarios. Introducing a novel framework called CLIP-FSAR, we combine a video-text contrastive objective with prototype modulation. The video-text contrastive objective aligns CLIP with the few-shot video task by contrasting videos with corresponding class text descriptions. The prototype modulation component utilizes transferable textual concepts from CLIP to dynamically refine visual prototypes using a temporal Transformer. Through this approach, CLIP-FSAR effectively leverages the semantic information in CLIP to enhance prototype accuracy and achieve precise few-shot classification. Extensive experiments on five standard benchmarks demonstrate the superior performance of our method, surpassing existing state-of-the-art approaches across various conditions. The source code and models will be made publicly accessible at https://github.com/alibaba-mmai-research/CLIP-FSAR."}, "1905.07826": {"original_text": "  Multi-instance video object segmentation is to segment specific instances\nthroughout a video sequence in pixel level, given only an annotated first\nframe. In this paper, we implement an effective fully convolutional networks\nwith U-Net similar structure built on top of OSVOS fine-tuned layer. We use\ninstance isolation to transform this multi-instance segmentation problem into\nbinary labeling problem, and use weighted cross entropy loss and dice\ncoefficient loss as our loss function. Our best model achieves F mean of 0.467\nand J mean of 0.424 on DAVIS dataset, which is a comparable performance with\nthe State-of-the-Art approach. But case analysis shows this model can achieve a\nsmoother contour and better instance coverage, meaning it better for recall\nfocused segmentation scenario. We also did experiments on other convolutional\nneural networks, including Seg-Net, Mask R-CNN, and provide insightful\ncomparison and discussion.\n", "rewritten_text": "The aim of multi-instance video object segmentation is to accurately segment specific objects in a video by utilizing pixel-level annotations from the first frame. In this study, we have developed a highly effective fully convolutional network based on the U-Net architecture, incorporating fine-tuning from OSVOS. By employing instance isolation, we convert the multi-instance segmentation task into a binary labeling problem. Our approach utilizes a combination of weighted cross entropy and dice coefficient loss functions. Our top-performing model achieves an F mean of 0.467 and a J mean of 0.424 on the DAVIS dataset, demonstrating competitive performance compared to the current state-of-the-art methods. Furthermore, detailed analysis reveals that our model produces smoother contours and better instance coverage, making it particularly suitable for scenarios requiring high recall in segmentation. Additionally, we conducted experiments with other convolutional neural networks such as Seg-Net and Mask R-CNN, providing valuable insights through comparative analysis and discussion."}, "2003.14282": {"original_text": "  A wide variety of transition-based algorithms are currently used for\ndependency parsers. Empirical studies have shown that performance varies across\ndifferent treebanks in such a way that one algorithm outperforms another on one\ntreebank and the reverse is true for a different treebank. There is often no\ndiscernible reason for what causes one algorithm to be more suitable for a\ncertain treebank and less so for another. In this paper we shed some light on\nthis by introducing the concept of an algorithm's inherent dependency\ndisplacement distribution. This characterises the bias of the algorithm in\nterms of dependency displacement, which quantify both distance and direction of\nsyntactic relations. We show that the similarity of an algorithm's inherent\ndistribution to a treebank's displacement distribution is clearly correlated to\nthe algorithm's parsing performance on that treebank, specifically with highly\nsignificant and substantial correlations for the predominant sentence lengths\nin Universal Dependency treebanks. We also obtain results which show a more\ndiscrete analysis of dependency displacement does not result in any meaningful\ncorrelations.\n", "rewritten_text": "Various transition-based algorithms are currently utilized in dependency parsers, with performance differing across different treebanks. One algorithm may outperform another on one treebank, while the opposite may be true for a different treebank. The reasons behind why a particular algorithm is more suitable for one treebank and less so for another are often unclear. This paper introduces the concept of an algorithm's inherent dependency displacement distribution to shed light on this issue. This distribution characterizes the bias of the algorithm in terms of dependency displacement, which encompasses both distance and direction of syntactic relations. The study demonstrates that the similarity between an algorithm's inherent distribution and a treebank's displacement distribution is closely linked to the algorithm's parsing performance on that treebank. Particularly, significant and substantial correlations are observed for predominant sentence lengths in Universal Dependency treebanks. Additionally, the study finds that a more detailed analysis of dependency displacement does not yield meaningful correlations."}, "2404.11682": {"original_text": "  Automated methods are becoming increasingly integrated into studies of\nformative feedback on students' science explanation writing. Most of this work,\nhowever, addresses students' responses to short answer questions. We\ninvestigate automated feedback on students' science explanation essays, where\nstudents must articulate multiple ideas. Feedback is based on a rubric that\nidentifies the main ideas students are prompted to include in explanatory\nessays about the physics of energy and mass, given their experiments with a\nsimulated roller coaster. We have found that students generally improve on\nrevised versions of their essays. Here, however, we focus on two factors that\naffect the accuracy of the automated feedback. First, we find that the main\nideas in the rubric differ with respect to how much freedom they afford in\nexplanations of the idea, thus explanation of a natural law is relatively\nconstrained. Students have more freedom in how they explain complex relations\nthey observe in their roller coasters, such as transfer of different forms of\nenergy. Second, by tracing the automated decision process, we can diagnose when\na student's statement lacks sufficient clarity for the automated tool to\nassociate it more strongly with one of the main ideas above all others. This in\nturn provides an opportunity for teachers and peers to help students reflect on\nhow to state their ideas more clearly.\n", "rewritten_text": "Automated methods are increasingly being incorporated into research on providing formative feedback for students' science explanation writing. While much of this research focuses on students' responses to short answer questions, our study explores automated feedback for students' science explanation essays, which require articulation of multiple ideas. The feedback is generated based on a rubric that outlines key concepts students are expected to include in essays explaining the physics of energy and mass, based on their experiments with a simulated roller coaster. Our findings show that students generally show improvement in revised versions of their essays. However, we specifically examine two factors influencing the accuracy of the automated feedback. Firstly, we observe that the rubric's main ideas vary in the level of detail required for explanations, with explanations of natural laws being more constrained compared to explanations of complex relationships observed in roller coasters, such as energy transfer. Secondly, by analyzing the automated decision-making process, we can identify instances where a student's statement lacks clarity, making it challenging for the automated tool to associate it with a specific main idea. This insight can guide teachers and peers in helping students enhance the clarity of their ideas."}, "2211.08462": {"original_text": "  Recent years have seen an increasing trend in the volume of personal media\ncaptured by users, thanks to the advent of smartphones and smart glasses,\nresulting in large media collections. Despite conversation being an intuitive\nhuman-computer interface, current efforts focus mostly on single-shot natural\nlanguage based media retrieval to aid users query their media and re-live their\nmemories. This severely limits the search functionality as users can neither\nask follow-up queries nor obtain information without first formulating a\nsingle-turn query.\n  In this work, we propose dialogs for connected memories as a powerful tool to\nempower users to search their media collection through a multi-turn,\ninteractive conversation. Towards this, we collect a new task-oriented dialog\ndataset COMET, which contains $11.5k$ user<->assistant dialogs (totaling $103k$\nutterances), grounded in simulated personal memory graphs. We employ a\nresource-efficient, two-phase data collection pipeline that uses: (1) a novel\nmultimodal dialog simulator that generates synthetic dialog flows grounded in\nmemory graphs, and, (2) manual paraphrasing to obtain natural language\nutterances. We analyze COMET, formulate four main tasks to benchmark meaningful\nprogress, and adopt state-of-the-art language models as strong baselines, in\norder to highlight the multimodal challenges captured by our dataset.\n", "rewritten_text": "In recent years, there has been a growing trend in the amount of personal media captured by users, thanks to the rise of smartphones and smart glasses, leading to the accumulation of large media collections. While conversation is a natural way for humans to interact with computers, current efforts primarily focus on single-shot natural language-based media retrieval to help users search their media and relive their memories. However, this approach severely limits search functionality as users are unable to ask follow-up questions or obtain information without first formulating a single-turn query.\n\nIn this study, we propose using dialogs for connected memories as a powerful tool to enable users to search their media collection through a multi-turn, interactive conversation. To achieve this, we have created a new task-oriented dialog dataset called COMET, which consists of 11.5k user<->assistant dialogs (totaling 103k utterances) grounded in simulated personal memory graphs. We have developed a resource-efficient, two-phase data collection process that involves a novel multimodal dialog simulator generating synthetic dialog flows based on memory graphs, and manual paraphrasing to obtain natural language utterances. We have analyzed the COMET dataset, defined four main tasks to measure progress, and utilized state-of-the-art language models as strong benchmarks to showcase the multimodal challenges captured by our dataset."}, "1911.03668": {"original_text": "  Natural Language Inference (NLI) aims to determine the logic relationships\n(i.e., entailment, neutral and contradiction) between a pair of premise and\nhypothesis. Recently, the alignment mechanism effectively helps NLI by\ncapturing the aligned parts (i.e., the similar segments) in the sentence pairs,\nwhich imply the perspective of entailment and contradiction. However, these\naligned parts will sometimes mislead the judgment of neutral relations.\nIntuitively, NLI should rely more on multiple perspectives to form a holistic\nview to eliminate bias. In this paper, we propose the Multi-Perspective\nInferrer (MPI), a novel NLI model that reasons relationships from multiple\nperspectives associated with the three relationships. The MPI determines the\nperspectives of different parts of the sentences via a routing-by-agreement\npolicy and makes the final decision from a holistic view. Additionally, we\nintroduce an auxiliary supervised signal to ensure the MPI to learn the\nexpected perspectives. Experiments on SNLI and MultiNLI show that 1) the MPI\nachieves substantial improvements on the base model, which verifies the\nmotivation of multi-perspective inference; 2) visualized evidence verifies that\nthe MPI learns highly interpretable perspectives as expected; 3) more\nimportantly, the MPI is architecture-free and compatible with the powerful\nBERT.\n", "rewritten_text": "The goal of Natural Language Inference (NLI) is to determine the logical relationships (entailment, neutral, contradiction) between a pair of premise and hypothesis sentences. Recently, the alignment mechanism has been effective in aiding NLI by identifying aligned parts (similar segments) in sentence pairs, indicating entailment and contradiction perspectives. However, these aligned parts can sometimes lead to incorrect judgments of neutral relations. It is intuitive that NLI should consider multiple perspectives to form a comprehensive view and reduce bias. This paper introduces the Multi-Perspective Inferrer (MPI), a new NLI model that evaluates relationships from various perspectives associated with the three types. The MPI identifies different perspectives of sentence parts through a routing-by-agreement policy and makes decisions based on a holistic view. Additionally, an auxiliary supervised signal is introduced to ensure the MPI learns the expected perspectives. Experiments on SNLI and MultiNLI demonstrate that: 1) the MPI significantly enhances the base model, supporting the need for multi-perspective inference; 2) visual evidence confirms that the MPI learns interpretable perspectives; and 3) importantly, the MPI is architecture-independent and compatible with BERT."}, "2403.04212": {"original_text": "  Providing emotional support through dialogue systems is becoming increasingly\nimportant in today's world, as it can support both mental health and social\ninteractions in many conversation scenarios. Previous works have shown that\nusing persona is effective for generating empathetic and supportive responses.\nThey have often relied on pre-provided persona rather than inferring them\nduring conversations. However, it is not always possible to obtain a user\npersona before the conversation begins. To address this challenge, we propose\nPESS (Persona Extraction through Semantic Similarity), a novel framework that\ncan automatically infer informative and consistent persona from dialogues. We\ndevise completeness loss and consistency loss based on semantic similarity\nscores. The completeness loss encourages the model to generate missing persona\ninformation, and the consistency loss guides the model to distinguish between\nconsistent and inconsistent persona. Our experimental results demonstrate that\nhigh-quality persona information inferred by PESS is effective in generating\nemotionally supportive responses.\n", "rewritten_text": "In today's world, providing emotional support through dialogue systems is increasingly crucial for mental health and social interactions in various conversation scenarios. Previous studies have highlighted the effectiveness of using personas to generate empathetic and supportive responses. However, these studies have typically relied on predefined personas rather than deriving them during conversations. Obtaining user personas before conversations may not always be feasible. To tackle this issue, we introduce PESS (Persona Extraction through Semantic Similarity), a new framework that can automatically deduce informative and consistent personas from dialogues. We introduce completeness loss and consistency loss metrics based on semantic similarity scores. The completeness loss encourages the model to fill in missing persona details, while the consistency loss helps the model differentiate between consistent and inconsistent personas. Our experiments show that the high-quality persona information inferred by PESS is successful in generating emotionally supportive responses."}, "1810.13391": {"original_text": "  During natural disasters and conflicts, information about what happened is\noften confusing, messy, and distributed across many sources. We would like to\nbe able to automatically identify relevant information and assemble it into\ncoherent narratives of what happened. To make this task accessible to neural\nmodels, we introduce Story Salads, mixtures of multiple documents that can be\ngenerated at scale. By exploiting the Wikipedia hierarchy, we can generate\nsalads that exhibit challenging inference problems. Story salads give rise to a\nnovel, challenging clustering task, where the objective is to group sentences\nfrom the same narratives. We demonstrate that simple bag-of-words similarity\nclustering falls short on this task and that it is necessary to take into\naccount global context and coherence.\n", "rewritten_text": "In times of natural disasters and conflicts, information can be chaotic and scattered across various sources, making it difficult to understand what exactly occurred. Our goal is to automatically identify relevant information and organize it into coherent narratives. To achieve this, we introduce Story Salads, which are mixtures of multiple documents that can be generated on a large scale. By leveraging the Wikipedia hierarchy, we can create Story Salads that present complex inference challenges. This approach introduces a new and demanding clustering task, where the aim is to group sentences from the same narratives. Our research shows that traditional bag-of-words similarity clustering is insufficient for this task, highlighting the importance of considering global context and coherence."}, "2407.02047": {"original_text": "  Multi-view counting (MVC) methods have shown their superiority over\nsingle-view counterparts, particularly in situations characterized by heavy\nocclusion and severe perspective distortions. However, hand-crafted heuristic\nfeatures and identical camera layout requirements in conventional MVC methods\nlimit their applicability and scalability in real-world scenarios.In this work,\nwe propose a concise 3D MVC framework called \\textbf{CountFormer}to elevate\nmulti-view image-level features to a scene-level volume representation and\nestimate the 3D density map based on the volume features. By incorporating a\ncamera encoding strategy, CountFormer successfully embeds camera parameters\ninto the volume query and image-level features, enabling it to handle various\ncamera layouts with significant differences.Furthermore, we introduce a feature\nlifting module capitalized on the attention mechanism to transform image-level\nfeatures into a 3D volume representation for each camera view. Subsequently,\nthe multi-view volume aggregation module attentively aggregates various\nmulti-view volumes to create a comprehensive scene-level volume representation,\nallowing CountFormer to handle images captured by arbitrary dynamic camera\nlayouts. The proposed method performs favorably against the state-of-the-art\napproaches across various widely used datasets, demonstrating its greater\nsuitability for real-world deployment compared to conventional MVC frameworks.\n", "rewritten_text": "The effectiveness of multi-view counting (MVC) methods surpasses that of single-view methods, especially in scenarios with heavy occlusion and severe perspective distortions. However, traditional MVC methods with hand-crafted features and specific camera layout requirements have limited practicality and scalability in real-world situations. This study introduces a streamlined 3D MVC framework named \\textbf{CountFormer} that enhances multi-view image features to a scene-level volume representation and estimates a 3D density map based on these features. By integrating a camera encoding strategy, CountFormer incorporates camera parameters into the volume query and image features, enabling it to accommodate diverse camera layouts. Additionally, a feature lifting module utilizing attention mechanisms transforms image features into a 3D volume representation for each camera view. The multi-view volume aggregation module then attentively combines these volumes to create a comprehensive scene-level representation, allowing CountFormer to handle images from various dynamic camera layouts. The proposed method outperforms existing approaches on popular datasets, showcasing its superior suitability for real-world applications compared to traditional MVC frameworks."}, "2312.07530": {"original_text": "  Weakly supervised 3D object detection aims to learn a 3D detector with lower\nannotation cost, e.g., 2D labels. Unlike prior work which still relies on few\naccurate 3D annotations, we propose a framework to study how to leverage\nconstraints between 2D and 3D domains without requiring any 3D labels.\nSpecifically, we employ visual data from three perspectives to establish\nconnections between 2D and 3D domains. First, we design a feature-level\nconstraint to align LiDAR and image features based on object-aware regions.\nSecond, the output-level constraint is developed to enforce the overlap between\n2D and projected 3D box estimations. Finally, the training-level constraint is\nutilized by producing accurate and consistent 3D pseudo-labels that align with\nthe visual data. We conduct extensive experiments on the KITTI dataset to\nvalidate the effectiveness of the proposed three constraints. Without using any\n3D labels, our method achieves favorable performance against state-of-the-art\napproaches and is competitive with the method that uses 500-frame 3D\nannotations. Code will be made publicly available at\nhttps://github.com/kuanchihhuang/VG-W3D.\n", "rewritten_text": "\"3D object detection with weak supervision aims to develop a detector using lower-cost annotations, such as 2D labels. Unlike previous methods that still rely on a few accurate 3D annotations, our framework explores leveraging constraints between 2D and 3D domains without the need for any 3D labels. We utilize visual data from three perspectives to establish connections between the two domains. Firstly, we introduce a feature-level constraint to align LiDAR and image features based on object-aware regions. Secondly, an output-level constraint is implemented to ensure overlap between 2D and projected 3D box estimations. Lastly, a training-level constraint is employed to generate accurate and consistent 3D pseudo-labels that align with the visual data. Extensive experiments on the KITTI dataset confirm the effectiveness of these three constraints. Our method achieves competitive performance compared to state-of-the-art approaches without using any 3D labels and is on par with methods utilizing 500-frame 3D annotations. The code will be publicly available at https://github.com/kuanchihhuang/VG-W3D.\""}, "2404.09976": {"original_text": "  Recently, diffusion transformers have gained wide attention with its\nexcellent performance in text-to-image and text-to-vidoe models, emphasizing\nthe need for transformers as backbone for diffusion models. Transformer-based\nmodels have shown better generalization capability compared to CNN-based models\nfor general vision tasks. However, much less has been explored in the existing\nliterature regarding the capabilities of transformer-based diffusion backbones\nand expanding their generative prowess to other datasets. This paper focuses on\nenabling a single pre-trained diffusion transformer model to scale across\nmultiple datasets swiftly, allowing for the completion of diverse generative\ntasks using just one model. To this end, we propose DiffScaler, an efficient\nscaling strategy for diffusion models where we train a minimal amount of\nparameters to adapt to different tasks. In particular, we learn task-specific\ntransformations at each layer by incorporating the ability to utilize the\nlearned subspaces of the pre-trained model, as well as the ability to learn\nadditional task-specific subspaces, which may be absent in the pre-training\ndataset. As these parameters are independent, a single diffusion model with\nthese task-specific parameters can be used to perform multiple tasks\nsimultaneously. Moreover, we find that transformer-based diffusion models\nsignificantly outperform CNN-based diffusion models methods while performing\nfine-tuning over smaller datasets. We perform experiments on four unconditional\nimage generation datasets. We show that using our proposed method, a single\npre-trained model can scale up to perform these conditional and unconditional\ntasks, respectively, with minimal parameter tuning while performing as close as\nfine-tuning an entire diffusion model for that particular task.\n", "rewritten_text": "Lately, diffusion transformers have garnered significant attention due to their impressive performance in text-to-image and text-to-video models, highlighting the importance of transformers as the foundation for diffusion models. Transformer-based models have demonstrated superior generalization abilities compared to CNN-based models for various vision tasks. However, there is limited exploration in the existing literature on the potential of transformer-based diffusion backbones and their extension to different datasets for generative purposes. This study focuses on enabling a single pre-trained diffusion transformer model to adapt efficiently across multiple datasets, enabling diverse generative tasks with a single model. Introducing DiffScaler, a streamlined scaling strategy for diffusion models, involves training a minimal set of parameters to cater to different tasks. Specifically, task-specific transformations are learned at each layer by leveraging the pre-trained model's learned subspaces and acquiring new task-specific subspaces, potentially absent in the pre-training dataset. As these parameters are independent, a single diffusion model with these task-specific parameters can handle multiple tasks simultaneously. Furthermore, transformer-based diffusion models outperform CNN-based diffusion models significantly during fine-tuning on smaller datasets. Experiments conducted on four unconditional image generation datasets demonstrate that our proposed method allows a single pre-trained model to scale up for both conditional and unconditional tasks with minimal parameter adjustments, performing nearly as well as fine-tuning an entire diffusion model for a specific task."}, "2406.06843": {"original_text": "  We introduce a data capture system and a new dataset named HO-Cap that can be\nused to study 3D reconstruction and pose tracking of hands and objects in\nvideos. The capture system uses multiple RGB-D cameras and a HoloLens headset\nfor data collection, avoiding the use of expensive 3D scanners or mocap\nsystems. We propose a semi-automatic method to obtain annotations of shape and\npose of hands and objects in the collected videos, which significantly reduces\nthe required annotation time compared to manual labeling. With this system, we\ncaptured a video dataset of humans using objects to perform different tasks, as\nwell as simple pick-and-place and handover of an object from one hand to the\nother, which can be used as human demonstrations for embodied AI and robot\nmanipulation research. Our data capture setup and annotation framework can be\nused by the community to reconstruct 3D shapes of objects and human hands and\ntrack their poses in videos.\n", "rewritten_text": "We present a new data capture system called HO-Cap along with a dataset that enables the study of 3D reconstruction and pose tracking of hands and objects in videos. The system utilizes multiple RGB-D cameras and a HoloLens headset for data collection, eliminating the need for expensive 3D scanners or mocap systems. We propose a semi-automatic method for obtaining annotations of hand and object shapes and poses in the videos, significantly reducing annotation time compared to manual labeling. The dataset includes videos of humans using objects for various tasks, such as pick-and-place and handover, serving as demonstrations for research in embodied AI and robot manipulation. Our setup and annotation framework can be utilized by the research community for reconstructing 3D shapes of objects and human hands and tracking their poses in videos."}, "2407.05769": {"original_text": "  In autonomous driving, LiDAR sensors are vital for acquiring 3D point clouds,\nproviding reliable geometric information. However, traditional sampling methods\nof preprocessing often ignore semantic features, leading to detail loss and\nground point interference in 3D object detection. To address this, we propose a\nmulti-branch two-stage 3D object detection framework using a Semantic-aware\nMulti-branch Sampling (SMS) module and multi-view consistency constraints. The\nSMS module includes random sampling, Density Equalization Sampling (DES) for\nenhancing distant objects, and Ground Abandonment Sampling (GAS) to focus on\nnon-ground points. The sampled multi-view points are processed through a\nConsistent KeyPoint Selection (CKPS) module to generate consistent keypoint\nmasks for efficient proposal sampling. The first-stage detector uses\nmulti-branch parallel learning with multi-view consistency loss for feature\naggregation, while the second-stage detector fuses multi-view data through a\nMulti-View Fusion Pooling (MVFP) module to precisely predict 3D objects. The\nexperimental results on the KITTI dataset and Waymo Open Dataset show that our\nmethod achieves excellent detection performance improvement for a variety of\nbackbones, especially for low-performance backbones with the simple network\nstructures.\n", "rewritten_text": "In the realm of autonomous driving, LiDAR sensors play a crucial role in capturing 3D point clouds to provide accurate geometric data. However, conventional preprocessing methods often overlook semantic details, resulting in a loss of information and interference from ground points in 3D object detection. To tackle this issue, we introduce a two-stage 3D object detection framework that utilizes a Semantic-aware Multi-branch Sampling (SMS) module and constraints for multi-view consistency. The SMS module incorporates various sampling techniques such as random sampling, Density Equalization Sampling (DES) for distant objects enhancement, and Ground Abandonment Sampling (GAS) to prioritize non-ground points. These sampled points are then processed through a Consistent KeyPoint Selection (CKPS) module to generate consistent keypoint masks for efficient proposal sampling. The initial detector employs multi-branch parallel learning with multi-view consistency loss for feature aggregation, while the subsequent detector combines multi-view data using a Multi-View Fusion Pooling (MVFP) module to accurately predict 3D objects. Our experimental results on the KITTI dataset and Waymo Open Dataset demonstrate significant improvements in detection performance across various backbone architectures, particularly benefiting low-performance backbones with simpler network structures."}, "2211.0089": {"original_text": "  Few-shot learning problem focuses on recognizing unseen classes given a few\nlabeled images. In recent effort, more attention is paid to fine-grained\nfeature embedding, ignoring the relationship among different distance metrics.\nIn this paper, for the first time, we investigate the contributions of\ndifferent distance metrics, and propose an adaptive fusion scheme, bringing\nsignificant improvements in few-shot classification. We start from a naive\nbaseline of confidence summation and demonstrate the necessity of exploiting\nthe complementary property of different distance metrics. By finding the\ncompetition problem among them, built upon the baseline, we propose an Adaptive\nMetrics Module (AMM) to decouple metrics fusion into metric-prediction fusion\nand metric-losses fusion. The former encourages mutual complementary, while the\nlatter alleviates metric competition via multi-task collaborative learning.\nBased on AMM, we design a few-shot classification framework AMTNet, including\nthe AMM and the Global Adaptive Loss (GAL), to jointly optimize the few-shot\ntask and auxiliary self-supervised task, making the embedding features more\nrobust. In the experiment, the proposed AMM achieves 2% higher performance than\nthe naive metrics fusion module, and our AMTNet outperforms the\nstate-of-the-arts on multiple benchmark datasets.\n", "rewritten_text": "The few-shot learning problem involves recognizing new classes with only a few labeled images. Recent efforts have focused on fine-grained feature embedding, but have overlooked the relationship between different distance metrics. This paper explores the impact of various distance metrics and introduces an adaptive fusion approach that significantly enhances few-shot classification. Starting with a basic confidence summation baseline, we highlight the importance of leveraging the complementary nature of different distance metrics. Through the Adaptive Metrics Module (AMM), we separate metrics fusion into metric-prediction fusion and metric-losses fusion to promote mutual complementarity and alleviate metric competition. Our few-shot classification framework, AMTNet, incorporates AMM and Global Adaptive Loss (GAL) to optimize both the few-shot task and auxiliary self-supervised task, enhancing the robustness of embedding features. Experimental results show that AMM achieves a 2% performance improvement over the basic metrics fusion module, and AMTNet surpasses existing methods on various benchmark datasets."}, "2407.13520": {"original_text": "  3D deblurring reconstruction techniques have recently seen significant\nadvancements with the development of Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). Although these techniques can recover relatively\nclear 3D reconstructions from blurry image inputs, they still face limitations\nin handling severe blurring and complex camera motion. To address these issues,\nwe propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting\n(EaDeblur-GS), which integrates event camera data to enhance the robustness of\n3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE)\nnetwork to estimate Gaussian center deviations and using novel loss functions,\nEaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating\nperformance comparable to state-of-the-art methods.\n", "rewritten_text": "Recently, there have been significant advancements in 3D deblurring reconstruction techniques, particularly with the introduction of Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). While these methods can generate clear 3D reconstructions from blurry images, they still struggle with severe blurring and complex camera movements. To overcome these challenges, we propose a new approach called Event-assisted 3D Deblur Reconstruction with Gaussian Splatting (EaDeblur-GS). This method leverages event camera data to improve the robustness of 3DGS against motion blur. By utilizing an Adaptive Deviation Estimator (ADE) network to estimate Gaussian center deviations and incorporating innovative loss functions, EaDeblur-GS can produce sharp 3D reconstructions in real-time, achieving performance on par with the latest techniques available."}, "2103.13678": {"original_text": "  Domain Adaptation is widely used in practical applications of neural machine\ntranslation, which aims to achieve good performance on both the general-domain\nand in-domain. However, the existing methods for domain adaptation usually\nsuffer from catastrophic forgetting, domain divergence, and model explosion. To\naddress these three problems, we propose a method of \"divide and conquer\" which\nis based on the importance of neurons or parameters in the translation model.\nIn our method, we first prune the model and only keep the important neurons or\nparameters, making them responsible for both general-domain and in-domain\ntranslation. Then we further train the pruned model supervised by the original\nunpruned model with the knowledge distillation method. Last we expand the model\nto the original size and fine-tune the added parameters for the in-domain\ntranslation. We conduct experiments on different languages and domains and the\nresults show that our method can achieve significant improvements compared with\nseveral strong baselines.\n", "rewritten_text": "Domain Adaptation is commonly utilized in real-world applications of neural machine translation to achieve high performance in both general-domain and in-domain scenarios. However, current domain adaptation methods often face challenges such as catastrophic forgetting, domain divergence, and model explosion. To tackle these issues, we propose a \"divide and conquer\" approach that focuses on the importance of neurons or parameters in the translation model. Our method involves initially pruning the model to retain only crucial neurons or parameters responsible for general-domain and in-domain translation. Subsequently, we train the pruned model under the supervision of the original unpruned model using knowledge distillation. Finally, we expand the model back to its original size and fine-tune the added parameters for in-domain translation. Experimental results across various languages and domains demonstrate that our method outperforms several strong baseline approaches."}, "2003.11536": {"original_text": "  Estimating the actual head orientation from 2D images, with regard to its\nthree degrees of freedom, is a well known problem that is highly significant\nfor a large number of applications involving head pose knowledge. Consequently,\nthis topic has been tackled by a plethora of methods and algorithms the most\npart of which exploits neural networks. Machine learning methods, indeed,\nachieve accurate head rotation values yet require an adequate training stage\nand, to that aim, a relevant number of positive and negative examples. In this\npaper we take a different approach to this topic by using fractal coding theory\nand particularly Partitioned Iterated Function Systems to extract the fractal\ncode from the input head image and to compare this representation to the\nfractal code of a reference model through Hamming distance. According to\nexperiments conducted on both the BIWI and the AFLW2000 databases, the proposed\nPIFS based head pose estimation method provides accurate yaw/pitch/roll angular\nvalues, with a performance approaching that of state of the art of\nmachine-learning based algorithms and exceeding most of non-training based\napproaches.\n", "rewritten_text": "Determining the precise orientation of a person's head in 2D images, considering its three degrees of freedom, is a well-known and crucial issue for various applications requiring knowledge of head pose. Many methods and algorithms, primarily utilizing neural networks, have been developed to address this challenge. While machine learning techniques can yield accurate head rotation values, they necessitate thorough training with a substantial number of positive and negative examples. This study takes a unique approach by employing fractal coding theory, specifically Partitioned Iterated Function Systems (PIFS), to extract the fractal code from the input head image. The extracted code is then compared to a reference model's fractal code using Hamming distance. Experimental results on the BIWI and AFLW2000 databases demonstrate that the proposed PIFS-based head pose estimation method delivers precise yaw, pitch, and roll angles, performing comparably to state-of-the-art machine-learning algorithms and surpassing many non-training-based methods."}, "1706.02241": {"original_text": "  Analogy completion has been a popular task in recent years for evaluating the\nsemantic properties of word embeddings, but the standard methodology makes a\nnumber of assumptions about analogies that do not always hold, either in recent\nbenchmark datasets or when expanding into other domains. Through an analysis of\nanalogies in the biomedical domain, we identify three assumptions: that of a\nSingle Answer for any given analogy, that the pairs involved describe the Same\nRelationship, and that each pair is Informative with respect to the other. We\npropose modifying the standard methodology to relax these assumptions by\nallowing for multiple correct answers, reporting MAP and MRR in addition to\naccuracy, and using multiple example pairs. We further present BMASS, a novel\ndataset for evaluating linguistic regularities in biomedical embeddings, and\ndemonstrate that the relationships described in the dataset pose significant\nsemantic challenges to current word embedding methods.\n", "rewritten_text": "\"Analogy completion has become a popular task in recent years for assessing the semantic properties of word embeddings. However, the traditional approach makes several assumptions about analogies that may not always be valid, especially when dealing with new benchmark datasets or different domains. By examining analogies in the biomedical field, we have identified three key assumptions: the presence of a single correct answer for each analogy, the requirement that the pairs involved share the same relationship, and the assumption that each pair provides relevant information about the other. To address these limitations, we suggest adjusting the standard methodology to allow for multiple correct answers, incorporating Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) metrics alongside accuracy, and utilizing multiple example pairs. Additionally, we introduce BMASS, a new dataset designed to evaluate linguistic patterns in biomedical embeddings, and show that the relationships within the dataset present significant semantic challenges for current word embedding techniques.\""}, "2312.16894": {"original_text": "  The widespread usage of cars and other large, heavy vehicles necessitates the\ndevelopment of an effective parking infrastructure. Additionally, algorithms\nfor detection and recognition of number plates are often used to identify\nautomobiles all around the world where standardized plate sizes and fonts are\nenforced, making recognition an effortless task. As a result, both kinds of\ndata can be combined to develop an intelligent parking system focuses on the\ntechnology of Automatic Number Plate Recognition (ANPR). Retrieving characters\nfrom an inputted number plate image is the sole purpose of ANPR which is a\ncostly procedure. In this article, we propose Chaurah, a minimal cost ANPR\nsystem that relies on a Raspberry Pi 3 that was specifically created for\nparking facilities. The system employs a dual-stage methodology, with the first\nstage being an ANPR system which makes use of two convolutional neural networks\n(CNNs). The primary locates and recognises license plates from a vehicle image,\nwhile the secondary performs Optical Character Recognition (OCR) to identify\nindividualized numbers from the number plate. An application built with Flutter\nand Firebase for database administration and license plate record comparison\nmakes up the second component of the overall solution. The application also\nacts as an user-interface for the billing mechanism based on parking time\nduration resulting in an all-encompassing software deployment of the study.\n", "rewritten_text": "The widespread use of cars and other large vehicles highlights the need for a well-developed parking infrastructure. Number plate detection and recognition algorithms are commonly employed worldwide to easily identify vehicles with standardized plate sizes and fonts. This allows for the integration of data to create an intelligent parking system centered around Automatic Number Plate Recognition (ANPR). ANPR focuses on extracting characters from inputted number plate images, which can be a costly process. This article introduces Chaurah, a cost-effective ANPR system utilizing a Raspberry Pi 3 designed specifically for parking facilities. The system utilizes a dual-stage approach, with the first stage employing two convolutional neural networks (CNNs) to locate and recognize license plates in vehicle images, while the second stage performs Optical Character Recognition (OCR) to identify individual numbers on the plate. Additionally, an application developed with Flutter and Firebase is used for database management and license plate record comparison, serving as an interface for billing based on parking duration. This comprehensive software deployment offers a complete solution for intelligent parking management."}, "2110.13032": {"original_text": "  Though there has been a large body of recent works in language modeling (LM)\nfor high resource languages such as English and Chinese, the area is still\nunexplored for low resource languages like Bengali and Hindi. We propose an end\nto end trainable memory efficient CNN architecture named CoCNN to handle\nspecific characteristics such as high inflection, morphological richness,\nflexible word order and phonetical spelling errors of Bengali and Hindi. In\nparticular, we introduce two learnable convolutional sub-models at word and at\nsentence level that are end to end trainable. We show that state-of-the-art\n(SOTA) Transformer models including pretrained BERT do not necessarily yield\nthe best performance for Bengali and Hindi. CoCNN outperforms pretrained BERT\nwith 16X less parameters, and it achieves much better performance than SOTA\nLSTM models on multiple real-world datasets. This is the first study on the\neffectiveness of different architectures drawn from three deep learning\nparadigms - Convolution, Recurrent, and Transformer neural nets for modeling\ntwo widely used languages, Bengali and Hindi.\n", "rewritten_text": "\"While there has been significant research in language modeling (LM) for well-resourced languages like English and Chinese, the field remains largely unexplored for low-resource languages such as Bengali and Hindi. Our proposal introduces a memory-efficient CNN architecture called CoCNN, designed to address the unique characteristics of Bengali and Hindi, including high inflection, morphological complexity, flexible word order, and phonetic spelling errors. CoCNN features two trainable convolutional sub-models at the word and sentence levels, demonstrating superior performance compared to state-of-the-art Transformer models like pretrained BERT. Despite having 16 times fewer parameters, CoCNN outperforms BERT and surpasses leading LSTM models across multiple real-world datasets. This study represents the first exploration of different deep learning architectures - Convolutional, Recurrent, and Transformer neural networks - for modeling Bengali and Hindi.\""}, "1305.4537": {"original_text": "  We describe a method for visual object detection based on an ensemble of\noptimized decision trees organized in a cascade of rejectors. The trees use\npixel intensity comparisons in their internal nodes and this makes them able to\nprocess image regions very fast. Experimental analysis is provided through a\nface detection problem. The obtained results are encouraging and demonstrate\nthat the method has practical value. Additionally, we analyse its sensitivity\nto noise and show how to perform fast rotation invariant object detection.\nComplete source code is provided at https://github.com/nenadmarkus/pico.\n", "rewritten_text": "We present a technique for detecting visual objects using a series of refined decision trees arranged in a rejector cascade. These trees employ pixel intensity comparisons in their nodes, enabling rapid processing of image areas. Our approach is evaluated through a face detection task, yielding promising results that highlight its practical utility. Furthermore, we investigate its resilience to noise and illustrate a method for swiftly detecting objects with rotation invariance. The full source code is available at https://github.com/nenadmarkus/pico."}, "2311.18835": {"original_text": "  Empowering models to dynamically accomplish tasks specified through natural\nlanguage instructions represents a promising path toward more capable and\ngeneral artificial intelligence. In this work, we introduce InstructSeq, an\ninstruction-conditioned multi-modal modeling framework that unifies diverse\nvision tasks through flexible natural language control and handling of both\nvisual and textual data. InstructSeq employs a multimodal transformer\narchitecture encompassing visual, language, and sequential modeling. We utilize\na visual encoder to extract image features and a text encoder to encode\ninstructions. An autoregressive transformer fuses the representations and\ngenerates sequential task outputs. By training with LLM-generated natural\nlanguage instructions, InstructSeq acquires a strong comprehension of free-form\ninstructions for specifying visual tasks. This provides an intuitive interface\nfor directing capabilities using flexible natural instructions. Without any\ntask-specific tuning, InstructSeq achieves compelling performance on semantic\nsegmentation, referring expression segmentation/comprehension, and image\ncaptioning. The flexible control and multi-task unification empower the model\nwith more human-like versatility and generalizability for computer vision. The\ncode will be released soon at https://github.com/rongyaofang/InstructSeq.\n", "rewritten_text": "Enabling models to dynamically complete tasks based on natural language instructions is a promising approach towards advancing artificial intelligence capabilities. This study introduces InstructSeq, a multi-modal modeling framework that integrates various vision tasks using natural language control and processing of visual and textual data. InstructSeq utilizes a multimodal transformer architecture that combines visual, language, and sequential modeling. It employs a visual encoder to extract image features and a text encoder to encode instructions. An autoregressive transformer merges the representations and generates task outputs in sequence. By training with language model-generated instructions, InstructSeq gains a strong understanding of natural language instructions for visual tasks, offering an intuitive way to guide capabilities using flexible instructions. InstructSeq demonstrates impressive performance on tasks such as semantic segmentation, referring expression segmentation/comprehension, and image captioning without task-specific tuning. Its flexible control and multi-task integration enhance the model's versatility and generalizability in computer vision, making it more human-like. The code for InstructSeq will soon be available at https://github.com/rongyaofang/InstructSeq."}, "2008.07018": {"original_text": "  We present AutoPose, a novel neural architecture search(NAS) framework that\nis capable of automatically discovering multiple parallel branches of\ncross-scale connections towards accurate and high-resolution 2D human pose\nestimation. Recently, high-performance hand-crafted convolutional networks for\npose estimation show growing demands on multi-scale fusion and high-resolution\nrepresentations. However, current NAS works exhibit limited flexibility on\nscale searching, they dominantly adopt simplified search spaces of\nsingle-branch architectures. Such simplification limits the fusion of\ninformation at different scales and fails to maintain high-resolution\nrepresentations. The presentedAutoPose framework is able to search for\nmulti-branch scales and network depth, in addition to the cell-level\nmicrostructure. Motivated by the search space, a novel bi-level optimization\nmethod is presented, where the network-level architecture is searched via\nreinforcement learning, and the cell-level search is conducted by the\ngradient-based method. Within 2.5 GPU days, AutoPose is able to find very\ncompetitive architectures on the MS COCO dataset, that are also transferable to\nthe MPII dataset. Our code is available at\nhttps://github.com/VITA-Group/AutoPose.\n", "rewritten_text": "We introduce AutoPose, an innovative framework for neural architecture search (NAS) that automatically discovers multiple parallel branches of cross-scale connections to enhance the accuracy and resolution of 2D human pose estimation. While traditional hand-crafted convolutional networks for pose estimation have shown impressive performance, there is a growing need for multi-scale fusion and high-resolution representations. However, existing NAS methods are limited in their ability to search across different scales, often focusing on simplified search spaces with single-branch architectures. This limitation hinders effective information fusion across scales and compromises high-resolution representations. AutoPose overcomes these limitations by searching for multi-branch scales, network depth, and cell-level microstructure. To achieve this, we propose a novel bi-level optimization approach where the network-level architecture is optimized using reinforcement learning, while the cell-level search is performed using gradient-based methods. In just 2.5 GPU days, AutoPose identifies competitive architectures on the MS COCO dataset that can also be transferred to the MPII dataset. Our code is accessible at https://github.com/VITA-Group/AutoPose."}, "1902.053": {"original_text": "  Deep learning, due to its unprecedented success in tasks such as image\nclassification, has emerged as a new tool in image reconstruction with\npotential to change the field. In this paper we demonstrate a crucial\nphenomenon: deep learning typically yields unstablemethods for image\nreconstruction. The instabilities usually occur in several forms: (1) tiny,\nalmost undetectable perturbations, both in the image and sampling domain, may\nresult in severe artefacts in the reconstruction, (2) a small structural\nchange, for example a tumour, may not be captured in the reconstructed image\nand (3) (a counterintuitive type of instability) more samples may yield poorer\nperformance. Our new stability test with algorithms and easy to use software\ndetects the instability phenomena. The test is aimed at researchers to test\ntheir networks for instabilities and for government agencies, such as the Food\nand Drug Administration (FDA), to secure safe use of deep learning methods.\n", "rewritten_text": "\"Deep learning, known for its remarkable success in tasks like image classification, has now become a valuable tool in image reconstruction, potentially revolutionizing the field. This study showcases a significant finding: deep learning often produces unstable methods for image reconstruction. These instabilities manifest in various ways, including subtle perturbations that can lead to significant artifacts in the reconstructed image, the potential for missing important structural changes like tumors, and the unexpected scenario where more samples result in poorer performance. Our novel stability test, along with user-friendly software, can identify these instability issues. This test is designed for researchers to assess their networks for instabilities and for regulatory bodies like the Food and Drug Administration (FDA) to ensure the safe implementation of deep learning techniques.\""}, "2203.08543": {"original_text": "  Deep Metric Learning (DML) proposes to learn metric spaces which encode\nsemantic similarities as embedding space distances. These spaces should be\ntransferable to classes beyond those seen during training. Commonly, DML\nmethods task networks to solve contrastive ranking tasks defined over binary\nclass assignments. However, such approaches ignore higher-level semantic\nrelations between the actual classes. This causes learned embedding spaces to\nencode incomplete semantic context and misrepresent the semantic relation\nbetween classes, impacting the generalizability of the learned metric space. To\ntackle this issue, we propose a language guidance objective for visual\nsimilarity learning. Leveraging language embeddings of expert- and\npseudo-classnames, we contextualize and realign visual representation spaces\ncorresponding to meaningful language semantics for better semantic consistency.\nExtensive experiments and ablations provide a strong motivation for our\nproposed approach and show language guidance offering significant,\nmodel-agnostic improvements for DML, achieving competitive and state-of-the-art\nresults on all benchmarks. Code available at\nhttps://github.com/ExplainableML/LanguageGuidance_for_DML.\n", "rewritten_text": "The concept of Deep Metric Learning (DML) involves acquiring metric spaces that represent semantic similarities through distances in an embedding space. These spaces should be adaptable to classes not encountered during training. Typically, DML methods require networks to address contrastive ranking tasks based on binary class assignments. However, this approach overlooks the higher-level semantic relationships between the actual classes, leading to embedding spaces that lack complete semantic context and misrepresent the relationships between classes. This impacts the ability of the metric space to generalize effectively. To address this issue, we introduce a language guidance objective for learning visual similarities. By utilizing language embeddings of expert- and pseudo-class names, we align visual representation spaces with meaningful language semantics to enhance semantic consistency. Through extensive experiments and ablations, we demonstrate the effectiveness of our proposed approach, showing that language guidance leads to significant improvements in DML performance across various benchmarks. Our method achieves competitive and state-of-the-art results. The code is available at https://github.com/ExplainableML/LanguageGuidance_for_DML."}, "1411.5057": {"original_text": "  In this paper, we propose a novel algorithm for analysis-based sparsity\nreconstruction. It can solve the generalized problem by structured sparsity\nregularization with an orthogonal basis and total variation regularization. The\nproposed algorithm is based on the iterative reweighted least squares (IRLS)\nmodel, which is further accelerated by the preconditioned conjugate gradient\nmethod. The convergence rate of the proposed algorithm is almost the same as\nthat of the traditional IRLS algorithms, that is, exponentially fast. Moreover,\nwith the specifically devised preconditioner, the computational cost for each\niteration is significantly less than that of traditional IRLS algorithms, which\nenables our approach to handle large scale problems. In addition to the fast\nconvergence, it is straightforward to apply our method to standard sparsity,\ngroup sparsity, overlapping group sparsity and TV based problems. Experiments\nare conducted on a practical application: compressive sensing magnetic\nresonance imaging. Extensive results demonstrate that the proposed algorithm\nachieves superior performance over 14 state-of-the-art algorithms in terms of\nboth accuracy and computational cost.\n", "rewritten_text": "\"In this paper, we introduce a new algorithm for sparsity reconstruction through analysis. Our method addresses the generalized problem using structured sparsity regularization with an orthogonal basis and total variation regularization. The algorithm is built on the iterative reweighted least squares (IRLS) model, enhanced by the preconditioned conjugate gradient method for faster convergence. Our algorithm demonstrates a convergence rate similar to traditional IRLS algorithms, with exponential speed. Additionally, the specially designed preconditioner reduces computational costs per iteration compared to traditional IRLS algorithms, making it suitable for large-scale problems. Our approach can be easily applied to various sparsity scenarios, such as standard sparsity, group sparsity, overlapping group sparsity, and TV-based problems. We validate our algorithm through experiments on compressive sensing magnetic resonance imaging, showing superior performance in accuracy and computational efficiency compared to 14 state-of-the-art algorithms.\""}, "2211.09809": {"original_text": "  Animating portraits using speech has received growing attention in recent\nyears, with various creative and practical use cases. An ideal generated video\nshould have good lip sync with the audio, natural facial expressions and head\nmotions, and high frame quality. In this work, we present SPACE, which uses\nspeech and a single image to generate high-resolution, and expressive videos\nwith realistic head pose, without requiring a driving video. It uses a\nmulti-stage approach, combining the controllability of facial landmarks with\nthe high-quality synthesis power of a pretrained face generator. SPACE also\nallows for the control of emotions and their intensities. Our method\noutperforms prior methods in objective metrics for image quality and facial\nmotions and is strongly preferred by users in pair-wise comparisons. The\nproject website is available at https://deepimagination.cc/SPACE/\n", "rewritten_text": "Animating portraits through speech has garnered increased interest in recent years due to its diverse creative and practical applications. An ideal generated video should feature accurate lip syncing, natural facial expressions and head movements, and high-quality frames. This study introduces SPACE, a method that utilizes speech and a single image to produce high-resolution, expressive videos with realistic head poses, all without the need for a reference video. SPACE employs a multi-stage approach that combines the controllability of facial landmarks with the synthesis capabilities of a pretrained face generator, enabling the manipulation of emotions and their intensities. Our method surpasses previous techniques in objective measures of image quality and facial movements, and is highly preferred by users in direct comparisons. For more information, visit the project website at https://deepimagination.cc/SPACE/"}, "2307.03602": {"original_text": "  Stereo vision systems have become popular in computer vision applications,\nsuch as 3D reconstruction, object tracking, and autonomous navigation. However,\ntraditional stereo vision systems that use rectilinear lenses may not be\nsuitable for certain scenarios due to their limited field of view. This has led\nto the popularity of vision systems based on one or multiple fisheye cameras in\ndifferent orientations, which can provide a field of view of 180x180 degrees or\nmore. However, fisheye cameras introduce significant distortion at the edges\nthat affects the accuracy of stereo matching and depth estimation. To overcome\nthese limitations, this paper proposes a method for distortion-removal and\ndepth estimation analysis for stereovision system using orthogonally divergent\nfisheye cameras (ODFC). The proposed method uses two virtual pinhole cameras\n(VPC), each VPC captures a small portion of the original view and presents it\nwithout any lens distortions, emulating the behavior of a pinhole camera. By\ncarefully selecting the captured regions, it is possible to create a stereo\npair using two VPCs. The performance of the proposed method is evaluated in\nboth simulation using virtual environment and experiments using real cameras\nand their results compared to stereo cameras with parallel optical axes. The\nresults demonstrate the effectiveness of the proposed method in terms of\ndistortion removal and depth estimation accuracy.\n", "rewritten_text": "Stereo vision systems are widely used in computer vision applications such as 3D reconstruction, object tracking, and autonomous navigation. However, traditional stereo vision systems with rectilinear lenses may not be suitable for all scenarios due to their limited field of view. As a result, vision systems utilizing one or multiple fisheye cameras in various orientations have gained popularity, offering a field of view of 180x180 degrees or more. Nevertheless, fisheye cameras introduce significant distortion at the edges, impacting the accuracy of stereo matching and depth estimation. To address these challenges, this study proposes a method for distortion removal and depth estimation analysis in stereo vision systems using orthogonally divergent fisheye cameras (ODFC). The method involves the use of two virtual pinhole cameras (VPC), each capturing a distortion-free portion of the original view to emulate a pinhole camera's behavior. By strategically selecting these captured regions, a stereo pair can be created using the two VPCs. The effectiveness of this method is evaluated through simulations in a virtual environment and experiments with real cameras, comparing the results to stereo cameras with parallel optical axes. The findings highlight the proposed method's success in distortion removal and depth estimation accuracy."}, "2410.11816": {"original_text": "  The automatic assembly problem has attracted increasing interest due to its\ncomplex challenges that involve 3D representation. This paper introduces\nJigsaw++, a novel generative method designed to tackle the multifaceted\nchallenges of reconstruction for the reassembly problem. Existing approach\nfocusing primarily on piecewise information for both part and fracture\nassembly, often overlooking the integration of complete object prior. Jigsaw++\ndistinguishes itself by learning a category-agnostic shape prior of complete\nobjects. It employs the proposed \"retargeting\" strategy that effectively\nleverages the output of any existing assembly method to generate complete shape\nreconstructions. This capability allows it to function orthogonally to the\ncurrent methods. Through extensive evaluations on Breaking Bad dataset and\nPartNet, Jigsaw++ has demonstrated its effectiveness, reducing reconstruction\nerrors and enhancing the precision of shape reconstruction, which sets a new\ndirection for future reassembly model developments.\n", "rewritten_text": "The automatic assembly problem has been gaining attention due to its complex challenges involving 3D representation. This paper presents Jigsaw++, a new generative method created to address the various difficulties of reconstruction in the reassembly problem. While existing approaches mainly focus on piecewise information for part and fracture assembly, they often neglect the integration of complete object information. Jigsaw++ stands out by learning a category-agnostic shape prior for complete objects. It utilizes a \"retargeting\" strategy to effectively use the output of any existing assembly method to generate complete shape reconstructions. This unique capability allows it to work independently from current methods. Through thorough evaluations on the Breaking Bad dataset and PartNet, Jigsaw++ has proven its effectiveness in reducing reconstruction errors and improving the accuracy of shape reconstruction, paving the way for future advancements in reassembly model development."}, "1806.05658": {"original_text": "  Seq2seq learning has produced promising results on summarization. However, in\nmany cases, system summaries still struggle to keep the meaning of the original\nintact. They may miss out important words or relations that play critical roles\nin the syntactic structure of source sentences. In this paper, we present\nstructure-infused copy mechanisms to facilitate copying important words and\nrelations from the source sentence to summary sentence. The approach naturally\ncombines source dependency structure with the copy mechanism of an abstractive\nsentence summarizer. Experimental results demonstrate the effectiveness of\nincorporating source-side syntactic information in the system, and our proposed\napproach compares favorably to state-of-the-art methods.\n", "rewritten_text": "\"Although Seq2seq learning has shown promise in summarization tasks, system-generated summaries often struggle to preserve the original meaning. They may overlook crucial words or relationships that are essential to the syntax of the source sentences. This paper introduces structure-infused copy mechanisms to enhance the retention of important elements from the source sentence in the summary. By integrating source dependency structure with the copy mechanism of an abstractive summarizer, our approach effectively incorporates syntactic information from the source side. Experimental results demonstrate the benefits of this approach, showing its superiority over current state-of-the-art methods.\""}, "2108.12545": {"original_text": "  Training deep networks for semantic segmentation requires large amounts of\nlabeled training data, which presents a major challenge in practice, as\nlabeling segmentation masks is a highly labor-intensive process. To address\nthis issue, we present a framework for semi-supervised and domain-adaptive\nsemantic segmentation, which is enhanced by self-supervised monocular depth\nestimation (SDE) trained only on unlabeled image sequences.\n  In particular, we utilize SDE as an auxiliary task comprehensively across the\nentire learning framework: First, we automatically select the most useful\nsamples to be annotated for semantic segmentation based on the correlation of\nsample diversity and difficulty between SDE and semantic segmentation. Second,\nwe implement a strong data augmentation by mixing images and labels using the\ngeometry of the scene. Third, we transfer knowledge from features learned\nduring SDE to semantic segmentation by means of transfer and multi-task\nlearning. And fourth, we exploit additional labeled synthetic data with\nCross-Domain DepthMix and Matching Geometry Sampling to align synthetic and\nreal data.\n  We validate the proposed model on the Cityscapes dataset, where all four\ncontributions demonstrate significant performance gains, and achieve\nstate-of-the-art results for semi-supervised semantic segmentation as well as\nfor semi-supervised domain adaptation. In particular, with only 1/30 of the\nCityscapes labels, our method achieves 92% of the fully-supervised baseline\nperformance and even 97% when exploiting additional data from GTA. The source\ncode is available at\nhttps://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth.\n", "rewritten_text": "Training deep neural networks for semantic segmentation is a challenging task due to the need for a large amount of labeled training data. Labeling segmentation masks is a time-consuming process. To tackle this issue, we introduce a framework for semi-supervised and domain-adaptive semantic segmentation. This framework leverages self-supervised monocular depth estimation (SDE) trained on unlabeled image sequences. Our approach involves utilizing SDE as an auxiliary task throughout the learning process. We automatically select the most relevant samples for annotation based on the correlation between SDE and semantic segmentation difficulty. Additionally, we enhance data augmentation by mixing images and labels using scene geometry, transfer knowledge from SDE to semantic segmentation through transfer and multi-task learning, and align synthetic and real data using Cross-Domain DepthMix and Matching Geometry Sampling. Our model is evaluated on the Cityscapes dataset, demonstrating significant performance improvements in semi-supervised semantic segmentation and domain adaptation. With only a fraction of the Cityscapes labels, our method achieves close to fully-supervised baseline performance. The source code is available at the provided GitHub link."}, "2405.12833": {"original_text": "  Automatic radiology report generation can alleviate the workload for\nphysicians and minimize regional disparities in medical resources, therefore\nbecoming an important topic in the medical image analysis field. It is a\nchallenging task, as the computational model needs to mimic physicians to\nobtain information from multi-modal input data (i.e., medical images, clinical\ninformation, medical knowledge, etc.), and produce comprehensive and accurate\nreports. Recently, numerous works emerged to address this issue using deep\nlearning-based methods, such as transformers, contrastive learning, and\nknowledge-base construction. This survey summarizes the key techniques\ndeveloped in the most recent works and proposes a general workflow for deep\nlearning-based report generation with five main components, including\nmulti-modality data acquisition, data preparation, feature learning, feature\nfusion/interaction, and report generation. The state-of-the-art methods for\neach of these components are highlighted. Additionally, training strategies,\npublic datasets, evaluation methods, current challenges, and future directions\nin this field are summarized. We have also conducted a quantitative comparison\nbetween different methods under the same experimental setting. This is the most\nup-to-date survey that focuses on multi-modality inputs and data fusion for\nradiology report generation. The aim is to provide comprehensive and rich\ninformation for researchers interested in automatic clinical report generation\nand medical image analysis, especially when using multimodal inputs, and assist\nthem in developing new algorithms to advance the field.\n", "rewritten_text": "The generation of radiology reports automatically can help reduce the workload for physicians and address regional disparities in medical resources. This has become a significant area of focus in the field of medical image analysis. It is a complex task, as the computational model must replicate the decision-making process of physicians by analyzing various types of input data, such as medical images, clinical information, and medical knowledge, to produce detailed and accurate reports. Recent research has introduced deep learning techniques like transformers, contrastive learning, and knowledge-base construction to tackle this challenge. This overview outlines the main approaches used in recent studies and proposes a general workflow for deep learning-based report generation, consisting of five key components: acquiring multi-modal data, preparing data, learning features, fusing/interacting features, and generating reports. The most advanced methods for each component are highlighted. The overview also covers training strategies, available datasets, evaluation methods, current obstacles, and future directions in this field. A quantitative comparison of different methods under the same experimental conditions is presented. This survey is the most up-to-date resource focusing on utilizing multi-modal inputs and data fusion for radiology report generation. Its goal is to offer comprehensive information for researchers interested in automatic clinical report generation and medical image analysis, particularly when working with multiple types of input data, to support the development of new algorithms and advancements in the field."}, "1809.10417": {"original_text": "  The tracking-by-detection framework receives growing attentions through the\nintegration with the Convolutional Neural Networks (CNNs). Existing\ntracking-by-detection based methods, however, fail to track objects with severe\nappearance variations. This is because the traditional convolutional operation\nis performed on fixed grids, and thus may not be able to find the correct\nresponse while the object is changing pose or under varying environmental\nconditions. In this paper, we propose a deformable convolution layer to enrich\nthe target appearance representations in the tracking-by-detection framework.\nWe aim to capture the target appearance variations via deformable convolution,\nwhich adaptively enhances its original features. In addition, we also propose a\ngated fusion scheme to control how the variations captured by the deformable\nconvolution affect the original appearance. The enriched feature representation\nthrough deformable convolution facilitates the discrimination of the CNN\nclassifier on the target object and background. Extensive experiments on the\nstandard benchmarks show that the proposed tracker performs favorably against\nstate-of-the-art methods.\n", "rewritten_text": "The use of Convolutional Neural Networks (CNNs) in the tracking-by-detection framework has gained increasing attention. However, current methods based on tracking-by-detection struggle to follow objects that undergo significant appearance changes. This is because traditional convolutional operations are limited by fixed grids, making it difficult to accurately track objects that change pose or encounter different environmental conditions. To address this issue, we introduce a deformable convolution layer in this study to enhance the representation of target appearances in the tracking-by-detection framework. Our goal is to capture variations in target appearances using deformable convolution, which dynamically improves the original features. Additionally, we propose a gated fusion approach to regulate how the variations identified by the deformable convolution impact the original appearance. The enhanced feature representation achieved through deformable convolution aids in distinguishing the target object from the background for the CNN classifier. Extensive experiments conducted on standard benchmarks demonstrate that our proposed tracker outperforms current state-of-the-art methods."}, "2111.15603": {"original_text": "  Modern neural networks are able to perform at least as well as humans in\nnumerous tasks involving object classification and image generation. However,\nsmall perturbations which are imperceptible to humans may significantly degrade\nthe performance of well-trained deep neural networks. We provide a\nDistributionally Robust Optimization (DRO) framework which integrates\nhuman-based image quality assessment methods to design optimal attacks that are\nimperceptible to humans but significantly damaging to deep neural networks.\nThrough extensive experiments, we show that our attack algorithm generates\nbetter-quality (less perceptible to humans) attacks than other state-of-the-art\nhuman imperceptible attack methods. Moreover, we demonstrate that DRO training\nusing our optimally designed human imperceptible attacks can improve group\nfairness in image classification. Towards the end, we provide an algorithmic\nimplementation to speed up DRO training significantly, which could be of\nindependent interest.\n", "rewritten_text": "\"Modern neural networks can match or exceed human performance in tasks such as object classification and image generation. However, even small imperceptible changes can greatly reduce the effectiveness of deep neural networks. Our approach, known as Distributionally Robust Optimization (DRO), incorporates human-based image quality assessment techniques to create attacks that are undetectable to humans yet highly damaging to neural networks. Our experiments demonstrate that our attack method produces higher-quality attacks that are less noticeable to humans compared to other cutting-edge imperceptible attack methods. Additionally, we show that training with our human-imperceptible attacks can enhance fairness in image classification. Finally, we present an algorithmic solution to accelerate DRO training, which may be of interest on its own.\""}, "2404.00829": {"original_text": "  Human writers often bookend their writing with ending sentences that relate\nback to the beginning sentences in order to compose a satisfying narrative that\n\"closes the loop.\" Motivated by this observation, we propose RENarGen, a\ncontrollable story-generation paradigm that generates narratives by ensuring\nthe first and last sentences are related and then infilling the middle\nsentences. Our contributions include an initial exploration of how various\nmethods of bookending from Narratology affect language modeling for stories.\nAutomatic and human evaluations indicate RENarGen produces better stories with\nmore narrative closure than current autoregressive models.\n", "rewritten_text": "\"Many human writers conclude their writing by connecting the ending sentences to the beginning sentences to create a cohesive narrative that 'closes the loop.' Inspired by this concept, we introduce RENarGen, a controllable story-generation approach that focuses on generating narratives where the first and last sentences are linked, followed by filling in the middle sentences. Our research explores the impact of different bookending techniques from Narratology on language modeling for storytelling. Both automated and human assessments show that RENarGen generates more satisfying stories with enhanced narrative coherence compared to existing autoregressive models.\""}, "1711.01062": {"original_text": "  With the development of depth cameras such as Kinect and Intel Realsense,\nRGB-D based human detection receives continuous research attention due to its\nusage in a variety of applications. In this paper, we propose a new\nMulti-Glimpse LSTM (MG-LSTM) network, in which multi-scale contextual\ninformation is sequentially integrated to promote the human detection\nperformance. Furthermore, we propose a feature fusion strategy based on our\nMG-LSTM network to better incorporate the RGB and depth information. To the\nbest of our knowledge, this is the first attempt to utilize LSTM structure for\nRGB-D based human detection. Our method achieves superior performance on two\npublicly available datasets.\n", "rewritten_text": "The ongoing research interest in RGB-D based human detection is fueled by advancements in depth cameras like Kinect and Intel Realsense. This paper introduces a novel Multi-Glimpse LSTM (MG-LSTM) network that integrates multi-scale contextual information to enhance human detection accuracy. Additionally, a feature fusion strategy based on the MG-LSTM network is proposed to effectively combine RGB and depth data. This study represents the pioneering use of LSTM architecture in RGB-D based human detection and demonstrates superior performance on two widely used datasets."}, "1810.07599": {"original_text": "  As facial appearance is subject to significant intra-class variations caused\nby the aging process over time, age-invariant face recognition (AIFR) remains a\nmajor challenge in face recognition community. To reduce the intra-class\ndiscrepancy caused by the aging, in this paper we propose a novel approach\n(namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep\nface features. Specifically, we decompose deep face features into two\northogonal components to represent age-related and identity-related features.\nAs a result, identity-related features that are robust to aging are then used\nfor AIFR. Besides, for complementing the existing cross-age datasets and\nadvancing the research in this field, we construct a brand-new large-scale\nCross-Age Face dataset (CAF). Extensive experiments conducted on the three\npublic domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have\nshown the effectiveness of the proposed approach and the value of the\nconstructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most\npopular general face recognition (GFR) dataset LFW additionally demonstrates\nthe comparable generalization performance on GFR.\n", "rewritten_text": "The aging process introduces significant variations in facial appearance, making age-invariant face recognition (AIFR) a challenging task in the face recognition community. In this study, we introduce a new method called Orthogonal Embedding CNNs (OE-CNNs) to address the intra-class differences caused by aging. Our approach decomposes deep face features into two orthogonal components representing age-related and identity-related features. By focusing on identity-related features that are resilient to aging, we aim to improve AIFR performance. Additionally, we have created a new large-scale Cross-Age Face dataset (CAF) to enhance research in this area. Our experiments on three public face aging datasets (MORPH Album 2, CACD-VS, and FG-NET) demonstrate the effectiveness of our approach and the value of the CAF dataset for AIFR. Furthermore, benchmarking our algorithm on the widely used LFW dataset shows comparable performance in general face recognition (GFR)."}, "2107.005": {"original_text": "  Driven by recent advances in object detection with deep neural networks, the\ntracking-by-detection paradigm has gained increasing prevalence in the research\ncommunity of multi-object tracking (MOT). It has long been known that\nappearance information plays an essential role in the detection-to-track\nassociation, which lies at the core of the tracking-by-detection paradigm.\nWhile most existing works consider the appearance distances between the\ndetections and the tracks, they ignore the statistical information implied by\nthe historical appearance distance records in the tracks, which can be\nparticularly useful when a detection has similar distances with two or more\ntracks. In this work, we propose a hybrid track association (HTA) algorithm\nthat models the historical appearance distances of a track with an incremental\nGaussian mixture model (IGMM) and incorporates the derived statistical\ninformation into the calculation of the detection-to-track association cost.\nExperimental results on three MOT benchmarks confirm that HTA effectively\nimproves the target identification performance with a small compromise to the\ntracking speed. Additionally, compared to many state-of-the-art trackers, the\nDeepSORT tracker equipped with HTA achieves better or comparable performance in\nterms of the balance of tracking quality and speed.\n", "rewritten_text": "Recent advancements in object detection using deep neural networks have led to the growing popularity of the tracking-by-detection approach in the field of multi-object tracking (MOT). It is widely recognized that appearance information is crucial for associating detections with tracks, a key aspect of the tracking-by-detection method. While existing studies typically focus on appearance distances between detections and tracks, they often overlook the valuable statistical insights provided by historical appearance distance data within tracks. This oversight can be particularly relevant when a detection is equidistant from multiple tracks. This study introduces a hybrid track association (HTA) algorithm that leverages an incremental Gaussian mixture model (IGMM) to capture the historical appearance distances of a track and integrates this statistical information into the calculation of detection-to-track association costs. Experimental results on three MOT benchmarks demonstrate that HTA significantly enhances target identification accuracy with minimal impact on tracking speed. Furthermore, when compared to several cutting-edge trackers, the DeepSORT tracker enhanced with HTA achieves superior or comparable performance in terms of tracking quality and speed balance."}, "1711.08879": {"original_text": "  Objects for detection usually have distinct characteristics in different\nsub-regions and different aspect ratios. However, in prevalent two-stage object\ndetection methods, Region-of-Interest (RoI) features are extracted by RoI\npooling with little emphasis on these translation-variant feature components.\nWe present feature selective networks to reform the feature representations of\nRoIs by exploiting their disparities among sub-regions and aspect ratios. Our\nnetwork produces the sub-region attention bank and aspect ratio attention bank\nfor the whole image. The RoI-based sub-region attention map and aspect ratio\nattention map are selectively pooled from the banks, and then used to refine\nthe original RoI features for RoI classification. Equipped with a light-weight\ndetection subnetwork, our network gets a consistent boost in detection\nperformance based on general ConvNet backbones (ResNet-101, GoogLeNet and\nVGG-16). Without bells and whistles, our detectors equipped with ResNet-101\nachieve more than 3% mAP improvement compared to counterparts on PASCAL VOC\n2007, PASCAL VOC 2012 and MS COCO datasets.\n", "rewritten_text": "Objects intended for detection typically exhibit unique characteristics across various sub-regions and aspect ratios. However, in common two-stage object detection approaches, Region-of-Interest (RoI) features are extracted through RoI pooling without much consideration for these translation-variant feature elements. Our proposed feature selective networks aim to enhance the representation of RoIs by leveraging the differences among sub-regions and aspect ratios. The network generates sub-region attention and aspect ratio attention banks for the entire image. Subsequently, the RoI-based sub-region attention map and aspect ratio attention map are selectively pooled from these banks and utilized to refine the original RoI features for classification. With the inclusion of a lightweight detection subnetwork, our network consistently improves detection performance when paired with standard ConvNet backbones such as ResNet-101, GoogLeNet, and VGG-16. Our detectors, particularly those utilizing ResNet-101, achieve over a 3% increase in mean Average Precision (mAP) compared to existing models on datasets like PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO, without any additional complexities."}, "2402.02145": {"original_text": "  In today's media landscape, where news outlets play a pivotal role in shaping\npublic opinion, it is imperative to address the issue of sentiment manipulation\nwithin news text. News writers often inject their own biases and emotional\nlanguage, which can distort the objectivity of reporting. This paper introduces\na novel approach to tackle this problem by reducing the polarity of latent\nsentiments in news content. Drawing inspiration from adversarial attack-based\nsentence perturbation techniques and a prompt based method using ChatGPT, we\nemploy transformation constraints to modify sentences while preserving their\ncore semantics. Using three perturbation methods: replacement, insertion, and\ndeletion coupled with a context-aware masked language model, we aim to maximize\nthe desired sentiment score for targeted news aspects through a beam search\nalgorithm. Our experiments and human evaluations demonstrate the effectiveness\nof these two models in achieving reduced sentiment polarity with minimal\nmodifications while maintaining textual similarity, fluency, and grammatical\ncorrectness. Comparative analysis confirms the competitive performance of the\nadversarial attack based perturbation methods and prompt-based methods,\noffering a promising solution to foster more objective news reporting and\ncombat emotional language bias in the media.\n", "rewritten_text": "In today's media environment, where news organizations have a significant impact on shaping public opinion, it is crucial to address the issue of manipulating sentiments in news articles. Journalists often introduce their own biases and emotional language, which can distort the impartiality of reporting. This study presents a new approach to addressing this problem by reducing the emotional intensity of underlying sentiments in news content. Inspired by techniques that involve adversarial attacks on sentences and a method using ChatGPT prompts, we apply transformation constraints to alter sentences while preserving their fundamental meaning. By utilizing three perturbation methods - replacement, insertion, and deletion - along with a context-aware masked language model, we aim to enhance the desired sentiment score for specific news aspects through a beam search algorithm. Our experiments and human assessments show the effectiveness of these models in decreasing sentiment intensity with minimal changes while maintaining textual similarity, fluency, and grammatical accuracy. Comparative analysis confirms the competitive performance of adversarial attack-based perturbation methods and prompt-based methods, offering a promising solution to promote more objective news reporting and counteract emotional language bias in the media."}, "2312.10437": {"original_text": "  Tender notices are usually sought by most of the companies at regular\nintervals as a means for obtaining the contracts of various projects. These\nnotices consist of all the required information like description of the work,\nperiod of construction, estimated amount of project, etc. In the context of\nNepal, tender notices are usually published in national as well as local\nnewspapers. The interested bidders should search all the related tender notices\nin newspapers. However, it is very tedious for these companies to manually\nsearch tender notices in every newspaper and figure out which bid is best\nsuited for them. This project is built with the purpose of solving this tedious\ntask of manually searching the tender notices. Initially, the newspapers are\ndownloaded in PDF format using the selenium library of python. After\ndownloading the newspapers, the e-papers are scanned and tender notices are\nautomatically extracted using a neural network. For extraction purposes,\ndifferent architectures of CNN namely ResNet, GoogleNet and Xception are used\nand a model with highest performance has been implemented. Finally, these\nextracted notices are then published on the website and are accessible to the\nusers. This project is helpful for construction companies as well as\ncontractors assuring quality and efficiency. This project has great application\nin the field of competitive bidding as well as managing them in a systematic\nmanner.\n", "rewritten_text": "Most companies regularly seek tender notices to secure contracts for various projects. These notices contain essential details such as work descriptions, construction timelines, project estimates, etc. In Nepal, tender notices are typically published in national and local newspapers. Interested bidders must diligently search for these notices in newspapers, which can be a cumbersome task. To address this issue, a project was developed to automate the process of searching for tender notices. Initially, newspapers are downloaded in PDF format using the Selenium library in Python. Subsequently, the e-papers are scanned, and tender notices are automatically extracted using a neural network. Different CNN architectures like ResNet, GoogleNet, and Xception are utilized for extraction, with the best-performing model being implemented. The extracted notices are then published on a website for user access. This project benefits construction companies and contractors by enhancing quality and efficiency, particularly in competitive bidding and systematic management of projects."}, "2405.18132": {"original_text": "  In recent years, the increasing demand for dynamic 3D assets in design and\ngaming applications has given rise to powerful generative pipelines capable of\nsynthesizing high-quality 4D objects. Previous methods generally rely on score\ndistillation sampling (SDS) algorithm to infer the unseen views and motion of\n4D objects, thus leading to unsatisfactory results with defects like\nover-saturation and Janus problem. Therefore, inspired by recent progress of\nvideo diffusion models, we propose to optimize a 4D representation by\nexplicitly generating multi-view videos from one input image. However, it is\nfar from trivial to handle practical challenges faced by such a pipeline,\nincluding dramatic temporal inconsistency, inter-frame geometry and texture\ndiversity, and semantic defects brought by video generation results. To address\nthese issues, we propose DG4D, a novel multi-stage framework that generates\nhigh-quality and consistent 4D assets without score distillation. Specifically,\ncollaborative techniques and solutions are developed, including an attention\ninjection strategy to synthesize temporal-consistent multi-view videos, a\nrobust and efficient dynamic reconstruction method based on Gaussian Splatting,\nand a refinement stage with diffusion prior for semantic restoration. The\nqualitative results and user preference study demonstrate that our framework\noutperforms the baselines in generation quality by a considerable margin. Code\nwill be released at \\url{https://github.com/jasongzy/EG4D}.\n", "rewritten_text": "\"In recent years, there has been a growing demand for dynamic 3D assets in design and gaming applications, leading to the development of advanced generative pipelines capable of creating high-quality 4D objects. Traditional methods often use the score distillation sampling (SDS) algorithm to predict the unseen views and motion of 4D objects, resulting in subpar outcomes with issues such as over-saturation and the Janus problem. Drawing inspiration from recent advancements in video diffusion models, we propose a new approach to enhance 4D representation by generating multi-view videos from a single input image. However, addressing practical challenges faced by this pipeline, such as temporal inconsistencies, diverse inter-frame geometry and textures, and semantic defects from video generation, is complex. To tackle these challenges, we introduce DG4D, a novel multi-stage framework that produces high-quality and coherent 4D assets without relying on score distillation. Our framework incorporates collaborative techniques and solutions, including an attention injection strategy for creating temporally consistent multi-view videos, a robust and efficient dynamic reconstruction method using Gaussian Splatting, and a refinement stage with diffusion prior for semantic restoration. Through qualitative results and user preference studies, we demonstrate that our framework significantly outperforms existing methods in terms of generation quality. The code for our framework will be made available at \\url{https://github.com/jasongzy/EG4D}.\""}, "2101.04929": {"original_text": "  Motivated by applications from computer vision to bioinformatics, the field\nof shape analysis deals with problems where one wants to analyze geometric\nobjects, such as curves, while ignoring actions that preserve their shape, such\nas translations, rotations, or reparametrizations. Mathematical tools have been\ndeveloped to define notions of distances, averages, and optimal deformations\nfor geometric objects. One such framework, which has proven to be successful in\nmany applications, is based on the square root velocity (SRV) transform, which\nallows one to define a computable distance between spatial curves regardless of\nhow they are parametrized. This paper introduces a supervised deep learning\nframework for the direct computation of SRV distances between curves, which\nusually requires an optimization over the group of reparametrizations that act\non the curves. The benefits of our approach in terms of computational speed and\naccuracy are illustrated via several numerical experiments.\n", "rewritten_text": "Motivated by various applications ranging from computer vision to bioinformatics, the field of shape analysis focuses on analyzing geometric objects, such as curves, while disregarding shape-preserving actions like translations, rotations, and reparametrizations. Mathematical tools have been developed to establish concepts of distances, averages, and optimal deformations for geometric objects. One successful framework, based on the square root velocity (SRV) transform, enables the computation of distances between spatial curves regardless of their parametrization. This study presents a supervised deep learning framework for directly calculating SRV distances between curves, eliminating the need for optimization over reparametrization groups. The advantages of our approach, including enhanced computational speed and accuracy, are demonstrated through various numerical experiments."}, "1811.07461": {"original_text": "  Humans naturally perceive a 3D scene in front of them through accumulation of\ninformation obtained from multiple interconnected projections of the scene and\nby interpreting their correspondence. This phenomenon has inspired artificial\nintelligence models to extract the depth and view angle of the observed scene\nby modeling the correspondence between different views of that scene. Our paper\nis built upon previous works in the field of unsupervised depth and relative\ncamera pose estimation from temporal consecutive video frames using deep\nlearning (DL) models. Our approach uses a hybrid learning framework introduced\nin a recent work called GeoNet, which leverages geometric constraints in the 3D\nscenes to synthesize a novel view from intermediate DL-based predicted depth\nand relative pose. However, the state-of-the-art unsupervised depth and pose\nestimation DL models are exclusively trained/tested on a few available outdoor\nscene datasets and we have shown they are hardly transferable to new scenes,\nespecially from indoor environments, in which estimation requires higher\nprecision and dealing with probable occlusions. This paper introduces \"Indoor\nGeoNet\", a weakly supervised depth and camera pose estimation model targeted\nfor indoor scenes. In Indoor GeoNet, we take advantage of the availability of\nindoor RGBD datasets collected by human or robot navigators, and added partial\n(i.e. weak) supervision in depth training into the model. Experimental results\nshowed that our model effectively generalizes to new scenes from different\nbuildings. Indoor GeoNet demonstrated significant depth and pose estimation\nerror reduction when compared to the original GeoNet, while showing 3 times\nmore reconstruction accuracy in synthesizing novel views in indoor\nenvironments.\n", "rewritten_text": "Humans naturally perceive a three-dimensional scene in front of them by gathering information from various interconnected perspectives of the scene and interpreting their relationships. This natural process has inspired artificial intelligence models to determine the depth and viewing angle of a scene by analyzing the connections between different viewpoints. Our study builds upon existing research in unsupervised depth and relative camera pose estimation using deep learning models, specifically focusing on temporal consecutive video frames. We adopt a hybrid learning approach based on the GeoNet framework, which utilizes geometric constraints in 3D scenes to generate a new view using predicted depth and relative pose from deep learning models. However, current unsupervised depth and pose estimation models are primarily trained and tested on outdoor datasets, making them less adaptable to new scenes, particularly indoor environments where higher precision and handling occlusions are crucial. To address this limitation, we introduce \"Indoor GeoNet,\" a weakly supervised model designed for indoor scenes. Leveraging indoor RGBD datasets and incorporating partial supervision in depth training, our model demonstrates effective generalization to new indoor scenes across different buildings. Indoor GeoNet outperforms the original GeoNet by reducing depth and pose estimation errors significantly and achieving three times greater reconstruction accuracy when synthesizing new views in indoor settings."}, "2208.04254": {"original_text": "  Image captioning models are usually trained according to human annotated\nground-truth captions, which could generate accurate but generic captions. In\nthis paper, we focus on generating distinctive captions that can distinguish\nthe target image from other similar images. To evaluate the distinctiveness of\ncaptions, we introduce a series of metrics that use large-scale vision-language\npre-training model CLIP to quantify the distinctiveness. To further improve the\ndistinctiveness of captioning models, we propose a simple and effective\ntraining strategy that trains the model by comparing target image with similar\nimage group and optimizing the group embedding gap. Extensive experiments are\nconducted on various baseline models to demonstrate the wide applicability of\nour strategy and the consistency of metric results with human evaluation. By\ncomparing the performance of our best model with existing state-of-the-art\nmodels, we claim that our model achieves new state-of-the-art towards\ndistinctiveness objective.\n", "rewritten_text": "The models used for image captioning are typically trained based on human-provided captions, which may result in accurate but generic descriptions. This study focuses on creating unique captions that can set apart the target image from similar ones. To assess the uniqueness of the captions, we introduce metrics that leverage the CLIP vision-language pre-training model. To enhance the distinctiveness of captioning models, we propose a straightforward yet effective training approach that involves comparing the target image with a group of similar images and optimizing the embedding gap within the group. Extensive experiments across various baseline models showcase the broad applicability of our approach and the alignment of metric results with human evaluations. By comparing our top-performing model with existing state-of-the-art models, we assert that our model sets a new benchmark in achieving distinctiveness in image captioning."}, "2212.07181": {"original_text": "  Neuromorphic vision or event vision is an advanced vision technology, where\nin contrast to the visible camera that outputs pixels, the event vision\ngenerates neuromorphic events every time there is a brightness change which\nexceeds a specific threshold in the field of view (FOV). This study focuses on\nleveraging neuromorphic event data for roadside object detection. This is a\nproof of concept towards building artificial intelligence (AI) based pipelines\nwhich can be used for forward perception systems for advanced vehicular\napplications. The focus is on building efficient state-of-the-art object\ndetection networks with better inference results for fast-moving forward\nperception using an event camera. In this article, the event-simulated A2D2\ndataset is manually annotated and trained on two different YOLOv5 networks\n(small and large variants). To further assess its robustness, single model\ntesting and ensemble model testing are carried out.\n", "rewritten_text": "Neuromorphic vision, also known as event vision, is an advanced technology that differs from traditional cameras by generating neuromorphic events in response to significant changes in brightness within the field of view (FOV), rather than outputting pixels. This research focuses on utilizing neuromorphic event data for detecting objects on roadsides, as a step towards developing artificial intelligence (AI) systems for advanced vehicle applications. The goal is to create efficient object detection networks that provide accurate results for quickly moving vehicles using event cameras. The study involves manually annotating and training the event-simulated A2D2 dataset with two different YOLOv5 networks (small and large versions), and evaluating their performance through single model and ensemble model testing."}, "2402.01217": {"original_text": "  Implicit neural representations, represented by Neural Radiance Fields\n(NeRF), have dominated research in 3D computer vision by virtue of high-quality\nvisual results and data-driven benefits. However, their realistic applications\nare hindered by the need for dense inputs and per-scene optimization. To solve\nthis problem, previous methods implement generalizable NeRFs by extracting\nlocal features from sparse inputs as conditions for the NeRF decoder. However,\nalthough this way can allow feed-forward reconstruction, they suffer from the\ninherent drawback of yielding sub-optimal results caused by erroneous\nreprojected features. In this paper, we focus on this problem and aim to\naddress it by introducing pre-trained generative priors to enable high-quality\ngeneralizable novel view synthesis. Specifically, we propose a novel Indirect\nDiffusion-guided NeRF framework, termed ID-NeRF, which leverages pre-trained\ndiffusion priors as a guide for the reprojected features created by the\nprevious paradigm. Notably, to enable 3D-consistent predictions, the proposed\nID-NeRF discards the way of direct supervision commonly used in prior 3D\ngenerative models and instead adopts a novel indirect prior injection strategy.\nThis strategy is implemented by distilling pre-trained knowledge into an\nimaginative latent space via score-based distillation, and an attention-based\nrefinement module is then proposed to leverage the embedded priors to improve\nreprojected features extracted from sparse inputs. We conduct extensive\nexperiments on multiple datasets to evaluate our method, and the results\ndemonstrate the effectiveness of our method in synthesizing novel views in a\ngeneralizable manner, especially in sparse settings.\n", "rewritten_text": "Implicit neural representations, such as Neural Radiance Fields (NeRF), have been prominent in 3D computer vision research due to their high-quality visual outcomes and data-driven advantages. However, their practical applications are limited by the requirement for dense inputs and scene-specific optimization. To address this issue, previous approaches have aimed to create adaptable NeRFs by utilizing local features from sparse inputs as conditions for the NeRF decoder. While this approach enables feed-forward reconstruction, it often leads to suboptimal results due to inaccuracies in reprojected features. This paper focuses on this challenge and proposes a solution by introducing pre-trained generative priors to facilitate high-quality, adaptable novel view synthesis. Specifically, a new framework called Indirect Diffusion-guided NeRF (ID-NeRF) is introduced, which utilizes pre-trained diffusion priors to guide the reprojected features generated by the existing approach. Notably, to ensure consistent 3D predictions, ID-NeRF deviates from the direct supervision method commonly used in previous 3D generative models and instead adopts an innovative indirect prior injection strategy. This strategy involves distilling pre-trained knowledge into a latent space through score-based distillation, followed by the introduction of an attention-based refinement module to leverage the embedded priors for enhancing reprojected features obtained from sparse inputs. Extensive experiments on various datasets validate the effectiveness of the proposed method in synthesizing novel views in a versatile manner, particularly in scenarios with limited data."}, "2207.02803": {"original_text": "  Recent advances in face forgery techniques produce nearly visually\nuntraceable deepfake videos, which could be leveraged with malicious\nintentions. As a result, researchers have been devoted to deepfake detection.\nPrevious studies have identified the importance of local low-level cues and\ntemporal information in pursuit to generalize well across deepfake methods,\nhowever, they still suffer from robustness problem against post-processings. In\nthis work, we propose the Local- & Temporal-aware Transformer-based Deepfake\nDetection (LTTD) framework, which adopts a local-to-global learning protocol\nwith a particular focus on the valuable temporal information within local\nsequences. Specifically, we propose a Local Sequence Transformer (LST), which\nmodels the temporal consistency on sequences of restricted spatial regions,\nwhere low-level information is hierarchically enhanced with shallow layers of\nlearned 3D filters. Based on the local temporal embeddings, we then achieve the\nfinal classification in a global contrastive way. Extensive experiments on\npopular datasets validate that our approach effectively spots local forgery\ncues and achieves state-of-the-art performance.\n", "rewritten_text": "Recent advancements in face manipulation techniques have led to the creation of deepfake videos that are extremely difficult to detect visually. This poses a potential threat as these videos could be used for malicious purposes. To address this issue, researchers have focused on developing methods to detect deepfakes. While previous studies have highlighted the importance of analyzing local visual cues and temporal information to improve detection accuracy across different deepfake methods, they have struggled with vulnerabilities to post-processing techniques. In this study, we introduce the Local- & Temporal-aware Transformer-based Deepfake Detection (LTTD) framework, which employs a local-to-global learning approach that emphasizes the significance of temporal information within local sequences. Specifically, we introduce a Local Sequence Transformer (LST) that captures temporal consistency within spatially restricted regions by enhancing low-level information using learned 3D filters in shallow layers. By leveraging local temporal embeddings, we are able to classify deepfakes in a global contrastive manner. Our extensive experiments on widely used datasets demonstrate that our approach effectively identifies local manipulation cues and achieves superior performance compared to existing methods."}, "2210.16621": {"original_text": "  Transformer-based architectures like BERT have achieved great success in a\nwide range of Natural Language tasks. Despite their decent performance, the\nmodels still have numerous parameters and high computational complexity,\nimpeding their deployment in resource-constrained environments. Post-Training\nQuantization (PTQ), which enables low-bit computations without extra training,\ncould be a promising tool. In this work, we conduct an empirical evaluation of\nthree PTQ methods on BERT-Base and BERT-Large: Linear Quantization (LQ),\nAnalytical Clipping for Integer Quantization (ACIQ), and Outlier Channel\nSplitting (OCS). OCS theoretically surpasses the others in minimizing the Mean\nSquare quantization Error and avoiding distorting the weights' outliers. That\nis consistent with the evaluation results of most language tasks of GLUE\nbenchmark and a reading comprehension task, SQuAD. Moreover, low-bit quantized\nBERT models could outperform the corresponding 32-bit baselines on several\nsmall language tasks, which we attribute to the alleviation of\nover-parameterization. We further explore the limit of quantization bit and\nshow that OCS could quantize BERT-Base and BERT-Large to 3-bits and retain 98%\nand 96% of the performance on the GLUE benchmark accordingly. Moreover, we\nconduct quantization on the whole BERT family, i.e., BERT models in different\nconfigurations, and comprehensively evaluate their performance on the GLUE\nbenchmark and SQuAD, hoping to provide valuable guidelines for their deployment\nin various computation environments.\n", "rewritten_text": "Transformer-based models such as BERT have been successful in various Natural Language tasks, but their large number of parameters and high computational complexity make them challenging to deploy in resource-limited settings. Post-Training Quantization (PTQ) offers a solution by enabling low-bit computations without additional training. This study evaluates three PTQ methods - Linear Quantization (LQ), Analytical Clipping for Integer Quantization (ACIQ), and Outlier Channel Splitting (OCS) - on BERT-Base and BERT-Large models. OCS shows promise in minimizing quantization error and preserving weight accuracy, as demonstrated in evaluations on language tasks like GLUE and SQuAD. Low-bit quantized BERT models even outperform their 32-bit counterparts on smaller tasks, potentially due to reduced over-parameterization. The study explores the limits of quantization bits and finds that OCS can quantize BERT-Base and BERT-Large to 3 bits while maintaining high performance on the GLUE benchmark. Additionally, quantization is applied to the entire BERT family, across different configurations, with performance evaluated on GLUE and SQuAD tasks to offer insights for deployment in diverse computational environments."}, "2401.10768": {"original_text": "  While large language models (LLMs) have demonstrated exceptional performance\nacross various tasks following human alignment, they may still generate\nresponses that sound plausible but contradict factual knowledge, a phenomenon\nknown as hallucination. In this paper, we demonstrate the feasibility of\nmitigating hallucinations by verifying and minimizing the inconsistency between\nexternal knowledge present in the alignment data and the intrinsic knowledge\nembedded within foundation LLMs. Specifically, we propose a novel approach\ncalled Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM\nto automatically formulate assessments based on external knowledge to evaluate\nthe knowledge boundaries of foundation LLMs. To address knowledge\ninconsistencies in the alignment data, KCA implements several specific\nstrategies to deal with these data instances. We demonstrate the superior\nefficacy of KCA in reducing hallucinations across six benchmarks, utilizing\nfoundation LLMs of varying backbones and scales. This confirms the\neffectiveness of mitigating hallucinations by reducing knowledge inconsistency.\nOur code, model weights, and data are openly accessible at\n\\url{https://github.com/fanqiwan/KCA}.\n", "rewritten_text": "\"In this paper, we explore how large language models (LLMs) can sometimes generate responses that appear plausible but contradict factual knowledge, a phenomenon known as hallucination. We introduce a new method called Knowledge Consistent Alignment (KCA) to address this issue by verifying and minimizing inconsistencies between external knowledge in alignment data and the inherent knowledge within LLMs. KCA utilizes a well-aligned LLM to assess the knowledge boundaries of foundation LLMs based on external knowledge. We demonstrate the effectiveness of KCA in reducing hallucinations across various benchmarks, using different types and sizes of LLMs. Our research confirms that reducing knowledge inconsistencies can significantly mitigate hallucinations. The code, model weights, and data are publicly available at \\url{https://github.com/fanqiwan/KCA}.\""}, "2201.01016": {"original_text": "  Recovering detailed facial geometry from a set of calibrated multi-view\nimages is valuable for its wide range of applications. Traditional multi-view\nstereo (MVS) methods adopt an optimization-based scheme to regularize the\nmatching cost. Recently, learning-based methods integrate all these into an\nend-to-end neural network and show superiority of efficiency. In this paper, we\npropose a novel architecture to recover extremely detailed 3D faces within\ndozens of seconds. Unlike previous learning-based methods that regularize the\ncost volume via 3D CNN, we propose to learn an implicit function for regressing\nthe matching cost. By fitting a 3D morphable model from multi-view images, the\nfeatures of multiple images are extracted and aggregated in the mesh-attached\nUV space, which makes the implicit function more effective in recovering\ndetailed facial shape. Our method outperforms SOTA learning-based MVS in\naccuracy by a large margin on the FaceScape dataset. The code and data are\nreleased in https://github.com/zhuhao-nju/mvfr.\n", "rewritten_text": "In this study, we introduce a new approach for reconstructing highly detailed 3D facial geometry from a series of calibrated multi-view images. Traditional methods for multi-view stereo (MVS) rely on optimization techniques to refine the matching process. Recently, there has been a shift towards learning-based methods that leverage neural networks for improved efficiency. Our proposed architecture aims to rapidly generate detailed 3D facial reconstructions in just seconds. Unlike previous approaches that use 3D CNNs to regularize cost volumes, we employ an implicit function to estimate matching costs. By leveraging a 3D morphable model and extracting features from multiple images in UV space, our method effectively captures intricate facial details. Our approach significantly outperforms state-of-the-art learning-based MVS methods in accuracy on the FaceScape dataset. The code and data for our method are available at https://github.com/zhuhao-nju/mvfr."}, "1809.05752": {"original_text": "  Readmission after discharge from a hospital is disruptive and costly,\nregardless of the reason. However, it can be particularly problematic for\npsychiatric patients, so predicting which patients may be readmitted is\ncritically important but also very difficult. Clinical narratives in\npsychiatric electronic health records (EHRs) span a wide range of topics and\nvocabulary; therefore, a psychiatric readmission prediction model must begin\nwith a robust and interpretable topic extraction component. We created a data\npipeline for using document vector similarity metrics to perform topic\nextraction on psychiatric EHR data in service of our long-term goal of creating\na readmission risk classifier. We show initial results for our topic extraction\nmodel and identify additional features we will be incorporating in the future.\n", "rewritten_text": "\"Returning to the hospital after being discharged is both disruptive and costly, regardless of the cause. This is especially challenging for psychiatric patients, making it crucial yet difficult to predict which patients may need readmission. The clinical narratives found in psychiatric electronic health records cover a wide range of topics and vocabulary, highlighting the need for a strong and understandable topic extraction component in a psychiatric readmission prediction model. To address this, we have developed a data pipeline that utilizes document vector similarity metrics for topic extraction from psychiatric EHR data, with the ultimate goal of creating a readmission risk classifier. Our initial results on the topic extraction model are presented, along with plans to incorporate additional features in the future.\""}, "2302.1443": {"original_text": "  3D hand tracking methods based on monocular RGB videos are easily affected by\nmotion blur, while event camera, a sensor with high temporal resolution and\ndynamic range, is naturally suitable for this task with sparse output and low\npower consumption. However, obtaining 3D annotations of fast-moving hands is\ndifficult for constructing event-based hand-tracking datasets. In this paper,\nwe provided an event-based speed adaptive hand tracker (ESAHT) to solve the\nhand tracking problem based on event camera. We enabled a CNN model trained on\na hand tracking dataset with slow motion, which enabled the model to leverage\nthe knowledge of RGB-based hand tracking solutions, to work on fast hand\ntracking tasks. To realize our solution, we constructed the first 3D hand\ntracking dataset captured by an event camera in a real-world environment,\nfigured out two data augment methods to narrow the domain gap between slow and\nfast motion data, developed a speed adaptive event stream segmentation method\nto handle hand movements in different moving speeds, and introduced a new\nevent-to-frame representation method adaptive to event streams with different\nlengths. Experiments showed that our solution outperformed RGB-based as well as\nprevious event-based solutions in fast hand tracking tasks, and our codes and\ndataset will be publicly available.\n", "rewritten_text": "3D hand tracking methods using monocular RGB videos can be affected by motion blur, whereas event cameras, known for their high temporal resolution and dynamic range, are well-suited for this task due to their sparse output and low power consumption. However, creating 3D annotations for quickly moving hands is challenging when building event-based hand-tracking datasets. This paper introduces an event-based speed adaptive hand tracker (ESAHT) designed to address hand tracking using event cameras. By training a CNN model on a hand tracking dataset with slow motion, the model can leverage knowledge from RGB-based hand tracking solutions to handle fast hand tracking tasks. To implement this solution, a 3D hand tracking dataset captured by an event camera in a real-world setting was created. Additionally, two data augmentation methods were developed to bridge the gap between slow and fast motion data, a speed adaptive event stream segmentation method was devised to accommodate hand movements at varying speeds, and a new event-to-frame representation method was introduced to adapt to event streams of different lengths. Experimental results demonstrate that this solution outperforms both RGB-based and previous event-based approaches in fast hand tracking tasks, and the codes and dataset will be made publicly available."}, "2407.15153": {"original_text": "  Video generation has drawn significant interest recently, pushing the\ndevelopment of large-scale models capable of producing realistic videos with\ncoherent motion. Due to memory constraints, these models typically generate\nshort video segments that are then combined into long videos. The merging\nprocess poses a significant challenge, as it requires ensuring smooth\ntransitions and overall consistency. In this paper, we introduce Anchored\nDiffusion, a novel method for synthesizing relatively long and seamless videos.\nWe extend Diffusion Transformers (DiTs) to incorporate temporal information,\ncreating our sequence-DiT (sDiT) model for generating short video segments.\nUnlike previous works, we train our model on video sequences with random\nnon-uniform temporal spacing and incorporate temporal information via external\nguidance, increasing flexibility and allowing it to capture both short and\nlong-term relationships. Furthermore, during inference, we leverage the\ntransformer architecture to modify the diffusion process, generating a batch of\nnon-uniform sequences anchored to a common frame, ensuring consistency\nregardless of temporal distance. To demonstrate our method, we focus on face\nreenactment, the task of creating a video from a source image that replicates\nthe facial expressions and movements from a driving video. Through\ncomprehensive experiments, we show our approach outperforms current techniques\nin producing longer consistent high-quality videos while offering editing\ncapabilities.\n", "rewritten_text": "Recently, there has been a growing interest in video generation, leading to the development of large-scale models that can create realistic videos with smooth motion. These models often generate short video segments due to memory limitations, which are then combined to form longer videos. The merging process presents a challenge in ensuring seamless transitions and overall coherence. This paper introduces Anchored Diffusion, a new method for producing long and continuous videos. By enhancing Diffusion Transformers (DiTs) with temporal information, we introduce the sequence-DiT (sDiT) model for generating short video segments. Unlike previous approaches, our model is trained on video sequences with random temporal spacing and incorporates temporal information through external guidance, enabling it to capture both short and long-term relationships. During inference, we utilize the transformer architecture to adjust the diffusion process, creating a batch of non-uniform sequences anchored to a common frame to ensure consistency regardless of temporal gaps. Our focus is on face reenactment, where a video is generated from a source image to mimic facial expressions and movements from a reference video. Through extensive experiments, we demonstrate that our method surpasses current techniques in producing longer, coherent, high-quality videos while also offering editing capabilities."}, "2009.03116": {"original_text": "  This paper presents the first Swedish evaluation benchmark for textual\nsemantic similarity. The benchmark is compiled by simply running the English\nSTS-B dataset through the Google machine translation API. This paper discusses\npotential problems with using such a simple approach to compile a Swedish\nevaluation benchmark, including translation errors, vocabulary variation, and\nproductive compounding. Despite some obvious problems with the resulting\ndataset, we use the benchmark to compare the majority of the currently existing\nSwedish text representations, demonstrating that native models outperform\nmultilingual ones, and that simple bag of words performs remarkably well.\n", "rewritten_text": "This paper introduces the initial Swedish assessment benchmark for textual semantic similarity. The benchmark is created by translating the English STS-B dataset using the Google machine translation API. The paper examines the challenges of utilizing this straightforward method to develop a Swedish evaluation benchmark, such as translation inaccuracies, vocabulary differences, and compound word formation. Despite the limitations of the dataset, the benchmark is employed to evaluate various Swedish text representations, revealing that native models surpass multilingual ones and that a basic bag of words approach performs exceptionally well."}, "2106.16138": {"original_text": "  In this paper, we introduce ELECTRA-style tasks to cross-lingual language\nmodel pre-training. Specifically, we present two pre-training tasks, namely\nmultilingual replaced token detection, and translation replaced token\ndetection. Besides, we pretrain the model, named as XLM-E, on both multilingual\nand parallel corpora. Our model outperforms the baseline models on various\ncross-lingual understanding tasks with much less computation cost. Moreover,\nanalysis shows that XLM-E tends to obtain better cross-lingual transferability.\n", "rewritten_text": "In this article, we incorporate ELECTRA-style tasks into cross-lingual language model pre-training. We introduce two pre-training tasks: multilingual replaced token detection and translation replaced token detection. Additionally, we pretrain the model, known as XLM-E, using both multilingual and parallel corpora. Our model surpasses baseline models in various cross-lingual understanding tasks while requiring less computation. Furthermore, analysis indicates that XLM-E demonstrates improved cross-lingual transferability."}, "2101.03929": {"original_text": "  Learning to capture dependencies between spatial positions is essential to\nmany visual tasks, especially the dense labeling problems like scene parsing.\nExisting methods can effectively capture long-range dependencies with\nself-attention mechanism while short ones by local convolution. However, there\nis still much gap between long-range and short-range dependencies, which\nlargely reduces the models' flexibility in application to diverse spatial\nscales and relationships in complicated natural scene images. To fill such a\ngap, we develop a Middle-Range (MR) branch to capture middle-range dependencies\nby restricting self-attention into local patches. Also, we observe that the\nspatial regions which have large correlations with others can be emphasized to\nexploit long-range dependencies more accurately, and thus propose a Reweighed\nLong-Range (RLR) branch. Based on the proposed MR and RLR branches, we build an\nOmni-Range Dependencies Network (ORDNet) which can effectively capture short-,\nmiddle- and long-range dependencies. Our ORDNet is able to extract more\ncomprehensive context information and well adapt to complex spatial variance in\nscene images. Extensive experiments show that our proposed ORDNet outperforms\nprevious state-of-the-art methods on three scene parsing benchmarks including\nPASCAL Context, COCO Stuff and ADE20K, demonstrating the superiority of\ncapturing omni-range dependencies in deep models for scene parsing task.\n", "rewritten_text": "Learning how to capture relationships between spatial positions is crucial for various visual tasks, particularly for complex labeling challenges like scene parsing. While current methods can effectively handle long-range dependencies using self-attention and short-range ones with local convolution, there remains a significant disparity between the two. This gap limits the adaptability of models across different spatial scales and relationships in intricate natural scene images. To address this issue, we introduce a Middle-Range (MR) branch that focuses on capturing middle-range dependencies through localized self-attention patches. Additionally, we identify that spatial regions with strong correlations can be highlighted to better leverage long-range dependencies, leading to the proposal of a Reweighed Long-Range (RLR) branch. By combining the MR and RLR branches, we create an Omni-Range Dependencies Network (ORDNet) capable of effectively capturing short-, middle-, and long-range dependencies. Our ORDNet enhances context extraction and adapts well to spatial variations in scene images. Extensive experiments demonstrate that our proposed ORDNet surpasses previous state-of-the-art methods on three scene parsing benchmarks (PASCAL Context, COCO Stuff, and ADE20K), showcasing the advantages of incorporating omni-range dependencies in deep models for scene parsing tasks."}, "2012.08514": {"original_text": "  In this paper, we propose an end-end model for producing furniture layout for\ninterior scene synthesis from the random vector. This proposed model is aimed\nto support professional interior designers to produce the interior decoration\nsolutions more quickly. The proposed model combines a conditional floor-plan\nmodule of the room, a conditional graphical floor-plan module of the room and a\nconditional layout module. As compared with the prior work on scene synthesis,\nour proposed three modules enhance the ability of auto-layout generation given\nthe dimensional category of the room. We conduct our experiments on the\nproposed real-world interior layout dataset that contains $191208$ designs from\nthe professional designers. Our numerical results demonstrate that the proposed\nmodel yields higher-quality layouts in comparison with the state-of-the-art\nmodel. The dataset and code are released\n\\href{https://github.com/CODE-SUBMIT/dataset3}{Dataset,Code}\n", "rewritten_text": "In this paper, we introduce a comprehensive model for generating furniture layouts in interior scenes using a random vector. The main goal of this model is to assist professional interior designers in creating interior decoration solutions more efficiently. Our model integrates a conditional floor-plan module, a conditional graphical floor-plan module, and a conditional layout module. These three modules, when compared to previous approaches in scene synthesis, improve the automatic layout generation based on the room's dimensions. We evaluate our model using a real-world interior layout dataset comprising 191,208 designs by professional designers. Our experimental results show that our model produces higher-quality layouts compared to existing models. The dataset and code can be accessed at the following link: [Dataset, Code](https://github.com/CODE-SUBMIT/dataset3)."}, "2004.14525": {"original_text": "  Inverted bottleneck layers, which are built upon depthwise convolutions, have\nbeen the predominant building blocks in state-of-the-art object detection\nmodels on mobile devices. In this work, we investigate the optimality of this\ndesign pattern over a broad range of mobile accelerators by revisiting the\nusefulness of regular convolutions. We discover that regular convolutions are a\npotent component to boost the latency-accuracy trade-off for object detection\non accelerators, provided that they are placed strategically in the network via\nneural architecture search. By incorporating regular convolutions in the search\nspace and directly optimizing the network architectures for object detection,\nwe obtain a family of object detection models, MobileDets, that achieve\nstate-of-the-art results across mobile accelerators. On the COCO object\ndetection task, MobileDets outperform MobileNetV3+SSDLite by 1.7 mAP at\ncomparable mobile CPU inference latencies. MobileDets also outperform\nMobileNetV2+SSDLite by 1.9 mAP on mobile CPUs, 3.7 mAP on Google EdgeTPU, 3.4\nmAP on Qualcomm Hexagon DSP and 2.7 mAP on Nvidia Jetson GPU without increasing\nlatency. Moreover, MobileDets are comparable with the state-of-the-art MnasFPN\non mobile CPUs even without using the feature pyramid, and achieve better mAP\nscores on both EdgeTPUs and DSPs with up to 2x speedup. Code and models are\navailable in the TensorFlow Object Detection API:\nhttps://github.com/tensorflow/models/tree/master/research/object_detection.\n", "rewritten_text": "\"In this study, we explore the effectiveness of using regular convolutions alongside inverted bottleneck layers, which are based on depthwise convolutions and have been widely used in cutting-edge object detection models for mobile devices. By incorporating regular convolutions strategically in the network through neural architecture search, we find that they play a crucial role in balancing latency and accuracy trade-offs for object detection on various mobile accelerators. Our research introduces a new family of object detection models called MobileDets, which outperform existing models like MobileNetV3+SSDLite and MobileNetV2+SSDLite on different mobile accelerators without increasing latency. MobileDets achieve state-of-the-art results on the COCO object detection task and demonstrate superior performance on mobile CPUs, Google EdgeTPU, Qualcomm Hexagon DSP, and Nvidia Jetson GPU. Additionally, MobileDets are comparable to MnasFPN on mobile CPUs and outperform it on EdgeTPUs and DSPs with significant speed improvements. The code and models for MobileDets are available in the TensorFlow Object Detection API repository.\""}, "2310.00031": {"original_text": "  Diffusion models are generative models with impressive text-to-image\nsynthesis capabilities and have spurred a new wave of creative methods for\nclassical machine learning tasks. However, the best way to harness the\nperceptual knowledge of these generative models for visual tasks is still an\nopen question. Specifically, it is unclear how to use the prompting interface\nwhen applying diffusion backbones to vision tasks. We find that automatically\ngenerated captions can improve text-image alignment and significantly enhance a\nmodel's cross-attention maps, leading to better perceptual performance. Our\napproach improves upon the current state-of-the-art (SOTA) in diffusion-based\nsemantic segmentation on ADE20K and the current overall SOTA for depth\nestimation on NYUv2. Furthermore, our method generalizes to the cross-domain\nsetting. We use model personalization and caption modifications to align our\nmodel to the target domain and find improvements over unaligned baselines. Our\ncross-domain object detection model, trained on Pascal VOC, achieves SOTA\nresults on Watercolor2K. Our cross-domain segmentation method, trained on\nCityscapes, achieves SOTA results on Dark Zurich-val and Nighttime Driving.\nProject page: https://www.vision.caltech.edu/tadp/. Code:\nhttps://github.com/damaggu/TADP.\n", "rewritten_text": "Diffusion models, known for their impressive text-to-image synthesis capabilities, have sparked a new wave of innovative approaches in traditional machine learning tasks. However, the optimal way to leverage the perceptual knowledge of these generative models for visual tasks remains a topic of ongoing exploration. In particular, the effective utilization of the prompting interface in conjunction with diffusion backbones for vision tasks is not yet clearly defined. Through our research, we have discovered that utilizing automatically generated captions can enhance text-image alignment and improve a model's cross-attention maps, resulting in enhanced perceptual performance. Our approach surpasses the current state-of-the-art in diffusion-based semantic segmentation on ADE20K and sets a new benchmark for depth estimation on NYUv2. Moreover, our method demonstrates effectiveness in cross-domain scenarios by employing model personalization and caption adjustments to align the model with the target domain, leading to performance enhancements over unaligned baselines. For instance, our cross-domain object detection model, trained on Pascal VOC, achieves state-of-the-art results on Watercolor2K. Similarly, our cross-domain segmentation method, trained on Cityscapes, achieves top performance on Dark Zurich-val and Nighttime Driving datasets. For more information, please visit our project page at https://www.vision.caltech.edu/tadp/. Source code is available at https://github.com/damaggu/TADP."}, "2402.15930": {"original_text": "  The writing examples of English language learners may be different from those\nof native speakers. Given that there is a significant differences in second\nlanguage (L2) learners' error types by their proficiency levels, this paper\nattempts to reduce overcorrection by examining the interaction between LLM's\nperformance and L2 language proficiency. Our method focuses on zero-shot and\nfew-shot prompting and fine-tuning models for GEC for learners of English as a\nforeign language based on the different proficiency. We investigate GEC results\nand find that overcorrection happens primarily in advanced language learners'\nwriting (proficiency C) rather than proficiency A (a beginner level) and\nproficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot\nprompting with writing examples of English learners, actually tend to exhibit\ndecreased recall measures. To make our claim concrete, we conduct a\ncomprehensive examination of GEC outcomes and their evaluation results based on\nlanguage proficiency.\n", "rewritten_text": "The writing produced by English language learners may vary from that of native speakers. This study aims to address the differences in error types among second language (L2) learners based on their proficiency levels, with a focus on reducing overcorrection. By analyzing the relationship between LLM performance and L2 proficiency, we utilize zero-shot and few-shot prompting techniques to fine-tune models for Grammar Error Correction (GEC) tailored to English as a foreign language learners of different proficiency levels. Our findings indicate that overcorrection is more prevalent in advanced language learners (proficiency level C) compared to beginners (proficiency level A) and intermediate learners (proficiency level B). Interestingly, fine-tuned LLMs and few-shot prompting using writing samples from English learners show reduced recall measures. To support our findings, we conduct a thorough evaluation of GEC outcomes based on language proficiency levels."}, "2306.10963": {"original_text": "  Adversarial patches are still a simple yet powerful white box attack that can\nbe used to fool object detectors by suppressing possible detections. The\npatches of these so-called evasion attacks are computational expensive to\nproduce and require full access to the attacked detector. This paper addresses\nthe problem of computational expensiveness by analyzing 375 generated patches,\ncalculating the principal components of these and show, that linear\ncombinations of the resulting \"eigenpatches\" can be used to fool object\ndetections successfully.\n", "rewritten_text": "\"Adversarial patches remain a straightforward yet potent white box attack method that can deceive object detectors by obstructing potential detections. These evasion attacks involve patches that are costly to create computationally and necessitate complete access to the targeted detector. This study tackles the issue of computational expense by examining 375 generated patches, determining their principal components, and demonstrating that linear combinations of the resulting 'eigenpatches' can effectively deceive object detectors.\""}, "1806.03028": {"original_text": "  Vehicle Make and Model Recognition (MMR) systems provide a fully automatic\nframework to recognize and classify different vehicle models. Several\napproaches have been proposed to address this challenge, however they can\nperform in restricted conditions. Here, we formulate the vehicle make and model\nrecognition as a fine-grained classification problem and propose a new\nconfigurable on-road vehicle make and model recognition framework. We benefit\nfrom the unsupervised feature learning methods and in more details we employ\nLocality constraint Linear Coding (LLC) method as a fast feature encoder for\nencoding the input SIFT features. The proposed method can perform in real\nenvironments of different conditions. This framework can recognize fifty models\nof vehicles and has an advantage to classify every other vehicle not belonging\nto one of the specified fifty classes as an unknown vehicle. The proposed MMR\nframework can be configured to become faster or more accurate based on the\napplication domain. The proposed approach is examined on two datasets including\nIranian on-road vehicle dataset and CompuCar dataset. The Iranian on-road\nvehicle dataset contains images of 50 models of vehicles captured in real\nsituations by traffic cameras in different weather and lighting conditions.\nExperimental results show superiority of the proposed framework over the\nstate-of-the-art methods on Iranian on-road vehicle datatset and comparable\nresults on CompuCar dataset with 97.5% and 98.4% accuracies, respectively.\n", "rewritten_text": "Vehicle Make and Model Recognition (MMR) systems offer a fully automated solution for identifying and categorizing various vehicle models. While existing approaches have limitations in certain conditions, we present a new adaptable on-road vehicle make and model recognition framework. By treating the recognition task as a fine-grained classification problem, we leverage unsupervised feature learning techniques, specifically utilizing the Locality constraint Linear Coding (LLC) method for efficient feature encoding of SIFT features. Our method is designed to operate effectively in diverse real-world environments. The framework can identify fifty different vehicle models and has the capability to classify any other vehicle not within these specified classes as an unknown vehicle. Additionally, the MMR framework can be adjusted for speed or accuracy depending on the specific application requirements. We evaluate our approach on two datasets, the Iranian on-road vehicle dataset and the CompuCar dataset. The Iranian dataset comprises images of 50 vehicle models captured in various real-world scenarios with different weather and lighting conditions. Experimental results demonstrate the superior performance of our framework compared to state-of-the-art methods, achieving accuracies of 97.5% and 98.4% on the Iranian and CompuCar datasets, respectively."}, "2110.10575": {"original_text": "  Recent research in opinion mining proposed word embedding-based topic\nmodeling methods that provide superior coherence compared to traditional topic\nmodeling. In this paper, we demonstrate how these methods can be used to\ndisplay correlated topic models on social media texts using SocialVisTUM, our\nproposed interactive visualization toolkit. It displays a graph with topics as\nnodes and their correlations as edges. Further details are displayed\ninteractively to support the exploration of large text collections, e.g.,\nrepresentative words and sentences of topics, topic and sentiment\ndistributions, hierarchical topic clustering, and customizable, predefined\ntopic labels. The toolkit optimizes automatically on custom data for optimal\ncoherence. We show a working instance of the toolkit on data crawled from\nEnglish social media discussions about organic food consumption. The\nvisualization confirms findings of a qualitative consumer research study.\nSocialVisTUM and its training procedures are accessible online.\n", "rewritten_text": "Recent studies in sentiment analysis have introduced new methods for topic modeling using word embeddings, which have shown to be more coherent than traditional approaches. This paper showcases how these techniques can be applied to social media texts through SocialVisTUM, an interactive visualization tool. The tool presents a graph where topics are represented as nodes and their relationships as edges. Users can explore various details interactively, such as key words and sentences for each topic, sentiment distributions, hierarchical topic clustering, and customizable labels. The toolkit automatically optimizes for coherence based on the input data. An example of the tool in action is demonstrated using data from English social media discussions on organic food consumption, aligning with findings from consumer research. SocialVisTUM and its training resources are available online for accessibility."}, "2404.00851": {"original_text": "  Pre-trained vision-language models have shown impressive success on various\ncomputer vision tasks with their zero-shot generalizability. Recently, prompt\nlearning approaches have been explored to efficiently and effectively adapt the\nvision-language models to a variety of downstream tasks. However, most existing\nprompt learning methods suffer from task overfitting since the general\nknowledge of the pre-trained vision language models is forgotten while the\nprompts are finetuned on a small data set from a specific target task. To\naddress this issue, we propose a Prompt Meta-Regularization (ProMetaR) to\nimprove the generalizability of prompt learning for vision-language models.\nSpecifically, ProMetaR meta-learns both the regularizer and the soft prompts to\nharness the task-specific knowledge from the downstream tasks and task-agnostic\ngeneral knowledge from the vision-language models. Further, ProMetaR augments\nthe task to generate multiple virtual tasks to alleviate the meta-overfitting.\nIn addition, we provide the analysis to comprehend how ProMetaR improves the\ngeneralizability of prompt tuning in the perspective of the gradient alignment.\nOur extensive experiments demonstrate that our ProMetaR improves the\ngeneralizability of conventional prompt learning methods under\nbase-to-base/base-to-new and domain generalization settings. The code of\nProMetaR is available at https://github.com/mlvlab/ProMetaR.\n", "rewritten_text": "\"Pre-trained vision-language models have achieved remarkable success in various computer vision tasks due to their ability to generalize without prior training. Recently, researchers have been exploring prompt learning techniques to efficiently adapt these models to different downstream tasks. However, many existing prompt learning methods struggle with overfitting to specific tasks, as they tend to forget the general knowledge of the pre-trained models while fine-tuning prompts on limited data from a particular task. To tackle this challenge, we introduce Prompt Meta-Regularization (ProMetaR) to enhance the generalizability of prompt learning for vision-language models. ProMetaR meta-learns both the regularizer and soft prompts to leverage task-specific knowledge from downstream tasks and general knowledge from the pre-trained models. Additionally, ProMetaR generates multiple virtual tasks to mitigate meta-overfitting. We also provide an analysis of how ProMetaR enhances prompt tuning generalizability through gradient alignment. Extensive experiments show that ProMetaR outperforms traditional prompt learning methods in base-to-base, base-to-new, and domain generalization scenarios. The code for ProMetaR can be found at https://github.com/mlvlab/ProMetaR.\""}, "2409.12539": {"original_text": "  In radiation therapy (RT), the reliance on pre-treatment computed tomography\n(CT) images encounter challenges due to anatomical changes, necessitating\nadaptive planning. Daily cone-beam CT (CBCT) imaging, pivotal for therapy\nadjustment, falls short in tissue density accuracy. To address this, our\ninnovative approach integrates diffusion models for CT image generation,\noffering precise control over data synthesis. Leveraging a self-training method\nwith knowledge distillation, we maximize CBCT data during therapy, complemented\nby sparse paired fan-beam CTs. This strategy, incorporated into\nstate-of-the-art diffusion-based models, surpasses conventional methods like\nPix2pix and CycleGAN. A meticulously curated dataset of 2800 paired CBCT and CT\nscans, supplemented by 4200 CBCT scans, undergoes preprocessing and teacher\nmodel training, including the Brownian Bridge Diffusion Model (BBDM).\nPseudo-label CT images are generated, resulting in a dataset combining 5600 CT\nimages with corresponding CBCT images. Thorough evaluation using MSE, SSIM,\nPSNR and LPIPS demonstrates superior performance against Pix2pix and CycleGAN.\nOur approach shows promise in generating high-quality CT images from CBCT scans\nin RT.\n", "rewritten_text": "\"In radiation therapy (RT), challenges arise when relying on pre-treatment computed tomography (CT) images due to anatomical changes, requiring adaptive planning. Daily cone-beam CT (CBCT) imaging is crucial for therapy adjustments but lacks accuracy in tissue density. Our innovative method integrates diffusion models to generate CT images with precise data synthesis. By utilizing a self-training approach with knowledge distillation, we enhance CBCT data quality during therapy, along with sparse paired fan-beam CTs. This strategy, integrated into advanced diffusion-based models, outperforms traditional methods like Pix2pix and CycleGAN. A carefully curated dataset of 2800 paired CBCT and CT scans, supplemented by 4200 CBCT scans, undergoes preprocessing and teacher model training, including the Brownian Bridge Diffusion Model (BBDM). Pseudo-label CT images are produced, resulting in a dataset of 5600 CT images paired with corresponding CBCT images. Comprehensive evaluation using MSE, SSIM, PSNR, and LPIPS demonstrates superior performance compared to Pix2pix and CycleGAN. Our approach shows potential in generating high-quality CT images from CBCT scans in RT.\""}, "1904.0169": {"original_text": "  We present MonoPSR, a monocular 3D object detection method that leverages\nproposals and shape reconstruction. First, using the fundamental relations of a\npinhole camera model, detections from a mature 2D object detector are used to\ngenerate a 3D proposal per object in a scene. The 3D location of these\nproposals prove to be quite accurate, which greatly reduces the difficulty of\nregressing the final 3D bounding box detection. Simultaneously, a point cloud\nis predicted in an object centered coordinate system to learn local scale and\nshape information. However, the key challenge is how to exploit shape\ninformation to guide 3D localization. As such, we devise aggregate losses,\nincluding a novel projection alignment loss, to jointly optimize these tasks in\nthe neural network to improve 3D localization accuracy. We validate our method\non the KITTI benchmark where we set new state-of-the-art results among\npublished monocular methods, including the harder pedestrian and cyclist\nclasses, while maintaining efficient run-time.\n", "rewritten_text": "We introduce MonoPSR, a monocular 3D object detection approach that utilizes proposals and shape reconstruction. Initially, based on the principles of a pinhole camera model, detections from a well-established 2D object detector are employed to create a 3D proposal for each object within a scene. The accuracy of the 3D locations of these proposals significantly simplifies the process of determining the final 3D bounding box detection. Additionally, a point cloud is forecasted in an object-centered coordinate system to grasp local scale and shape details. Nevertheless, the primary challenge lies in effectively utilizing shape data to assist in 3D localization. To address this, we introduce combined losses, including a unique projection alignment loss, to collectively enhance these tasks within the neural network and enhance 3D localization precision. Our method is assessed on the KITTI benchmark, where we achieve new leading results compared to other monocular methods, particularly in the more challenging pedestrian and cyclist categories, all while maintaining efficient runtime."}, "2408.02291": {"original_text": "  Unsupervised 3D keypoints estimation from Point Cloud Data (PCD) is a complex\ntask, even more challenging when an object shape is deforming. As keypoints\nshould be semantically and geometrically consistent across all the 3D frames -\neach keypoint should be anchored to a specific part of the deforming shape\nirrespective of intrinsic and extrinsic motion. This paper presents, \"SelfGeo\",\na self-supervised method that computes persistent 3D keypoints of non-rigid\nobjects from arbitrary PCDs without the need of human annotations. The gist of\nSelfGeo is to estimate keypoints between frames that respect invariant\nproperties of deforming bodies. Our main contribution is to enforce that\nkeypoints deform along with the shape while keeping constant geodesic distances\namong them. This principle is then propagated to the design of a set of losses\nwhich minimization let emerge repeatable keypoints in specific semantic\nlocations of the non-rigid shape. We show experimentally that the use of\ngeodesic has a clear advantage in challenging dynamic scenes and with different\nclasses of deforming shapes (humans and animals). Code and data are available\nat: https://github.com/IIT-PAVIS/SelfGeo\n", "rewritten_text": "The task of estimating 3D keypoints from Point Cloud Data (PCD) without supervision is complex, especially when dealing with deforming object shapes. Ensuring that keypoints are consistent both semantically and geometrically across all 3D frames is crucial, anchoring each keypoint to a specific part of the deforming shape regardless of motion. This paper introduces \"SelfGeo,\" a self-supervised approach for identifying persistent 3D keypoints on non-rigid objects from various PCDs without requiring human annotations. SelfGeo focuses on estimating keypoints that maintain invariant properties of deforming bodies between frames. Our key contribution is enforcing that keypoints deform in conjunction with the shape while preserving constant geodesic distances among them. This principle guides the development of a series of loss functions aimed at producing consistent keypoints in specific semantic locations on non-rigid shapes. Experimental results demonstrate the effectiveness of utilizing geodesic distances in challenging dynamic scenarios across different classes of deforming shapes such as humans and animals. The code and data for SelfGeo can be accessed at: https://github.com/IIT-PAVIS/SelfGeo"}, "2304.0723": {"original_text": "  Pedestrian attribute recognition (PAR) has received increasing attention\nbecause of its wide application in video surveillance and pedestrian analysis.\nExtracting robust feature representation is one of the key challenges in this\ntask. The existing methods mainly use the convolutional neural network (CNN) as\nthe backbone network to extract features. However, these methods mainly focus\non small discriminative regions while ignoring the global perspective. To\novercome these limitations, we propose a pure transformer-based multi-task PAR\nnetwork named PARFormer, which includes four modules. In the feature extraction\nmodule, we build a transformer-based strong baseline for feature extraction,\nwhich achieves competitive results on several PAR benchmarks compared with the\nexisting CNN-based baseline methods. In the feature processing module, we\npropose an effective data augmentation strategy named batch random mask (BRM)\nblock to reinforce the attentive feature learning of random patches.\nFurthermore, we propose a multi-attribute center loss (MACL) to enhance the\ninter-attribute discriminability in the feature representations. In the\nviewpoint perception module, we explore the impact of viewpoints on pedestrian\nattributes, and propose a multi-view contrastive loss (MCVL) that enables the\nnetwork to exploit the viewpoint information. In the attribute recognition\nmodule, we alleviate the negative-positive imbalance problem to generate the\nattribute predictions. The above modules interact and jointly learn a highly\ndiscriminative feature space, and supervise the generation of the final\nfeatures. Extensive experimental results show that the proposed PARFormer\nnetwork performs well compared to the state-of-the-art methods on several\npublic datasets, including PETA, RAP, and PA100K. Code will be released at\nhttps://github.com/xwf199/PARFormer.\n", "rewritten_text": "Pedestrian attribute recognition (PAR) has been gaining increased attention due to its broad applications in video surveillance and pedestrian analysis. A key challenge in this task is to extract robust feature representations. While existing methods typically rely on convolutional neural networks (CNNs) for feature extraction, they often focus on small discriminative regions and overlook the global perspective. To address these limitations, we introduce a transformer-based multi-task PAR network called PARFormer, consisting of four modules. \n\nIn the feature extraction module, we establish a transformer-based strong baseline for feature extraction, delivering competitive results on various PAR benchmarks compared to CNN-based methods. The feature processing module introduces a data augmentation strategy called batch random mask (BRM) block to enhance attentive feature learning of random patches. Additionally, we propose a multi-attribute center loss (MACL) to improve inter-attribute discriminability in feature representations. \n\nThe viewpoint perception module investigates the impact of viewpoints on pedestrian attributes and introduces a multi-view contrastive loss (MCVL) to leverage viewpoint information. In the attribute recognition module, we address the negative-positive imbalance issue to generate accurate attribute predictions. These modules collaborate to learn a highly discriminative feature space and supervise the generation of final features. \n\nExtensive experiments demonstrate that the PARFormer network outperforms state-of-the-art methods on public datasets such as PETA, RAP, and PA100K. The code will be available at https://github.com/xwf199/PARFormer."}, "2402.09967": {"original_text": "  Large Language Models (LLMs) excel in generating personalized content and\nfacilitating interactive dialogues, showcasing their remarkable aptitude for a\nmyriad of applications. However, their capabilities in reasoning and providing\nexplainable outputs, especially within the context of reasoning abilities,\nremain areas for improvement. In this study, we delve into the reasoning\nabilities of LLMs, highlighting the current challenges and limitations that\nhinder their effectiveness in complex reasoning scenarios.\n", "rewritten_text": "Large Language Models (LLMs) are highly skilled at creating personalized content and enabling interactive conversations, demonstrating their impressive versatility for various applications. Nevertheless, their proficiency in reasoning and delivering understandable results, particularly in terms of reasoning capabilities, are aspects that could be enhanced. This research explores the reasoning skills of LLMs, emphasizing the existing obstacles and constraints that impede their performance in intricate reasoning situations."}, "2212.05911": {"original_text": "  Deep learning has emerged as an effective solution for solving the task of\nobject detection in images but at the cost of requiring large labeled datasets.\nTo mitigate this cost, semi-supervised object detection methods, which consist\nin leveraging abundant unlabeled data, have been proposed and have already\nshown impressive results. However, most of these methods require linking a\npseudo-label to a ground-truth object by thresholding. In previous works, this\nthreshold value is usually determined empirically, which is time consuming, and\nonly done for a single data distribution. When the domain, and thus the data\ndistribution, changes, a new and costly parameter search is necessary. In this\nwork, we introduce our method Adaptive Self-Training for Object Detection\n(ASTOD), which is a simple yet effective teacher-student method. ASTOD\ndetermines without cost a threshold value based directly on the ground value of\nthe score histogram. To improve the quality of the teacher predictions, we also\npropose a novel pseudo-labeling procedure. We use different views of the\nunlabeled images during the pseudo-labeling step to reduce the number of missed\npredictions and thus obtain better candidate labels. Our teacher and our\nstudent are trained separately, and our method can be used in an iterative\nfashion by replacing the teacher by the student. On the MS-COCO dataset, our\nmethod consistently performs favorably against state-of-the-art methods that do\nnot require a threshold parameter, and shows competitive results with methods\nthat require a parameter sweep search. Additional experiments with respect to a\nsupervised baseline on the DIOR dataset containing satellite images lead to\nsimilar conclusions, and prove that it is possible to adapt the score threshold\nautomatically in self-training, regardless of the data distribution. The code\nis available at https:// github.com/rvandeghen/ASTOD\n", "rewritten_text": "Deep learning has become a popular solution for object detection in images, but it typically relies on large labeled datasets. To address this issue, semi-supervised object detection methods have been developed to utilize unlabeled data, yielding promising results. However, many existing methods require manual thresholding to link pseudo-labels to ground-truth objects, which can be time-consuming and limited to a specific data distribution. In this study, we introduce Adaptive Self-Training for Object Detection (ASTOD), a teacher-student method that automatically determines a threshold value based on the score histogram. We also propose a new pseudo-labeling approach to enhance teacher predictions by considering multiple views of unlabeled images. Our method trains the teacher and student separately and can be iteratively used by switching roles. On the MS-COCO dataset, ASTOD consistently outperforms state-of-the-art methods without requiring a threshold parameter and competes well with methods that do. Experiments on the DIOR dataset demonstrate the adaptability of our approach in automatically adjusting the score threshold during self-training, regardless of the data distribution. The code for ASTOD is available at https://github.com/rvandeghen/ASTOD."}, "1507.02772": {"original_text": "  Data encoded as symmetric positive definite (SPD) matrices frequently arise\nin many areas of computer vision and machine learning. While these matrices\nform an open subset of the Euclidean space of symmetric matrices, viewing them\nthrough the lens of non-Euclidean Riemannian geometry often turns out to be\nbetter suited in capturing several desirable data properties. However,\nformulating classical machine learning algorithms within such a geometry is\noften non-trivial and computationally expensive. Inspired by the great success\nof dictionary learning and sparse coding for vector-valued data, our goal in\nthis paper is to represent data in the form of SPD matrices as sparse conic\ncombinations of SPD atoms from a learned dictionary via a Riemannian geometric\napproach. To that end, we formulate a novel Riemannian optimization objective\nfor dictionary learning and sparse coding in which the representation loss is\ncharacterized via the affine invariant Riemannian metric. We also present a\ncomputationally simple algorithm for optimizing our model. Experiments on\nseveral computer vision datasets demonstrate superior classification and\nretrieval performance using our approach when compared to sparse coding via\nalternative non-Riemannian formulations.\n", "rewritten_text": "Symmetric positive definite (SPD) matrices are commonly encountered in computer vision and machine learning. While these matrices belong to an open subset of symmetric matrices in Euclidean space, a non-Euclidean Riemannian geometry perspective often proves more effective in capturing key data properties. However, adapting traditional machine learning algorithms to this geometry can be challenging and resource-intensive. Drawing inspiration from the success of dictionary learning and sparse coding for vector data, this paper aims to represent SPD matrices as sparse conic combinations of SPD atoms from a learned dictionary using a Riemannian geometric approach. A novel Riemannian optimization objective is formulated for dictionary learning and sparse coding, with the representation loss characterized by the affine invariant Riemannian metric. Additionally, a computationally efficient algorithm is proposed for optimizing the model. Experimental results on various computer vision datasets show that our approach outperforms sparse coding using non-Riemannian formulations in terms of classification and retrieval performance."}, "1603.09742": {"original_text": "  Semantic segmentation is critical to image content understanding and object\nlocalization. Recent development in fully-convolutional neural network (FCN)\nhas enabled accurate pixel-level labeling. One issue in previous works is that\nthe FCN based method does not exploit the object boundary information to\ndelineate segmentation details since the object boundary label is ignored in\nthe network training. To tackle this problem, we introduce a double branch\nfully convolutional neural network, which separates the learning of the\ndesirable semantic class labeling with mask-level object proposals guided by\nrelabeled boundaries. This network, called object boundary guided FCN\n(OBG-FCN), is able to integrate the distinct properties of object shape and\nclass features elegantly in a fully convolutional way with a designed masking\narchitecture. We conduct experiments on the PASCAL VOC segmentation benchmark,\nand show that the end-to-end trainable OBG-FCN system offers great improvement\nin optimizing the target semantic segmentation quality.\n", "rewritten_text": "Semantic segmentation plays a crucial role in understanding image content and locating objects. Recent advancements in fully-convolutional neural networks (FCNs) have allowed for precise labeling at the pixel level. A limitation of previous approaches is that FCN-based methods do not utilize object boundary information, leading to segmentation details being overlooked due to the exclusion of object boundary labels during network training. To address this issue, we propose a double-branch fully convolutional neural network known as Object Boundary Guided FCN (OBG-FCN). This network separates the learning of semantic class labeling and mask-level object proposals, guided by relabeled boundaries, to effectively integrate object shape and class features. Through experiments on the PASCAL VOC segmentation benchmark, we demonstrate that the end-to-end trainable OBG-FCN system significantly enhances the quality of semantic segmentation optimization."}, "2004.03677": {"original_text": "  Image manipulation can be considered a special case of image generation where\nthe image to be produced is a modification of an existing image. Image\ngeneration and manipulation have been, for the most part, tasks that operate on\nraw pixels. However, the remarkable progress in learning rich image and object\nrepresentations has opened the way for tasks such as text-to-image or\nlayout-to-image generation that are mainly driven by semantics. In our work, we\naddress the novel problem of image manipulation from scene graphs, in which a\nuser can edit images by merely applying changes in the nodes or edges of a\nsemantic graph that is generated from the image. Our goal is to encode image\ninformation in a given constellation and from there on generate new\nconstellations, such as replacing objects or even changing relationships\nbetween objects, while respecting the semantics and style from the original\nimage. We introduce a spatio-semantic scene graph network that does not require\ndirect supervision for constellation changes or image edits. This makes it\npossible to train the system from existing real-world datasets with no\nadditional annotation effort.\n", "rewritten_text": "Image manipulation can be seen as a unique form of image creation in which the resulting image is a modification of an existing one. Traditionally, image generation and manipulation have focused on manipulating individual pixels. However, advancements in learning complex image and object representations have paved the way for tasks like text-to-image or layout-to-image generation that rely more on semantics. In our research, we tackle the innovative challenge of manipulating images using scene graphs, allowing users to edit images by adjusting nodes or edges in a semantic graph derived from the image. Our objective is to encode image details in a specific arrangement and then generate new arrangements, such as replacing objects or altering object relationships, while preserving the original image's semantics and style. We present a spatio-semantic scene graph network that can learn to make constellation changes or image edits without explicit supervision, enabling training on real-world datasets without additional annotations."}, "2208.09669": {"original_text": "  Contextualized word embeddings in language models have given much advance to\nNLP. Intuitively, sentential information is integrated into the representation\nof words, which can help model polysemy. However, context sensitivity also\nleads to the variance of representations, which may break the semantic\nconsistency for synonyms. We quantify how much the contextualized embeddings of\neach word sense vary across contexts in typical pre-trained models. Results\nshow that contextualized embeddings can be highly consistent across contexts.\nIn addition, part-of-speech, number of word senses, and sentence length have an\ninfluence on the variance of sense representations. Interestingly, we find that\nword representations are position-biased, where the first words in different\ncontexts tend to be more similar. We analyze such a phenomenon and also propose\na simple way to alleviate such bias in distance-based word sense disambiguation\nsettings.\n", "rewritten_text": "\"Word embeddings that are contextualized in language models have significantly advanced NLP. Essentially, these embeddings incorporate sentence-level information into word representations, aiding in capturing multiple meanings of words. However, the context sensitivity can also result in varying representations, potentially affecting the consistency of synonyms. Our study measures the extent of variation in contextualized word embeddings across different contexts in common pre-trained models. The findings reveal that these embeddings can exhibit high consistency across contexts. Furthermore, factors such as part-of-speech, number of word meanings, and sentence length influence the variability of sense representations. Notably, we observe a positional bias in word representations, where words appearing at the beginning of sentences in various contexts tend to be more similar. We delve into this phenomenon and suggest a straightforward method to mitigate such bias in word sense disambiguation tasks based on distance.\""}, "1706.00842": {"original_text": "  We present a fully automatic method employing convolutional neural networks\nbased on the 2D U-net architecture and random forest classifier to solve the\nautomatic liver lesion segmentation problem of the ISBI 2017 Liver Tumor\nSegmentation Challenge (LiTS). In order to constrain the ROI in which the\ntumors could be located, a liver segmentation is performed first. For the organ\nsegmentation, an ensemble of convolutional networks is trained to segment a\nliver using a set of 179 liver CT datasets from liver surgery planning. Inside\nof the liver ROI a neural network, trained using 127 challenge training\ndatasets, identifies tumor candidates, which are subsequently filtered with a\nrandom forest classifier yielding the final tumor segmentation. The evaluation\non the 70 challenge test cases resulted in a mean Dice coefficient of 0.65,\nranking our method in the second place.\n", "rewritten_text": "Our method utilizes convolutional neural networks in a fully automated approach, based on the 2D U-net architecture and a random forest classifier, to address the liver lesion segmentation challenge in the ISBI 2017 Liver Tumor Segmentation Challenge (LiTS). Initially, a liver segmentation is conducted to confine the region of interest for potential tumors. An ensemble of convolutional networks is trained on 179 liver CT datasets for organ segmentation. Within the liver region, a neural network trained on 127 challenge training datasets identifies tumor candidates, which are then refined using a random forest classifier to produce the final tumor segmentation. Our method achieved a mean Dice coefficient of 0.65 on the 70 challenge test cases, placing us in second position in the rankings."}, "2108.07848": {"original_text": "  Identifying players in sports videos by recognizing their jersey numbers is a\nchallenging task in computer vision. We have designed and implemented a\nmulti-task learning network for jersey number recognition. In order to train a\nnetwork to recognize jersey numbers, two output label representations are used\n(1) Holistic - considers the entire jersey number as one class, and (2)\nDigit-wise - considers the two digits in a jersey number as two separate\nclasses. The proposed network learns both holistic and digit-wise\nrepresentations through a multi-task loss function. We determine the optimal\nweights to be assigned to holistic and digit-wise losses through an ablation\nstudy. Experimental results demonstrate that the proposed multi-task learning\nnetwork performs better than the constituent holistic and digit-wise\nsingle-task learning networks.\n", "rewritten_text": "Identifying players in sports videos by recognizing their jersey numbers presents a challenging task in computer vision. Our approach involves the development and implementation of a multi-task learning network specifically for jersey number recognition. To facilitate the training of the network in recognizing jersey numbers, we utilize two distinct output label representations: Holistic, which treats the entire jersey number as a single class, and Digit-wise, which treats the individual digits within a jersey number as separate classes. The network is trained to learn both holistic and digit-wise representations simultaneously using a multi-task loss function. By conducting an ablation study, we determine the optimal weights for the holistic and digit-wise losses. Our experimental findings indicate that the proposed multi-task learning network outperforms the individual holistic and digit-wise single-task learning networks."}, "2104.01136": {"original_text": "  We design a family of image classification architectures that optimize the\ntrade-off between accuracy and efficiency in a high-speed regime. Our work\nexploits recent findings in attention-based architectures, which are\ncompetitive on highly parallel processing hardware. We revisit principles from\nthe extensive literature on convolutional neural networks to apply them to\ntransformers, in particular activation maps with decreasing resolutions. We\nalso introduce the attention bias, a new way to integrate positional\ninformation in vision transformers. As a result, we propose LeVIT: a hybrid\nneural network for fast inference image classification. We consider different\nmeasures of efficiency on different hardware platforms, so as to best reflect a\nwide range of application scenarios. Our extensive experiments empirically\nvalidate our technical choices and show they are suitable to most\narchitectures. Overall, LeViT significantly outperforms existing convnets and\nvision transformers with respect to the speed/accuracy tradeoff. For example,\nat 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on\nCPU. We release the code at https://github.com/facebookresearch/LeViT\n", "rewritten_text": "We have developed a series of image classification architectures that aim to balance accuracy and efficiency in high-speed settings. Our approach leverages recent advancements in attention-based architectures, which perform well on hardware capable of highly parallel processing. By incorporating principles from convolutional neural networks into transformers, particularly focusing on activation maps with decreasing resolutions, we have introduced the attention bias to effectively integrate positional information in vision transformers. This has led to the creation of LeVIT, a hybrid neural network designed for rapid inference in image classification tasks. We have evaluated the efficiency of LeVIT across various hardware platforms to cater to a wide range of application scenarios. Through extensive experiments, we have validated our technical decisions and demonstrated their compatibility with most architectures. LeViT surpasses existing convolutional networks and vision transformers in terms of the trade-off between speed and accuracy. For instance, at 80% ImageNet top-1 accuracy, LeViT is five times faster than EfficientNet on CPU. The code for LeViT is available at https://github.com/facebookresearch/LeViT."}, "2103.02984": {"original_text": "  Abrupt motion of camera or objects in a scene result in a blurry video, and\ntherefore recovering high quality video requires two types of enhancements:\nvisual enhancement and temporal upsampling. A broad range of research attempted\nto recover clean frames from blurred image sequences or temporally upsample\nframes by interpolation, yet there are very limited studies handling both\nproblems jointly. In this work, we present a novel framework for deblurring,\ninterpolating and extrapolating sharp frames from a motion-blurred video in an\nend-to-end manner. We design our framework by first learning the pixel-level\nmotion that caused the blur from the given inputs via optical flow estimation\nand then predict multiple clean frames by warping the decoded features with the\nestimated flows. To ensure temporal coherence across predicted frames and\naddress potential temporal ambiguity, we propose a simple, yet effective\nflow-based rule. The effectiveness and favorability of our approach are\nhighlighted through extensive qualitative and quantitative evaluations on\nmotion-blurred datasets from high speed videos.\n", "rewritten_text": "Sudden camera or object movements in a scene can lead to a blurry video. To improve video quality, two types of enhancements are needed: visual enhancement and temporal upsampling. Previous research has focused on recovering clear frames from blurred image sequences or increasing frame rate through interpolation. However, there is limited research that addresses both issues simultaneously. This study introduces a new framework for deblurring, interpolating, and extrapolating sharp frames from motion-blurred videos in a seamless process. The framework utilizes optical flow estimation to learn the motion causing the blur and then generates multiple clear frames by warping decoded features with the estimated flows. To maintain temporal consistency and address potential ambiguity, a simple yet effective flow-based rule is proposed. The effectiveness of this approach is demonstrated through thorough qualitative and quantitative evaluations on high-speed video datasets with motion blur."}, "1611.09932": {"original_text": "  Compared to earlier multistage frameworks using CNN features, recent\nend-to-end deep approaches for fine-grained recognition essentially enhance the\nmid-level learning capability of CNNs. Previous approaches achieve this by\nintroducing an auxiliary network to infuse localization information into the\nmain classification network, or a sophisticated feature encoding method to\ncapture higher order feature statistics. We show that mid-level representation\nlearning can be enhanced within the CNN framework, by learning a bank of\nconvolutional filters that capture class-specific discriminative patches\nwithout extra part or bounding box annotations. Such a filter bank is well\nstructured, properly initialized and discriminatively learned through a novel\nasymmetric multi-stream architecture with convolutional filter supervision and\na non-random layer initialization. Experimental results show that our approach\nachieves state-of-the-art on three publicly available fine-grained recognition\ndatasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and\nvisualizations are provided to understand our approach.\n", "rewritten_text": "In contrast to previous multistage frameworks that utilized CNN features, recent end-to-end deep learning methods for fine-grained recognition have significantly improved the mid-level learning capabilities of CNNs. Instead of relying on auxiliary networks or complex feature encoding techniques, our approach enhances mid-level representation learning directly within the CNN architecture. By training a set of convolutional filters to capture class-specific discriminative patches without the need for additional annotations, our method achieves state-of-the-art performance on three fine-grained recognition datasets (CUB-200-2011, Stanford Cars, and FGVC-Aircraft). Our approach is based on a structured filter bank, initialized properly, and learned discriminatively through a unique asymmetric multi-stream architecture with convolutional filter supervision and non-random layer initialization. Ablation studies and visualizations are included to provide insights into our methodology."}, "1504.01013": {"original_text": "  Recent advances in semantic image segmentation have mostly been achieved by\ntraining deep convolutional neural networks (CNNs). We show how to improve\nsemantic segmentation through the use of contextual information; specifically,\nwe explore `patch-patch' context between image regions, and `patch-background'\ncontext. For learning from the patch-patch context, we formulate Conditional\nRandom Fields (CRFs) with CNN-based pairwise potential functions to capture\nsemantic correlations between neighboring patches. Efficient piecewise training\nof the proposed deep structured model is then applied to avoid repeated\nexpensive CRF inference for back propagation. For capturing the\npatch-background context, we show that a network design with traditional\nmulti-scale image input and sliding pyramid pooling is effective for improving\nperformance. Our experimental results set new state-of-the-art performance on a\nnumber of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC\n2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an\nintersection-over-union score of 78.0 on the challenging PASCAL VOC 2012\ndataset.\n", "rewritten_text": "Recent advancements in semantic image segmentation have primarily been made by training deep convolutional neural networks (CNNs). Our research focuses on enhancing semantic segmentation by incorporating contextual information. Specifically, we investigate the relationships between image regions using 'patch-patch' context and 'patch-background' context. To leverage the patch-patch context, we utilize Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations among neighboring patches. We employ efficient piecewise training of the deep structured model to avoid repetitive costly CRF inference during backpropagation. Additionally, to capture the patch-background context, we demonstrate the effectiveness of a network design featuring traditional multi-scale image input and sliding pyramid pooling in enhancing performance. Our experimental findings demonstrate superior performance on various popular semantic segmentation datasets, such as NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. Notably, we achieve an intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012 dataset."}, "2106.1262": {"original_text": "  The self-attention-based model, transformer, is recently becoming the leading\nbackbone in the field of computer vision. In spite of the impressive success\nmade by transformers in a variety of vision tasks, it still suffers from heavy\ncomputation and intensive memory costs. To address this limitation, this paper\npresents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).\nWe start by observing a large amount of redundant computation, mainly spent on\nuncorrelated input patches, and then introduce an interpretable module to\ndynamically and gracefully drop these redundant patches. This novel framework\nis then extended to a hierarchical structure, where uncorrelated tokens at\ndifferent stages are gradually removed, resulting in a considerable shrinkage\nof computational cost. We include extensive experiments on both image and video\ntasks, where our method could deliver up to 1.4x speed-up for state-of-the-art\nmodels like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy.\nMore importantly, contrary to other acceleration approaches, our method is\ninherently interpretable with substantial visual evidence, making vision\ntransformer closer to a more human-understandable architecture while being\nlighter. We demonstrate that the interpretability that naturally emerged in our\nframework can outperform the raw attention learned by the original visual\ntransformer, as well as those generated by off-the-shelf interpretation\nmethods, with both qualitative and quantitative results. Project Page:\nhttp://people.csail.mit.edu/bpan/ia-red/.\n", "rewritten_text": "The transformer model, which relies on self-attention, is increasingly becoming the primary architecture in computer vision. Despite transformers' success in various vision tasks, they still face challenges related to high computation and memory requirements. To overcome this issue, this study introduces the Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$). By identifying and eliminating redundant computation, particularly in uncorrelated input patches, this framework effectively reduces computational costs. The approach is further developed into a hierarchical structure to progressively remove uncorrelated tokens at different stages, significantly reducing computational overhead. Experimental results on image and video tasks demonstrate that IA-RED$^2$ can speed up state-of-the-art models like DeiT and TimeSformer by up to 1.4 times with minimal accuracy loss. Importantly, this method offers interpretability through visual evidence, making the vision transformer architecture more understandable while maintaining efficiency. The study shows that the interpretability embedded in IA-RED$^2$ outperforms raw attention mechanisms in the original visual transformer and off-the-shelf interpretation methods, as supported by qualitative and quantitative analyses. Project Page: http://people.csail.mit.edu/bpan/ia-red/."}, "1803.03852": {"original_text": "  Tracking the pose of instruments is a central problem in image-guided\nsurgery. For microscopic scenarios, optical coherence tomography (OCT) is\nincreasingly used as an imaging modality. OCT is suitable for accurate pose\nestimation due to its micrometer range resolution and volumetric field of view.\nHowever, OCT image processing is challenging due to speckle noise and\nreflection artifacts in addition to the images' 3D nature. We address pose\nestimation from OCT volume data with a new deep learning-based tracking\nframework. For this purpose, we design a new 3D convolutional neural network\n(CNN) architecture to directly predict the 6D pose of a small marker geometry\nfrom OCT volumes. We use a hexapod robot to automatically acquire labeled data\npoints which we use to train 3D CNN architectures for multi-output regression.\nWe use this setup to provide an in-depth analysis on deep learning-based pose\nestimation from volumes. Specifically, we demonstrate that exploiting volume\ninformation for pose estimation yields higher accuracy than relying on 2D\nrepresentations with depth information. Supporting this observation, we provide\nquantitative and qualitative results that 3D CNNs effectively exploit the depth\nstructure of marker objects. Regarding the deep learning aspect, we present\nefficient design principles for 3D CNNs, making use of insights from the 2D\ndeep learning community. In particular, we present Inception3D as a new\narchitecture which performs best for our application. We show that our deep\nlearning approach reaches errors at our ground-truth label's resolution. We\nachieve a mean average error of $\\SI{14.89 \\pm 9.3}{\\micro\\metre}$ and\n$\\SI{0.096 \\pm 0.072}{\\degree}$ for position and orientation learning,\nrespectively.\n", "rewritten_text": "\"Tracking the position of instruments is a key challenge in image-guided surgery, especially in microscopic settings where optical coherence tomography (OCT) is increasingly utilized for imaging. OCT's high resolution and wide field of view make it suitable for accurate position estimation, despite the difficulties posed by speckle noise and reflection artifacts in the 3D images. To tackle this, we introduce a novel deep learning-based tracking framework for estimating instrument position from OCT volume data. Our approach involves developing a new 3D convolutional neural network (CNN) architecture to predict the 6D position of a small marker geometry directly from OCT volumes. By training the network with labeled data points acquired using a hexapod robot, we demonstrate the effectiveness of 3D CNNs in leveraging volume information for improved accuracy compared to traditional 2D representations. Our study showcases the benefits of exploiting depth structure in marker objects for pose estimation, with our proposed Inception3D architecture achieving the best performance. Our deep learning approach achieves errors close to the ground-truth labels, with mean average errors of 14.89 \u00b1 9.3 \u00b5m for position and 0.096 \u00b1 0.072 degrees for orientation learning.\""}, "2104.04986": {"original_text": "  Aspect-based Sentiment Analysis (ABSA), aiming at predicting the polarities\nfor aspects, is a fine-grained task in the field of sentiment analysis.\nPrevious work showed syntactic information, e.g. dependency trees, can\neffectively improve the ABSA performance. Recently, pre-trained models (PTMs)\nalso have shown their effectiveness on ABSA. Therefore, the question naturally\narises whether PTMs contain sufficient syntactic information for ABSA so that\nwe can obtain a good ABSA model only based on PTMs. In this paper, we firstly\ncompare the induced trees from PTMs and the dependency parsing trees on several\npopular models for the ABSA task, showing that the induced tree from fine-tuned\nRoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis\nexperiments reveal that the FT-RoBERTa Induced Tree is more\nsentiment-word-oriented and could benefit the ABSA task. The experiments also\nshow that the pure RoBERTa-based model can outperform or approximate to the\nprevious SOTA performances on six datasets across four languages since it\nimplicitly incorporates the task-oriented syntactic information.\n", "rewritten_text": "Aspect-based Sentiment Analysis (ABSA) is a detailed task within sentiment analysis that focuses on predicting the polarities of different aspects. Previous research has demonstrated that incorporating syntactic information, such as dependency trees, can enhance ABSA performance. Recently, pre-trained models (PTMs) have also proven to be effective for ABSA. This raises the question of whether PTMs contain enough syntactic information to build a strong ABSA model solely based on them. In this study, we compare the trees generated by PTMs with dependency parsing trees from various models for ABSA. Our findings show that the tree generated by fine-tuned RoBERTa (FT-RoBERTa) performs better than the parser-provided tree. Further analysis reveals that the FT-RoBERTa Induced Tree is more focused on sentiment words and is beneficial for the ABSA task. Our experiments demonstrate that a RoBERTa-based model can achieve superior or comparable results to previous state-of-the-art performances on six datasets in four languages, as it implicitly integrates task-specific syntactic information."}, "2012.11581": {"original_text": "  Humans live within a 3D space and constantly interact with it to perform\ntasks. Such interactions involve physical contact between surfaces that is\nsemantically meaningful. Our goal is to learn how humans interact with scenes\nand leverage this to enable virtual characters to do the same. To that end, we\nintroduce a novel Human-Scene Interaction (HSI) model that encodes proximal\nrelationships, called POSA for \"Pose with prOximitieS and contActs\". The\nrepresentation of interaction is body-centric, which enables it to generalize\nto new scenes. Specifically, POSA augments the SMPL-X parametric human body\nmodel such that, for every mesh vertex, it encodes (a) the contact probability\nwith the scene surface and (b) the corresponding semantic scene label. We learn\nPOSA with a VAE conditioned on the SMPL-X vertices, and train on the PROX\ndataset, which contains SMPL-X meshes of people interacting with 3D scenes, and\nthe corresponding scene semantics from the PROX-E dataset. We demonstrate the\nvalue of POSA with two applications. First, we automatically place 3D scans of\npeople in scenes. We use a SMPL-X model fit to the scan as a proxy and then\nfind its most likely placement in 3D. POSA provides an effective representation\nto search for \"affordances\" in the scene that match the likely contact\nrelationships for that pose. We perform a perceptual study that shows\nsignificant improvement over the state of the art on this task. Second, we show\nthat POSA's learned representation of body-scene interaction supports monocular\nhuman pose estimation that is consistent with a 3D scene, improving on the\nstate of the art. Our model and code are available for research purposes at\nhttps://posa.is.tue.mpg.de.\n", "rewritten_text": "Humans exist in a three-dimensional space and regularly engage with it to carry out tasks. These interactions involve meaningful physical contact between surfaces. Our objective is to understand how humans interact with their surroundings and use this knowledge to enable virtual characters to do the same. To achieve this, we introduce a new model called Human-Scene Interaction (HSI), referred to as POSA (Pose with prOximitieS and contActs). The POSA model focuses on encoding close relationships and is centered around the human body, allowing it to adapt to different environments. Specifically, POSA enhances the SMPL-X parametric human body model by incorporating information about the likelihood of contact with the scene surface and the corresponding semantic label of the scene for each mesh vertex. We train POSA using a Variational Autoencoder (VAE) conditioned on SMPL-X vertices, utilizing data from the PROX dataset, which includes SMPL-X meshes of individuals interacting with three-dimensional scenes, along with scene semantics from the PROX-E dataset. We showcase the utility of POSA through two applications. Firstly, we automate the placement of 3D scans of individuals in scenes by fitting a SMPL-X model to the scan and determining its optimal position in 3D space. POSA facilitates the identification of scene elements that align with the expected contact relationships for a given pose, as demonstrated by a perceptual study showing significant advancements over existing methods. Secondly, we illustrate that POSA's learned representation of body-scene interaction supports monocular human pose estimation that aligns with a three-dimensional scene, surpassing current standards. Our model and code are accessible for research purposes at https://posa.is.tue.mpg.de."}, "1609.04079": {"original_text": "  We present a single-shot system to recover surface geometry of objects with\nspatially-varying albedos, from images captured under a calibrated RGB\nphotometric stereo setup---with three light directions multiplexed across\ndifferent color channels in the observed RGB image. Since the problem is\nill-posed point-wise, we assume that the albedo map can be modeled as\npiece-wise constant with a restricted number of distinct albedo values. We show\nthat under ideal conditions, the shape of a non-degenerate local constant\nalbedo surface patch can theoretically be recovered exactly. Moreover, we\npresent a practical and efficient algorithm that uses this model to robustly\nrecover shape from real images. Our method first reasons about shape locally in\na dense set of patches in the observed image, producing shape distributions for\nevery patch. These local distributions are then combined to produce a single\nconsistent surface normal map. We demonstrate the efficacy of the approach\nthrough experiments on both synthetic renderings as well as real captured\nimages.\n", "rewritten_text": "We introduce a system that can accurately determine the surface geometry of objects with varying albedos using a single image captured under a calibrated RGB photometric stereo setup. This setup involves three light directions multiplexed across different color channels in the observed RGB image. Due to the inherent ambiguity of the problem at a point-wise level, we assume that the albedo map can be represented as piece-wise constant with a limited number of distinct albedo values. Our research demonstrates that, under ideal circumstances, the shape of a non-degenerate local constant albedo surface patch can be precisely reconstructed. Additionally, we propose an efficient algorithm that leverages this model to reliably recover shape information from real-world images. Our method analyzes shape locally within a dense array of patches in the observed image, generating shape distributions for each patch. These local distributions are then aggregated to create a unified surface normal map. We validate the effectiveness of our approach through experiments conducted on both synthetic renderings and actual images captured in real-world scenarios."}, "2305.15699": {"original_text": "  Understanding action recognition in egocentric videos has emerged as a vital\nresearch topic with numerous practical applications. With the limitation in the\nscale of egocentric data collection, learning robust deep learning-based action\nrecognition models remains difficult. Transferring knowledge learned from the\nlarge-scale exocentric data to the egocentric data is challenging due to the\ndifference in videos across views. Our work introduces a novel cross-view\nlearning approach to action recognition (CVAR) that effectively transfers\nknowledge from the exocentric to the selfish view. First, we present a novel\ngeometric-based constraint into the self-attention mechanism in Transformer\nbased on analyzing the camera positions between two views. Then, we propose a\nnew cross-view self-attention loss learned on unpaired cross-view data to\nenforce the self-attention mechanism learning to transfer knowledge across\nviews. Finally, to further improve the performance of our cross-view learning\napproach, we present the metrics to measure the correlations in videos and\nattention maps effectively. Experimental results on standard egocentric action\nrecognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, and\nEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-art\nperformance.\n", "rewritten_text": "The study of recognizing actions in egocentric videos has become a crucial area of research with various practical uses. Due to the limited availability of egocentric data, developing robust deep learning models for action recognition remains challenging. Transferring knowledge from large-scale exocentric data to egocentric data is difficult due to differences in video perspectives. Our research introduces a new method called Cross-View Action Recognition (CVAR) that effectively transfers knowledge from exocentric to egocentric views. We incorporate a novel geometric constraint into the self-attention mechanism of the Transformer model by analyzing camera positions in different views. Additionally, we propose a cross-view self-attention loss function trained on unpaired cross-view data to facilitate knowledge transfer. To enhance the performance of our cross-view learning approach, we introduce metrics to measure video and attention map correlations effectively. Experimental results on standard egocentric action recognition datasets, such as Charades-Ego, EPIC-Kitchens-55, and EPIC-Kitchens-100, demonstrate the effectiveness and state-of-the-art performance of our approach."}, "2407.10753": {"original_text": "  Accurate depth information is crucial for enhancing the performance of\nmulti-view 3D object detection. Despite the success of some existing multi-view\n3D detectors utilizing pixel-wise depth supervision, they overlook two\nsignificant phenomena: 1) the depth supervision obtained from LiDAR points is\nusually distributed on the surface of the object, which is not so friendly to\nexisting DETR-based 3D detectors due to the lack of the depth of 3D object\ncenter; 2) for distant objects, fine-grained depth estimation of the whole\nobject is more challenging. Therefore, we argue that the object-wise depth (or\n3D center of the object) is essential for accurate detection. In this paper, we\npropose a new multi-view 3D object detector named OPEN, whose main idea is to\neffectively inject object-wise depth information into the network through our\nproposed object-wise position embedding. Specifically, we first employ an\nobject-wise depth encoder, which takes the pixel-wise depth map as a prior, to\naccurately estimate the object-wise depth. Then, we utilize the proposed\nobject-wise position embedding to encode the object-wise depth information into\nthe transformer decoder, thereby producing 3D object-aware features for final\ndetection. Extensive experiments verify the effectiveness of our proposed\nmethod. Furthermore, OPEN achieves a new state-of-the-art performance with\n64.4% NDS and 56.7% mAP on the nuScenes test benchmark.\n", "rewritten_text": "Accurate depth information plays a crucial role in improving the performance of multi-view 3D object detection. While some existing multi-view 3D detectors have been successful using pixel-wise depth supervision, they tend to overlook two important factors. Firstly, depth supervision from LiDAR points is typically concentrated on the object's surface, which may not align well with DETR-based 3D detectors that lack the depth of the object's center. Secondly, accurately estimating fine-grained depth for distant objects poses a greater challenge. Therefore, we argue that object-wise depth, or the 3D center of the object, is vital for precise detection. In this study, we introduce a novel multi-view 3D object detector called OPEN, which focuses on integrating object-wise depth information into the network using our proposed object-wise position embedding. Initially, we employ an object-wise depth encoder to estimate object-wise depth accurately based on the pixel-wise depth map. Subsequently, we use the object-wise position embedding to incorporate this depth information into the transformer decoder, generating 3D object-aware features for the final detection. Extensive experiments confirm the effectiveness of our approach, with OPEN achieving a new state-of-the-art performance of 64.4% NDS and 56.7% mAP on the nuScenes test benchmark."}, "2308.10445": {"original_text": "  Incremental learning aims to overcome catastrophic forgetting when learning\ndeep networks from sequential tasks. With impressive learning efficiency and\nperformance, prompt-based methods adopt a fixed backbone to sequential tasks by\nlearning task-specific prompts. However, existing prompt-based methods heavily\nrely on strong pretraining (typically trained on ImageNet-21k), and we find\nthat their models could be trapped if the potential gap between the pretraining\ntask and unknown future tasks is large. In this work, we develop a learnable\nAdaptive Prompt Generator (APG). The key is to unify the prompt retrieval and\nprompt learning processes into a learnable prompt generator. Hence, the whole\nprompting process can be optimized to reduce the negative effects of the gap\nbetween tasks effectively. To make our APG avoid learning ineffective\nknowledge, we maintain a knowledge pool to regularize APG with the feature\ndistribution of each class. Extensive experiments show that our method\nsignificantly outperforms advanced methods in exemplar-free incremental\nlearning without (strong) pretraining. Besides, under strong retraining, our\nmethod also has comparable performance to existing prompt-based models, showing\nthat our method can still benefit from pretraining. Codes can be found at\nhttps://github.com/TOM-tym/APG\n", "rewritten_text": "Incremental learning is a technique designed to address the issue of catastrophic forgetting that occurs when training deep networks on a series of tasks. Prompt-based methods have shown impressive efficiency and performance by using a fixed backbone for sequential tasks and learning task-specific prompts. However, existing prompt-based methods heavily rely on strong pretraining, such as training on ImageNet-21k, which can lead to models becoming stuck when faced with new tasks that are significantly different from the pretraining task. In this study, we introduce a learnable Adaptive Prompt Generator (APG) that combines prompt retrieval and learning processes into a single, adaptable prompt generator. This approach allows for the optimization of the prompting process to mitigate the negative impact of task variations. To prevent the learning of irrelevant knowledge, we incorporate a knowledge pool that regularizes the APG based on the feature distribution of each class. Our experiments demonstrate that our method outperforms existing techniques in incremental learning without strong pretraining. Additionally, when retrained with strong supervision, our method achieves comparable performance to traditional prompt-based models, indicating that it can still benefit from pretraining. The code for our method is available at https://github.com/TOM-tym/APG."}, "2409.18355": {"original_text": "  Cone Beam Computed Tomography (CBCT) finds diverse applications in medicine.\nEnsuring high image quality in CBCT scans is essential for accurate diagnosis\nand treatment delivery. Yet, the susceptibility of CBCT images to noise and\nartifacts undermines both their usefulness and reliability. Existing methods\ntypically address CBCT artifacts through image-to-image translation approaches.\nThese methods, however, are limited by the artifact types present in the\ntraining data, which may not cover the complete spectrum of CBCT degradations\nstemming from variations in imaging protocols. Gathering additional data to\nencompass all possible scenarios can often pose a challenge. To address this,\nwe present SinoSynth, a physics-based degradation model that simulates various\nCBCT-specific artifacts to generate a diverse set of synthetic CBCT images from\nhigh-quality CT images without requiring pre-aligned data. Through extensive\nexperiments, we demonstrate that several different generative networks trained\non our synthesized data achieve remarkable results on heterogeneous\nmulti-institutional datasets, outperforming even the same networks trained on\nactual data. We further show that our degradation model conveniently provides\nan avenue to enforce anatomical constraints in conditional generative models,\nyielding high-quality and structure-preserving synthetic CT images.\n", "rewritten_text": "Cone Beam Computed Tomography (CBCT) is widely used in medicine for various purposes. It is crucial to maintain high image quality in CBCT scans to ensure accurate diagnosis and treatment. However, CBCT images are prone to noise and artifacts, which can affect their reliability and usefulness. Current methods for addressing CBCT artifacts typically involve image-to-image translation techniques. These methods have limitations as they may not cover the full range of CBCT degradations due to variations in imaging protocols. Gathering additional data to encompass all possible scenarios can be challenging. To tackle this issue, we introduce SinoSynth, a physics-based model that simulates CBCT-specific artifacts to generate synthetic CBCT images from high-quality CT images without the need for pre-aligned data. Our experiments demonstrate that generative networks trained on our synthesized data perform exceptionally well on diverse datasets, surpassing networks trained on actual data. Additionally, our degradation model allows for enforcing anatomical constraints in conditional generative models, resulting in high-quality synthetic CT images that preserve anatomical structures."}, "1904.07904": {"original_text": "  Spoken question answering (SQA) is challenging due to complex reasoning on\ntop of the spoken documents. The recent studies have also shown the\ncatastrophic impact of automatic speech recognition (ASR) errors on SQA.\nTherefore, this work proposes to mitigate the ASR errors by aligning the\nmismatch between ASR hypotheses and their corresponding reference\ntranscriptions. An adversarial model is applied to this domain adaptation task,\nwhich forces the model to learn domain-invariant features the QA model can\neffectively utilize in order to improve the SQA results. The experiments\nsuccessfully demonstrate the effectiveness of our proposed model, and the\nresults are better than the previous best model by 2% EM score.\n", "rewritten_text": "Answering spoken questions is difficult because it requires complex reasoning based on spoken documents. Recent research has highlighted the significant negative impact of errors in automatic speech recognition (ASR) on spoken question answering (SQA). To address this issue, this study suggests reducing ASR errors by aligning ASR hypotheses with their correct reference transcriptions. An adversarial model is used for this domain adaptation task, encouraging the model to learn features that are consistent across different domains and can be effectively used by the question answering model to enhance SQA performance. The experiments confirm the effectiveness of our proposed model, showing a 2% improvement in EM score compared to the previous best model."}, "2310.00936": {"original_text": "  Recent studies on StyleGAN variants show promising performances for various\ngeneration tasks. In these models, latent codes have traditionally been\nmanipulated and searched for the desired images. However, this approach\nsometimes suffers from a lack of photorealism in generated images due to a lack\nof knowledge about the geometry of the trained latent space. In this paper, we\nshow a simple unsupervised method that provides well-trained local latent\nsubspace, enabling latent code navigation while preserving the photorealism of\nthe generated images. Specifically, the method identifies densely mapped latent\nspaces and restricts latent manipulations within the local latent subspace.\nExperimental results demonstrate that images generated within the local latent\nsubspace maintain photorealism even when the latent codes are significantly and\nrepeatedly manipulated. Moreover, experiments show that the method can be\napplied to latent code optimization for various types of style-based models.\nOur empirical evidence of the method will benefit applications in style-based\nmodels.\n", "rewritten_text": "Recent research on StyleGAN variations has shown promising results in generating various types of images. These models typically involve manipulating latent codes to produce desired images. However, this approach can sometimes result in generated images lacking photorealism due to a limited understanding of the latent space geometry. This paper introduces a straightforward unsupervised method that establishes a well-trained local latent subspace, allowing for effective navigation of latent codes while maintaining photorealism in generated images. The method identifies densely mapped latent spaces and confines manipulations to the local latent subspace. Experimental findings indicate that images generated within this local latent subspace retain photorealism even with significant and repeated code manipulations. Furthermore, the method is shown to be applicable for optimizing latent codes in various style-based models, offering practical benefits for such applications."}, "2310.00796": {"original_text": "  Strong inductive biases enable learning from little data and help\ngeneralization outside of the training distribution. Popular neural\narchitectures such as Transformers lack strong structural inductive biases for\nseq2seq NLP tasks on their own. Consequently, they struggle with systematic\ngeneralization beyond the training distribution, e.g. with extrapolating to\nlonger inputs, even when pre-trained on large amounts of text. We show how a\nstructural inductive bias can be efficiently injected into a seq2seq model by\npre-training it to simulate structural transformations on synthetic data.\nSpecifically, we inject an inductive bias towards Finite State Transducers\n(FSTs) into a Transformer by pre-training it to simulate FSTs given their\ndescriptions. Our experiments show that our method imparts the desired\ninductive bias, resulting in improved systematic generalization and better\nfew-shot learning for FST-like tasks. Our analysis shows that fine-tuned models\naccurately capture the state dynamics of the unseen underlying FSTs, suggesting\nthat the simulation process is internalized by the fine-tuned model.\n", "rewritten_text": "\"Having strong inductive biases is crucial for learning from limited data and enhancing generalization beyond the training set. While popular neural architectures like Transformers lack these strong biases for seq2seq NLP tasks on their own, they face challenges in systematic generalization beyond the training data, such as extrapolating to longer inputs, even after being pre-trained on extensive text. In this study, we demonstrate how a structural inductive bias can be effectively incorporated into a seq2seq model by pre-training it to mimic structural transformations using synthetic data. Specifically, we introduce an inductive bias towards Finite State Transducers (FSTs) into a Transformer by pre-training it to replicate FSTs based on their descriptions. Our experiments reveal that this method successfully introduces the desired inductive bias, leading to enhanced systematic generalization and improved few-shot learning for FST-like tasks. Our analysis indicates that fine-tuned models accurately capture the state dynamics of unseen underlying FSTs, suggesting that the simulation process becomes internalized by the fine-tuned model.\""}, "2309.11566": {"original_text": "  We introduce SignBank+, a clean version of the SignBank dataset, optimized\nfor machine translation between spoken language text and SignWriting, a\nphonetic sign language writing system. In addition to previous work that\nemploys complex factorization techniques to enable translation between text and\nSignWriting, we show that a traditional text-to-text translation approach\nperforms equally effectively on the cleaned SignBank+ dataset. Our evaluation\nresults indicate that models trained on SignBank+ surpass those on the original\ndataset, establishing a new benchmark for SignWriting-based sign language\ntranslation and providing an open resource for future research.\n", "rewritten_text": "We present SignBank+, a refined version of the SignBank dataset designed for machine translation between spoken language text and SignWriting, a phonetic sign language writing system. Unlike previous methods that use complex factorization techniques for translation, we demonstrate that a conventional text-to-text translation approach works just as well on the improved SignBank+ dataset. Our findings show that models trained on SignBank+ outperform those trained on the original dataset, setting a new standard for SignWriting-based sign language translation and offering a valuable resource for further research."}, "2212.00244": {"original_text": "  Domain adaptation for Cross-LiDAR 3D detection is challenging due to the\nlarge gap on the raw data representation with disparate point densities and\npoint arrangements. By exploring domain-invariant 3D geometric characteristics\nand motion patterns, we present an unsupervised domain adaptation method that\novercomes above difficulties. First, we propose the Spatial Geometry Alignment\nmodule to extract similar 3D shape geometric features of the same object class\nto align two domains, while eliminating the effect of distinct point\ndistributions. Second, we present Temporal Motion Alignment module to utilize\nmotion features in sequential frames of data to match two domains. Prototypes\ngenerated from two modules are incorporated into the pseudo-label reweighting\nprocedure and contribute to our effective self-training framework for the\ntarget domain. Extensive experiments show that our method achieves\nstate-of-the-art performance on cross-device datasets, especially for the\ndatasets with large gaps captured by mechanical scanning LiDARs and solid-state\nLiDARs in various scenes. Project homepage is at\nhttps://github.com/4DVLab/CL3D.git\n", "rewritten_text": "Adapting Cross-LiDAR 3D detection across different domains poses a significant challenge due to variations in raw data representation, such as point densities and arrangements. To address this issue, we introduce an unsupervised domain adaptation approach that focuses on identifying domain-invariant 3D geometric characteristics and motion patterns. Our method includes two key modules: the Spatial Geometry Alignment module, which extracts similar 3D shape features to align object classes between domains, and the Temporal Motion Alignment module, which leverages motion features in sequential frames to match domains. The prototypes generated by these modules are integrated into a pseudo-label reweighting process, forming an effective self-training framework for the target domain. Through extensive experiments, we demonstrate that our approach outperforms existing methods on cross-device datasets, particularly those with significant differences between mechanical scanning LiDARs and solid-state LiDARs in various environments. For more information, please visit our project homepage at https://github.com/4DVLab/CL3D.git."}, "2210.10239": {"original_text": "  This paper aims to investigate representation learning for large scale visual\nplace recognition, which consists of determining the location depicted in a\nquery image by referring to a database of reference images. This is a\nchallenging task due to the large-scale environmental changes that can occur\nover time (i.e., weather, illumination, season, traffic, occlusion). Progress\nis currently challenged by the lack of large databases with accurate ground\ntruth. To address this challenge, we introduce GSV-Cities, a new image dataset\nproviding the widest geographic coverage to date with highly accurate ground\ntruth, covering more than 40 cities across all continents over a 14-year\nperiod. We subsequently explore the full potential of recent advances in deep\nmetric learning to train networks specifically for place recognition, and\nevaluate how different loss functions influence performance. In addition, we\nshow that performance of existing methods substantially improves when trained\non GSV-Cities. Finally, we introduce a new fully convolutional aggregation\nlayer that outperforms existing techniques, including GeM, NetVLAD and\nCosPlace, and establish a new state-of-the-art on large-scale benchmarks, such\nas Pittsburgh, Mapillary-SLS, SPED and Nordland. The dataset and code are\navailable for research purposes at https://github.com/amaralibey/gsv-cities.\n", "rewritten_text": "The purpose of this study is to explore representation learning for large-scale visual place recognition, which involves identifying the location shown in a query image by comparing it to a database of reference images. This task is challenging due to significant environmental changes that can occur over time, such as weather, lighting, seasons, traffic, and obstructions. One of the main obstacles in this field is the lack of large databases with accurate ground truth. To overcome this challenge, we introduce GSV-Cities, a new image dataset that offers extensive geographic coverage and precise ground truth for over 40 cities worldwide spanning a 14-year period. We leverage recent advancements in deep metric learning to train networks specifically for place recognition and investigate the impact of different loss functions on performance. Our results demonstrate significant performance enhancements when existing methods are trained on the GSV-Cities dataset. Additionally, we introduce a novel fully convolutional aggregation layer that surpasses current techniques like GeM, NetVLAD, and CosPlace, setting a new state-of-the-art on prominent large-scale benchmarks such as Pittsburgh, Mapillary-SLS, SPED, and Nordland. The dataset and code are accessible for research purposes at https://github.com/amaralibey/gsv-cities."}, "2202.04101": {"original_text": "  Photoplethysmography (PPG) signals have become a key technology in many\nfields, such as medicine, well-being, or sports. Our work proposes a set of\npipelines to extract remote PPG signals (rPPG) from the face robustly,\nreliably, and configurable. We identify and evaluate the possible choices in\nthe critical steps of unsupervised rPPG methodologies. We assess a\nstate-of-the-art processing pipeline in six different datasets, incorporating\nimportant corrections in the methodology that ensure reproducible and fair\ncomparisons. In addition, we extend the pipeline by proposing three novel\nideas; 1) a new method to stabilize the detected face based on a rigid mesh\nnormalization; 2) a new method to dynamically select the different regions in\nthe face that provide the best raw signals, and 3) a new RGB to rPPG\ntransformation method, called Orthogonal Matrix Image Transformation (OMIT)\nbased on QR decomposition, that increases robustness against compression\nartifacts. We show that all three changes introduce noticeable improvements in\nretrieving rPPG signals from faces, obtaining state-of-the-art results compared\nwith unsupervised, non-learning-based methodologies and, in some databases,\nvery close to supervised, learning-based methods. We perform a comparative\nstudy to quantify the contribution of each proposed idea. In addition, we\ndepict a series of observations that could help in future implementations.\n", "rewritten_text": "The use of Photoplethysmography (PPG) signals has become essential in various fields such as medicine, well-being, and sports. Our research introduces a series of processes to effectively extract remote PPG signals (rPPG) from facial features with robustness, reliability, and configurability. We analyze and compare different approaches in the key stages of unsupervised rPPG methodologies. Through evaluating a cutting-edge processing pipeline across six diverse datasets, we incorporate crucial adjustments to ensure consistent and equitable comparisons. Furthermore, we enhance the pipeline by introducing three innovative concepts: 1) a novel technique for stabilizing facial detection using rigid mesh normalization; 2) a method for dynamically selecting optimal facial regions for raw signal extraction; and 3) a new RGB to rPPG transformation method named Orthogonal Matrix Image Transformation (OMIT) based on QR decomposition, which enhances resilience against compression artifacts. Our findings demonstrate significant enhancements in retrieving rPPG signals from faces with these modifications, achieving top-tier results compared to unsupervised, non-learning-based methods and, in some instances, approaching the performance of supervised, learning-based approaches. We conduct a detailed analysis to quantify the impact of each proposed idea and provide insights for future implementations."}, "1803.05449": {"original_text": "  We introduce SentEval, a toolkit for evaluating the quality of universal\nsentence representations. SentEval encompasses a variety of tasks, including\nbinary and multi-class classification, natural language inference and sentence\nsimilarity. The set of tasks was selected based on what appears to be the\ncommunity consensus regarding the appropriate evaluations for universal\nsentence representations. The toolkit comes with scripts to download and\npreprocess datasets, and an easy interface to evaluate sentence encoders. The\naim is to provide a fairer, less cumbersome and more centralized way for\nevaluating sentence representations.\n", "rewritten_text": "\"We present SentEval, a tool designed to assess the effectiveness of universal sentence representations. SentEval covers a range of tasks such as binary and multi-class classification, natural language inference, and sentence similarity. The selection of tasks was informed by the general agreement within the community on the suitable evaluations for universal sentence representations. The toolkit includes scripts for dataset downloading and preprocessing, as well as a user-friendly interface for evaluating sentence encoders. Our goal is to offer a more equitable, streamlined, and centralized approach to evaluating sentence representations.\""}, "1506.05929": {"original_text": "  Previous work has shown that the artist of an artwork can be identified by\nuse of computational methods that analyse digital images. However, the\ndigitised artworks are often investigated at a coarse scale discarding many of\nthe important details that may define an artist's style. In recent years high\nresolution images of artworks have become available, which, combined with\nincreased processing power and new computational techniques, allow us to\nanalyse digital images of artworks at a very fine scale. In this work we train\nand evaluate a Convolutional Neural Network (CNN) on the task of artist\nattribution using artwork images of varying resolutions. To this end, we\ncombine two existing methods to enable the application of high resolution\nimages to CNNs. By comparing the attribution performances obtained at different\nscales, we find that in most cases finer scales are beneficial to the\nattribution performance, whereas for a minority of the artists, coarser scales\nappear to be preferable. We conclude that artist attribution would benefit from\na multi-scale CNN approach which vastly expands the possibilities for\ncomputational art forensics.\n", "rewritten_text": "Prior research has demonstrated that computational methods can be utilized to identify the artist of an artwork by analyzing digital images. However, when digitized artworks are examined at a broad scale, many crucial details that define an artist's style are often overlooked. With the availability of high-resolution images of artworks in recent years, along with advancements in processing power and computational techniques, it is now possible to analyze digital images of artworks at a much finer scale. In this study, we train and assess a Convolutional Neural Network (CNN) for artist attribution using artwork images of varying resolutions. To achieve this, we merge two existing methods to facilitate the use of high-resolution images in CNNs. Through comparing attribution performances at different scales, we observe that finer scales generally enhance attribution performance, although for a few artists, coarser scales may be more suitable. Our findings suggest that a multi-scale CNN approach would enhance artist attribution by significantly expanding the capabilities of computational art forensics."}, "2112.03803": {"original_text": "  Despite the great progress in video understanding made by deep convolutional\nneural networks, feature representation learned by existing methods may be\nbiased to static visual cues. To address this issue, we propose a novel method\nto suppress static visual cues (SSVC) based on probabilistic analysis for\nself-supervised video representation learning. In our method, video frames are\nfirst encoded to obtain latent variables under standard normal distribution via\nnormalizing flows. By modelling static factors in a video as a random variable,\nthe conditional distribution of each latent variable becomes shifted and scaled\nnormal. Then, the less-varying latent variables along time are selected as\nstatic cues and suppressed to generate motion-preserved videos. Finally,\npositive pairs are constructed by motion-preserved videos for contrastive\nlearning to alleviate the problem of representation bias to static cues. The\nless-biased video representation can be better generalized to various\ndownstream tasks. Extensive experiments on publicly available benchmarks\ndemonstrate that the proposed method outperforms the state of the art when only\nsingle RGB modality is used for pre-training.\n", "rewritten_text": "\"Despite the significant advancements in video comprehension achieved by deep convolutional neural networks, the feature representation learned by current methods may exhibit a bias towards static visual cues. To tackle this issue, we introduce a new approach called the Static Visual Cues Suppression (SSVC) method, which is based on probabilistic analysis for self-supervised video representation learning. Our method involves encoding video frames to derive latent variables following a standard normal distribution using normalizing flows. By treating static elements in a video as a random variable, the conditional distribution of each latent variable is transformed into a shifted and scaled normal distribution. Subsequently, we identify and suppress the less variable latent variables over time as static cues to produce motion-preserved videos. These motion-preserved videos are then used to create positive pairs for contrastive learning, mitigating the bias towards static cues in the representation. The resulting video representation, which is less biased, can be more effectively applied to a range of downstream tasks. Extensive experiments conducted on publicly available benchmarks demonstrate that our proposed method surpasses the current state of the art when utilizing only a single RGB modality for pre-training.\""}, "2103.16889": {"original_text": "  Current state-of-the-art visual recognition systems usually rely on the\nfollowing pipeline: (a) pretraining a neural network on a large-scale dataset\n(e.g., ImageNet) and (b) finetuning the network weights on a smaller,\ntask-specific dataset. Such a pipeline assumes the sole weight adaptation is\nable to transfer the network capability from one domain to another domain,\nbased on a strong assumption that a fixed architecture is appropriate for all\ndomains. However, each domain with a distinct recognition target may need\ndifferent levels/paths of feature hierarchy, where some neurons may become\nredundant, and some others are re-activated to form new network structures. In\nthis work, we prove that dynamically adapting network architectures tailored\nfor each domain task along with weight finetuning benefits in both efficiency\nand effectiveness, compared to the existing image recognition pipeline that\nonly tunes the weights regardless of the architecture. Our method can be easily\ngeneralized to an unsupervised paradigm by replacing supernet training with\nself-supervised learning in the source domain tasks and performing linear\nevaluation in the downstream tasks. This further improves the search efficiency\nof our method. Moreover, we also provide principled and empirical analysis to\nexplain why our approach works by investigating the ineffectiveness of existing\nneural architecture search. We find that preserving the joint distribution of\nthe network architecture and weights is of importance. This analysis not only\nbenefits image recognition but also provides insights for crafting neural\nnetworks. Experiments on five representative image recognition tasks such as\nperson re-identification, age estimation, gender recognition, image\nclassification, and unsupervised domain adaptation demonstrate the\neffectiveness of our method.\n", "rewritten_text": "The latest visual recognition systems typically follow a two-step process: first, training a neural network on a large dataset like ImageNet, and then fine-tuning the network on a smaller, specific dataset. This process assumes that adjusting the network weights alone can transfer its capabilities across different domains, based on the assumption that a fixed architecture is suitable for all domains. However, each domain may require varying levels of feature hierarchy, leading to some neurons becoming redundant while others are activated to create new network structures. Our research demonstrates that dynamically adapting network architectures for each domain task, in addition to weight fine-tuning, improves efficiency and effectiveness compared to the traditional approach that focuses solely on weight tuning. Our method can also be applied to unsupervised learning by using self-supervised learning in the source domain tasks and linear evaluation in downstream tasks, enhancing search efficiency. Furthermore, we offer both theoretical and empirical analysis to explain why our approach is successful, highlighting the limitations of existing neural architecture search and emphasizing the importance of preserving the joint distribution of network architecture and weights. These insights not only benefit image recognition but also inform the design of neural networks. Experiments on various image recognition tasks, including person re-identification, age estimation, gender recognition, image classification, and unsupervised domain adaptation, validate the effectiveness of our method."}, "2003.13239": {"original_text": "  Cross view feature fusion is the key to address the occlusion problem in\nhuman pose estimation. The current fusion methods need to train a separate\nmodel for every pair of cameras making them difficult to scale. In this work,\nwe introduce MetaFuse, a pre-trained fusion model learned from a large number\nof cameras in the Panoptic dataset. The model can be efficiently adapted or\nfinetuned for a new pair of cameras using a small number of labeled images. The\nstrong adaptation power of MetaFuse is due in large part to the proposed\nfactorization of the original fusion model into two parts (1) a generic fusion\nmodel shared by all cameras, and (2) lightweight camera-dependent\ntransformations. Furthermore, the generic model is learned from many cameras by\na meta-learning style algorithm to maximize its adaptation capability to\nvarious camera poses. We observe in experiments that MetaFuse finetuned on the\npublic datasets outperforms the state-of-the-arts by a large margin which\nvalidates its value in practice.\n", "rewritten_text": "The key to solving the occlusion problem in human pose estimation is through cross-view feature fusion. Current fusion methods require training a separate model for each pair of cameras, making scalability challenging. This study introduces MetaFuse, a pre-trained fusion model trained on a large number of cameras in the Panoptic dataset. MetaFuse can be efficiently adapted or fine-tuned for a new camera pair using a small set of labeled images. Its strong adaptation capability is achieved by factorizing the fusion model into a generic part shared by all cameras and lightweight camera-specific transformations. The generic model is trained using a meta-learning algorithm on multiple cameras to enhance its adaptability to different camera poses. Experimental results show that MetaFuse, when fine-tuned on public datasets, significantly outperforms existing methods, demonstrating its practical value."}, "2011.12528": {"original_text": "  We propose a novel reference-based video colorization framework with\nspatiotemporal correspondence. Reference-based methods colorize grayscale\nframes referencing a user input color frame. Existing methods suffer from the\ncolor leakage between objects and the emergence of average colors, derived from\nnon-local semantic correspondence in space. To address this issue, we warp\ncolors only from the regions on the reference frame restricted by\ncorrespondence in time. We propagate masks as temporal correspondences, using\ntwo complementary tracking approaches: off-the-shelf instance tracking for high\nperformance segmentation, and newly proposed dense tracking to track various\ntypes of objects. By restricting temporally-related regions for referencing\ncolors, our approach propagates faithful colors throughout the video.\nExperiments demonstrate that our method outperforms state-of-the-art methods\nquantitatively and qualitatively.\n", "rewritten_text": "We introduce a new video colorization framework that uses a reference frame and spatiotemporal correspondence. Traditional reference-based methods colorize grayscale frames by referring to a user-provided color frame. However, existing methods often experience color bleeding between objects and the appearance of average colors due to non-local semantic connections in space. To tackle this issue, we selectively transfer colors from regions in the reference frame that are temporally aligned. We achieve this by utilizing temporal masks as correspondences, employing both off-the-shelf instance tracking for precise segmentation and a novel dense tracking method for tracking various object types. By restricting color referencing to temporally related regions, our approach ensures accurate color propagation throughout the video. Experimental results demonstrate that our technique surpasses current state-of-the-art methods in terms of both quantitative and qualitative performance."}}