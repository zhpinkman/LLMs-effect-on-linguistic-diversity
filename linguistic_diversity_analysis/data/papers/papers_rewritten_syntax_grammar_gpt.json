{"1812.10788": {"original_text": "  Hyperspectral remote sensing is a prominent research topic in data\nprocessing. Most of the spectral unmixing algorithms are developed by adopting\nthe linear mixing models. Nonnegative matrix factorization (NMF) and its\ndevelopments are used widely for estimation of signatures and fractional\nabundances in the SU problem. Sparsity constraints was added to NMF, and was\nregularized by $ L_ {q} $ norm. In this paper, at first hyperspectral images\nare clustered by fuzzy c- means method, and then a new algorithm based on\nsparsity constrained distributed optimization is used for spectral unmixing. In\nthe proposed algorithm, a network including clusters is employed. Each pixel in\nthe hyperspectral images considered as a node in this network. The proposed\nalgorithm is optimized with diffusion LMS strategy, and then the update\nequations for fractional abundance and signature matrices are obtained.\nSimulation results based on defined performance metrics illustrate advantage of\nthe proposed algorithm in spectral unmixing of hyperspectral data compared with\nother methods.\n", "rewritten_text": "Hyperspectral remote sensing is a prominent research topic in data processing, with most spectral unmixing algorithms being developed using linear mixing models. Nonnegative matrix factorization (NMF) and its variations are widely utilized for estimating signatures and fractional abundances in the spectral unmixing (SU) problem. To enhance NMF, sparsity constraints were incorporated and regularized using the $L_{q}$ norm. \n\nIn this study, hyperspectral images are initially clustered using the fuzzy c-means method, followed by the application of a novel algorithm based on sparsity-constrained distributed optimization for spectral unmixing. The proposed algorithm employs a network comprising clusters, with each pixel in the hyperspectral images represented as a node in this network. Optimization of the proposed algorithm is achieved through the diffusion least mean squares (LMS) strategy, leading to the derivation of update equations for fractional abundance and signature matrices.\n\nSimulation results, based on defined performance metrics, demonstrate the advantages of the proposed algorithm in spectral unmixing of hyperspectral data compared to other methods."}, "2305.04628": {"original_text": "  In this paper, we introduce a novel framework for the challenging problem of\nOne-Shot Unsupervised Domain Adaptation (OSUDA), which aims to adapt to a\ntarget domain with only a single unlabeled target sample. Unlike existing\napproaches that rely on large labeled source and unlabeled target data, our\nTarget-driven One-Shot UDA (TOS-UDA) approach employs a learnable augmentation\nstrategy guided by the target sample's style to align the source distribution\nwith the target distribution. Our method consists of three modules: an\naugmentation module, a style alignment module, and a classifier. Unlike\nexisting methods, our augmentation module allows for strong transformations of\nthe source samples, and the style of the single target sample available is\nexploited to guide the augmentation by ensuring perceptual similarity.\nFurthermore, our approach integrates augmentation with style alignment,\neliminating the need for separate pre-training on additional datasets. Our\nmethod outperforms or performs comparably to existing OS-UDA methods on the\nDigits and DomainNet benchmarks.\n", "rewritten_text": "This paper presents a new framework for addressing the challenging problem of One-Shot Unsupervised Domain Adaptation (OSUDA), which involves adapting to a target domain using only a single unlabeled target sample. Unlike current approaches that depend on large labeled source and unlabeled target data, our Target-driven One-Shot UDA (TOS-UDA) method utilizes a learnable augmentation strategy guided by the style of the target sample to align the source distribution with the target distribution. Our method comprises three modules: an augmentation module, a style alignment module, and a classifier. In contrast to existing methods, our augmentation module allows for significant transformations of the source samples, leveraging the style of the single target sample to guide the augmentation process and ensure perceptual similarity. Additionally, our approach integrates augmentation with style alignment, removing the necessity for separate pre-training on additional datasets. Our method demonstrates superior or comparable performance to existing OS-UDA methods on the Digits and DomainNet benchmarks."}, "2011.14589": {"original_text": "  Monocular 3D object detection, with the aim of predicting the geometric\nproperties of on-road objects, is a promising research topic for the\nintelligent perception systems of autonomous driving. Most state-of-the-art\nmethods follow a keypoint-based paradigm, where the keypoints of objects are\npredicted and employed as the basis for regressing the other geometric\nproperties. In this work, a unified network named as FADNet is presented to\naddress the task of monocular 3D object detection. In contrast to previous\nkeypoint-based methods, we propose to divide the output modalities into\ndifferent groups according to the estimation difficulty of object properties.\nDifferent groups are treated differently and sequentially associated by a\nconvolutional Gated Recurrent Unit. Another contribution of this work is the\nstrategy of depth hint augmentation. To provide characterized depth patterns as\nhints for depth estimation, a dedicated depth hint module is designed to\ngenerate row-wise features named as depth hints, which are explicitly\nsupervised in a bin-wise manner. The contributions of this work are validated\nby conducting experiments and ablation study on the KITTI benchmark. Without\nutilizing depth priors, post optimization, or other refinement modules, our\nnetwork performs competitively against state-of-the-art methods while\nmaintaining a decent running speed.\n", "rewritten_text": "Monocular 3D object detection is a promising research topic in the field of intelligent perception systems for autonomous driving, focusing on predicting the geometric properties of on-road objects. Most current methods adopt a keypoint-based approach, where object keypoints are predicted and used as the foundation for estimating other geometric properties. This study introduces a unified network called FADNet to tackle monocular 3D object detection. Unlike previous keypoint-based methods, we propose grouping the output modalities based on the difficulty of estimating object properties. These groups are treated differently and sequentially connected using a convolutional Gated Recurrent Unit. Additionally, a key contribution of this work is the implementation of depth hint augmentation. A dedicated depth hint module is designed to generate row-wise features, known as depth hints, to provide specific depth patterns as guidance for depth estimation. These depth hints are supervised explicitly in a bin-wise manner. The effectiveness of these contributions is demonstrated through experiments and an ablation study conducted on the KITTI benchmark. Our network achieves competitive performance against state-of-the-art methods without relying on depth priors, post-optimization, or other refinement modules, while maintaining a satisfactory running speed."}, "2307.13958": {"original_text": "  Recently, vision transformer based multimodal learning methods have been\nproposed to improve the robustness of face anti-spoofing (FAS) systems.\nHowever, multimodal face data collected from the real world is often imperfect\ndue to missing modalities from various imaging sensors. Recently,\nflexible-modal FAS~\\cite{yu2023flexible} has attracted more attention, which\naims to develop a unified multimodal FAS model using complete multimodal face\ndata but is insensitive to test-time missing modalities. In this paper, we\ntackle one main challenge in flexible-modal FAS, i.e., when missing modality\noccurs either during training or testing in real-world situations. Inspired by\nthe recent success of the prompt learning in language models, we propose\n\\textbf{V}isual \\textbf{P}rompt flexible-modal \\textbf{FAS} (VP-FAS), which\nlearns the modal-relevant prompts to adapt the frozen pre-trained foundation\nmodel to downstream flexible-modal FAS task. Specifically, both vanilla visual\nprompts and residual contextual prompts are plugged into multimodal\ntransformers to handle general missing-modality cases, while only requiring\nless than 4\\% learnable parameters compared to training the entire model.\nFurthermore, missing-modality regularization is proposed to force models to\nlearn consistent multimodal feature embeddings when missing partial modalities.\nExtensive experiments conducted on two multimodal FAS benchmark datasets\ndemonstrate the effectiveness of our VP-FAS framework that improves the\nperformance under various missing-modality cases while alleviating the\nrequirement of heavy model re-training.\n", "rewritten_text": "Recently, there have been advancements in vision transformer-based multimodal learning methods aimed at enhancing the robustness of face anti-spoofing (FAS) systems. However, the multimodal face data obtained from real-world scenarios often lacks certain modalities due to variations in imaging sensors. A recent approach known as flexible-modal FAS~\\cite{yu2023flexible} has garnered increased attention. This approach focuses on creating a unified multimodal FAS model using complete face data while being able to handle missing modalities during testing. \n\nIn this study, we address a key challenge in flexible-modal FAS, specifically the occurrence of missing modalities during training or testing in real-world settings. Drawing inspiration from the success of prompt learning in language models, we introduce the Visual Prompt flexible-modal FAS (VP-FAS) framework. VP-FAS learns modal-relevant prompts to fine-tune a pre-trained foundation model for the flexible-modal FAS task. The framework incorporates both vanilla visual prompts and residual contextual prompts into multimodal transformers to address general missing-modality scenarios, all while utilizing less than 4% of learnable parameters compared to training the entire model.\n\nMoreover, we propose a missing-modality regularization technique to encourage models to learn consistent multimodal feature embeddings even in the absence of certain modalities. Extensive experiments conducted on two benchmark datasets for multimodal FAS showcase the effectiveness of our VP-FAS framework. Our approach demonstrates improved performance across various missing-modality scenarios, reducing the need for extensive model re-training."}, "1803.0634": {"original_text": "  We present a method for estimating detailed scene illumination using human\nfaces in a single image. In contrast to previous works that estimate lighting\nin terms of low-order basis functions or distant point lights, our technique\nestimates illumination at a higher precision in the form of a non-parametric\nenvironment map. Based on the observation that faces can exhibit strong\nhighlight reflections from a broad range of lighting directions, we propose a\ndeep neural network for extracting highlights from faces, and then trace these\nreflections back to the scene to acquire the environment map. Since real\ntraining data for highlight extraction is very limited, we introduce an\nunsupervised scheme for finetuning the network on real images, based on the\nconsistent diffuse chromaticity of a given face seen in multiple real images.\nIn tracing the estimated highlights to the environment, we reduce the blurring\neffect of skin reflectance on reflected light through a deconvolution\ndetermined by prior knowledge on face material properties. Comparisons to\nprevious techniques for highlight extraction and illumination estimation show\nthe state-of-the-art performance of this approach on a variety of indoor and\noutdoor scenes.\n", "rewritten_text": "We introduce a novel method for accurately estimating detailed scene illumination by leveraging human faces within a single image. Unlike previous approaches that rely on low-order basis functions or distant point lights to estimate lighting, our technique achieves a higher precision by generating a non-parametric environment map. Our approach is motivated by the fact that faces can display distinct highlight reflections from a wide range of lighting angles. To address this, we propose a deep neural network designed to extract highlights from faces and then map these reflections back to the scene to construct the environment map.\n\nGiven the scarcity of real training data for highlight extraction, we implement an unsupervised scheme to fine-tune the network using real images. This fine-tuning process is based on the consistent diffuse chromaticity of a specific face observed across multiple real images. To mitigate the blurring effect of skin reflectance on reflected light during the tracing of estimated highlights to the environment, we employ a deconvolution technique informed by prior knowledge of face material properties.\n\nComparative evaluations against existing methods for highlight extraction and illumination estimation demonstrate the superior performance of our approach across a diverse set of indoor and outdoor scenes."}, "1902.00595": {"original_text": "  Intuitively, human readers cope easily with errors in text; typos,\nmisspelling, word substitutions, etc. do not unduly disrupt natural reading.\nPrevious work indicates that letter transpositions result in increased reading\ntimes, but it is unclear if this effect generalizes to more natural errors. In\nthis paper, we report an eye-tracking study that compares two error types\n(letter transpositions and naturally occurring misspelling) and two error rates\n(10% or 50% of all words contain errors). We find that human readers show\nunimpaired comprehension in spite of these errors, but error words cause more\nreading difficulty than correct words. Also, transpositions are more difficult\nthan misspellings, and a high error rate increases difficulty for all words,\nincluding correct ones. We then present a computational model that uses\ncharacter-based (rather than traditional word-based) surprisal to account for\nthese results. The model explains that transpositions are harder than\nmisspellings because they contain unexpected letter combinations. It also\nexplains the error rate effect: upcoming words are more difficultto predict\nwhen the context is degraded, leading to increased surprisal.\n", "rewritten_text": "Human readers intuitively handle errors in text with ease; typos, misspellings, and word substitutions do not significantly disrupt natural reading flow. Previous research suggests that letter transpositions can lead to longer reading times, but it remains uncertain whether this effect extends to more common errors. In this study, we present findings from an eye-tracking experiment comparing two types of errors (letter transpositions and naturally occurring misspellings) and two error rates (10% or 50% of all words containing errors). Our results show that despite the presence of errors, human readers maintain comprehension, although error words pose greater reading challenges compared to correct words. Specifically, transpositions prove more challenging than misspellings, and a higher error rate increases difficulty across all words, including those without errors. Additionally, we introduce a computational model that utilizes character-based surprisal, rather than traditional word-based methods, to explain these findings. The model clarifies that transpositions are more difficult due to unexpected letter combinations, and it elucidates the impact of error rates: words become harder to predict in degraded contexts, resulting in heightened surprisal."}, "cs/0406031": {"original_text": "  This paper describes a standalone, publicly-available implementation of the\nResolution of Anaphora Procedure (RAP) given by Lappin and Leass (1994). The\nRAP algorithm resolves third person pronouns, lexical anaphors, and identifies\npleonastic pronouns. Our implementation, JavaRAP, fills a current need in\nanaphora resolution research by providing a reference implementation that can\nbe benchmarked against current algorithms. The implementation uses the\nstandard, publicly available Charniak (2000) parser as input, and generates a\nlist of anaphora-antecedent pairs as output. Alternately, an in-place\nannotation or substitution of the anaphors with their antecedents can be\nproduced. Evaluation on the MUC-6 co-reference task shows that JavaRAP has an\naccuracy of 57.9%, similar to the performance given previously in the\nliterature (e.g., Preiss 2002).\n", "rewritten_text": "This paper presents a standalone, publicly available implementation of the Resolution of Anaphora Procedure (RAP) developed by Lappin and Leass (1994). The RAP algorithm is designed to resolve third-person pronouns, lexical anaphors, and identify pleonastic pronouns. Our implementation, JavaRAP, addresses a current need in anaphora resolution research by offering a reference implementation that can be used for benchmarking against existing algorithms. JavaRAP utilizes the standard, publicly available Charniak (2000) parser as input and produces a list of anaphora-antecedent pairs as output. Additionally, it can generate in-place annotations or substitutions of the anaphors with their antecedents. Evaluation on the MUC-6 co-reference task demonstrates that JavaRAP achieves an accuracy of 57.9%, consistent with the performance reported in previous literature (e.g., Preiss 2002)."}, "2010.13508": {"original_text": "  The SHApe Recovery from Partial textured 3D scans challenge, SHARP 2020, is\nthe first edition of a challenge fostering and benchmarking methods for\nrecovering complete textured 3D scans from raw incomplete data. SHARP 2020 is\norganised as a workshop in conjunction with ECCV 2020. There are two\ncomplementary challenges, the first one on 3D human scans, and the second one\non generic objects. Challenge 1 is further split into two tracks, focusing,\nfirst, on large body and clothing regions, and, second, on fine body details. A\nnovel evaluation metric is proposed to quantify jointly the shape\nreconstruction, the texture reconstruction and the amount of completed data.\nAdditionally, two unique datasets of 3D scans are proposed, to provide raw\nground-truth data for the benchmarks. The datasets are released to the\nscientific community. Moreover, an accompanying custom library of software\nroutines is also released to the scientific community. It allows for processing\n3D scans, generating partial data and performing the evaluation. Results of the\ncompetition, analysed in comparison to baselines, show the validity of the\nproposed evaluation metrics, and highlight the challenging aspects of the task\nand of the datasets. Details on the SHARP 2020 challenge can be found at\nhttps://cvi2.uni.lu/sharp2020/.\n", "rewritten_text": "The Shape Recovery from Partial Textured 3D Scans Challenge, known as SHARP 2020, marks the inaugural edition of a challenge aimed at promoting and evaluating methods for reconstructing complete textured 3D scans from raw incomplete data. SHARP 2020 is being held as a workshop in conjunction with ECCV 2020. The challenge consists of two distinct components: the first focusing on 3D human scans, and the second on generic objects. Challenge 1 is further divided into two tracks, with one emphasizing large body and clothing regions, and the other concentrating on fine body details. A novel evaluation metric has been introduced to comprehensively measure the quality of shape reconstruction, texture reconstruction, and the completeness of the data. Additionally, two unique datasets of 3D scans have been curated to serve as raw ground-truth data for the benchmarks, which have been made available to the scientific community. Furthermore, a specialized library of software routines has been developed and shared with the scientific community to facilitate the processing of 3D scans, generation of partial data, and evaluation of results. The competition outcomes, when compared against baseline results, demonstrate the effectiveness of the proposed evaluation metrics and underscore the challenging nature of the task and datasets. For more information on the SHARP 2020 challenge, please visit https://cvi2.uni.lu/sharp2020/."}, "2305.08844": {"original_text": "  Despite their unprecedented success, even the largest language models make\nmistakes. Similar to how humans learn and improve using feedback, previous work\nproposed providing language models with natural language feedback to guide them\nin repairing their outputs. Because human-generated critiques are expensive to\nobtain, researchers have devised learned critique generators in lieu of human\ncritics while assuming one can train downstream models to utilize generated\nfeedback. However, this approach does not apply to black-box or limited access\nmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\nlarge general-purpose language agents, fine-tuning is neither computationally\nnor spatially efficient as it results in multiple copies of the network. In\nthis work, we introduce RL4F (Reinforcement Learning for Feedback), a\nmulti-agent collaborative framework where the critique generator is trained to\nmaximize end-task performance of GPT-3, a fixed model more than 200 times its\nsize. RL4F produces critiques that help GPT-3 revise its outputs. We study\nthree datasets for action planning, summarization and alphabetization and show\nrelative improvements up to 10% in multiple text similarity metrics over other\nlearned, retrieval-augmented or prompting-based critique generators.\n", "rewritten_text": "Despite their remarkable success, even the largest language models are prone to errors. Just as humans learn and grow through feedback, previous research has suggested providing language models with natural language feedback to assist them in refining their outputs. Obtaining human-generated critiques can be costly, leading researchers to develop learned critique generators as substitutes for human critics. The idea is to train downstream models to incorporate the generated feedback. However, this approach is not applicable to black-box or limited access models like ChatGPT, which cannot undergo fine-tuning. Additionally, in the age of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient, as it results in redundant network copies.\n\nIn this study, we introduce RL4F (Reinforcement Learning for Feedback), a collaborative multi-agent framework where the critique generator is trained to enhance the end-task performance of GPT-3, a fixed model over 200 times its size. RL4F generates critiques that assist GPT-3 in refining its outputs. We evaluate RL4F on three datasets for action planning, summarization, and alphabetization, demonstrating relative improvements of up to 10% in various text similarity metrics compared to other learned, retrieval-augmented, or prompting-based critique generators."}, "2312.14697": {"original_text": "  Polarization information of the light can provide rich cues for computer\nvision and scene understanding tasks, such as the type of material, pose, and\nshape of the objects. With the advent of new and cheap polarimetric sensors,\nthis imaging modality is becoming accessible to a wider public for solving\nproblems such as pose estimation, 3D reconstruction, underwater navigation, and\ndepth estimation. However, we observe several limitations regarding the usage\nof this sensorial modality, as well as a lack of standards and publicly\navailable tools to analyze polarization images. Furthermore, although\npolarization camera manufacturers usually provide acquisition tools to\ninterface with their cameras, they rarely include processing algorithms that\nmake use of the polarization information. In this paper, we review recent\nadvances in applications that involve polarization imaging, including a\ncomprehensive survey of recent advances on polarization for vision and robotics\nperception tasks. We also introduce a complete software toolkit that provides\ncommon standards to communicate with and process information from most of the\nexisting micro-grid polarization cameras on the market. The toolkit also\nimplements several image processing algorithms for this modality, and it is\npublicly available on GitHub: https://github.com/vibot-lab/Pola4all_JEI_2023.\n", "rewritten_text": "Polarization information from light can offer valuable cues for tasks in computer vision and scene understanding, such as determining the material type, pose, and shape of objects. The accessibility of new, affordable polarimetric sensors has expanded the reach of this imaging modality to a broader audience, enabling solutions for challenges like pose estimation, 3D reconstruction, underwater navigation, and depth estimation. However, we have identified various limitations in utilizing this sensory modality, along with a lack of standards and publicly accessible tools for analyzing polarization images.\n\nMoreover, while polarization camera manufacturers typically provide acquisition tools for interfacing with their cameras, they seldom include processing algorithms that leverage polarization information. This paper presents a review of recent advancements in applications involving polarization imaging, encompassing a thorough examination of recent progress in polarization for vision and robotics perception tasks. Additionally, we introduce a comprehensive software toolkit that establishes common standards for communication and information processing from the majority of micro-grid polarization cameras available in the market.\n\nThe toolkit incorporates multiple image processing algorithms tailored for this modality and is openly accessible on GitHub at: https://github.com/vibot-lab/Pola4all_JEI_2023."}, "1809.04191": {"original_text": "  To realize the promise of ubiquitous embedded deep network inference, it is\nessential to seek limits of energy and area efficiency. To this end,\nlow-precision networks offer tremendous promise because both energy and area\nscale down quadratically with the reduction in precision. Here we demonstrate\nResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on\nthe ImageNet classification benchmark that, at 8-bit precision exceed the\naccuracy of the full-precision baseline networks after one epoch of finetuning,\nthereby leveraging the availability of pretrained models. We also demonstrate\nResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn 4-bit models that match\nthe accuracy of the full-precision baseline networks -- the highest scores to\ndate. Surprisingly, the weights of the low-precision networks are very close\n(in cosine similarity) to the weights of the corresponding baseline networks,\nmaking training from scratch unnecessary.\n  We find that gradient noise due to quantization during training increases\nwith reduced precision, and seek ways to overcome this noise. The number of\niterations required by SGD to achieve a given training error is related to the\nsquare of (a) the distance of the initial solution from the final plus (b) the\nmaximum variance of the gradient estimates. Therefore, we (a) reduce solution\ndistance by starting with pretrained fp32 precision baseline networks and\nfine-tuning, and (b) combat gradient noise introduced by quantization by\ntraining longer and reducing learning rates. Sensitivity analysis indicates\nthat these simple techniques, coupled with proper activation function range\ncalibration to take full advantage of the limited precision, are sufficient to\ndiscover low-precision networks, if they exist, close to fp32 precision\nbaseline networks. The results herein provide evidence that 4-bits suffice for\nclassification.\n", "rewritten_text": "In order to fully realize the potential of ubiquitous embedded deep network inference, it is crucial to explore the boundaries of energy and area efficiency. Low-precision networks show great promise in this regard, as both energy consumption and area requirements decrease quadratically with precision reduction. In this study, we present the performance of ResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on the ImageNet classification benchmark. These networks, operating at 8-bit precision, surpass the accuracy of full-precision baseline networks after just one epoch of finetuning, leveraging the availability of pretrained models. Additionally, we showcase 4-bit models of ResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn that achieve accuracy levels on par with full-precision baseline networks, marking the highest scores achieved to date.\n\nRemarkably, the weights of the low-precision networks exhibit a high degree of similarity (in terms of cosine similarity) to the weights of their corresponding baseline networks, rendering training from scratch unnecessary. Our investigation reveals that gradient noise resulting from quantization during training escalates with decreasing precision, prompting us to explore strategies to mitigate this noise. We observe that the number of iterations required by Stochastic Gradient Descent (SGD) to reach a specific training error is influenced by the square of (a) the initial solution's distance from the final solution and (b) the maximum variance of gradient estimates.\n\nTo address this, we adopt two key approaches: firstly, we minimize the solution distance by commencing training with pretrained fp32 precision baseline networks and fine-tuning them; secondly, we counteract gradient noise induced by quantization by prolonging training duration and reducing learning rates. Our sensitivity analysis indicates that these straightforward techniques, combined with appropriate calibration of activation function ranges to optimize limited precision, are effective in identifying low-precision networks that closely approximate fp32 precision baseline networks, if such networks exist. The findings presented herein provide compelling evidence that 4-bit precision is sufficient for classification tasks."}, "2103.14799": {"original_text": "  Image representation is an important topic in computer vision and pattern\nrecognition. It plays a fundamental role in a range of applications towards\nunderstanding visual contents. Moment-based image representation has been\nreported to be effective in satisfying the core conditions of semantic\ndescription due to its beneficial mathematical properties, especially geometric\ninvariance and independence. This paper presents a comprehensive survey of the\northogonal moments for image representation, covering recent advances in\nfast/accurate calculation, robustness/invariance optimization, definition\nextension, and application. We also create a software package for a variety of\nwidely-used orthogonal moments and evaluate such methods in a same base. The\npresented theory analysis, software implementation, and evaluation results can\nsupport the community, particularly in developing novel techniques and\npromoting real-world applications.\n", "rewritten_text": "Image representation is a crucial aspect of computer vision and pattern recognition, playing a fundamental role in various applications aimed at understanding visual content. Moment-based image representation has been recognized for its effectiveness in meeting the core requirements of semantic description, thanks to its advantageous mathematical properties, notably geometric invariance and independence. This paper offers a comprehensive survey of orthogonal moments for image representation, encompassing recent advancements in fast and accurate calculation, robustness and invariance optimization, definition extension, and application. Additionally, we have developed a software package that includes a variety of widely-used orthogonal moments and have evaluated these methods on a consistent basis. The theoretical analysis, software implementation, and evaluation results presented here can provide valuable support to the community, particularly in the development of innovative techniques and the advancement of real-world applications."}, "2406.19297": {"original_text": "  Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we\ndemonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)\napproach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.\n", "rewritten_text": "Continuous learning involves incrementally training a model on a sequence of tasks to learn new tasks while minimizing performance drop on previous tasks. Current approaches at the intersection of Continuous Learning and Visual Question Answering (VQA) do not explore how the multimodal nature of the input impacts the learning dynamics of a model. In this paper, we show that each modality evolves at different rates across a range of tasks, a phenomenon observed in both traditional encoder-only models and modern Vision & Language (VL) models. Building on this insight, we introduce a modality-aware feature distillation (MAFED) approach that surpasses existing baselines across models of various scales in three multimodal continuous learning scenarios. Additionally, we present ablations demonstrating that modality-aware distillation complements experience replay. Our findings underscore the significance of addressing modality-specific dynamics to prevent forgetting in multimodal continuous learning."}, "2410.15865": {"original_text": "  Grammatical features such as number and gender serve two central functions in\nhuman languages. While they encode salient semantic attributes like numerosity\nand animacy, they also offload sentence processing cost by predictably linking\nwords together via grammatical agreement. Grammars exhibit consistent\norganizational patterns across diverse languages, invariably rooted in a\nsemantic foundation, a widely confirmed but still theoretically unexplained\nphenomenon. To explain the basis of universal grammatical patterns, we unify\ntwo fundamental properties of grammar, semantic encoding and agreement-based\npredictability, into a single information-theoretic objective under cognitive\nconstraints. Our analyses reveal that grammatical organization provably\ninherits from perceptual attributes, but that grammars empirically prioritize\nfunctional goals, promoting efficient language processing over semantic\nencoding.\n", "rewritten_text": "Grammatical features, such as number and gender, play two central roles in human languages. They not only convey important semantic attributes like numerosity and animacy but also help reduce the processing cost of sentences by establishing predictable connections between words through grammatical agreement. Across a variety of languages, grammars consistently demonstrate organizational patterns that are deeply rooted in semantics, a phenomenon that is widely acknowledged but not yet fully explained theoretically. In order to shed light on the universal basis of grammatical patterns, we propose a unified approach that combines two key aspects of grammar: semantic encoding and agreement-based predictability. By framing these properties within an information-theoretic framework and considering cognitive constraints, our analysis shows that the structure of grammar is influenced by perceptual attributes, while also prioritizing functional objectives to enhance efficient language processing over semantic representation."}, "2306.05119": {"original_text": "  Factuality is important to dialogue summarization. Factual error correction\n(FEC) of model-generated summaries is one way to improve factuality. Current\nFEC evaluation that relies on factuality metrics is not reliable and detailed\nenough. To address this problem, we are the first to manually annotate a FEC\ndataset for dialogue summarization containing 4000 items and propose FERRANTI,\na fine-grained evaluation framework based on reference correction that\nautomatically evaluates the performance of FEC models on different error\ncategories. Using this evaluation framework, we conduct sufficient experiments\nwith FEC approaches under a variety of settings and find the best training\nmodes and significant differences in the performance of the existing approaches\non different factual error categories.\n", "rewritten_text": "Ensuring factual accuracy is crucial for dialogue summarization. One effective method to enhance factuality is through correcting factual errors (FEC) in summaries generated by models. However, the current evaluation of FEC, which relies on factuality metrics, lacks reliability and detailed analysis. To tackle this issue, we have taken the initiative to manually annotate a dataset specifically for FEC in dialogue summarization, comprising 4000 items. Introducing FERRANTI, a sophisticated evaluation framework centered on reference correction, we aim to automatically assess the efficacy of FEC models across various error categories.\n\nThrough the utilization of this evaluation framework, we have conducted comprehensive experiments with different FEC approaches under diverse settings. Our findings reveal optimal training methods and notable variations in the performance of existing approaches across distinct factual error categories."}, "2410.16190": {"original_text": "  This work explores how human judgement about salient regions of an image can\nbe introduced into deep convolutional neural network (DCNN) training.\nTraditionally, training of DCNNs is purely data-driven. This often results in\nlearning features of the data that are only coincidentally correlated with\nclass labels. Human saliency can guide network training using our proposed new\ncomponent of the loss function that ConveYs Brain Oversight to Raise\nGeneralization (CYBORG) and penalizes the model for using non-salient regions.\nThis mechanism produces DCNNs achieving higher accuracy and generalization\ncompared to using the same training data without human salience. Experimental\nresults demonstrate that CYBORG applies across multiple network architectures\nand problem domains (detection of synthetic faces, iris presentation attacks\nand anomalies in chest X-rays), while requiring significantly less data than\ntraining without human saliency guidance. Visualizations show that\nCYBORG-trained models' saliency is more consistent across independent training\nruns than traditionally-trained models, and also in better agreement with\nhumans. To lower the cost of collecting human annotations, we also explore\nusing deep learning to provide automated annotations. CYBORG training of CNNs\naddresses important issues such as reducing the appetite for large training\nsets, increasing interpretability, and reducing fragility by generalizing\nbetter to new types of data.\n", "rewritten_text": "This study investigates the integration of human judgment regarding salient regions in an image into the training of deep convolutional neural networks (DCNNs). Traditionally, DCNN training is solely data-driven, often leading to the learning of features in the data that are only coincidentally correlated with class labels. By incorporating human saliency into network training through a novel component of the loss function called ConveYs Brain Oversight to Raise Generalization (CYBORG), the model is penalized for utilizing non-salient regions. This approach results in DCNNs achieving higher accuracy and generalization compared to training without human saliency. Experimental findings demonstrate the effectiveness of CYBORG across various network architectures and problem domains, such as detecting synthetic faces, iris presentation attacks, and anomalies in chest X-rays, while requiring significantly less data than training without human saliency guidance. Visualizations reveal that models trained with CYBORG exhibit more consistent saliency patterns across independent training runs compared to traditionally-trained models, aligning better with human perception. To reduce the cost of collecting human annotations, the study also explores the use of deep learning for automated annotations. The CYBORG training of CNNs addresses key issues including reducing the dependency on large training sets, enhancing interpretability, and improving generalization to new data types, thereby mitigating fragility."}, "2311.07611": {"original_text": "  In this study we intentionally introduce biases into large language model\nresponses in an attempt to create specific personas for interactive media\npurposes. We explore the differences between open source models such as\nFalcon-7b and the GPT-4 model from Open AI, and we quantify some differences in\nresponses afforded by the two systems. We find that the guardrails in the GPT-4\nmixture of experts models with a supervisor, while useful in assuring AI\nalignment in general, are detrimental in trying to construct personas with a\nvariety of uncommon viewpoints. This study aims to set the groundwork for\nfuture exploration in intentional biases of large language models such that\nthese practices can be applied in the creative field, and new forms of media.\n", "rewritten_text": "This study intentionally introduces biases into responses generated by large language models to create specific personas for interactive media purposes. A comparison is made between open source models like Falcon-7b and the GPT-4 model from Open AI, with an analysis of the differences in responses produced by these systems. The study reveals that the guardrails in the GPT-4 mixture of experts models, supervised for AI alignment, are beneficial for general alignment but hinder the construction of personas with diverse and uncommon viewpoints. The aim of this study is to lay the foundation for future research on intentional biases in large language models, enabling the application of these practices in the creative field and new forms of media."}, "2409.01522": {"original_text": "  Long-term motion generation is a challenging task that requires producing\ncoherent and realistic sequences over extended durations. Current methods\nprimarily rely on framewise motion representations, which capture only static\nspatial details and overlook temporal dynamics. This approach leads to\nsignificant redundancy across the temporal dimension, complicating the\ngeneration of effective long-term motion. To overcome these limitations, we\nintroduce the novel concept of Lagrangian Motion Fields, specifically designed\nfor long-term motion generation. By treating each joint as a Lagrangian\nparticle with uniform velocity over short intervals, our approach condenses\nmotion representations into a series of \"supermotions\" (analogous to\nsuperpixels). This method seamlessly integrates static spatial information with\ninterpretable temporal dynamics, transcending the limitations of existing\nnetwork architectures and motion sequence content types. Our solution is\nversatile and lightweight, eliminating the need for neural network\npreprocessing. Our approach excels in tasks such as long-term music-to-dance\ngeneration and text-to-motion generation, offering enhanced efficiency,\nsuperior generation quality, and greater diversity compared to existing\nmethods. Additionally, the adaptability of Lagrangian Motion Fields extends to\napplications like infinite motion looping and fine-grained controlled motion\ngeneration, highlighting its broad utility. Video demonstrations are available\nat \\url{https://plyfager.github.io/LaMoG}.\n", "rewritten_text": "Generating long-term motion is a complex task that involves creating coherent and realistic sequences over extended periods. Current methods mainly rely on framewise motion representations, which capture static spatial details but overlook temporal dynamics. This approach results in significant redundancy across time, making it challenging to generate effective long-term motion. To address these challenges, we introduce the innovative concept of Lagrangian Motion Fields, specifically tailored for long-term motion generation. In our approach, each joint is treated as a Lagrangian particle with uniform velocity over short intervals, condensing motion representations into a series of \"supermotions\" (similar to superpixels). This method seamlessly combines static spatial information with interpretable temporal dynamics, surpassing the limitations of existing network architectures and motion sequence content types. Our solution is versatile and lightweight, eliminating the need for neural network preprocessing. It excels in tasks such as long-term music-to-dance generation and text-to-motion generation, providing improved efficiency, superior generation quality, and greater diversity compared to current methods. Moreover, the adaptability of Lagrangian Motion Fields extends to applications like infinite motion looping and fine-grained controlled motion generation, showcasing its broad utility. Video demonstrations can be viewed at \\url{https://plyfager.github.io/LaMoG}."}, "1704.06756": {"original_text": "  We have developed convolutional neural networks (CNN) for a facial expression\nrecognition task. The goal is to classify each facial image into one of the\nseven facial emotion categories considered in this study. We trained CNN models\nwith different depth using gray-scale images. We developed our models in Torch\nand exploited Graphics Processing Unit (GPU) computation in order to expedite\nthe training process. In addition to the networks performing based on raw pixel\ndata, we employed a hybrid feature strategy by which we trained a novel CNN\nmodel with the combination of raw pixel data and Histogram of Oriented\nGradients (HOG) features. To reduce the overfitting of the models, we utilized\ndifferent techniques including dropout and batch normalization in addition to\nL2 regularization. We applied cross validation to determine the optimal\nhyper-parameters and evaluated the performance of the developed models by\nlooking at their training histories. We also present the visualization of\ndifferent layers of a network to show what features of a face can be learned by\nCNN models.\n", "rewritten_text": "We have developed convolutional neural networks (CNN) for a facial expression recognition task with the aim of classifying each facial image into one of the seven facial emotion categories considered in this study. Our CNN models were trained with varying depths using gray-scale images. These models were developed in Torch, and we utilized Graphics Processing Unit (GPU) computation to expedite the training process.\n\nIn addition to the networks performing based on raw pixel data, we implemented a hybrid feature strategy. This involved training a novel CNN model with a combination of raw pixel data and Histogram of Oriented Gradients (HOG) features. To address overfitting, we employed various techniques such as dropout, batch normalization, and L2 regularization.\n\nCross-validation was applied to determine the optimal hyperparameters, and the performance of the developed models was evaluated by analyzing their training histories. Furthermore, we visually represented different layers of a network to demonstrate the facial features learned by the CNN models."}, "2312.17292": {"original_text": "  Word embedding methods (WEMs) are extensively used for representing text\ndata. The dimensionality of these embeddings varies across various tasks and\nimplementations. The effect of dimensionality change on the accuracy of the\ndownstream task is a well-explored question. However, how the dimensionality\nchange affects the bias of word embeddings needs to be investigated. Using the\nEnglish Wikipedia corpus, we study this effect for two static (Word2Vec and\nfastText) and two context-sensitive (ElMo and BERT) WEMs. We have two\nobservations. First, there is a significant variation in the bias of word\nembeddings with the dimensionality change. Second, there is no uniformity in\nhow the dimensionality change affects the bias of word embeddings. These\nfactors should be considered while selecting the dimensionality of word\nembeddings.\n", "rewritten_text": "Word embedding methods (WEMs) are widely utilized for text data representation. The dimensionality of these embeddings varies depending on the task and implementation. The impact of changing dimensionality on downstream task accuracy has been extensively studied. However, the effect of dimensionality change on the bias of word embeddings requires further investigation. In this study, we examine this effect using the English Wikipedia corpus with two static methods (Word2Vec and fastText) and two context-sensitive methods (ElMo and BERT). Our findings reveal two key observations. Firstly, there is a notable variation in the bias of word embeddings with changes in dimensionality. Secondly, the impact of dimensionality change on the bias of word embeddings is not consistent across all cases. These factors should be taken into account when determining the dimensionality of word embeddings."}, "2401.07745": {"original_text": "  Open-vocabulary 3D instance segmentation is cutting-edge for its ability to\nsegment 3D instances without predefined categories. However, progress in 3D\nlags behind its 2D counterpart due to limited annotated 3D data. To address\nthis, recent works first generate 2D open-vocabulary masks through 2D models\nand then merge them into 3D instances based on metrics calculated between two\nneighboring frames. In contrast to these local metrics, we propose a novel\nmetric, view consensus rate, to enhance the utilization of multi-view\nobservations. The key insight is that two 2D masks should be deemed part of the\nsame 3D instance if a significant number of other 2D masks from different views\ncontain both these two masks. Using this metric as edge weight, we construct a\nglobal mask graph where each mask is a node. Through iterative clustering of\nmasks showing high view consensus, we generate a series of clusters, each\nrepresenting a distinct 3D instance. Notably, our model is training-free.\nThrough extensive experiments on publicly available datasets, including\nScanNet++, ScanNet200 and MatterPort3D, we demonstrate that our method achieves\nstate-of-the-art performance in open-vocabulary 3D instance segmentation. Our\nproject page is at https://pku-epic.github.io/MaskClustering.\n", "rewritten_text": "Cutting-edge open-vocabulary 3D instance segmentation is notable for its ability to segment 3D instances without predefined categories. However, progress in 3D segmentation lags behind its 2D counterpart due to limited annotated 3D data. To overcome this challenge, recent studies have adopted a strategy where they first generate 2D open-vocabulary masks using 2D models and then merge them into 3D instances based on metrics calculated between adjacent frames.\n\nIn contrast to the local metrics used in previous approaches, we introduce a novel metric called the view consensus rate to improve the utilization of multi-view observations. The key idea is that two 2D masks should be considered part of the same 3D instance if a significant number of other 2D masks from different views contain both of these masks. By using this metric as an edge weight, we construct a global mask graph where each mask serves as a node. Through iterative clustering of masks that exhibit high view consensus, we generate a series of clusters, each representing a distinct 3D instance. Notably, our model does not require training.\n\nThrough extensive experiments conducted on publicly available datasets, including ScanNet++, ScanNet200, and MatterPort3D, we demonstrate that our method achieves state-of-the-art performance in open-vocabulary 3D instance segmentation. For more information, please visit our project page at https://pku-epic.github.io/MaskClustering."}, "2404.05916": {"original_text": "  Echocardiography segmentation for cardiac analysis is time-consuming and\nresource-intensive due to the variability in image quality and the necessity to\nprocess scans from various standard views. While current automated segmentation\nmethods in echocardiography show promising performance, they are trained on\nspecific scan views to analyze corresponding data. However, this solution has a\nlimitation as the number of required models increases with the number of\nstandard views. To address this, in this paper, we present a prompt-driven\nuniversal method for view-agnostic echocardiography analysis. Considering the\ndomain shift between standard views, we first introduce a method called prompt\nmatching, aimed at learning prompts specific to different views by matching\nprompts and querying input embeddings using a pre-trained vision model. Then,\nwe utilized a pre-trained medical language model to align textual information\nwith pixel data for accurate segmentation. Extensive experiments on three\nstandard views showed that our approach significantly outperforms the\nstate-of-the-art universal methods and achieves comparable or even better\nperformances over the segmentation model trained and tested on same views.\n", "rewritten_text": "Echocardiography segmentation for cardiac analysis can be time-consuming and resource-intensive due to variations in image quality and the need to process scans from different standard views. While current automated segmentation methods in echocardiography have shown promising performance, they are typically trained on specific scan views to analyze corresponding data. However, a limitation arises as the number of required models increases with the number of standard views.\n\nTo address this challenge, this paper introduces a prompt-driven universal method for view-agnostic echocardiography analysis. Recognizing the domain shift between standard views, we propose a method called prompt matching. This method aims to learn prompts specific to different views by matching prompts and querying input embeddings using a pre-trained vision model. Additionally, we leverage a pre-trained medical language model to align textual information with pixel data for accurate segmentation.\n\nExtensive experiments conducted on three standard views demonstrate that our approach significantly outperforms the current state-of-the-art universal methods. Furthermore, our method achieves comparable or even superior performance compared to the segmentation model trained and tested on the same views."}, "1908.09884": {"original_text": "  We consider the problem of discovering novel object categories in an image\ncollection. While these images are unlabelled, we also assume prior knowledge\nof related but different image classes. We use such prior knowledge to reduce\nthe ambiguity of clustering, and improve the quality of the newly discovered\nclasses. Our contributions are twofold. The first contribution is to extend\nDeep Embedded Clustering to a transfer learning setting; we also improve the\nalgorithm by introducing a representation bottleneck, temporal ensembling, and\nconsistency. The second contribution is a method to estimate the number of\nclasses in the unlabelled data. This also transfers knowledge from the known\nclasses, using them as probes to diagnose different choices for the number of\nclasses in the unlabelled subset. We thoroughly evaluate our method,\nsubstantially outperforming state-of-the-art techniques in a large number of\nbenchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN.\n", "rewritten_text": "We address the challenge of identifying new object categories within a collection of images. Although these images lack labels, we leverage prior knowledge of related but distinct image classes to enhance clustering accuracy and elevate the quality of the newly identified classes. Our contributions are twofold. Firstly, we enhance Deep Embedded Clustering by adapting it to a transfer learning framework and refining the algorithm through the introduction of a representation bottleneck, temporal ensembling, and consistency. Secondly, we propose a method to estimate the number of classes present in the unlabeled data. This method leverages knowledge from known classes, using them as probes to evaluate various options for the number of classes within the unlabeled subset. Our approach undergoes thorough evaluation and significantly outperforms state-of-the-art techniques across multiple benchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN."}, "2212.08334": {"original_text": "  Scene understanding has made tremendous progress over the past few years, as\ndata acquisition systems are now providing an increasing amount of data of\nvarious modalities (point cloud, depth, RGB...). However, this improvement\ncomes at a large cost on computation resources and data annotation\nrequirements. To analyze geometric information and images jointly, many\napproaches rely on both a 2D loss and 3D loss, requiring not only 2D per\npixel-labels but also 3D per-point labels. However, obtaining a 3D groundtruth\nis challenging, time-consuming and error-prone. In this paper, we show that\nimage segmentation can benefit from 3D geometric information without requiring\na 3D groundtruth, by training the geometric feature extraction and the 2D\nsegmentation network jointly, in an end-to-end fashion, using only the 2D\nsegmentation loss. Our method starts by extracting a map of 3D features\ndirectly from a provided point cloud by using a lightweight 3D neural network.\nThe 3D feature map, merged with the RGB image, is then used as an input to a\nclassical image segmentation network. Our method can be applied to many 2D\nsegmentation networks, improving significantly their performance with only a\nmarginal network weight increase and light input dataset requirements, since no\n3D groundtruth is required.\n", "rewritten_text": "In recent years, there has been significant progress in scene understanding due to advancements in data acquisition systems that now offer a wealth of data from various modalities such as point cloud, depth, and RGB. However, this progress has come at a high computational cost and increased data annotation needs. Many methods that aim to analyze geometric information and images together utilize both 2D and 3D losses, necessitating not only 2D per-pixel labels but also 3D per-point labels. Yet, obtaining accurate 3D ground truth data is a challenging, time-consuming, and error-prone task.\n\nThis paper proposes a novel approach where image segmentation can leverage 3D geometric information without the need for 3D ground truth data. The method involves training the geometric feature extraction and 2D segmentation network simultaneously in an end-to-end manner using only the 2D segmentation loss. Initially, a map of 3D features is extracted directly from a given point cloud using a lightweight 3D neural network. This 3D feature map is then combined with the RGB image and fed into a traditional image segmentation network.\n\nThe proposed method can be integrated into various 2D segmentation networks, leading to a significant enhancement in their performance with minimal increase in network weight and light input dataset requirements, as it eliminates the necessity for 3D ground truth data."}, "2309.09379": {"original_text": "  Nowadays, photogrammetrically derived point clouds are widely used in many\ncivilian applications due to their low cost and flexibility in acquisition.\nTypically, photogrammetric point clouds are assessed through reference data\nsuch as LiDAR point clouds. However, when reference data are not available, the\nassessment of photogrammetric point clouds may be challenging. Since these\npoint clouds are algorithmically derived, their accuracies and precisions are\nhighly varying with the camera networks, scene complexity, and dense image\nmatching (DIM) algorithms, and there is no standard error metric to determine\nper-point errors. The theory of internal reliability of camera networks has\nbeen well studied through first-order error estimation of Bundle Adjustment\n(BA), which is used to understand the errors of 3D points assuming known\nmeasurement errors. However, the measurement errors of the DIM algorithms are\nintricate to an extent that every single point may have its error function\ndetermined by factors such as pixel intensity, texture entropy, and surface\nsmoothness. Despite the complexity, there exist a few common metrics that may\naid the process of estimating the posterior reliability of the derived points,\nespecially in a multi-view stereo (MVS) setup when redundancies are present. In\nthis paper, by using an aerial oblique photogrammetric block with LiDAR\nreference data, we analyze several internal matching metrics within a common\nMVS framework, including statistics in ray convergence, intersection angles,\nDIM energy, etc.\n", "rewritten_text": "In contemporary times, photogrammetrically derived point clouds are widely utilized in various civilian applications due to their cost-effectiveness and flexibility in acquisition. Typically, the assessment of photogrammetric point clouds involves comparing them to reference data such as LiDAR point clouds. However, in cases where reference data is unavailable, evaluating the accuracy of photogrammetric point clouds can be challenging. These point clouds are generated algorithmically, leading to variations in accuracy and precision based on factors like camera networks, scene complexity, and dense image matching (DIM) algorithms. Currently, there is no standardized error metric for determining per-point errors.\n\nThe theory of internal reliability of camera networks has been extensively researched, primarily through first-order error estimation of Bundle Adjustment (BA), which helps in understanding the errors of 3D points assuming known measurement errors. However, the measurement errors of DIM algorithms are complex, with each point potentially having its error function influenced by factors such as pixel intensity, texture entropy, and surface smoothness. Despite this complexity, there are a few common metrics that can assist in estimating the reliability of derived points, particularly in a multi-view stereo (MVS) setup where redundancies exist.\n\nIn this study, we analyze various internal matching metrics within a common MVS framework using an aerial oblique photogrammetric block with LiDAR reference data. These metrics include statistics on ray convergence, intersection angles, DIM energy, and others."}, "2306.05061": {"original_text": "  Multi-task visual perception has a wide range of applications in scene\nunderstanding such as autonomous driving. In this work, we devise an efficient\nunified framework to solve multiple common perception tasks, including instance\nsegmentation, semantic segmentation, monocular 3D detection, and depth\nestimation. Simply sharing the same visual feature representations for these\ntasks impairs the performance of tasks, while independent task-specific feature\nextractors lead to parameter redundancy and latency. Thus, we design two\nfeature-merge branches to learn feature basis, which can be useful to, and thus\nshared by, multiple perception tasks. Then, each task takes the corresponding\nfeature basis as the input of the prediction task head to fulfill a specific\ntask. In particular, one feature merge branch is designed for instance-level\nrecognition the other for dense predictions. To enhance inter-branch\ncommunication, the instance branch passes pixel-wise spatial information of\neach instance to the dense branch using efficient dynamic convolution\nweighting. Moreover, a simple but effective dynamic routing mechanism is\nproposed to isolate task-specific features and leverage common properties among\ntasks. Our proposed framework, termed D2BNet, demonstrates a unique approach to\nparameter-efficient predictions for multi-task perception. In addition, as\ntasks benefit from co-training with each other, our solution achieves on par\nresults on partially labeled settings on nuScenes and outperforms previous\nworks for 3D detection and depth estimation on the Cityscapes dataset with full\nsupervision.\n", "rewritten_text": "Multi-task visual perception has a wide range of applications in scene understanding, such as autonomous driving. In this study, we have developed an efficient unified framework to address multiple common perception tasks, including instance segmentation, semantic segmentation, monocular 3D detection, and depth estimation. Simply sharing the same visual feature representations for these tasks can hinder performance, while using independent task-specific feature extractors can result in parameter redundancy and latency. To overcome these challenges, we have designed two feature-merge branches to learn feature bases that can be shared by multiple perception tasks. Each task then utilizes the corresponding feature base as input for the prediction task head to accomplish its specific goal.\n\nSpecifically, one feature merge branch is tailored for instance-level recognition, while the other is focused on dense predictions. To facilitate communication between the branches, the instance branch conveys pixel-wise spatial information of each instance to the dense branch through efficient dynamic convolution weighting. Additionally, we have introduced a simple yet effective dynamic routing mechanism to segregate task-specific features and capitalize on common properties across tasks. Our proposed framework, named D2BNet, presents a novel approach to achieving parameter-efficient predictions for multi-task perception.\n\nFurthermore, through co-training with each other, the tasks in our framework demonstrate comparable results in partially labeled settings on nuScenes and outperform previous works in 3D detection and depth estimation on the Cityscapes dataset under full supervision."}, "2107.00977": {"original_text": "  Cardiac ultrasound imaging is used to diagnose various heart diseases. Common\nanalysis pipelines involve manual processing of the video frames by expert\nclinicians. This suffers from intra- and inter-observer variability. We propose\na novel approach to ultrasound video analysis using a transformer architecture\nbased on a Residual Auto-Encoder Network and a BERT model adapted for token\nclassification. This enables videos of any length to be processed. We apply our\nmodel to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection\nand the automated computation of the left ventricular ejection fraction. We\nachieve an average frame distance of 3.36 frames for the ES and 7.17 frames for\nthe ED on videos of arbitrary length. Our end-to-end learnable approach can\nestimate the ejection fraction with a MAE of 5.95 and $R^2$ of 0.52 in 0.15s\nper video, showing that segmentation is not the only way to predict ejection\nfraction. Code and models are available at https://github.com/HReynaud/UVT.\n", "rewritten_text": "Cardiac ultrasound imaging is utilized for diagnosing various heart diseases. Typically, the analysis process involves manual processing of video frames by expert clinicians, which can lead to inconsistencies between different observers. To address this issue, we present a new method for ultrasound video analysis that employs a transformer architecture incorporating a Residual Auto-Encoder Network and a BERT model customized for token classification. This innovative approach allows for the processing of videos of any length.\n\nOur model is specifically applied to the identification of End-Systolic (ES) and End-Diastolic (ED) frames, as well as the automated calculation of the left ventricular ejection fraction. Remarkably, we achieve an average frame distance of 3.36 frames for ES and 7.17 frames for ED in videos of varying durations. Through our end-to-end learnable technique, we can estimate the ejection fraction with a Mean Absolute Error (MAE) of 5.95 and an $R^2$ value of 0.52 within a mere 0.15 seconds per video. This demonstrates that segmentation is not the sole method for predicting ejection fraction.\n\nFor those interested, the code and models are accessible at https://github.com/HReynaud/UVT."}, "1710.05994": {"original_text": "  Recent advancements in neutron and X-ray sources, instrumentation and data\ncollection modes have significantly increased the experimental data size (which\ncould easily contain 10$^{8}$ -- 10$^{10}$ data points), so that conventional\nvolumetric visualization approaches become inefficient for both still imaging\nand interactive OpenGL rendition in a 3D setting. We introduce a new approach\nbased on the unsupervised machine learning algorithm, Density-Based Spatial\nClustering of Applications with Noise (DBSCAN), to efficiently analyze and\nvisualize large volumetric datasets. Here we present two examples of analyzing\nand visualizing datasets from the diffuse scattering experiment of a single\ncrystal sample and the tomographic reconstruction of a neutron scanning of a\nturbine blade. We found that by using the intensity as the weighting factor in\nthe clustering process, DBSCAN becomes very effective in de-noising and\nfeature/boundary detection, and thus enables better visualization of the\nhierarchical internal structures of the neutron scattering data.\n", "rewritten_text": "Recent advancements in neutron and X-ray sources, instrumentation, and data collection modes have significantly increased the size of experimental data, which can now easily contain 10$^{8}$ to 10$^{10}$ data points. As a result, conventional volumetric visualization approaches have become inefficient for both still imaging and interactive OpenGL rendition in a 3D setting. To address this challenge, we propose a novel approach based on the unsupervised machine learning algorithm, Density-Based Spatial Clustering of Applications with Noise (DBSCAN), to efficiently analyze and visualize large volumetric datasets.\n\nIn this study, we demonstrate the effectiveness of our approach by presenting two examples of analyzing and visualizing datasets. The first example involves the analysis of data from a diffuse scattering experiment on a single crystal sample, while the second example focuses on the tomographic reconstruction of a neutron scanning of a turbine blade. Our results show that by incorporating intensity as a weighting factor in the clustering process, DBSCAN proves to be highly effective in de-noising and detecting features/boundaries. This, in turn, enables improved visualization of the hierarchical internal structures present in the neutron scattering data."}, "1812.0081": {"original_text": "  It is well known that the generative adversarial nets (GANs) are remarkably\ndifficult to train. The recently proposed Wasserstein GAN (WGAN) creates\nprincipled research directions towards addressing these issues. But we found in\npractice that gradient penalty WGANs (GP-WGANs) still suffer from training\ninstability. In this paper, we combine a Total Variational (TV) regularizing\nterm into the WGAN formulation instead of weight clipping or gradient penalty,\nwhich implies that the Lipschitz constraint is enforced on the critic network.\nOur proposed method is more stable at training than GP-WGANs and works well\nacross varied GAN architectures. We also present a method to control the\ntrade-off between image diversity and visual quality. It does not bring any\ncomputation burden.\n", "rewritten_text": "It is widely acknowledged that training generative adversarial networks (GANs) can be quite challenging. The introduction of Wasserstein GAN (WGAN) has provided a structured approach to tackle these difficulties. However, our practical experience reveals that gradient penalty WGANs (GP-WGANs) still encounter issues with training stability. In this study, we incorporate a Total Variational (TV) regularization term into the WGAN framework instead of relying on weight clipping or gradient penalty. This modification ensures the enforcement of the Lipschitz constraint on the critic network. Our proposed method demonstrates improved training stability compared to GP-WGANs and performs effectively across various GAN architectures. Additionally, we introduce a technique to manage the balance between image diversity and visual quality without adding any computational overhead."}, "1709.06247": {"original_text": "  Most of convolutional neural networks share the same characteristic: each\nconvolutional layer is followed by a nonlinear activation layer where Rectified\nLinear Unit (ReLU) is the most widely used. In this paper, we argue that the\ndesigned structure with the equal ratio between these two layers may not be the\nbest choice since it could result in the poor generalization ability. Thus, we\ntry to investigate a more suitable method on using ReLU to explore the better\nnetwork architectures. Specifically, we propose a proportional module to keep\nthe ratio between convolution and ReLU amount to be N:M (N>M). The proportional\nmodule can be applied in almost all networks with no extra computational cost\nto improve the performance. Comprehensive experimental results indicate that\nthe proposed method achieves better performance on different benchmarks with\ndifferent network architectures, thus verify the superiority of our work.\n", "rewritten_text": "Most convolutional neural networks share a common characteristic: each convolutional layer is typically followed by a nonlinear activation layer, with Rectified Linear Unit (ReLU) being the most commonly used. This paper argues that a design structure with an equal ratio between these two layers may not be optimal, potentially leading to poor generalization ability. Therefore, we aim to explore a more suitable method for utilizing ReLU to enhance network architectures. Specifically, we introduce a proportional module that maintains a ratio of N:M (where N>M) between the convolution and ReLU layers. This proportional module can be seamlessly integrated into various networks without incurring additional computational costs, thereby enhancing performance. Extensive experimental results demonstrate that our proposed method outperforms existing approaches across different benchmarks and network architectures, underscoring the effectiveness of our work."}, "2201.06674": {"original_text": "  Providing feedback on the argumentation of the learner is essential for\ndeveloping critical thinking skills, however, it requires a lot of time and\neffort. To mitigate the overload on teachers, we aim to automate a process of\nproviding feedback, especially giving diagnostic comments which point out the\nweaknesses inherent in the argumentation. It is recommended to give specific\ndiagnostic comments so that learners can recognize the diagnosis without\nmisinterpretation. However, it is not obvious how the task of providing\nspecific diagnostic comments should be formulated. We present a formulation of\nthe task as template selection and slot filling to make an automatic evaluation\neasier and the behavior of the model more tractable. The key to the formulation\nis the possibility of creating a template set that is sufficient for practical\nuse. In this paper, we define three criteria that a template set should\nsatisfy: expressiveness, informativeness, and uniqueness, and verify the\nfeasibility of creating a template set that satisfies these criteria as a first\ntrial. We will show that it is feasible through an annotation study that\nconverts diagnostic comments given in a text to a template format. The corpus\nused in the annotation study is publicly available.\n", "rewritten_text": "Providing feedback on a learner's argumentation is crucial for developing critical thinking skills. However, this process demands significant time and effort from teachers. In order to alleviate this burden, our goal is to automate the feedback process, particularly by offering diagnostic comments that highlight the inherent weaknesses in the argumentation. It is advisable to provide precise diagnostic comments to ensure that learners can accurately interpret the feedback. Yet, determining how to formulate these specific diagnostic comments is not straightforward.\n\nWe propose a method of framing this task as template selection and slot filling to streamline automatic evaluation and enhance the model's effectiveness. The key aspect of this approach lies in the ability to establish a template set that is practical for use. In this paper, we outline three criteria that a template set must meet: expressiveness, informativeness, and uniqueness. We then assess the feasibility of creating a template set that fulfills these criteria through an initial trial.\n\nOur validation involves an annotation study that transforms diagnostic comments from text into a template format. The corpus utilized in this study is publicly accessible."}, "2008.01167": {"original_text": "  Current anchor-free object detectors label all the features that spatially\nfall inside a predefined central region of a ground-truth box as positive. This\napproach causes label noise during training, since some of these positively\nlabeled features may be on the background or an occluder object, or they are\nsimply not discriminative features. In this paper, we propose a new labeling\nstrategy aimed to reduce the label noise in anchor-free detectors. We sum-pool\npredictions stemming from individual features into a single prediction. This\nallows the model to reduce the contributions of non-discriminatory features\nduring training. We develop a new one-stage, anchor-free object detector,\nPPDet, to employ this labeling strategy during training and a similar\nprediction pooling method during inference. On the COCO dataset, PPDet achieves\nthe best performance among anchor-free top-down detectors and performs on-par\nwith the other state-of-the-art methods. It also outperforms all major\none-stage and two-stage methods in small object detection (${AP}_{S}$ $31.4$).\nCode is available at https://github.com/nerminsamet/ppdet\n", "rewritten_text": "The current anchor-free object detectors label all features that spatially fall inside a predefined central region of a ground-truth box as positive. This approach leads to label noise during training, as some of these positively labeled features may belong to the background or an occluder object, or they may simply not be discriminative features. In this paper, we propose a new labeling strategy aimed at reducing label noise in anchor-free detectors. Our approach involves sum-pooling predictions from individual features into a single prediction. This enables the model to diminish the influence of non-discriminatory features during training. We introduce a new one-stage, anchor-free object detector called PPDet, which incorporates this labeling strategy during training and a similar prediction pooling method during inference. On the COCO dataset, PPDet achieves the highest performance among anchor-free top-down detectors and is on par with other state-of-the-art methods. It also outperforms all major one-stage and two-stage methods in small object detection (AP_S 31.4). The code for PPDet is available at https://github.com/nerminsamet/ppdet."}, "1807.10657": {"original_text": "  Saliency map estimation in computer vision aims to estimate the locations\nwhere people gaze in images. Since people tend to look at objects in images,\nthe parameters of the model pretrained on ImageNet for image classification are\nuseful for the saliency map estimation. However, there is no research on the\nrelationship between the image classification accuracy and the performance of\nthe saliency map estimation. In this paper, it is shown that there is a strong\ncorrelation between image classification accuracy and saliency map estimation\naccuracy. We also investigated the effective architecture based on multi scale\nimages and the upsampling layers to refine the saliency-map resolution. Our\nmodel achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003\ndatasets. In the MIT Saliency Benchmark, our model achieved the best\nperformance in some metrics and competitive results in the other metrics.\n", "rewritten_text": "The estimation of saliency maps in computer vision aims to identify the areas in images where people typically focus their gaze. As individuals tend to direct their attention towards objects within images, leveraging the parameters of a model pretrained on ImageNet for image classification proves beneficial for saliency map estimation. Despite this, there has been a lack of research exploring the connection between image classification accuracy and the effectiveness of saliency map estimation. This paper reveals a significant correlation between the accuracy of image classification and the precision of saliency map estimation. Additionally, an investigation into an efficient architecture utilizing multi-scale images and upsampling layers to enhance the resolution of saliency maps was conducted. The model developed in this study achieved state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003 datasets. Notably, in the MIT Saliency Benchmark, our model outperformed others in certain metrics and delivered competitive results in others."}, "1812.05642": {"original_text": "  Unsupervised learning for geometric perception (depth, optical flow, etc.) is\nof great interest to autonomous systems. Recent works on unsupervised learning\nhave made considerable progress on perceiving geometry; however, they usually\nignore the coherence of objects and perform poorly under scenarios with dark\nand noisy environments. In contrast, supervised learning algorithms, which are\nrobust, require large labeled geometric dataset. This paper introduces SIGNet,\na novel framework that provides robust geometry perception without requiring\ngeometrically informative labels. Specifically, SIGNet integrates semantic\ninformation to make depth and flow predictions consistent with objects and\nrobust to low lighting conditions. SIGNet is shown to improve upon the\nstate-of-the-art unsupervised learning for depth prediction by 30% (in squared\nrelative error). In particular, SIGNet improves the dynamic object class\nperformance by 39% in depth prediction and 29% in flow prediction. Our code\nwill be made available at https://github.com/mengyuest/SIGNet\n", "rewritten_text": "Unsupervised learning for geometric perception, including depth and optical flow, is a topic of significant interest for autonomous systems. While recent advancements in unsupervised learning have shown progress in perceiving geometry, they often overlook object coherence and struggle in dark and noisy environments. On the other hand, supervised learning algorithms, though robust, rely on large labeled geometric datasets.\n\nThis paper introduces SIGNet, a novel framework that enhances geometry perception without the need for geometrically informative labels. SIGNet leverages semantic information to ensure that depth and flow predictions align with objects and remain reliable even in low-light conditions. SIGNet demonstrates a 30% improvement over the current state-of-the-art unsupervised learning methods for depth prediction, with a notable 39% enhancement in dynamic object class performance and a 29% improvement in flow prediction.\n\nFor those interested, our code will be accessible at https://github.com/mengyuest/SIGNet."}, "2212.09098": {"original_text": "  Fine-grained semantic segmentation of a person's face and head, including\nfacial parts and head components, has progressed a great deal in recent years.\nHowever, it remains a challenging task, whereby considering ambiguous\nocclusions and large pose variations are particularly difficult. To overcome\nthese difficulties, we propose a novel framework termed Mask-FPAN. It uses a\nde-occlusion module that learns to parse occluded faces in a semi-supervised\nway. In particular, face landmark localization, face occlusionstimations, and\ndetected head poses are taken into account. A 3D morphable face model combined\nwith the UV GAN improves the robustness of 2D face parsing. In addition, we\nintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for face\nparing work. The proposed Mask-FPAN framework addresses the face parsing\nproblem in the wild and shows significant performance improvements with MIOU\nfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging face\ndatasets.\n", "rewritten_text": "Significant progress has been made in fine-grained semantic segmentation of a person's face and head, encompassing facial parts and head components in recent years. However, this task remains challenging, especially when dealing with ambiguous occlusions and large pose variations. To address these challenges, we present a novel framework called Mask-FPAN. This framework incorporates a de-occlusion module that learns to parse occluded faces in a semi-supervised manner. It considers face landmark localization, face occlusion estimations, and detected head poses. By combining a 3D morphable face model with the UV GAN, the robustness of 2D face parsing is enhanced. Additionally, we introduce two new datasets, FaceOccMask-HQ and CelebAMaskOcc-HQ, for face parsing tasks. The proposed Mask-FPAN framework tackles the face parsing issue in real-world scenarios and demonstrates significant performance enhancements, increasing the MIOU from 0.7353 to 0.9013 compared to the current state-of-the-art on challenging face datasets."}, "2206.08347": {"original_text": "  By leveraging contrastive learning, clustering, and other pretext tasks,\nunsupervised methods for learning image representations have reached impressive\nresults on standard benchmarks. The result has been a crowded field - many\nmethods with substantially different implementations yield results that seem\nnearly identical on popular benchmarks, such as linear evaluation on ImageNet.\nHowever, a single result does not tell the whole story. In this paper, we\ncompare methods using performance-based benchmarks such as linear evaluation,\nnearest neighbor classification, and clustering for several different datasets,\ndemonstrating the lack of a clear front-runner within the current\nstate-of-the-art. In contrast to prior work that performs only supervised vs.\nunsupervised comparison, we compare several different unsupervised methods\nagainst each other. To enrich this comparison, we analyze embeddings with\nmeasurements such as uniformity, tolerance, and centered kernel alignment\n(CKA), and propose two new metrics of our own: nearest neighbor graph\nsimilarity and linear prediction overlap. We reveal through our analysis that\nin isolation, single popular methods should not be treated as though they\nrepresent the field as a whole, and that future work ought to consider how to\nleverage the complimentary nature of these methods. We also leverage CKA to\nprovide a framework to robustly quantify augmentation invariance, and provide a\nreminder that certain types of invariance will be undesirable for downstream\ntasks.\n", "rewritten_text": "By utilizing contrastive learning, clustering, and other pretext tasks, unsupervised methods for learning image representations have achieved impressive results on standard benchmarks. This has led to a proliferation of methods with significantly different implementations producing results that appear almost identical on popular benchmarks, such as linear evaluation on ImageNet. However, a single result does not provide a comprehensive picture. In this paper, we compare methods using performance-based benchmarks like linear evaluation, nearest neighbor classification, and clustering across various datasets, highlighting the absence of a clear front-runner within the current state-of-the-art.\n\nIn contrast to previous studies that solely compare supervised versus unsupervised methods, we conduct a comparative analysis of multiple unsupervised methods. To enhance this comparison, we evaluate embeddings using metrics such as uniformity, tolerance, and centered kernel alignment (CKA), and introduce two new metrics: nearest neighbor graph similarity and linear prediction overlap. Our analysis reveals that individual popular methods should not be considered representative of the entire field in isolation, emphasizing the importance of exploring the complementary nature of these methods in future research.\n\nFurthermore, we leverage CKA to establish a robust framework for quantifying augmentation invariance and caution against certain types of invariance that may be detrimental for downstream tasks."}, "2108.08432": {"original_text": "  Deep learning has achieved remarkable success in medicalimage segmentation,\nbut it usually requires a large numberof images labeled with fine-grained\nsegmentation masks, andthe annotation of these masks can be very expensive\nandtime-consuming. Therefore, recent methods try to use un-supervised domain\nadaptation (UDA) methods to borrow in-formation from labeled data from other\ndatasets (source do-mains) to a new dataset (target domain). However, due tothe\nabsence of labels in the target domain, the performance ofUDA methods is much\nworse than that of the fully supervisedmethod. In this paper, we propose a\nweakly supervised do-main adaptation setting, in which we can partially label\nnewdatasets with bounding boxes, which are easier and cheaperto obtain than\nsegmentation masks. Accordingly, we proposea new weakly-supervised domain\nadaptation method calledBox-Adapt, which fully explores the fine-grained\nsegmenta-tion mask in the source domain and the weak bounding boxin the target\ndomain. Our Box-Adapt is a two-stage methodthat first performs joint training\non the source and target do-mains, and then conducts self-training with the\npseudo-labelsof the target domain. We demonstrate the effectiveness of\nourmethod in the liver segmentation task. Weakly supervised do-main adaptation\n", "rewritten_text": "Deep learning has made significant strides in medical image segmentation, but it typically necessitates a substantial number of images annotated with detailed segmentation masks. The process of annotating these masks can be both costly and time-consuming. Consequently, recent approaches have turned to unsupervised domain adaptation (UDA) techniques to leverage information from labeled data in other datasets (source domains) for application in a new dataset (target domain). However, the lack of labels in the target domain results in UDA methods performing notably poorer compared to fully supervised methods.\n\nIn this study, we introduce a weakly supervised domain adaptation framework where new datasets can be partially labeled with bounding boxes, which are easier and more cost-effective to obtain than segmentation masks. To this end, we propose a novel weakly supervised domain adaptation method named Box-Adapt. This method fully exploits the fine-grained segmentation masks in the source domain and the weak bounding boxes in the target domain. Box-Adapt operates in two stages: initially engaging in joint training on both the source and target domains, followed by self-training using the pseudo-labels from the target domain.\n\nWe showcase the efficacy of our approach in the context of liver segmentation tasks, demonstrating the potential of weakly supervised domain adaptation."}, "2203.02231": {"original_text": "  Light field disparity estimation is an essential task in computer vision with\nvarious applications. Although supervised learning-based methods have achieved\nboth higher accuracy and efficiency than traditional optimization-based\nmethods, the dependency on ground-truth disparity for training limits the\noverall generalization performance not to say for real-world scenarios where\nthe ground-truth disparity is hard to capture. In this paper, we argue that\nunsupervised methods can achieve comparable accuracy, but, more importantly,\nmuch higher generalization capacity and efficiency than supervised methods.\nSpecifically, we present the Occlusion Pattern Aware Loss, named OPAL, which\nsuccessfully extracts and encodes the general occlusion patterns inherent in\nthe light field for loss calculation. OPAL enables: i) accurate and robust\nestimation by effectively handling occlusions without using any ground-truth\ninformation for training and ii) much efficient performance by significantly\nreducing the network parameters required for accurate inference. Besides, a\ntransformer-based network and a refinement module are proposed for achieving\neven more accurate results. Extensive experiments demonstrate our method not\nonly significantly improves the accuracy compared with the SOTA unsupervised\nmethods, but also possesses strong generalization capacity, even for real-world\ndata, compared with supervised methods. Our code will be made publicly\navailable.\n", "rewritten_text": "Estimating light field disparity is a crucial task in computer vision with a wide range of applications. While supervised learning methods have shown superior accuracy and efficiency compared to traditional optimization-based approaches, their reliance on ground-truth disparity data for training hampers overall generalization performance, especially in real-world scenarios where capturing ground-truth disparity is challenging. \n\nIn this study, we propose that unsupervised methods can achieve comparable accuracy, along with significantly higher generalization capacity and efficiency than supervised methods. We introduce the Occlusion Pattern Aware Loss (OPAL), a novel approach that effectively captures and encodes occlusion patterns inherent in light fields for loss calculation. OPAL facilitates accurate and robust disparity estimation by handling occlusions without the need for ground-truth information during training, leading to improved efficiency by reducing the required network parameters for accurate inference. Additionally, we present a transformer-based network and a refinement module to further enhance accuracy.\n\nExtensive experiments demonstrate that our method not only enhances accuracy significantly compared to state-of-the-art unsupervised methods but also exhibits strong generalization capacity, even with real-world data, surpassing supervised methods. We plan to release our code publicly for further research and application."}, "2006.1038": {"original_text": "  Video semantic segmentation is active in recent years benefited from the\ngreat progress of image semantic segmentation. For such a task, the per-frame\nimage segmentation is generally unacceptable in practice due to high\ncomputation cost. To tackle this issue, many works use the flow-based feature\npropagation to reuse the features of previous frames. However, the optical flow\nestimation inevitably suffers inaccuracy and then causes the propagated\nfeatures distorted. In this paper, we propose distortion-aware feature\ncorrection to alleviate the issue, which improves video segmentation\nperformance by correcting distorted propagated features. To be specific, we\nfirstly propose to transfer distortion patterns from feature into image space\nand conduct effective distortion map prediction. Benefited from the guidance of\ndistortion maps, we proposed Feature Correction Module (FCM) to rectify\npropagated features in the distorted areas. Our proposed method can\nsignificantly boost the accuracy of video semantic segmentation at a low price.\nThe extensive experimental results on Cityscapes and CamVid show that our\nmethod outperforms the recent state-of-the-art methods.\n", "rewritten_text": "In recent years, video semantic segmentation has seen significant advancements, largely due to the progress made in image semantic segmentation. However, performing per-frame image segmentation for this task is often impractical due to the high computational cost involved. To address this challenge, many studies have turned to flow-based feature propagation to recycle features from previous frames. Nevertheless, the accuracy of optical flow estimation can be compromised, leading to distorted propagated features.\n\nIn this paper, we introduce a distortion-aware feature correction approach to mitigate this issue, enhancing video segmentation performance by rectifying distorted propagated features. Specifically, we propose transferring distortion patterns from features to image space and predicting effective distortion maps. Leveraging these distortion maps, we introduce the Feature Correction Module (FCM) to rectify features in distorted regions. Our method offers a cost-effective means to significantly improve the accuracy of video semantic segmentation.\n\nExtensive experiments conducted on Cityscapes and CamVid datasets demonstrate that our approach surpasses recent state-of-the-art methods in terms of performance."}, "2101.11251": {"original_text": "  Junctions reflect the important geometrical structure information of the\nimage, and are of primary significance to applications such as image matching\nand motion analysis. Previous event-based feature extraction methods are mainly\nfocused on corners, which mainly find their locations, however, ignoring the\ngeometrical structure information like orientations and scales of edges. This\npaper adapts the frame-based a-contrario junction detector(ACJ) to event data,\nproposing the event-based a-contrario junction detector(e-ACJ), which yields\njunctions' locations while giving the scales and orientations of their\nbranches. The proposed method relies on an a-contrario model and can operate on\nasynchronous events directly without generating synthesized event frames. We\nevaluate the performance on public event datasets. The result shows our method\nsuccessfully finds the orientations and scales of branches, while maintaining\nhigh accuracy in junction's location.\n", "rewritten_text": "Junctions play a crucial role in conveying important geometric information within an image, making them essential for applications such as image matching and motion analysis. While previous feature extraction methods have primarily focused on identifying corners and their locations, they often overlook key geometric details such as edge orientations and scales. This paper introduces a novel approach by adapting the frame-based a-contrario junction detector (ACJ) to event data, resulting in the event-based a-contrario junction detector (e-ACJ). The e-ACJ not only identifies junction locations but also provides information on the scales and orientations of their branches. By leveraging an a-contrario model, this method can directly process asynchronous events without the need for generating synthetic event frames. Performance evaluation on public event datasets demonstrates that our approach successfully captures branch orientations and scales while maintaining high accuracy in junction location detection."}, "2210.16239": {"original_text": "  The Hyperspectral image (HSI) contains several hundred bands of the same\nregion called the Ground Truth (GT). The bands are taken in juxtaposed\nfrequencies, but some of them are noisily measured or contain no information.\nFor the classification, the selection of bands, affects significantly the\nresults of classification, in fact, using a subset of relevant bands, these\nresults can be better than those obtained using all bands, from which the need\nto reduce the dimensionality of the HSI. In this paper, a categorization of\ndimensionality reduction methods, according to the generation process, is\npresented. Furthermore, we reproduce an algorithm based on mutual information\n(MI) to reduce dimensionality by features selection and we introduce an\nalgorithm using mutual information and homogeneity. The two schemas are a\nfilter strategy. Finally, to validate this, we consider the case study AVIRIS\nHSI 92AV3C.\n  Keywords: Hyperspectrale images; classification; features selection; mutual\ninformation; homogeneity\n", "rewritten_text": "The Hyperspectral image (HSI) comprises several hundred bands capturing the same region known as the Ground Truth (GT). These bands are captured at different frequencies, with some being affected by noise or lacking information. The selection of bands significantly impacts the classification results. Utilizing a subset of relevant bands can yield better results compared to using all bands, highlighting the importance of reducing the dimensionality of the HSI. This paper categorizes dimensionality reduction methods based on their generation process. Additionally, we present an algorithm that leverages mutual information (MI) for feature selection to reduce dimensionality, along with an algorithm that combines mutual information and homogeneity. Both approaches follow a filter strategy. To validate these methods, we examine the AVIRIS HSI 92AV3C case study.\n\nKeywords: Hyperspectral images; classification; feature selection; mutual information; homogeneity"}, "2108.07127": {"original_text": "  We translate a closed text that is known in advance and available in many\nlanguages into a new and severely low resource language. Most human translation\nefforts adopt a portion-based approach to translate consecutive pages/chapters\nin order, which may not suit machine translation. We compare the portion-based\napproach that optimizes coherence of the text locally with the random sampling\napproach that increases coverage of the text globally. Our results show that\nthe random sampling approach performs better. When training on a seed corpus of\n~1,000 lines from the Bible and testing on the rest of the Bible (~30,000\nlines), random sampling gives a performance gain of +11.0 BLEU using English as\na simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a\nMayan language. Furthermore, we compare three ways of updating machine\ntranslation models with increasing amount of human post-edited data through\niterations. We find that adding newly post-edited data to training after\nvocabulary update without self-supervision performs the best. We propose an\nalgorithm for human and machine to work together seamlessly to translate a\nclosed text into a severely low resource language.\n", "rewritten_text": "We translate a predetermined closed text, available in multiple languages, into a new and severely low-resource language. While most human translation efforts typically follow a portion-based approach to sequentially translate pages or chapters, this method may not be ideal for machine translation. In our study, we compare the portion-based approach, which focuses on local coherence optimization, with the random sampling approach, which aims to enhance global text coverage. Our findings indicate that the random sampling approach yields superior results.\n\nIn our experiments, training on a seed corpus of approximately 1,000 lines from the Bible and testing on the remaining ~30,000 lines, the random sampling method demonstrates a performance improvement of +11.0 BLEU when using English as a simulated low-resource language, and +4.9 BLEU when using Eastern Pokomchi, a Mayan language.\n\nAdditionally, we explore three methods for updating machine translation models with increasing amounts of human post-edited data through iterative processes. Our analysis reveals that incorporating newly post-edited data into training after vocabulary updates, without self-supervision, produces the best outcomes.\n\nWe propose an algorithm that facilitates seamless collaboration between humans and machines to translate a closed text into a severely low-resource language."}, "2103.10978": {"original_text": "  This paper addresses the problem of 3D human body shape and pose estimation\nfrom RGB images. Recent progress in this field has focused on single images,\nvideo or multi-view images as inputs. In contrast, we propose a new task: shape\nand pose estimation from a group of multiple images of a human subject, without\nconstraints on subject pose, camera viewpoint or background conditions between\nimages in the group. Our solution to this task predicts distributions over SMPL\nbody shape and pose parameters conditioned on the input images in the group. We\nprobabilistically combine predicted body shape distributions from each image to\nobtain a final multi-image shape prediction. We show that the additional body\nshape information present in multi-image input groups improves 3D human shape\nestimation metrics compared to single-image inputs on the SSP-3D dataset and a\nprivate dataset of tape-measured humans. In addition, predicting distributions\nover 3D bodies allows us to quantify pose prediction uncertainty, which is\nuseful when faced with challenging input images with significant occlusion. Our\nmethod demonstrates meaningful pose uncertainty on the 3DPW dataset and is\ncompetitive with the state-of-the-art in terms of pose estimation metrics.\n", "rewritten_text": "This paper explores the challenge of estimating 3D human body shape and pose from RGB images. While recent advancements in this area have concentrated on using single images, videos, or multi-view images as inputs, we introduce a novel approach: estimating shape and pose from a collection of multiple images of a human subject, without imposing constraints on subject pose, camera viewpoint, or background conditions across the images in the group. Our method for this task involves predicting distributions over SMPL body shape and pose parameters based on the input images in the group. By probabilistically combining the predicted body shape distributions from each image, we derive a final multi-image shape prediction. Our results demonstrate that leveraging additional body shape information from multi-image input groups enhances 3D human shape estimation metrics compared to single-image inputs on both the SSP-3D dataset and a private dataset of tape-measured humans. Furthermore, by predicting distributions over 3D bodies, we are able to quantify pose prediction uncertainty, which proves valuable when dealing with challenging input images featuring significant occlusion. Our approach showcases meaningful pose uncertainty on the 3DPW dataset and performs competitively with the state-of-the-art in terms of pose estimation metrics."}, "2307.01447": {"original_text": "  Accurately matching local features between a pair of images is a challenging\ncomputer vision task. Previous studies typically use attention based graph\nneural networks (GNNs) with fully-connected graphs over keypoints within/across\nimages for visual and geometric information reasoning. However, in the context\nof feature matching, considerable keypoints are non-repeatable due to occlusion\nand failure of the detector, and thus irrelevant for message passing. The\nconnectivity with non-repeatable keypoints not only introduces redundancy,\nresulting in limited efficiency, but also interferes with the representation\naggregation process, leading to limited accuracy. Targeting towards high\naccuracy and efficiency, we propose MaKeGNN, a sparse attention-based GNN\narchitecture which bypasses non-repeatable keypoints and leverages matchable\nones to guide compact and meaningful message passing. More specifically, our\nBilateral Context-Aware Sampling Module first dynamically samples two small\nsets of well-distributed keypoints with high matchability scores from the image\npair. Then, our Matchable Keypoint-Assisted Context Aggregation Module regards\nsampled informative keypoints as message bottlenecks and thus constrains each\nkeypoint only to retrieve favorable contextual information from intra- and\ninter- matchable keypoints, evading the interference of irrelevant and\nredundant connectivity with non-repeatable ones. Furthermore, considering the\npotential noise in initial keypoints and sampled matchable ones, the MKACA\nmodule adopts a matchability-guided attentional aggregation operation for purer\ndata-dependent context propagation. By these means, we achieve the\nstate-of-the-art performance on relative camera estimation, fundamental matrix\nestimation, and visual localization, while significantly reducing computational\nand memory complexity compared to typical attentional GNNs.\n", "rewritten_text": "Accurately matching local features between a pair of images poses a significant challenge in computer vision. Previous studies have commonly employed attention-based graph neural networks (GNNs) with fully-connected graphs that encompass keypoints within and across images for visual and geometric information reasoning. However, in the context of feature matching, many keypoints are non-repeatable due to occlusion and detector failures, rendering them irrelevant for effective message passing. The inclusion of non-repeatable keypoints not only introduces redundancy, limiting efficiency, but also disrupts the representation aggregation process, resulting in reduced accuracy.\n\nTo address the need for both high accuracy and efficiency, we introduce MaKeGNN, a sparse attention-based GNN architecture that circumvents non-repeatable keypoints and utilizes matchable ones to facilitate concise and meaningful message passing. Specifically, our Bilateral Context-Aware Sampling Module dynamically selects two small sets of well-distributed keypoints with high matchability scores from the image pair. Subsequently, our Matchable Keypoint-Assisted Context Aggregation Module treats the sampled informative keypoints as message bottlenecks, allowing each keypoint to selectively gather relevant contextual information from intra- and inter-matchable keypoints, thereby avoiding the interference of irrelevant and redundant connections with non-repeatable ones.\n\nMoreover, to account for potential noise in the initial keypoints and sampled matchable ones, the MKACA module employs a matchability-guided attentional aggregation operation to ensure purer data-dependent context propagation. Through these strategies, we achieve state-of-the-art performance in relative camera estimation, fundamental matrix estimation, and visual localization, while significantly reducing computational and memory complexity compared to conventional attentional GNNs."}, "1703.00087": {"original_text": "  Lesion segmentation is the first step in most automatic melanoma recognition\nsystems. Deficiencies and difficulties in dermoscopic images such as color\ninconstancy, hair occlusion, dark corners and color charts make lesion\nsegmentation an intricate task. In order to detect the lesion in the presence\nof these problems, we propose a supervised saliency detection method tailored\nfor dermoscopic images based on the discriminative regional feature integration\n(DRFI). DRFI method incorporates multi-level segmentation, regional contrast,\nproperty, background descriptors, and a random forest regressor to create\nsaliency scores for each region in the image. In our improved saliency\ndetection method, mDRFI, we have added some new features to regional property\ndescriptors. Also, in order to achieve more robust regional background\ndescriptors, a thresholding algorithm is proposed to obtain a new\npseudo-background region. Findings reveal that mDRFI is superior to DRFI in\ndetecting the lesion as the salient object in dermoscopic images. The proposed\noverall lesion segmentation framework uses detected saliency map to construct\nan initial mask of the lesion through thresholding and post-processing\noperations. The initial mask is then evolving in a level set framework to fit\nbetter on the lesion's boundaries. The results of evaluation tests on three\npublic datasets show that our proposed segmentation method outperforms the\nother conventional state-of-the-art segmentation algorithms and its performance\nis comparable with most recent approaches that are based on deep convolutional\nneural networks.\n", "rewritten_text": "Lesion segmentation serves as the initial step in the majority of automatic melanoma recognition systems. The complexities and challenges present in dermoscopic images, such as color inconsistencies, hair occlusions, dark corners, and color charts, render lesion segmentation a highly intricate task. To address the detection of lesions amidst these issues, we introduce a supervised saliency detection method specifically designed for dermoscopic images, utilizing the Discriminative Regional Feature Integration (DRFI) approach.\n\nThe DRFI method integrates multi-level segmentation, regional contrast, property descriptors, background descriptors, and a random forest regressor to generate saliency scores for each region within the image. Enhancing this saliency detection method, our modified version, mDRFI, incorporates additional features to the regional property descriptors. Furthermore, to enhance the robustness of regional background descriptors, we introduce a thresholding algorithm to define a new pseudo-background region.\n\nResults indicate that mDRFI surpasses DRFI in effectively identifying the lesion as the salient object in dermoscopic images. Our proposed lesion segmentation framework utilizes the detected saliency map to establish an initial lesion mask through thresholding and post-processing procedures. Subsequently, the initial mask undergoes evolution within a level set framework to better conform to the boundaries of the lesion.\n\nEvaluation tests conducted on three public datasets demonstrate that our proposed segmentation method outperforms conventional state-of-the-art segmentation algorithms and exhibits comparable performance to recent approaches based on deep convolutional neural networks."}, "2405.12209": {"original_text": "  Recent advancements in large language models (LLMs) have showcased\nsignificant improvements in mathematics. However, traditional math benchmarks\nlike GSM8k offer a unidimensional perspective, falling short in providing a\nholistic assessment of the LLMs' math capabilities. To address this gap, we\nintroduce MathBench, a new benchmark that rigorously assesses the mathematical\ncapabilities of large language models. MathBench spans a wide range of\nmathematical disciplines, offering a detailed evaluation of both theoretical\nunderstanding and practical problem-solving skills. The benchmark progresses\nthrough five distinct stages, from basic arithmetic to college mathematics, and\nis structured to evaluate models at various depths of knowledge. Each stage\nincludes theoretical questions and application problems, allowing us to measure\na model's mathematical proficiency and its ability to apply concepts in\npractical scenarios. MathBench aims to enhance the evaluation of LLMs'\nmathematical abilities, providing a nuanced view of their knowledge\nunderstanding levels and problem solving skills in a bilingual context. The\nproject is released at https://github.com/open-compass/MathBench .\n", "rewritten_text": "Recent advancements in large language models (LLMs) have demonstrated significant improvements in mathematics. However, traditional math benchmarks like GSM8k offer a one-dimensional perspective, lacking in providing a comprehensive assessment of the LLMs' mathematical capabilities. To bridge this gap, we present MathBench, a new benchmark that rigorously evaluates the mathematical abilities of large language models. MathBench covers a wide range of mathematical disciplines, offering a thorough evaluation of both theoretical comprehension and practical problem-solving skills. The benchmark progresses through five distinct stages, ranging from basic arithmetic to college-level mathematics, and is designed to assess models at varying levels of knowledge depth. Each stage comprises theoretical questions and application problems, enabling us to gauge a model's mathematical proficiency and its capacity to apply concepts in real-world scenarios. MathBench aims to enhance the evaluation of LLMs' mathematical aptitudes, providing a nuanced perspective on their levels of knowledge and problem-solving skills in a bilingual context. The project is available at https://github.com/open-compass/MathBench."}, "2407.03993": {"original_text": "  Natural language counterfactual generation aims to minimally modify a given\ntext such that the modified text will be classified into a different class. The\ngenerated counterfactuals provide insight into the reasoning behind a model's\npredictions by highlighting which words significantly influence the outcomes.\nAdditionally, they can be used to detect model fairness issues and augment the\ntraining data to enhance the model's robustness. A substantial amount of\nresearch has been conducted to generate counterfactuals for various NLP tasks,\nemploying different models and methodologies. With the rapid growth of studies\nin this field, a systematic review is crucial to guide future researchers and\ndevelopers. To bridge this gap, this survey provides a comprehensive overview\nof textual counterfactual generation methods, particularly those based on Large\nLanguage Models. We propose a new taxonomy that systematically categorizes the\ngeneration methods into four groups and summarizes the metrics for evaluating\nthe generation quality. Finally, we discuss ongoing research challenges and\noutline promising directions for future work.\n", "rewritten_text": "Natural language counterfactual generation aims to make minimal modifications to a given text in order to classify the modified text into a different class. The generated counterfactuals offer insights into the rationale behind a model's predictions by highlighting the words that significantly influence the outcomes. Moreover, they can help identify issues of fairness in the model and enhance the model's robustness by augmenting the training data. Extensive research has been conducted on generating counterfactuals for various NLP tasks, utilizing different models and methodologies. Given the rapid expansion of studies in this domain, a systematic review is essential to provide guidance for future researchers and developers. To address this need, this survey presents a comprehensive overview of methods for generating textual counterfactuals, particularly those based on Large Language Models. We introduce a new taxonomy that systematically categorizes the generation methods into four groups and summarizes the metrics used to evaluate the quality of generation. Finally, we discuss current research challenges and outline promising directions for future work."}, "1909.13411": {"original_text": "  Mesoscale eddies play a significant role in marine energy transport, marine\nbiological environment and marine climate. Due to their huge impact on the\nocean, mesoscale eddy detection has become a hot research area in recent years.\nTherefore, more and more people are entering the field of mesoscale eddy\ndetection. However, the existing detection methods mainly based on traditional\ndetection methods typically only use Sea Surface Height (SSH) as a variable to\ndetect, resulting in inaccurate performance. In this paper, we propose a\nmesoscale eddy detection method based on multivariate fusion data to solve this\nproblem. We not only use the SSH variable, but also add the two variables: Sea\nSurface Temperature (SST) and velocity of flow, achieving a multivariate\ninformation fusion input. We design a novel symmetric network, which merges\nlow-level feature maps from the downsampling pathway and high-level feature\nmaps from the upsampling pathway by lateral connection. In addition, we apply\ndilated convolutions to network structure to increase the receptive field and\nobtain more contextual information in the case of constant parameter. In the\nend, we demonstrate the effectiveness of our method on dataset provided by us,\nachieving the test set performance of 97.06% , greatly improved the performance\nof previous methods of mesoscale eddy detection.\n", "rewritten_text": "Mesoscale eddies play a significant role in the transport of marine energy, the marine biological environment, and marine climate. Their substantial impact on the ocean has made mesoscale eddy detection a prominent research area in recent years, attracting an increasing number of individuals to enter this field. However, current detection methods primarily rely on traditional approaches that typically utilize Sea Surface Height (SSH) as the sole variable for detection, leading to inaccuracies in performance.\n\nIn this study, we propose a novel mesoscale eddy detection method based on the fusion of multivariate data to address this issue. Our approach incorporates not only the SSH variable but also integrates two additional variables: Sea Surface Temperature (SST) and flow velocity, resulting in a multivariate information fusion input. We have developed a unique symmetric network that combines low-level feature maps from the downsampling pathway with high-level feature maps from the upsampling pathway through lateral connections.\n\nFurthermore, we have incorporated dilated convolutions into the network structure to enhance the receptive field and capture more contextual information while maintaining a constant parameter setting. Our method has been evaluated on a dataset provided by us, demonstrating a test set performance of 97.06%, significantly surpassing the performance of previous mesoscale eddy detection methods."}, "1808.07733": {"original_text": "  We analyze the performance of different sentiment classification models on\nsyntactically complex inputs like A-but-B sentences. The first contribution of\nthis analysis addresses reproducible research: to meaningfully compare\ndifferent models, their accuracies must be averaged over far more random seeds\nthan what has traditionally been reported. With proper averaging in place, we\nnotice that the distillation model described in arXiv:1603.06318v4 [cs.LG],\nwhich incorporates explicit logic rules for sentiment classification, is\nineffective. In contrast, using contextualized ELMo embeddings\n(arXiv:1802.05365v2 [cs.CL]) instead of logic rules yields significantly better\nperformance. Additionally, we provide analysis and visualizations that\ndemonstrate ELMo's ability to implicitly learn logic rules. Finally, a\ncrowdsourced analysis reveals how ELMo outperforms baseline models even on\nsentences with ambiguous sentiment labels.\n", "rewritten_text": "We conducted an analysis on the performance of various sentiment classification models when dealing with syntactically complex inputs, such as A-but-B sentences. The primary focus of this analysis is on reproducible research. To enable a meaningful comparison of different models, it is essential to average their accuracies over a significantly larger number of random seeds than what has traditionally been reported. Upon implementing proper averaging, we observed that the distillation model outlined in arXiv:1603.06318v4 [cs.LG], which integrates explicit logic rules for sentiment classification, proves to be ineffective. Conversely, utilizing contextualized ELMo embeddings (arXiv:1802.05365v2 [cs.CL]) in place of logic rules results in notably improved performance.\n\nFurthermore, we present an in-depth analysis and visual representations showcasing ELMo's capability to implicitly learn logic rules. Through a crowdsourced analysis, it becomes evident that ELMo surpasses baseline models even when dealing with sentences that have ambiguous sentiment labels."}, "2011.09634": {"original_text": "  In this paper, we teach machines to understand visuals and natural language\nby learning the mapping between sentences and noisy video snippets without\nexplicit annotations. Firstly, we define a self-supervised learning framework\nthat captures the cross-modal information. A novel adversarial learning module\nis then introduced to explicitly handle the noises in the natural videos, where\nthe subtitle sentences are not guaranteed to be strongly corresponded to the\nvideo snippets. For training and evaluation, we contribute a new dataset\n`ApartmenTour' that contains a large number of online videos and subtitles. We\ncarry out experiments on the bidirectional retrieval tasks between sentences\nand videos, and the results demonstrate that our proposed model achieves the\nstate-of-the-art performance on both retrieval tasks and exceeds several strong\nbaselines. The dataset can be downloaded at https://github.com/zyj-13/WAL.\n", "rewritten_text": "This paper presents a method for teaching machines to comprehend visuals and natural language by learning the relationship between sentences and noisy video snippets without explicit annotations. Initially, a self-supervised learning framework is defined to capture cross-modal information. A novel adversarial learning module is then introduced to address the noise present in natural videos, where the subtitle sentences may not strongly correspond to the video snippets. To facilitate training and evaluation, a new dataset called 'ApartmenTour' is introduced, which includes a large collection of online videos and subtitles. Experimental studies are conducted on bidirectional retrieval tasks involving sentences and videos, with results showing that our proposed model outperforms several strong baselines and achieves state-of-the-art performance on both retrieval tasks. The dataset is available for download at https://github.com/zyj-13/WAL."}, "1708.01654": {"original_text": "  We demonstrate the use of shape-from-shading (SfS) to improve both the\nquality and the robustness of 3D reconstruction of dynamic objects captured by\na single camera. Unlike previous approaches that made use of SfS as a\npost-processing step, we offer a principled integrated approach that solves\ndynamic object tracking and reconstruction and SfS as a single unified cost\nfunction. Moving beyond Lambertian S f S , we propose a general approach that\nmodels both specularities and shading while simultaneously tracking and\nreconstructing general dynamic objects. Solving these problems jointly prevents\nthe kinds of tracking failures which can not be recovered from by pipeline\napproaches. We show state-of-the-art results both qualitatively and\nquantitatively.\n", "rewritten_text": "We present a novel approach utilizing shape-from-shading (SfS) to enhance the quality and robustness of 3D reconstruction for dynamic objects captured by a single camera. In contrast to previous methods that treated SfS as a post-processing step, our integrated approach addresses dynamic object tracking, reconstruction, and SfS within a unified cost function framework. Going beyond traditional Lambertian SfS, our method introduces a comprehensive model that accounts for specularities and shading while concurrently tracking and reconstructing diverse dynamic objects. By jointly solving these challenges, we mitigate tracking failures that are irrecoverable in conventional pipeline approaches. Our results demonstrate cutting-edge performance both qualitatively and quantitatively."}, "1711.07721": {"original_text": "  Post capture refocusing effect in smartphone cameras is achievable by using\nfocal stacks. However, the accuracy of this effect is totally dependent on the\ncombination of the depth layers in the stack. The accuracy of the extended\ndepth of field effect in this application can be improved significantly by\ncomputing an accurate depth map which has been an open issue for decades. To\ntackle this issue, in this paper, a framework is proposed based on\nPreconditioned Alternating Direction Method of Multipliers (PADMM) for depth\nfrom the focal stack and synthetic defocus application. In addition to its\nability to provide high structural accuracy and occlusion handling, the\noptimization function of the proposed method can, in fact, converge faster and\nbetter than state of the art methods. The evaluation has been done on 21 sets\nof focal stacks and the optimization function has been compared against 5 other\nmethods. Preliminary results indicate that the proposed method has a better\nperformance in terms of structural accuracy and optimization in comparison to\nthe current state of the art methods.\n", "rewritten_text": "Achieving a post-capture refocusing effect in smartphone cameras is possible through the use of focal stacks. However, the accuracy of this effect relies entirely on the combination of depth layers within the stack. Enhancing the accuracy of the extended depth of field effect in this application can be significantly improved by generating a precise depth map, which has been a longstanding challenge. To address this issue, this paper introduces a framework based on the Preconditioned Alternating Direction Method of Multipliers (PADMM) for depth estimation from the focal stack and synthetic defocus application. Apart from its capability to deliver high structural accuracy and handle occlusions effectively, the optimization function of the proposed method demonstrates faster and superior convergence compared to existing methods. The evaluation was conducted on 21 sets of focal stacks, with the optimization function being benchmarked against five other methods. Preliminary results suggest that the proposed method outperforms current state-of-the-art methods in terms of structural accuracy and optimization efficiency."}, "2004.06376": {"original_text": "  Understanding the shape of a scene from a single color image is a formidable\ncomputer vision task. However, most methods aim to predict the geometry of\nsurfaces that are visible to the camera, which is of limited use when planning\npaths for robots or augmented reality agents. Such agents can only move when\ngrounded on a traversable surface, which we define as the set of classes which\nhumans can also walk over, such as grass, footpaths and pavement. Models which\npredict beyond the line of sight often parameterize the scene with voxels or\nmeshes, which can be expensive to use in machine learning frameworks.\n  We introduce a model to predict the geometry of both visible and occluded\ntraversable surfaces, given a single RGB image as input. We learn from stereo\nvideo sequences, using camera poses, per-frame depth and semantic segmentation\nto form training data, which is used to supervise an image-to-image network. We\ntrain models from the KITTI driving dataset, the indoor Matterport dataset, and\nfrom our own casually captured stereo footage. We find that a surprisingly low\nbar for spatial coverage of training scenes is required. We validate our\nalgorithm against a range of strong baselines, and include an assessment of our\npredictions for a path-planning task.\n", "rewritten_text": "Understanding the shape of a scene from a single color image presents a challenging computer vision task. Most existing methods focus on predicting the geometry of visible surfaces captured by the camera, which has limited utility for tasks such as robot path planning or augmented reality applications. These agents require a traversable surface to move, defined as surfaces that humans can walk on, such as grass, footpaths, and pavement. Models that extend predictions beyond the camera's line of sight often use voxel or mesh representations, which can be computationally expensive in machine learning frameworks.\n\nIn this study, we propose a model that predicts the geometry of both visible and occluded traversable surfaces based on a single RGB image input. Our approach leverages stereo video sequences, incorporating camera poses, per-frame depth information, and semantic segmentation to create training data for supervising an image-to-image network. We train our models on diverse datasets, including the KITTI driving dataset, the indoor Matterport dataset, and our own stereo footage captured informally. Surprisingly, we find that a relatively modest spatial coverage of training scenes is sufficient.\n\nTo evaluate the effectiveness of our algorithm, we compare it against several strong baselines and assess its performance in a path-planning scenario."}, "2107.08173": {"original_text": "  This ability to learn consecutive tasks without forgetting how to perform\npreviously trained problems is essential for developing an online dialogue\nsystem. This paper proposes an effective continual learning for the\ntask-oriented dialogue system with iterative network pruning, expanding and\nmasking (TPEM), which preserves performance on previously encountered tasks\nwhile accelerating learning progress on subsequent tasks. Specifically, TPEM\n(i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts\nnetwork expanding to create free weights for new tasks, and (iii) introduces\ntask-specific network masking to alleviate the negative impact of fixed weights\nof old tasks on new tasks. We conduct extensive experiments on seven different\ntasks from three benchmark datasets and show empirically that TPEM leads to\nsignificantly improved results over the strong competitors. For\nreproducibility, we submit the code and data at:\nhttps://github.com/siat-nlp/TPEM\n", "rewritten_text": "This paper introduces an effective approach to continual learning for task-oriented dialogue systems, called Iterative Network Pruning, Expanding, and Masking (TPEM). TPEM is crucial for developing an online dialogue system as it enables learning consecutive tasks without forgetting previously trained problems. The method preserves performance on tasks encountered earlier while accelerating progress on subsequent tasks. Specifically, TPEM leverages network pruning to retain knowledge of old tasks, adopts network expanding to allocate free weights for new tasks, and introduces task-specific network masking to mitigate the negative impact of fixed weights from old tasks on new tasks. Extensive experiments were conducted on seven tasks from three benchmark datasets, demonstrating empirically that TPEM outperforms strong competitors. For reproducibility, the code and data are available at: https://github.com/siat-nlp/TPEM"}, "1811.09789": {"original_text": "  There has been much recent work on image captioning models that describe the\nfactual aspects of an image. Recently, some models have incorporated\nnon-factual aspects into the captions, such as sentiment or style. However,\nsuch models typically have difficulty in balancing the semantic aspects of the\nimage and the non-factual dimensions of the caption; in addition, it can be\nobserved that humans may focus on different aspects of an image depending on\nthe chosen sentiment or style of the caption. To address this, we design an\nattention-based model to better add sentiment to image captions. The model\nembeds and learns sentiment with respect to image-caption data, and uses both\nhigh-level and word-level sentiment information during the learning process.\nThe model outperforms the state-of-the-art work in image captioning with\nsentiment using standard evaluation metrics. An analysis of generated captions\nalso shows that our model does this by a better selection of the\nsentiment-bearing adjectives and adjective-noun pairs.\n", "rewritten_text": "Recent research has focused on image captioning models that primarily describe the factual aspects of an image. Some models have started incorporating non-factual elements into captions, such as sentiment or style. However, these models often struggle to strike a balance between the semantic content of the image and the non-factual dimensions of the caption. Furthermore, it has been observed that humans may emphasize different aspects of an image based on the sentiment or style conveyed in the caption.\n\nTo tackle this issue, we have developed an attention-based model aimed at enhancing the integration of sentiment into image captions. Our model embeds and learns sentiment within the context of image-caption data, utilizing both high-level and word-level sentiment information throughout the learning process. Through our experiments, the model has demonstrated superior performance compared to existing state-of-the-art approaches in image captioning with sentiment, as measured by standard evaluation metrics. An analysis of the generated captions further reveals that our model achieves this by making better selections of sentiment-bearing adjectives and adjective-noun pairs."}, "2310.15724": {"original_text": "  Pre-trained language models (PLMs) have achieved remarkable results on NLP\ntasks but at the expense of huge parameter sizes and the consequent\ncomputational costs. In this paper, we propose Variator, a parameter-efficient\nacceleration method that enhances computational efficiency through\nplug-and-play compression plugins. Compression plugins are designed to reduce\nthe sequence length via compressing multiple hidden vectors into one and\ntrained with original PLMs frozen. Different from traditional model\nacceleration methods, which compress PLMs to smaller sizes, Variator offers two\ndistinct advantages: (1) In real-world applications, the plug-and-play nature\nof our compression plugins enables dynamic selection of different compression\nplugins with varying acceleration ratios based on the current workload. (2) The\ncompression plugin comprises a few compact neural network layers with minimal\nparameters, significantly saving storage and memory overhead, particularly in\nscenarios with a growing number of tasks. We validate the effectiveness of\nVariator on seven datasets. Experimental results show that Variator can save\n53% computational costs using only 0.9% additional parameters with a\nperformance drop of less than 2%. Moreover, when the model scales to billions\nof parameters, Variator matches the strong performance of uncompressed PLMs.\n", "rewritten_text": "Pre-trained language models (PLMs) have achieved remarkable results in NLP tasks, albeit with the drawback of large parameter sizes and the subsequent computational costs. This paper introduces Variator, a parameter-efficient acceleration method that improves computational efficiency by utilizing plug-and-play compression plugins. These compression plugins are designed to reduce sequence length by consolidating multiple hidden vectors into one, and they are trained while the original PLMs remain frozen. Unlike traditional model acceleration techniques that shrink PLMs to smaller sizes, Variator offers two key advantages: \n\n1. In real-world applications, the plug-and-play nature of our compression plugins allows for dynamic selection of different compression plugins with varying acceleration ratios based on the current workload.\n2. The compression plugin consists of a few compact neural network layers with minimal parameters, resulting in significant savings in storage and memory overhead, especially in scenarios with an increasing number of tasks.\n\nThe effectiveness of Variator is validated on seven datasets. Experimental results demonstrate that Variator can reduce computational costs by 53% using only 0.9% additional parameters, with a performance decrease of less than 2%. Furthermore, as the model scales to billions of parameters, Variator achieves performance comparable to uncompressed PLMs."}, "2110.02929": {"original_text": "  Event-based dynamic vision sensors provide very sparse output in the form of\nspikes, which makes them suitable for low-power applications. Convolutional\nspiking neural networks model such event-based data and develop their full\nenergy-saving potential when deployed on asynchronous neuromorphic hardware.\nEvent-based vision being a nascent field, the sensitivity of spiking neural\nnetworks to potentially malicious adversarial attacks has received little\nattention so far. We show how white-box adversarial attack algorithms can be\nadapted to the discrete and sparse nature of event-based visual data, and\ndemonstrate smaller perturbation magnitudes at higher success rates than the\ncurrent state-of-the-art algorithms. For the first time, we also verify the\neffectiveness of these perturbations directly on neuromorphic hardware.\nFinally, we discuss the properties of the resulting perturbations, the effect\nof adversarial training as a defense strategy, and future directions.\n", "rewritten_text": "Event-based dynamic vision sensors produce sparse output in the form of spikes, making them ideal for low-power applications. Convolutional spiking neural networks are designed to model this event-based data and maximize energy efficiency when implemented on asynchronous neuromorphic hardware. Despite the emerging nature of event-based vision technology, the vulnerability of spiking neural networks to potential adversarial attacks has not been extensively explored. In this study, we demonstrate how white-box adversarial attack algorithms can be tailored to the discrete and sparse characteristics of event-based visual data, resulting in smaller perturbations with higher success rates compared to current state-of-the-art algorithms. Additionally, we present the first verification of the impact of these perturbations on neuromorphic hardware. Furthermore, we analyze the properties of the perturbations, the effectiveness of adversarial training as a defense mechanism, and outline potential future research directions."}, "2405.20259": {"original_text": "  The proliferation of deep learning solutions and the scarcity of large\nannotated datasets pose significant challenges in real-world applications.\nVarious strategies have been explored to overcome this challenge, with data\naugmentation (DA) approaches emerging as prominent solutions. DA approaches\ninvolve generating additional examples by transforming existing labeled data,\nthereby enriching the dataset and helping deep learning models achieve improved\ngeneralization without succumbing to overfitting. In real applications, where\nsolutions based on deep learning are widely used, there is facial expression\nrecognition (FER), which plays an essential role in human communication,\nimproving a range of knowledge areas (e.g., medicine, security, and marketing).\nIn this paper, we propose a simple and comprehensive face data augmentation\napproach based on mixed face component regularization that outperforms the\nclassical DA approaches from the literature, including the MixAugment which is\na specific approach for the target task in two well-known FER datasets existing\nin the literature.\n", "rewritten_text": "The proliferation of deep learning solutions and the scarcity of large annotated datasets present significant challenges in real-world applications. Various strategies have been explored to address this challenge, with data augmentation (DA) approaches emerging as prominent solutions. DA approaches involve generating additional examples by transforming existing labeled data, thereby enriching the dataset and helping deep learning models achieve improved generalization without succumbing to overfitting.\n\nIn real-world applications where deep learning solutions are widely utilized, facial expression recognition (FER) plays a crucial role in human communication, enhancing various knowledge areas such as medicine, security, and marketing. This paper introduces a simple and comprehensive face data augmentation approach based on mixed face component regularization. This approach outperforms classical DA methods found in the literature, including MixAugment, which is a specific approach for the target task in two well-known FER datasets."}, "2308.09983": {"original_text": "  Early detection of dysplasia of the cervix is critical for cervical cancer\ntreatment. However, automatic cervical dysplasia diagnosis via visual\ninspection, which is more appropriate in low-resource settings, remains a\nchallenging problem. Though promising results have been obtained by recent deep\nlearning models, their performance is significantly hindered by the limited\nscale of the available cervix datasets. Distinct from previous methods that\nlearn from a single dataset, we propose to leverage cross-domain cervical\nimages that were collected in different but related clinical studies to improve\nthe model's performance on the targeted cervix dataset. To robustly learn the\ntransferable information across datasets, we propose a novel prototype-based\nknowledge filtering method to estimate the transferability of cross-domain\nsamples. We further optimize the shared feature space by aligning the\ncross-domain image representations simultaneously on domain level with early\nalignment and class level with supervised contrastive learning, which endows\nmodel training and knowledge transfer with stronger robustness. The empirical\nresults on three real-world benchmark cervical image datasets show that our\nproposed method outperforms the state-of-the-art cervical dysplasia visual\ninspection by an absolute improvement of 4.7% in top-1 accuracy, 7.0% in\nprecision, 1.4% in recall, 4.6% in F1 score, and 0.05 in ROC-AUC.\n", "rewritten_text": "Early detection of cervical dysplasia is crucial for effective cervical cancer treatment. However, automatic diagnosis of cervical dysplasia through visual inspection, which is more suitable for low-resource settings, presents a significant challenge. While recent deep learning models have shown promising results, their performance is hindered by the limited scale of available cervix datasets. In contrast to previous approaches that rely on a single dataset, we propose leveraging cross-domain cervical images from various related clinical studies to enhance the model's performance on the targeted cervix dataset. To effectively learn transferable information across datasets, we introduce a novel prototype-based knowledge filtering method to assess the transferability of cross-domain samples. Additionally, we optimize the shared feature space by aligning cross-domain image representations at both the domain and class levels simultaneously, using early alignment and supervised contrastive learning. This approach enhances the robustness of model training and knowledge transfer. Empirical results on three real-world benchmark cervical image datasets demonstrate that our proposed method surpasses the current state-of-the-art in cervical dysplasia visual inspection, achieving an absolute improvement of 4.7% in top-1 accuracy, 7.0% in precision, 1.4% in recall, 4.6% in F1 score, and 0.05 in ROC-AUC."}, "1704.02268": {"original_text": "  Unsupervised learning of visual similarities is of paramount importance to\ncomputer vision, particularly due to lacking training data for fine-grained\nsimilarities. Deep learning of similarities is often based on relationships\nbetween pairs or triplets of samples. Many of these relations are unreliable\nand mutually contradicting, implying inconsistencies when trained without\nsupervision information that relates different tuples or triplets to each\nother. To overcome this problem, we use local estimates of reliable\n(dis-)similarities to initially group samples into compact surrogate classes\nand use local partial orders of samples to classes to link classes to each\nother. Similarity learning is then formulated as a partial ordering task with\nsoft correspondences of all samples to classes. Adopting a strategy of\nself-supervision, a CNN is trained to optimally represent samples in a mutually\nconsistent manner while updating the classes. The similarity learning and\ngrouping procedure are integrated in a single model and optimized jointly. The\nproposed unsupervised approach shows competitive performance on detailed pose\nestimation and object classification.\n", "rewritten_text": "Unsupervised learning of visual similarities is crucial in computer vision, especially when there is a lack of training data for fine-grained similarities. Deep learning of similarities often relies on relationships between pairs or triplets of samples. However, many of these relationships are unreliable and contradictory, leading to inconsistencies when trained without supervision information that connects different tuples or triplets. To address this issue, we utilize local estimates of reliable (dis-)similarities to initially group samples into compact surrogate classes. We then use local partial orders of samples within these classes to establish connections between classes. The process of similarity learning is then framed as a partial ordering task with soft correspondences of all samples to classes.\n\nBy employing a self-supervision strategy, a Convolutional Neural Network (CNN) is trained to effectively represent samples in a consistent manner while updating the classes. The similarity learning and grouping procedure are integrated into a single model and optimized jointly. This unsupervised approach demonstrates competitive performance in detailed pose estimation and object classification."}, "1909.07598": {"original_text": "  Multi-hop question answering (QA) requires an information retrieval (IR)\nsystem that can find \\emph{multiple} supporting evidence needed to answer the\nquestion, making the retrieval process very challenging. This paper introduces\nan IR technique that uses information of entities present in the initially\nretrieved evidence to learn to `\\emph{hop}' to other relevant evidence. In a\nsetting, with more than \\textbf{5 million} Wikipedia paragraphs, our approach\nleads to significant boost in retrieval performance. The retrieved evidence\nalso increased the performance of an existing QA model (without any training)\non the \\hotpot benchmark by \\textbf{10.59} F1.\n", "rewritten_text": "Multi-hop question answering (QA) is a challenging task that necessitates an information retrieval (IR) system capable of locating multiple pieces of supporting evidence to answer a question effectively. This paper presents an IR technique that leverages information about entities found in the initially retrieved evidence to navigate to other relevant evidence, a process referred to as \"hopping.\" In an environment containing over 5 million Wikipedia paragraphs, our approach significantly enhances retrieval performance. The retrieved evidence also enhances the performance of an existing QA model, without any additional training, on the Hotpot benchmark by 10.59 F1 points."}, "2211.06726": {"original_text": "  Vision Transformer (ViT) is a pioneering deep learning framework that can\naddress real-world computer vision issues, such as image classification and\nobject recognition. Importantly, ViTs are proven to outperform traditional deep\nlearning models, such as convolutional neural networks (CNNs). Relatively\nrecently, a number of ViT mutations have been transplanted into the field of\nmedical imaging, thereby resolving a variety of critical classification and\nsegmentation challenges, especially in terms of brain imaging data. In this\nwork, we provide a novel multimodal deep learning pipeline, MultiCrossViT,\nwhich is capable of analyzing both structural MRI (sMRI) and static functional\nnetwork connectivity (sFNC) data for the prediction of schizophrenia disease.\nOn a dataset with minimal training subjects, our novel model can achieve an AUC\nof 0.832. Finally, we visualize multiple brain regions and covariance patterns\nmost relevant to schizophrenia based on the resulting ViT attention maps by\nextracting features from transformer encoders.\n", "rewritten_text": "The Vision Transformer (ViT) is an innovative deep learning framework designed to tackle real-world computer vision challenges, such as image classification and object recognition. ViTs have demonstrated superior performance compared to traditional deep learning models like convolutional neural networks (CNNs). Recently, various ViT variants have been applied in the field of medical imaging, effectively addressing critical classification and segmentation tasks, particularly in brain imaging data. In this study, we introduce a novel multimodal deep learning pipeline called MultiCrossViT, capable of analyzing both structural MRI (sMRI) and static functional network connectivity (sFNC) data to predict schizophrenia disease. Our model achieves an AUC of 0.832 on a dataset with limited training subjects. Additionally, we use ViT attention maps to visualize key brain regions and covariance patterns relevant to schizophrenia by extracting features from transformer encoders."}, "1607.02504": {"original_text": "  We present a method to predict image deformations based on patch-wise image\nappearance. Specifically, we design a patch-based deep encoder-decoder network\nwhich learns the pixel/voxel-wise mapping between image appearance and\nregistration parameters. Our approach can predict general deformation\nparameterizations, however, we focus on the large deformation diffeomorphic\nmetric mapping (LDDMM) registration model. By predicting the LDDMM\nmomentum-parameterization we retain the desirable theoretical properties of\nLDDMM, while reducing computation time by orders of magnitude: combined with\npatch pruning, we achieve a 1500x/66x speed up compared to GPU-based\noptimization for 2D/3D image registration. Our approach has better prediction\naccuracy than predicting deformation or velocity fields and results in\ndiffeomorphic transformations. Additionally, we create a Bayesian probabilistic\nversion of our network, which allows evaluation of deformation field\nuncertainty through Monte Carlo sampling using dropout at test time. We show\nthat deformation uncertainty highlights areas of ambiguous deformations. We\ntest our method on the OASIS brain image dataset in 2D and 3D.\n", "rewritten_text": "We introduce a method for predicting image deformations based on patch-wise image appearance. Specifically, we have developed a patch-based deep encoder-decoder network that learns the mapping between image appearance and registration parameters at the pixel/voxel level. While our approach is capable of predicting general deformation parameterizations, our primary focus is on the large deformation diffeomorphic metric mapping (LDDMM) registration model. By predicting the LDDMM momentum parameterization, we are able to preserve the advantageous theoretical properties of LDDMM while significantly reducing computation time by orders of magnitude. Through the combination of patch pruning, we have achieved a speedup of 1500x/66x compared to GPU-based optimization for 2D/3D image registration.\n\nOur method demonstrates superior prediction accuracy compared to predicting deformation or velocity fields, resulting in diffeomorphic transformations. Furthermore, we have developed a Bayesian probabilistic version of our network, enabling the evaluation of deformation field uncertainty through Monte Carlo sampling using dropout during testing. This uncertainty analysis highlights areas where deformations are ambiguous.\n\nTo validate our approach, we conducted experiments using the OASIS brain image dataset in both 2D and 3D settings."}, "1911.08007": {"original_text": "  The classification of streets on road networks has been focused on the\nvehicular transportational features of streets such as arterials, major roads,\nminor roads and so forth based on their transportational use. City authorities\non the other hand have been shifting to more urban inclusive planning of\nstreets, encompassing the side use of a street combined with the\ntransportational features of a street. In such classification schemes, streets\nare labeled for example as commercial throughway, residential neighborhood,\npark etc. This modern approach to urban planning has been adopted by major\ncities such as the city of San Francisco, the states of Florida and\nPennsylvania among many others. Currently, the process of labeling streets\naccording to their contexts is manual and hence is tedious and time consuming.\nIn this paper, we propose an approach to collect and label imagery data then\ndeploy advancements in computer vision towards modern urban planning. We\ncollect and label street imagery then train deep convolutional neural networks\n(CNN) to perform the classification of street context. We show that CNN models\ncan perform well achieving accuracies in the 81% to 87%, we then visualize\nsamples from the embedding space of streets using the t-SNE method and apply\nclass activation mapping methods to interpret the features in street imagery\ncontributing to output classification from a model.\n", "rewritten_text": "The focus of street classification on road networks has traditionally centered on the vehicular transportation aspects of streets, such as arterials, major roads, minor roads, and so on, based on their transportation usage. However, city authorities are increasingly transitioning towards a more comprehensive urban planning approach for streets, which considers not only their transportation functions but also their surrounding uses. In these new classification systems, streets are categorized as, for example, commercial throughways, residential neighborhoods, parks, etc.\n\nThis modern urban planning approach has been embraced by major cities like San Francisco, as well as states such as Florida and Pennsylvania, among others. Currently, the process of labeling streets based on their contexts is manual, making it a tedious and time-consuming task. In this paper, we propose a method to gather and label imagery data, followed by the application of advancements in computer vision to enhance modern urban planning practices.\n\nOur approach involves collecting and labeling street imagery, then training deep convolutional neural networks (CNN) to classify street contexts. Our results demonstrate that CNN models can achieve high accuracies ranging from 81% to 87%. Furthermore, we visualize samples from the embedding space of streets using the t-SNE method and utilize class activation mapping techniques to interpret the features in street imagery that influence the model's classification output."}, "1811.11387": {"original_text": "  The success of deep neural networks generally requires a vast amount of\ntraining data to be labeled, which is expensive and unfeasible in scale,\nespecially for video collections. To alleviate this problem, in this paper, we\npropose 3DRotNet: a fully self-supervised approach to learn spatiotemporal\nfeatures from unlabeled videos. A set of rotations are applied to all videos,\nand a pretext task is defined as prediction of these rotations. When\naccomplishing this task, 3DRotNet is actually trained to understand the\nsemantic concepts and motions in videos. In other words, it learns a\nspatiotemporal video representation, which can be transferred to improve video\nunderstanding tasks in small datasets. Our extensive experiments successfully\ndemonstrate the effectiveness of the proposed framework on action recognition,\nleading to significant improvements over the state-of-the-art self-supervised\nmethods. With the self-supervised pre-trained 3DRotNet from large datasets, the\nrecognition accuracy is boosted up by 20.4% on UCF101 and 16.7% on HMDB51\nrespectively, compared to the models trained from scratch.\n", "rewritten_text": "The success of deep neural networks typically hinges on the availability of a substantial amount of labeled training data, which can be costly and impractical on a large scale, particularly for video collections. To address this challenge, this paper introduces 3DRotNet: a fully self-supervised approach designed to extract spatiotemporal features from unlabeled videos. The methodology involves applying a series of rotations to all videos and formulating a pretext task centered around predicting these rotations. By successfully completing this task, 3DRotNet effectively learns to discern the semantic concepts and movements within videos. Essentially, it acquires a spatiotemporal video representation that can be leveraged to enhance video comprehension tasks with limited datasets. Extensive experiments validate the efficacy of the proposed framework in action recognition, showcasing significant advancements over existing self-supervised techniques. Through the utilization of the self-supervised pre-trained 3DRotNet on large datasets, recognition accuracy sees a notable increase of 20.4% on UCF101 and 16.7% on HMDB51, respectively, compared to models trained from scratch."}, "2402.01619": {"original_text": "  Program induction (PI) has become a promising paradigm for using knowledge\nbases (KBs) to help large language models (LLMs) answer complex\nknowledge-intensive questions. Nonetheless, PI typically relies on a large\nnumber of parallel question-program pairs to make the LLM aware of the schema\nof the given KB, and is thus challenging for many low-resourced KBs that lack\nannotated data. To this end, we propose KB-Plugin, a plug-and-play framework\nthat enables LLMs to induce programs over any low-resourced KB. Firstly,\nKB-Plugin adopts self-supervised learning to encode the detailed schema\ninformation of a given KB into a pluggable module, namely schema plugin.\nSecondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB\nto train another pluggable module, namely PI plugin, which can help the LLM\nextract question-relevant schema information from the schema plugin of any KB\nand utilize this information to induce programs over this KB. Experiments on\nfive heterogeneous KBQA datasets show that KB-Plugin achieves better or\ncomparable performance with 25$\\times$ smaller backbone LLM compared to SoTA PI\nmethods for low-resourced KBs, and even approaches the performance of\nsupervised methods. Our code and data are available at\nhttps://github.com/THU-KEG/KB-Plugin.\n", "rewritten_text": "Program induction (PI) has emerged as a promising paradigm for leveraging knowledge bases (KBs) to assist large language models (LLMs) in answering complex, knowledge-intensive questions. However, PI typically requires a substantial number of parallel question-program pairs to familiarize the LLM with the schema of the provided KB. This poses a challenge for many low-resourced KBs that lack annotated data. In response to this challenge, we introduce KB-Plugin, a plug-and-play framework designed to empower LLMs to induce programs over any low-resourced KB.\n\nKB-Plugin employs self-supervised learning to encode the detailed schema information of a given KB into a pluggable module called the schema plugin. Additionally, KB-Plugin leverages abundant annotated data from a rich-resourced KB to train another pluggable module, known as the PI plugin. The PI plugin assists the LLM in extracting question-relevant schema information from the schema plugin of any KB, enabling the LLM to induce programs over the KB based on this information.\n\nExperimental results on five diverse KBQA datasets demonstrate that KB-Plugin achieves performance that is either superior or comparable to state-of-the-art PI methods for low-resourced KBs, while utilizing a backbone LLM that is 25 times smaller. Furthermore, KB-Plugin even approaches the performance of supervised methods. For those interested, our code and data can be accessed at https://github.com/THU-KEG/KB-Plugin."}, "1812.01458": {"original_text": "  Recent advances in deep learning have shown exciting promise in filling large\nholes and lead to another orientation for image inpainting. However, existing\nlearning-based methods often create artifacts and fallacious textures because\nof insufficient cognition understanding. Previous generative networks are\nlimited with single receptive type and give up pooling in consideration of\ndetail sharpness. Human cognition is constant regardless of the target\nattribute. As multiple receptive fields improve the ability of abstract image\ncharacterization and pooling can keep feature invariant, specifically, deep\ninception learning is adopted to promote high-level feature representation and\nenhance model learning capacity for local patches. Moreover, approaches for\ngenerating diverse mask images are introduced and a random mask dataset is\ncreated. We benchmark our methods on ImageNet, Places2 dataset, and CelebA-HQ.\nExperiments for regular, irregular, and custom regions completion are all\nperformed and free-style image inpainting is also presented. Quantitative\ncomparisons with previous state-of-the-art methods show that ours obtain much\nmore natural image completions.\n", "rewritten_text": "Recent advancements in deep learning have demonstrated promising potential in addressing significant gaps and have led to a new approach to image inpainting. However, current learning-based techniques often result in artifacts and inaccurate textures due to a lack of comprehensive understanding of cognition. Previous generative networks have been constrained by a single receptive type and have sacrificed pooling to maintain detailed sharpness. Human cognition remains consistent regardless of the target attribute. By utilizing multiple receptive fields to enhance abstract image characterization and incorporating pooling to preserve feature consistency, deep inception learning has been adopted to enhance high-level feature representation and improve the model's learning capacity for local patches. Additionally, methods for generating diverse mask images have been introduced, and a random mask dataset has been developed. Our methods have been evaluated on the ImageNet, Places2 dataset, and CelebA-HQ. We have conducted experiments on completing regular, irregular, and custom regions, as well as free-style image inpainting. Quantitative comparisons with previous state-of-the-art methods demonstrate that our approach yields significantly more natural image completions."}, "cs/0207005": {"original_text": "  We present a broad coverage Japanese grammar written in the HPSG formalism\nwith MRS semantics. The grammar is created for use in real world applications,\nsuch that robustness and performance issues play an important role. It is\nconnected to a POS tagging and word segmentation tool. This grammar is being\ndeveloped in a multilingual context, requiring MRS structures that are easily\ncomparable across languages.\n", "rewritten_text": "We introduce a comprehensive Japanese grammar formulated in the HPSG formalism, accompanied by MRS semantics. This grammar is designed for practical applications, emphasizing robustness and performance. It is integrated with a POS tagging and word segmentation tool. The development of this grammar occurs within a multilingual framework, necessitating MRS structures that facilitate cross-linguistic comparisons."}, "2307.10696": {"original_text": "  Improving the feature representation ability is the foundation of many whole\nslide pathological image (WSIs) tasks. Recent works have achieved great success\nin pathological-specific self-supervised learning (SSL). However, most of them\nonly focus on learning patch-level representations, thus there is still a gap\nbetween pretext and slide-level downstream tasks, e.g., subtyping, grading and\nstaging. Aiming towards slide-level representations, we propose Slide-Level\nPrototypical Distillation (SLPD) to explore intra- and inter-slide semantic\nstructures for context modeling on WSIs. Specifically, we iteratively perform\nintra-slide clustering for the regions (4096x4096 patches) within each WSI to\nyield the prototypes and encourage the region representations to be closer to\nthe assigned prototypes. By representing each slide with its prototypes, we\nfurther select similar slides by the set distance of prototypes and assign the\nregions by cross-slide prototypes for distillation. SLPD achieves\nstate-of-the-art results on multiple slide-level benchmarks and demonstrates\nthat representation learning of semantic structures of slides can make a\nsuitable proxy task for WSI analysis. Code will be available at\nhttps://github.com/Carboxy/SLPD.\n", "rewritten_text": "Improving the ability to represent features is fundamental for many tasks involving whole-slide pathological images (WSIs). Recent studies have made significant progress in pathological-specific self-supervised learning (SSL). However, most of these studies have focused solely on learning representations at the patch level, leaving a gap between the initial task and downstream slide-level tasks such as subtyping, grading, and staging. To address this gap and aim for slide-level representations, we introduce Slide-Level Prototypical Distillation (SLPD). SLPD is designed to explore intra- and inter-slide semantic structures to enhance context modeling on WSIs. \n\nIn SLPD, we conduct iterative intra-slide clustering on regions (4096x4096 patches) within each WSI to identify prototypes and encourage region representations to align closely with these prototypes. By representing each slide with its prototypes, we then identify similar slides based on the distance between their prototypes and assign regions based on cross-slide prototypes for distillation. SLPD has achieved state-of-the-art results on multiple slide-level benchmarks, showcasing that learning semantic structures of slides can serve as a valuable proxy task for WSI analysis. The code for SLPD will be made available at https://github.com/Carboxy/SLPD."}, "2205.05869": {"original_text": "  We address the task of view synthesis, generating novel views of a scene\ngiven a set of images as input. In many recent works such as NeRF (Mildenhall\net al., 2020), the scene geometry is parameterized using neural implicit\nrepresentations (i.e., MLPs). Implicit neural representations have achieved\nimpressive visual quality but have drawbacks in computational efficiency. In\nthis work, we propose a new approach that performs view synthesis using point\nclouds. It is the first point-based method that achieves better visual quality\nthan NeRF while being 100x faster in rendering speed. Our approach builds on\nexisting works on differentiable point-based rendering but introduces a novel\ntechnique we call \"Sculpted Neural Points (SNP)\", which significantly improves\nthe robustness to errors and holes in the reconstructed point cloud. We further\npropose to use view-dependent point features based on spherical harmonics to\ncapture non-Lambertian surfaces, and new designs in the point-based rendering\npipeline that further boost the performance. Finally, we show that our system\nsupports fine-grained scene editing. Code is available at\nhttps://github.com/princeton-vl/SNP.\n", "rewritten_text": "We focus on the task of view synthesis, which involves generating new views of a scene based on a set of input images. Recent works, such as NeRF (Mildenhall et al., 2020), have utilized neural implicit representations, specifically Multi-Layer Perceptrons (MLPs), to parameterize scene geometry. While implicit neural representations have demonstrated impressive visual quality, they are limited in computational efficiency.\n\nIn this study, we introduce a novel approach to view synthesis using point clouds. Our method is the first point-based technique to achieve superior visual quality compared to NeRF, while also being 100 times faster in rendering speed. Building upon previous research on differentiable point-based rendering, we introduce a new technique called \"Sculpted Neural Points (SNP)\" that enhances robustness to errors and holes in the reconstructed point cloud.\n\nAdditionally, we propose the use of view-dependent point features based on spherical harmonics to capture non-Lambertian surfaces, along with new designs in the point-based rendering pipeline to further enhance performance. Our system also supports fine-grained scene editing capabilities. The code for our approach is available at https://github.com/princeton-vl/SNP."}, "1910.03484": {"original_text": "  In Natural Language Generation (NLG), End-to-End (E2E) systems trained\nthrough deep learning have recently gained a strong interest. Such deep models\nneed a large amount of carefully annotated data to reach satisfactory\nperformance. However, acquiring such datasets for every new NLG application is\na tedious and time-consuming task. In this paper, we propose a semi-supervised\ndeep learning scheme that can learn from non-annotated data and annotated data\nwhen available. It uses an NLG and a Natural Language Understanding (NLU)\nsequence-to-sequence models which are learned jointly to compensate for the\nlack of annotation. Experiments on two benchmark datasets show that, with\nlimited amount of annotated data, the method can achieve very competitive\nresults while not using any pre-processing or re-scoring tricks. These findings\nopen the way to the exploitation of non-annotated datasets which is the current\nbottleneck for the E2E NLG system development to new applications.\n", "rewritten_text": "In the field of Natural Language Generation (NLG), there has been a growing interest in End-to-End (E2E) systems trained using deep learning. These deep models require a significant amount of meticulously annotated data to achieve satisfactory performance. However, the process of obtaining such datasets for each new NLG application is laborious and time-consuming. \n\nThis paper introduces a novel semi-supervised deep learning approach that can effectively learn from both annotated and non-annotated data. The proposed method utilizes NLG and Natural Language Understanding (NLU) sequence-to-sequence models, which are trained jointly to address the lack of annotation. Experimental results on two benchmark datasets demonstrate that, even with a limited amount of annotated data, the approach can deliver highly competitive results without the need for pre-processing or re-scoring techniques. \n\nThese findings pave the way for leveraging non-annotated datasets, which currently represent a bottleneck in the development of E2E NLG systems for new applications."}, "2109.09883": {"original_text": "  Few-shot classification aims at classifying categories of a novel task by\nlearning from just a few (typically, 1 to 5) labelled examples. An effective\napproach to few-shot classification involves a prior model trained on a\nlarge-sample base domain, which is then finetuned over the novel few-shot task\nto yield generalizable representations. However, task-specific finetuning is\nprone to overfitting due to the lack of enough training examples. To alleviate\nthis issue, we propose a new finetuning approach based on contrastive learning\nthat reuses unlabelled examples from the base domain in the form of\ndistractors. Unlike the nature of unlabelled data used in prior works,\ndistractors belong to classes that do not overlap with the novel categories. We\ndemonstrate for the first time that inclusion of such distractors can\nsignificantly boost few-shot generalization. Our technical novelty includes a\nstochastic pairing of examples sharing the same category in the few-shot task\nand a weighting term that controls the relative influence of task-specific\nnegatives and distractors. An important aspect of our finetuning objective is\nthat it is agnostic to distractor labels and hence applicable to various base\ndomain settings. Compared to state-of-the-art approaches, our method shows\naccuracy gains of up to $12\\%$ in cross-domain and up to $5\\%$ in unsupervised\nprior-learning settings.\n", "rewritten_text": "Few-shot classification is the task of categorizing new tasks by learning from a small number of labeled examples, typically ranging from 1 to 5. An effective strategy for few-shot classification involves utilizing a pre-trained model on a large dataset, which is then fine-tuned on the new few-shot task to generate generalizable representations. However, fine-tuning specific to the task can lead to overfitting due to the limited number of training examples available. To address this challenge, we introduce a novel fine-tuning approach based on contrastive learning, which leverages unlabeled examples from the base dataset as distractors. Unlike previous methods that use unlabeled data, these distractors come from classes that do not intersect with the new categories. Our research demonstrates, for the first time, that incorporating such distractors can significantly enhance few-shot generalization performance. Our innovative approach includes a stochastic pairing of examples sharing the same category in the few-shot task, along with a weighting term that regulates the impact of task-specific negatives and distractors. A key feature of our fine-tuning objective is its independence from distractor labels, making it adaptable to various base dataset scenarios. When compared to state-of-the-art techniques, our method achieves accuracy improvements of up to 12% in cross-domain scenarios and up to 5% in unsupervised prior-learning setups."}, "2003.12137": {"original_text": "  We explore novel approaches to the task of image generation from their\nrespective captions, building on state-of-the-art GAN architectures.\nParticularly, we baseline our models with the Attention-based GANs that learn\nattention mappings from words to image features. To better capture the features\nof the descriptions, we then built a novel cyclic design that learns an inverse\nfunction to maps the image back to original caption. Additionally, we\nincorporated recently developed BERT pretrained word embeddings as our initial\ntext featurizer and observe a noticeable improvement in qualitative and\nquantitative performance compared to the Attention GAN baseline.\n", "rewritten_text": "We are exploring innovative approaches to generating images based on their corresponding captions, utilizing advanced GAN architectures. Specifically, we are establishing a foundation for our models using Attention-based GANs, which are capable of learning attention mappings from words to image features. In order to more effectively capture the essence of the descriptions, we have developed a unique cyclic design that learns an inverse function to map the image back to the original caption. Furthermore, we have integrated BERT pretrained word embeddings as our initial text featurizer, resulting in a significant enhancement in both qualitative and quantitative performance when compared to the Attention GAN baseline."}, "2003.02683": {"original_text": "  We introduce the first method for automatic image generation from scene-level\nfreehand sketches. Our model allows for controllable image generation by\nspecifying the synthesis goal via freehand sketches. The key contribution is an\nattribute vector bridged Generative Adversarial Network called EdgeGAN, which\nsupports high visual-quality object-level image content generation without\nusing freehand sketches as training data. We have built a large-scale composite\ndataset called SketchyCOCO to support and evaluate the solution. We validate\nour approach on the tasks of both object-level and scene-level image generation\non SketchyCOCO. Through quantitative, qualitative results, human evaluation and\nablation studies, we demonstrate the method's capacity to generate realistic\ncomplex scene-level images from various freehand sketches.\n", "rewritten_text": "We present a novel method for automatically generating images from freehand sketches at the scene level. Our model enables precise control over image generation by defining the desired outcome through freehand sketches. The primary innovation lies in the development of EdgeGAN, a Generative Adversarial Network that utilizes an attribute vector to facilitate the creation of high-quality object-level images without the need for freehand sketches in the training data. To support and assess our solution, we have curated a comprehensive dataset named SketchyCOCO. Our approach is validated through the generation of object-level and scene-level images on SketchyCOCO. By leveraging quantitative and qualitative results, human evaluations, and ablation studies, we showcase the method's ability to produce realistic and intricate scene-level images from diverse freehand sketches."}, "2409.00942": {"original_text": "  Normalizing flows, a category of probabilistic models famed for their\ncapabilities in modeling complex data distributions, have exhibited remarkable\nefficacy in unsupervised anomaly detection. This paper explores the potential\nof normalizing flows in multi-class anomaly detection, wherein the normal data\nis compounded with multiple classes without providing class labels. Through the\nintegration of vector quantization (VQ), we empower the flow models to\ndistinguish different concepts of multi-class normal data in an unsupervised\nmanner, resulting in a novel flow-based unified method, named VQ-Flow.\nSpecifically, our VQ-Flow leverages hierarchical vector quantization to\nestimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for\nconcept distinction and its concomitant Concept-Specific Pattern Codebook\n(CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow\nare conditioned on the concept-specific patterns captured in CSPC, capable of\nmodeling specific normal patterns associated with different concepts. Moreover,\nCPC further enables our VQ-Flow for concept-aware distribution modeling,\nfaithfully mimicking the intricate multi-class normal distribution through a\nmixed Gaussian distribution reparametrized on the conceptual prototypes.\nThrough the introduction of vector quantization, the proposed VQ-Flow advances\nthe state-of-the-art in multi-class anomaly detection within a unified training\nscheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase\nis publicly available at https://github.com/cool-xuan/vqflow.\n", "rewritten_text": "Normalizing flows, a category of probabilistic models renowned for their ability to model complex data distributions, have demonstrated exceptional effectiveness in unsupervised anomaly detection. This study delves into the potential of normalizing flows in multi-class anomaly detection, where normal data is combined with multiple classes without the provision of class labels. By incorporating vector quantization (VQ), we enhance the flow models to differentiate between various concepts of multi-class normal data in an unsupervised manner, resulting in a novel flow-based unified approach known as VQ-Flow.\n\nSpecifically, our VQ-Flow utilizes hierarchical vector quantization to estimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for concept differentiation and its corresponding Concept-Specific Pattern Codebook (CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow are conditioned on the concept-specific patterns captured in CSPC, enabling them to model specific normal patterns associated with different concepts. Furthermore, CPC enables our VQ-Flow to perform concept-aware distribution modeling, accurately replicating the intricate multi-class normal distribution through a mixed Gaussian distribution reparametrized based on the conceptual prototypes.\n\nBy introducing vector quantization, the proposed VQ-Flow advances the state-of-the-art in multi-class anomaly detection within a unified training framework, achieving a Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase is publicly accessible at https://github.com/cool-xuan/vqflow."}, "1708.00801": {"original_text": "  We study the impact of big models (in terms of the degree of lexicalization)\nand big data (in terms of the training corpus size) on dependency grammar\ninduction. We experimented with L-DMV, a lexicalized version of Dependency\nModel with Valence and L-NDMV, our lexicalized extension of the Neural\nDependency Model with Valence. We find that L-DMV only benefits from very small\ndegrees of lexicalization and moderate sizes of training corpora. L-NDMV can\nbenefit from big training data and lexicalization of greater degrees,\nespecially when enhanced with good model initialization, and it achieves a\nresult that is competitive with the current state-of-the-art.\n", "rewritten_text": "We investigated the impact of large models, in terms of lexicalization, and extensive data, in terms of training corpus size, on the induction of dependency grammar. Our experiments involved L-DMV, a lexicalized version of the Dependency Model with Valence, and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence. Our findings indicate that L-DMV shows improvements with minimal levels of lexicalization and moderate training corpus sizes. On the other hand, L-NDMV benefits from large training datasets and higher degrees of lexicalization, particularly when combined with effective model initialization. This approach achieves competitive results compared to the current state-of-the-art methods."}, "2303.1309": {"original_text": "  Recent trends in semi-supervised learning have significantly boosted the\nperformance of 3D semi-supervised medical image segmentation. Compared with 2D\nimages, 3D medical volumes involve information from different directions, e.g.,\ntransverse, sagittal, and coronal planes, so as to naturally provide\ncomplementary views. These complementary views and the intrinsic similarity\namong adjacent 3D slices inspire us to develop a novel annotation way and its\ncorresponding semi-supervised model for effective segmentation. Specifically,\nwe firstly propose the orthogonal annotation by only labeling two orthogonal\nslices in a labeled volume, which significantly relieves the burden of\nannotation. Then, we perform registration to obtain the initial pseudo labels\nfor sparsely labeled volumes. Subsequently, by introducing unlabeled volumes,\nwe propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that\nexploits dense pseudo labels in early stage and sparse labels in later stage\nand meanwhile forces consistent output of two networks. Experimental results on\nthree benchmark datasets validated our effectiveness in performance and\nefficiency in annotation. For example, with only 10 annotated slices, our\nmethod reaches a Dice up to 86.93% on KiTS19 dataset.\n", "rewritten_text": "Recent advancements in semi-supervised learning have significantly enhanced the performance of 3D semi-supervised medical image segmentation. Unlike 2D images, 3D medical volumes incorporate information from various directions, such as transverse, sagittal, and coronal planes, offering complementary perspectives. Leveraging these complementary views and the inherent similarities among adjacent 3D slices, we have devised a novel annotation approach and a corresponding semi-supervised model for efficient segmentation.\n\nOur approach introduces the concept of orthogonal annotation, where only two orthogonal slices within a labeled volume are annotated, thereby reducing the annotation workload. Subsequently, registration is employed to generate initial pseudo labels for sparsely labeled volumes. By incorporating unlabeled volumes, we introduce a dual-network framework known as Dense-Sparse Co-training (DeSCO). This framework leverages dense pseudo labels in the initial stages and sparse labels in later stages, while ensuring consistent outputs from both networks.\n\nExperimental results on three benchmark datasets confirm the effectiveness and efficiency of our approach in terms of performance and annotation. Notably, with just 10 annotated slices, our method achieves a Dice score of up to 86.93% on the KiTS19 dataset."}, "2401.08123": {"original_text": "  Guided depth super-resolution (GDSR) involves restoring missing depth details\nusing the high-resolution RGB image of the same scene. Previous approaches have\nstruggled with the heterogeneity and complementarity of the multi-modal inputs,\nand neglected the issues of modal misalignment, geometrical misalignment, and\nfeature selection. In this study, we rethink some essential components in GDSR\nnetworks and propose a simple yet effective Dynamic Dual Alignment and\nAggregation network (D2A2). D2A2 mainly consists of 1) a dynamic dual alignment\nmodule that adapts to alleviate the modal misalignment via a learnable domain\nalignment block and geometrically align cross-modal features by learning the\noffset; and 2) a mask-to-pixel feature aggregate module that uses the gated\nmechanism and pixel attention to filter out irrelevant texture noise from RGB\nfeatures and combine the useful features with depth features. By combining the\nstrengths of RGB and depth features while minimizing disturbance introduced by\nthe RGB image, our method with simple reuse and redesign of basic components\nachieves state-of-the-art performance on multiple benchmark datasets. The code\nis available at https://github.com/JiangXinni/D2A2.\n", "rewritten_text": "The process of Guided Depth Super-Resolution (GDSR) involves enhancing missing depth details by utilizing the high-resolution RGB image of the same scene. Previous methods have encountered challenges due to the diversity and complementarity of the multi-modal inputs, as well as overlooking issues such as modal misalignment, geometrical misalignment, and feature selection. In this research, we reevaluate key elements in GDSR networks and introduce a straightforward yet efficient Dynamic Dual Alignment and Aggregation network (D2A2). D2A2 primarily comprises: 1) a dynamic dual alignment module that adjusts to address modal misalignment through a trainable domain alignment block and aligns cross-modal features geometrically by learning the offset; and 2) a mask-to-pixel feature aggregation module that employs a gated mechanism and pixel attention to eliminate irrelevant texture noise from RGB features, merging valuable features with depth features. By leveraging the strengths of RGB and depth features while minimizing disturbances introduced by the RGB image, our method, which involves simple reuse and redesign of fundamental components, achieves top-tier performance across various benchmark datasets. The code can be accessed at https://github.com/JiangXinni/D2A2."}, "2112.09414": {"original_text": "  By highlighting the regions of the input image that contribute the most to\nthe decision, saliency maps have become a popular method to make neural\nnetworks interpretable. In medical imaging, they are particularly well-suited\nto explain neural networks in the context of abnormality localization. However,\nfrom our experiments, they are less suited to classification problems where the\nfeatures that allow to distinguish between the different classes are spatially\ncorrelated, scattered and definitely non-trivial. In this paper we thus propose\na new paradigm for better interpretability. To this end we provide the user\nwith relevant and easily interpretable information so that he can form his own\nopinion. We use Disentangled Variational Auto-Encoders which latent\nrepresentation is divided into two components: the non-interpretable part and\nthe disentangled part. The latter accounts for the categorical variables\nexplicitly representing the different classes of interest. In addition to\nproviding the class of a given input sample, such a model offers the\npossibility to transform the sample from a given class to a sample of another\nclass, by modifying the value of the categorical variables in the latent\nrepresentation. This paves the way to easier interpretation of class\ndifferences. We illustrate the relevance of this approach in the context of\nautomatic sex determination from hip bones in forensic medicine. The features\nencoded by the model, that distinguish the different classes were found to be\nconsistent with expert knowledge.\n", "rewritten_text": "Saliency maps have gained popularity as a method for making neural networks interpretable by highlighting the regions of the input image that have the most impact on the decision. In the field of medical imaging, saliency maps are particularly useful for explaining neural networks in the context of abnormality localization. However, our experiments have shown that they are less effective for classification problems where the distinguishing features between classes are spatially correlated, scattered, and complex.\n\nIn this paper, we propose a new paradigm to enhance interpretability. Our approach aims to provide users with relevant and easily understandable information so they can form their own opinions. We utilize Disentangled Variational Auto-Encoders, which divide the latent representation into two components: the non-interpretable part and the disentangled part. The disentangled part explicitly represents the categorical variables corresponding to different classes of interest.\n\nThis model not only identifies the class of a given input sample but also allows for transforming the sample from one class to another by adjusting the categorical variables in the latent representation. This capability facilitates a clearer interpretation of class differences. We demonstrate the effectiveness of this approach in the context of automatic sex determination using hip bones in forensic medicine. The features encoded by the model that distinguish between classes align with expert knowledge."}, "1910.06431": {"original_text": "  There has been great success recently in tackling challenging NLP tasks by\nneural networks which have been pre-trained and fine-tuned on large amounts of\ntask data. In this paper, we investigate one such model, BERT for\nquestion-answering, with the aim to analyze why it is able to achieve\nsignificantly better results than other models. We run DeepLIFT on the model\npredictions and test the outcomes to monitor shift in the attention values for\ninput. We also cluster the results to analyze any possible patterns similar to\nhuman reasoning depending on the kind of input paragraph and question the model\nis trying to answer.\n", "rewritten_text": "Recently, there has been significant success in addressing challenging NLP tasks using neural networks that are pre-trained and fine-tuned on extensive task data. This paper explores a specific model, BERT, for question-answering, aiming to analyze the reasons behind its superior performance compared to other models. We employ DeepLIFT on the model's predictions and evaluate the results to track changes in attention values for the input. Additionally, we cluster the outcomes to identify potential patterns resembling human reasoning, based on the type of input paragraph and question the model is tasked with answering."}, "2410.15277": {"original_text": "  Retrieval-augmented generation (RAG) can supplement large language models\n(LLMs) by integrating external knowledge. However, as the number of retrieved\ndocuments increases, the input length to LLMs grows linearly, causing a\ndramatic increase in latency and a degradation in long-context understanding.\nThis is particularly serious for multi-hop questions that require a chain of\nreasoning across documents. To accelerate inference, reduce costs, and minimize\ndistractions, this paper presents BRIEF (Bridging Retrieval and Inference\nthrough Evidence Fusion), a lightweight approach that performs query-aware\nmulti-hop reasoning by compressing retrieved documents into highly dense\ntextual summaries to integrate into in-context learning. To enable learning\ncompression for multi-hop reasoning, we curate synthetic data by extracting\natomic proposition expressions that encapsulate distinct factoids from the\nsource documents to compose synthetic summaries. Based on our synthetic data\nbuilt entirely by open-source models, BRIEF generates more concise summaries\nand enables a range of LLMs to achieve exceptional open-domain question\nanswering (QA) performance. For example, on HotpotQA, BRIEF improves the\ncompression rate by 2 times compared to the state-of-the-art baseline, while\noutperforming it by 3.00% EM and 4.16% F1 with Flan-UL2 as the reader LM. It\nalso generates more concise summaries than proprietary GPT-3.5, while\ndemonstrating nearly identical QA performance.\n", "rewritten_text": "Retrieval-augmented generation (RAG) can enhance large language models (LLMs) by incorporating external knowledge. However, as the number of retrieved documents increases, the input length to LLMs grows linearly, leading to a significant rise in latency and a decline in understanding of long-context information. This issue is particularly critical for multi-hop questions that necessitate a series of logical steps across documents. To expedite inference, reduce expenses, and minimize distractions, this paper introduces BRIEF (Bridging Retrieval and Inference through Evidence Fusion), a lightweight method that conducts query-aware multi-hop reasoning by condensing retrieved documents into highly compact textual summaries for integration into contextual learning. To facilitate compression learning for multi-hop reasoning, we curate synthetic data by extracting atomic proposition expressions that encapsulate distinct factoids from the source documents to form synthetic summaries. Leveraging our synthetic data constructed entirely by open-source models, BRIEF generates more succinct summaries and enables various LLMs to achieve outstanding performance in open-domain question answering (QA). For instance, on HotpotQA, BRIEF enhances the compression rate by 2 times compared to the state-of-the-art baseline, while surpassing it by 3.00% in EM and 4.16% in F1 with Flan-UL2 as the reader LM. It also produces more concise summaries than the proprietary GPT-3.5, while showcasing nearly identical QA performance."}, "2403.17512": {"original_text": "  Improving the efficiency of current neural networks and modeling them in\nbiological neural systems have become popular research directions in recent\nyears. Pulse-coupled neural network (PCNN) is a well applicated model for\nimitating the computation characteristics of the human brain in computer vision\nand neural network fields. However, differences between the PCNN and biological\nneural systems remain: limited neural connection, high computational cost, and\nlack of stochastic property. In this study, random-coupled neural network\n(RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic\ncomputing via a random inactivation process. This process randomly closes some\nneural connections in the RCNN model, realized by the random inactivation\nweight matrix of link input. This releases the computational burden of PCNN,\nmaking it affordable to achieve vast neural connections. Furthermore, the image\nand video processing mechanisms of RCNN are researched. It encodes constant\nstimuli as periodic spike trains and periodic stimuli as chaotic spike trains,\nthe same as biological neural information encoding characteristics. Finally,\nthe RCNN is applicated to image segmentation, fusion, and pulse shape\ndiscrimination subtasks. It is demonstrated to be robust, efficient, and highly\nanti-noised, with outstanding performance in all applications mentioned above.\n", "rewritten_text": "In recent years, there has been a growing interest in improving the efficiency of current neural networks and modeling them after biological neural systems. One popular research direction involves the use of Pulse-Coupled Neural Networks (PCNN) as a model to mimic the computational characteristics of the human brain in the fields of computer vision and neural networks. However, differences persist between PCNN and biological neural systems, such as limited neural connections, high computational costs, and a lack of stochastic properties.\n\nThis study introduces the Random-Coupled Neural Network (RCNN) as a solution to address these challenges encountered in PCNN's neuromorphic computing. The RCNN incorporates a random inactivation process that selectively closes some neural connections within the model through a random inactivation weight matrix of link input. This approach alleviates the computational burden of PCNN, enabling the establishment of a vast number of neural connections.\n\nMoreover, the study explores the image and video processing mechanisms of RCNN. It encodes constant stimuli as periodic spike trains and periodic stimuli as chaotic spike trains, mirroring the information encoding characteristics of biological neural systems. The RCNN is then applied to tasks such as image segmentation, fusion, and pulse shape discrimination, demonstrating robustness, efficiency, and high resistance to noise. The model exhibits outstanding performance across all mentioned applications."}, "1907.0916": {"original_text": "  Facial Micro-Expressions (MEs) are spontaneous, involuntary facial movements\nwhen a person experiences an emotion but deliberately or unconsciously attempts\nto conceal his or her genuine emotions. Recently, ME recognition has attracted\nincreasing attention due to its potential applications such as clinical\ndiagnosis, business negotiation, interrogations, and security. However, it is\nexpensive to build large scale ME datasets, mainly due to the difficulty of\ninducing spontaneous MEs. This limits the application of deep learning\ntechniques which require lots of training data. In this paper, we propose a\nsimple, efficient yet robust descriptor called Extended Local Binary Patterns\non Three Orthogonal Planes (ELBPTOP) for ME recognition. ELBPTOP consists of\nthree complementary binary descriptors: LBPTOP and two novel ones Radial\nDifference LBPTOP (RDLBPTOP) and Angular Difference LBPTOP (ADLBPTOP), which\nexplore the local second order information along the radial and angular\ndirections contained in ME video sequences. ELBPTOP is a novel ME descriptor\ninspired by unique and subtle facial movements. It is computationally efficient\nand only marginally increases the cost of computing LBPTOP, yet is extremely\neffective for ME recognition. In addition, by firstly introducing Whitened\nPrincipal Component Analysis (WPCA) to ME recognition, we can further obtain\nmore compact and discriminative feature representations, then achieve\nsignificantly computational savings. Extensive experimental evaluation on three\npopular spontaneous ME datasets SMIC, CASME II and SAMM show that our proposed\nELBPTOP approach significantly outperforms the previous state-of-the-art on all\nthree single evaluated datasets and achieves promising results on\ncross-database recognition.Our code will be made available.\n", "rewritten_text": "Facial Micro-Expressions (MEs) are spontaneous, involuntary facial movements that occur when a person experiences an emotion but consciously or unconsciously tries to hide their true feelings. The recognition of MEs has recently gained attention for its potential applications in clinical diagnosis, business negotiations, interrogations, and security. However, the creation of large-scale ME datasets is costly due to the challenge of eliciting spontaneous MEs, which hinders the use of deep learning techniques that rely on extensive training data.\n\nIn this study, we introduce a new descriptor called Extended Local Binary Patterns on Three Orthogonal Planes (ELBPTOP) for ME recognition. ELBPTOP comprises three binary descriptors: LBPTOP, Radial Difference LBPTOP (RDLBPTOP), and Angular Difference LBPTOP (ADLBPTOP). These descriptors capture local second-order information along radial and angular directions in ME video sequences. ELBPTOP is a novel descriptor inspired by subtle facial movements, offering computational efficiency without significantly increasing the computational cost of LBPTOP, yet proving highly effective for ME recognition.\n\nFurthermore, we propose the use of Whitened Principal Component Analysis (WPCA) in ME recognition to obtain more compact and discriminative feature representations, leading to substantial computational savings. Extensive experiments conducted on three prominent spontaneous ME datasets (SMIC, CASME II, and SAMM) demonstrate that our ELBPTOP approach outperforms the previous state-of-the-art methods on all three datasets individually and shows promising results in cross-database recognition. Our code will be made publicly available for further research and applications."}, "2402.11431": {"original_text": "  To address the issue of increased triangulation uncertainty caused by\nselecting views with small camera baselines in Structure from Motion (SFM) view\nselection, this paper proposes a robust error-resistant view selection method.\nThe method utilizes a triangulation-based computation to obtain an\nerror-resistant model, which is then used to construct an error-resistant\nmatrix. The sorting results of each row in the error-resistant matrix determine\nthe candidate view set for each view. By traversing the candidate view sets of\nall views and completing the missing views based on the error-resistant matrix,\nthe integrity of 3D reconstruction is ensured. Experimental comparisons between\nthis method and the exhaustive method with the highest accuracy in the COLMAP\nprogram are conducted in terms of average reprojection error and absolute\ntrajectory error in the reconstruction results. The proposed method\ndemonstrates an average reduction of 29.40% in reprojection error accuracy and\n5.07% in absolute trajectory error on the TUM dataset and DTU dataset.\n", "rewritten_text": "This paper introduces a robust error-resistant view selection method to tackle the issue of increased triangulation uncertainty resulting from selecting views with small camera baselines in Structure from Motion (SFM) view selection. The method employs triangulation-based computation to generate an error-resistant model, which is then used to create an error-resistant matrix. The sorting outcomes of each row in the error-resistant matrix determine the candidate view set for each view. By iterating through the candidate view sets of all views and filling in the missing views based on the error-resistant matrix, the method ensures the integrity of 3D reconstruction. Experimental comparisons between this method and the exhaustive method with the highest accuracy in the COLMAP program are carried out, focusing on average reprojection error and absolute trajectory error in the reconstruction results. The proposed method exhibits an average reduction of 29.40% in reprojection error accuracy and 5.07% in absolute trajectory error on both the TUM dataset and DTU dataset."}, "2306.12693": {"original_text": "  This paper gives an Indic-to-Indic (IL-IL) MNMT baseline model for 11 ILs\nimplemented on the Samanantar corpus and analyzed on the Flores-200 corpus. All\nthe models are evaluated using the BLEU score. In addition, the languages are\nclassified under three groups namely East Indo- Aryan (EI), Dravidian (DR), and\nWest Indo-Aryan (WI). The effect of language relatedness on MNMT model\nefficiency is studied. Owing to the presence of large corpora from English (EN)\nto ILs, MNMT IL-IL models using EN as a pivot are also built and examined. To\nachieve this, English- Indic (EN-IL) models are also developed, with and\nwithout the usage of related languages. Results reveal that using related\nlanguages is beneficial for the WI group only, while it is detrimental for the\nEI group and shows an inconclusive effect on the DR group, but it is useful for\nEN-IL models. Thus, related language groups are used to develop pivot MNMT\nmodels. Furthermore, the IL corpora are transliterated from the corresponding\nscripts to a modified ITRANS script, and the best MNMT models from the previous\napproaches are built on the transliterated corpus. It is observed that the\nusage of pivot models greatly improves MNMT baselines with AS-TA achieving the\nminimum BLEU score and PA-HI achieving the maximum score. Among languages, AS,\nML, and TA achieve the lowest BLEU score, whereas HI, PA, and GU perform the\nbest. Transliteration also helps the models with few exceptions. The best\nincrement of scores is observed in ML, TA, and BN and the worst average\nincrement is observed in KN, HI, and PA, across all languages. The best model\nobtained is the PA-HI language pair trained on PAWI transliterated corpus which\ngives 24.29 BLEU.\n", "rewritten_text": "This paper presents an Indic-to-Indic (IL-IL) MNMT baseline model for 11 ILs, implemented on the Samanantar corpus and analyzed on the Flores-200 corpus. The models are evaluated using the BLEU score. The languages are classified into three groups: East Indo-Aryan (EI), Dravidian (DR), and West Indo-Aryan (WI). The study investigates the impact of language relatedness on MNMT model efficiency. Additionally, MNMT IL-IL models using English (EN) as a pivot are constructed and examined due to the availability of large corpora from English to ILs. English-Indic (EN-IL) models are developed, with and without the utilization of related languages.\n\nThe results indicate that using related languages benefits the WI group, while it is disadvantageous for the EI group and shows inconclusive effects on the DR group. However, it is useful for EN-IL models. Consequently, related language groups are leveraged to create pivot MNMT models. Furthermore, the IL corpora are transliterated from their respective scripts to a modified ITRANS script, and the best MNMT models from previous approaches are built on the transliterated corpus.\n\nThe study observes that the utilization of pivot models significantly enhances MNMT baselines, with AS-TA achieving the lowest BLEU score and PA-HI achieving the highest score. Among the languages, AS, ML, and TA have the lowest BLEU scores, while HI, PA, and GU perform the best. Transliteration also aids the models with some exceptions. The most substantial score improvements are seen in ML, TA, and BN, while the smallest average improvements are observed in KN, HI, and PA across all languages. The best model identified is the PA-HI language pair trained on the PAWI transliterated corpus, yielding a BLEU score of 24.29."}, "2403.08487": {"original_text": "  Diffusion models pose risks of privacy breaches and copyright disputes,\nprimarily stemming from the potential utilization of unauthorized data during\nthe training phase. The Training Membership Inference (TMI) task aims to\ndetermine whether a specific sample has been used in the training process of a\ntarget model, representing a critical tool for privacy violation verification.\nHowever, the increased stochasticity inherent in diffusion renders traditional\nshadow-model-based or metric-based methods ineffective when applied to\ndiffusion models. Moreover, existing methods only yield binary classification\nlabels which lack necessary comprehensibility in practical applications. In\nthis paper, we explore a novel perspective for the TMI task by leveraging the\nintrinsic generative priors within the diffusion model. Compared with unseen\nsamples, training samples exhibit stronger generative priors within the\ndiffusion model, enabling the successful reconstruction of substantially\ndegraded training images. Consequently, we propose the Degrade Restore Compare\n(DRC) framework. In this framework, an image undergoes sequential degradation\nand restoration, and its membership is determined by comparing it with the\nrestored counterpart. Experimental results verify that our approach not only\nsignificantly outperforms existing methods in terms of accuracy but also\nprovides comprehensible decision criteria, offering evidence for potential\nprivacy violations.\n", "rewritten_text": "Diffusion models present risks of privacy breaches and copyright disputes, primarily due to the potential use of unauthorized data during the training phase. The Training Membership Inference (TMI) task is designed to ascertain whether a specific sample was included in the training of a target model, serving as a crucial tool for verifying privacy violations. However, the increased stochastic nature inherent in diffusion makes traditional shadow-model-based or metric-based methods ineffective when applied to diffusion models. Additionally, current methods only offer binary classification labels, lacking the necessary clarity for practical applications.\n\nIn this study, we explore a new approach to the TMI task by leveraging the intrinsic generative priors within the diffusion model. Training samples demonstrate stronger generative priors within the diffusion model compared to unseen samples, allowing for the successful reconstruction of significantly degraded training images. As a result, we introduce the Degrade Restore Compare (DRC) framework. Within this framework, an image undergoes sequential degradation and restoration, and its membership is determined by comparing it with the restored version. Experimental results confirm that our approach not only surpasses existing methods in terms of accuracy but also provides clear decision criteria, offering evidence of potential privacy violations."}, "2309.03390": {"original_text": "  In this paper, we seek a new method in designing an iris recognition system.\nIn this method, first the Haar wavelet features are extracted from iris images.\nThe advantage of using these features is the high-speed extraction, as well as\nbeing unique to each iris. Then the back propagation neural network (BPNN) is\nused as a classifier. In this system, the BPNN parallel algorithms and their\nimplementation on GPUs have been used by the aid of CUDA in order to speed up\nthe learning process. Finally, the system performance and the speeding outcomes\nin a way that this algorithm is done in series are presented.\n", "rewritten_text": "This paper introduces a novel approach to designing an iris recognition system. The method involves extracting Haar wavelet features from iris images, known for their high-speed extraction and uniqueness to each iris. These features are then utilized in a backpropagation neural network (BPNN) as a classifier. To enhance the learning process, BPNN parallel algorithms are implemented on GPUs using CUDA technology. The study concludes by presenting the system's performance and the accelerated outcomes achieved through the sequential execution of this algorithm."}, "2311.10436": {"original_text": "  Since their inception, embeddings have become a primary ingredient in many\nflavours of Natural Language Processing (NLP) tasks supplanting earlier types\nof representation. Even though multilingual embeddings have been used for the\nincreasing number of multilingual tasks, due to the scarcity of parallel\ntraining data, low-resource languages such as Sinhala, tend to focus more on\nmonolingual embeddings. Then when it comes to the aforementioned multi-lingual\ntasks, it is challenging to utilize these monolingual embeddings given that\neven if the embedding spaces have a similar geometric arrangement due to an\nidentical training process, the embeddings of the languages considered are not\naligned. This is solved by the embedding alignment task. Even in this,\nhigh-resource language pairs are in the limelight while low-resource languages\nsuch as Sinhala which is in dire need of help seem to have fallen by the\nwayside. In this paper, we try to align Sinhala and English word embedding\nspaces based on available alignment techniques and introduce a benchmark for\nSinhala language embedding alignment. In addition to that, to facilitate the\nsupervised alignment, as an intermediate task, we also introduce\nSinhala-English alignment datasets. These datasets serve as our anchor datasets\nfor supervised word embedding alignment. Even though we do not obtain results\ncomparable to the high-resource languages such as French, German, or Chinese,\nwe believe our work lays the groundwork for more specialized alignment between\nEnglish and Sinhala embeddings.\n", "rewritten_text": "Since their inception, embeddings have become a crucial component in various Natural Language Processing (NLP) tasks, replacing earlier forms of representation. While multilingual embeddings have been utilized for a growing number of multilingual tasks, low-resource languages like Sinhala tend to rely more on monolingual embeddings due to the lack of parallel training data. However, when it comes to the aforementioned multilingual tasks, leveraging these monolingual embeddings poses a challenge. Even if the embedding spaces share a similar geometric arrangement due to the same training process, the embeddings of the languages in question are not aligned. This misalignment is addressed through the embedding alignment task. Unfortunately, high-resource language pairs receive more attention, leaving low-resource languages such as Sinhala in need of assistance overlooked.\n\nIn this paper, we aim to align Sinhala and English word embedding spaces using existing alignment techniques and introduce a benchmark for Sinhala language embedding alignment. Additionally, to facilitate supervised alignment, we introduce Sinhala-English alignment datasets as an intermediate task. These datasets serve as the foundation for our supervised word embedding alignment efforts. While our results may not be on par with high-resource languages like French, German, or Chinese, we believe our work establishes a basis for more specialized alignment between English and Sinhala embeddings."}, "2301.08245": {"original_text": "  Estimating depth from images nowadays yields outstanding results, both in\nterms of in-domain accuracy and generalization. However, we identify two main\nchallenges that remain open in this field: dealing with non-Lambertian\nmaterials and effectively processing high-resolution images. Purposely, we\npropose a novel dataset that includes accurate and dense ground-truth labels at\nhigh resolution, featuring scenes containing several specular and transparent\nsurfaces. Our acquisition pipeline leverages a novel deep space-time stereo\nframework, enabling easy and accurate labeling with sub-pixel precision. The\ndataset is composed of 606 samples collected in 85 different scenes, each\nsample includes both a high-resolution pair (12 Mpx) as well as an unbalanced\nstereo pair (Left: 12 Mpx, Right: 1.1 Mpx), typical of modern mobile devices\nthat mount sensors with different resolutions. Additionally, we provide\nmanually annotated material segmentation masks and 15K unlabeled samples. The\ndataset is composed of a train set and two test sets, the latter devoted to the\nevaluation of stereo and monocular depth estimation networks. Our experiments\nhighlight the open challenges and future research directions in this field.\n", "rewritten_text": "Estimating depth from images in the present day produces exceptional results, demonstrating high accuracy within the specific domain and in terms of generalization. However, two primary challenges persist in this field: addressing non-Lambertian materials and effectively processing high-resolution images. To tackle these challenges, we introduce a new dataset that offers precise and dense ground-truth labels at high resolution. This dataset showcases scenes with various specular and transparent surfaces. Our data collection process utilizes an innovative deep space-time stereo framework, facilitating easy and precise labeling with sub-pixel accuracy.\n\nThe dataset comprises 606 samples gathered from 85 different scenes. Each sample includes a high-resolution pair (12 megapixels) and an imbalanced stereo pair (Left: 12 megapixels, Right: 1.1 megapixels), mimicking the sensor resolutions found in modern mobile devices. Additionally, we provide manually annotated material segmentation masks and 15,000 unlabeled samples. The dataset is divided into a training set and two test sets, with the latter specifically designed for evaluating stereo and monocular depth estimation networks. Through our experiments, we shed light on the existing challenges and outline potential future research directions in this field."}, "1709.00813": {"original_text": "  Various text analysis techniques exist, which attempt to uncover unstructured\ninformation from text. In this work, we explore using statistical dependence\nmeasures for textual classification, representing text as word vectors. Student\nsatisfaction scores on a 3-point scale and their free text comments written\nabout university subjects are used as the dataset. We have compared two textual\nrepresentations: a frequency word representation and term frequency\nrelationship to word vectors, and found that word vectors provide a greater\naccuracy. However, these word vectors have a large number of features which\naggravates the burden of computational complexity. Thus, we explored using a\nnon-linear dependency measure for feature selection by maximizing the\ndependence between the text reviews and corresponding scores. Our quantitative\nand qualitative analysis on a student satisfaction dataset shows that our\napproach achieves comparable accuracy to the full feature vector, while being\nan order of magnitude faster in testing. These text analysis and feature\nreduction techniques can be used for other textual data applications such as\nsentiment analysis.\n", "rewritten_text": "Various text analysis techniques exist to uncover unstructured information from text. In this study, we investigate the use of statistical dependence measures for textual classification by representing text as word vectors. The dataset comprises student satisfaction scores on a 3-point scale and their free-text comments on university subjects. We compared two textual representations: a frequency word representation and term frequency relationship to word vectors, and found that word vectors yield higher accuracy. However, these word vectors have a large number of features, increasing computational complexity. Therefore, we explored using a non-linear dependency measure for feature selection by maximizing the dependence between text reviews and corresponding scores. Our quantitative and qualitative analysis on a student satisfaction dataset demonstrates that our approach achieves comparable accuracy to the full feature vector, while being significantly faster during testing. These text analysis and feature reduction techniques can be applied to other textual data applications, such as sentiment analysis."}, "1704.0803": {"original_text": "  Some lung diseases are related to bronchial airway structures and morphology.\nAlthough airway segmentation from chest CT volumes is an important task in the\ncomputer-aided diagnosis and surgery assistance systems for the chest, complete\n3-D airway structure segmentation is a quite challenging task due to its\ncomplex tree-like structure. In this paper, we propose a new airway\nsegmentation method from 3D chest CT volumes based on volume of interests (VOI)\nusing gradient vector flow (GVF). This method segments the bronchial regions by\napplying the cavity enhancement filter (CEF) to trace the bronchial tree\nstructure from the trachea. It uses the CEF in the VOI to segment each branch.\nAnd a tube-likeness function based on GVF and the GVF magnitude map in each VOI\nare utilized to assist predicting the positions and directions of child\nbranches. By calculating the tube-likeness function based on GVF and the GVF\nmagnitude map, the airway-like candidate structures are identified and their\ncentrelines are extracted. Based on the extracted centrelines, we can detect\nthe branch points of the bifurcations and directions of the airway branches in\nthe next level. At the same time, a leakage detection is performed to avoid the\nleakage by analysing the pixel information and the shape information of airway\ncandidate regions extracted in the VOI. Finally, we unify all of the extracted\nbronchial regions to form an integrated airway tree. Preliminary experiments\nusing four cases of chest CT volumes demonstrated that the proposed method can\nextract more bronchial branches in comparison with other methods.\n", "rewritten_text": "Some lung diseases are associated with the structures and morphology of bronchial airways. Airway segmentation from chest CT volumes is a crucial task in computer-aided diagnosis and surgery assistance systems for the chest. However, complete 3-D airway structure segmentation poses a significant challenge due to its intricate tree-like structure. \n\nIn this study, we introduce a novel airway segmentation method from 3D chest CT volumes based on volume of interests (VOI) using gradient vector flow (GVF). This method segments the bronchial regions by employing the cavity enhancement filter (CEF) to trace the bronchial tree structure from the trachea. The CEF is utilized in the VOI to segment each branch. Additionally, a tube-likeness function based on GVF and the GVF magnitude map in each VOI is used to predict the positions and directions of child branches. \n\nBy calculating the tube-likeness function based on GVF and the GVF magnitude map, airway-like candidate structures are identified, and their centrelines are extracted. This extraction allows for the detection of branch points of bifurcations and directions of airway branches in the subsequent level. Simultaneously, a leakage detection is conducted to prevent leakage by analyzing the pixel and shape information of airway candidate regions extracted in the VOI. \n\nUltimately, all extracted bronchial regions are unified to form an integrated airway tree. Preliminary experiments involving four cases of chest CT volumes have shown that the proposed method can extract more bronchial branches compared to other methods."}, "1906.03249": {"original_text": "  Word embeddings are traditionally trained on a large corpus in an\nunsupervised setting, with no specific design for incorporating domain\nknowledge. This can lead to unsatisfactory performances when training data\noriginate from heterogeneous domains. In this paper, we propose two novel\nmechanisms for domain-aware word embedding training, namely domain indicator\nand domain attention, which integrate domain-specific knowledge into the widely\nused SG and CBOW models, respectively. The two methods are based on a joint\nlearning paradigm and ensure that words in a target domain are intensively\nfocused when trained on a source domain corpus. Qualitative and quantitative\nevaluation confirm the validity and effectiveness of our models. Compared to\nbaseline methods, our method is particularly effective in near-cold-start\nscenarios.\n", "rewritten_text": "In traditional word embedding training, embeddings are typically trained on a large corpus in an unsupervised manner, without a specific framework for incorporating domain knowledge. This approach can result in subpar performance when the training data come from diverse domains. This paper introduces two innovative mechanisms for domain-aware word embedding training: domain indicator and domain attention. These mechanisms integrate domain-specific knowledge into the widely used SG and CBOW models, respectively. Both methods operate on a joint learning paradigm, ensuring that words in a target domain receive focused attention when trained on a source domain corpus. Qualitative and quantitative evaluations validate the efficacy of our models. Our method demonstrates particular effectiveness in near-cold-start scenarios compared to baseline methods."}, "2305.17975": {"original_text": "  Automated assembly of 3D fractures is essential in orthopedics, archaeology,\nand our daily life. This paper presents Jigsaw, a novel framework for\nassembling physically broken 3D objects from multiple pieces. Our approach\nleverages hierarchical features of global and local geometry to match and align\nthe fracture surfaces. Our framework consists of four components: (1) front-end\npoint feature extractor with attention layers, (2) surface segmentation to\nseparate fracture and original parts, (3) multi-parts matching to find\ncorrespondences among fracture surface points, and (4) robust global alignment\nto recover the global poses of the pieces. We show how to jointly learn\nsegmentation and matching and seamlessly integrate feature matching and\nrigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and\nachieve superior performance compared to state-of-the-art methods. Our method\nalso generalizes well to diverse fracture modes, objects, and unseen instances.\nTo the best of our knowledge, this is the first learning-based method designed\nspecifically for 3D fracture assembly over multiple pieces. Our code is\navailable at https://jiaxin-lu.github.io/Jigsaw/.\n", "rewritten_text": "Automated assembly of 3D fractures is crucial in various fields such as orthopedics, archaeology, and everyday life. This paper introduces Jigsaw, a novel framework designed for reconstructing physically broken 3D objects from multiple pieces. Our approach utilizes hierarchical features of global and local geometry to accurately match and align fracture surfaces. The framework comprises four key components: (1) a front-end point feature extractor with attention layers, (2) surface segmentation to distinguish between fracture and original parts, (3) multi-parts matching to establish correspondences among fracture surface points, and (4) robust global alignment to restore the global poses of the pieces. We demonstrate the simultaneous learning of segmentation and matching, as well as the seamless integration of feature matching and rigidity constraints. Evaluation of Jigsaw on the Breaking Bad dataset reveals superior performance compared to state-of-the-art methods. Furthermore, our method exhibits strong generalization capabilities across diverse fracture modes, objects, and unseen instances. To the best of our knowledge, this represents the first learning-based approach specifically tailored for 3D fracture assembly using multiple pieces. The code for our framework is accessible at https://jiaxin-lu.github.io/Jigsaw/."}, "2312.08367": {"original_text": "  In this work, we propose an efficient Video-Language Alignment (ViLA)\nnetwork. Our ViLA model addresses both efficient frame sampling and effective\ncross-modal alignment in a unified way. In our ViLA network, we design a new\nlearnable text-guided Frame-Prompter together with a new cross-modal\ndistillation (QFormer-Distiller) module. Pre-trained large image-language\nmodels have shown promising results on problems such as visual question\nanswering (VQA). However, how to efficiently and effectively sample video\nframes when adapting pre-trained large image-language model to video-language\nalignment is still the major challenge. Compared with prior work, our ViLA\nmodel demonstrates the capability of selecting key frames with critical\ncontents, thus improving the video-language alignment accuracy while reducing\nthe inference latency +3.3% on NExT-QA Temporal with 3.0X speed up). Overall,\nour ViLA network outperforms the state-of-the-art methods on the video\nquestion-answering benchmarks: +4.6% on STAR Interaction, +2.2% on STAR average\nwith 3.0X speed up, ours 2-frames out-perform SeViLA 4-frames on the VLEP\ndataset with 4.2X speed-up. The code will be available at\nhttps://github.com/xijun-cs/ViLA.\n", "rewritten_text": "In this study, we introduce an efficient Video-Language Alignment (ViLA) network. The ViLA model addresses the challenges of both efficient frame sampling and effective cross-modal alignment in a cohesive manner. Within our ViLA network, we have developed a novel learnable text-guided Frame-Prompter in conjunction with a new cross-modal distillation module called QFormer-Distiller. While pre-trained large image-language models have shown promise in tasks like visual question answering (VQA), the efficient and effective sampling of video frames remains a significant challenge when adapting these models to video-language alignment.\n\nIn comparison to previous research, our ViLA model showcases the ability to select key frames containing crucial content, thereby enhancing video-language alignment accuracy while reducing inference latency by 3.3% on NExT-QA Temporal with a 3.0X speedup. Overall, our ViLA network surpasses state-of-the-art methods on video question-answering benchmarks, achieving a 4.6% improvement on STAR Interaction, a 2.2% enhancement on STAR average with a 3.0X speedup, and outperforming SeViLA's 4-frame model with our 2-frame model on the VLEP dataset with a 4.2X speedup.\n\nThe code for our ViLA network will be accessible at https://github.com/xijun-cs/ViLA."}, "1809.10692": {"original_text": "  In this paper, we target the problem of fracture classification from clinical\nX-Ray images towards an automated Computer Aided Diagnosis (CAD) system.\nAlthough primarily dealing with an image classification problem, we argue that\nlocalizing the fracture in the image is crucial to make good class predictions.\nTherefore, we propose and thoroughly analyze several schemes for simultaneous\nfracture localization and classification. We show that using an auxiliary\nlocalization task, in general, improves the classification performance.\nMoreover, it is possible to avoid the need for additional localization\nannotations thanks to recent advancements in weakly-supervised deep learning\napproaches. Among such approaches, we investigate and adapt Spatial\nTransformers (ST), Self-Transfer Learning (STL), and localization from global\npooling layers. We provide a detailed quantitative and qualitative validation\non a dataset of 1347 femur fractures images and report high accuracy with\nregard to inter-expert correlation values reported in the literature. Our\ninvestigations show that i) lesion localization improves the classification\noutcome, ii) weakly-supervised methods improve baseline classification without\nany additional cost, iii) STL guides feature activations and boost performance.\nWe plan to make both the dataset and code available.\n", "rewritten_text": "This paper focuses on the problem of classifying fractures in clinical X-ray images for an automated Computer-Aided Diagnosis (CAD) system. While the main challenge is image classification, we emphasize the importance of accurately localizing the fracture within the image to enhance classification accuracy. To address this, we propose and thoroughly examine various methods for simultaneous fracture localization and classification. Our research demonstrates that incorporating an auxiliary localization task generally enhances classification performance. Furthermore, recent advancements in weakly-supervised deep learning techniques allow us to achieve accurate localization without the need for additional annotations. We explore and adapt Spatial Transformers (ST), Self-Transfer Learning (STL), and localization from global pooling layers among these approaches. Through detailed quantitative and qualitative validation on a dataset comprising 1347 femur fracture images, we achieve high accuracy compared to inter-expert correlation values reported in existing literature. Our findings indicate that lesion localization positively impacts classification results, weakly-supervised methods enhance baseline classification without added costs, and STL optimizes feature activations to improve performance. We intend to release both the dataset and code for further research and application."}, "2309.16039": {"original_text": "  We present a series of long-context LLMs that support effective context\nwindows of up to 32,768 tokens. Our model series are built through continual\npretraining from Llama 2 with longer training sequences and on a dataset where\nlong texts are upsampled. We perform extensive evaluation on language modeling,\nsynthetic context probing tasks, and a wide range of research benchmarks. On\nresearch benchmarks, our models achieve consistent improvements on most regular\ntasks and significant improvements on long-context tasks over Llama 2. Notably,\nwith a cost-effective instruction tuning procedure that does not require\nhuman-annotated long instruction data, the 70B variant can already surpass\ngpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.\nAlongside these results, we provide an in-depth analysis on the individual\ncomponents of our method. We delve into Llama's position encodings and discuss\nits limitation in modeling long dependencies. We also examine the impact of\nvarious design choices in the pretraining process, including the data mix and\nthe training curriculum of sequence lengths -- our ablation experiments suggest\nthat having abundant long texts in the pretrain dataset is not the key to\nachieving strong performance, and we empirically verify that long context\ncontinual pretraining is more efficient and similarly effective compared to\npretraining from scratch with long sequences.\n", "rewritten_text": "We introduce a series of long-context Large Language Models (LLMs) that are capable of effectively processing context windows containing up to 32,768 tokens. These models are developed by continuously pretraining from Llama 2 using longer training sequences and a dataset that includes upsampled long texts. Our evaluation includes thorough assessments in language modeling, synthetic context probing tasks, and a diverse set of research benchmarks. Across these benchmarks, our models consistently outperform Llama 2 on most standard tasks and demonstrate significant advancements on tasks requiring long-context understanding. Notably, our 70B variant surpasses the overall performance of gpt-3.5-turbo-16k on a range of long-context tasks using a cost-effective instruction tuning procedure that does not rely on human-annotated long instruction data.\n\nIn addition to presenting these results, we conduct a detailed analysis of the individual components of our approach. We investigate Llama's position encodings and highlight their limitations in modeling long dependencies. Furthermore, we explore the impact of various design choices in the pretraining process, such as the data mix and the training curriculum of sequence lengths. Our ablation experiments indicate that having a surplus of long texts in the pretraining dataset is not the primary factor in achieving strong performance. We empirically confirm that continual pretraining with long context is more efficient and equally effective compared to starting pretraining from scratch with long sequences."}, "2403.02211": {"original_text": "  Popular methods usually use a degradation model in a supervised way to learn\na watermark removal model. However, it is true that reference images are\ndifficult to obtain in the real world, as well as collected images by cameras\nsuffer from noise. To overcome these drawbacks, we propose a perceptive\nself-supervised learning network for noisy image watermark removal (PSLNet) in\nthis paper. PSLNet depends on a parallel network to remove noise and\nwatermarks. The upper network uses task decomposition ideas to remove noise and\nwatermarks in sequence. The lower network utilizes the degradation model idea\nto simultaneously remove noise and watermarks. Specifically, mentioned paired\nwatermark images are obtained in a self supervised way, and paired noisy images\n(i.e., noisy and reference images) are obtained in a supervised way. To enhance\nthe clarity of obtained images, interacting two sub-networks and fusing\nobtained clean images are used to improve the effects of image watermark\nremoval in terms of structural information and pixel enhancement. Taking into\ntexture information account, a mixed loss uses obtained images and features to\nachieve a robust model of noisy image watermark removal. Comprehensive\nexperiments show that our proposed method is very effective in comparison with\npopular convolutional neural networks (CNNs) for noisy image watermark removal.\nCodes can be obtained at https://github.com/hellloxiaotian/PSLNet.\n", "rewritten_text": "Commonly, popular methods utilize a supervised degradation model to learn a watermark removal model. However, obtaining reference images in the real world can be challenging, and images captured by cameras often contain noise. To address these challenges, this paper introduces a perceptive self-supervised learning network for noisy image watermark removal, known as PSLNet. PSLNet relies on a parallel network for noise and watermark removal. The upper network employs task decomposition principles to sequentially remove noise and watermarks, while the lower network leverages the degradation model concept to simultaneously eliminate noise and watermarks. Paired watermark images are acquired through self-supervised learning, while paired noisy images (comprising noisy and reference images) are obtained through supervised learning. To enhance image clarity, the approach involves the interaction of two sub-networks and the fusion of clean images to improve the effectiveness of image watermark removal in terms of structural information and pixel enhancement. By considering texture information, a mixed loss function is employed using obtained images and features to establish a robust model for noisy image watermark removal. Extensive experiments demonstrate the superior efficacy of our proposed method compared to popular convolutional neural networks (CNNs) for noisy image watermark removal. The source code is available at https://github.com/hellloxiaotian/PSLNet."}, "2104.09386": {"original_text": "  Mapping a single exposure low dynamic range (LDR) image into a high dynamic\nrange (HDR) is considered among the most strenuous image to image translation\ntasks due to exposure-related missing information. This study tackles the\nchallenges of single-shot LDR to HDR mapping by proposing a novel two-stage\ndeep network. Notably, our proposed method aims to reconstruct an HDR image\nwithout knowing hardware information, including camera response function (CRF)\nand exposure settings. Therefore, we aim to perform image enhancement task like\ndenoising, exposure correction, etc., in the first stage. Additionally, the\nsecond stage of our deep network learns tone mapping and bit-expansion from a\nconvex set of data samples. The qualitative and quantitative comparisons\ndemonstrate that the proposed method can outperform the existing LDR to HDR\nworks with a marginal difference. Apart from that, we collected an LDR image\ndataset incorporating different camera systems. The evaluation with our\ncollected real-world LDR images illustrates that the proposed method can\nreconstruct plausible HDR images without presenting any visual artefacts. Code\navailable: https://github. com/sharif-apu/twostageHDR_NTIRE21.\n", "rewritten_text": "Mapping a single exposure low dynamic range (LDR) image into a high dynamic range (HDR) is considered one of the most challenging tasks in image translation due to the missing information related to exposure. This study addresses the difficulties of converting single-shot LDR to HDR by introducing a novel two-stage deep network. Our method aims to reconstruct an HDR image without requiring specific hardware information, such as the camera response function (CRF) and exposure settings. In the first stage, we focus on tasks like denoising and exposure correction to enhance the image. The second stage of our deep network is designed to learn tone mapping and bit-expansion from a set of data samples. Qualitative and quantitative comparisons show that our method can surpass existing LDR to HDR techniques with only a slight difference. Additionally, we have compiled an LDR image dataset that includes various camera systems. Evaluation using our real-world LDR images demonstrates that our method can generate realistic HDR images without introducing any visual artifacts. The code is available at: https://github.com/sharif-apu/twostageHDR_NTIRE21."}, "2409.09170": {"original_text": "  The diagnosis and treatment of individuals with communication disorders\noffers many opportunities for the application of speech technology, but\nresearch so far has not adequately considered: the diversity of conditions, the\nrole of pragmatic deficits, and the challenges of limited data. This paper\nexplores how a general-purpose model of perceived pragmatic similarity may\novercome these limitations. It explains how it might support several use cases\nfor clinicians and clients, and presents evidence that a simple model can\nprovide value, and in particular can capture utterance aspects that are\nrelevant to diagnoses of autism and specific language impairment.\n", "rewritten_text": "The diagnosis and treatment of individuals with communication disorders present numerous opportunities for applying speech technology. However, current research has not sufficiently addressed the diversity of conditions, the significance of pragmatic deficits, and the difficulties posed by limited data. This paper delves into how a universal model of perceived pragmatic similarity could address these shortcomings. It elucidates how such a model could assist clinicians and clients in various scenarios and provides evidence that a straightforward model can offer value, particularly in capturing speech elements crucial for diagnosing autism and specific language impairment."}, "2309.04462": {"original_text": "  Real-world application of chest X-ray abnormality classification requires\ndealing with several challenges: (i) limited training data; (ii) training and\nevaluation sets that are derived from different domains; and (iii) classes that\nappear during training may have partial overlap with classes of interest during\nevaluation. To address these challenges, we present an integrated framework\ncalled Generalized Cross-Domain Multi-Label Few-Shot Learning (GenCDML-FSL).\nThe framework supports overlap in classes during training and evaluation,\ncross-domain transfer, adopts meta-learning to learn using few training\nsamples, and assumes each chest X-ray image is either normal or associated with\none or more abnormalities. Furthermore, we propose Generalized Episodic\nTraining (GenET), a training strategy that equips models to operate with\nmultiple challenges observed in the GenCDML-FSL scenario. Comparisons with\nwell-established methods such as transfer learning, hybrid transfer learning,\nand multi-label meta-learning on multiple datasets show the superiority of our\napproach.\n", "rewritten_text": "The real-world application of classifying chest X-ray abnormalities presents several challenges, including limited training data, training and evaluation sets from different domains, and potential overlap between classes during training and evaluation. To address these challenges, we introduce an integrated framework called Generalized Cross-Domain Multi-Label Few-Shot Learning (GenCDML-FSL). This framework accommodates class overlap, cross-domain transfer, utilizes meta-learning with few training samples, and assumes each chest X-ray image is either normal or associated with one or more abnormalities. Additionally, we propose Generalized Episodic Training (GenET) as a training strategy to prepare models for the challenges encountered in the GenCDML-FSL scenario. Comparative analyses with established methods such as transfer learning, hybrid transfer learning, and multi-label meta-learning across multiple datasets demonstrate the superiority of our approach."}, "1811.071": {"original_text": "  Few-shot deep learning is a topical challenge area for scaling visual\nrecognition to open ended growth of unseen new classes with limited labeled\nexamples. A promising approach is based on metric learning, which trains a deep\nembedding to support image similarity matching. Our insight is that effective\ngeneral purpose matching requires non-linear comparison of features at multiple\nabstraction levels. We thus propose a new deep comparison network comprised of\nembedding and relation modules that learn multiple non-linear distance metrics\nbased on different levels of features simultaneously. Furthermore, to reduce\nover-fitting and enable the use of deeper embeddings, we represent images as\ndistributions rather than vectors via learning parameterized Gaussian noise\nregularization. The resulting network achieves excellent performance on both\nminiImageNet and tieredImageNet.\n", "rewritten_text": "Few-shot deep learning presents a significant challenge in scaling visual recognition to accommodate the continuous growth of unseen new classes with limited labeled examples. A promising approach involves metric learning, which involves training a deep embedding to facilitate image similarity matching. Our key insight is that effective general-purpose matching necessitates non-linear comparison of features at various abstraction levels. Therefore, we propose a novel deep comparison network that consists of embedding and relation modules, which are designed to learn multiple non-linear distance metrics based on different levels of features simultaneously. Additionally, to mitigate overfitting and enable the utilization of deeper embeddings, we represent images as distributions rather than vectors by employing parameterized Gaussian noise regularization. The resulting network demonstrates outstanding performance on both miniImageNet and tieredImageNet datasets."}, "2306.07279": {"original_text": "  We introduce Cap3D, an automatic approach for generating descriptive text for\n3D objects. This approach utilizes pretrained models from image captioning,\nimage-text alignment, and LLM to consolidate captions from multiple views of a\n3D asset, completely side-stepping the time-consuming and costly process of\nmanual annotation. We apply Cap3D to the recently introduced large-scale 3D\ndataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted\nusing 41k human annotations from the same dataset, demonstrates that Cap3D\nsurpasses human-authored descriptions in terms of quality, cost, and speed.\nThrough effective prompt engineering, Cap3D rivals human performance in\ngenerating geometric descriptions on 17k collected annotations from the ABO\ndataset. Finally, we finetune Text-to-3D models on Cap3D and human captions,\nand show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E,\nand DreamFusion.\n", "rewritten_text": "Introducing Cap3D, an automated approach for generating descriptive text for 3D objects. This method leverages pretrained models from image captioning, image-text alignment, and LLM to combine captions from various perspectives of a 3D asset, eliminating the need for labor-intensive and expensive manual annotation processes. Cap3D was applied to Objaverse, a recently introduced large-scale 3D dataset, resulting in 660k 3D-text pairs. Evaluation using 41k human annotations from the same dataset revealed that Cap3D outperforms human-generated descriptions in terms of quality, cost, and speed. By employing effective prompt engineering, Cap3D achieves performance comparable to humans in producing geometric descriptions based on 17k annotations from the ABO dataset. Furthermore, we fine-tuned Text-to-3D models using Cap3D and human captions, demonstrating Cap3D's superior performance over state-of-the-art models such as Point-E, Shape-E, and DreamFusion."}, "2311.08469": {"original_text": "  Language technologies that accurately model the dynamics of events must\nperform commonsense reasoning. Existing work evaluating commonsense reasoning\nfocuses on making inferences about common, everyday situations. To instead\ninvestigate the ability to model unusual, unexpected, and unlikely situations,\nwe explore the task of uncommonsense abductive reasoning. Given a piece of\ncontext with an unexpected outcome, this task requires reasoning abductively to\ngenerate an explanation that makes the unexpected outcome more likely in the\ncontext. To this end, we curate and release a new English language corpus\ncalled UNcommonsense. We characterize the performance differences between human\nexplainers and the best-performing large language models, finding that\nmodel-enhanced human-written explanations achieve the highest quality by\ntrading off between specificity and diversity. Finally, we experiment with\nseveral imitation learning algorithms to train open and accessible language\nmodels on this task. When compared with the vanilla supervised fine-tuning\napproach, these methods consistently reduce lose rates on both common and\nuncommonsense abductive reasoning judged by human evaluators.\n", "rewritten_text": "Language technologies that accurately model event dynamics must incorporate commonsense reasoning. Current research on commonsense reasoning primarily focuses on making inferences about typical, everyday situations. However, to explore the capability of modeling unusual, unexpected, and improbable scenarios, we delve into the realm of uncommonsense abductive reasoning. This task involves generating explanations through abductive reasoning when presented with a context that yields an unexpected outcome, thereby making the unexpected result more plausible within that context.\n\nIn pursuit of this objective, we have curated and introduced a new English language corpus named UNcommonsense. By comparing the performance of human explainers with that of the top-performing large language models, we have observed that model-enhanced human-written explanations achieve the highest quality by balancing specificity and diversity. Furthermore, we have conducted experiments using various imitation learning algorithms to train accessible language models on this task. These methods consistently outperform the conventional supervised fine-tuning approach, as evidenced by reduced error rates in both common and uncommonsense abductive reasoning, as evaluated by human judges."}, "2305.1374": {"original_text": "  Tense inconsistency frequently occurs in machine translation. However, there\nare few criteria to assess the model's mastery of tense prediction from a\nlinguistic perspective. In this paper, we present a parallel tense test set,\ncontaining French-English 552 utterances. We also introduce a corresponding\nbenchmark, tense prediction accuracy. With the tense test set and the\nbenchmark, researchers are able to measure the tense consistency performance of\nmachine translation systems for the first time.\n", "rewritten_text": "Tense inconsistency is a common issue in machine translation, yet there are limited criteria available to evaluate the model's proficiency in predicting tense from a linguistic standpoint. This paper introduces a parallel tense test set comprising 552 French-English utterances. Additionally, a corresponding benchmark, tense prediction accuracy, is presented. By utilizing the tense test set and benchmark, researchers can now assess the tense consistency performance of machine translation systems for the first time."}, "2410.19294": {"original_text": "  Vision-language models, such as CLIP, have shown impressive generalization\ncapacities when using appropriate text descriptions. While optimizing prompts\non downstream labeled data has proven effective in improving performance, these\nmethods entail labor costs for annotations and are limited by their quality.\nAdditionally, since CLIP is pre-trained on highly imbalanced Web-scale data, it\nsuffers from inherent label bias that leads to suboptimal performance. To\ntackle the above challenges, we propose a label-Free prompt distribution\nlearning and bias correction framework, dubbed as **Frolic**, which boosts\nzero-shot performance without the need for labeled data. Specifically, our\nFrolic learns distributions over prompt prototypes to capture diverse visual\nrepresentations and adaptively fuses these with the original CLIP through\nconfidence matching. This fused model is further enhanced by correcting label\nbias via a label-free logit adjustment. Notably, our method is not only\ntraining-free but also circumvents the necessity for hyper-parameter tuning.\nExtensive experimental results across 16 datasets demonstrate the efficacy of\nour approach, particularly outperforming the state-of-the-art by an average of\n$2.6\\%$ on 10 datasets with CLIP ViT-B/16 and achieving an average margin of\n$1.5\\%$ on ImageNet and its five distribution shifts with CLIP ViT-B/16. Codes\nare available in https://github.com/zhuhsingyuu/Frolic.\n", "rewritten_text": "Vision-language models like CLIP have demonstrated impressive generalization capabilities when provided with appropriate text descriptions. While optimizing prompts on labeled data downstream has proven effective in enhancing performance, these methods come with the drawback of requiring labor-intensive annotations and being constrained by their quality. Moreover, due to CLIP being pre-trained on highly imbalanced Web-scale data, it suffers from inherent label bias that results in suboptimal performance.\n\nTo address the aforementioned challenges, we introduce a novel framework called **Frolic**, which focuses on label-free prompt distribution learning and bias correction. This framework enhances zero-shot performance without the need for labeled data. Specifically, Frolic learns distributions over prompt prototypes to encompass diverse visual representations and then adaptively integrates these with the original CLIP model through confidence matching. Furthermore, the fused model is refined by correcting label bias through a label-free logit adjustment. Notably, our method is not only training-free but also eliminates the requirement for hyper-parameter tuning.\n\nExtensive experimental results across 16 datasets showcase the effectiveness of our approach, surpassing the state-of-the-art by an average of $2.6\\% on 10 datasets using CLIP ViT-B/16 and achieving an average margin of $1.5\\% on ImageNet and its five distribution shifts with CLIP ViT-B/16. The source code is available at https://github.com/zhuhsingyuu/Frolic."}, "1809.01219": {"original_text": "  A novel graph-to-tree conversion mechanism called the deep-tree generation\n(DTG) algorithm is first proposed to predict text data represented by graphs.\nThe DTG method can generate a richer and more accurate representation for nodes\n(or vertices) in graphs. It adds flexibility in exploring the vertex\nneighborhood information to better reflect the second order proximity and\nhomophily equivalence in a graph. Then, a Deep-Tree Recursive Neural Network\n(DTRNN) method is presented and used to classify vertices that contains text\ndata in graphs. To demonstrate the effectiveness of the DTRNN method, we apply\nit to three real-world graph datasets and show that the DTRNN method\noutperforms several state-of-the-art benchmarking methods.\n", "rewritten_text": "The novel deep-tree generation (DTG) algorithm introduces a graph-to-tree conversion mechanism for predicting text data represented by graphs. This method enhances the representation of nodes (or vertices) in graphs, providing a more comprehensive and accurate depiction. By incorporating flexibility in exploring vertex neighborhood information, the DTG method effectively captures second-order proximity and homophily equivalence within the graph structure. Subsequently, a Deep-Tree Recursive Neural Network (DTRNN) approach is introduced for classifying vertices containing text data in graphs. Through experimentation on three real-world graph datasets, the effectiveness of the DTRNN method is demonstrated, showcasing superior performance compared to various state-of-the-art benchmarking methods."}, "2403.09636": {"original_text": "  Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.\n", "rewritten_text": "Transformers have become the foundation of large language models (LLMs). However, the efficiency of generation is hindered by the necessity to store a cache of key-value representations for previous tokens in memory, with the size increasing linearly with the input sequence length and batch size. To address this issue, we introduce Dynamic Memory Compression (DMC), a method for compressing the key-value cache online during inference. Notably, the model is trained to apply varying compression ratios across different heads and layers. By integrating DMC into pre-trained LLMs like Llama 2 (7B, 13B, and 70B), we achieve a significant throughput increase of up to 7x during auto-regressive inference on an NVIDIA H100 GPU. DMC is implemented through continued pre-training on a minimal portion of the original data without introducing additional parameters. It maintains the original downstream performance while achieving up to 4x cache compression, surpassing up-trained grouped-query attention (GQA) and key-value eviction policies (H$_2$O, TOVA). Combining GQA and DMC can lead to further improvements. Therefore, DMC can seamlessly replace key-value caching in existing LLMs to accommodate longer contexts and larger batches within a given memory constraint."}, "2108.06536": {"original_text": "  We address the problem of generalized zero-shot semantic segmentation (GZS3)\npredicting pixel-wise semantic labels for seen and unseen classes. Most GZS3\nmethods adopt a generative approach that synthesizes visual features of unseen\nclasses from corresponding semantic ones (e.g., word2vec) to train novel\nclassifiers for both seen and unseen classes. Although generative methods show\ndecent performance, they have two limitations: (1) the visual features are\nbiased towards seen classes; (2) the classifier should be retrained whenever\nnovel unseen classes appear. We propose a discriminative approach to address\nthese limitations in a unified framework. To this end, we leverage visual and\nsemantic encoders to learn a joint embedding space, where the semantic encoder\ntransforms semantic features to semantic prototypes that act as centers for\nvisual features of corresponding classes. Specifically, we introduce\nboundary-aware regression (BAR) and semantic consistency (SC) losses to learn\ndiscriminative features. Our approach to exploiting the joint embedding space,\ntogether with BAR and SC terms, alleviates the seen bias problem. At test time,\nwe avoid the retraining process by exploiting semantic prototypes as a\nnearest-neighbor (NN) classifier. To further alleviate the bias problem, we\nalso propose an inference technique, dubbed Apollonius calibration (AC), that\nmodulates the decision boundary of the NN classifier to the Apollonius circle\nadaptively. Experimental results demonstrate the effectiveness of our\nframework, achieving a new state of the art on standard benchmarks.\n", "rewritten_text": "We tackle the challenge of generalized zero-shot semantic segmentation (GZS3), which involves predicting pixel-wise semantic labels for both seen and unseen classes. While most GZS3 methods typically take a generative approach by generating visual features of unseen classes from corresponding semantic features (e.g., word2vec) to train classifiers for both seen and unseen classes, these methods have two main limitations: (1) the visual features tend to be biased towards seen classes, and (2) the classifier needs to be retrained whenever new unseen classes are introduced. \n\nTo overcome these limitations, we propose a discriminative approach within a unified framework. Our method leverages visual and semantic encoders to learn a joint embedding space. In this space, the semantic encoder transforms semantic features into semantic prototypes that serve as centers for the visual features of corresponding classes. We introduce boundary-aware regression (BAR) and semantic consistency (SC) losses to learn discriminative features. By exploiting the joint embedding space and incorporating BAR and SC terms, we mitigate the bias towards seen classes. \n\nDuring testing, we eliminate the need for retraining by utilizing semantic prototypes as a nearest-neighbor (NN) classifier. Additionally, we introduce an inference technique called Apollonius calibration (AC) to adaptively modulate the decision boundary of the NN classifier to the Apollonius circle, further reducing bias. Experimental results demonstrate the effectiveness of our framework, achieving a new state-of-the-art performance on standard benchmarks."}, "1907.10815": {"original_text": "  Improvements in data-capture and face modeling techniques have enabled us to\ncreate high-fidelity realistic face models. However, driving these realistic\nface models requires special input data, e.g. 3D meshes and unwrapped textures.\nAlso, these face models expect clean input data taken under controlled lab\nenvironments, which is very different from data collected in the wild. All\nthese constraints make it challenging to use the high-fidelity models in\ntracking for commodity cameras. In this paper, we propose a self-supervised\ndomain adaptation approach to enable the animation of high-fidelity face models\nfrom a commodity camera. Our approach first circumvents the requirement for\nspecial input data by training a new network that can directly drive a face\nmodel just from a single 2D image. Then, we overcome the domain mismatch\nbetween lab and uncontrolled environments by performing self-supervised domain\nadaptation based on \"consecutive frame texture consistency\" based on the\nassumption that the appearance of the face is consistent over consecutive\nframes, avoiding the necessity of modeling the new environment such as lighting\nor background. Experiments show that we are able to drive a high-fidelity face\nmodel to perform complex facial motion from a cellphone camera without\nrequiring any labeled data from the new domain.\n", "rewritten_text": "Advancements in data-capture and face modeling techniques have facilitated the creation of highly realistic face models. However, operating these lifelike face models necessitates specific input data, such as 3D meshes and unwrapped textures. Furthermore, these face models rely on pristine input data acquired in controlled lab settings, which markedly differs from data gathered in uncontrolled environments. These constraints pose challenges in utilizing high-fidelity models for tracking with commodity cameras. \n\nIn this study, we introduce a self-supervised domain adaptation approach to enable the animation of high-fidelity face models using a commodity camera. Our approach initially eliminates the need for specialized input data by training a novel network capable of directly animating a face model from a single 2D image. Subsequently, we address the domain disparity between lab-controlled and uncontrolled environments by implementing self-supervised domain adaptation based on \"consecutive frame texture consistency.\" This method operates under the assumption that facial appearance remains consistent across successive frames, thereby obviating the need to model new environmental factors like lighting or background. \n\nExperimental results demonstrate our ability to animate a high-fidelity face model to execute intricate facial movements using a cellphone camera, without the necessity of labeled data from the new domain."}, "2311.10651": {"original_text": "  Analysis of the 3D Texture is indispensable for various tasks, such as\nretrieval, segmentation, classification, and inspection of sculptures, knitted\nfabrics, and biological tissues. A 3D texture is a locally repeated surface\nvariation independent of the surface's overall shape and can be determined\nusing the local neighborhood and its characteristics. Existing techniques\ntypically employ computer vision techniques that analyze a 3D mesh globally,\nderive features, and then utilize the obtained features for retrieval or\nclassification. Several traditional and learning-based methods exist in the\nliterature, however, only a few are on 3D texture, and nothing yet, to the best\nof our knowledge, on the unsupervised schemes. This paper presents an original\nframework for the unsupervised segmentation of the 3D texture on the mesh\nmanifold. We approach this problem as binary surface segmentation, partitioning\nthe mesh surface into textured and non-textured regions without prior\nannotation. We devise a mutual transformer-based system comprising a label\ngenerator and a cleaner. The two models take geometric image representations of\nthe surface mesh facets and label them as texture or non-texture across an\niterative mutual learning scheme. Extensive experiments on three publicly\navailable datasets with diverse texture patterns demonstrate that the proposed\nframework outperforms standard and SOTA unsupervised techniques and competes\nreasonably with supervised methods.\n", "rewritten_text": "Analysis of 3D textures is essential for various tasks, including retrieval, segmentation, classification, and inspection of sculptures, knitted fabrics, and biological tissues. A 3D texture refers to a locally repeated surface variation that is independent of the overall shape of the surface and can be identified by examining the local neighborhood and its characteristics. Current techniques typically utilize computer vision methods to analyze 3D meshes globally, extract features, and then apply these features for retrieval or classification purposes.\n\nWhile the literature contains various traditional and learning-based approaches, only a few focus specifically on 3D textures, and as far as we are aware, none address unsupervised schemes. This paper introduces an innovative framework for unsupervised segmentation of 3D textures on mesh surfaces. The problem is approached as binary surface segmentation, dividing the mesh surface into textured and non-textured regions without prior annotations.\n\nOur approach involves a mutual transformer-based system consisting of a label generator and a cleaner. These models analyze geometric image representations of the surface mesh facets and classify them as either texture or non-texture through an iterative mutual learning process. Extensive experiments conducted on three publicly available datasets with diverse texture patterns demonstrate that the proposed framework surpasses standard and state-of-the-art unsupervised techniques and performs competitively with supervised methods."}, "1906.05416": {"original_text": "  We introduce a novel method of generating synthetic question answering\ncorpora by combining models of question generation and answer extraction, and\nby filtering the results to ensure roundtrip consistency. By pretraining on the\nresulting corpora we obtain significant improvements on SQuAD2 and NQ,\nestablishing a new state-of-the-art on the latter. Our synthetic data\ngeneration models, for both question generation and answer extraction, can be\nfully reproduced by finetuning a publicly available BERT model on the\nextractive subsets of SQuAD2 and NQ. We also describe a more powerful variant\nthat does full sequence-to-sequence pretraining for question generation,\nobtaining exact match and F1 at less than 0.1% and 0.4% from human performance\non SQuAD2.\n", "rewritten_text": "We present a new approach to creating synthetic question answering datasets by merging models for question generation and answer extraction, and then refining the outcomes to ensure consistency throughout. Through pretraining on these datasets, we achieve significant enhancements on SQuAD2 and NQ, setting a new benchmark on the latter. Our models for generating synthetic data, encompassing both question generation and answer extraction, can be entirely replicated by fine-tuning a publicly accessible BERT model on the extractive subsets of SQuAD2 and NQ. Additionally, we outline a more robust version that conducts full sequence-to-sequence pretraining for question generation, achieving exact match and F1 scores within 0.1% and 0.4% of human performance on SQuAD2."}, "2404.02393": {"original_text": "  While multilingual machine translation (MNMT) systems hold substantial\npromise, they also have security vulnerabilities. Our research highlights that\nMNMT systems can be susceptible to a particularly devious style of backdoor\nattack, whereby an attacker injects poisoned data into a low-resource language\npair to cause malicious translations in other languages, including\nhigh-resource languages. Our experimental results reveal that injecting less\nthan 0.01% poisoned data into a low-resource language pair can achieve an\naverage 20% attack success rate in attacking high-resource language pairs. This\ntype of attack is of particular concern, given the larger attack surface of\nlanguages inherent to low-resource settings. Our aim is to bring attention to\nthese vulnerabilities within MNMT systems with the hope of encouraging the\ncommunity to address security concerns in machine translation, especially in\nthe context of low-resource languages.\n", "rewritten_text": "Our research emphasizes that while multilingual machine translation (MNMT) systems offer significant potential, they also possess security vulnerabilities. Specifically, MNMT systems can be vulnerable to a sophisticated backdoor attack in which an attacker inserts poisoned data into a low-resource language pair to generate malicious translations in other languages, including those with high resources. Our experimental findings demonstrate that introducing less than 0.01% poisoned data into a low-resource language pair can result in an average 20% success rate in attacking high-resource language pairs. This type of attack is particularly worrisome due to the broader attack surface of languages in low-resource environments. Our objective is to raise awareness about these vulnerabilities in MNMT systems in the hopes of prompting the community to address security issues in machine translation, particularly in the realm of low-resource languages."}, "2104.00556": {"original_text": "  Two-view structure-from-motion (SfM) is the cornerstone of 3D reconstruction\nand visual SLAM. Existing deep learning-based approaches formulate the problem\nby either recovering absolute pose scales from two consecutive frames or\npredicting a depth map from a single image, both of which are ill-posed\nproblems. In contrast, we propose to revisit the problem of deep two-view SfM\nby leveraging the well-posedness of the classic pipeline. Our method consists\nof 1) an optical flow estimation network that predicts dense correspondences\nbetween two frames; 2) a normalized pose estimation module that computes\nrelative camera poses from the 2D optical flow correspondences, and 3) a\nscale-invariant depth estimation network that leverages epipolar geometry to\nreduce the search space, refine the dense correspondences, and estimate\nrelative depth maps. Extensive experiments show that our method outperforms all\nstate-of-the-art two-view SfM methods by a clear margin on KITTI depth, KITTI\nVO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth\nestimation.\n", "rewritten_text": "The cornerstone of 3D reconstruction and visual SLAM is the two-view structure-from-motion (SfM) technique. Current deep learning-based methods tackle this problem by either recovering absolute pose scales from two consecutive frames or predicting a depth map from a single image, both of which are considered ill-posed problems. In contrast, our approach involves revisiting the deep two-view SfM problem by capitalizing on the well-posed nature of the traditional pipeline. Our method comprises: 1) an optical flow estimation network that forecasts dense correspondences between two frames; 2) a normalized pose estimation module that calculates relative camera poses from the 2D optical flow correspondences; and 3) a scale-invariant depth estimation network that utilizes epipolar geometry to narrow down the search space, enhance the dense correspondences, and estimate relative depth maps. Extensive experiments demonstrate that our method significantly outperforms all existing two-view SfM methods on various datasets including KITTI depth, KITTI VO, MVS, Scenes11, and SUN3D in terms of both relative pose and depth estimation."}, "2309.03903": {"original_text": "  Training data for video segmentation are expensive to annotate. This impedes\nextensions of end-to-end algorithms to new video segmentation tasks, especially\nin large-vocabulary settings. To 'track anything' without training on video\ndata for every individual task, we develop a decoupled video segmentation\napproach (DEVA), composed of task-specific image-level segmentation and\nclass/task-agnostic bi-directional temporal propagation. Due to this design, we\nonly need an image-level model for the target task (which is cheaper to train)\nand a universal temporal propagation model which is trained once and\ngeneralizes across tasks. To effectively combine these two modules, we use\nbi-directional propagation for (semi-)online fusion of segmentation hypotheses\nfrom different frames to generate a coherent segmentation. We show that this\ndecoupled formulation compares favorably to end-to-end approaches in several\ndata-scarce tasks including large-vocabulary video panoptic segmentation,\nopen-world video segmentation, referring video segmentation, and unsupervised\nvideo object segmentation. Code is available at:\nhttps://hkchengrex.github.io/Tracking-Anything-with-DEVA\n", "rewritten_text": "Annotating training data for video segmentation is costly, which hinders the expansion of end-to-end algorithms to new video segmentation tasks, particularly in scenarios with a large vocabulary. To address the challenge of tracking various objects without the need to train on video data for each specific task, we introduce a decoupled video segmentation approach called DEVA. DEVA consists of task-specific image-level segmentation and a class/task-agnostic bi-directional temporal propagation. This design allows us to utilize a more affordable image-level model for the target task and a universal temporal propagation model that is trained once and can be applied across different tasks. By effectively integrating these two modules using bi-directional propagation, we can fuse segmentation hypotheses from various frames in a (semi-)online manner to produce a coherent segmentation. Our study demonstrates that this decoupled approach outperforms end-to-end methods in several data-scarce tasks, such as large-vocabulary video panoptic segmentation, open-world video segmentation, referring video segmentation, and unsupervised video object segmentation. The code for DEVA is accessible at: https://hkchengrex.github.io/Tracking-Anything-with-DEVA"}, "2312.08869": {"original_text": "  We are living in a world surrounded by diverse and \"smart\" devices with rich\nmodalities of sensing ability. Conveniently capturing the interactions between\nus humans and these objects remains far-reaching. In this paper, we present\nI'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the\nhuman and object in a novel setting: using a minimal amount of RGB camera and\nobject-mounted Inertial Measurement Unit (IMU). It combines general motion\ninference and category-aware refinement. For the former, we introduce a\nholistic human-object tracking method to fuse the IMU signals and the RGB\nstream and progressively recover the human motions and subsequently the\ncompanion object motions. For the latter, we tailor a category-aware motion\ndiffusion model, which is conditioned on both the raw IMU observations and the\nresults from the previous stage under over-parameterization representation. It\nsignificantly refines the initial results and generates vivid body, hand, and\nobject motions. Moreover, we contribute a large dataset with ground truth human\nand object motions, dense RGB inputs, and rich object-mounted IMU measurements.\nExtensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid\ncapture setting. Our dataset and code will be released to the community.\n", "rewritten_text": "We live in a world surrounded by a variety of \"smart\" devices equipped with advanced sensing capabilities. However, effectively capturing the interactions between humans and these objects remains a challenging task. In this paper, we introduce I'm-HOI, a monocular system designed to accurately capture the 3D movements of both humans and objects in a unique setup using only a minimal setup of an RGB camera and an Inertial Measurement Unit (IMU) attached to the object.\n\nI'm-HOI combines general motion inference with category-aware refinement. To achieve this, we propose a comprehensive human-object tracking method that integrates signals from the IMU and RGB stream to reconstruct human movements and subsequently the movements of the associated object. Additionally, we develop a category-aware motion diffusion model that refines the results by considering raw IMU data and previous stage outcomes in an over-parameterized representation. This model enhances the accuracy of the captured body, hand, and object movements.\n\nFurthermore, we provide a large dataset containing ground truth human and object motions, detailed RGB inputs, and comprehensive IMU measurements attached to the objects. Extensive experiments demonstrate the effectiveness of I'm-HOI in a hybrid capture environment. We plan to release our dataset and code to the research community for further exploration and development."}, "1810.04864": {"original_text": "  We present a comparison of word-based and character-based\nsequence-to-sequence models for data-to-text natural language generation, which\ngenerate natural language descriptions for structured inputs. On the datasets\nof two recent generation challenges, our models achieve comparable or better\nautomatic evaluation results than the best challenge submissions. Subsequent\ndetailed statistical and human analyses shed light on the differences between\nthe two input representations and the diversity of the generated texts. In a\ncontrolled experiment with synthetic training data generated from templates, we\ndemonstrate the ability of neural models to learn novel combinations of the\ntemplates and thereby generalize beyond the linguistic structures they were\ntrained on.\n", "rewritten_text": "We conducted a comparison between word-based and character-based sequence-to-sequence models for data-to-text natural language generation. These models are designed to produce natural language descriptions based on structured inputs. Our study focused on datasets from two recent generation challenges, where our models achieved comparable or superior automatic evaluation results compared to the top submissions in those challenges. Through subsequent detailed statistical and human analyses, we were able to gain insights into the differences between the two input representations and the diversity of the generated texts.\n\nIn a controlled experiment using synthetic training data generated from templates, we showcased the neural models' capability to learn new combinations of templates and generalize beyond the linguistic structures they were initially trained on."}, "2405.16116": {"original_text": "  Scene Graph Generation (SGG) can extract abstract semantic relations between\nentities in images as graph representations. This task holds strong promises\nfor other downstream tasks such as the embodied cognition of an autonomous\nagent. However, to power such applications, SGG needs to solve the gap of\nreal-time latency. In this work, we propose to investigate the bottlenecks of\ncurrent approaches for real-time constraint applications. Then, we propose a\nsimple yet effective implementation of a real-time SGG approach using YOLOV8 as\nan object detection backbone. Our implementation is the first to obtain more\nthan 48 FPS for the task with no loss of accuracy, successfully outperforming\nany other lightweight approaches. Our code is freely available at\nhttps://github.com/Maelic/SGG-Benchmark.\n", "rewritten_text": "The generation of Scene Graphs (SGG) involves extracting abstract semantic relations between entities in images in the form of graph representations. This capability holds great potential for various downstream tasks, such as enhancing the embodied cognition of autonomous agents. However, in order to support such applications, SGG must address the challenge of real-time latency. \n\nIn this study, we aim to investigate the limitations of current approaches for real-time applications. Subsequently, we propose a straightforward yet efficient implementation of a real-time SGG method utilizing YOLOV8 as the object detection backbone. Our implementation is the first to achieve over 48 frames per second (FPS) for this task without sacrificing accuracy, surpassing all other lightweight approaches. \n\nOur code is openly accessible at https://github.com/Maelic/SGG-Benchmark."}, "2012.08549": {"original_text": "  Voice Assistants such as Alexa, Siri, and Google Assistant typically use a\ntwo-stage Spoken Language Understanding pipeline; first, an Automatic Speech\nRecognition (ASR) component to process customer speech and generate text\ntranscriptions, followed by a Natural Language Understanding (NLU) component to\nmap transcriptions to an actionable hypothesis. An end-to-end (E2E) system that\ngoes directly from speech to a hypothesis is a more attractive option. These\nsystems were shown to be smaller, faster, and better optimized. However, they\nrequire massive amounts of end-to-end training data and in addition, don't take\nadvantage of the already available ASR and NLU training data.\n  In this work, we propose an E2E system that is designed to jointly train on\nmultiple speech-to-text tasks, such as ASR (speech-transcription) and SLU\n(speech-hypothesis), and text-to-text tasks, such as NLU (text-hypothesis). We\ncall this the Audio-Text All-Task (AT-AT) Model and we show that it beats the\nperformance of E2E models trained on individual tasks, especially ones trained\non limited data. We show this result on an internal music dataset and two\npublic datasets, FluentSpeech and SNIPS Audio, where we achieve\nstate-of-the-art results. Since our model can process both speech and text\ninput sequences and learn to predict a target sequence, it also allows us to do\nzero-shot E2E SLU by training on only text-hypothesis data (without any speech)\nfrom a new domain. We evaluate this ability of our model on the Facebook TOP\ndataset and set a new benchmark for zeroshot E2E performance. We will soon\nrelease the audio data collected for the TOP dataset for future research.\n", "rewritten_text": "Voice assistants like Alexa, Siri, and Google Assistant typically employ a two-stage Spoken Language Understanding pipeline. Initially, an Automatic Speech Recognition (ASR) component processes customer speech to generate text transcriptions. This is followed by a Natural Language Understanding (NLU) component that maps the transcriptions to an actionable hypothesis. While an end-to-end (E2E) system that directly translates speech into a hypothesis is appealing due to its smaller size, faster speed, and better optimization, it necessitates vast amounts of end-to-end training data and does not leverage existing ASR and NLU training data.\n\nIn this study, we introduce an E2E system designed to simultaneously train on multiple speech-to-text tasks, such as ASR (speech-transcription) and SLU (speech-hypothesis), as well as text-to-text tasks, such as NLU (text-hypothesis). Termed the Audio-Text All-Task (AT-AT) Model, we demonstrate its superior performance over E2E models trained on individual tasks, particularly those trained on limited data. Our findings are based on an internal music dataset and two public datasets, FluentSpeech and SNIPS Audio, where we achieve state-of-the-art results.\n\nGiven that our model can process both speech and text input sequences and predict a target sequence, it enables zero-shot E2E SLU by training solely on text-hypothesis data (without any speech) from a new domain. We assess this capability on the Facebook TOP dataset and establish a new benchmark for zero-shot E2E performance. The audio data collected for the TOP dataset will be made available for future research endeavors."}, "2207.09059": {"original_text": "  Few-shot open-set recognition aims to classify both seen and novel images\ngiven only limited training data of seen classes. The challenge of this task is\nthat the model is required not only to learn a discriminative classifier to\nclassify the pre-defined classes with few training data but also to reject\ninputs from unseen classes that never appear at training time. In this paper,\nwe propose to solve the problem from two novel aspects. First, instead of\nlearning the decision boundaries between seen classes, as is done in standard\nclose-set classification, we reserve space for unseen classes, such that images\nlocated in these areas are recognized as the unseen classes. Second, to\neffectively learn such decision boundaries, we propose to utilize the\nbackground features from seen classes. As these background regions do not\nsignificantly contribute to the decision of close-set classification, it is\nnatural to use them as the pseudo unseen classes for classifier learning. Our\nextensive experiments show that our proposed method not only outperforms\nmultiple baselines but also sets new state-of-the-art results on three popular\nbenchmarks, namely tieredImageNet, miniImageNet, and Caltech-USCD\nBirds-200-2011 (CUB).\n", "rewritten_text": "Few-shot open-set recognition aims to classify both seen and novel images with limited training data of seen classes. The challenge of this task lies in the model's requirement to not only learn a discriminative classifier for pre-defined classes with minimal training data but also to reject inputs from unseen classes that were not present during training. In this paper, we introduce two novel approaches to address this problem.\n\nFirstly, instead of focusing solely on learning decision boundaries between seen classes, as typically done in standard close-set classification, we allocate space for unseen classes. This allows images located within these areas to be recognized as belonging to unseen classes. Secondly, to effectively learn these decision boundaries, we propose utilizing background features from seen classes. These background regions, which do not significantly impact close-set classification decisions, can serve as pseudo unseen classes for classifier learning.\n\nOur extensive experiments demonstrate that our proposed method not only surpasses multiple baselines but also achieves new state-of-the-art results on three prominent benchmarks: tieredImageNet, miniImageNet, and Caltech-USCD Birds-200-2011 (CUB)."}, "1710.04943": {"original_text": "  In this paper, we report on our efforts for using Deep Learning for\nclassifying artifacts and their features in digital visuals as a part of the\nNeoclassica framework. It was conceived to provide scholars with new methods\nfor analyzing and classifying artifacts and aesthetic forms from the era of\nClassicism. The framework accommodates both traditional knowledge\nrepresentation as a formal ontology and data-driven knowledge discovery, where\ncultural patterns will be identified by means of algorithms in statistical\nanalysis and machine learning. We created a Deep Learning approach trained on\nphotographs to classify the objects inside these photographs. In a next step,\nwe will apply a different Deep Learning approach. It is capable of locating\nmultiple objects inside an image and classifying them with a high accuracy.\n", "rewritten_text": "This paper discusses our use of Deep Learning to classify artifacts and their features in digital images within the Neoclassica framework. The framework aims to offer scholars new methods for analyzing and categorizing artifacts and aesthetic forms from the Classical era. It combines traditional knowledge representation through a formal ontology with data-driven knowledge discovery, enabling the identification of cultural patterns using algorithms in statistical analysis and machine learning. We developed a Deep Learning model trained on photographs to classify objects within the images. Our next step involves applying a different Deep Learning approach capable of accurately locating and classifying multiple objects within an image."}, "2111.07239": {"original_text": "  Object detection has achieved promising performance on clean datasets, but\nhow to achieve better tradeoff between the adversarial robustness and clean\nprecision is still under-explored. Adversarial training is the mainstream\nmethod to improve robustness, but most of the works will sacrifice clean\nprecision to gain robustness than standard training. In this paper, we propose\nUnified Decoupled Feature Alignment (UDFA), a novel fine-tuning paradigm which\nachieves better performance than existing methods, by fully exploring the\ncombination between self-knowledge distillation and adversarial training for\nobject detection. We first use decoupled fore/back-ground features to construct\nself-knowledge distillation branch between clean feature representation from\npretrained detector (served as teacher) and adversarial feature representation\nfrom student detector. Then we explore the self-knowledge distillation from a\nnew angle by decoupling original branch into a self-supervised learning branch\nand a new self-knowledge distillation branch. With extensive experiments on the\nPASCAL-VOC and MS-COCO benchmarks, the evaluation results show that UDFA can\nsurpass the standard training and state-of-the-art adversarial training methods\nfor object detection. For example, compared with teacher detector, our approach\non GFLV2 with ResNet-50 improves clean precision by 2.2 AP on PASCAL-VOC;\ncompared with SOTA adversarial training methods, our approach improves clean\nprecision by 1.6 AP, while improving adversarial robustness by 0.5 AP. Our code\nwill be available at https://github.com/grispeut/udfa.\n", "rewritten_text": "Object detection has shown promising performance on clean datasets, but there is still a lack of exploration on how to achieve a better tradeoff between adversarial robustness and clean precision. While adversarial training is the mainstream method for improving robustness, most approaches tend to sacrifice clean precision in favor of gaining robustness compared to standard training methods. \n\nIn this paper, we introduce Unified Decoupled Feature Alignment (UDFA), a novel fine-tuning paradigm that outperforms existing methods by fully leveraging the combination of self-knowledge distillation and adversarial training for object detection. Our approach involves using decoupled foreground and background features to establish a self-knowledge distillation branch between the clean feature representation from a pretrained detector (acting as the teacher) and the adversarial feature representation from the student detector. Additionally, we explore self-knowledge distillation from a new perspective by splitting the original branch into a self-supervised learning branch and a new self-knowledge distillation branch.\n\nThrough extensive experiments on the PASCAL-VOC and MS-COCO benchmarks, our evaluation results demonstrate that UDFA surpasses both standard training and state-of-the-art adversarial training methods for object detection. For instance, when compared to the teacher detector, our approach using GFLV2 with ResNet-50 enhances clean precision by 2.2 average precision (AP) on PASCAL-VOC. Furthermore, compared to current state-of-the-art adversarial training methods, our approach boosts clean precision by 1.6 AP while enhancing adversarial robustness by 0.5 AP. \n\nOur code will be made available at https://github.com/grispeut/udfa."}, "2304.05694": {"original_text": "  Self-attention modules have demonstrated remarkable capabilities in capturing\nlong-range relationships and improving the performance of point cloud tasks.\nHowever, point cloud objects are typically characterized by complex,\ndisordered, and non-Euclidean spatial structures with multiple scales, and\ntheir behavior is often dynamic and unpredictable. The current self-attention\nmodules mostly rely on dot product multiplication and dimension alignment among\nquery-key-value features, which cannot adequately capture the multi-scale\nnon-Euclidean structures of point cloud objects. To address these problems,\nthis paper proposes a self-attention plug-in module with its variants,\nMulti-scale Geometry-aware Transformer (MGT). MGT processes point cloud data\nwith multi-scale local and global geometric information in the following three\naspects. At first, the MGT divides point cloud data into patches with multiple\nscales. Secondly, a local feature extractor based on sphere mapping is proposed\nto explore the geometry inner each patch and generate a fixed-length\nrepresentation for each patch. Thirdly, the fixed-length representations are\nfed into a novel geodesic-based self-attention to capture the global\nnon-Euclidean geometry between patches. Finally, all the modules are integrated\ninto the framework of MGT with an end-to-end training scheme. Experimental\nresults demonstrate that the MGT vastly increases the capability of capturing\nmulti-scale geometry using the self-attention mechanism and achieves strong\ncompetitive performance on mainstream point cloud benchmarks.\n", "rewritten_text": "Self-attention modules have shown remarkable capabilities in capturing long-range relationships and enhancing the performance of point cloud tasks. However, point cloud objects typically exhibit complex, disordered, and non-Euclidean spatial structures with multiple scales, and their behavior is often dynamic and unpredictable. The current self-attention modules primarily rely on dot product multiplication and dimension alignment among query-key-value features, which may not adequately capture the multi-scale non-Euclidean structures of point cloud objects.\n\nTo tackle these challenges, this paper introduces a self-attention plug-in module and its variants, the Multi-scale Geometry-aware Transformer (MGT). MGT processes point cloud data by incorporating multi-scale local and global geometric information in three key aspects. Firstly, MGT divides point cloud data into patches with varying scales. Secondly, it proposes a local feature extractor based on sphere mapping to delve into the geometry within each patch and generate a fixed-length representation for each patch. Thirdly, the fixed-length representations are inputted into a novel geodesic-based self-attention mechanism to capture the global non-Euclidean geometry between patches. Subsequently, all these modules are integrated into the MGT framework with an end-to-end training scheme.\n\nExperimental results demonstrate that MGT significantly enhances the ability to capture multi-scale geometry using the self-attention mechanism and achieves strong competitive performance on mainstream point cloud benchmarks."}, "2108.07708": {"original_text": "  Can language models learn grounded representations from text distribution\nalone? This question is both central and recurrent in natural language\nprocessing; authors generally agree that grounding requires more than textual\ndistribution. We propose to experimentally test this claim: if any two words\nhave different meanings and yet cannot be distinguished from distribution\nalone, then grounding is out of the reach of text-based models. To that end, we\npresent early work on an online game for the collection of human judgments on\nthe distributional similarity of word pairs in five languages. We further\nreport early results of our data collection campaign.\n", "rewritten_text": "Is it possible for language models to acquire grounded representations solely from text distribution? This question is fundamental and frequently revisited in the field of natural language processing. Most authors agree that grounding necessitates more than just textual distribution. We aim to empirically investigate this assertion: if two words have distinct meanings but cannot be differentiated based on distribution alone, then text-based models fall short of achieving grounding. To this end, we introduce preliminary research on an online game designed to gather human judgments on the distributional similarity of word pairs in five languages. Additionally, we provide initial findings from our data collection efforts."}, "2302.03194": {"original_text": "  We propose two methods to make unsupervised domain adaptation (UDA) more\nparameter efficient using adapters, small bottleneck layers interspersed with\nevery layer of the large-scale pre-trained language model (PLM). The first\nmethod deconstructs UDA into a two-step process: first by adding a domain\nadapter to learn domain-invariant information and then by adding a task adapter\nthat uses domain-invariant information to learn task representations in the\nsource domain. The second method jointly learns a supervised classifier while\nreducing the divergence measure. Compared to strong baselines, our simple\nmethods perform well in natural language inference (MNLI) and the cross-domain\nsentiment classification task. We even outperform unsupervised domain\nadaptation methods such as DANN and DSN in sentiment classification, and we are\nwithin 0.85% F1 for natural language inference task, by fine-tuning only a\nfraction of the full model parameters. We release our code at\nhttps://github.com/declare-lab/domadapter\n", "rewritten_text": "We propose two methods to enhance the parameter efficiency of unsupervised domain adaptation (UDA) by incorporating adapters, which are small bottleneck layers inserted within each layer of a large-scale pre-trained language model (PLM). The first method breaks down UDA into a two-step process: initially by integrating a domain adapter to acquire domain-invariant information, and subsequently by introducing a task adapter that utilizes this domain-invariant information to learn task representations within the source domain. The second method involves the simultaneous learning of a supervised classifier while minimizing the divergence measure. Our straightforward approaches exhibit strong performance in tasks such as natural language inference (MNLI) and cross-domain sentiment classification when compared to robust baselines. In sentiment classification, we surpass unsupervised domain adaptation methods like DANN and DSN, while achieving an F1 score within 0.85% of the baseline for the natural language inference task. Notably, these results are achieved by fine-tuning only a fraction of the full model parameters. Our code is publicly available at https://github.com/declare-lab/domadapter."}, "2103.04059": {"original_text": "  Few-shot class incremental learning (FSCIL) portrays the problem of learning\nnew concepts gradually, where only a few examples per concept are available to\nthe learner. Due to the limited number of examples for training, the techniques\ndeveloped for standard incremental learning cannot be applied verbatim to\nFSCIL. In this work, we introduce a distillation algorithm to address the\nproblem of FSCIL and propose to make use of semantic information during\ntraining. To this end, we make use of word embeddings as semantic information\nwhich is cheap to obtain and which facilitate the distillation process.\nFurthermore, we propose a method based on an attention mechanism on multiple\nparallel embeddings of visual data to align visual and semantic vectors, which\nreduces issues related to catastrophic forgetting. Via experiments on\nMiniImageNet, CUB200, and CIFAR100 dataset, we establish new state-of-the-art\nresults by outperforming existing approaches.\n", "rewritten_text": "Few-shot class incremental learning (FSCIL) addresses the challenge of gradually learning new concepts with limited examples available per concept. Traditional incremental learning techniques are not directly applicable to FSCIL due to the scarcity of training examples. In this study, we present a distillation algorithm designed specifically for FSCIL, leveraging semantic information during training. We utilize word embeddings as a cost-effective source of semantic information to enhance the distillation process. Additionally, we propose an attention-based method that aligns visual and semantic vectors using multiple parallel embeddings of visual data, mitigating issues associated with catastrophic forgetting. Through experiments on the MiniImageNet, CUB200, and CIFAR100 datasets, we demonstrate superior performance compared to existing methods, establishing new state-of-the-art results."}, "1710.07395": {"original_text": "  In the wake of a polarizing election, the cyber world is laden with hate\nspeech. Context accompanying a hate speech text is useful for identifying hate\nspeech, which however has been largely overlooked in existing datasets and hate\nspeech detection models. In this paper, we provide an annotated corpus of hate\nspeech with context information well kept. Then we propose two types of hate\nspeech detection models that incorporate context information, a logistic\nregression model with context features and a neural network model with learning\ncomponents for context. Our evaluation shows that both models outperform a\nstrong baseline by around 3% to 4% in F1 score and combining these two models\nfurther improve the performance by another 7% in F1 score.\n", "rewritten_text": "Following a divisive election, the online sphere is rife with hate speech. The presence of context alongside hate speech text is crucial for accurately identifying such harmful content. Unfortunately, existing datasets and hate speech detection models have largely overlooked the importance of context. This paper presents an annotated corpus of hate speech that includes well-preserved context information. Subsequently, we introduce two types of hate speech detection models that integrate context information: a logistic regression model with context features and a neural network model with context-aware learning components. Our evaluation demonstrates that both models surpass a strong baseline by approximately 3% to 4% in F1 score. Furthermore, the combination of these two models results in an additional 7% improvement in F1 score."}, "2401.17597": {"original_text": "  Multi-turn dialogues are characterized by their extended length and the\npresence of turn-taking conversations. Traditional language models often\noverlook the distinct features of these dialogues by treating them as regular\ntext. In this paper, we propose a speaker-enhanced pre-training method for long\ndialogue summarization, which leverages the inherent structure of multiple-turn\ndialogues. To support our study, we curate a diverse dataset that includes\ntranscripts from real-world scenarios, movie or TV show transcripts, and\ndialogues generated by a Large Language Model. We then perform a pre-training,\nwhich encompasses the detection of speaker changes, and masked utterance\ngeneration. Experimental results of fine-tuned models demonstrate that our\nmodel achieves state-of-the-art performance on downstream benchmarks with long\ncontext, surpassing baseline models and highlighting the effectiveness of our\napproach. Our findings highlight the importance of curating pre-training\ndatasets that exhibit diversity and variations in length distribution to ensure\neffective alignment with downstream datasets.\n", "rewritten_text": "Multi-turn dialogues are characterized by their extended length and the presence of turn-taking conversations. Traditional language models often overlook the distinct features of these dialogues by treating them as regular text. In this paper, we propose a speaker-enhanced pre-training method for long dialogue summarization, leveraging the inherent structure of multiple-turn dialogues. To support our study, we curate a diverse dataset that includes transcripts from real-world scenarios, movie or TV show transcripts, and dialogues generated by a Large Language Model. Our pre-training process involves detecting speaker changes and generating masked utterances. Experimental results from fine-tuned models show that our approach achieves state-of-the-art performance on downstream benchmarks with long context, outperforming baseline models and underscoring the effectiveness of our method. Our findings emphasize the importance of curating pre-training datasets that demonstrate diversity and variations in length distribution to ensure effective alignment with downstream datasets."}, "2309.01036": {"original_text": "  Spatial transcriptomics is an emerging technology that aligns histopathology\nimages with spatially resolved gene expression profiling. It holds the\npotential for understanding many diseases but faces significant bottlenecks\nsuch as specialized equipment and domain expertise. In this work, we present\nSEPAL, a new model for predicting genetic profiles from visual tissue\nappearance. Our method exploits the biological biases of the problem by\ndirectly supervising relative differences with respect to mean expression, and\nleverages local visual context at every coordinate to make predictions using a\ngraph neural network. This approach closes the gap between complete locality\nand complete globality in current methods. In addition, we propose a novel\nbenchmark that aims to better define the task by following current best\npractices in transcriptomics and restricting the prediction variables to only\nthose with clear spatial patterns. Our extensive evaluation in two different\nhuman breast cancer datasets indicates that SEPAL outperforms previous\nstate-of-the-art methods and other mechanisms of including spatial context.\n", "rewritten_text": "Spatial transcriptomics is an emerging technology that combines histopathology images with spatially resolved gene expression profiling. This technology has the potential to enhance our understanding of various diseases, but it faces significant challenges, such as the need for specialized equipment and domain expertise. In this study, we introduce SEPAL, a novel model designed to predict genetic profiles based on visual tissue characteristics. Our method capitalizes on the biological biases inherent in the problem by directly supervising relative differences in gene expression levels compared to the mean expression. Furthermore, it utilizes local visual context at each coordinate to make predictions through a graph neural network. This innovative approach bridges the gap between complete locality and globality found in current methods.\n\nAdditionally, we propose a new benchmark that aims to refine the task by adhering to current best practices in transcriptomics. This benchmark restricts the prediction variables to those exhibiting clear spatial patterns. Through extensive evaluation using two distinct human breast cancer datasets, we demonstrate that SEPAL surpasses previous state-of-the-art methods and other approaches that incorporate spatial context."}, "2011.0212": {"original_text": "  Diabetic retinopathy (DR) is one of the leading causes of blindness. However,\nno specific symptoms of early DR lead to a delayed diagnosis, which results in\ndisease progression in patients. To determine the disease severity levels,\nophthalmologists need to focus on the discriminative parts of the fundus\nimages. In recent years, deep learning has achieved great success in medical\nimage analysis. However, most works directly employ algorithms based on\nconvolutional neural networks (CNNs), which ignore the fact that the difference\namong classes is subtle and gradual. Hence, we consider automatic image grading\nof DR as a fine-grained classification task, and construct a bilinear model to\nidentify the pathologically discriminative areas. In order to leverage the\nordinal information among classes, we use an ordinal regression method to\nobtain the soft labels. In addition, other than only using a categorical loss\nto train our network, we also introduce the metric loss to learn a more\ndiscriminative feature space. Experimental results demonstrate the superior\nperformance of the proposed method on two public IDRiD and DeepDR datasets.\n", "rewritten_text": "Diabetic retinopathy (DR) is a leading cause of blindness, with early DR often lacking specific symptoms that can lead to delayed diagnosis and disease progression in patients. To accurately determine the severity levels of the disease, ophthalmologists must focus on the distinguishing features in fundus images. While deep learning has shown significant success in medical image analysis, many existing approaches rely solely on convolutional neural networks (CNNs) that may overlook the subtle and gradual differences among disease classes.\n\nTherefore, we approach the task of automatic image grading of DR as a fine-grained classification challenge. We propose a bilinear model designed to pinpoint the pathologically discriminative regions in the images. To account for the ordinal nature of the disease classes, we utilize an ordinal regression method to generate soft labels. In addition to employing a categorical loss for network training, we introduce a metric loss to enhance the network's ability to learn a more discriminative feature space.\n\nExperimental results validate the effectiveness of our method on two publicly available datasets, IDRiD and DeepDR, showcasing superior performance compared to existing approaches."}, "1905.02462": {"original_text": "  Recently, image super-resolution has been widely studied and achieved\nsignificant progress by leveraging the power of deep convolutional neural\nnetworks. However, there has been limited advancement in video super-resolution\n(VSR) due to the complex temporal patterns in videos. In this paper, we\ninvestigate how to adapt state-of-the-art methods of image super-resolution for\nvideo super-resolution. The proposed adapting method is straightforward. The\ninformation among successive frames is well exploited, while the overhead on\nthe original image super-resolution method is negligible. Furthermore, we\npropose a learning-based method to ensemble the outputs from multiple\nsuper-resolution models. Our methods show superior performance and rank second\nin the NTIRE2019 Video Super-Resolution Challenge Track 1.\n", "rewritten_text": "In recent years, there has been significant progress in the field of image super-resolution, largely attributed to the utilization of deep convolutional neural networks. However, advancements in video super-resolution (VSR) have been limited due to the intricate temporal patterns present in videos. This study aims to explore the adaptation of cutting-edge image super-resolution techniques for video super-resolution. The proposed adaptation method is straightforward, effectively leveraging information across successive frames while incurring minimal additional overhead on the original image super-resolution process. Additionally, a learning-based approach is introduced to combine outputs from multiple super-resolution models. Our methods have demonstrated superior performance, achieving the second rank in the NTIRE2019 Video Super-Resolution Challenge Track 1."}, "2312.14157": {"original_text": "  3D hand tracking from a monocular video is a very challenging problem due to\nhand interactions, occlusions, left-right hand ambiguity, and fast motion. Most\nexisting methods rely on RGB inputs, which have severe limitations under\nlow-light conditions and suffer from motion blur. In contrast, event cameras\ncapture local brightness changes instead of full image frames and do not suffer\nfrom the described effects. Unfortunately, existing image-based techniques\ncannot be directly applied to events due to significant differences in the data\nmodalities. In response to these challenges, this paper introduces the first\nframework for 3D tracking of two fast-moving and interacting hands from a\nsingle monocular event camera. Our approach tackles the left-right hand\nambiguity with a novel semi-supervised feature-wise attention mechanism and\nintegrates an intersection loss to fix hand collisions. To facilitate advances\nin this research domain, we release a new synthetic large-scale dataset of two\ninteracting hands, Ev2Hands-S, and a new real benchmark with real event streams\nand ground-truth 3D annotations, Ev2Hands-R. Our approach outperforms existing\nmethods in terms of the 3D reconstruction accuracy and generalises to real data\nunder severe light conditions.\n", "rewritten_text": "Tracking hands in 3D using a monocular video presents significant challenges, including hand interactions, occlusions, left-right hand ambiguity, and fast motion. Most current methods rely on RGB inputs, which are limited in low-light conditions and susceptible to motion blur. In contrast, event cameras detect local brightness changes rather than full image frames, avoiding these issues. However, existing image-based techniques are not directly applicable to event data due to substantial differences in data modalities.\n\nTo address these challenges, this paper introduces a novel framework for tracking two fast-moving and interacting hands in 3D using a single monocular event camera. Our approach resolves left-right hand ambiguity through a unique semi-supervised feature-wise attention mechanism and incorporates an intersection loss to prevent hand collisions. To advance research in this field, we have introduced a new synthetic large-scale dataset, Ev2Hands-S, featuring two interacting hands, as well as a real benchmark dataset, Ev2Hands-R, containing real event streams and ground-truth 3D annotations.\n\nOur approach surpasses existing methods in terms of 3D reconstruction accuracy and demonstrates robust performance in real-world scenarios with challenging lighting conditions."}, "2010.08243": {"original_text": "  3D object detectors based only on LiDAR point clouds hold the\nstate-of-the-art on modern street-view benchmarks. However, LiDAR-based\ndetectors poorly generalize across domains due to domain shift. In the case of\nLiDAR, in fact, domain shift is not only due to changes in the environment and\nin the object appearances, as for visual data from RGB cameras, but is also\nrelated to the geometry of the point clouds (e.g., point density variations).\nThis paper proposes SF-UDA$^{3D}$, the first Source-Free Unsupervised Domain\nAdaptation (SF-UDA) framework to domain-adapt the state-of-the-art PointRCNN 3D\ndetector to target domains for which we have no annotations (unsupervised),\nneither we hold images nor annotations of the source domain (source-free).\nSF-UDA$^{3D}$ is novel on both aspects. Our approach is based on\npseudo-annotations, reversible scale-transformations and motion coherency.\nSF-UDA$^{3D}$ outperforms both previous domain adaptation techniques based on\nfeatures alignment and state-of-the-art 3D object detection methods which\nadditionally use few-shot target annotations or target annotation statistics.\nThis is demonstrated by extensive experiments on two large-scale datasets,\ni.e., KITTI and nuScenes.\n", "rewritten_text": "3D object detectors that rely solely on LiDAR point clouds currently represent the state-of-the-art in modern street-view benchmarks. However, LiDAR-based detectors exhibit poor generalization across different domains due to domain shift. Unlike visual data from RGB cameras, where domain shift is primarily attributed to changes in the environment and object appearances, in the case of LiDAR, it is also influenced by variations in the geometry of the point clouds, such as differences in point density.\n\nThis paper introduces SF-UDA$^{3D}$, the first Source-Free Unsupervised Domain Adaptation (SF-UDA) framework designed to adapt the state-of-the-art PointRCNN 3D detector to target domains lacking annotations (unsupervised) and without access to images or annotations from the source domain (source-free). SF-UDA$^{3D}$ is innovative in addressing both of these aspects. Our approach leverages pseudo-annotations, reversible scale-transformations, and motion coherency.\n\nThrough extensive experiments on two large-scale datasets, namely KITTI and nuScenes, SF-UDA$^{3D}$ demonstrates superior performance compared to previous domain adaptation techniques that focus on feature alignment and state-of-the-art 3D object detection methods utilizing few-shot target annotations or target annotation statistics."}, "2305.09407": {"original_text": "  Visual quality inspection in high performance manufacturing can benefit from\nautomation, due to cost savings and improved rigor. Deep learning techniques\nare the current state of the art for generic computer vision tasks like\nclassification and object detection. Manufacturing data can pose a challenge\nfor deep learning because data is highly repetitive and there are few images of\ndefects or deviations to learn from. Deep learning models trained with such\ndata can be fragile and sensitive to context, and can under-detect new defects\nnot found in the training data. In this work, we explore training defect\ndetection models to learn specific defects out of context, so that they are\nmore likely to be detected in new situations. We demonstrate how models trained\non diverse images containing a common defect type can pick defects out in new\ncircumstances. Such generic models could be more robust to new defects not\nfound data collected for training, and can reduce data collection impediments\nto implementing visual inspection on production lines. Additionally, we\ndemonstrate that object detection models trained to predict a label and\nbounding box outperform classifiers that predict a label only on held out test\ndata typical of manufacturing inspection tasks. Finally, we studied the factors\nthat affect generalization in order to train models that work under a wider\nrange of conditions.\n", "rewritten_text": "Visual quality inspection in high-performance manufacturing can greatly benefit from automation, leading to cost savings and enhanced rigor. Currently, deep learning techniques represent the cutting edge in generic computer vision tasks such as classification and object detection. However, applying deep learning to manufacturing data presents challenges due to the highly repetitive nature of the data and the limited number of images depicting defects or deviations for learning purposes. Consequently, deep learning models trained on such data may be fragile and context-sensitive, potentially failing to detect new defects not present in the training data.\n\nIn this study, we investigate the training of defect detection models to recognize specific defects regardless of context, thereby increasing the likelihood of detecting them in novel situations. By training models on a diverse set of images featuring a common defect type, we demonstrate their ability to identify defects in new circumstances. These generic models have the potential to be more resilient to novel defects not encountered during training data collection, thereby easing the implementation of visual inspection on production lines by reducing data collection barriers.\n\nFurthermore, our research shows that object detection models trained to predict both a label and a bounding box outperform classifiers that only predict a label when tested on typical manufacturing inspection data. Finally, we analyze the factors influencing generalization to develop models capable of performing effectively across a broader range of conditions."}, "2406.09858": {"original_text": "  The visual quality of an image is confounded by a number of intertwined\nfactors including its semantic content, distortion characteristics and\nappearance properties such as brightness, contrast, sharpness, and\ncolourfulness. Distilling high level knowledge about all these quality bearing\nattributes is crucial for developing objective Image Quality Assessment\n(IQA).While existing solutions have modeled some of these aspects, a\ncomprehensive solution that involves all these important quality related\nattributes has not yet been developed. In this paper, we present a new blind\nIQA (BIQA) model termed Self-supervision and Vision-Language supervision Image\nQUality Evaluator (SLIQUE) that features a joint vision-language and visual\ncontrastive representation learning framework for acquiring high level\nknowledge about the images semantic contents, distortion characteristics and\nappearance properties for IQA. For training SLIQUE, we have developed a\nsystematic approach to constructing a first of its kind large image database\nannotated with all three categories of quality relevant texts. The Text\nAnnotated Distortion, Appearance and Content (TADAC) database has over 1.6\nmillion images annotated with textual descriptions of their semantic contents,\ndistortion characteristics and appearance properties. The method for\nconstructing TADAC and the database itself will be particularly useful for\nexploiting vision-language modeling for advanced IQA applications. Extensive\nexperimental results show that SLIQUE has superior performances over state of\nthe art, demonstrating the soundness of its design principle and the\neffectiveness of its implementation.\n", "rewritten_text": "The quality of an image is influenced by various factors, including its semantic content, distortion characteristics, and appearance properties such as brightness, contrast, sharpness, and colorfulness. Understanding and incorporating high-level knowledge of these quality attributes is essential for the development of objective Image Quality Assessment (IQA) methods. While some existing solutions have addressed certain aspects of image quality, a comprehensive approach that considers all these important attributes simultaneously is yet to be established. \n\nThis paper introduces a novel blind IQA (BIQA) model called Self-supervision and Vision-Language supervision Image Quality Evaluator (SLIQUE). SLIQUE utilizes a joint vision-language and visual contrastive representation learning framework to capture in-depth knowledge about the semantic content, distortion characteristics, and appearance properties of images for IQA purposes. To train SLIQUE, a systematic method was devised to create a unique large image database named Text Annotated Distortion, Appearance, and Content (TADAC). This database contains over 1.6 million images annotated with textual descriptions of their semantic content, distortion characteristics, and appearance properties. The construction process of TADAC and the database itself are valuable for leveraging vision-language modeling in advanced IQA applications.\n\nExtensive experimental results demonstrate that SLIQUE outperforms current state-of-the-art methods, validating the robustness of its design principles and the effectiveness of its implementation."}, "2406.16860": {"original_text": "  We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, addressing the difficulties involved in consolidating and\ninterpreting results from various tasks, and introduce a new vision-centric\nbenchmark, CV-Bench. To further improve visual grounding, we propose the\nSpatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that\nintegrates high-resolution vision features with LLMs while reducing the number\nof tokens. Additionally, we discuss the curation of high-quality visual\ninstruction-tuning data from publicly available sources, emphasizing the\nimportance of data source balancing and distribution ratio. Collectively,\nCambrian-1 not only achieves state-of-the-art performance but also serves as a\ncomprehensive, open cookbook for instruction-tuned MLLMs. We provide model\nweights, code, supporting tools, datasets, and detailed instruction-tuning and\nevaluation recipes. We hope our release will inspire and accelerate\nadvancements in multimodal systems and visual representation learning.\n", "rewritten_text": "Introducing Cambrian-1, a family of multimodal Large Language Models (LLMs) designed with a vision-centric approach. While more powerful language models can enhance multimodal capabilities, the design choices for vision components are often inadequately explored and disconnected from visual representation learning research. This gap impedes accurate sensory grounding in real-world scenarios. Our study utilizes LLMs and visual instruction tuning as an interface to assess various visual representations, providing fresh insights into different models and architectures - whether self-supervised, strongly supervised, or a combination of both - through experiments involving over 20 vision encoders. We critically analyze existing MLLM benchmarks, tackling the challenges associated with consolidating and interpreting results from diverse tasks, and introduce a new vision-centric benchmark, CV-Bench. To further enhance visual grounding, we introduce the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that merges high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we delve into the curation of high-quality visual instruction-tuning data from publicly available sources, underscoring the significance of balancing data sources and distribution ratios. In totality, Cambrian-1 not only attains state-of-the-art performance but also acts as a comprehensive, open guide for instruction-tuned MLLMs. We offer model weights, code, supportive tools, datasets, and detailed instruction-tuning and evaluation methodologies. We aspire that our release will ignite and expedite progress in multimodal systems and visual representation learning."}, "2403.09593": {"original_text": "  Names are essential to both human cognition and vision-language models.\nOpen-vocabulary models utilize class names as text prompts to generalize to\ncategories unseen during training. However, the precision of these names is\noften overlooked in existing datasets. In this paper, we address this\nunderexplored problem by presenting a framework for \"renovating\" names in\nopen-vocabulary segmentation benchmarks (RENOVATE). Our framework features a\nrenaming model that enhances the quality of names for each visual segment.\nThrough experiments, we demonstrate that our renovated names help train\nstronger open-vocabulary models with up to 15% relative improvement and\nsignificantly enhance training efficiency with improved data quality. We also\nshow that our renovated names improve evaluation by better measuring\nmisclassification and enabling fine-grained model analysis. We will provide our\ncode and relabelings for several popular segmentation datasets (MS COCO,\nADE20K, Cityscapes) to the research community.\n", "rewritten_text": "Names play a crucial role in human cognition and vision-language models. Open-vocabulary models leverage class names as text prompts to generalize to categories not encountered during training. However, the accuracy of these names is often neglected in current datasets. This paper addresses this overlooked issue by introducing a framework called \"RENOVATE\" for enhancing names in open-vocabulary segmentation benchmarks. Our framework includes a renaming model that improves the quality of names for each visual segment. Through experiments, we demonstrate that our refined names contribute to training stronger open-vocabulary models, resulting in up to a 15% relative improvement and significantly enhancing training efficiency by improving data quality. Furthermore, we show that our enhanced names enhance evaluation by better assessing misclassification and enabling detailed model analysis. We will make our code and relabelings for various popular segmentation datasets (MS COCO, ADE20K, Cityscapes) available to the research community."}, "2310.13201": {"original_text": "  Early identification of abnormalities in plants is an important task for\nensuring proper growth and achieving high yields from crops. Precision\nagriculture can significantly benefit from modern computer vision tools to make\nfarming strategies addressing these issues efficient and effective. As farming\nlands are typically quite large, farmers have to manually check vast areas to\ndetermine the status of the plants and apply proper treatments. In this work,\nwe consider the problem of automatically identifying abnormal regions in maize\nplants from images captured by a UAV. Using deep learning techniques, we have\ndeveloped a methodology which can detect different levels of abnormality (i.e.,\nlow, medium, high or no abnormality) in maize plants independently of their\ngrowth stage. The primary goal is to identify anomalies at the earliest\npossible stage in order to maximize the effectiveness of potential treatments.\nAt the same time, the proposed system can provide valuable information to human\nannotators for ground truth data collection by helping them to focus their\nattention on a much smaller set of images only. We have experimented with two\ndifferent but complimentary approaches, the first considering abnormality\ndetection as a classification problem and the second considering it as a\nregression problem. Both approaches can be generalized to different types of\nabnormalities and do not make any assumption about the abnormality occurring at\nan early plant growth stage which might be easier to detect due to the plants\nbeing smaller and easier to separate. As a case study, we have considered a\npublicly available data set which exhibits mostly Nitrogen deficiency in maize\nplants of various growth stages. We are reporting promising preliminary results\nwith an 88.89\\% detection accuracy of low abnormality and 100\\% detection\naccuracy of no abnormality.\n", "rewritten_text": "Early detection of plant abnormalities is crucial for ensuring optimal growth and maximizing crop yields. Precision agriculture stands to benefit greatly from the integration of modern computer vision tools, which can enhance farming strategies and make them more efficient and effective. Given the vast expanse of farming lands, farmers often face the challenge of manually inspecting large areas to assess plant health and administer appropriate treatments.\n\nIn this study, we focus on the automated identification of abnormal regions in maize plants using images captured by unmanned aerial vehicles (UAVs). Leveraging deep learning techniques, we have developed a methodology capable of detecting varying levels of abnormalities (low, medium, high, or none) in maize plants, regardless of their growth stage. The primary objective is to detect anomalies at the earliest possible stage to optimize treatment effectiveness.\n\nFurthermore, our proposed system not only aids in anomaly detection but also provides valuable guidance to human annotators for ground truth data collection. By streamlining the focus onto a smaller subset of images, the system enhances the efficiency of data annotation processes. Our experimentation involved two distinct yet complementary approaches: one treating abnormality detection as a classification problem and the other as a regression problem. These approaches are versatile and can be applied to different types of abnormalities without assuming that abnormalities occur only in early plant growth stages, where detection may be simpler due to the plants' smaller size and easier differentiation.\n\nAs a case study, we utilized a publicly available dataset primarily showcasing Nitrogen deficiency in maize plants at various growth stages. Our preliminary findings demonstrate promising results, with an 88.89% accuracy in detecting low abnormalities and a 100% accuracy in detecting no abnormalities."}, "1711.0664": {"original_text": "  We investigate the problem of producing structured graph representations of\nvisual scenes. Our work analyzes the role of motifs: regularly appearing\nsubstructures in scene graphs. We present new quantitative insights on such\nrepeated structures in the Visual Genome dataset. Our analysis shows that\nobject labels are highly predictive of relation labels but not vice-versa. We\nalso find that there are recurring patterns even in larger subgraphs: more than\n50% of graphs contain motifs involving at least two relations. Our analysis\nmotivates a new baseline: given object detections, predict the most frequent\nrelation between object pairs with the given labels, as seen in the training\nset. This baseline improves on the previous state-of-the-art by an average of\n3.6% relative improvement across evaluation settings. We then introduce Stacked\nMotif Networks, a new architecture designed to capture higher order motifs in\nscene graphs that further improves over our strong baseline by an average 7.1%\nrelative gain. Our code is available at github.com/rowanz/neural-motifs.\n", "rewritten_text": "We are investigating the production of structured graph representations of visual scenes, focusing on the role of motifs - regularly occurring substructures in scene graphs. Our research provides new quantitative insights into these repeated structures using the Visual Genome dataset. Our analysis reveals that object labels are highly predictive of relation labels, but not the other way around. Additionally, we observe recurring patterns in larger subgraphs, with over 50% of graphs containing motifs involving at least two relations.\n\nBased on our analysis, we propose a new baseline approach: given object detections, predict the most common relation between object pairs with the provided labels from the training set. This baseline outperforms the previous state-of-the-art by an average of 3.6% across evaluation settings. Furthermore, we introduce Stacked Motif Networks, a novel architecture designed to capture higher-order motifs in scene graphs, which enhances our baseline by an average relative gain of 7.1%.\n\nFor those interested, our code can be accessed at github.com/rowanz/neural-motifs."}, "2207.01932": {"original_text": "  Image Coding for Machines (ICM) aims to compress images for AI tasks analysis\nrather than meeting human perception. Learning a kind of feature that is both\ngeneral (for AI tasks) and compact (for compression) is pivotal for its\nsuccess. In this paper, we attempt to develop an ICM framework by learning\nuniversal features while also considering compression. We name such features as\nomnipotent features and the corresponding framework as Omni-ICM. Considering\nself-supervised learning (SSL) improves feature generalization, we integrate it\nwith the compression task into the Omni-ICM framework to learn omnipotent\nfeatures. However, it is non-trivial to coordinate semantics modeling in SSL\nand redundancy removing in compression, so we design a novel information\nfiltering (IF) module between them by co-optimization of instance\ndistinguishment and entropy minimization to adaptively drop information that is\nweakly related to AI tasks (e.g., some texture redundancy). Different from\nprevious task-specific solutions, Omni-ICM could directly support AI tasks\nanalysis based on the learned omnipotent features without joint training or\nextra transformation. Albeit simple and intuitive, Omni-ICM significantly\noutperforms existing traditional and learning-based codecs on multiple\nfundamental vision tasks.\n", "rewritten_text": "The goal of Image Coding for Machines (ICM) is to compress images for AI task analysis rather than focusing on human perception. It is crucial for the success of ICM to learn features that are both general for AI tasks and compact for compression purposes. This paper presents an ICM framework called Omni-ICM, which aims to develop universal features while also addressing compression needs. These features are referred to as omnipotent features within the Omni-ICM framework. By incorporating self-supervised learning (SSL) to enhance feature generalization, we integrate SSL with the compression task in the Omni-ICM framework to learn omnipotent features. To address the challenge of coordinating semantics modeling in SSL and redundancy removal in compression, we introduce an information filtering (IF) module. This module optimizes instance distinguishability and entropy minimization to selectively discard information weakly related to AI tasks, such as certain texture redundancies. Unlike previous task-specific approaches, Omni-ICM enables direct support for AI task analysis using the learned omnipotent features without requiring joint training or additional transformations. Despite its simplicity and intuitiveness, Omni-ICM demonstrates significant performance improvements over traditional and learning-based codecs across various fundamental vision tasks."}, "1711.07183": {"original_text": "  Generating adversarial examples is an intriguing problem and an important way\nof understanding the working mechanism of deep neural networks. Most existing\napproaches generated perturbations in the image space, i.e., each pixel can be\nmodified independently. However, in this paper we pay special attention to the\nsubset of adversarial examples that correspond to meaningful changes in 3D\nphysical properties (like rotation and translation, illumination condition,\netc.). These adversaries arguably pose a more serious concern, as they\ndemonstrate the possibility of causing neural network failure by easy\nperturbations of real-world 3D objects and scenes.\n  In the contexts of object classification and visual question answering, we\naugment state-of-the-art deep neural networks that receive 2D input images with\na rendering module (either differentiable or not) in front, so that a 3D scene\n(in the physical space) is rendered into a 2D image (in the image space), and\nthen mapped to a prediction (in the output space). The adversarial\nperturbations can now go beyond the image space, and have clear meanings in the\n3D physical world. Though image-space adversaries can be interpreted as\nper-pixel albedo change, we verify that they cannot be well explained along\nthese physically meaningful dimensions, which often have a non-local effect.\nBut it is still possible to successfully attack beyond the image space on the\nphysical space, though this is more difficult than image-space attacks,\nreflected in lower success rates and heavier perturbations required.\n", "rewritten_text": "Generating adversarial examples is a fascinating problem and a crucial method for understanding the operational mechanisms of deep neural networks. While most current approaches focus on creating perturbations in the image space, where each pixel can be independently modified, this paper directs its attention to a subset of adversarial examples that entail meaningful alterations in 3D physical properties such as rotation, translation, and illumination conditions. These particular adversaries are deemed more concerning as they illustrate the potential for neural network failures through simple perturbations of real-world 3D objects and scenes.\n\nIn the realms of object classification and visual question answering, we enhance cutting-edge deep neural networks designed to process 2D input images by incorporating a rendering module at the forefront. This module, whether differentiable or not, renders a 3D scene in physical space into a 2D image in the image space, which is then translated into a prediction in the output space. By doing so, adversarial perturbations can extend beyond the image space, carrying clear implications in the 3D physical realm. While perturbations in the image space can be seen as per-pixel albedo changes, we demonstrate that they do not align well with these physically meaningful dimensions, often resulting in non-local effects. Nevertheless, successful attacks beyond the image space onto the physical space are achievable, albeit more challenging than image-space attacks, as evidenced by lower success rates and the need for more substantial perturbations."}, "2105.02039": {"original_text": "  In this paper, we fill the research gap by adopting state-of-the-art computer\nvision techniques for the data extraction stage in a data mining system. As\nshown in Fig.1, this stage contains two subtasks, namely, plot element\ndetection and data conversion. For building a robust box detector, we\ncomprehensively compare different deep learning-based methods and find a\nsuitable method to detect box with high precision. For building a robust point\ndetector, a fully convolutional network with feature fusion module is adopted,\nwhich can distinguish close points compared to traditional methods. The\nproposed system can effectively handle various chart data without making\nheuristic assumptions. For data conversion, we translate the detected element\ninto data with semantic value. A network is proposed to measure feature\nsimilarities between legends and detected elements in the legend matching\nphase. Furthermore, we provide a baseline on the competition of Harvesting raw\ntables from Infographics. Some key factors have been found to improve the\nperformance of each stage. Experimental results demonstrate the effectiveness\nof the proposed system.\n", "rewritten_text": "This paper addresses a research gap by utilizing state-of-the-art computer vision techniques for the data extraction stage within a data mining system. The data extraction stage, illustrated in Fig. 1, consists of two subtasks: plot element detection and data conversion. To develop a robust box detector, we conducted a comprehensive comparison of various deep learning-based methods to identify a method that can detect boxes with high precision. For the creation of a robust point detector, we employed a fully convolutional network with a feature fusion module, enabling the distinction of close points compared to traditional methods. Our proposed system is capable of effectively handling diverse chart data without relying on heuristic assumptions. In terms of data conversion, we translate the detected elements into data with semantic value. We introduce a network designed to measure feature similarities between legends and detected elements during the legend matching phase. Additionally, we establish a baseline for the competition of harvesting raw tables from infographics. Through our research, we have identified key factors that enhance the performance of each stage. Experimental results validate the effectiveness of the proposed system."}, "2401.08508": {"original_text": "  Sentiment analysis and emotion detection are important research topics in\nnatural language processing (NLP) and benefit many downstream tasks. With the\nwidespread application of LLMs, researchers have started exploring the\napplication of LLMs based on instruction-tuning in the field of sentiment\nanalysis. However, these models only focus on single aspects of affective\nclassification tasks (e.g. sentimental polarity or categorical emotions), and\noverlook the regression tasks (e.g. sentiment strength or emotion intensity),\nwhich leads to poor performance in downstream tasks. The main reason is the\nlack of comprehensive affective instruction tuning datasets and evaluation\nbenchmarks, which cover various affective classification and regression tasks.\nMoreover, although emotional information is useful for downstream tasks,\nexisting downstream datasets lack high-quality and comprehensive affective\nannotations. In this paper, we propose EmoLLMs, the first series of\nopen-sourced instruction-following LLMs for comprehensive affective analysis\nbased on fine-tuning various LLMs with instruction data, the first multi-task\naffective analysis instruction dataset (AAID) with 234K data samples based on\nvarious classification and regression tasks to support LLM instruction tuning,\nand a comprehensive affective evaluation benchmark (AEB) with 14 tasks from\nvarious sources and domains to test the generalization ability of LLMs. We\npropose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various\naffective instruction tasks. We compare our model with a variety of LLMs on\nAEB, where our models outperform all other open-sourced LLMs, and surpass\nChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve\nthe ChatGPT-level and GPT-4-level generalization capabilities on affective\nanalysis tasks, and demonstrates our models can be used as affective annotation\ntools.\n", "rewritten_text": "Sentiment analysis and emotion detection are crucial research areas in natural language processing (NLP) that have a significant impact on various downstream tasks. The advent of Large Language Models (LLMs) has prompted researchers to delve into the realm of sentiment analysis through instruction-tuning techniques. However, existing models primarily concentrate on specific aspects of affective classification tasks, such as sentimental polarity or categorical emotions, while neglecting regression tasks like sentiment strength or emotion intensity. This oversight results in subpar performance in downstream applications. The primary contributing factor to this issue is the absence of comprehensive affective instruction tuning datasets and evaluation benchmarks that encompass a wide range of affective classification and regression tasks.\n\nFurthermore, although emotional information is valuable for downstream tasks, current datasets lack thorough and high-quality affective annotations. In response to these challenges, this paper introduces EmoLLMs, a novel series of open-sourced instruction-following LLMs designed for comprehensive affective analysis. EmoLLMs are created by fine-tuning various LLMs with instruction data, utilizing the first multi-task affective analysis instruction dataset (AAID) comprising 234K data samples across diverse classification and regression tasks to facilitate LLM instruction tuning. Additionally, a comprehensive affective evaluation benchmark (AEB) is introduced, featuring 14 tasks sourced from various domains to assess the generalization capabilities of LLMs in affective analysis.\n\nThrough the fine-tuning of LLMs with AAID, EmoLLMs address a multitude of affective instruction tasks. Comparative evaluations on AEB demonstrate that our models outperform all other open-sourced LLMs and even surpass ChatGPT and GPT-4 in most tasks. This achievement signifies that EmoLLMs exhibit generalization capabilities comparable to ChatGPT and GPT-4 in affective analysis tasks, positioning them as effective tools for affective annotation purposes."}, "2210.05564": {"original_text": "  Semantic segmentation is a fundamental topic in computer vision. Several deep\nlearning methods have been proposed for semantic segmentation with outstanding\nresults. However, these models require a lot of densely annotated images. To\naddress this problem, we propose a new algorithm that uses HyperGraph\nConvolutional Networks for Weakly-supervised Semantic Segmentation\n(HyperGCN-WSS). Our algorithm constructs spatial and k-Nearest Neighbor (k-NN)\ngraphs from the images in the dataset to generate the hypergraphs. Then, we\ntrain a specialized HyperGraph Convolutional Network (HyperGCN) architecture\nusing some weak signals. The outputs of the HyperGCN are denominated\npseudo-labels, which are later used to train a DeepLab model for semantic\nsegmentation. HyperGCN-WSS is evaluated on the PASCAL VOC 2012 dataset for\nsemantic segmentation, using scribbles or clicks as weak signals. Our algorithm\nshows competitive performance against previous methods.\n", "rewritten_text": "Semantic segmentation is a crucial aspect of computer vision, with numerous deep learning techniques having been developed to achieve remarkable results in this field. However, these models typically rely on a large number of densely annotated images. In response to this challenge, we introduce a novel algorithm called HyperGraph Convolutional Networks for Weakly-supervised Semantic Segmentation (HyperGCN-WSS). Our approach involves constructing spatial and k-Nearest Neighbor (k-NN) graphs from the dataset images to form hypergraphs. Subsequently, we train a specialized HyperGraph Convolutional Network (HyperGCN) architecture using weak signals. The HyperGCN produces pseudo-labels, which are then utilized to train a DeepLab model for semantic segmentation. We evaluate the performance of HyperGCN-WSS on the PASCAL VOC 2012 dataset for semantic segmentation, employing scribbles or clicks as weak signals. Our algorithm demonstrates competitive performance compared to existing methods."}, "2112.0618": {"original_text": "  We present 360-DFPE, a sequential floor plan estimation method that directly\ntakes 360-images as input without relying on active sensors or 3D information.\nOur approach leverages a loosely coupled integration between a monocular visual\nSLAM solution and a monocular 360-room layout approach, which estimate camera\nposes and layout geometries, respectively. Since our task is to sequentially\ncapture the floor plan using monocular images, the entire scene structure, room\ninstances, and room shapes are unknown. To tackle these challenges, we first\nhandle the scale difference between visual odometry and layout geometry via\nformulating an entropy minimization process, which enables us to directly align\n360-layouts without knowing the entire scene in advance. Second, to\nsequentially identify individual rooms, we propose a novel room identification\nalgorithm that tracks every room along the camera exploration using geometry\ninformation. Lastly, to estimate the final shape of the room, we propose a\nshortest path algorithm with an iterative coarse-to-fine strategy, which\nimproves prior formulations with higher accuracy and faster run-time. Moreover,\nwe collect a new floor plan dataset with challenging large-scale scenes,\nproviding both point clouds and sequential 360-image information. Experimental\nresults show that our monocular solution achieves favorable performance against\nthe current state-of-the-art algorithms that rely on active sensors and require\nthe entire scene reconstruction data in advance.\n", "rewritten_text": "Introducing 360-DFPE, a novel method for estimating sequential floor plans that utilizes 360-degree images as input, eliminating the need for active sensors or 3D data. Our approach involves integrating a monocular visual SLAM solution with a monocular 360-room layout method to determine camera poses and layout geometries, respectively. Given the challenge of capturing floor plans sequentially with unknown scene structures, room instances, and shapes, we address these issues by first addressing the scale disparity between visual odometry and layout geometry through an entropy minimization process. This process allows for the direct alignment of 360-layouts without prior knowledge of the entire scene. Additionally, we introduce a new room identification algorithm that tracks individual rooms during camera exploration based on geometry information. To estimate the final room shape, we propose a shortest path algorithm with an iterative coarse-to-fine strategy, enhancing accuracy and runtime efficiency compared to previous methods. Furthermore, we have curated a comprehensive floor plan dataset featuring complex large-scale scenes, including point clouds and sequential 360-image data. Experimental results demonstrate that our monocular solution outperforms existing algorithms that rely on active sensors and necessitate complete scene reconstruction data in advance."}, "2205.08811": {"original_text": "  Object pose estimation is crucial for robotic applications and augmented\nreality. Beyond instance level 6D object pose estimation methods, estimating\ncategory-level pose and shape has become a promising trend. As such, a new\nresearch field needs to be supported by well-designed datasets. To provide a\nbenchmark with high-quality ground truth annotations to the community, we\nintroduce a multimodal dataset for category-level object pose estimation with\nphotometrically challenging objects termed PhoCaL. PhoCaL comprises 60 high\nquality 3D models of household objects over 8 categories including highly\nreflective, transparent and symmetric objects. We developed a novel\nrobot-supported multi-modal (RGB, depth, polarisation) data acquisition and\nannotation process. It ensures sub-millimeter accuracy of the pose for opaque\ntextured, shiny and transparent objects, no motion blur and perfect camera\nsynchronisation. To set a benchmark for our dataset, state-of-the-art RGB-D and\nmonocular RGB methods are evaluated on the challenging scenes of PhoCaL.\n", "rewritten_text": "Object pose estimation is essential for robotic applications and augmented reality. In addition to instance-level 6D object pose estimation methods, there is a growing interest in estimating category-level pose and shape. This emerging trend highlights the need for a new research field supported by well-designed datasets. To address this, we have introduced a multimodal dataset named PhoCaL for category-level object pose estimation, featuring photometrically challenging objects.\n\nPhoCaL includes 60 high-quality 3D models of household objects across 8 categories, encompassing highly reflective, transparent, and symmetric objects. Our approach involves a novel robot-supported multi-modal data acquisition and annotation process, incorporating RGB, depth, and polarization data. This process ensures sub-millimeter accuracy in pose estimation for opaque textured, shiny, and transparent objects, with no motion blur and perfect camera synchronization.\n\nTo establish a benchmark for our dataset, we have evaluated state-of-the-art RGB-D and monocular RGB methods on the challenging scenes within PhoCaL."}, "2005.14439": {"original_text": "  Dynamic routing networks, aimed at finding the best routing paths in the\nnetworks, have achieved significant improvements to neural networks in terms of\naccuracy and efficiency. In this paper, we see dynamic routing networks in a\nfresh light, formulating a routing method as a mapping from a sample space to a\nrouting space. From the perspective of space mapping, prevalent methods of\ndynamic routing didn't consider how inference paths would be distributed in the\nrouting space. Thus, we propose a novel method, termed CoDiNet, to model the\nrelationship between a sample space and a routing space by regularizing the\ndistribution of routing paths with the properties of consistency and diversity.\nSpecifically, samples with similar semantics should be mapped into the same\narea in routing space, while those with dissimilar semantics should be mapped\ninto different areas. Moreover, we design a customizable dynamic routing\nmodule, which can strike a balance between accuracy and efficiency. When\ndeployed upon ResNet models, our method achieves higher performance and\neffectively reduces average computational cost on four widely used datasets.\n", "rewritten_text": "Dynamic routing networks have made significant advancements in improving the accuracy and efficiency of neural networks by identifying the best routing paths within the network. This paper presents a fresh perspective on dynamic routing networks, defining a routing method as a mapping from a sample space to a routing space. Unlike existing methods that overlook the distribution of inference paths in the routing space, we introduce a novel approach called CoDiNet. This method aims to establish a connection between the sample space and the routing space by regulating the distribution of routing paths based on consistency and diversity principles.\n\nOur proposed method ensures that samples with similar semantics are grouped together in the routing space, while those with different semantics are separated into distinct areas. Additionally, we introduce a customizable dynamic routing module that strikes a balance between accuracy and efficiency. When implemented on ResNet models, our approach demonstrates superior performance and effectively reduces the average computational cost across four commonly used datasets."}, "2110.05594": {"original_text": "  We present a modern solution to the multi-view photometric stereo problem\n(MVPS). Our work suitably exploits the image formation model in a MVPS\nexperimental setup to recover the dense 3D reconstruction of an object from\nimages. We procure the surface orientation using a photometric stereo (PS)\nimage formation model and blend it with a multi-view neural radiance field\nrepresentation to recover the object's surface geometry. Contrary to the\nprevious multi-staged framework to MVPS, where the position, iso-depth\ncontours, or orientation measurements are estimated independently and then\nfused later, our method is simple to implement and realize. Our method performs\nneural rendering of multi-view images while utilizing surface normals estimated\nby a deep photometric stereo network. We render the MVPS images by considering\nthe object's surface normals for each 3D sample point along the viewing\ndirection rather than explicitly using the density gradient in the volume space\nvia 3D occupancy information. We optimize the proposed neural radiance field\nrepresentation for the MVPS setup efficiently using a fully connected deep\nnetwork to recover the 3D geometry of an object. Extensive evaluation on the\nDiLiGenT-MV benchmark dataset shows that our method performs better than the\napproaches that perform only PS or only multi-view stereo (MVS) and provides\ncomparable results against the state-of-the-art multi-stage fusion methods.\n", "rewritten_text": "We introduce a modern solution to the multi-view photometric stereo problem (MVPS). Our approach effectively leverages the image formation model within an MVPS experimental setup to achieve the dense 3D reconstruction of an object from images. By utilizing a photometric stereo (PS) image formation model to determine surface orientation and integrating it with a multi-view neural radiance field representation, we are able to reconstruct the object's surface geometry. In contrast to previous multi-staged frameworks for MVPS, which involve independent estimation of position, iso-depth contours, or orientation measurements followed by fusion, our method is straightforward to implement and execute.\n\nOur method involves neural rendering of multi-view images while incorporating surface normals estimated by a deep photometric stereo network. Rather than explicitly relying on density gradient in volume space through 3D occupancy information, we render the MVPS images by considering the object's surface normals for each 3D sample point along the viewing direction. We efficiently optimize the proposed neural radiance field representation for the MVPS setup using a fully connected deep network to recover the object's 3D geometry.\n\nExtensive evaluation on the DiLiGenT-MV benchmark dataset demonstrates that our method outperforms approaches that solely utilize PS or multi-view stereo (MVS), and it yields comparable results to state-of-the-art multi-stage fusion methods."}, "2206.13263": {"original_text": "  Robust maritime obstacle detection is critical for safe navigation of\nautonomous boats and timely collision avoidance. The current state-of-the-art\nis based on deep segmentation networks trained on large datasets. However,\nper-pixel ground truth labeling of such datasets is labor-intensive and\nexpensive. We propose a new scaffolding learning regime (SLR) that leverages\nweak annotations consisting of water edges, the horizon location, and obstacle\nbounding boxes to train segmentation-based obstacle detection networks, thereby\nreducing the required ground truth labeling effort by a factor of twenty. SLR\ntrains an initial model from weak annotations and then alternates between\nre-estimating the segmentation pseudo-labels and improving the network\nparameters. Experiments show that maritime obstacle segmentation networks\ntrained using SLR on weak annotations not only match but outperform the same\nnetworks trained with dense ground truth labels, which is a remarkable result.\nIn addition to the increased accuracy, SLR also increases domain generalization\nand can be used for domain adaptation with a low manual annotation load. The\nSLR code and pre-trained models are available at\nhttps://github.com/lojzezust/SLR .\n", "rewritten_text": "Ensuring robust maritime obstacle detection is crucial for the safe navigation of autonomous boats and timely collision avoidance. The current state-of-the-art approach relies on deep segmentation networks trained on extensive datasets. However, the process of labeling per-pixel ground truth in such datasets is both labor-intensive and costly. To address this challenge, we introduce a novel learning framework called Scaffolding Learning Regime (SLR). SLR utilizes weak annotations, including water edges, horizon location, and obstacle bounding boxes, to train segmentation-based obstacle detection networks. This innovative approach significantly reduces the effort required for ground truth labeling by a factor of twenty.\n\nThe SLR methodology involves training an initial model using weak annotations and then iteratively refining the segmentation pseudo-labels while improving the network parameters. Experimental results demonstrate that maritime obstacle segmentation networks trained with SLR on weak annotations not only match but surpass the performance of networks trained with dense ground truth labels, showcasing a remarkable achievement. Furthermore, SLR enhances domain generalization and facilitates domain adaptation with minimal manual annotation requirements.\n\nFor those interested, the SLR code and pre-trained models can be accessed at https://github.com/lojzezust/SLR."}, "2204.07126": {"original_text": "  Recent development of neural implicit function has shown tremendous success\non high-quality 3D shape reconstruction. However, most works divide the space\ninto inside and outside of the shape, which limits their representing power to\nsingle-layer and watertight shapes. This limitation leads to tedious data\nprocessing (converting non-watertight raw data to watertight) as well as the\nincapability of representing general object shapes in the real world. In this\nwork, we propose a novel method to represent general shapes including\nnon-watertight shapes and shapes with multi-layer surfaces. We introduce\nGeneral Implicit Function for 3D Shape (GIFS), which models the relationships\nbetween every two points instead of the relationships between points and\nsurfaces. Instead of dividing 3D space into predefined inside-outside regions,\nGIFS encodes whether two points are separated by any surface. Experiments on\nShapeNet show that GIFS outperforms previous state-of-the-art methods in terms\nof reconstruction quality, rendering efficiency, and visual fidelity. Project\npage is available at https://jianglongye.com/gifs .\n", "rewritten_text": "The recent development of neural implicit functions has demonstrated significant success in high-quality 3D shape reconstruction. However, most existing works partition the space into inside and outside regions of the shape, limiting their ability to represent single-layer and watertight shapes. This limitation results in laborious data processing tasks, such as converting non-watertight raw data to watertight, and an inability to accurately represent diverse object shapes in the real world.\n\nIn this study, we propose a novel method for representing general shapes, including non-watertight shapes and shapes with multi-layer surfaces. Our approach, General Implicit Function for 3D Shape (GIFS), focuses on modeling the relationships between every pair of points rather than just points and surfaces. Unlike traditional methods that divide 3D space into predefined inside-outside regions, GIFS determines whether two points are separated by any surface.\n\nExperimental results on ShapeNet demonstrate that GIFS surpasses previous state-of-the-art techniques in terms of reconstruction quality, rendering efficiency, and visual fidelity. For more information, please visit the project page at https://jianglongye.com/gifs."}, "2103.03166": {"original_text": "  Annotated medical images are typically rarer than labeled natural images\nsince they are limited by domain knowledge and privacy constraints. Recent\nadvances in transfer and contrastive learning have provided effective solutions\nto tackle such issues from different perspectives. The state-of-the-art\ntransfer learning (e.g., Big Transfer (BiT)) and contrastive learning (e.g.,\nSimple Siamese Contrastive Learning (SimSiam)) approaches have been\ninvestigated independently, without considering the complementary nature of\nsuch techniques. It would be appealing to accelerate contrastive learning with\ntransfer learning, given that slow convergence speed is a critical limitation\nof modern contrastive learning approaches. In this paper, we investigate the\nfeasibility of aligning BiT with SimSiam. From empirical analyses, different\nnormalization techniques (Group Norm in BiT vs. Batch Norm in SimSiam) are the\nkey hurdle of adapting BiT to SimSiam. When combining BiT with SimSiam, we\nevaluated the performance of using BiT, SimSiam, and BiT+SimSiam on CIFAR-10\nand HAM10000 datasets. The results suggest that the BiT models accelerate the\nconvergence speed of SimSiam. When used together, the model gives superior\nperformance over both of its counterparts. We hope this study will motivate\nresearchers to revisit the task of aggregating big pre-trained models with\ncontrastive learning models for image analysis.\n", "rewritten_text": "Annotated medical images are typically less common than labeled natural images due to limitations imposed by domain knowledge and privacy constraints. Recent advancements in transfer and contrastive learning have offered effective solutions to address these challenges from various perspectives. State-of-the-art transfer learning methods, such as Big Transfer (BiT), and contrastive learning techniques, like Simple Siamese Contrastive Learning (SimSiam), have been explored independently without considering their complementary nature. Combining contrastive learning with transfer learning could potentially enhance the speed of convergence, which is a critical limitation of modern contrastive learning approaches.\n\nIn this study, we investigate the feasibility of aligning BiT with SimSiam. Our empirical analyses reveal that different normalization techniques (Group Norm in BiT versus Batch Norm in SimSiam) present a significant obstacle in adapting BiT to SimSiam. By combining BiT with SimSiam, we evaluate the performance of using BiT, SimSiam, and BiT+SimSiam on CIFAR-10 and HAM10000 datasets. The results indicate that BiT models can accelerate the convergence speed of SimSiam. Furthermore, when used in conjunction, the combined model demonstrates superior performance compared to each individual model. We hope that this study will inspire researchers to reconsider the integration of large pre-trained models with contrastive learning models for image analysis."}, "2401.00642": {"original_text": "  During times of increasing antibiotic resistance and the spread of infectious\ndiseases like COVID-19, it is important to classify genes related to antibiotic\nresistance. As natural language processing has advanced with transformer-based\nlanguage models, many language models that learn characteristics of nucleotide\nsequences have also emerged. These models show good performance in classifying\nvarious features of nucleotide sequences. When classifying nucleotide\nsequences, not only the sequence itself, but also various background knowledge\nis utilized. In this study, we use not only a nucleotide sequence-based\nlanguage model but also a text language model based on PubMed articles to\nreflect more biological background knowledge in the model. We propose a method\nto fine-tune the nucleotide sequence language model and the text language model\nbased on various databases of antibiotic resistance genes. We also propose an\nLLM-based augmentation technique to supplement the data and an ensemble method\nto effectively combine the two models. We also propose a benchmark for\nevaluating the model. Our method achieved better performance than the\nnucleotide sequence language model in the drug resistance class prediction.\n", "rewritten_text": "During a time marked by increasing antibiotic resistance and the spread of infectious diseases such as COVID-19, it becomes crucial to categorize genes associated with antibiotic resistance. With the advancement of natural language processing through transformer-based language models, numerous models have emerged that can learn the characteristics of nucleotide sequences effectively. These models demonstrate strong performance in classifying various features of nucleotide sequences. When categorizing nucleotide sequences, it is essential to consider not only the sequence itself but also various background knowledge.\n\nIn this study, we not only utilize a nucleotide sequence-based language model but also incorporate a text language model based on PubMed articles to incorporate more biological background knowledge into the model. We introduce a method to fine-tune both the nucleotide sequence language model and the text language model using diverse databases of antibiotic resistance genes. Additionally, we propose an LLM-based augmentation technique to enhance the data and an ensemble method to effectively merge the two models. Furthermore, we suggest a benchmark for evaluating the model's performance.\n\nOur method outperformed the nucleotide sequence language model in predicting drug resistance classes."}, "2406.15718": {"original_text": "  As large language models (LLMs) increasingly permeate daily lives, there is a\ngrowing demand for real-time interactions that mirror human conversations.\nTraditional turn-based chat systems driven by LLMs prevent users from verbally\ninteracting with the system while it is generating responses. To overcome these\nlimitations, we adapt existing LLMs to \\textit{duplex models} so that these\nLLMs can listen for users while generating output and dynamically adjust\nthemselves to provide users with instant feedback. % such as in response to\ninterruptions. Specifically, we divide the queries and responses of\nconversations into several time slices and then adopt a\ntime-division-multiplexing (TDM) encoding-decoding strategy to\npseudo-simultaneously process these slices. Furthermore, to make LLMs\nproficient enough to handle real-time conversations, we build a fine-tuning\ndataset consisting of alternating time slices of queries and responses as well\nas covering typical feedback types in instantaneous interactions. Our\nexperiments show that although the queries and responses of conversations are\nsegmented into incomplete slices for processing, LLMs can preserve their\noriginal performance on standard benchmarks with a few fine-tuning steps on our\ndataset. Automatic and human evaluation indicate that duplex models make\nuser-AI interactions more natural and human-like, and greatly improve user\nsatisfaction compared to vanilla LLMs. Our duplex model and dataset will be\nreleased.\n", "rewritten_text": "As large language models (LLMs) become more prevalent in daily life, there is a growing demand for real-time interactions that mimic human conversations. Traditional turn-based chat systems, which are powered by LLMs, restrict users from engaging verbally with the system while it generates responses. To address these limitations, we have modified existing LLMs into \"duplex models\" that can listen to users while generating output, dynamically adjusting to provide instant feedback, including responses to interruptions. \n\nSpecifically, we segment the queries and responses of conversations into multiple time slices and utilize a time-division-multiplexing (TDM) encoding-decoding strategy to process these slices pseudo-simultaneously. Additionally, to enhance LLMs' ability to handle real-time conversations, we create a fine-tuning dataset that includes alternating time slices of queries and responses, covering various types of feedback common in instantaneous interactions. Our experiments demonstrate that despite processing conversations in incomplete slices, LLMs maintain their original performance on standard benchmarks with minimal fine-tuning on our dataset.\n\nBoth automatic and human evaluations confirm that duplex models enhance user-AI interactions, making them more natural and human-like, leading to significantly increased user satisfaction compared to conventional LLMs. Our duplex model and dataset will be made publicly available."}, "2011.01535": {"original_text": "  3D-LaneNet+ is a camera-based DNN method for anchor free 3D lane detection\nwhich is able to detect 3d lanes of any arbitrary topology such as splits,\nmerges, as well as short and perpendicular lanes. We follow recently proposed\n3D-LaneNet, and extend it to enable the detection of these previously\nunsupported lane topologies. Our output representation is an anchor free,\nsemi-local tile representation that breaks down lanes into simple lane segments\nwhose parameters can be learnt. In addition we learn, per lane instance,\nfeature embedding that reasons for the global connectivity of locally detected\nsegments to form full 3d lanes. This combination allows 3D-LaneNet+ to avoid\nusing lane anchors, non-maximum suppression, and lane model fitting as in the\noriginal 3D-LaneNet. We demonstrate the efficacy of 3D-LaneNet+ using both\nsynthetic and real world data. Results show significant improvement relative to\nthe original 3D-LaneNet that can be attributed to better generalization to\ncomplex lane topologies, curvatures and surface geometries.\n", "rewritten_text": "3D-LaneNet+ is a camera-based deep neural network (DNN) method designed for anchor-free 3D lane detection. It has the capability to detect 3D lanes of various topologies, including splits, merges, short lanes, and perpendicular lanes. Building upon the foundation of the recently proposed 3D-LaneNet, we have extended its functionality to detect these previously unsupported lane configurations.\n\nOur approach involves an output representation that is anchor-free and semi-local, utilizing a tile representation to break down lanes into simple lane segments with learnable parameters. Furthermore, we incorporate feature embedding on a per-lane basis to establish global connectivity among locally detected segments, enabling the formation of complete 3D lanes. By combining these techniques, 3D-LaneNet+ eliminates the need for lane anchors, non-maximum suppression, and lane model fitting as seen in the original 3D-LaneNet.\n\nThe effectiveness of 3D-LaneNet+ is demonstrated through experiments using both synthetic and real-world data. The results exhibit a significant improvement over the original 3D-LaneNet, showcasing enhanced generalization capabilities towards complex lane topologies, curvatures, and surface geometries."}, "1612.06496": {"original_text": "  Image segmentation is a popular area of research in computer vision that has\nmany applications in automated image processing. A recent technique called\npiecewise flat embeddings (PFE) has been proposed for use in image\nsegmentation; PFE transforms image pixel data into a lower dimensional\nrepresentation where similar pixels are pulled close together and dissimilar\npixels are pushed apart. This technique has shown promising results, but its\noriginal formulation is not computationally feasible for large images. We\npropose two improvements to the algorithm for computing PFE: first, we\nreformulate portions of the algorithm to enable various linear algebra\noperations to be performed in parallel; second, we propose utilizing an\niterative linear solver (preconditioned conjugate gradient) to quickly solve a\nlinear least-squares problem that occurs in the inner loop of a nested\niteration. With these two computational improvements, we show on a publicly\navailable image database that PFE can be sped up by an order of magnitude\nwithout sacrificing segmentation performance. Our results make this technique\nmore practical for use on large data sets, not only for image segmentation, but\nfor general data clustering problems.\n", "rewritten_text": "Image segmentation is a widely researched area in computer vision with numerous applications in automated image processing. A recent technique known as piecewise flat embeddings (PFE) has been introduced for image segmentation. PFE works by transforming image pixel data into a lower-dimensional representation, where similar pixels are brought closer together while dissimilar pixels are pushed apart. Although this technique has shown promising results, its original formulation is not computationally feasible for large images. To address this limitation, we propose two enhancements to the algorithm for computing PFE. Firstly, we suggest reformulating certain parts of the algorithm to allow for parallel execution of various linear algebra operations. Secondly, we recommend the use of an iterative linear solver, specifically the preconditioned conjugate gradient method, to efficiently solve a linear least-squares problem that arises in the inner loop of a nested iteration. By implementing these two computational improvements, we demonstrate on a publicly available image database that PFE can be accelerated by an order of magnitude without compromising segmentation performance. These findings render the technique more practical for handling large datasets, not only for image segmentation but also for general data clustering tasks."}, "1511.06783": {"original_text": "  We present a novel dataset and a novel algorithm for recognizing activities\nof daily living (ADL) from a first-person wearable camera. Handled objects are\ncrucially important for egocentric ADL recognition. For specific examination of\nobjects related to users' actions separately from other objects in an\nenvironment, many previous works have addressed the detection of handled\nobjects in images captured from head-mounted and chest-mounted cameras.\nNevertheless, detecting handled objects is not always easy because they tend to\nappear small in images. They can be occluded by a user's body. As described\nherein, we mount a camera on a user's wrist. A wrist-mounted camera can capture\nhandled objects at a large scale, and thus it enables us to skip object\ndetection process. To compare a wrist-mounted camera and a head-mounted camera,\nwe also develop a novel and publicly available dataset that includes videos and\nannotations of daily activities captured simultaneously by both cameras.\nAdditionally, we propose a discriminative video representation that retains\nspatial and temporal information after encoding frame descriptors extracted by\nConvolutional Neural Networks (CNN).\n", "rewritten_text": "We introduce a new dataset and algorithm designed for recognizing activities of daily living (ADL) using a first-person wearable camera. The identification of handled objects plays a crucial role in egocentric ADL recognition. Previous studies have focused on detecting handled objects in images taken from head-mounted and chest-mounted cameras to analyze objects related to users' actions separately from other environmental elements. However, detecting handled objects can be challenging due to their small appearance in images and potential occlusion by the user's body.\n\nIn this study, we propose mounting a camera on the user's wrist. A wrist-mounted camera offers the advantage of capturing handled objects on a larger scale, eliminating the need for a separate object detection process. To compare the effectiveness of a wrist-mounted camera versus a head-mounted camera, we have developed a new publicly available dataset containing videos and annotations of daily activities captured simultaneously by both types of cameras.\n\nFurthermore, we introduce a novel discriminative video representation that preserves spatial and temporal information by encoding frame descriptors extracted through Convolutional Neural Networks (CNN)."}, "2305.11806": {"original_text": "  Neural metrics for machine translation evaluation, such as COMET, exhibit\nsignificant improvements in their correlation with human judgments, as compared\nto traditional metrics based on lexical overlap, such as BLEU. Yet, neural\nmetrics are, to a great extent, \"black boxes\" returning a single sentence-level\nscore without transparency about the decision-making process. In this work, we\ndevelop and compare several neural explainability methods and demonstrate their\neffectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our\nstudy reveals that these metrics leverage token-level information that can be\ndirectly attributed to translation errors, as assessed through comparison of\ntoken-level neural saliency maps with Multidimensional Quality Metrics (MQM)\nannotations and with synthetically-generated critical translation errors. To\nease future research, we release our code at:\nhttps://github.com/Unbabel/COMET/tree/explainable-metrics.\n", "rewritten_text": "Neural metrics, such as COMET, used for evaluating machine translation have shown significant improvements in their correlation with human judgments compared to traditional metrics like BLEU, which are based on lexical overlap. However, neural metrics are often considered \"black boxes\" as they provide a single sentence-level score without transparency regarding their decision-making process. \n\nIn this study, we have developed and compared various neural explainability methods to demonstrate their effectiveness in interpreting state-of-the-art fine-tuned neural metrics. Our research reveals that these metrics utilize token-level information that can be directly linked to translation errors. This was determined by comparing token-level neural saliency maps with Multidimensional Quality Metrics (MQM) annotations and synthetically-generated critical translation errors.\n\nTo facilitate further research in this area, we have made our code available at: https://github.com/Unbabel/COMET/tree/explainable-metrics."}, "1906.11143": {"original_text": "  Accurate segmentation of the optic disc (OD) and cup (OC)in fundus images\nfrom different datasets is critical for glaucoma disease screening. The\ncross-domain discrepancy (domain shift) hinders the generalization of deep\nneural networks to work on different domain datasets.In this work, we present\nan unsupervised domain adaptation framework,called Boundary and Entropy-driven\nAdversarial Learning (BEAL), to improve the OD and OC segmentation performance,\nespecially on the ambiguous boundary regions. In particular, our proposed BEAL\nframe-work utilizes the adversarial learning to encourage the boundary\nprediction and mask probability entropy map (uncertainty map) of the target\ndomain to be similar to the source ones, generating more accurate boundaries\nand suppressing the high uncertainty predictions of OD and OC segmentation. We\nevaluate the proposed BEAL framework on two public retinal fundus image\ndatasets (Drishti-GS and RIM-ONE-r3), and the experiment results demonstrate\nthat our method outperforms the state-of-the-art unsupervised domain adaptation\nmethods. Codes will be available at https://github.com/EmmaW8/BEAL.\n", "rewritten_text": "Accurate segmentation of the optic disc (OD) and cup (OC) in fundus images from different datasets is crucial for glaucoma disease screening. The challenge of cross-domain discrepancy, also known as domain shift, poses a barrier to the generalization of deep neural networks across various datasets. In this study, we introduce an unsupervised domain adaptation framework named Boundary and Entropy-driven Adversarial Learning (BEAL) to enhance the segmentation performance of OD and OC, particularly in the ambiguous boundary regions.\n\nOur proposed BEAL framework leverages adversarial learning to promote similarity between the boundary prediction and mask probability entropy map (uncertainty map) of the target domain and those of the source domain. This approach leads to more precise boundaries and reduces high uncertainty predictions in OD and OC segmentation. We assess the effectiveness of the BEAL framework on two public retinal fundus image datasets (Drishti-GS and RIM-ONE-r3), and the experimental results demonstrate that our method surpasses the current state-of-the-art unsupervised domain adaptation techniques. The code will be accessible at https://github.com/EmmaW8/BEAL."}, "2108.08109": {"original_text": "  Illustrations are an essential transmission instrument. For an historian, the\nfirst step in studying their evolution in a corpus of similar manuscripts is to\nidentify which ones correspond to each other. This image collation task is\ndaunting for manuscripts separated by many lost copies, spreading over\ncenturies, which might have been completely re-organized and greatly modified\nto adapt to novel knowledge or belief and include hundreds of illustrations.\nOur contributions in this paper are threefold. First, we introduce the task of\nillustration collation and a large annotated public dataset to evaluate\nsolutions, including 6 manuscripts of 2 different texts with more than 2 000\nillustrations and 1 200 annotated correspondences. Second, we analyze state of\nthe art similarity measures for this task and show that they succeed in simple\ncases but struggle for large manuscripts when the illustrations have undergone\nvery significant changes and are discriminated only by fine details. Finally,\nwe show clear evidence that significant performance boosts can be expected by\nexploiting cycle-consistent correspondences. Our code and data are available on\nhttp://imagine.enpc.fr/~shenx/ImageCollation.\n", "rewritten_text": "Illustrations serve as crucial tools for transmitting information. When studying the evolution of illustrations in a collection of similar manuscripts, historians must first identify corresponding images. This task becomes challenging when dealing with manuscripts that are separated by lost copies, span centuries, and may have been extensively reorganized and modified to reflect new knowledge or beliefs, incorporating hundreds of illustrations.\n\nThis paper presents three main contributions. Firstly, we introduce the concept of illustration collation and provide a large annotated public dataset for evaluating solutions. The dataset includes six manuscripts from two different texts, featuring over 2,000 illustrations and 1,200 annotated correspondences. Secondly, we examine the current state-of-the-art similarity measures for this task and find that they perform well in simple cases but struggle with large manuscripts where illustrations have undergone significant changes and are distinguished only by subtle details. Lastly, we demonstrate that leveraging cycle-consistent correspondences can lead to significant performance improvements.\n\nOur code and data can be accessed at http://imagine.enpc.fr/~shenx/ImageCollation."}, "2204.01062": {"original_text": "  Several popular computer vision (CV) datasets, specifically employed for\nObject Detection (OD) in autonomous driving tasks exhibit biases due to a range\nof factors including weather and lighting conditions. These biases may impair a\nmodel's generalizability, rendering it ineffective for OD in novel and unseen\ndatasets. Especially, in autonomous driving, it may prove extremely high risk\nand unsafe for the vehicle and its surroundings. This work focuses on\nunderstanding these datasets better by identifying such \"good-weather\" bias.\nMethods to mitigate such bias which allows the OD models to perform better and\nimprove the robustness are also demonstrated. A simple yet effective OD\nframework for studying bias mitigation is proposed. Using this framework, the\nperformance on popular datasets is analyzed and a significant difference in\nmodel performance is observed. Additionally, a knowledge transfer technique and\na synthetic image corruption technique are proposed to mitigate the identified\nbias. Finally, using the DAWN dataset, the findings are validated on the OD\ntask, demonstrating the effectiveness of our techniques in mitigating\nreal-world \"good-weather\" bias. The experiments show that the proposed\ntechniques outperform baseline methods by averaged fourfold improvement.\n", "rewritten_text": "Several popular computer vision (CV) datasets used for Object Detection (OD) in autonomous driving tasks exhibit biases stemming from various factors, such as weather and lighting conditions. These biases can hinder a model's ability to generalize, making it ineffective when applied to new and unseen datasets. In the context of autonomous driving, this limitation poses a significant risk to the safety of the vehicle and its surroundings.\n\nThis study aims to enhance the understanding of these datasets by identifying biases associated with \"good-weather\" conditions. Methods to address and mitigate such biases, thereby improving the performance and robustness of OD models, are presented. A straightforward yet efficient OD framework is proposed for investigating bias mitigation. Through the application of this framework, the performance of models on popular datasets is evaluated, revealing a notable disparity in model performance.\n\nFurthermore, novel techniques including knowledge transfer and synthetic image corruption are introduced to counteract the identified biases. The effectiveness of these techniques in mitigating the real-world \"good-weather\" bias is validated using the DAWN dataset for the OD task. Experimental results demonstrate that the proposed techniques outperform baseline methods, achieving an average fourfold improvement."}, "2302.01097": {"original_text": "  Tree kernels have been proposed to be used in many areas as the automatic\nlearning of natural language applications. In this paper, we propose a new\nlinear time algorithm based on the concept of weighted tree automata for\nSubTree kernel computation. First, we introduce a new class of weighted tree\nautomata, called Root-Weighted Tree Automata, and their associated formal tree\nseries. Then we define, from this class, the SubTree automata that represent\ncompact computational models for finite tree languages. This allows us to\ndesign a theoretically guaranteed linear-time algorithm for computing the\nSubTree Kernel based on weighted tree automata intersection. The key idea\nbehind the proposed algorithm is to replace DAG reduction and nodes sorting\nsteps used in previous approaches by states equivalence classes computation\nallowed in the weighted tree automata approach. Our approach has three major\nadvantages: it is output-sensitive, it is free sensitive from the tree types\n(ordered trees versus unordered trees), and it is well adapted to any\nincremental tree kernel based learning methods. Finally, we conduct a variety\nof comparative experiments on a wide range of synthetic tree languages datasets\nadapted for a deep algorithm analysis. The obtained results show that the\nproposed algorithm outperforms state-of-the-art methods.\n", "rewritten_text": "Tree kernels have been proposed for use in various applications involving the automatic learning of natural language. In this paper, we present a novel linear time algorithm that is based on the concept of weighted tree automata for computing the SubTree kernel. Initially, we introduce a new category of weighted tree automata known as Root-Weighted Tree Automata, along with their corresponding formal tree series. Subsequently, we define the SubTree automata from this category, which serve as concise computational models for finite tree languages. This framework enables the development of a linear-time algorithm for computing the SubTree Kernel with theoretical guarantees, utilizing the intersection of weighted tree automata. The key innovation of our algorithm is the utilization of state equivalence classes computation in place of the DAG reduction and node sorting steps employed in previous methods. Our approach offers three primary advantages: it is output-sensitive, agnostic to tree types (ordered versus unordered trees), and well-suited for incremental tree kernel-based learning techniques. To validate our approach, we conduct a series of comparative experiments on diverse synthetic tree language datasets tailored for in-depth algorithmic analysis. The results demonstrate that our proposed algorithm surpasses existing state-of-the-art methods."}, "2404.10877": {"original_text": "  In this paper, we aim to generate text classification data given arbitrary\nclass definitions (i.e., user instruction), so one can train a small text\nclassifier without any human annotation or raw corpus. Compared with pioneer\nattempts, our proposed Incubator is the first framework that can handle\ncomplicated and even mutually dependent classes (e.g., \"TED Talk given by\nEducator\" and \"Other\"). Specifically, Incubator is an LLM firstly tuned on the\ninstruction-to-data mappings that we obtained from classification datasets and\ndescriptions on HuggingFace together with in-context augmentation by GPT-4. We\nthen refine Incubator by learning on the cluster centers of semantic textual\nembeddings to emphasize the uniformity and semantic diversity in generations.\nWe compare Incubator on various classification tasks with strong baselines such\nas direct LLM-based inference and training data generation by prompt\nengineering. Experiments show Incubator is able to (1) perform well on\ntraditional benchmarks, (2) take label dependency and user preference into\nconsideration, and (3) enable logical text mining by incubating multiple\nclassifiers.\n", "rewritten_text": "This paper aims to generate text classification data based on arbitrary class definitions provided by the user, allowing for the training of a small text classifier without the need for human annotation or a raw corpus. Our proposed framework, Incubator, is the first of its kind capable of handling complex and interdependent classes, such as \"TED Talk given by Educator\" and \"Other.\" The Incubator is an LLM initially fine-tuned using instruction-to-data mappings derived from classification datasets and descriptions on HuggingFace, with in-context augmentation by GPT-4. Subsequently, we enhance the Incubator by training on the cluster centers of semantic textual embeddings to emphasize uniformity and semantic diversity in the generated text. We evaluate the performance of Incubator on various classification tasks against strong baselines, including direct LLM-based inference and training data generation through prompt engineering. Our experiments demonstrate that Incubator excels in traditional benchmarks, considers label dependencies and user preferences, and facilitates logical text mining by incubating multiple classifiers."}, "2210.09782": {"original_text": "  This paper focuses on developing a more effective method of hierarchical\npropagation for semi-supervised Video Object Segmentation (VOS). Based on\nvision transformers, the recently-developed Associating Objects with\nTransformers (AOT) approach introduces hierarchical propagation into VOS and\nhas shown promising results. The hierarchical propagation can gradually\npropagate information from past frames to the current frame and transfer the\ncurrent frame feature from object-agnostic to object-specific. However, the\nincrease of object-specific information will inevitably lead to the loss of\nobject-agnostic visual information in deep propagation layers. To solve such a\nproblem and further facilitate the learning of visual embeddings, this paper\nproposes a Decoupling Features in Hierarchical Propagation (DeAOT) approach.\nFirstly, DeAOT decouples the hierarchical propagation of object-agnostic and\nobject-specific embeddings by handling them in two independent branches.\nSecondly, to compensate for the additional computation from dual-branch\npropagation, we propose an efficient module for constructing hierarchical\npropagation, i.e., Gated Propagation Module, which is carefully designed with\nsingle-head attention. Extensive experiments show that DeAOT significantly\noutperforms AOT in both accuracy and efficiency. On YouTube-VOS, DeAOT can\nachieve 86.0% at 22.4fps and 82.0% at 53.4fps. Without test-time augmentations,\nwe achieve new state-of-the-art performance on four benchmarks, i.e.,\nYouTube-VOS (86.2%), DAVIS 2017 (86.2%), DAVIS 2016 (92.9%), and VOT 2020\n(0.622). Project page: https://github.com/z-x-yang/AOT.\n", "rewritten_text": "This paper focuses on developing a more effective method of hierarchical propagation for semi-supervised Video Object Segmentation (VOS). The recently-developed Associating Objects with Transformers (AOT) approach, based on vision transformers, introduces hierarchical propagation into VOS and has shown promising results. The hierarchical propagation gradually transfers information from past frames to the current frame, transforming the current frame feature from object-agnostic to object-specific. However, the increase in object-specific information may result in the loss of object-agnostic visual information in deep propagation layers. To address this issue and enhance the learning of visual embeddings, this paper proposes the Decoupling Features in Hierarchical Propagation (DeAOT) approach.\n\nDeAOT decouples the hierarchical propagation of object-agnostic and object-specific embeddings by managing them in two independent branches. To offset the additional computation required by dual-branch propagation, an efficient module called the Gated Propagation Module is proposed for constructing hierarchical propagation. This module is meticulously designed with single-head attention. Extensive experiments demonstrate that DeAOT significantly outperforms AOT in terms of both accuracy and efficiency. On YouTube-VOS, DeAOT achieves 86.0% at 22.4fps and 82.0% at 53.4fps. Notably, without test-time augmentations, DeAOT achieves new state-of-the-art performance on four benchmarks: YouTube-VOS (86.2%), DAVIS 2017 (86.2%), DAVIS 2016 (92.9%), and VOT 2020 (0.622).\n\nFor more information, please visit the project page at: https://github.com/z-x-yang/AOT."}, "2406.18173": {"original_text": "  Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases.\n", "rewritten_text": "Managing long texts poses a challenge for large language models (LLMs) due to their limited context window sizes. This study introduces UIO-LLMs, an unbiased incremental optimization approach for memory-enhanced transformers in long-context scenarios. Initially, we conceptualize the process as a streamlined encoder-decoder framework. Here, the weights-shared encoder and decoder respectively encapsulate a context segment into memories and utilize these memories to predict outputs of the subsequent segment. Subsequently, by treating our memory-enhanced transformers as fully-connected recurrent neural networks (RNNs), we refine the training process using the Truncated Backpropagation Through Time (TBPTT) algorithm, which integrates innovative incremental optimization techniques. These techniques not only reduce time complexity but also address the bias in gradient computation through an unbiased optimization process. UIO-LLMs effectively manage long contexts, such as extending the context window of Llama2-7b-chat from 4K to 100K tokens with minimal 2% additional parameters, while maintaining nearly linear inference cost as context length increases."}, "2306.07713": {"original_text": "  Segment anything model (SAM), as the name suggests, is claimed to be capable\nof cutting out any object and demonstrates impressive zero-shot transfer\nperformance with the guidance of prompts. However, there is currently a lack of\ncomprehensive evaluation regarding its robustness under various corruptions.\nUnderstanding the robustness of SAM across different corruption scenarios is\ncrucial for its real-world deployment. Prior works show that SAM is biased\ntowards texture (style) rather than shape, motivated by which we start by\ninvestigating its robustness against style transfer, which is synthetic\ncorruption. Following by interpreting the effects of synthetic corruption as\nstyle changes, we proceed to conduct a comprehensive evaluation for its\nrobustness against 15 types of common corruption. These corruptions mainly fall\ninto categories such as digital, noise, weather, and blur, and within each\ncorruption category, we explore 5 severity levels to simulate real-world\ncorruption scenarios. Beyond the corruptions, we further assess the robustness\nof SAM against local occlusion and local adversarial patch attacks. To the best\nof our knowledge, our work is the first of its kind to evaluate the robustness\nof SAM under style change, local occlusion, and local adversarial patch\nattacks. Given that patch attacks visible to human eyes are easily detectable,\nwe further assess its robustness against global adversarial attacks that are\nimperceptible to human eyes. Overall, this work provides a comprehensive\nempirical study of the robustness of SAM, evaluating its performance under\nvarious corruptions and extending the assessment to critical aspects such as\nlocal occlusion, local adversarial patch attacks, and global adversarial\nattacks. These evaluations yield valuable insights into the practical\napplicability and effectiveness of SAM in addressing real-world challenges.\n", "rewritten_text": "The Segment Anything Model (SAM), as its name implies, is touted for its ability to cut out any object and showcase impressive zero-shot transfer performance when guided by prompts. However, there is currently a lack of comprehensive evaluation regarding its robustness under various corruptions. Understanding SAM's robustness across different corruption scenarios is crucial for its real-world deployment. Previous studies indicate that SAM exhibits a bias towards texture (style) rather than shape. Motivated by this, we begin by examining its resilience against style transfer, a form of synthetic corruption. We then analyze the impact of synthetic corruption as style changes and proceed to conduct a thorough evaluation of SAM's robustness against 15 common types of corruption. These corruptions are primarily categorized as digital, noise, weather, and blur. Within each corruption category, we explore five severity levels to simulate real-world corruption scenarios. In addition to these corruptions, we also evaluate SAM's robustness against local occlusion and local adversarial patch attacks. To the best of our knowledge, our study is the first of its kind to assess SAM's robustness under style changes, local occlusion, and local adversarial patch attacks. Since patch attacks that are visible to the human eye are easily detectable, we further evaluate SAM's resilience against global adversarial attacks that are imperceptible to human eyes. Overall, this study offers a comprehensive empirical analysis of SAM's robustness, assessing its performance under various corruptions and extending the evaluation to critical aspects such as local occlusion, local adversarial patch attacks, and global adversarial attacks. These evaluations provide valuable insights into the practical applicability and effectiveness of SAM in addressing real-world challenges."}, "2103.1399": {"original_text": "  A fundamental challenge faced by existing Fine-Grained Sketch-Based Image\nRetrieval (FG-SBIR) models is the data scarcity -- model performances are\nlargely bottlenecked by the lack of sketch-photo pairs. Whilst the number of\nphotos can be easily scaled, each corresponding sketch still needs to be\nindividually produced. In this paper, we aim to mitigate such an upper-bound on\nsketch data, and study whether unlabelled photos alone (of which they are many)\ncan be cultivated for performances gain. In particular, we introduce a novel\nsemi-supervised framework for cross-modal retrieval that can additionally\nleverage large-scale unlabelled photos to account for data scarcity. At the\ncentre of our semi-supervision design is a sequential photo-to-sketch\ngeneration model that aims to generate paired sketches for unlabelled photos.\nImportantly, we further introduce a discriminator guided mechanism to guide\nagainst unfaithful generation, together with a distillation loss based\nregularizer to provide tolerance against noisy training samples. Last but not\nleast, we treat generation and retrieval as two conjugate problems, where a\njoint learning procedure is devised for each module to mutually benefit from\neach other. Extensive experiments show that our semi-supervised model yields\nsignificant performance boost over the state-of-the-art supervised\nalternatives, as well as existing methods that can exploit unlabelled photos\nfor FG-SBIR.\n", "rewritten_text": "One of the primary challenges faced by current Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) models is the issue of data scarcity. The performance of these models is significantly hindered by the lack of sketch-photo pairs. While it is relatively easy to increase the number of photos, generating each corresponding sketch individually remains a time-consuming task. This paper aims to address this limitation by exploring the potential of utilizing unlabelled photos, which are abundant, to enhance performance.\n\nSpecifically, we propose a novel semi-supervised framework for cross-modal retrieval that leverages large-scale unlabelled photos to overcome data scarcity. Central to our semi-supervised approach is a sequential photo-to-sketch generation model designed to create paired sketches for unlabelled photos. To ensure the fidelity of the generated sketches, we introduce a discriminator-guided mechanism and a distillation loss-based regularizer to handle noisy training samples.\n\nFurthermore, we view generation and retrieval as interconnected problems and develop a joint learning procedure for both modules to benefit mutually. Extensive experiments demonstrate that our semi-supervised model outperforms state-of-the-art supervised alternatives and existing methods that utilize unlabelled photos for FG-SBIR."}, "1412.7854": {"original_text": "  Traditional object recognition approaches apply feature extraction, part\ndeformation handling, occlusion handling and classification sequentially while\nthey are independent from each other. Ouyang and Wang proposed a model for\njointly learning of all of the mentioned processes using one deep neural\nnetwork. We utilized, and manipulated their toolbox in order to apply it in car\ndetection scenarios where it had not been tested. Creating a single deep\narchitecture from these components, improves the interaction between them and\ncan enhance the performance of the whole system. We believe that the approach\ncan be used as a general purpose object detection toolbox. We tested the\nalgorithm on UIUC car dataset, and achieved an outstanding result. The accuracy\nof our method was 97 % while the previously reported results showed an accuracy\nof up to 91 %. We strongly believe that having an experiment on a larger\ndataset can show the advantage of using deep models over shallow ones.\n", "rewritten_text": "Traditional object recognition approaches typically involve applying feature extraction, part deformation handling, occlusion handling, and classification sequentially, with each process being independent from the others. Ouyang and Wang introduced a model that integrates all these processes into one deep neural network for joint learning. We adapted and customized their toolbox to implement it in car detection scenarios where it had not been previously tested. By consolidating these components into a single deep architecture, we aimed to enhance their interaction and improve the overall system performance. We consider this approach to be a versatile object detection toolbox that can be applied across various domains.\n\nOur experimentation involved testing the algorithm on the UIUC car dataset, yielding outstanding results. Our method achieved an accuracy of 97%, surpassing the previously reported accuracy of up to 91%. We are confident that conducting experiments on larger datasets will further demonstrate the advantages of utilizing deep models over shallow ones."}, "1905.03672": {"original_text": "  In this paper, we are interested in boosting the representation capability of\nconvolution neural networks which utilizing the inverted residual structure.\nBased on the success of Inverted Residual structure[Sandler et al. 2018] and\nInterleaved Low-Rank Group Convolutions[Sun et al. 2018], we rethink this two\npattern of neural network structure, rather than NAS(Neural architecture\nsearch) method[Zoph and Le 2017; Pham et al. 2018; Liu et al. 2018b], we\nintroduce uneven point-wise group convolution, which provide a novel search\nspace for designing basic blocks to obtain better trade-off between\nrepresentation capability and computational cost. Meanwhile, we propose two\nnovel information flow patterns that will enable cross-group information flow\nfor multiple group convolution layers with and without any channel\npermute/shuffle operation. Dense experiments on image classification task show\nthat our proposed model, named Seesaw-Net, achieves state-of-the-art(SOTA)\nperformance with limited computation and memory cost. Our code will be\nopen-source and available together with pre-trained models.\n", "rewritten_text": "This paper focuses on enhancing the representation capability of convolutional neural networks by utilizing the inverted residual structure. Drawing from the success of the Inverted Residual structure (Sandler et al., 2018) and Interleaved Low-Rank Group Convolutions (Sun et al., 2018), we reconsider these two patterns of neural network structure. Instead of relying on the NAS (Neural Architecture Search) method (Zoph and Le, 2017; Pham et al., 2018; Liu et al., 2018b), we introduce uneven point-wise group convolution. This introduces a new search space for designing basic blocks to achieve a better balance between representation capability and computational cost.\n\nAdditionally, we propose two novel information flow patterns that facilitate cross-group information flow for multiple group convolution layers, with or without any channel permute/shuffle operation. Extensive experiments on image classification tasks demonstrate that our proposed model, named Seesaw-Net, attains state-of-the-art (SOTA) performance while maintaining limited computation and memory costs. Our code will be open-source and made available along with pre-trained models."}, "1912.1163": {"original_text": "  Person re-identification has attracted many researchers' attention for its\nwide application, but it is still a very challenging task because only part of\nthe image information can be used for personnel matching. Most of current\nmethods uses CNN to learn to embeddings that can capture semantic similarity\ninformation among data points. Many of the state-of-the-arts methods use\ncomplex network structures with multiple branches that fuse multiple features\nwhile training or testing, using classification loss, Triplet loss or a\ncombination of the two as loss function. However, the method that using Triplet\nloss as loss function converges slowly, and the method in which pull features\nof the same class as close as possible in features space leads to poor feature\nstability. This paper will combine the ranking motivated structured loss,\nproposed a new metric learning loss function that make the features of the same\nclass are sparsely distributed into the range of small hyperspheres and the\nfeatures of different classes are uniformly distributed at a clearly angle. And\nadopted a new single-branch network structure that only using global feature\ncan also get great performance. The validity of our method is verified on the\nMarket1501 and DukeMTMC-ReID person re-identification datasets. Finally\nacquires 90.9% rank-1 accuracy and 80.8% mAP on DukeMTMC-reID, 95.3% rank-1\naccuracy and 88.7% mAP on Market1501. Codes and models are available in\nGithub.https://github.com/Qidian213/Ranked_Person_ReID.\n", "rewritten_text": "Person re-identification has garnered significant attention from researchers due to its broad range of applications. However, it remains a highly challenging task as only a portion of the image information can be utilized for matching individuals. Most current methods employ Convolutional Neural Networks (CNN) to learn embeddings that can capture semantic similarity information among data points. Many state-of-the-art methods utilize complex network structures with multiple branches to fuse multiple features during training or testing, employing classification loss, Triplet loss, or a combination of both as the loss function.\n\nNevertheless, methods using Triplet loss as the loss function tend to converge slowly, and those that aim to bring features of the same class as close as possible in feature space often result in poor feature stability. This paper introduces a novel metric learning loss function that combines ranking-motivated structured loss. This new approach ensures that features of the same class are sparsely distributed within small hyperspheres, while features of different classes are uniformly distributed at distinct angles. Additionally, a new single-branch network structure that solely utilizes global features demonstrates excellent performance.\n\nThe effectiveness of our method is validated on the Market1501 and DukeMTMC-ReID person re-identification datasets, achieving a rank-1 accuracy of 90.9% and mAP of 80.8% on DukeMTMC-ReID, and a rank-1 accuracy of 95.3% and mAP of 88.7% on Market1501. The codes and models are accessible on Github at https://github.com/Qidian213/Ranked_Person_ReID."}, "1907.1144": {"original_text": "  Pooling is one of the main elements in convolutional neural networks. The\npooling reduces the size of the feature map, enabling training and testing with\na limited amount of computation. This paper proposes a new pooling method named\nuniversal pooling. Unlike the existing pooling methods such as average pooling,\nmax pooling, and stride pooling with fixed pooling function, universal pooling\ngenerates any pooling function, depending on a given problem and dataset.\nUniversal pooling was inspired by attention methods and can be considered as a\nchannel-wise form of local spatial attention. Universal pooling is trained\njointly with the main network and it is shown that it includes the existing\npooling methods. Finally, when applied to two benchmark problems, the proposed\nmethod outperformed the existing pooling methods and performed with the\nexpected diversity, adapting to the given problem.\n", "rewritten_text": "Pooling is a key component in convolutional neural networks, as it reduces the size of the feature map, allowing for training and testing with less computational resources. This paper introduces a novel pooling technique called universal pooling. Unlike traditional methods like average pooling, max pooling, and stride pooling, which use fixed pooling functions, universal pooling can generate any pooling function based on the specific problem and dataset. Inspired by attention mechanisms, universal pooling can be viewed as a channel-wise form of local spatial attention. It is trained concurrently with the main network and encompasses existing pooling methods. In experiments on two benchmark tasks, the proposed universal pooling method outperformed traditional pooling techniques, showcasing adaptability and diversity in addressing different problems."}, "2406.17236": {"original_text": "  Although recent years have witnessed significant advancements in image\nediting thanks to the remarkable progress of text-to-image diffusion models,\nthe problem of non-rigid image editing still presents its complexities and\nchallenges. Existing methods often fail to achieve consistent results due to\nthe absence of unique identity characteristics. Thus, learning a personalized\nidentity prior might help with consistency in the edited results. In this\npaper, we explore a novel task: learning the personalized identity prior for\ntext-based non-rigid image editing. To address the problems in jointly learning\nprior and editing the image, we present LIPE, a two-stage framework designed to\ncustomize the generative model utilizing a limited set of images of the same\nsubject, and subsequently employ the model with learned prior for non-rigid\nimage editing. Experimental results demonstrate the advantages of our approach\nin various editing scenarios over past related leading methods in qualitative\nand quantitative ways.\n", "rewritten_text": "In recent years, there have been significant advancements in image editing due to the progress of text-to-image diffusion models. However, the challenge of non-rigid image editing remains complex. Existing methods often struggle to produce consistent results because they lack unique identity characteristics. Therefore, incorporating a personalized identity prior could improve the consistency of edited images. This paper introduces a novel task: learning the personalized identity prior for text-based non-rigid image editing. To tackle the issues of simultaneously learning the prior and editing the image, we propose LIPE, a two-stage framework. LIPE is designed to customize the generative model using a limited set of images of the same subject and then utilize the model with the learned prior for non-rigid image editing. Experimental results demonstrate the advantages of our approach in various editing scenarios compared to previous leading methods, both qualitatively and quantitatively."}, "2406.12679": {"original_text": "  Large Language Models (LLMs) are increasingly being used in educational and\nlearning applications. Research has demonstrated that controlling for style, to\nfit the needs of the learner, fosters increased understanding, promotes\ninclusion, and helps with knowledge distillation. To understand the\ncapabilities and limitations of contemporary LLMs in style control, we\nevaluated five state-of-the-art models: GPT-3.5, GPT-4, GPT-4o, Llama-3, and\nMistral-instruct- 7B across two style control tasks. We observed significant\ninconsistencies in the first task, with model performances averaging between\n5th and 8th grade reading levels for tasks intended for first-graders, and\nstandard deviations up to 27.6. For our second task, we observed a\nstatistically significant improvement in performance from 0.02 to 0.26.\nHowever, we find that even without stereotypes in reference texts, LLMs often\ngenerated culturally insensitive content during their tasks. We provide a\nthorough analysis and discussion of the results.\n", "rewritten_text": "Large Language Models (LLMs) are increasingly utilized in educational and learning applications. Research has shown that adjusting for style to cater to the learner's needs enhances comprehension, fosters inclusivity, and aids in knowledge transfer. In order to assess the capabilities and constraints of modern LLMs in style control, we conducted an evaluation on five cutting-edge models: GPT-3.5, GPT-4, GPT-4o, Llama-3, and Mistral-instruct-7B, focusing on two style control tasks. In the first task, we noted significant inconsistencies, with model performances ranging from 5th to 8th grade reading levels for tasks designed for first-graders, and standard deviations reaching up to 27.6. In the second task, we observed a statistically significant performance enhancement from 0.02 to 0.26. Nevertheless, we discovered that even in the absence of stereotypes in the reference texts, LLMs frequently generated culturally insensitive content during their tasks. A comprehensive analysis and discussion of the findings are presented."}, "1907.06882": {"original_text": "  Majority of state-of-the-art monocular depth estimation methods are\nsupervised learning approaches. The success of such approaches heavily depends\non the high-quality depth labels which are expensive to obtain. Some recent\nmethods try to learn depth networks by leveraging unsupervised cues from\nmonocular videos which are easier to acquire but less reliable. In this paper,\nwe propose to resolve this dilemma by transferring knowledge from synthetic\nvideos with easily obtainable ground-truth depth labels. Due to the stylish\ndifference between synthetic and real images, we propose a\ntemporally-consistent domain adaptation (TCDA) approach that simultaneously\nexplores labels in the synthetic domain and temporal constraints in the videos\nto improve style transfer and depth prediction. Furthermore, we make use of the\nground-truth optical flow and pose information in the synthetic data to learn\nmoving mask and pose prediction networks. The learned moving masks can filter\nout moving regions that produces erroneous temporal constraints and the\nestimated poses provide better initializations for estimating temporal\nconstraints. Experimental results demonstrate the effectiveness of our method\nand comparable performance against state-of-the-art.\n", "rewritten_text": "The majority of state-of-the-art monocular depth estimation methods are supervised learning approaches. The success of these approaches heavily relies on high-quality depth labels, which can be costly to obtain. Some recent methods attempt to train depth networks by utilizing unsupervised cues from monocular videos, which are easier to acquire but less reliable. In this paper, we propose a solution to this challenge by transferring knowledge from synthetic videos that have readily available ground-truth depth labels. Due to the significant differences between synthetic and real images, we introduce a temporally-consistent domain adaptation (TCDA) approach that simultaneously leverages labels in the synthetic domain and temporal constraints in the videos to enhance style transfer and depth prediction. Additionally, we utilize ground-truth optical flow and pose information in the synthetic data to train moving mask and pose prediction networks. The learned moving masks can filter out moving regions that introduce incorrect temporal constraints, while the estimated poses offer improved initializations for estimating temporal constraints. Experimental results demonstrate the effectiveness of our method and show comparable performance to state-of-the-art techniques."}, "2104.14839": {"original_text": "  Recently, various neural encoder-decoder models pioneered by Seq2Seq\nframework have been proposed to achieve the goal of generating more abstractive\nsummaries by learning to map input text to output text. At a high level, such\nneural models can freely generate summaries without any constraint on the words\nor phrases used. Moreover, their format is closer to human-edited summaries and\noutput is more readable and fluent. However, the neural model's abstraction\nability is a double-edged sword. A commonly observed problem with the generated\nsummaries is the distortion or fabrication of factual information in the\narticle. This inconsistency between the original text and the summary has\ncaused various concerns over its applicability, and the previous evaluation\nmethods of text summarization are not suitable for this issue. In response to\nthe above problems, the current research direction is predominantly divided\ninto two categories, one is to design fact-aware evaluation metrics to select\noutputs without factual inconsistency errors, and the other is to develop new\nsummarization systems towards factual consistency. In this survey, we focus on\npresenting a comprehensive review of these fact-specific evaluation methods and\ntext summarization models.\n", "rewritten_text": "Recently, a number of neural encoder-decoder models, led by the Seq2Seq framework, have emerged with the aim of producing more abstractive summaries by learning to translate input text into output text. These neural models have the capability to generate summaries without being restricted by specific words or phrases, resulting in summaries that closely resemble those edited by humans and are more coherent and readable. However, the abstraction capacity of these neural models presents a challenge. One common issue observed in the generated summaries is the distortion or fabrication of factual information from the original article. This discrepancy between the original text and the summary has raised concerns regarding its practicality, as existing evaluation methods for text summarization are inadequate in addressing this problem. To tackle these challenges, current research efforts are primarily focused on two main approaches: developing fact-aware evaluation metrics to identify outputs free from factual inconsistencies, and creating new summarization systems that prioritize factual accuracy. This survey aims to provide a comprehensive overview of these fact-specific evaluation methods and text summarization models."}, "1701.01619": {"original_text": "  We present an approach to effectively use millions of images with noisy\nannotations in conjunction with a small subset of cleanly-annotated images to\nlearn powerful image representations. One common approach to combine clean and\nnoisy data is to first pre-train a network using the large noisy dataset and\nthen fine-tune with the clean dataset. We show this approach does not fully\nleverage the information contained in the clean set. Thus, we demonstrate how\nto use the clean annotations to reduce the noise in the large dataset before\nfine-tuning the network using both the clean set and the full set with reduced\nnoise. The approach comprises a multi-task network that jointly learns to clean\nnoisy annotations and to accurately classify images. We evaluate our approach\non the recently released Open Images dataset, containing ~9 million images,\nmultiple annotations per image and over 6000 unique classes. For the small\nclean set of annotations we use a quarter of the validation set with ~40k\nimages. Our results demonstrate that the proposed approach clearly outperforms\ndirect fine-tuning across all major categories of classes in the Open Image\ndataset. Further, our approach is particularly effective for a large number of\nclasses with wide range of noise in annotations (20-80% false positive\nannotations).\n", "rewritten_text": "We propose an approach to effectively utilize millions of images with noisy annotations in combination with a small subset of cleanly-annotated images to develop robust image representations. A common method for integrating clean and noisy data involves initially pre-training a network using the extensive noisy dataset and subsequently fine-tuning it with the clean dataset. However, we find that this approach does not fully exploit the information present in the clean set. Therefore, we illustrate a method to utilize the clean annotations to reduce noise in the large dataset before fine-tuning the network using both the clean set and the full set with reduced noise. This method involves a multi-task network that simultaneously learns to clean noisy annotations and accurately classify images. Our approach is evaluated on the recently released Open Images dataset, which comprises approximately 9 million images, multiple annotations per image, and over 6000 unique classes. For the small clean annotation set, we utilize a quarter of the validation set consisting of around 40,000 images. Our results demonstrate that the proposed approach significantly outperforms direct fine-tuning across all major categories of classes in the Open Images dataset. Furthermore, our approach proves to be particularly effective for a large number of classes with a wide range of noise in annotations (20-80% false positive annotations)."}, "1905.04215": {"original_text": "  We study the problem of unsupervised domain adaptation which aims to adapt\nmodels trained on a labeled source domain to a completely unlabeled target\ndomain. Recently, the cluster assumption has been applied to unsupervised\ndomain adaptation and achieved strong performance. One critical factor in\nsuccessful training of the cluster assumption is to impose the\nlocally-Lipschitz constraint to the model. Existing methods only impose the\nlocally-Lipschitz constraint around the training points while miss the other\nareas, such as the points in-between training data. In this paper, we address\nthis issue by encouraging the model to behave linearly in-between training\npoints. We propose a new regularization method called Virtual Mixup Training\n(VMT), which is able to incorporate the locally-Lipschitz constraint to the\nareas in-between training data. Unlike the traditional mixup model, our method\nconstructs the combination samples without using the label information,\nallowing it to apply to unsupervised domain adaptation. The proposed method is\ngeneric and can be combined with most existing models such as the recent\nstate-of-the-art model called VADA. Extensive experiments demonstrate that VMT\nsignificantly improves the performance of VADA on six domain adaptation\nbenchmark datasets. For the challenging task of adapting MNIST to SVHN, VMT can\nimprove the accuracy of VADA by over 30\\%. Code is available at\n\\url{https://github.com/xudonmao/VMT}.\n", "rewritten_text": "We investigate the issue of unsupervised domain adaptation, which involves adjusting models trained on a labeled source domain to an entirely unlabeled target domain. Recently, the cluster assumption has been utilized in unsupervised domain adaptation with notable success. A key element in effectively implementing the cluster assumption during training is to enforce the locally-Lipschitz constraint on the model. However, current methods only apply this constraint around the training points, neglecting other areas such as the points between training data. In this study, we tackle this limitation by promoting linear behavior of the model between training points. We introduce a novel regularization technique named Virtual Mixup Training (VMT), which integrates the locally-Lipschitz constraint into the regions between training data points. Unlike conventional mixup models, our approach generates combined samples without relying on label information, making it suitable for unsupervised domain adaptation. The proposed method is versatile and can be integrated with various existing models, including the state-of-the-art VADA model. Extensive experiments illustrate that VMT significantly enhances the performance of VADA across six domain adaptation benchmark datasets. Particularly, in the challenging task of adapting MNIST to SVHN, VMT boosts the accuracy of VADA by more than 30%. The code is accessible at \\url{https://github.com/xudonmao/VMT}."}, "2311.17590": {"original_text": "  Achieving high synchronization in the synthesis of realistic, speech-driven\ntalking head videos presents a significant challenge. Traditional Generative\nAdversarial Networks (GAN) struggle to maintain consistent facial identity,\nwhile Neural Radiance Fields (NeRF) methods, although they can address this\nissue, often produce mismatched lip movements, inadequate facial expressions,\nand unstable head poses. A lifelike talking head requires synchronized\ncoordination of subject identity, lip movements, facial expressions, and head\nposes. The absence of these synchronizations is a fundamental flaw, leading to\nunrealistic and artificial outcomes. To address the critical issue of\nsynchronization, identified as the \"devil\" in creating realistic talking heads,\nwe introduce SyncTalk. This NeRF-based method effectively maintains subject\nidentity, enhancing synchronization and realism in talking head synthesis.\nSyncTalk employs a Face-Sync Controller to align lip movements with speech and\ninnovatively uses a 3D facial blendshape model to capture accurate facial\nexpressions. Our Head-Sync Stabilizer optimizes head poses, achieving more\nnatural head movements. The Portrait-Sync Generator restores hair details and\nblends the generated head with the torso for a seamless visual experience.\nExtensive experiments and user studies demonstrate that SyncTalk outperforms\nstate-of-the-art methods in synchronization and realism. We recommend watching\nthe supplementary video: https://ziqiaopeng.github.io/synctalk\n", "rewritten_text": "Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos is a significant challenge. Traditional Generative Adversarial Networks (GAN) struggle to maintain consistent facial identity, while Neural Radiance Fields (NeRF) methods, although capable of addressing this issue, often result in mismatched lip movements, inadequate facial expressions, and unstable head poses. A lifelike talking head necessitates synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw that leads to unrealistic and artificial outcomes.\n\nTo tackle the critical issue of synchronization, identified as the \"devil\" in creating realistic talking heads, we introduce SyncTalk. This NeRF-based method effectively preserves subject identity, enhancing synchronization and realism in talking head synthesis. SyncTalk utilizes a Face-Sync Controller to align lip movements with speech and creatively employs a 3D facial blendshape model to capture precise facial expressions. Our Head-Sync Stabilizer optimizes head poses, resulting in more natural head movements. The Portrait-Sync Generator restores hair details and seamlessly blends the generated head with the torso for a cohesive visual experience.\n\nExtensive experiments and user studies demonstrate that SyncTalk surpasses state-of-the-art methods in synchronization and realism. We recommend viewing the supplementary video at: https://ziqiaopeng.github.io/synctalk."}, "1901.07656": {"original_text": "  Word vector representations are well developed tools for various NLP and\nMachine Learning tasks and are known to retain significant semantic and\nsyntactic structure of languages. But they are prone to carrying and amplifying\nbias which can perpetrate discrimination in various applications. In this work,\nwe explore new simple ways to detect the most stereotypically gendered words in\nan embedding and remove the bias from them. We verify how names are masked\ncarriers of gender bias and then use that as a tool to attenuate bias in\nembeddings. Further, we extend this property of names to show how names can be\nused to detect other types of bias in the embeddings such as bias based on\nrace, ethnicity, and age.\n", "rewritten_text": "Word vector representations are advanced tools in the fields of NLP and Machine Learning, known for preserving significant semantic and syntactic structures of languages. However, they are susceptible to carrying and amplifying biases that can lead to discrimination in various applications. In this study, we propose novel methods to identify the most stereotypically gendered words in an embedding and mitigate bias associated with them. We demonstrate that names serve as prominent carriers of gender bias and leverage this insight to reduce bias in embeddings. Moreover, we leverage the discriminatory potential of names to detect biases related to race, ethnicity, and age within embeddings."}, "2210.02318": {"original_text": "  Recently, two-stage Deformable DETR introduced the query-based two-stage\nhead, a new type of two-stage head different from the region-based two-stage\nheads of classical detectors as Faster R-CNN. In query-based two-stage heads,\nthe second stage selects one feature per detection processed by a transformer,\ncalled the query, as opposed to pooling a rectangular grid of features\nprocessed by CNNs as in region-based detectors. In this work, we improve the\nquery-based head by improving the prior of the cross-attention operation with\nanchors, significantly speeding up the convergence while increasing its\nperformance. Additionally, we empirically show that by improving the\ncross-attention prior, auxiliary losses and iterative bounding box mechanisms\ntypically used by DETR-based detectors are no longer needed. By combining the\nbest of both the classical and the DETR-based detectors, our FQDet head peaks\nat 45.4 AP on the 2017 COCO validation set when using a ResNet-50+TPN backbone,\nonly after training for 12 epochs using the 1x schedule. We outperform other\nhigh-performing two-stage heads such as e.g. Cascade R-CNN, while using the\nsame backbone and while being computationally cheaper. Additionally, when using\nthe large ResNeXt-101-DCN+TPN backbone and multi-scale testing, our FQDet head\nachieves 52.9 AP on the 2017 COCO test-dev set after only 12 epochs of\ntraining. Code is released at https://github.com/CedricPicron/FQDet .\n", "rewritten_text": "Recently, the two-stage Deformable DETR introduced a new type of two-stage head known as the query-based two-stage head. This differs from the region-based two-stage heads found in classical detectors like Faster R-CNN. In the query-based two-stage heads, the second stage selects one feature per detection processed by a transformer, referred to as the query. This is in contrast to pooling a rectangular grid of features processed by CNNs, as seen in region-based detectors.\n\nIn this study, we enhance the query-based head by improving the prior of the cross-attention operation with anchors. This enhancement significantly accelerates convergence while also boosting performance. Furthermore, we demonstrate empirically that by enhancing the cross-attention prior, auxiliary losses and iterative bounding box mechanisms typically utilized by DETR-based detectors are no longer necessary.\n\nBy combining the strengths of both classical and DETR-based detectors, our FQDet head achieves a peak of 45.4 AP on the 2017 COCO validation set when utilizing a ResNet-50+TPN backbone, after only 12 epochs of training using the 1x schedule. We surpass other high-performing two-stage heads such as Cascade R-CNN, while maintaining the same backbone and being more computationally efficient.\n\nMoreover, when employing the large ResNeXt-101-DCN+TPN backbone and multi-scale testing, our FQDet head attains 52.9 AP on the 2017 COCO test-dev set after just 12 epochs of training. The code is available at https://github.com/CedricPicron/FQDet."}, "1411.634": {"original_text": "  While widely acknowledged as highly effective in computer vision, multi-label\nMRFs with non-convex priors are difficult to optimize. To tackle this, we\nintroduce an algorithm that iteratively approximates the original energy with\nan appropriately weighted surrogate energy that is easier to minimize. Our\nalgorithm guarantees that the original energy decreases at each iteration. In\nparticular, we consider the scenario where the global minimizer of the weighted\nsurrogate energy can be obtained by a multi-label graph cut algorithm, and show\nthat our algorithm then lets us handle of large variety of non-convex priors.\nWe demonstrate the benefits of our method over state-of-the-art MRF energy\nminimization techniques on stereo and inpainting problems.\n", "rewritten_text": "While multi-label Markov Random Fields (MRFs) with non-convex priors are widely recognized as highly effective in computer vision, they are challenging to optimize. In order to address this issue, we propose an algorithm that iteratively approximates the original energy function with a properly weighted surrogate energy function that is easier to minimize. Our algorithm ensures that the original energy function decreases with each iteration. Specifically, we focus on the scenario where the global minimizer of the weighted surrogate energy function can be obtained using a multi-label graph cut algorithm. We demonstrate that our algorithm enables us to handle a wide range of non-convex priors effectively. Furthermore, we showcase the advantages of our approach over state-of-the-art MRF energy minimization techniques in solving stereo and inpainting problems."}, "2009.02649": {"original_text": "  Causality visualization can help people understand temporal chains of events,\nsuch as messages sent in a distributed system, cause and effect in a historical\nconflict, or the interplay between political actors over time. However, as the\nscale and complexity of these event sequences grows, even these visualizations\ncan become overwhelming to use. In this paper, we propose the use of textual\nnarratives as a data-driven storytelling method to augment causality\nvisualization. We first propose a design space for how textual narratives can\nbe used to describe causal data. We then present results from a crowdsourced\nuser study where participants were asked to recover causality information from\ntwo causality visualizations--causal graphs and Hasse diagrams--with and\nwithout an associated textual narrative. Finally, we describe CAUSEWORKS, a\ncausality visualization system for understanding how specific interventions\ninfluence a causal model. The system incorporates an automatic textual\nnarrative mechanism based on our design space. We validate CAUSEWORKS through\ninterviews with experts who used the system for understanding complex events.\n", "rewritten_text": "Causality visualization is a valuable tool for helping individuals comprehend the temporal chains of events, such as messages transmitted in a distributed system, cause and effect in a historical conflict, or the interactions among political actors over time. However, as the complexity and scale of these event sequences increase, even these visualizations can become overwhelming. In this paper, we propose the integration of textual narratives as a data-driven storytelling method to enhance causality visualization. \n\nInitially, we outline a design framework for utilizing textual narratives to depict causal data. Subsequently, we present findings from a crowdsourced user study in which participants were tasked with extracting causality information from two types of causality visualizations\u2014causal graphs and Hasse diagrams\u2014with and without an accompanying textual narrative. \n\nLastly, we introduce CAUSEWORKS, a causality visualization system designed to facilitate the understanding of how specific interventions impact a causal model. This system includes an automated textual narrative feature based on our design framework. We validate CAUSEWORKS through interviews with experts who utilized the system to comprehend complex events."}, "2204.07183": {"original_text": "  We propose an interactive approach for 3D instance segmentation, where users\ncan iteratively collaborate with a deep learning model to segment objects in a\n3D point cloud directly. Current methods for 3D instance segmentation are\ngenerally trained in a fully-supervised fashion, which requires large amounts\nof costly training labels, and does not generalize well to classes unseen\nduring training. Few works have attempted to obtain 3D segmentation masks using\nhuman interactions. Existing methods rely on user feedback in the 2D image\ndomain. As a consequence, users are required to constantly switch between 2D\nimages and 3D representations, and custom architectures are employed to combine\nmultiple input modalities. Therefore, integration with existing standard 3D\nmodels is not straightforward. The core idea of this work is to enable users to\ninteract directly with 3D point clouds by clicking on desired 3D objects of\ninterest~(or their background) to interactively segment the scene in an\nopen-world setting. Specifically, our method does not require training data\nfrom any target domain, and can adapt to new environments where no appropriate\ntraining sets are available. Our system continuously adjusts the object\nsegmentation based on the user feedback and achieves accurate dense 3D\nsegmentation masks with minimal human effort (few clicks per object). Besides\nits potential for efficient labeling of large-scale and varied 3D datasets, our\napproach, where the user directly interacts with the 3D environment, enables\nnew applications in AR/VR and human-robot interaction.\n", "rewritten_text": "We present an interactive approach for 3D instance segmentation, allowing users to collaborate iteratively with a deep learning model to directly segment objects in a 3D point cloud. Current methods for 3D instance segmentation are typically trained in a fully-supervised manner, necessitating large amounts of expensive training labels and exhibiting poor generalization to unseen classes during training. Few studies have explored obtaining 3D segmentation masks through human interactions, with existing methods relying on user feedback in the 2D image domain. Consequently, users must frequently switch between 2D images and 3D representations, requiring the use of custom architectures to integrate multiple input modalities, making integration with standard 3D models challenging. The fundamental concept of this work is to empower users to interact directly with 3D point clouds by clicking on desired 3D objects of interest (or their background) to interactively segment the scene in an open-world setting. Notably, our method does not rely on training data from any specific target domain and can adapt to new environments lacking appropriate training sets. Our system dynamically adjusts object segmentation based on user feedback, achieving precise dense 3D segmentation masks with minimal human effort (few clicks per object). In addition to its potential for efficiently labeling large-scale and diverse 3D datasets, our approach, which involves direct user interaction with the 3D environment, opens up new possibilities for applications in AR/VR and human-robot interaction."}, "2012.0536": {"original_text": "  Semantic aware reconstruction is more advantageous than geometric-only\nreconstruction for future robotic and AR/VR applications because it represents\nnot only where things are, but also what things are. Object-centric mapping is\na task to build an object-level reconstruction where objects are separate and\nmeaningful entities that convey both geometry and semantic information. In this\npaper, we present MOLTR, a solution to object-centric mapping using only\nmonocular image sequences and camera poses. It is able to localise, track, and\nreconstruct multiple objects in an online fashion when an RGB camera captures a\nvideo of the surrounding. Given a new RGB frame, MOLTR firstly applies a\nmonocular 3D detector to localise objects of interest and extract their shape\ncodes that represent the object shapes in a learned embedding space. Detections\nare then merged to existing objects in the map after data association. Motion\nstate (i.e. kinematics and the motion status) of each object is tracked by a\nmultiple model Bayesian filter and object shape is progressively refined by\nfusing multiple shape code. We evaluate localisation, tracking, and\nreconstruction on benchmarking datasets for indoor and outdoor scenes, and show\nsuperior performance over previous approaches.\n", "rewritten_text": "Semantic-aware reconstruction offers more advantages than geometric-only reconstruction for future robotic and AR/VR applications. This is because it not only indicates the location of objects but also provides information about their identity. Object-centric mapping involves creating a reconstruction at the object level, where objects are treated as distinct and meaningful entities that convey both geometric and semantic details. In this study, we introduce MOLTR, a method for object-centric mapping that utilizes monocular image sequences and camera poses. MOLTR can localize, track, and reconstruct multiple objects in real-time as an RGB camera records the surroundings. When presented with a new RGB frame, MOLTR first employs a monocular 3D detector to identify objects of interest and extract their shape codes, which represent the object shapes in a learned embedding space. The detections are then integrated into the existing map through data association. The motion state (including kinematics and motion status) of each object is tracked using a multiple model Bayesian filter, while the object shape is refined progressively by combining multiple shape codes. We assess the performance of MOLTR in terms of localization, tracking, and reconstruction using benchmark datasets for indoor and outdoor scenes, demonstrating its superior performance compared to previous methods."}, "1605.06052": {"original_text": "  Similarity scores in face recognition represent the proximity between pairs\nof images as computed by a matching algorithm. Given a large set of images and\nthe proximities between all pairs, a similarity score space is defined. Cluster\nanalysis was applied to the similarity score space to develop various\ntaxonomies. Given the number of subjects in the dataset, we used hierarchical\nmethods to aggregate images of the same subject. We also explored the hierarchy\nabove and below the subject level, including clusters that reflect gender and\nethnicity. Evidence supports the existence of clustering by race, gender,\nsubject, and illumination condition.\n", "rewritten_text": "In face recognition, similarity scores indicate the closeness between image pairs calculated by a matching algorithm. By analyzing a vast collection of images and their proximities, a similarity score space is established. Cluster analysis was utilized on this space to create different taxonomies. To group images of the same subject based on the dataset size, hierarchical methods were employed. Additionally, exploration was conducted at levels above and below the subject, encompassing clusters related to gender and ethnicity. There is evidence to suggest clustering based on race, gender, subject, and lighting conditions."}, "2401.00653": {"original_text": "  Deceptive images can be shared in seconds with social networking services,\nposing substantial risks. Tampering traces, such as boundary artifacts and\nhigh-frequency information, have been significantly emphasized by massive\nnetworks in the Image Manipulation Localization (IML) field. However, they are\nprone to image post-processing operations, which limit the generalization and\nrobustness of existing methods. We present a novel Prompt-IML framework. We\nobserve that humans tend to discern the authenticity of an image based on both\nsemantic and high-frequency information, inspired by which, the proposed\nframework leverages rich semantic knowledge from pre-trained visual foundation\nmodels to assist IML. We are the first to design a framework that utilizes\nvisual foundation models specially for the IML task. Moreover, we design a\nFeature Alignment and Fusion module to align and fuse features of semantic\nfeatures with high-frequency features, which aims at locating tampered regions\nfrom multiple perspectives. Experimental results demonstrate that our model can\nachieve better performance on eight typical fake image datasets and outstanding\nrobustness.\n", "rewritten_text": "Deceptive images can be quickly shared through social networking services, posing significant risks. The Image Manipulation Localization (IML) field has placed considerable emphasis on identifying tampering traces, such as boundary artifacts and high-frequency information, through large networks. However, these methods are vulnerable to image post-processing operations, limiting their generalizability and robustness. Introducing a novel framework called Prompt-IML, we have observed that humans often determine the authenticity of an image based on both semantic and high-frequency information. Drawing inspiration from this observation, our framework utilizes rich semantic knowledge from pre-trained visual foundation models to enhance IML. We are the first to develop a framework that specifically leverages visual foundation models for the IML task. Additionally, we have designed a Feature Alignment and Fusion module to align and merge semantic features with high-frequency features, enabling the identification of tampered regions from multiple perspectives. Experimental results show that our model outperforms existing methods on eight common fake image datasets, demonstrating exceptional performance and robustness."}, "1809.08495": {"original_text": "  Earlier work demonstrates the promise of deep-learning-based approaches for\npoint cloud segmentation; however, these approaches need to be improved to be\npractically useful. To this end, we introduce a new model SqueezeSegV2 that is\nmore robust to dropout noise in LiDAR point clouds. With improved model\nstructure, training loss, batch normalization and additional input channel,\nSqueezeSegV2 achieves significant accuracy improvement when trained on real\ndata. Training models for point cloud segmentation requires large amounts of\nlabeled point-cloud data, which is expensive to obtain. To sidestep the cost of\ncollection and annotation, simulators such as GTA-V can be used to create\nunlimited amounts of labeled, synthetic data. However, due to domain shift,\nmodels trained on synthetic data often do not generalize well to the real\nworld. We address this problem with a domain-adaptation training pipeline\nconsisting of three major components: 1) learned intensity rendering, 2)\ngeodesic correlation alignment, and 3) progressive domain calibration. When\ntrained on real data, our new model exhibits segmentation accuracy improvements\nof 6.0-8.6% over the original SqueezeSeg. When training our new model on\nsynthetic data using the proposed domain adaptation pipeline, we nearly double\ntest accuracy on real-world data, from 29.0% to 57.4%. Our source code and\nsynthetic dataset will be open-sourced.\n", "rewritten_text": "Previous research has shown the potential of deep learning approaches for segmenting point clouds; however, these methods require enhancements to be practically effective. In this study, we present a novel model, SqueezeSegV2, designed to be more resilient to dropout noise in LiDAR point clouds. By refining the model's structure, training loss, batch normalization, and incorporating an additional input channel, SqueezeSegV2 demonstrates a significant increase in accuracy when trained on authentic data. The process of training models for point cloud segmentation demands a substantial amount of labeled point-cloud data, which can be costly to acquire. To mitigate the expenses associated with data collection and annotation, simulators like GTA-V can generate limitless labeled synthetic data. Nevertheless, models trained on synthetic data often struggle to generalize to real-world scenarios due to domain shift. To tackle this challenge, we propose a domain-adaptation training pipeline comprising three key components: 1) learned intensity rendering, 2) geodesic correlation alignment, and 3) progressive domain calibration. Our new model exhibits a 6.0-8.6% enhancement in segmentation accuracy over the original SqueezeSeg when trained on real data. Furthermore, by training our new model on synthetic data using the suggested domain adaptation pipeline, we observe a nearly twofold increase in test accuracy on real-world data, from 29.0% to 57.4%. We plan to release our source code and synthetic dataset as open-source resources."}, "2209.11214": {"original_text": "  Automatic tomato disease recognition from leaf images is vital to avoid crop\nlosses by applying control measures on time. Even though recent deep\nlearning-based tomato disease recognition methods with classical training\nprocedures showed promising recognition results, they demand large labelled\ndata and involve expensive training. The traditional deep learning models\nproposed for tomato disease recognition also consume high memory and storage\nbecause of a high number of parameters. While lightweight networks overcome\nsome of these issues to a certain extent, they continue to show low performance\nand struggle to handle imbalanced data. In this paper, a novel Siamese\nnetwork-based lightweight framework is proposed for automatic tomato leaf\ndisease recognition. This framework achieves the highest accuracy of 96.97% on\nthe tomato subset obtained from the PlantVillage dataset and 95.48% on the\nTaiwan tomato leaf disease dataset. Experimental results further confirm that\nthe proposed framework is effective with imbalanced and small data. The\nbackbone deep network integrated with this framework is lightweight with\napproximately 2.9629 million trainable parameters, which is way lower than\nexisting lightweight deep networks.\n", "rewritten_text": "Recognizing tomato diseases automatically from leaf images is crucial for preventing crop losses through timely application of control measures. While recent deep learning-based methods for tomato disease recognition have shown promising results, they typically require a large amount of labeled data and involve costly training procedures. Traditional deep learning models designed for tomato disease recognition also tend to consume high memory and storage due to a large number of parameters. Although lightweight networks have been developed to address some of these challenges, they often exhibit lower performance and struggle with imbalanced data.\n\nIn this study, we propose a novel lightweight framework based on a Siamese network for automatic recognition of tomato leaf diseases. Our framework achieves an impressive accuracy of 96.97% on the tomato subset from the PlantVillage dataset and 95.48% on the Taiwan tomato leaf disease dataset. Experimental results demonstrate the effectiveness of our framework, particularly with imbalanced and small datasets. The backbone deep network integrated into this framework is lightweight, with approximately 2.9629 million trainable parameters, significantly fewer than existing lightweight deep networks."}, "1912.01496": {"original_text": "  Stories are diverse and highly personalized, resulting in a large possible\noutput space for story generation. Existing end-to-end approaches produce\nmonotonous stories because they are limited to the vocabulary and knowledge in\na single training dataset. This paper introduces KG-Story, a three-stage\nframework that allows the story generation model to take advantage of external\nKnowledge Graphs to produce interesting stories. KG-Story distills a set of\nrepresentative words from the input prompts, enriches the word set by using\nexternal knowledge graphs, and finally generates stories based on the enriched\nword set. This distill-enrich-generate framework allows the use of external\nresources not only for the enrichment phase, but also for the distillation and\ngeneration phases. In this paper, we show the superiority of KG-Story for\nvisual storytelling, where the input prompt is a sequence of five photos and\nthe output is a short story. Per the human ranking evaluation, stories\ngenerated by KG-Story are on average ranked better than that of the\nstate-of-the-art systems. Our code and output stories are available at\nhttps://github.com/zychen423/KE-VIST.\n", "rewritten_text": "Stories are diverse and highly personalized, leading to a vast potential output space for story generation. Existing end-to-end approaches often result in monotonous stories due to their confinement to the vocabulary and knowledge within a single training dataset. This paper presents KG-Story, a three-stage framework that empowers the story generation model to leverage external Knowledge Graphs in order to craft engaging narratives.\n\nKG-Story operates through a distill-enrich-generate process, wherein it first distills a set of representative words from the input prompts, then enriches this word set by incorporating external knowledge graphs, and ultimately generates stories based on the enriched vocabulary. This framework not only utilizes external resources for enrichment but also integrates them into the distillation and generation phases.\n\nThe study demonstrates the effectiveness of KG-Story in visual storytelling scenarios, where the input prompt consists of a sequence of five photos and the desired output is a concise story. According to human ranking evaluations, stories generated by KG-Story consistently outperform those produced by state-of-the-art systems. The code and output stories can be accessed at https://github.com/zychen423/KE-VIST."}, "2307.16825": {"original_text": "  With sufficient paired training samples, the supervised deep learning methods\nhave attracted much attention in image denoising because of their superior\nperformance. However, it is still very challenging to widely utilize the\nsupervised methods in real cases due to the lack of paired noisy-clean images.\nMeanwhile, most self-supervised denoising methods are ineffective as well when\napplied to the real-world denoising tasks because of their strict assumptions\nin applications. For example, as a typical method for self-supervised\ndenoising, the original blind spot network (BSN) assumes that the noise is\npixel-wise independent, which is much different from the real cases. To solve\nthis problem, we propose a novel self-supervised real image denoising framework\nnamed Sampling Difference As Perturbation (SDAP) based on Random Sub-samples\nGeneration (RSG) with a cyclic sample difference loss. Specifically, we dig\ndeeper into the properties of BSN to make it more suitable for real noise.\nSurprisingly, we find that adding an appropriate perturbation to the training\nimages can effectively improve the performance of BSN. Further, we propose that\nthe sampling difference can be considered as perturbation to achieve better\nresults. Finally we propose a new BSN framework in combination with our RSG\nstrategy. The results show that it significantly outperforms other\nstate-of-the-art self-supervised denoising methods on real-world datasets. The\ncode is available at https://github.com/p1y2z3/SDAP.\n", "rewritten_text": "Supervised deep learning methods for image denoising have garnered significant attention due to their superior performance when provided with sufficient paired training samples. However, the widespread utilization of these methods in real-world scenarios remains challenging due to the scarcity of paired noisy-clean images. Additionally, many self-supervised denoising methods prove ineffective in practical denoising tasks due to their stringent assumptions during application.\n\nFor instance, the original Blind Spot Network (BSN), a common self-supervised denoising method, assumes pixel-wise independent noise, which diverges from real-world scenarios. To address this issue, we introduce a novel self-supervised real image denoising framework called Sampling Difference As Perturbation (SDAP) based on Random Sub-samples Generation (RSG) with a cyclic sample difference loss. By delving deeper into the characteristics of BSN, we enhance its suitability for real noise by introducing appropriate perturbations to the training images, leading to improved performance.\n\nMoreover, we propose that sampling difference can serve as a perturbation to yield superior results. Finally, we present a new BSN framework integrated with our RSG strategy, demonstrating significant performance enhancements over existing state-of-the-art self-supervised denoising methods on real-world datasets. The code for our approach is accessible at https://github.com/p1y2z3/SDAP."}, "2409.16685": {"original_text": "  Integrating aerial imagery-based scene generation into applications like\nautonomous driving and gaming enhances realism in 3D environments, but\nchallenges remain in creating detailed content for occluded areas and ensuring\nreal-time, consistent rendering. In this paper, we introduce Skyeyes, a novel\nframework that can generate photorealistic sequences of ground view images\nusing only aerial view inputs, thereby creating a ground roaming experience.\nMore specifically, we combine a 3D representation with a view consistent\ngeneration model, which ensures coherence between generated images. This method\nallows for the creation of geometrically consistent ground view images, even\nwith large view gaps. The images maintain improved spatial-temporal coherence\nand realism, enhancing scene comprehension and visualization from aerial\nperspectives. To the best of our knowledge, there are no publicly available\ndatasets that contain pairwise geo-aligned aerial and ground view imagery.\nTherefore, we build a large, synthetic, and geo-aligned dataset using Unreal\nEngine. Both qualitative and quantitative analyses on this synthetic dataset\ndisplay superior results compared to other leading synthesis approaches. See\nthe project page for more results:\nhttps://chaoren2357.github.io/website-skyeyes/.\n", "rewritten_text": "Incorporating aerial imagery-based scene generation into applications such as autonomous driving and gaming elevates realism in 3D environments. However, challenges persist in developing detailed content for obscured areas and ensuring real-time, consistent rendering. This paper introduces Skyeyes, a novel framework that can produce photorealistic sequences of ground view images solely from aerial view inputs, thereby offering a ground roaming experience. Specifically, we merge a 3D representation with a view-consistent generation model to maintain coherence among generated images. This approach enables the creation of geometrically consistent ground view images, even in the presence of significant view gaps. The resulting images exhibit enhanced spatial-temporal coherence and realism, improving scene comprehension and visualization from aerial perspectives. Notably, there are currently no publicly available datasets containing pairwise geo-aligned aerial and ground view imagery. To address this gap, we have constructed a large, synthetic, and geo-aligned dataset using Unreal Engine. Both qualitative and quantitative analyses conducted on this synthetic dataset demonstrate superior results compared to other leading synthesis approaches. For further details and results, please visit the project page at: https://chaoren2357.github.io/website-skyeyes/."}, "2210.02844": {"original_text": "  In this paper, we explore the following question: Are synonym substitution\nattacks really synonym substitution attacks (SSAs)? We approach this question\nby examining how SSAs replace words in the original sentence and show that\nthere are still unresolved obstacles that make current SSAs generate invalid\nadversarial samples. We reveal that four widely used word substitution methods\ngenerate a large fraction of invalid substitution words that are ungrammatical\nor do not preserve the original sentence's semantics. Next, we show that the\nsemantic and grammatical constraints used in SSAs for detecting invalid word\nreplacements are highly insufficient in detecting invalid adversarial samples.\n", "rewritten_text": "This paper delves into the question: Are attacks known as synonym substitution attacks (SSAs) truly synonym substitution attacks? We address this inquiry by analyzing how SSAs replace words in the original sentence and demonstrate that there are still unresolved challenges that cause current SSAs to produce invalid adversarial samples. We find that four commonly used word substitution methods yield a significant number of invalid substitution words that are either ungrammatical or fail to maintain the original sentence's meaning. Furthermore, we establish that the semantic and grammatical constraints employed in SSAs to identify improper word replacements are largely inadequate in detecting invalid adversarial samples."}, "2307.09769": {"original_text": "  Unsupervised domain adaptation (UDA) has increasingly gained interests for\nits capacity to transfer the knowledge learned from a labeled source domain to\nan unlabeled target domain. However, typical UDA methods require concurrent\naccess to both the source and target domain data, which largely limits its\napplication in medical scenarios where source data is often unavailable due to\nprivacy concern. To tackle the source data-absent problem, we present a novel\ntwo-stage source-free domain adaptation (SFDA) framework for medical image\nsegmentation, where only a well-trained source segmentation model and unlabeled\ntarget data are available during domain adaptation. Specifically, in the\nprototype-anchored feature alignment stage, we first utilize the weights of the\npre-trained pixel-wise classifier as source prototypes, which preserve the\ninformation of source features. Then, we introduce the bi-directional transport\nto align the target features with class prototypes by minimizing its expected\ncost. On top of that, a contrastive learning stage is further devised to\nutilize those pixels with unreliable predictions for a more compact target\nfeature distribution. Extensive experiments on a cross-modality medical\nsegmentation task demonstrate the superiority of our method in large domain\ndiscrepancy settings compared with the state-of-the-art SFDA approaches and\neven some UDA methods. Code is available at\nhttps://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) has garnered increasing interest for its ability to transfer knowledge learned from a labeled source domain to an unlabeled target domain. However, typical UDA methods require simultaneous access to both the source and target domain data, which significantly limits their application in medical scenarios where source data is often unavailable due to privacy concerns. To address the issue of missing source data, we propose a novel two-stage source-free domain adaptation (SFDA) framework for medical image segmentation. This framework utilizes only a well-trained source segmentation model and unlabeled target data during domain adaptation.\n\nIn the first stage, called prototype-anchored feature alignment, we leverage the weights of the pre-trained pixel-wise classifier as source prototypes to retain the information of source features. Subsequently, we introduce bi-directional transport to align the target features with class prototypes by minimizing the expected cost. Additionally, a contrastive learning stage is implemented to utilize pixels with unreliable predictions for a more compact target feature distribution.\n\nExtensive experiments on a cross-modality medical segmentation task demonstrate the superiority of our method in scenarios with significant domain discrepancies compared to state-of-the-art SFDA approaches and even some UDA methods. The code for our approach is available at https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA."}, "1811.08015": {"original_text": "  This paper introduces the problem of automatic font pairing. Font pairing is\nan important design task that is difficult for novices. Given a font selection\nfor one part of a document (e.g., header), our goal is to recommend a font to\nbe used in another part (e.g., body) such that the two fonts used together look\nvisually pleasing. There are three main challenges in font pairing. First, this\nis a fine-grained problem, in which the subtle distinctions between fonts may\nbe important. Second, rules and conventions of font pairing given by human\nexperts are difficult to formalize. Third, font pairing is an asymmetric\nproblem in that the roles played by header and body fonts are not\ninterchangeable. To address these challenges, we propose automatic font pairing\nthrough learning visual relationships from large-scale human-generated font\npairs. We introduce a new database for font pairing constructed from millions\nof PDF documents available on the Internet. We propose two font pairing\nalgorithms: dual-space k-NN and asymmetric similarity metric learning (ASML).\nThese two methods automatically learn fine-grained relationships from\nlarge-scale data. We also investigate several baseline methods based on the\nrules from professional designers. Experiments and user studies demonstrate the\neffectiveness of our proposed dataset and methods.\n", "rewritten_text": "This paper presents the issue of automatic font pairing, which is a challenging design task for beginners. The objective is to recommend a font for one part of a document (e.g., header) that complements another part (e.g., body) to achieve visual harmony. Font pairing poses three main challenges. Firstly, it is a nuanced problem where subtle differences between fonts are crucial. Secondly, formalizing the rules and conventions of font pairing established by human experts is complex. Thirdly, font pairing is asymmetric as the roles of header and body fonts are not interchangeable.\n\nTo tackle these challenges, we propose an automatic font pairing approach that leverages visual relationships learned from a vast collection of human-generated font pairs. We introduce a novel font pairing database created from millions of publicly available PDF documents on the Internet. Our proposed algorithms, dual-space k-NN and asymmetric similarity metric learning (ASML), automatically capture fine-grained relationships from extensive data. Additionally, we explore various baseline methods derived from professional designers' guidelines. Through experiments and user studies, we demonstrate the effectiveness of our dataset and methodologies."}, "2403.15119": {"original_text": "  Person re-identification (ReID) has made great strides thanks to the\ndata-driven deep learning techniques. However, the existing benchmark datasets\nlack diversity, and models trained on these data cannot generalize well to\ndynamic wild scenarios. To meet the goal of improving the explicit\ngeneralization of ReID models, we develop a new Open-World, Diverse,\nCross-Spatial-Temporal dataset named OWD with several distinct features. 1)\nDiverse collection scenes: multiple independent open-world and highly dynamic\ncollecting scenes, including streets, intersections, shopping malls, etc. 2)\nDiverse lighting variations: long time spans from daytime to nighttime with\nabundant illumination changes. 3) Diverse person status: multiple camera\nnetworks in all seasons with normal/adverse weather conditions and diverse\npedestrian appearances (e.g., clothes, personal belongings, poses, etc.). 4)\nProtected privacy: invisible faces for privacy critical applications. To\nimprove the implicit generalization of ReID, we further propose a Latent Domain\nExpansion (LDE) method to develop the potential of source data, which decouples\ndiscriminative identity-relevant and trustworthy domain-relevant features and\nimplicitly enforces domain-randomized identity feature space expansion with\nricher domain diversity to facilitate domain invariant representations. Our\ncomprehensive evaluations with most benchmark datasets in the community are\ncrucial for progress, although this work is far from the grand goal toward\nopen-world and dynamic wild applications.\n", "rewritten_text": "Person re-identification (ReID) has advanced significantly due to data-driven deep learning techniques. However, the current benchmark datasets lack diversity, leading to models trained on these data struggling to generalize effectively in dynamic real-world scenarios. In order to enhance the explicit generalization capabilities of ReID models, we have introduced a novel dataset called Open-World, Diverse, Cross-Spatial-Temporal (OWD) with unique features:\n\n1) Diverse collection scenes: OWD includes multiple independent open-world environments with high dynamism, such as streets, intersections, and shopping malls.\n2) Diverse lighting variations: The dataset covers extended time periods ranging from daytime to nighttime, encompassing a wide range of lighting conditions.\n3) Diverse person statuses: OWD features multiple camera networks capturing various seasons, weather conditions, and diverse pedestrian appearances, including clothing, personal belongings, and poses.\n4) Protected privacy: For privacy-sensitive applications, OWD ensures the anonymity of individuals by concealing their faces.\n\nTo enhance the implicit generalization of ReID, we have introduced the Latent Domain Expansion (LDE) method. This approach leverages the potential of source data by separating discriminative identity-relevant features from domain-relevant features, thereby promoting domain-randomized expansion of the identity feature space with increased domain diversity. This facilitates the creation of domain-invariant representations.\n\nOur extensive evaluations, conducted with numerous benchmark datasets within the research community, are essential for progress. However, it is important to note that while this work represents a significant step forward, achieving the ultimate goal of open-world and dynamic wild applications remains a challenging endeavor."}, "1906.03731": {"original_text": "  Attention mechanisms have recently boosted performance on a range of NLP\ntasks. Because attention layers explicitly weight input components'\nrepresentations, it is also often assumed that attention can be used to\nidentify information that models found important (e.g., specific contextualized\nword tokens). We test whether that assumption holds by manipulating attention\nweights in already-trained text classification models and analyzing the\nresulting differences in their predictions. While we observe some ways in which\nhigher attention weights correlate with greater impact on model predictions, we\nalso find many ways in which this does not hold, i.e., where gradient-based\nrankings of attention weights better predict their effects than their\nmagnitudes. We conclude that while attention noisily predicts input components'\noverall importance to a model, it is by no means a fail-safe indicator.\n", "rewritten_text": "Recent advancements in attention mechanisms have significantly improved performance across various natural language processing (NLP) tasks. These mechanisms involve attention layers that explicitly assign weights to input components' representations. It is commonly assumed that attention can help identify important information that the models have deemed significant, such as specific contextualized word tokens. To test the validity of this assumption, we conducted experiments where we manipulated the attention weights in pre-trained text classification models and analyzed the resulting changes in their predictions.\n\nOur findings reveal that while there are instances where higher attention weights correspond to a greater impact on model predictions, there are also cases where this relationship does not hold true. Specifically, we observed that gradient-based rankings of attention weights are more reliable in predicting their effects than the actual magnitudes of the weights themselves. Therefore, we conclude that while attention mechanisms can provide some insight into the overall importance of input components to a model, they are not infallible indicators."}, "1611.08134": {"original_text": "  Color based re-identification methods usually rely on a distance function to\nmeasure the similarity between individuals. In this paper we study the behavior\nof several histogram distance measures in different color spaces. We wonder\nwhether there is a particular histogram distance measure better than others,\nlikewise also, if there is a color space that present better discrimination\nfeatures. Several experiments are designed and evaluated in several images to\nobtain measures against various color spaces. We test in several image\ndatabases. A measure ranking is generated to calculate the area under the CMC,\nthis area is the indicator used to evaluate which distance measure and color\nspace present the best performance for the considered databases. Also, other\nparameters such as the image division in horizontal stripes and number of\nhistogram bins, have been studied.\n", "rewritten_text": "Color-based re-identification methods typically utilize a distance function to assess the similarity between individuals. This study examines the performance of various histogram distance measures across different color spaces. The research aims to determine if there is a superior histogram distance measure and if certain color spaces offer better discrimination features. Multiple experiments are conducted and evaluated using various images to compare measures across different color spaces. The evaluation includes testing on multiple image databases. A measure ranking is generated to calculate the area under the CMC curve, which serves as an indicator to determine the optimal distance measure and color space for the databases under consideration. Additionally, other parameters such as image division into horizontal stripes and the number of histogram bins are also investigated."}, "2003.05065": {"original_text": "  For many of the physical phenomena around us, we have developed sophisticated\nmodels explaining their behavior. Nevertheless, measuring physical properties\nfrom visual observations is challenging due to the high number of causally\nunderlying physical parameters -- including material properties and external\nforces. In this paper, we propose to measure latent physical properties for\ncloth in the wind without ever having seen a real example before. Our solution\nis an iterative refinement procedure with simulation at its core. The algorithm\ngradually updates the physical model parameters by running a simulation of the\nobserved phenomenon and comparing the current simulation to a real-world\nobservation. The correspondence is measured using an embedding function that\nmaps physically similar examples to nearby points. We consider a case study of\ncloth in the wind, with curling flags as our leading example -- a seemingly\nsimple phenomena but physically highly involved. Based on the physics of cloth\nand its visual manifestation, we propose an instantiation of the embedding\nfunction. For this mapping, modeled as a deep network, we introduce a spectral\nlayer that decomposes a video volume into its temporal spectral power and\ncorresponding frequencies. Our experiments demonstrate that the proposed method\ncompares favorably to prior work on the task of measuring cloth material\nproperties and external wind force from a real-world video.\n", "rewritten_text": "Many physical phenomena are explained by sophisticated models, but measuring physical properties from visual observations is challenging due to numerous underlying physical parameters, such as material properties and external forces. This paper introduces a novel approach to measuring latent physical properties for cloth in the wind without prior examples. The proposed solution involves an iterative refinement procedure with simulation as its core. The algorithm updates physical model parameters by simulating the observed phenomenon and comparing it to real-world observations. A correspondence metric is used, employing an embedding function that maps similar physical examples to nearby points. The study focuses on cloth in the wind, specifically curling flags, which may seem simple but are actually complex physically. Drawing on the physics of cloth and its visual characteristics, an instantiation of the embedding function is proposed. This instantiation, modeled as a deep network, includes a spectral layer that decomposes a video volume into temporal spectral power and corresponding frequencies. Experimental results show that the proposed method outperforms previous approaches in measuring cloth material properties and external wind force from real-world videos."}, "2408.14860": {"original_text": "  This paper presents DiffSurf, a transformer-based denoising diffusion model\nfor generating and reconstructing 3D surfaces. Specifically, we design a\ndiffusion transformer architecture that predicts noise from noisy 3D surface\nvertices and normals. With this architecture, DiffSurf is able to generate 3D\nsurfaces in various poses and shapes, such as human bodies, hands, animals and\nman-made objects. Further, DiffSurf is versatile in that it can address various\n3D downstream tasks including morphing, body shape variation and 3D human mesh\nfitting to 2D keypoints. Experimental results on 3D human model benchmarks\ndemonstrate that DiffSurf can generate shapes with greater diversity and higher\nquality than previous generative models. Furthermore, when applied to the task\nof single-image 3D human mesh recovery, DiffSurf achieves accuracy comparable\nto prior techniques at a near real-time rate.\n", "rewritten_text": "This paper introduces DiffSurf, a transformer-based denoising diffusion model designed for generating and reconstructing 3D surfaces. The model utilizes a diffusion transformer architecture to predict noise from noisy 3D surface vertices and normals. With this architecture, DiffSurf can produce 3D surfaces in various poses and shapes, including human bodies, hands, animals, and man-made objects. Additionally, DiffSurf is versatile and can be applied to a range of 3D downstream tasks, such as morphing, body shape variation, and fitting 3D human meshes to 2D keypoints. Experimental results on 3D human model benchmarks show that DiffSurf can generate shapes with greater diversity and higher quality compared to previous generative models. Moreover, when used for single-image 3D human mesh recovery, DiffSurf achieves accuracy similar to prior techniques at a nearly real-time rate."}, "2111.1341": {"original_text": "  Manual annotation of medical images is highly subjective, leading to\ninevitable and huge annotation biases. Deep learning models may surpass human\nperformance on a variety of tasks, but they may also mimic or amplify these\nbiases. Although we can have multiple annotators and fuse their annotations to\nreduce stochastic errors, we cannot use this strategy to handle the bias caused\nby annotators' preferences. In this paper, we highlight the issue of\nannotator-related biases on medical image segmentation tasks, and propose a\nPreference-involved Annotation Distribution Learning (PADL) framework to\naddress it from the perspective of disentangling an annotator's preference from\nstochastic errors using distribution learning so as to produce not only a meta\nsegmentation but also the segmentation possibly made by each annotator. Under\nthis framework, a stochastic error modeling (SEM) module estimates the meta\nsegmentation map and average stochastic error map, and a series of human\npreference modeling (HPM) modules estimate each annotator's segmentation and\nthe corresponding stochastic error. We evaluated our PADL framework on two\nmedical image benchmarks with different imaging modalities, which have been\nannotated by multiple medical professionals, and achieved promising performance\non all five medical image segmentation tasks.\n", "rewritten_text": "Manual annotation of medical images is highly subjective, resulting in significant annotation biases. While deep learning models have the potential to outperform humans in various tasks, they may also replicate or magnify these biases. Although employing multiple annotators and combining their annotations can help mitigate stochastic errors, it does not effectively address biases stemming from annotators' preferences. This paper sheds light on the issue of annotator-related biases in medical image segmentation tasks and introduces a novel approach called Preference-involved Annotation Distribution Learning (PADL) framework to tackle this challenge. The PADL framework aims to disentangle an annotator's preference from stochastic errors through distribution learning, enabling the generation of both a meta segmentation and individual annotator-based segmentations. Within this framework, a Stochastic Error Modeling (SEM) module estimates the meta segmentation map and average stochastic error map, while a series of Human Preference Modeling (HPM) modules estimate each annotator's segmentation and their respective stochastic errors. The effectiveness of the PADL framework was evaluated on two medical image benchmarks featuring different imaging modalities, annotated by multiple medical professionals. The results demonstrated promising performance across all five medical image segmentation tasks."}, "2312.06598": {"original_text": "  Early action recognition is an important and challenging problem that enables\nthe recognition of an action from a partially observed video stream where the\nactivity is potentially unfinished or even not started. In this work, we\npropose a novel model that learns a prototypical representation of the full\naction for each class and uses it to regularize the architecture and the visual\nrepresentations of the partial observations. Our model is very simple in design\nand also efficient. We decompose the video into short clips, where a visual\nencoder extracts features from each clip independently. Later, a decoder\naggregates together in an online fashion features from all the clips for the\nfinal class prediction. During training, for each partial observation, the\nmodel is jointly trained to both predict the label as well as the action\nprototypical representation which acts as a regularizer. We evaluate our method\non multiple challenging real-world datasets and outperform the current\nstate-of-the-art by a significant margin. For example, on early recognition\nobserving only the first 10% of each video, our method improves the SOTA by\n+2.23 Top-1 accuracy on Something-Something-v2, +3.55 on UCF-101, +3.68 on\nSSsub21, and +5.03 on EPIC-Kitchens-55, where prior work used either\nmulti-modal inputs (e.g. optical-flow) or batched inference. Finally, we also\npresent exhaustive ablation studies to motivate the design choices we made, as\nwell as gather insights regarding what our model is learning semantically.\n", "rewritten_text": "Early action recognition is a crucial and complex task that involves identifying an action from a video stream that may be incomplete or not yet initiated. In this study, we introduce a novel model that learns a prototypical representation of the complete action for each class. This representation is utilized to regularize both the architecture and visual features of the partial observations. Our model is characterized by its simplicity in design and efficiency.\n\nThe video is segmented into short clips, with a visual encoder extracting features independently from each clip. Subsequently, a decoder aggregates features from all clips in an online manner for the final class prediction. During training, the model is simultaneously trained to predict the label and the action prototypical representation, which serves as a regularizer.\n\nWe assess the effectiveness of our approach on various challenging real-world datasets and demonstrate significant improvements over the current state-of-the-art methods. Notably, in early recognition scenarios where only the initial 10% of each video is observed, our method achieves a notable increase in Top-1 accuracy. Specifically, we observe improvements of +2.23 on Something-Something-v2, +3.55 on UCF-101, +3.68 on SSsub21, and +5.03 on EPIC-Kitchens-55. It is worth mentioning that prior works relied on either multi-modal inputs (e.g., optical-flow) or batched inference.\n\nFurthermore, we conduct comprehensive ablation studies to elucidate the rationale behind our design decisions and gain insights into the semantic learning capabilities of our model."}, "2007.1562": {"original_text": "  Named Entity Recognition (NER) is a fundamental NLP task, commonly formulated\nas classification over a sequence of tokens. Morphologically-Rich Languages\n(MRLs) pose a challenge to this basic formulation, as the boundaries of Named\nEntities do not necessarily coincide with token boundaries, rather, they\nrespect morphological boundaries. To address NER in MRLs we then need to answer\ntwo fundamental questions, namely, what are the basic units to be labeled, and\nhow can these units be detected and classified in realistic settings, i.e.,\nwhere no gold morphology is available. We empirically investigate these\nquestions on a novel NER benchmark, with parallel tokenlevel and morpheme-level\nNER annotations, which we develop for Modern Hebrew, a morphologically\nrich-and-ambiguous language. Our results show that explicitly modeling\nmorphological boundaries leads to improved NER performance, and that a novel\nhybrid architecture, in which NER precedes and prunes morphological\ndecomposition, greatly outperforms the standard pipeline, where morphological\ndecomposition strictly precedes NER, setting a new performance bar for both\nHebrew NER and Hebrew morphological decomposition tasks.\n", "rewritten_text": "Named Entity Recognition (NER) is a crucial task in Natural Language Processing (NLP), typically involving the classification of a sequence of tokens. Morphologically-Rich Languages (MRLs) present a challenge to this task due to the fact that the boundaries of Named Entities may not align with token boundaries, but rather with morphological boundaries. To tackle NER in MRLs, it is essential to address two key questions: what are the fundamental units to be labeled, and how can these units be identified and classified in real-world scenarios where gold morphology data is not available. \n\nWe conducted an empirical investigation into these questions using a new NER benchmark that includes both token-level and morpheme-level NER annotations, specifically designed for Modern Hebrew, a language known for its rich and ambiguous morphology. Our findings demonstrate that explicitly considering morphological boundaries leads to enhanced NER performance. Furthermore, we introduce a novel hybrid architecture where NER precedes and refines morphological decomposition, which significantly outperforms the traditional pipeline approach where morphological decomposition precedes NER. This sets a new benchmark for both Hebrew NER and Hebrew morphological decomposition tasks."}, "2102.12452": {"original_text": "  Probing classifiers have emerged as one of the prominent methodologies for\ninterpreting and analyzing deep neural network models of natural language\nprocessing. The basic idea is simple -- a classifier is trained to predict some\nlinguistic property from a model's representations -- and has been used to\nexamine a wide variety of models and properties. However, recent studies have\ndemonstrated various methodological limitations of this approach. This article\ncritically reviews the probing classifiers framework, highlighting their\npromises, shortcomings, and advances.\n", "rewritten_text": "Probing classifiers have become a prominent methodology for interpreting and analyzing deep neural network models in natural language processing. The fundamental concept is straightforward: a classifier is trained to predict a linguistic property from a model's representations. This approach has been applied to investigate a diverse range of models and properties. Nevertheless, recent studies have revealed several methodological limitations associated with this method. This article provides a critical review of the probing classifiers framework, emphasizing their potential, drawbacks, and advancements."}, "2408.11567": {"original_text": "  Point cloud analysis has achieved significant development and is\nwell-performed in multiple downstream tasks like point cloud classification and\nsegmentation, etc. Being conscious of the simplicity of the position encoding\nstructure in Transformer-based architectures, we attach importance to the\nposition encoding as a high-dimensional part and the patch encoder to offer\nmulti-scale information. Together with the sequential Transformer, the whole\nmodule with position encoding comprehensively constructs a multi-scale feature\nabstraction module that considers both the local parts from the patch and the\nglobal parts from center points as position encoding. With only a few\nparameters, the position embedding module fits the setting of PEFT\n(Parameter-Efficient Fine-Tuning) tasks pretty well. Thus we unfreeze these\nparameters as a fine-tuning part. At the same time, we review the existing\nprompt and adapter tuning methods, proposing a fresh way of prompts and\nsynthesizing them with adapters as dynamic adjustments. Our Proposed method of\nPEFT tasks, namely PPT, with only 1.05% of parameters for training, gets\nstate-of-the-art results in several mainstream datasets, such as 95.01%\naccuracy in the ScanObjectNN OBJ_BG dataset. Codes will be released at\nhttps://github.com/zsc000722/PPT.\n", "rewritten_text": "Point cloud analysis has made significant advancements and excels in various downstream tasks, such as point cloud classification and segmentation. Recognizing the straightforwardness of position encoding in Transformer-based architectures, we emphasize the importance of position encoding as a high-dimensional component and the patch encoder to provide multi-scale information. By combining the sequential Transformer, the entire module, including position encoding, forms a multi-scale feature abstraction module that incorporates both local parts from the patch and global parts from center points as position encoding. The position embedding module, with minimal parameters, aligns well with Parameter-Efficient Fine-Tuning (PEFT) tasks, leading us to unfreeze these parameters for fine-tuning. Additionally, we revisit existing prompt and adapter tuning methods, introducing a novel approach to prompts and integrating them with adapters for dynamic adjustments. Our proposed method for PEFT tasks, named PPT, achieves state-of-the-art results with only 1.05% of parameters for training, including a remarkable 95.01% accuracy in the ScanObjectNN OBJ_BG dataset. The code will be available at https://github.com/zsc000722/PPT."}, "1607.0003": {"original_text": "  Human evaluation of machine translation normally uses sentence-level measures\nsuch as relative ranking or adequacy scales. However, these provide no insight\ninto possible errors, and do not scale well with sentence length. We argue for\na semantics-based evaluation, which captures what meaning components are\nretained in the MT output, thus providing a more fine-grained analysis of\ntranslation quality, and enabling the construction and tuning of\nsemantics-based MT. We present a novel human semantic evaluation measure, Human\nUCCA-based MT Evaluation (HUME), building on the UCCA semantic representation\nscheme. HUME covers a wider range of semantic phenomena than previous methods\nand does not rely on semantic annotation of the potentially garbled MT output.\nWe experiment with four language pairs, demonstrating HUME's broad\napplicability, and report good inter-annotator agreement rates and correlation\nwith human adequacy scores.\n", "rewritten_text": "Human evaluation of machine translation typically relies on sentence-level measures such as relative ranking or adequacy scales. However, these methods do not offer insights into potential errors and struggle to accommodate longer sentences. We propose a semantics-based evaluation approach that focuses on identifying the retained meaning components in machine translation outputs. This approach allows for a more detailed analysis of translation quality and facilitates the development and refinement of semantics-based machine translation systems.\n\nIntroducing a new human semantic evaluation measure called Human UCCA-based MT Evaluation (HUME), we leverage the UCCA semantic representation scheme. HUME offers a broader coverage of semantic phenomena compared to existing methods and does not require semantic annotation of potentially distorted machine translation outputs. Through experiments involving four language pairs, we demonstrate the versatility of HUME and highlight its strong inter-annotator agreement rates and correlation with human adequacy scores."}, "2205.0809": {"original_text": "  Event cameras are bio-inspired sensors that capture per-pixel asynchronous\nintensity change rather than the synchronous absolute intensity frames captured\nby a classical camera sensor. Such cameras are ideal for robotics applications\nsince they have high temporal resolution, high dynamic range and low latency.\nHowever, due to their high temporal resolution, event cameras are particularly\nsensitive to flicker such as from fluorescent or LED lights. During every cycle\nfrom bright to dark, pixels that image a flickering light source generate many\nevents that provide little or no useful information for a robot, swamping the\nuseful data in the scene. In this paper, we propose a novel linear filter to\npreprocess event data to remove unwanted flicker events from an event stream.\nThe proposed algorithm achieves over 4.6 times relative improvement in the\nsignal-to-noise ratio when compared to the raw event stream due to the\neffective removal of flicker from fluorescent lighting. Thus, it is ideally\nsuited to robotics applications that operate in indoor settings or scenes\nilluminated by flickering light sources.\n", "rewritten_text": "Event cameras, which are bio-inspired sensors, capture per-pixel asynchronous intensity changes rather than the synchronous absolute intensity frames captured by classical camera sensors. These cameras are well-suited for robotics applications due to their high temporal resolution, high dynamic range, and low latency. However, their high temporal resolution makes event cameras particularly sensitive to flicker, such as that from fluorescent or LED lights. Pixels capturing a flickering light source generate numerous events during each cycle from bright to dark, inundating the scene with uninformative data and obscuring useful information for a robot.\n\nIn this paper, we introduce a novel linear filter designed to preprocess event data and eliminate unwanted flicker events from an event stream. The proposed algorithm significantly enhances the signal-to-noise ratio by over 4.6 times compared to the raw event stream, effectively eliminating flicker from fluorescent lighting. This makes it an ideal solution for robotics applications operating in indoor environments or scenes illuminated by flickering light sources."}, "2205.00627": {"original_text": "  White matter fiber clustering is an important strategy for white matter\nparcellation, which enables quantitative analysis of brain connections in\nhealth and disease. In combination with expert neuroanatomical labeling,\ndata-driven white matter fiber clustering is a powerful tool for creating\natlases that can model white matter anatomy across individuals. While widely\nused fiber clustering approaches have shown good performance using classical\nunsupervised machine learning techniques, recent advances in deep learning\nreveal a promising direction toward fast and effective fiber clustering. In\nthis work, we propose a novel deep learning framework for white matter fiber\nclustering, Deep Fiber Clustering (DFC), which solves the unsupervised\nclustering problem as a self-supervised learning task with a domain-specific\npretext task to predict pairwise fiber distances. This process learns a\nhigh-dimensional embedding feature representation for each fiber, regardless of\nthe order of fiber points reconstructed during tractography. We design a novel\nnetwork architecture that represents input fibers as point clouds and allows\nthe incorporation of additional sources of input information from gray matter\nparcellation to improve anatomical coherence of clusters. In addition, DFC\nconducts outlier removal naturally by rejecting fibers with low cluster\nassignment probability. We evaluate DFC on three independently acquired\ncohorts, including data from 220 individuals across genders, ages (young and\nelderly adults), and different health conditions (healthy control and multiple\nneuropsychiatric disorders). We compare DFC to several state-of-the-art white\nmatter fiber clustering algorithms. Experimental results demonstrate superior\nperformance of DFC in terms of cluster compactness, generalization ability,\nanatomical coherence, and computational efficiency.\n", "rewritten_text": "White matter fiber clustering is a crucial technique for white matter parcellation, facilitating the quantitative analysis of brain connections in both health and disease. When combined with expert neuroanatomical labeling, data-driven white matter fiber clustering becomes a potent tool for constructing atlases that can accurately represent white matter anatomy across individuals. While traditional fiber clustering methods have demonstrated strong performance using classical unsupervised machine learning techniques, recent advancements in deep learning present a promising avenue towards rapid and efficient fiber clustering.\n\nIn this study, we introduce a novel deep learning framework for white matter fiber clustering, known as Deep Fiber Clustering (DFC). DFC addresses the unsupervised clustering problem by treating it as a self-supervised learning task, employing a domain-specific pretext task to predict pairwise fiber distances. This approach results in the acquisition of a high-dimensional embedding feature representation for each fiber, independent of the order of fiber points reconstructed during tractography. We have developed a unique network architecture that interprets input fibers as point clouds, allowing for the integration of additional input information from gray matter parcellation to enhance the anatomical coherence of clusters.\n\nMoreover, DFC naturally eliminates outliers by disregarding fibers with low cluster assignment probability. To assess the performance of DFC, we conducted evaluations on three distinct cohorts, comprising data from 220 individuals spanning various genders, ages (including young and elderly adults), and health conditions (healthy controls and individuals with multiple neuropsychiatric disorders). We compared DFC against several state-of-the-art white matter fiber clustering algorithms. The experimental results showcase the superior performance of DFC in terms of cluster compactness, generalization ability, anatomical coherence, and computational efficiency."}, "1707.08401": {"original_text": "  In the last two decades Computer Aided Diagnostics (CAD) systems were\ndeveloped to help radiologists analyze screening mammograms. The benefits of\ncurrent CAD technologies appear to be contradictory and they should be improved\nto be ultimately considered useful. Since 2012 deep convolutional neural\nnetworks (CNN) have been a tremendous success in image recognition, reaching\nhuman performance. These methods have greatly surpassed the traditional\napproaches, which are similar to currently used CAD solutions. Deep CNN-s have\nthe potential to revolutionize medical image analysis. We propose a CAD system\nbased on one of the most successful object detection frameworks, Faster R-CNN.\nThe system detects and classifies malignant or benign lesions on a mammogram\nwithout any human intervention. The proposed method sets the state of the art\nclassification performance on the public INbreast database, AUC = 0.95 . The\napproach described here has achieved the 2nd place in the Digital Mammography\nDREAM Challenge with AUC = 0.85 . When used as a detector, the system reaches\nhigh sensitivity with very few false positive marks per image on the INbreast\ndataset. Source code, the trained model and an OsiriX plugin are availaible\nonline at https://github.com/riblidezso/frcnn_cad .\n", "rewritten_text": "Over the past two decades, Computer-Aided Diagnostics (CAD) systems have been developed to assist radiologists in analyzing screening mammograms. The current CAD technologies have shown contradictory benefits and require further improvement to be deemed truly useful. Since 2012, deep convolutional neural networks (CNN) have achieved remarkable success in image recognition, matching human performance levels. These advanced methods have outperformed traditional approaches, including those used in existing CAD solutions, and hold the potential to revolutionize medical image analysis.\n\nWe propose a CAD system based on the highly effective object detection framework, Faster R-CNN. This system autonomously detects and classifies malignant or benign lesions on mammograms without the need for human intervention. Our proposed method demonstrates state-of-the-art classification performance on the publicly available INbreast database, achieving an AUC of 0.95. Furthermore, this approach secured the 2nd place in the Digital Mammography DREAM Challenge with an AUC of 0.85. When utilized as a detector, the system exhibits high sensitivity with minimal false positive identifications per image on the INbreast dataset.\n\nFor those interested, the source code, trained model, and an OsiriX plugin are accessible online at https://github.com/riblidezso/frcnn_cad."}, "1604.02975": {"original_text": "  We propose a novel Coupled Projection multi-task Metric Learning (CP-mtML)\nmethod for large scale face retrieval. In contrast to previous works which were\nlimited to low dimensional features and small datasets, the proposed method\nscales to large datasets with high dimensional face descriptors. It utilises\npairwise (dis-)similarity constraints as supervision and hence does not require\nexhaustive class annotation for every training image. While, traditionally,\nmulti-task learning methods have been validated on same dataset but different\ntasks, we work on the more challenging setting with heterogeneous datasets and\ndifferent tasks. We show empirical validation on multiple face image datasets\nof different facial traits, e.g. identity, age and expression. We use classic\nLocal Binary Pattern (LBP) descriptors along with the recent Deep Convolutional\nNeural Network (CNN) features. The experiments clearly demonstrate the\nscalability and improved performance of the proposed method on the tasks of\nidentity and age based face image retrieval compared to competitive existing\nmethods, on the standard datasets and with the presence of a million distractor\nface images.\n", "rewritten_text": "We introduce a novel method called Coupled Projection multi-task Metric Learning (CP-mtML) for large-scale face retrieval. Unlike previous approaches that were limited to low-dimensional features and small datasets, our method is designed to handle large datasets with high-dimensional face descriptors. It leverages pairwise (dis-)similarity constraints as supervision, eliminating the need for exhaustive class annotation for every training image. While traditional multi-task learning methods have typically been evaluated on the same dataset but different tasks, we tackle a more challenging scenario involving heterogeneous datasets and different tasks.\n\nOur method is empirically validated on multiple face image datasets showcasing various facial traits such as identity, age, and expression. We combine classic Local Binary Pattern (LBP) descriptors with state-of-the-art Deep Convolutional Neural Network (CNN) features. The experiments clearly illustrate the scalability and enhanced performance of our method in identity and age-based face image retrieval tasks compared to existing methods. These results are demonstrated on standard datasets, even in the presence of a million distractor face images."}, "1705.10872": {"original_text": "  We introduce a novel loss for learning local feature descriptors which is\ninspired by the Lowe's matching criterion for SIFT. We show that the proposed\nloss that maximizes the distance between the closest positive and closest\nnegative patch in the batch is better than complex regularization methods; it\nworks well for both shallow and deep convolution network architectures.\nApplying the novel loss to the L2Net CNN architecture results in a compact\ndescriptor -- it has the same dimensionality as SIFT (128) that shows\nstate-of-art performance in wide baseline stereo, patch verification and\ninstance retrieval benchmarks. It is fast, computing a descriptor takes about 1\nmillisecond on a low-end GPU.\n", "rewritten_text": "We present a new loss function designed for learning local feature descriptors, drawing inspiration from Lowe's matching criterion for SIFT. Our study demonstrates that the proposed loss, which focuses on maximizing the distance between the nearest positive and negative patches within a batch, outperforms complex regularization techniques. This approach proves effective across both shallow and deep convolutional network architectures.\n\nWhen applied to the L2Net CNN architecture, the novel loss yields a compact descriptor with a dimensionality matching that of SIFT (128). This descriptor showcases cutting-edge performance in various tasks such as wide baseline stereo, patch verification, and instance retrieval benchmarks. Notably, it boasts rapid computation speeds, with descriptor generation taking approximately 1 millisecond on a low-end GPU."}, "2103.17107": {"original_text": "  In this paper, the multi-task learning of lightweight convolutional neural\nnetworks is studied for face identification and classification of facial\nattributes (age, gender, ethnicity) trained on cropped faces without margins.\nThe necessity to fine-tune these networks to predict facial expressions is\nhighlighted. Several models are presented based on MobileNet, EfficientNet and\nRexNet architectures. It was experimentally demonstrated that they lead to near\nstate-of-the-art results in age, gender and race recognition on the UTKFace\ndataset and emotion classification on the AffectNet dataset. Moreover, it is\nshown that the usage of the trained models as feature extractors of facial\nregions in video frames leads to 4.5% higher accuracy than the previously known\nstate-of-the-art single models for the AFEW and the VGAF datasets from the\nEmotiW challenges. The models and source code are publicly available at\nhttps://github.com/HSE-asavchenko/face-emotion-recognition.\n", "rewritten_text": "This paper investigates the multi-task learning of lightweight convolutional neural networks for face identification and classification of facial attributes (age, gender, ethnicity). The networks are trained on cropped faces without margins, emphasizing the need to fine-tune them for predicting facial expressions. Various models are introduced based on MobileNet, EfficientNet, and RexNet architectures. Experimental results demonstrate that these models achieve near state-of-the-art performance in age, gender, and race recognition using the UTKFace dataset, as well as emotion classification on the AffectNet dataset. Additionally, utilizing the trained models as feature extractors for facial regions in video frames yields a 4.5% increase in accuracy compared to previously established state-of-the-art single models on the AFEW and VGAF datasets from the EmotiW challenges. The models and source code are publicly accessible at https://github.com/HSE-asavchenko/face-emotion-recognition."}, "2310.06474": {"original_text": "  While large language models (LLMs) exhibit remarkable capabilities across a\nwide range of tasks, they pose potential safety concerns, such as the\n``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to\nexhibit undesirable behavior. Although several preventive measures have been\ndeveloped to mitigate the potential risks associated with LLMs, they have\nprimarily focused on English. In this study, we reveal the presence of\nmultilingual jailbreak challenges within LLMs and consider two potential risky\nscenarios: unintentional and intentional. The unintentional scenario involves\nusers querying LLMs using non-English prompts and inadvertently bypassing the\nsafety mechanisms, while the intentional scenario concerns malicious users\ncombining malicious instructions with multilingual prompts to deliberately\nattack LLMs. The experimental results reveal that in the unintentional\nscenario, the rate of unsafe content increases as the availability of languages\ndecreases. Specifically, low-resource languages exhibit about three times the\nlikelihood of encountering harmful content compared to high-resource languages,\nwith both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts\ncan exacerbate the negative impact of malicious instructions, with\nastonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for\nGPT-4. To handle such a challenge in the multilingual context, we propose a\nnovel \\textsc{Self-Defense} framework that automatically generates multilingual\ntraining data for safety fine-tuning. Experimental results show that ChatGPT\nfine-tuned with such data can achieve a substantial reduction in unsafe content\ngeneration. Data is available at\n\\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.\n", "rewritten_text": "Large language models (LLMs) demonstrate remarkable capabilities across various tasks, yet they raise potential safety concerns, such as the \"jailbreak\" issue, where malicious instructions can manipulate LLMs to exhibit undesirable behavior. While several preventive measures have been developed to address the risks associated with LLMs, these efforts have predominantly focused on English. This study uncovers multilingual jailbreak challenges within LLMs and examines two potential risky scenarios: unintentional and intentional.\n\nThe unintentional scenario involves users querying LLMs with non-English prompts, inadvertently bypassing safety mechanisms. Conversely, the intentional scenario involves malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. Experimental results indicate that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Low-resource languages show approximately three times the likelihood of encountering harmful content compared to high-resource languages, observed in both ChatGPT and GPT-4.\n\nIn the intentional scenario, multilingual prompts can amplify the negative impact of malicious instructions, resulting in high rates of unsafe output: 80.92% for ChatGPT and 40.71% for GPT-4. To address this multilingual challenge, we propose a novel \\textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results demonstrate that ChatGPT fine-tuned with such data significantly reduces unsafe content generation. The data is accessible at \\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}."}, "2110.13793": {"original_text": "  With camera resolution ever increasing and the need to rapidly recalibrate\nrobotic platforms in less than ideal environments, there is a need for faster\nand more robust chessboard fiducial marker detectors. A new chessboard detector\nis proposed that is specifically designed for: high resolution images,\nfocus/motion blur, harsh lighting conditions, and background clutter. This is\naccomplished using a new x-corner detector, where for the first time blur is\nestimated and used in a novel way to enhance corner localization, edge\nvalidation, and connectivity. Performance is measured and compared against\nother libraries using a diverse set of images created by combining multiple\nthird party datasets and including new specially crafted scenarios designed to\nstress the state-of-the-art. The proposed detector has the best F1- Score of\n0.97, runs 1.9x faster than next fastest, and is a top performer for corner\naccuracy, while being the only detector to have consistent good performance in\nall scenarios.\n", "rewritten_text": "A new chessboard detector is proposed to address the increasing camera resolution and the need for rapid recalibration of robotic platforms in challenging environments. This detector is specifically designed to excel in high-resolution images, focus/motion blur, harsh lighting conditions, and background clutter. It introduces a novel x-corner detector that estimates blur for enhanced corner localization, edge validation, and connectivity. Performance evaluation involves comparing this new detector with other libraries using a diverse image set derived from multiple third-party datasets and newly created scenarios to stress test the current technology. The proposed detector achieves an impressive F1-Score of 0.97, operates 1.9 times faster than the next fastest option, and excels in corner accuracy across all scenarios, distinguishing itself as the top performer with consistent high performance."}, "2309.13492": {"original_text": "  Today's image style transfer methods have difficulty retaining humans face\nindividual features after the whole stylizing process. This occurs because the\nfeatures like face geometry and people's expressions are not captured by the\ngeneral-purpose image classifiers like the VGG-19 pre-trained models. This\npaper proposes the use of embeddings from an auxiliary pre-trained face\nrecognition model to encourage the algorithm to propagate human face features\nfrom the content image to the final stylized result.\n", "rewritten_text": "Current image style transfer methods struggle to preserve the distinct features of human faces throughout the stylization process. This challenge arises due to the inability of general-purpose image classifiers, such as the VGG-19 pre-trained models, to accurately capture elements like facial geometry and individual expressions. To address this issue, this paper suggests leveraging embeddings from an additional pre-trained face recognition model. By incorporating these embeddings, the algorithm is encouraged to better propagate human facial characteristics from the original content image to the ultimate stylized output."}, "2206.04242": {"original_text": "  Despite advances in image classification methods, detecting the samples not\nbelonging to the training classes is still a challenging problem. There has\nbeen a burst of interest in this subject recently, which is called Open-Set\nRecognition (OSR). In OSR, the goal is to achieve both the classification and\ndetecting out-of-distribution (OOD) samples. Several ideas have been proposed\nto push the empirical result further through complicated techniques. We believe\nthat such complication is indeed not necessary. To this end, we have shown that\nMaximum Softmax Probability (MSP), as the simplest baseline for OSR, applied on\nVision Transformers (ViTs) as the base classifier that is trained with non-OOD\naugmentations can surprisingly outperform many recent methods. Non-OOD\naugmentations are the ones that do not alter the data distribution by much. Our\nresults outperform state-of-the-art in CIFAR-10 datasets, and is also better\nthan most of the current methods in SVHN and MNIST. We show that training\naugmentation has a significant effect on the performance of ViTs in the OSR\ntasks, and while they should produce significant diversity in the augmented\nsamples, the generated sample OOD-ness must remain limited.\n", "rewritten_text": "Despite advancements in image classification methods, detecting samples that do not belong to the training classes remains a challenging issue. Recently, there has been a surge of interest in this area known as Open-Set Recognition (OSR). In OSR, the objective is to achieve both classification accuracy and the ability to identify out-of-distribution (OOD) samples. Various complex techniques have been proposed to enhance empirical results, but we argue that such complexity may not be necessary.\n\nOur research demonstrates that utilizing the Maximum Softmax Probability (MSP) as a straightforward baseline for OSR, applied to Vision Transformers (ViTs) as the base classifier trained with non-OOD augmentations, can surprisingly outperform many recent methods. Non-OOD augmentations refer to those that minimally alter the data distribution. Our findings show superior performance compared to state-of-the-art approaches on CIFAR-10 datasets, as well as outperforming most current methods on SVHN and MNIST datasets.\n\nWe highlight the significant impact of training augmentations on the performance of ViTs in OSR tasks. While these augmentations should introduce diversity in the augmented samples, it is crucial to ensure that the OOD-ness of the generated samples remains limited."}, "2010.01041": {"original_text": "  Planar homography estimation is foundational to many computer vision\nproblems, such as Simultaneous Localization and Mapping (SLAM) and Augmented\nReality (AR). However, conditions of high variance confound even the\nstate-of-the-art algorithms. In this report, we analyze the performance of two\nrecently published methods using Convolutional Neural Networks (CNNs) that are\nmeant to replace the more traditional feature-matching based approaches to the\nestimation of homography. Our evaluation of the CNN based methods focuses\nparticularly on measuring the performance under conditions of significant\nnoise, illumination shift, and occlusion. We also measure the benefits of\ntraining CNNs to varying degrees of noise. Additionally, we compare the effect\nof using color images instead of grayscale images for inputs to CNNs. Finally,\nwe compare the results against baseline feature-matching based homography\nestimation methods using SIFT, SURF, and ORB. We find that CNNs can be trained\nto be more robust against noise, but at a small cost to accuracy in the\nnoiseless case. Additionally, CNNs perform significantly better in conditions\nof extreme variance than their feature-matching based counterparts. With regard\nto color inputs, we conclude that with no change in the CNN architecture to\ntake advantage of the additional information in the color planes, the\ndifference in performance using color inputs or grayscale inputs is negligible.\nAbout the CNNs trained with noise-corrupted inputs, we show that training a CNN\nto a specific magnitude of noise leads to a \"Goldilocks Zone\" with regard to\nthe noise levels where that CNN performs best.\n", "rewritten_text": "Planar homography estimation is crucial for various computer vision applications, including Simultaneous Localization and Mapping (SLAM) and Augmented Reality (AR). Despite advancements in algorithms, challenges arise in scenarios with high variance. This report examines the effectiveness of two recently developed Convolutional Neural Network (CNN) methods designed to replace traditional feature-matching approaches in homography estimation. Our evaluation focuses on assessing these CNN-based methods' performance in the presence of significant noise, changes in illumination, and occlusion. We also investigate the impact of training CNNs with varying levels of noise and compare the use of color versus grayscale images as inputs to the CNNs. Furthermore, we compare the results with baseline homography estimation methods using SIFT, SURF, and ORB. Our findings reveal that CNNs can be trained to exhibit greater robustness against noise, albeit with a slight trade-off in accuracy under noiseless conditions. Additionally, CNNs outperform feature-matching methods in scenarios with high variance. Regarding color inputs, we determine that without modifying the CNN architecture to leverage the additional color information, the performance difference between color and grayscale inputs is minimal. In the case of CNNs trained with noise-corrupted inputs, we identify a \"Goldilocks Zone\" where a specific level of noise during training results in optimal performance for the CNN."}, "2308.10168": {"original_text": "  Since the recent prosperity of Large Language Models (LLMs), there have been\ninterleaved discussions regarding how to reduce hallucinations from LLM\nresponses, how to increase the factuality of LLMs, and whether Knowledge Graphs\n(KGs), which store the world knowledge in a symbolic form, will be replaced\nwith LLMs. In this paper, we try to answer these questions from a new angle:\nHow knowledgeable are LLMs?\n  To answer this question, we constructed Head-to-Tail, a benchmark that\nconsists of 18K question-answer (QA) pairs regarding head, torso, and tail\nfacts in terms of popularity. We designed an automated evaluation method and a\nset of metrics that closely approximate the knowledge an LLM confidently\ninternalizes. Through a comprehensive evaluation of 16 publicly available LLMs,\nwe show that existing LLMs are still far from being perfect in terms of their\ngrasp of factual knowledge, especially for facts of torso-to-tail entities.\n", "rewritten_text": "In light of the recent success of Large Language Models (LLMs), there have been ongoing discussions about reducing hallucinations in LLM responses, enhancing the factual accuracy of LLMs, and the potential replacement of Knowledge Graphs (KGs) \u2013 which store world knowledge symbolically \u2013 with LLMs. This paper aims to address these questions from a fresh perspective: How knowledgeable are LLMs?\n\nTo explore this inquiry, we developed Head-to-Tail, a benchmark comprising 18,000 question-answer (QA) pairs related to head, torso, and tail facts categorized by popularity. We devised an automated evaluation approach and a set of metrics that closely reflect the depth of knowledge that an LLM confidently possesses. Through a thorough assessment of 16 publicly accessible LLMs, we demonstrate that current LLMs still have significant room for improvement in terms of their understanding of factual information, particularly concerning torso-to-tail entities."}, "1912.06683": {"original_text": "  Semantic image segmentation plays a pivotal role in many vision applications\nincluding autonomous driving and medical image analysis. Most of the former\napproaches move towards enhancing the performance in terms of accuracy with a\nlittle awareness of computational efficiency. In this paper, we introduce\nLiteSeg, a lightweight architecture for semantic image segmentation. In this\nwork, we explore a new deeper version of Atrous Spatial Pyramid Pooling module\n(ASPP) and apply short and long residual connections, and depthwise separable\nconvolution, resulting in a faster and efficient model. LiteSeg architecture is\nintroduced and tested with multiple backbone networks as Darknet19, MobileNet,\nand ShuffleNet to provide multiple trade-offs between accuracy and\ncomputational cost. The proposed model LiteSeg, with MobileNetV2 as a backbone\nnetwork, achieves an accuracy of 67.81% mean intersection over union at 161\nframes per second with $640 \\times 360$ resolution on the Cityscapes dataset.\n", "rewritten_text": "Semantic image segmentation is crucial in various vision applications, such as autonomous driving and medical image analysis. While previous approaches have focused on improving accuracy, often neglecting computational efficiency, this paper introduces LiteSeg, a lightweight architecture designed for semantic image segmentation. The study presents a novel, deeper version of the Atrous Spatial Pyramid Pooling module (ASPP), incorporating short and long residual connections, as well as depthwise separable convolution to create a faster and more efficient model.\n\nLiteSeg's architecture is evaluated using multiple backbone networks, including Darknet19, MobileNet, and ShuffleNet, to offer a range of trade-offs between accuracy and computational cost. The proposed LiteSeg model, utilizing MobileNetV2 as the backbone network, achieves an accuracy of 67.81% mean intersection over union at 161 frames per second with a resolution of $640 \\times 360$ on the Cityscapes dataset."}, "2402.14281": {"original_text": "  Map representation learned by expert demonstrations has shown promising\nresearch value. However, recent advancements in the visual navigation field\nface challenges due to the lack of human datasets in the real world for\nefficient supervised representation learning of the environments. We present a\nLandmark-Aware Visual Navigation (LAVN) dataset to allow for supervised\nlearning of human-centric exploration policies and map building. We collect RGB\nobservation and human point-click pairs as a human annotator explores virtual\nand real-world environments with the goal of full coverage exploration of the\nspace. The human annotators also provide distinct landmark examples along each\ntrajectory, which we intuit will simplify the task of map or graph building and\nlocalization. These human point-clicks serve as direct supervision for waypoint\nprediction when learning to explore in environments. Our dataset covers a wide\nspectrum of scenes, including rooms in indoor environments, as well as walkways\noutdoors. Dataset is available at DOI: 10.5281/zenodo.10608067.\n", "rewritten_text": "Expert demonstrations have demonstrated the research value of map representation. However, recent advancements in the field of visual navigation are encountering challenges due to the scarcity of human datasets in real-world settings for efficient supervised representation learning of environments. To address this gap, we introduce the Landmark-Aware Visual Navigation (LAVN) dataset, designed to facilitate the supervised learning of human-centric exploration policies and map construction.\n\nIn the LAVN dataset, we capture RGB observations and human point-click pairs as a human annotator explores both virtual and real-world environments with the objective of achieving comprehensive spatial coverage. Along each trajectory, human annotators also provide distinct landmark examples, which are expected to streamline the processes of map or graph building and localization. These human point-click pairs serve as direct supervision for predicting waypoints during the learning process of exploring environments.\n\nOur dataset encompasses a diverse range of scenes, including indoor rooms and outdoor walkways. For access to the dataset, please refer to the DOI: 10.5281/zenodo.10608067."}, "2408.05184": {"original_text": "  This paper describes our solution of the first subtask from the AXOLOTL-24\nshared task on Semantic Change Modeling. The goal of this subtask is to\ndistribute a given set of usages of a polysemous word from a newer time period\nbetween senses of this word from an older time period and clusters representing\ngained senses of this word. We propose and experiment with three new methods\nsolving this task. Our methods achieve SOTA results according to both official\nmetrics of the first substask. Additionally, we develop a model that can tell\nif a given word usage is not described by any of the provided sense\ndefinitions. This model serves as a component in one of our methods, but can\npotentially be useful on its own.\n", "rewritten_text": "This paper presents our solution to the initial subtask of the AXOLOTL-24 shared task on Semantic Change Modeling. The objective of this subtask is to allocate a given set of usages of a polysemous word from a more recent time period among senses of the word from an earlier time period and clusters representing newly acquired senses of the word. We introduce and evaluate three novel methods for addressing this task. Our methods demonstrate state-of-the-art results based on the official metrics of the first subtask. Furthermore, we introduce a model capable of determining if a given word usage does not align with any of the provided sense definitions. While this model functions as a component within one of our methods, it also holds potential for standalone utility."}, "2312.00335": {"original_text": "  Self-supervised learning (SSL) approaches have recently shown substantial\nsuccess in learning visual representations from unannotated images. Compared\nwith photographic images, medical images acquired with the same imaging\nprotocol exhibit high consistency in anatomy. To exploit this anatomical\nconsistency, this paper introduces a novel SSL approach, called PEAC (patch\nembedding of anatomical consistency), for medical image analysis. Specifically,\nin this paper, we propose to learn global and local consistencies via stable\ngrid-based matching, transfer pre-trained PEAC models to diverse downstream\ntasks, and extensively demonstrate that (1) PEAC achieves significantly better\nperformance than the existing state-of-the-art fully/self-supervised methods,\nand (2) PEAC captures the anatomical structure consistency across views of the\nsame patient and across patients of different genders, weights, and healthy\nstatuses, which enhances the interpretability of our method for medical image\nanalysis.\n", "rewritten_text": "Recently, self-supervised learning (SSL) approaches have demonstrated significant success in learning visual representations from unannotated images. In comparison to photographic images, medical images obtained using the same imaging protocol display a high level of consistency in anatomy. To leverage this anatomical consistency, this paper introduces a novel SSL approach known as PEAC (patch embedding of anatomical consistency) for medical image analysis. The proposed method focuses on learning global and local consistencies through stable grid-based matching, transferring pre-trained PEAC models to various downstream tasks, and extensively showcasing that PEAC not only outperforms existing state-of-the-art fully/self-supervised methods but also captures anatomical structure consistency across views of the same patient and among patients of different genders, weights, and health statuses. This capability enhances the interpretability of our method for medical image analysis."}, "2408.15159": {"original_text": "  Translating written sentences from oral languages to a sequence of manual and\nnon-manual gestures plays a crucial role in building a more inclusive society\nfor deaf and hard-of-hearing people. Facial expressions (non-manual), in\nparticular, are responsible for encoding the grammar of the sentence to be\nspoken, applying punctuation, pronouns, or emphasizing signs. These non-manual\ngestures are closely related to the semantics of the sentence being spoken and\nalso to the utterance of the speaker's emotions. However, most Sign Language\nProduction (SLP) approaches are centered on synthesizing manual gestures and do\nnot focus on modeling the speakers expression. This paper introduces a new\nmethod focused in synthesizing facial expressions for sign language. Our goal\nis to improve sign language production by integrating sentiment information in\nfacial expression generation. The approach leverages a sentence sentiment and\nsemantic features to sample from a meaningful representation space, integrating\nthe bias of the non-manual components into the sign language production\nprocess. To evaluate our method, we extend the Frechet Gesture Distance (FGD)\nand propose a new metric called Frechet Expression Distance (FED) and apply an\nextensive set of metrics to assess the quality of specific regions of the face.\nThe experimental results showed that our method achieved state of the art,\nbeing superior to the competitors on How2Sign and PHOENIX14T datasets.\nMoreover, our architecture is based on a carefully designed graph pyramid that\nmakes it simpler, easier to train, and capable of leveraging emotions to\nproduce facial expressions.\n", "rewritten_text": "Translating written sentences from spoken languages into a sequence of manual and non-manual gestures is essential for creating a more inclusive society for individuals who are deaf or hard of hearing. Facial expressions, specifically non-manual gestures, play a crucial role in encoding the grammar of the sentence, including punctuation, pronouns, and emphasizing signs. These non-manual gestures are intricately linked to the semantics of the spoken sentence and the speaker's emotional delivery.\n\nDespite the significance of non-manual gestures, most Sign Language Production (SLP) methods focus primarily on synthesizing manual gestures and overlook the modeling of facial expressions. This paper introduces a novel approach that concentrates on synthesizing facial expressions for sign language, aiming to enhance sign language production by incorporating sentiment information into facial expression generation. The method utilizes sentence sentiment and semantic features to sample from a meaningful representation space, integrating non-manual components' bias into the sign language production process.\n\nTo assess the effectiveness of the proposed method, we extend the evaluation beyond the traditional Frechet Gesture Distance (FGD) metric and introduce a new metric called Frechet Expression Distance (FED). An array of metrics is applied to evaluate specific facial regions' quality. Experimental results demonstrate that our method outperforms competitors on datasets such as How2Sign and PHOENIX14T, achieving state-of-the-art performance. Furthermore, our architecture is based on a meticulously designed graph pyramid, making it more straightforward, easier to train, and capable of leveraging emotions to generate facial expressions."}, "2011.08894": {"original_text": "  Medical image segmentation is a relevant task as it serves as the first step\nfor several diagnosis processes, thus it is indispensable in clinical usage.\nWhilst major success has been reported using supervised techniques, they assume\na large and well-representative labelled set. This is a strong assumption in\nthe medical domain where annotations are expensive, time-consuming, and\ninherent to human bias. To address this problem, unsupervised techniques have\nbeen proposed in the literature yet it is still an open problem due to the\ndifficulty of learning any transformation pattern. In this work, we present a\nnovel optimisation model framed into a new CNN-based contrastive registration\narchitecture for unsupervised medical image segmentation. The core of our\napproach is to exploit image-level registration and feature-level from a\ncontrastive learning mechanism, to perform registration-based segmentation.\nFirstly, we propose an architecture to capture the image-to-image\ntransformation pattern via registration for unsupervised medical image\nsegmentation. Secondly, we embed a contrastive learning mechanism into the\nregistration architecture to enhance the discriminating capacity of the network\nin the feature-level. We show that our proposed technique mitigates the major\ndrawbacks of existing unsupervised techniques. We demonstrate, through\nnumerical and visual experiments, that our technique substantially outperforms\nthe current state-of-the-art unsupervised segmentation methods on two major\nmedical image datasets.\n", "rewritten_text": "Medical image segmentation is a crucial task that serves as the initial step in various diagnostic processes, making it essential for clinical applications. While supervised techniques have shown significant success, they rely on having a large and well-represented labeled dataset. This assumption is particularly challenging in the medical field, where annotations are costly, time-consuming, and subject to human bias.\n\nTo tackle this issue, unsupervised techniques have been proposed in the literature. However, learning transformation patterns remains a significant challenge. In this study, we introduce a novel optimization model within a new CNN-based contrastive registration framework for unsupervised medical image segmentation. Our approach leverages image-level registration and feature-level contrastive learning to enable registration-based segmentation.\n\nOur architecture aims to capture image-to-image transformation patterns through registration for unsupervised medical image segmentation. Additionally, we integrate a contrastive learning mechanism into the registration architecture to enhance the network's discriminative capabilities at the feature level. We demonstrate that our proposed technique addresses the key limitations of existing unsupervised methods.\n\nThrough numerical and visual experiments, we show that our technique significantly outperforms current state-of-the-art unsupervised segmentation methods on two major medical image datasets."}, "2301.02307": {"original_text": "  Narrated ''how-to'' videos have emerged as a promising data source for a wide\nrange of learning problems, from learning visual representations to training\nrobot policies. However, this data is extremely noisy, as the narrations do not\nalways describe the actions demonstrated in the video. To address this problem\nwe introduce the novel task of visual narration detection, which entails\ndetermining whether a narration is visually depicted by the actions in the\nvideo. We propose What You Say is What You Show (WYS^2), a method that\nleverages multi-modal cues and pseudo-labeling to learn to detect visual\nnarrations with only weakly labeled data. Our model successfully detects visual\nnarrations in in-the-wild videos, outperforming strong baselines, and we\ndemonstrate its impact for state-of-the-art summarization and temporal\nalignment of instructional videos.\n", "rewritten_text": "Narrated \"how-to\" videos have become a valuable data source for a variety of learning tasks, such as learning visual representations and training robot policies. However, this data is often noisy because the narrations may not accurately describe the actions shown in the video. To tackle this issue, we propose a new task called visual narration detection, which involves determining whether the narration aligns with the actions in the video. Our approach, named What You Say is What You Show (WYS^2), utilizes multi-modal cues and pseudo-labeling to learn how to detect visual narrations using weakly labeled data. Our model effectively identifies visual narrations in real-world videos, surpassing strong baseline methods. Furthermore, we showcase the impact of our model on enhancing state-of-the-art video summarization and temporal alignment in instructional videos."}, "2403.05170": {"original_text": "  This paper proposes a new pipeline for long-tail (LT) recognition. Instead of\nre-weighting or re-sampling, we utilize the long-tailed dataset itself to\ngenerate a balanced proxy that can be optimized through cross-entropy (CE).\nSpecifically, a randomly initialized diffusion model, trained exclusively on\nthe long-tailed dataset, is employed to synthesize new samples for\nunderrepresented classes. Then, we utilize the inherent information in the\noriginal dataset to filter out harmful samples and keep the useful ones. Our\nstrategy, Diffusion model for Long-Tail recognition (DiffuLT), represents a\npioneering utilization of generative models in long-tail recognition. DiffuLT\nachieves state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT,\nsurpassing the best competitors with non-trivial margins. Abundant ablations\nmake our pipeline interpretable, too. The whole generation pipeline is done\nwithout any external data or pre-trained model weights, making it highly\ngeneralizable to real-world long-tailed settings.\n", "rewritten_text": "This paper introduces a novel pipeline for long-tail (LT) recognition. Instead of re-weighting or re-sampling, we leverage the long-tailed dataset itself to create a balanced proxy that can be optimized using cross-entropy (CE). Specifically, we employ a randomly initialized diffusion model, trained exclusively on the long-tailed dataset, to generate new samples for underrepresented classes. Subsequently, we utilize the original dataset's inherent information to filter out detrimental samples while retaining the beneficial ones. Our approach, named Diffusion model for Long-Tail recognition (DiffuLT), marks a pioneering use of generative models in long-tail recognition. DiffuLT outperforms the top competitors by significant margins on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT, achieving state-of-the-art results. Furthermore, thorough ablations enhance the interpretability of our pipeline. Importantly, the entire generation process is conducted without relying on external data or pre-trained model weights, ensuring high generalizability to real-world long-tailed scenarios."}, "2404.14779": {"original_text": "  This study presents a comprehensive analysis and comparison of two\npredominant fine-tuning methodologies - full-parameter fine-tuning and\nparameter-efficient tuning - within the context of medical Large Language\nModels (LLMs). We developed and refined a series of LLMs, based on the Llama-2\narchitecture, specifically designed to enhance medical knowledge retrieval,\nreasoning, and question-answering capabilities. Our experiments systematically\nevaluate the effectiveness of these tuning strategies across various well-known\nmedical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of\n72% on the US Medical Licensing Examination (USMLE) datasets, setting a new\nstandard in performance for openly available medical LLMs. Through this\ncomparative analysis, we aim to identify the most effective and efficient\nmethod for fine-tuning LLMs in the medical domain, thereby contributing\nsignificantly to the advancement of AI-driven healthcare applications.\n", "rewritten_text": "This study provides a thorough analysis and comparison of two primary fine-tuning methodologies - full-parameter fine-tuning and parameter-efficient tuning - in the context of medical Large Language Models (LLMs). A series of LLMs, based on the Llama-2 architecture, were developed and refined to specifically enhance medical knowledge retrieval, reasoning, and question-answering capabilities. The experiments conducted systematically assess the effectiveness of these tuning strategies across various established medical benchmarks. Notably, our medical LLM, Med42, achieved an accuracy level of 72% on the US Medical Licensing Examination (USMLE) datasets, setting a new performance standard for publicly available medical LLMs. Through this comparative analysis, our goal is to determine the most effective and efficient method for fine-tuning LLMs in the medical field, thereby making a significant contribution to the advancement of AI-driven healthcare applications."}, "2411.09694": {"original_text": "  Reranking a list of candidates from a machine translation system with an\nexternal scoring model and returning the highest-scoring candidate remains a\nsimple and effective method for improving the overall output quality.\nTranslation scoring models continue to grow in size, with the best models being\ncomparable to generation models. Thus, reranking can add substantial\ncomputational cost to the translation pipeline. In this work, we pose reranking\nas a Bayesian optimization (BayesOpt) problem. By strategically selecting\ncandidates to score based on a balance of exploration and exploitation, we show\nthat it is possible to find top-scoring candidates when scoring only a fraction\nof the candidate list. For instance, our method achieves the same CometKiwi\nscore using only 70 scoring evaluations compared a baseline system using 180.\nWe present a multi-fidelity setting for BayesOpt, where the candidates are\nfirst scored with a cheaper but noisier proxy scoring model, which further\nimproves the cost-performance tradeoff when using smaller but well-trained\ndistilled proxy scorers.\n", "rewritten_text": "Reranking a list of candidates from a machine translation system using an external scoring model and selecting the highest-scoring candidate remains a simple and effective method for enhancing the overall output quality. Translation scoring models are increasing in size, with the top models comparable to generation models. Consequently, reranking can introduce significant computational costs to the translation pipeline. In this study, we frame reranking as a Bayesian optimization (BayesOpt) problem. By strategically choosing candidates to score based on a balance of exploration and exploitation, we demonstrate the ability to identify top-scoring candidates by evaluating only a fraction of the candidate list. For example, our approach achieves the same CometKiwi score with just 70 scoring evaluations compared to a baseline system requiring 180 evaluations. We introduce a multi-fidelity setting for BayesOpt, where candidates are initially scored using a less expensive but noisier proxy scoring model. This approach further enhances the cost-performance tradeoff, particularly when employing smaller yet well-trained distilled proxy scorers."}, "2407.07080": {"original_text": "  Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.\n", "rewritten_text": "Training large language models (LLMs) in low-resource languages, such as Hebrew, presents unique challenges. This paper introduces DictaLM2.0 and DictaLM2.0-Instruct, two LLMs derived from the Mistral model and trained on a substantial corpus of approximately 200 billion tokens in both Hebrew and English. Adapting a pre-trained model to a new language requires specialized techniques that differ significantly from training a model from scratch or further training existing models in well-resourced languages like English. The novel training methodologies outlined in this paper facilitate effective learning and adaptation to the linguistic properties of Hebrew. Additionally, DictaLM2.0-Instruct was fine-tuned on a comprehensive instruct dataset to enhance its performance on task-specific instructions. To rigorously evaluate the models, a new benchmark suite for Hebrew LLM evaluation is introduced, covering a diverse set of tasks including Question Answering, Sentiment Analysis, Winograd Schema Challenge, Translation, and Summarization. This work not only addresses the complexities of training LLMs in low-resource languages but also proposes a framework that can be utilized to adapt other LLMs to various non-English languages, contributing to the broader field of multilingual NLP."}, "2211.08544": {"original_text": "  Quantization-aware training (QAT) receives extensive popularity as it well\nretains the performance of quantized networks. In QAT, the contemporary\nexperience is that all quantized weights are updated for an entire training\nprocess. In this paper, this experience is challenged based on an interesting\nphenomenon we observed. Specifically, a large portion of quantized weights\nreaches the optimal quantization level after a few training epochs, which we\nrefer to as the partly scratch-off lottery ticket. This\nstraightforward-yet-valuable observation naturally inspires us to zero out\ngradient calculations of these weights in the remaining training period to\navoid meaningless updating. To effectively find the ticket, we develop a\nheuristic method, dubbed lottery ticket scratcher (LTS), which freezes a weight\nonce the distance between the full-precision one and its quantization level is\nsmaller than a controllable threshold. Surprisingly, the proposed LTS typically\neliminates 50%-70% weight updating and 25%-35% FLOPs of the backward pass,\nwhile still resulting on par with or even better performance than the compared\nbaseline. For example, compared with the baseline, LTS improves 2-bit\nMobileNetV2 by 5.05%, eliminating 46% weight updating and 23% FLOPs of the\nbackward pass. Code is at url{https://github.com/zysxmu/LTS}.\n", "rewritten_text": "Quantization-aware training (QAT) has gained significant popularity for effectively maintaining the performance of quantized networks. In QAT, a common practice involves updating all quantized weights throughout the entire training process. However, in this paper, we challenge this conventional approach based on an intriguing phenomenon we have observed. Specifically, we have found that a substantial portion of quantized weights reaches the optimal quantization level after just a few training epochs, which we have termed the \"partly scratch-off lottery ticket.\"\n\nThis simple yet valuable observation has inspired us to modify our approach by zeroing out gradient calculations for these weights during the remaining training period to prevent unnecessary updates. To efficiently identify these \"winning tickets,\" we have developed a heuristic method called the lottery ticket scratcher (LTS). The LTS freezes a weight once the difference between its full-precision value and its quantization level falls below a user-defined threshold.\n\nRemarkably, our proposed LTS method typically reduces weight updating by 50%-70% and decreases the number of floating-point operations (FLOPs) in the backward pass by 25%-35%, all while maintaining performance on par with or even surpassing the baseline. For instance, when applied to a 2-bit MobileNetV2 model, LTS improves performance by 5.05%, with a reduction of 46% in weight updating and 23% in FLOPs during the backward pass compared to the baseline. For those interested, the code for LTS can be found at: [https://github.com/zysxmu/LTS](https://github.com/zysxmu/LTS)."}, "2001.04642": {"original_text": "  We address the dual problems of novel view synthesis and environment\nreconstruction from hand-held RGBD sensors. Our contributions include 1)\nmodeling highly specular objects, 2) modeling inter-reflections and Fresnel\neffects, and 3) enabling surface light field reconstruction with the same input\nneeded to reconstruct shape alone. In cases where scene surface has a strong\nmirror-like material component, we generate highly detailed environment images,\nrevealing room composition, objects, people, buildings, and trees visible\nthrough windows. Our approach yields state of the art view synthesis\ntechniques, operates on low dynamic range imagery, and is robust to geometric\nand calibration errors.\n", "rewritten_text": "We tackle the challenges of novel view synthesis and environment reconstruction using hand-held RGBD sensors. Our contributions encompass: \n1) Modeling highly specular objects,\n2) Modeling inter-reflections and Fresnel effects, and \n3) Enabling surface light field reconstruction with the same input required for shape reconstruction alone. \nIn scenarios where the scene surface contains a significant mirror-like material component, we are able to generate detailed environment images that showcase room composition, objects, people, buildings, and trees visible through windows. Our method delivers cutting-edge view synthesis techniques, functions effectively with low dynamic range imagery, and remains resilient to geometric and calibration errors."}, "1601.00199": {"original_text": "  Active Appearance Models (AAMs) are one of the most popular and\nwell-established techniques for modeling deformable objects in computer vision.\nIn this paper, we study the problem of fitting AAMs using Compositional\nGradient Descent (CGD) algorithms. We present a unified and complete view of\nthese algorithms and classify them with respect to three main characteristics:\ni) cost function; ii) type of composition; and iii) optimization method.\nFurthermore, we extend the previous view by: a) proposing a novel Bayesian cost\nfunction that can be interpreted as a general probabilistic formulation of the\nwell-known project-out loss; b) introducing two new types of composition,\nasymmetric and bidirectional, that combine the gradients of both image and\nappearance model to derive better conver- gent and more robust CGD algorithms;\nand c) providing new valuable insights into existent CGD algorithms by\nreinterpreting them as direct applications of the Schur complement and the\nWiberg method. Finally, in order to encourage open research and facilitate\nfuture comparisons with our work, we make the implementa- tion of the\nalgorithms studied in this paper publicly available as part of the Menpo\nProject.\n", "rewritten_text": "Active Appearance Models (AAMs) are widely recognized as a leading technique for modeling deformable objects in computer vision. This paper delves into the fitting of AAMs using Compositional Gradient Descent (CGD) algorithms. We offer a comprehensive analysis of these algorithms, categorizing them based on three key characteristics: i) cost function; ii) type of composition; and iii) optimization method.\n\nMoreover, we enhance the existing perspective by: a) introducing a novel Bayesian cost function that serves as a general probabilistic representation of the renowned project-out loss; b) unveiling two new types of composition, asymmetric and bidirectional, which leverage both image and appearance model gradients to yield more effective and resilient CGD algorithms; and c) shedding new light on current CGD algorithms by reinterpreting them as direct applications of the Schur complement and the Wiberg method.\n\nTo foster open research and facilitate future comparisons with our findings, we have made the implementation of the algorithms discussed in this paper publicly accessible as part of the Menpo Project."}, "1709.01695": {"original_text": "  3D action recognition was shown to benefit from a covariance representation\nof the input data (joint 3D positions). A kernel machine feed with such feature\nis an effective paradigm for 3D action recognition, yielding state-of-the-art\nresults. Yet, the whole framework is affected by the well-known scalability\nissue. In fact, in general, the kernel function has to be evaluated for all\npairs of instances inducing a Gram matrix whose complexity is quadratic in the\nnumber of samples. In this work we reduce such complexity to be linear by\nproposing a novel and explicit feature map to approximate the kernel function.\nThis allows to train a linear classifier with an explicit feature encoding,\nwhich implicitly implements a Log-Euclidean machine in a scalable fashion. Not\nonly we prove that the proposed approximation is unbiased, but also we work out\nan explicit strong bound for its variance, attesting a theoretical superiority\nof our approach with respect to existing ones. Experimentally, we verify that\nour representation provides a compact encoding and outperforms other\napproximation schemes on a number of publicly available benchmark datasets for\n3D action recognition.\n", "rewritten_text": "The use of a covariance representation of joint 3D positions has been shown to enhance 3D action recognition. Employing a kernel machine fed with such features proves to be an effective approach, leading to cutting-edge results. However, the scalability of the entire framework is a concern due to the need to evaluate the kernel function for all pairs of instances, resulting in a quadratic complexity in the number of samples. In this study, we address this issue by introducing a novel and explicit feature map to approximate the kernel function, reducing the complexity to a linear level. This enables the training of a linear classifier with an explicit feature encoding, effectively implementing a Log-Euclidean machine in a scalable manner. We demonstrate that our proposed approximation is unbiased and provide a strong bound for its variance, establishing the theoretical superiority of our approach over existing methods. Through experiments, we confirm that our representation offers a concise encoding and surpasses other approximation techniques on various publicly available benchmark datasets for 3D action recognition."}, "2212.0583": {"original_text": "  Directly training a document-to-document (Doc2Doc) neural machine translation\n(NMT) via Transformer from scratch, especially on small datasets usually fails\nto converge. Our dedicated probing tasks show that 1) both the absolute\nposition and relative position information gets gradually weakened or even\nvanished once it reaches the upper encoder layers, and 2) the vanishing of\nabsolute position information in encoder output causes the training failure of\nDoc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer\n(P-Transformer) to enhance both the absolute and relative position information\nin both self-attention and cross-attention. Specifically, we integrate absolute\npositional information, i.e., position embeddings, into the query-key pairs\nboth in self-attention and cross-attention through a simple yet effective\naddition operation. Moreover, we also integrate relative position encoding in\nself-attention. The proposed P-Transformer utilizes sinusoidal position\nencoding and does not require any task-specified position embedding, segment\nembedding, or attention mechanism. Through the above methods, we build a\nDoc2Doc NMT model with P-Transformer, which ingests the source document and\ncompletely generates the target document in a sequence-to-sequence (seq2seq)\nway. In addition, P-Transformer can be applied to seq2seq-based\ndocument-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)\ntranslation. Extensive experimental results of Doc2Doc NMT show that\nP-Transformer significantly outperforms strong baselines on widely-used 9\ndocument-level datasets in 7 language pairs, covering small-, middle-, and\nlarge-scales, and achieves a new state-of-the-art. Experimentation on discourse\nphenomena shows that our Doc2Doc NMT models improve the translation quality in\nboth BLEU and discourse coherence. We make our code available on Github.\n", "rewritten_text": "Training a document-to-document (Doc2Doc) neural machine translation (NMT) model directly from scratch using a Transformer often fails to converge, especially when working with small datasets. Our dedicated probing tasks have revealed two key findings: 1) the absolute and relative position information tends to weaken or disappear as it progresses through the upper encoder layers, and 2) the loss of absolute position information in the encoder output leads to training failures in Doc2Doc NMT models. To address this issue, we introduce a position-aware Transformer (P-Transformer) designed to enhance both absolute and relative position information in self-attention and cross-attention mechanisms. Specifically, we incorporate absolute positional information, such as position embeddings, into the query-key pairs in both self-attention and cross-attention using a simple yet effective addition operation. Additionally, we integrate relative position encoding into self-attention. The proposed P-Transformer utilizes sinusoidal position encoding and eliminates the need for task-specific position embeddings, segment embeddings, or attention mechanisms.\n\nBy implementing these enhancements, we develop a Doc2Doc NMT model with P-Transformer that takes in a source document and generates the target document in a sequence-to-sequence (seq2seq) manner. Furthermore, the P-Transformer can be applied to document-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent) translation tasks. Extensive experimental results on Doc2Doc NMT demonstrate that the P-Transformer outperforms strong baselines across nine widely-used document-level datasets in seven language pairs, spanning small, medium, and large scales, achieving a new state-of-the-art performance. Evaluation on discourse phenomena indicates that our Doc2Doc NMT models enhance translation quality in terms of both BLEU scores and discourse coherence. Our code is publicly available on Github."}, "2308.05430": {"original_text": "  In this work, we propose an ensemble modeling approach for multimodal action\nrecognition. We independently train individual modality models using a variant\nof focal loss tailored to handle the long-tailed distribution of the MECCANO\n[21] dataset. Based on the underlying principle of focal loss, which captures\nthe relationship between tail (scarce) classes and their prediction\ndifficulties, we propose an exponentially decaying variant of focal loss for\nour current task. It initially emphasizes learning from the hard misclassified\nexamples and gradually adapts to the entire range of examples in the dataset.\nThis annealing process encourages the model to strike a balance between\nfocusing on the sparse set of hard samples, while still leveraging the\ninformation provided by the easier ones. Additionally, we opt for the late\nfusion strategy to combine the resultant probability distributions from RGB and\nDepth modalities for final action prediction. Experimental evaluations on the\nMECCANO dataset demonstrate the effectiveness of our approach.\n", "rewritten_text": "In this study, we introduce an ensemble modeling approach for multimodal action recognition. We train individual modality models independently using a modified version of focal loss designed to address the long-tailed distribution of the MECCANO [21] dataset. Drawing on the core concept of focal loss, which captures the relationship between rare classes and their prediction challenges, we propose an exponentially decaying variant of focal loss tailored to our specific task. This variant initially prioritizes learning from the challenging misclassified examples and gradually adjusts to encompass the full spectrum of examples in the dataset. This gradual adjustment process encourages the model to strike a balance between focusing on the scarce difficult samples and leveraging the information from the easier ones. Furthermore, we employ a late fusion strategy to merge the resulting probability distributions from the RGB and Depth modalities for the final action prediction. Experimental assessments conducted on the MECCANO dataset illustrate the efficacy of our approach."}, "1810.1161": {"original_text": "  Despite remarkable advances in image synthesis research, existing works often\nfail in manipulating images under the context of large geometric\ntransformations. Synthesizing person images conditioned on arbitrary poses is\none of the most representative examples where the generation quality largely\nrelies on the capability of identifying and modeling arbitrary transformations\non different body parts. Current generative models are often built on local\nconvolutions and overlook the key challenges (e.g. heavy occlusions, different\nviews or dramatic appearance changes) when distinct geometric changes happen\nfor each part, caused by arbitrary pose manipulations. This paper aims to\nresolve these challenges induced by geometric variability and spatial\ndisplacements via a new Soft-Gated Warping Generative Adversarial Network\n(Warping-GAN), which is composed of two stages: 1) it first synthesizes a\ntarget part segmentation map given a target pose, which depicts the\nregion-level spatial layouts for guiding image synthesis with higher-level\nstructure constraints; 2) the Warping-GAN equipped with a soft-gated\nwarping-block learns feature-level mapping to render textures from the original\nimage into the generated segmentation map. Warping-GAN is capable of\ncontrolling different transformation degrees given distinct target poses.\nMoreover, the proposed warping-block is light-weight and flexible enough to be\ninjected into any networks. Human perceptual studies and quantitative\nevaluations demonstrate the superiority of our Warping-GAN that significantly\noutperforms all existing methods on two large datasets.\n", "rewritten_text": "Despite significant advancements in image synthesis research, current works often struggle to manipulate images effectively when faced with large geometric transformations. One notable example is the synthesis of person images based on arbitrary poses, where the quality of generation heavily depends on the ability to recognize and model diverse transformations across various body parts. Many existing generative models rely on local convolutions and overlook the significant challenges posed by complex geometric changes resulting from arbitrary pose manipulations, such as heavy occlusions, different viewpoints, or drastic appearance alterations for each body part.\n\nThis paper aims to address these challenges arising from geometric variability and spatial displacements through the introduction of a novel Soft-Gated Warping Generative Adversarial Network (Warping-GAN). The Warping-GAN consists of two stages: firstly, it generates a target part segmentation map based on a specified pose, illustrating region-level spatial layouts to guide image synthesis with higher-level structural constraints; secondly, the Warping-GAN, equipped with a soft-gated warping-block, learns feature-level mapping to transfer textures from the original image onto the generated segmentation map. This approach allows the Warping-GAN to adjust transformation degrees according to different target poses. Additionally, the proposed warping-block is lightweight and adaptable for integration into various networks.\n\nHuman perceptual studies and quantitative evaluations confirm the superiority of our Warping-GAN, showcasing significant performance improvements over existing methods on two extensive datasets."}, "2410.10407": {"original_text": "  The widespread dissemination of false information through manipulative\ntactics that combine deceptive text and images threatens the integrity of\nreliable sources of information. While there has been research on detecting\nfake news in high resource languages using multimodal approaches, methods for\nlow resource Indic languages primarily rely on textual analysis. This\ndifference highlights the need for robust methods that specifically address\nmultimodal fake news in Indic languages, where the lack of extensive datasets\nand tools presents a significant obstacle to progress. To this end, we\nintroduce the Multimodal Multilingual dataset for Indic Fake News Detection\n(MMIFND). This meticulously curated dataset consists of 28,085 instances\ndistributed across Hindi, Bengali, Marathi, Malayalam, Tamil, Gujarati and\nPunjabi. We further propose the Multimodal Multilingual Caption-aware framework\nfor Fake News Detection (MMCFND). MMCFND utilizes pre-trained unimodal encoders\nand pairwise encoders from a foundational model that aligns vision and\nlanguage, allowing for extracting deep representations from visual and textual\ncomponents of news articles. The multimodal fusion encoder in the foundational\nmodel integrates text and image representations derived from its pairwise\nencoders to generate a comprehensive cross modal representation. Furthermore,\nwe generate descriptive image captions that provide additional context to\ndetect inconsistencies and manipulations. The retrieved features are then fused\nand fed into a classifier to determine the authenticity of news articles. The\ncurated dataset can potentially accelerate research and development in low\nresource environments significantly. Thorough experimentation on MMIFND\ndemonstrates that our proposed framework outperforms established methods for\nextracting relevant fake news detection features.\n", "rewritten_text": "The integrity of reliable sources of information is threatened by the widespread dissemination of false information using manipulative tactics that combine deceptive text and images. While research has focused on detecting fake news in high-resource languages through multimodal approaches, methods for low-resource Indic languages primarily rely on textual analysis. This disparity underscores the necessity for robust methods that specifically target multimodal fake news in Indic languages, where the lack of extensive datasets and tools poses a significant obstacle to progress. To address this, we introduce the Multimodal Multilingual dataset for Indic Fake News Detection (MMIFND), comprising 28,085 instances distributed across Hindi, Bengali, Marathi, Malayalam, Tamil, Gujarati, and Punjabi.\n\nAdditionally, we propose the Multimodal Multilingual Caption-aware framework for Fake News Detection (MMCFND). MMCFND leverages pre-trained unimodal encoders and pairwise encoders from a foundational model that aligns vision and language, enabling the extraction of deep representations from the visual and textual components of news articles. The multimodal fusion encoder in the foundational model integrates text and image representations from its pairwise encoders to create a comprehensive cross-modal representation. Moreover, we generate descriptive image captions to provide additional context for detecting inconsistencies and manipulations. The extracted features are then fused and input into a classifier to determine the authenticity of news articles.\n\nThe curated dataset has the potential to significantly accelerate research and development in low-resource environments. Extensive experimentation on MMIFND demonstrates that our proposed framework surpasses established methods in extracting relevant features for fake news detection."}, "2401.17207": {"original_text": "  A comprehensive understanding of the organizational principles in the human\nbrain requires, among other factors, well-quantifiable descriptors of nerve\nfiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a\nmicroscopic imaging technique that enables insights into the fine-grained\norganization of myelinated nerve fibers with high resolution. Descriptors\ncharacterizing the fiber architecture observed in 3D-PLI would enable\ndownstream analysis tasks such as multimodal correlation studies, clustering,\nand mapping. However, best practices for observer-independent characterization\nof fiber architecture in 3D-PLI are not yet available. To this end, we propose\nthe application of a fully data-driven approach to characterize nerve fiber\narchitecture in 3D-PLI images using self-supervised representation learning. We\nintroduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the\nspatial neighborhood of texture examples across histological brain sections of\na 3D reconstructed volume to sample positive pairs for contrastive learning. We\ncombine this sampling strategy with specifically designed image augmentations\nto gain robustness to typical variations in 3D-PLI parameter maps. The approach\nis demonstrated for the 3D reconstructed occipital lobe of a vervet monkey\nbrain. We show that extracted features are highly sensitive to different\nconfigurations of nerve fibers, yet robust to variations between consecutive\nbrain sections arising from histological processing. We demonstrate their\npractical applicability for retrieving clusters of homogeneous fiber\narchitecture and performing data mining for interactively selected templates of\nspecific components of fiber architecture such as U-fibers.\n", "rewritten_text": "To gain a comprehensive understanding of the organizational principles within the human brain, it is essential to have well-quantifiable descriptors of nerve fiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a microscopic technique that provides detailed insights into the intricate organization of myelinated nerve fibers with high resolution. Descriptors that characterize the fiber architecture observed in 3D-PLI can facilitate various downstream analysis tasks, including multimodal correlation studies, clustering, and mapping. However, there is currently a lack of established best practices for the observer-independent characterization of fiber architecture in 3D-PLI.\n\nIn this study, we propose the utilization of a fully data-driven approach to characterize nerve fiber architecture in 3D-PLI images through self-supervised representation learning. We introduce a 3D-Context Contrastive Learning (CL-3D) objective that leverages the spatial neighborhood of texture examples across histological brain sections of a 3D reconstructed volume to sample positive pairs for contrastive learning. This sampling strategy is combined with specifically designed image augmentations to enhance robustness to typical variations in 3D-PLI parameter maps.\n\nThe effectiveness of this approach is demonstrated using the 3D reconstructed occipital lobe of a vervet monkey brain. Our results show that the extracted features are highly sensitive to different configurations of nerve fibers while remaining robust to variations between consecutive brain sections resulting from histological processing. These features prove to be practically applicable for identifying clusters of homogeneous fiber architecture and conducting data mining to identify specific components of fiber architecture, such as U-fibers."}, "1703.04454": {"original_text": "  We address the problem of estimating human pose and body shape from 3D scans\nover time. Reliable estimation of 3D body shape is necessary for many\napplications including virtual try-on, health monitoring, and avatar creation\nfor virtual reality. Scanning bodies in minimal clothing, however, presents a\npractical barrier to these applications. We address this problem by estimating\nbody shape under clothing from a sequence of 3D scans. Previous methods that\nhave exploited body models produce smooth shapes lacking personalized details.\nWe contribute a new approach to recover a personalized shape of the person. The\nestimated shape deviates from a parametric model to fit the 3D scans. We\ndemonstrate the method using high quality 4D data as well as sequences of\nvisual hulls extracted from multi-view images. We also make available BUFF, a\nnew 4D dataset that enables quantitative evaluation\n(http://buff.is.tue.mpg.de). Our method outperforms the state of the art in\nboth pose estimation and shape estimation, qualitatively and quantitatively.\n", "rewritten_text": "We tackle the challenge of estimating human pose and body shape from 3D scans over time. Accurate estimation of 3D body shape is crucial for various applications such as virtual try-on, health monitoring, and avatar creation for virtual reality. However, scanning bodies in minimal clothing poses a practical obstacle to these applications. To address this issue, we propose estimating body shape under clothing from a sequence of 3D scans. Existing methods that leverage body models tend to generate smooth shapes lacking personalized details. In contrast, we introduce a novel approach to reconstruct a personalized shape of the individual. Our method adjusts the estimated shape to align with the 3D scans, deviating from a parametric model. We showcase the effectiveness of our technique using high-quality 4D data and sequences of visual hulls derived from multi-view images. Additionally, we introduce BUFF, a new 4D dataset that facilitates quantitative evaluation (http://buff.is.tue.mpg.de). Our method surpasses the current state of the art in both pose and shape estimation, demonstrating superior performance both qualitatively and quantitatively."}, "2002.01359": {"original_text": "  This paper gives an overview of the Schema-Guided Dialogue State Tracking\ntask of the 8th Dialogue System Technology Challenge. The goal of this task is\nto develop dialogue state tracking models suitable for large-scale virtual\nassistants, with a focus on data-efficient joint modeling across domains and\nzero-shot generalization to new APIs. This task provided a new dataset\nconsisting of over 16000 dialogues in the training set spanning 16 domains to\nhighlight these challenges, and a baseline model capable of zero-shot\ngeneralization to new APIs. Twenty-five teams participated, developing a range\nof neural network models, exceeding the performance of the baseline model by a\nvery high margin. The submissions incorporated a variety of pre-trained\nencoders and data augmentation techniques. This paper describes the task\ndefinition, dataset and evaluation methodology. We also summarize the approach\nand results of the submitted systems to highlight the overall trends in the\nstate-of-the-art.\n", "rewritten_text": "This paper provides an overview of the Schema-Guided Dialogue State Tracking task from the 8th Dialogue System Technology Challenge. The objective of this task is to create dialogue state tracking models suitable for large-scale virtual assistants, emphasizing data-efficient joint modeling across domains and zero-shot generalization to new APIs. To illustrate these challenges, a new dataset comprising over 16,000 dialogues across 16 domains was introduced, along with a baseline model capable of zero-shot generalization to new APIs. Twenty-five teams took part in the challenge, employing various neural network models that outperformed the baseline model significantly. The submissions featured a range of pre-trained encoders and data augmentation techniques. The paper outlines the task definition, dataset, and evaluation methodology, as well as summarizes the approaches and results of the submitted systems to showcase the prevailing trends in the state-of-the-art."}, "2403.10574": {"original_text": "  The rich spatio-temporal information is crucial to capture the complicated\ntarget appearance variations in visual tracking. However, most top-performing\ntracking algorithms rely on many hand-crafted components for spatio-temporal\ninformation aggregation. Consequently, the spatio-temporal information is far\naway from being fully explored. To alleviate this issue, we propose an adaptive\ntracker with spatio-temporal transformers (named AQATrack), which adopts simple\nautoregressive queries to effectively learn spatio-temporal information without\nmany hand-designed components. Firstly, we introduce a set of learnable and\nautoregressive queries to capture the instantaneous target appearance changes\nin a sliding window fashion. Then, we design a novel attention mechanism for\nthe interaction of existing queries to generate a new query in current frame.\nFinally, based on the initial target template and learnt autoregressive\nqueries, a spatio-temporal information fusion module (STM) is designed for\nspatiotemporal formation aggregation to locate a target object. Benefiting from\nthe STM, we can effectively combine the static appearance and instantaneous\nchanges to guide robust tracking. Extensive experiments show that our method\nsignificantly improves the tracker's performance on six popular tracking\nbenchmarks: LaSOT, LaSOText, TrackingNet, GOT-10k, TNL2K, and UAV123.\n", "rewritten_text": "The rich spatio-temporal information is essential for capturing the complex variations in target appearance during visual tracking. However, many of the leading tracking algorithms heavily rely on numerous manually crafted components for aggregating spatio-temporal information. As a result, there is still much untapped potential in exploring spatio-temporal information fully. To address this challenge, we propose an adaptive tracker called AQATrack, which incorporates spatio-temporal transformers. This approach utilizes simple autoregressive queries to effectively learn spatio-temporal information without the need for extensive hand-designed components.\n\nOur method begins by introducing a set of learnable and autoregressive queries to capture instantaneous changes in target appearance within a sliding window framework. Subsequently, we develop a novel attention mechanism that facilitates the interaction of existing queries to generate a new query in the current frame. Finally, leveraging the initial target template and the learned autoregressive queries, we design a spatio-temporal information fusion module (STM) for aggregating spatio-temporal information to accurately locate a target object. Through the STM, we can seamlessly integrate static appearance features with instantaneous changes to enhance tracking robustness.\n\nExtensive experiments demonstrate that our approach significantly enhances the performance of the tracker across six widely-used tracking benchmarks: LaSOT, LaSOText, TrackingNet, GOT-10k, TNL2K, and UAV123."}, "2206.1503": {"original_text": "  Question Answering (QA) is one of the most important natural language\nprocessing (NLP) tasks. It aims using NLP technologies to generate a\ncorresponding answer to a given question based on the massive unstructured\ncorpus. With the development of deep learning, more and more challenging QA\ndatasets are being proposed, and lots of new methods for solving them are also\nemerging. In this paper, we investigate influential QA datasets that have been\nreleased in the era of deep learning. Specifically, we begin with introducing\ntwo of the most common QA tasks - textual question answer and visual question\nanswering - separately, covering the most representative datasets, and then\ngive some current challenges of QA research.\n", "rewritten_text": "Question Answering (QA) stands as a crucial task in natural language processing (NLP). Its objective is to utilize NLP technologies to produce an appropriate answer to a given question from a vast unstructured corpus. The advancement of deep learning has led to the introduction of increasingly complex QA datasets and the emergence of numerous new methods for addressing them. This study delves into the significant QA datasets that have been unveiled during the deep learning era. Specifically, we commence by presenting two prevalent QA tasks - textual question answering and visual question answering - individually, encompassing the most notable datasets. Subsequently, we outline some of the current challenges encountered in QA research."}, "1604.04333": {"original_text": "  Deep Convolutional Neural Networks (CNN) have exhibited superior performance\nin many visual recognition tasks including image classification, object\ndetection, and scene label- ing, due to their large learning capacity and\nresistance to overfit. For the image classification task, most of the current\ndeep CNN- based approaches take the whole size-normalized image as input and\nhave achieved quite promising results. Compared with the previously dominating\napproaches based on feature extraction, pooling, and classification, the deep\nCNN-based approaches mainly rely on the learning capability of deep CNN to\nachieve superior results: the burden of minimizing intra-class variation while\nmaximizing inter-class difference is entirely dependent on the implicit feature\nlearning component of deep CNN; we rely upon the implicitly learned filters and\npooling component to select the discriminative regions, which correspond to the\nactivated neurons. However, if the irrelevant regions constitute a large\nportion of the image of interest, the classification performance of the deep\nCNN, which takes the whole image as input, can be heavily affected. To solve\nthis issue, we propose a novel latent CNN framework, which treats the most\ndiscriminate region as a latent variable. We can jointly learn the global CNN\nwith the latent CNN to avoid the aforementioned big irrelevant region issue,\nand our experimental results show the evident advantage of the proposed latent\nCNN over traditional deep CNN: latent CNN outperforms the state-of-the-art\nperformance of deep CNN on standard benchmark datasets including the CIFAR-10,\nCIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset.\n", "rewritten_text": "Deep Convolutional Neural Networks (CNN) have demonstrated superior performance in various visual recognition tasks such as image classification, object detection, and scene labeling. This is attributed to their large learning capacity and resistance to overfitting. In the context of image classification, most current deep CNN-based approaches utilize the entire size-normalized image as input and have achieved promising results. In contrast to previous methods that relied on feature extraction, pooling, and classification, deep CNN-based approaches leverage the learning capabilities of deep CNN to achieve superior outcomes. The responsibility of minimizing intra-class variation and maximizing inter-class differences is primarily placed on the implicit feature learning component of deep CNN. By utilizing the learned filters and pooling components, discriminative regions corresponding to activated neurons are selected.\n\nHowever, when irrelevant regions make up a significant portion of the image, the classification performance of deep CNN, which takes the entire image as input, can be significantly impacted. To address this challenge, we introduce a novel latent CNN framework that treats the most discriminative region as a latent variable. By jointly training the global CNN with the latent CNN, we mitigate the issue of irrelevant regions and our experimental results demonstrate the clear advantage of the proposed latent CNN over traditional deep CNN. The latent CNN surpasses the state-of-the-art performance of deep CNN on standard benchmark datasets such as CIFAR-10, CIFAR-100, MNIST, and PASCAL VOC 2007 Classification dataset."}, "2301.0734": {"original_text": "  Semi-Supervised Semantic Segmentation aims at training the segmentation model\nwith limited labeled data and a large amount of unlabeled data. To effectively\nleverage the unlabeled data, pseudo labeling, along with the teacher-student\nframework, is widely adopted in semi-supervised semantic segmentation. Though\nproved to be effective, this paradigm suffers from incorrect pseudo labels\nwhich inevitably exist and are taken as auxiliary training data. To alleviate\nthe negative impact of incorrect pseudo labels, we delve into the current\nSemi-Supervised Semantic Segmentation frameworks. We argue that the unlabeled\ndata with pseudo labels can facilitate the learning of representative features\nin the feature extractor, but it is unreliable to supervise the mask predictor.\nMotivated by this consideration, we propose a novel framework, Gentle Teaching\nAssistant (GTA-Seg) to disentangle the effects of pseudo labels on feature\nextractor and mask predictor of the student model. Specifically, in addition to\nthe original teacher-student framework, our method introduces a teaching\nassistant network which directly learns from pseudo labels generated by the\nteacher network. The gentle teaching assistant (GTA) is coined gentle since it\nonly transfers the beneficial feature representation knowledge in the feature\nextractor to the student model in an Exponential Moving Average (EMA) manner,\nprotecting the student model from the negative influences caused by unreliable\npseudo labels in the mask predictor. The student model is also supervised by\nreliable labeled data to train an accurate mask predictor, further facilitating\nfeature representation. Extensive experiment results on benchmark datasets\nvalidate that our method shows competitive performance against previous\nmethods. Code is available at https://github.com/Jin-Ying/GTA-Seg.\n", "rewritten_text": "Semi-Supervised Semantic Segmentation aims to train the segmentation model using a limited amount of labeled data and a large quantity of unlabeled data. To effectively utilize the unlabeled data, the approach of pseudo labeling, in conjunction with the teacher-student framework, is widely embraced in semi-supervised semantic segmentation. While proven to be effective, this approach is hindered by the presence of incorrect pseudo labels, which are inevitably incorporated as supplementary training data. In order to mitigate the adverse effects of these incorrect pseudo labels, we delve into the existing Semi-Supervised Semantic Segmentation frameworks.\n\nWe posit that the unlabeled data with pseudo labels can enhance the learning of representative features within the feature extractor, but may not be reliable for supervising the mask predictor. Motivated by this observation, we introduce a novel framework called Gentle Teaching Assistant (GTA-Seg) to separate the impacts of pseudo labels on the feature extractor and mask predictor of the student model. In our proposed method, alongside the original teacher-student framework, we introduce a teaching assistant network that directly learns from pseudo labels generated by the teacher network.\n\nThe gentle teaching assistant (GTA) is termed 'gentle' as it selectively transfers beneficial feature representation knowledge from the feature extractor to the student model in an Exponential Moving Average (EMA) manner, shielding the student model from the negative influences stemming from unreliable pseudo labels in the mask predictor. Additionally, the student model is supervised by reliable labeled data to train an accurate mask predictor, thereby further enhancing feature representation.\n\nExtensive experimental results on benchmark datasets validate that our method demonstrates competitive performance compared to previous approaches. The code for our method is available at https://github.com/Jin-Ying/GTA-Seg."}, "2108.03886": {"original_text": "  A meme is an part of media created to share an opinion or emotion across the\ninternet. Due to its popularity, memes have become the new forms of\ncommunication on social media. However, due to its nature, they are being used\nin harmful ways such as trolling and cyberbullying progressively. Various data\nmodelling methods create different possibilities in feature extraction and\nturning them into beneficial information. The variety of modalities included in\ndata plays a significant part in predicting the results. We try to explore the\nsignificance of visual features of images in classifying memes. Memes are a\nblend of both image and text, where the text is embedded into the image. We try\nto incorporate the memes as troll and non-trolling memes based on the images\nand the text on them. However, the images are to be analysed and combined with\nthe text to increase performance. Our work illustrates different textual\nanalysis methods and contrasting multimodal methods ranging from simple merging\nto cross attention to utilising both worlds' - best visual and textual\nfeatures. The fine-tuned cross-lingual language model, XLM, performed the best\nin textual analysis, and the multimodal transformer performs the best in\nmultimodal analysis.\n", "rewritten_text": "A meme is a form of media created to express an opinion or emotion across the internet. With their rising popularity, memes have emerged as a new mode of communication on social media. However, their potential for harm is evident as they are increasingly used for trolling and cyberbullying. Various data modeling techniques offer diverse possibilities for extracting features and transforming them into valuable information. The inclusion of different modalities in the data significantly influences result prediction.\n\nOur research focuses on exploring the importance of visual features in classifying memes. Memes combine both images and text, with the text integrated into the image itself. We aim to categorize memes as either trolling or non-trolling based on the content of the images and text. Analyzing the images and text together is crucial for enhancing performance. Our study demonstrates various textual analysis methods and compares multimodal approaches, from simple merging to cross-attention, to leverage the best visual and textual features.\n\nIn our experiments, the fine-tuned cross-lingual language model XLM excelled in textual analysis, while the multimodal transformer outperformed in multimodal analysis."}, "2305.03973": {"original_text": "  Implicit Discourse Relation Recognition (IDRR) is a sophisticated and\nchallenging task to recognize the discourse relations between the arguments\nwith the absence of discourse connectives. The sense labels for each discourse\nrelation follow a hierarchical classification scheme in the annotation process\n(Prasad et al., 2008), forming a hierarchy structure. Most existing works do\nnot well incorporate the hierarchy structure but focus on the syntax features\nand the prior knowledge of connectives in the manner of pure text\nclassification. We argue that it is more effective to predict the paths inside\nthe hierarchical tree (e.g., \"Comparison -> Contrast -> however\") rather than\nflat labels (e.g., Contrast) or connectives (e.g., however). We propose a\nprompt-based path prediction method to utilize the interactive information and\nintrinsic senses among the hierarchy in IDRR. This is the first work that\ninjects such structure information into pre-trained language models via prompt\ntuning, and the performance of our solution shows significant and consistent\nimprovement against competitive baselines.\n", "rewritten_text": "Implicit Discourse Relation Recognition (IDRR) presents a sophisticated and challenging task involving the recognition of discourse relations between arguments in the absence of discourse connectives. The sense labels assigned to each discourse relation adhere to a hierarchical classification scheme during the annotation process (Prasad et al., 2008), resulting in a structured hierarchy. While many existing studies primarily focus on syntax features and prior knowledge of connectives for text classification, they often overlook the hierarchical structure. We posit that predicting paths within the hierarchical tree (e.g., \"Comparison -> Contrast -> however\") is more effective than simply using flat labels (e.g., Contrast) or connectives (e.g., however). To address this, we introduce a prompt-based path prediction method that leverages interactive information and intrinsic senses within the hierarchy of IDRR. Our approach marks the first instance of incorporating such structural information into pre-trained language models through prompt tuning. The performance of our solution demonstrates significant and consistent improvements over competitive baselines."}, "1908.06665": {"original_text": "  Recently, significant progresses have been made in object detection on common\nbenchmarks (i.e., Pascal VOC). However, object detection in real world is still\nchallenging due to the serious data imbalance. Images in real world are\ndominated by easy samples like the wide range of background and some easily\nrecognizable objects, for example. Although two-stage detectors like Faster\nR-CNN achieved big successes in object detection due to the strategy of\nextracting region proposals by region proposal network, they show their poor\nadaption in real-world object detection as a result of without considering\nmining hard samples during extracting region proposals. To address this issue,\nwe propose a Cascade framework of Region Proposal Networks, referred to as\nC-RPNs. The essence of C-RPNs is adopting multiple stages to mine hard samples\nwhile extracting region proposals and learn stronger classifiers. Meanwhile, a\nfeature chain and a score chain are proposed to help learning more\ndiscriminative representations for proposals. Moreover, a loss function of\ncascade stages is designed to train cascade classifiers through\nbackpropagation. Our proposed method has been evaluated on Pascal VOC and\nseveral challenging datasets like BSBDV 2017, CityPersons, etc. Our method\nachieves competitive results compared with the current state-of-the-arts and\nall-sided improvements in error analysis, validating its efficacy for detection\nin real world.\n", "rewritten_text": "Significant progress has recently been made in object detection on common benchmarks, such as Pascal VOC. However, object detection in the real world remains challenging due to serious data imbalances. Real-world images are predominantly composed of easy samples, such as a wide range of backgrounds and easily recognizable objects. While two-stage detectors like Faster R-CNN have achieved success in object detection by extracting region proposals through a region proposal network, they demonstrate poor adaptation in real-world scenarios due to a lack of consideration for mining hard samples during proposal extraction.\n\nTo tackle this issue, we propose a Cascade framework of Region Proposal Networks, known as C-RPNs. The core concept of C-RPNs involves employing multiple stages to mine hard samples during proposal extraction and to enhance classifier learning. Additionally, we introduce a feature chain and a score chain to facilitate the learning of more discriminative representations for proposals. Furthermore, a cascade stages loss function is designed to train cascade classifiers through backpropagation.\n\nOur proposed method has been evaluated on Pascal VOC and various challenging datasets such as BSBDV 2017 and CityPersons. Our approach achieves competitive results compared to current state-of-the-art methods and demonstrates comprehensive improvements in error analysis, validating its effectiveness for real-world object detection."}, "2001.04388": {"original_text": "  The efficient fusion of depth maps is a key part of most state-of-the-art 3D\nreconstruction methods. Besides requiring high accuracy, these depth fusion\nmethods need to be scalable and real-time capable. To this end, we present a\nnovel real-time capable machine learning-based method for depth map fusion.\nSimilar to the seminal depth map fusion approach by Curless and Levoy, we only\nupdate a local group of voxels to ensure real-time capability. Instead of a\nsimple linear fusion of depth information, we propose a neural network that\npredicts non-linear updates to better account for typical fusion errors. Our\nnetwork is composed of a 2D depth routing network and a 3D depth fusion network\nwhich efficiently handle sensor-specific noise and outliers. This is especially\nuseful for surface edges and thin objects for which the original approach\nsuffers from thickening artifacts. Our method outperforms the traditional\nfusion approach and related learned approaches on both synthetic and real data.\nWe demonstrate the performance of our method in reconstructing fine geometric\ndetails from noise and outlier contaminated data on various scenes.\n", "rewritten_text": "The effective fusion of depth maps plays a crucial role in the majority of cutting-edge 3D reconstruction techniques. These depth fusion methods not only demand high accuracy but also require scalability and real-time capability. In light of this, we introduce a novel machine learning-based method for real-time depth map fusion. Drawing inspiration from the pioneering depth map fusion method developed by Curless and Levoy, our approach focuses on updating a local group of voxels to ensure real-time performance. Instead of a basic linear fusion of depth information, we propose the use of a neural network that predicts non-linear updates to better address common fusion errors. Our network comprises a 2D depth routing network and a 3D depth fusion network, which effectively manage sensor-specific noise and outliers. This is particularly beneficial for surface edges and slender objects, areas where the original approach tends to exhibit thickening artifacts. Our method surpasses both the traditional fusion approach and other similar learned methods when tested on synthetic and real data. We showcase the effectiveness of our method in accurately reconstructing intricate geometric details from data contaminated with noise and outliers across various scenes."}, "1810.08768": {"original_text": "  Motion estimation (ME) and motion compensation (MC) have been widely used for\nclassical video frame interpolation systems over the past decades. Recently, a\nnumber of data-driven frame interpolation methods based on convolutional neural\nnetworks have been proposed. However, existing learning based methods typically\nestimate either flow or compensation kernels, thereby limiting performance on\nboth computational efficiency and interpolation accuracy. In this work, we\npropose a motion estimation and compensation driven neural network for video\nframe interpolation. A novel adaptive warping layer is developed to integrate\nboth optical flow and interpolation kernels to synthesize target frame pixels.\nThis layer is fully differentiable such that both the flow and kernel\nestimation networks can be optimized jointly. The proposed model benefits from\nthe advantages of motion estimation and compensation methods without using\nhand-crafted features. Compared to existing methods, our approach is\ncomputationally efficient and able to generate more visually appealing results.\nFurthermore, the proposed MEMC-Net can be seamlessly adapted to several video\nenhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive\nquantitative and qualitative evaluations demonstrate that the proposed method\nperforms favorably against the state-of-the-art video frame interpolation and\nenhancement algorithms on a wide range of datasets.\n", "rewritten_text": "Motion estimation (ME) and motion compensation (MC) have long been utilized in traditional video frame interpolation systems. In recent years, there has been a surge in data-driven frame interpolation techniques leveraging convolutional neural networks. However, existing learning-based methods often focus on estimating either flow or compensation kernels, leading to limitations in computational efficiency and interpolation accuracy.\n\nIn this study, we introduce a novel approach: a neural network driven by motion estimation and compensation for video frame interpolation. We have developed an innovative adaptive warping layer that combines optical flow and interpolation kernels to synthesize target frame pixels. This layer is fully differentiable, enabling simultaneous optimization of both the flow and kernel estimation networks. Our model harnesses the benefits of motion estimation and compensation methods without relying on hand-crafted features.\n\nCompared to existing methods, our approach is computationally efficient and produces visually superior results. Moreover, the proposed MEMC-Net can seamlessly adapt to various video enhancement tasks such as super-resolution, denoising, and deblocking. Extensive quantitative and qualitative evaluations demonstrate the superior performance of our method compared to state-of-the-art video frame interpolation and enhancement algorithms across a diverse range of datasets."}, "2303.12793": {"original_text": "  This work focuses on sign language retrieval-a recently proposed task for\nsign language understanding. Sign language retrieval consists of two sub-tasks:\ntext-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval.\nDifferent from traditional video-text retrieval, sign language videos, not only\ncontain visual signals but also carry abundant semantic meanings by themselves\ndue to the fact that sign languages are also natural languages. Considering\nthis character, we formulate sign language retrieval as a cross-lingual\nretrieval problem as well as a video-text retrieval task. Concretely, we take\ninto account the linguistic properties of both sign languages and natural\nlanguages, and simultaneously identify the fine-grained cross-lingual (i.e.,\nsign-to-word) mappings while contrasting the texts and the sign videos in a\njoint embedding space. This process is termed as cross-lingual contrastive\nlearning. Another challenge is raised by the data scarcity issue-sign language\ndatasets are orders of magnitude smaller in scale than that of speech\nrecognition. We alleviate this issue by adopting a domain-agnostic sign encoder\npre-trained on large-scale sign videos into the target domain via\npseudo-labeling. Our framework, termed as domain-aware sign language retrieval\nvia Cross-lingual Contrastive learning or CiCo for short, outperforms the\npioneering method by large margins on various datasets, e.g., +22.4 T2V and\n+28.0 V2T R@1 improvements on How2Sign dataset, and +13.7 T2V and +17.1 V2T R@1\nimprovements on PHOENIX-2014T dataset. Code and models are available at:\nhttps://github.com/FangyunWei/SLRT.\n", "rewritten_text": "This work focuses on sign language retrieval, a recently proposed task for understanding sign language. Sign language retrieval comprises two sub-tasks: text-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval. Unlike traditional video-text retrieval, sign language videos not only convey visual signals but also carry rich semantic meanings on their own, as sign languages are natural languages. Taking into consideration this characteristic, we approach sign language retrieval as both a cross-lingual retrieval problem and a video-text retrieval task. Specifically, we consider the linguistic properties of both sign languages and natural languages, and simultaneously identify the detailed cross-lingual (sign-to-word) mappings while contrasting the texts and sign videos in a shared embedding space. This process is referred to as cross-lingual contrastive learning.\n\nAnother challenge arises from the scarcity of data\u2014sign language datasets are significantly smaller in scale compared to speech recognition datasets. To address this challenge, we mitigate the issue by utilizing a domain-agnostic sign encoder pre-trained on large-scale sign videos and transferring it to the target domain through pseudo-labeling. Our framework, named domain-aware sign language retrieval via Cross-lingual Contrastive learning (CiCo), significantly outperforms the leading method on various datasets. For instance, we achieve improvements of +22.4 in T2V and +28.0 in V2T R@1 on the How2Sign dataset, and +13.7 in T2V and +17.1 in V2T R@1 on the PHOENIX-2014T dataset. The code and models can be accessed at: https://github.com/FangyunWei/SLRT."}, "2410.01928": {"original_text": "  In the domain of battery research, the processing of high-resolution\nmicroscopy images is a challenging task, as it involves dealing with complex\nimages and requires a prior understanding of the components involved. The\nutilization of deep learning methodologies for image analysis has attracted\nconsiderable interest in recent years, with multiple investigations employing\nsuch techniques for image segmentation and analysis within the realm of battery\nresearch. However, the automated analysis of high-resolution microscopy images\nfor detecting phases and components in composite materials is still an\nunderexplored area. This work proposes a novel workflow for detecting\ncomponents and phase segmentation from raw high resolution transmission\nelectron microscopy (TEM) images using a trained U-Net segmentation model. The\ndeveloped model can expedite the detection of components and phase\nsegmentation, diminishing the temporal and cognitive demands associated with\nscrutinizing an extensive array of TEM images, thereby mitigating the potential\nfor human errors. This approach presents a novel and efficient image analysis\napproach with broad applicability beyond the battery field and holds potential\nfor application in other related domains characterized by phase and composition\ndistribution, such as alloy production.\n", "rewritten_text": "In the field of battery research, processing high-resolution microscopy images presents a challenging task due to the complexity of the images and the need for a prior understanding of the components involved. The use of deep learning methodologies for image analysis has garnered significant interest in recent years, with numerous studies employing these techniques for image segmentation and analysis in battery research. However, the automated analysis of high-resolution microscopy images to detect phases and components in composite materials remains an underexplored area.\n\nThis study introduces a novel workflow for detecting components and phase segmentation in raw high-resolution transmission electron microscopy (TEM) images using a trained U-Net segmentation model. The developed model streamlines the detection of components and phase segmentation, reducing the time and cognitive effort required to analyze a large number of TEM images and minimizing the potential for human errors. This approach offers an innovative and efficient image analysis method with broad applicability beyond the battery field, showing promise for application in other domains characterized by phase and composition distribution, such as alloy production."}, "2103.15467": {"original_text": "  Unsupervised domain adaptation (UDA) becomes more and more popular in\ntackling real-world problems without ground truth of the target domain. Though\ntedious annotation work is not required, UDA unavoidably faces two problems: 1)\nhow to narrow the domain discrepancy to boost the transferring performance; 2)\nhow to improve pseudo annotation producing mechanism for self-supervised\nlearning (SSL). In this paper, we focus on UDA for semantic segmentation task.\nFirstly, we introduce adversarial learning into style gap bridging mechanism to\nkeep the style information from two domains in the similar space. Secondly, to\nkeep the balance of pseudo labels on each category, we propose a\ncategory-adaptive threshold mechanism to choose category-wise pseudo labels for\nSSL. The experiments are conducted using GTA5 as the source domain, Cityscapes\nas the target domain. The results show that our model outperforms the\nstate-of-the-arts with a noticeable gain on cross-domain adaptation tasks.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) is increasingly popular for addressing real-world problems without access to ground truth data from the target domain. While UDA eliminates the need for laborious annotation work, it encounters two key challenges: 1) reducing domain discrepancy to enhance transfer performance, and 2) enhancing the mechanism for generating pseudo annotations in self-supervised learning (SSL). This study focuses on UDA for semantic segmentation tasks. \n\nTo address these challenges, we first incorporate adversarial learning into a style gap bridging mechanism to align style information from two domains in a shared space. Additionally, we propose a category-adaptive threshold mechanism to ensure a balanced distribution of pseudo labels across different categories for SSL. Experimental evaluations are conducted using GTA5 as the source domain and Cityscapes as the target domain. The results demonstrate that our model surpasses existing state-of-the-art methods, achieving significant improvements in cross-domain adaptation tasks."}, "2203.17013": {"original_text": "  Video streams are utilised to guide minimally-invasive surgery and diagnostic\nprocedures in a wide range of procedures, and many computer assisted techniques\nhave been developed to automatically analyse them. These approaches can provide\nadditional information to the surgeon such as lesion detection, instrument\nnavigation, or anatomy 3D shape modeling. However, the necessary image features\nto recognise these patterns are not always reliably detected due to the\npresence of irregular light patterns such as specular highlight reflections. In\nthis paper, we aim at removing specular highlights from endoscopic videos using\nmachine learning. We propose using a temporal generative adversarial network\n(GAN) to inpaint the hidden anatomy under specularities, inferring its\nappearance spatially and from neighbouring frames where they are not present in\nthe same location. This is achieved using in-vivo data of gastric endoscopy\n(Hyper-Kvasir) in a fully unsupervised manner that relies on automatic\ndetection of specular highlights. System evaluations show significant\nimprovements to traditional methods through direct comparison as well as other\nmachine learning techniques through an ablation study that depicts the\nimportance of the network's temporal and transfer learning components. The\ngeneralizability of our system to different surgical setups and procedures was\nalso evaluated qualitatively on in-vivo data of gastric endoscopy and ex-vivo\nporcine data (SERV-CT, SCARED). We also assess the effect of our method in\ncomputer vision tasks that underpin 3D reconstruction and camera motion\nestimation, namely stereo disparity, optical flow, and sparse point feature\nmatching. These are evaluated quantitatively and qualitatively and results show\na positive effect of specular highlight inpainting on these tasks in a novel\ncomprehensive analysis.\n", "rewritten_text": "Video streams are used to assist in minimally-invasive surgery and diagnostic procedures across a wide range of medical practices. Various computer-assisted techniques have been developed to automatically analyze these streams, providing surgeons with additional information such as lesion detection, instrument navigation, and 3D anatomy modeling. However, the reliable detection of necessary image features to recognize patterns is often hindered by irregular light patterns like specular highlight reflections.\n\nIn this study, our objective is to eliminate specular highlights from endoscopic videos using machine learning. We propose the utilization of a temporal generative adversarial network (GAN) to inpaint the obscured anatomy beneath specularities, predicting its appearance spatially and across neighboring frames where they are not present in the same location. This approach is based on in-vivo data from gastric endoscopy (Hyper-Kvasir) and is conducted in a fully unsupervised manner, relying on the automatic detection of specular highlights.\n\nSystem evaluations demonstrate significant enhancements over traditional methods through direct comparisons and ablation studies with other machine learning techniques, highlighting the importance of the network's temporal and transfer learning components. The system's generalizability to various surgical setups and procedures is qualitatively assessed using in-vivo data from gastric endoscopy and ex-vivo porcine data (SERV-CT, SCARED).\n\nFurthermore, we evaluate the impact of our method on computer vision tasks essential for 3D reconstruction and camera motion estimation, including stereo disparity, optical flow, and sparse point feature matching. These evaluations are conducted both quantitatively and qualitatively, revealing a positive influence of specular highlight inpainting on these tasks in a comprehensive analysis."}, "2010.02808": {"original_text": "  We propose a method to learn image representations from uncurated videos. We\ncombine a supervised loss from off-the-shelf object detectors and\nself-supervised losses which naturally arise from the video-shot-frame-object\nhierarchy present in each video. We report competitive results on 19 transfer\nlearning tasks of the Visual Task Adaptation Benchmark (VTAB), and on 8\nout-of-distribution-generalization tasks, and discuss the benefits and\nshortcomings of the proposed approach. In particular, it improves over the\nbaseline on all 18/19 few-shot learning tasks and 8/8 out-of-distribution\ngeneralization tasks. Finally, we perform several ablation studies and analyze\nthe impact of the pretrained object detector on the performance across this\nsuite of tasks.\n", "rewritten_text": "We present a method for learning image representations from uncurated videos by combining a supervised loss derived from off-the-shelf object detectors with self-supervised losses that naturally stem from the video-shot-frame-object hierarchy within each video. Our study showcases competitive results across 19 transfer learning tasks within the Visual Task Adaptation Benchmark (VTAB), as well as 8 out-of-distribution generalization tasks. We delve into the advantages and limitations of our proposed approach, noting its enhancements over the baseline in 18 out of 19 few-shot learning tasks and all 8 out-of-distribution generalization tasks. Additionally, we conduct various ablation studies to examine the impact of the pretrained object detector on performance across this array of tasks."}, "1903.12529": {"original_text": "  While deep neural networks (DNN) based single image super-resolution (SISR)\nmethods are rapidly gaining popularity, they are mainly designed for the\nwidely-used bicubic degradation, and there still remains the fundamental\nchallenge for them to super-resolve low-resolution (LR) image with arbitrary\nblur kernels. In the meanwhile, plug-and-play image restoration has been\nrecognized with high flexibility due to its modular structure for easy plug-in\nof denoiser priors. In this paper, we propose a principled formulation and\nframework by extending bicubic degradation based deep SISR with the help of\nplug-and-play framework to handle LR images with arbitrary blur kernels.\nSpecifically, we design a new SISR degradation model so as to take advantage of\nexisting blind deblurring methods for blur kernel estimation. To optimize the\nnew degradation induced energy function, we then derive a plug-and-play\nalgorithm via variable splitting technique, which allows us to plug any\nsuper-resolver prior rather than the denoiser prior as a modular part.\nQuantitative and qualitative evaluations on synthetic and real LR images\ndemonstrate that the proposed deep plug-and-play super-resolution framework is\nflexible and effective to deal with blurry LR images.\n", "rewritten_text": "While deep neural networks (DNN)-based single image super-resolution (SISR) methods are rapidly gaining popularity, they are primarily tailored for the commonly encountered bicubic degradation. However, a fundamental challenge persists in their ability to enhance low-resolution (LR) images affected by arbitrary blur kernels. Concurrently, plug-and-play image restoration has garnered attention for its high flexibility, owing to its modular structure that allows for the seamless integration of denoiser priors.\n\nIn this study, we introduce a principled formulation and framework that extends the capabilities of bicubic degradation-based deep SISR by leveraging the plug-and-play framework to address LR images with arbitrary blur kernels. Specifically, we introduce a novel SISR degradation model that capitalizes on existing blind deblurring methods for blur kernel estimation. To optimize the resulting degradation-induced energy function, we employ a plug-and-play algorithm utilizing variable splitting techniques, enabling the incorporation of any super-resolver prior as a modular component, rather than being limited to denoiser priors.\n\nQuantitative and qualitative assessments conducted on synthetic and real LR images demonstrate the efficacy and flexibility of the proposed deep plug-and-play super-resolution framework in handling blurry LR images."}, "2407.05092": {"original_text": "  Computational and human perception are often considered separate approaches\nfor studying sound changes over time; few works have touched on the\nintersection of both. To fill this research gap, we provide a pioneering review\ncontrasting computational with human perception from the perspectives of\nmethods and tasks. Overall, computational approaches rely on computer-driven\nmodels to perceive historical sound changes on etymological datasets, while\nhuman approaches use listener-driven models to perceive ongoing sound changes\non recording corpora. Despite their differences, both approaches complement\neach other on phonetic and acoustic levels, showing the potential to achieve a\nmore comprehensive perception of sound change. Moreover, we call for a\ncomparative study on the datasets used by both approaches to investigate the\ninfluence of historical sound changes on ongoing changes. Lastly, we discuss\nthe applications of sound change in computational linguistics, and point out\nthat perceiving sound change alone is insufficient, as many processes of\nlanguage change are complex, with entangled changes at syntactic, semantic, and\nphonetic levels.\n", "rewritten_text": "The study of sound changes over time often distinguishes between computational and human perception as separate approaches. However, there is a lack of research that explores the intersection of these two perspectives. To address this gap, we present a groundbreaking review that contrasts computational and human perception in terms of methods and tasks. \n\nComputational approaches typically utilize computer-driven models to analyze historical sound changes in etymological datasets. On the other hand, human approaches rely on listener-driven models to observe ongoing sound changes in recording corpora. Despite their distinct methodologies, both approaches complement each other at phonetic and acoustic levels, offering the potential for a more comprehensive understanding of sound change. \n\nWe advocate for a comparative analysis of the datasets employed by both approaches to investigate how historical sound changes influence ongoing transformations. Additionally, we highlight the significance of considering sound change within the broader context of language evolution, as language change involves intricate processes that encompass syntactic, semantic, and phonetic levels. \n\nIn conclusion, we emphasize the importance of recognizing that perceiving sound change in isolation is inadequate, given the complexity of language evolution. This complexity underscores the need to consider intertwined changes across various linguistic dimensions."}, "1812.03621": {"original_text": "  The majority of Multi-Object Tracking (MOT) algorithms based on the\ntracking-by-detection scheme do not use higher order dependencies among objects\nor tracklets, which makes them less effective in handling complex scenarios. In\nthis work, we present a new near-online MOT algorithm based on non-uniform\nhypergraph, which can model different degrees of dependencies among tracklets\nin a unified objective. The nodes in the hypergraph correspond to the tracklets\nand the hyperedges with different degrees encode various kinds of dependencies\namong them. Specifically, instead of setting the weights of hyperedges with\ndifferent degrees empirically, they are learned automatically using the\nstructural support vector machine algorithm (SSVM). Several experiments are\ncarried out on various challenging datasets (i.e., PETS09, ParkingLot sequence,\nSubwayFace, and MOT16 benchmark), to demonstrate that our method achieves\nfavorable performance against the state-of-the-art MOT methods.\n", "rewritten_text": "The majority of Multi-Object Tracking (MOT) algorithms that are based on the tracking-by-detection scheme do not utilize higher-order dependencies among objects or tracklets, which limits their effectiveness in handling complex scenarios. In this study, we introduce a novel near-online MOT algorithm that is founded on a non-uniform hypergraph. This approach allows for the modeling of varying degrees of dependencies among tracklets within a unified objective framework. The nodes within the hypergraph represent the tracklets, while the hyperedges with different degrees encode diverse types of dependencies among them. Instead of assigning weights to hyperedges with different degrees empirically, our method automatically learns these weights using the structural support vector machine algorithm (SSVM). We conducted several experiments on challenging datasets such as PETS09, ParkingLot sequence, SubwayFace, and MOT16 benchmark to showcase that our approach outperforms state-of-the-art MOT methods."}, "2211.08358": {"original_text": "  Few-shot classification has made great strides due to foundation models that,\nthrough priming and prompting, are highly effective few-shot learners. However,\nthis approach has high variance both across different sets of few shots (data\nselection) and across different finetuning runs (run variability). This is\nproblematic not only because it impedes the fair comparison of different\napproaches, but especially because it makes few-shot learning too unreliable\nfor many real-world applications. To alleviate these issues, we make two\ncontributions for more stable and effective few-shot learning: First, we\npropose novel ensembling methods and show that they substantially reduce run\nvariability. Second, we introduce a new active learning (AL) criterion for data\nselection and present the first AL-based approach specifically tailored towards\nprompt-based learning. In our experiments, we show that our combined method,\nMEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),\nimproves overall performance of prompt-based finetuning by 2.3 points on five\ndiverse tasks. We publicly share our code and data splits in\nhttps://github.com/akoksal/MEAL.\n", "rewritten_text": "Few-shot classification has advanced significantly thanks to foundation models that, through priming and prompting, serve as highly effective few-shot learners. However, this approach exhibits high variance across different sets of few shots (data selection) and across various finetuning runs (run variability). This poses a challenge not only in terms of hindering the fair comparison of different approaches but also in rendering few-shot learning too unreliable for many real-world applications. To address these issues, we present two key contributions aimed at enhancing the stability and effectiveness of few-shot learning.\n\nFirstly, we propose innovative ensembling methods that have been demonstrated to significantly reduce run variability. Secondly, we introduce a new active learning (AL) criterion for data selection and unveil the first AL-based approach specifically tailored for prompt-based learning. Through our experiments, we demonstrate that our integrated method, MEAL (Multiprompt Finetuning and Prediction Ensembling with Active Learning), enhances the overall performance of prompt-based finetuning by 2.3 points across five diverse tasks. We have made our code and data splits publicly available at https://github.com/akoksal/MEAL."}, "1909.09934": {"original_text": "  We propose methods to train convolutional neural networks (CNNs) with both\nbinarized weights and activations, leading to quantized models that are\nspecifically friendly to mobile devices with limited power capacity and\ncomputation resources. Previous works on quantizing CNNs often seek to\napproximate the floating-point information using a set of discrete values,\nwhich we call value approximation, typically assuming the same architecture as\nthe full-precision networks. Here we take a novel \"structure approximation\"\nview of quantization -- it is very likely that different architectures designed\nfor low-bit networks may be better for achieving good performance. In\nparticular, we propose a \"network decomposition\" strategy, termed Group-Net, in\nwhich we divide the network into groups. Thus, each full-precision group can be\neffectively reconstructed by aggregating a set of homogeneous binary branches.\nIn addition, we learn effective connections among groups to improve the\nrepresentation capability. Moreover, the proposed Group-Net shows strong\ngeneralization to other tasks. For instance, we extend Group-Net for accurate\nsemantic segmentation by embedding rich context into the binary structure.\nFurthermore, for the first time, we apply binary neural networks to object\ndetection. Experiments on both classification, semantic segmentation and object\ndetection tasks demonstrate the superior performance of the proposed methods\nover various quantized networks in the literature. Our methods outperform the\nprevious best binary neural networks in terms of accuracy and computation\nefficiency.\n", "rewritten_text": "We present novel methods for training convolutional neural networks (CNNs) with binarized weights and activations. These methods result in quantized models that are optimized for mobile devices with limited power capacity and computational resources. Previous approaches to quantizing CNNs have focused on approximating floating-point information using discrete values, a technique known as value approximation. These approaches typically maintain the same architecture as full-precision networks. In contrast, our approach introduces a new perspective on quantization called \"structure approximation.\" We suggest that different architectures tailored for low-bit networks may yield better performance. \n\nOur proposed strategy, Group-Net, involves decomposing the network into groups. Each group consists of homogeneous binary branches that collectively reconstruct the full-precision group. We also incorporate effective connections among groups to enhance representation capability. Group-Net demonstrates strong generalization across various tasks. For example, we enhance Group-Net for accurate semantic segmentation by integrating rich context into the binary structure. Additionally, we pioneer the application of binary neural networks in object detection.\n\nExperimental results across classification, semantic segmentation, and object detection tasks showcase the superior performance of our methods compared to existing quantized networks. Our methods surpass previous state-of-the-art binary neural networks in terms of accuracy and computational efficiency."}, "2407.04024": {"original_text": "  Deep unfolding methods and transformer architecture have recently shown\npromising results in hyperspectral image (HSI) reconstruction. However, there\nstill exist two issues: (1) in the data subproblem, most methods represents the\nstepsize utilizing a learnable parameter. Nevertheless, for different spectral\nchannel, error between features and ground truth is unequal. (2) Transformer\nstruggles to balance receptive field size with pixel-wise detail information.\nTo overcome the aforementioned drawbacks, We proposed an adaptive step-size\nperception unfolding network (ASPUN), a deep unfolding network based on FISTA\nalgorithm, which uses an adaptive step-size perception module to estimate the\nupdate step-size of each spectral channel. In addition, we design a Non-local\nHybrid Attention Transformer(NHAT) module for fully leveraging the receptive\nfield advantage of transformer. By plugging the NLHA into the Non-local\nInformation Aggregation (NLIA) module, the unfolding network can achieve better\nreconstruction results. Experimental results show that our ASPUN is superior to\nthe existing SOTA algorithms and achieves the best performance.\n", "rewritten_text": "Recently, deep unfolding methods and transformer architecture have demonstrated promising results in hyperspectral image (HSI) reconstruction. However, two issues persist: \n\n1. In the data subproblem, most methods represent the stepsize using a learnable parameter. However, the error between features and ground truth varies across different spectral channels. \n\n2. The transformer struggles to balance receptive field size with pixel-wise detail information. \n\nTo address these drawbacks, we introduce an adaptive step-size perception unfolding network (ASPUN). ASPUN is a deep unfolding network based on the FISTA algorithm, incorporating an adaptive step-size perception module to estimate the update step-size for each spectral channel. Additionally, we introduce a Non-local Hybrid Attention Transformer (NHAT) module to fully leverage the receptive field advantage of the transformer. By integrating the NHAT into the Non-local Information Aggregation (NLIA) module, the unfolding network achieves improved reconstruction results. Experimental results demonstrate that ASPUN outperforms existing state-of-the-art algorithms, delivering the best performance."}, "1810.10519": {"original_text": "  With the increasing use of social networks and mobile devices, the number of\nvideos posted on the Internet is growing exponentially. Among the inappropriate\ncontents published on the Internet, pornography is one of the most worrying as\nit can be accessed by teens and children. Two spatiotemporal CNNs, VGG-C3D CNN\nand ResNet R(2+1)D CNN, were assessed for pornography detection in videos in\nthe present study. Experimental results using the Pornography-800 dataset\nshowed that these spatiotemporal CNNs performed better than some\nstate-of-the-art methods based on bag of visual words and are competitive with\nother CNN-based approaches, reaching accuracy of 95.1%.\n", "rewritten_text": "The proliferation of social networks and mobile devices has led to a significant increase in the number of videos being shared on the Internet. Among the various inappropriate contents circulating online, pornography stands out as a particularly concerning issue due to its accessibility to teenagers and children. In a recent study, two spatiotemporal CNNs, namely VGG-C3D CNN and ResNet R(2+1)D CNN, were evaluated for their effectiveness in detecting pornography in videos. The experimental findings, based on the Pornography-800 dataset, demonstrated that these spatiotemporal CNNs outperformed some traditional methods relying on bag of visual words and were on par with other CNN-based approaches, achieving an impressive accuracy rate of 95.1%."}, "2406.10209": {"original_text": "  Large language models can memorize and repeat their training data, causing\nprivacy and copyright risks. To mitigate memorization, we introduce a subtle\nmodification to the next-token training objective that we call the goldfish\nloss. During training, randomly sampled subsets of tokens are excluded from the\nloss computation. These dropped tokens are not memorized by the model, which\nprevents verbatim reproduction of a complete chain of tokens from the training\nset. We run extensive experiments training billion-scale Llama-2 models, both\npre-trained and trained from scratch, and demonstrate significant reductions in\nextractable memorization with little to no impact on downstream benchmarks.\n", "rewritten_text": "Large language models have the capability to memorize and replicate their training data, posing risks related to privacy and copyright. In order to address this issue, we propose a subtle modification to the next-token training objective, which we refer to as the goldfish loss. This modification involves excluding randomly sampled subsets of tokens from the loss computation during training. By doing so, the model does not memorize these dropped tokens, thereby preventing the exact reproduction of an entire sequence of tokens from the training set. \n\nWe conducted extensive experiments involving the training of billion-scale Llama-2 models, both pre-trained and trained from scratch. Our results demonstrate significant reductions in extractable memorization, with minimal to no impact on downstream benchmarks."}, "2110.14398": {"original_text": "  To consider Hawrami and Zaza (Zazaki) standalone languages or dialects of a\nlanguage have been discussed and debated for a while among linguists active in\nstudying Iranian languages. The question of whether those languages/dialects\nbelong to the Kurdish language or if they are independent descendants of\nIranian languages was answered by MacKenzie (1961). However, a majority of\npeople who speak the dialects are against that answer. Their disapproval mainly\nseems to be based on the sociological, cultural, and historical relationship\namong the speakers of the dialects. While the case of Hawrami and Zaza has\nremained unexplored and under-examined, an almost unanimous agreement exists\nabout the classification of Kurmanji and Sorani as Kurdish dialects. The\nrelated studies to address the mentioned cases are primarily qualitative.\nHowever, computational linguistics could approach the question from a\nquantitative perspective. In this research, we look into three questions from a\nlinguistic distance point of view. First, how similar or dissimilar Hawrami and\nZaza are, considering no common geographical coexistence between the two.\nSecond, what about Kurmanji and Sorani that have geographical overlap. Finally,\nwhat is the distance among all these dialects, pair by pair? We base our\ncomputation on phonetic presentations of these dialects (languages), and we\ncalculate various linguistic distances among the pairs. We analyze the data and\ndiscuss the results to conclude.\n", "rewritten_text": "The classification of Hawrami and Zaza (Zazaki) as standalone languages or dialects of a language has been a topic of discussion and debate among linguists specializing in Iranian languages. MacKenzie (1961) provided an answer to the question of whether these languages/dialects are part of the Kurdish language or independent descendants of Iranian languages. However, a majority of speakers of these dialects disagree with this conclusion. Their dissent appears to be primarily rooted in the sociological, cultural, and historical connections among the speakers of these dialects.\n\nWhile the status of Hawrami and Zaza remains largely unexplored and under-examined, there is almost unanimous agreement on the classification of Kurmanji and Sorani as Kurdish dialects. Existing studies addressing these cases are predominantly qualitative in nature. However, computational linguistics could offer a quantitative perspective on this issue.\n\nIn this study, we aim to investigate three questions from a linguistic distance standpoint. Firstly, we will examine the similarities or differences between Hawrami and Zaza, considering their lack of geographical coexistence. Secondly, we will explore the relationship between Kurmanji and Sorani, which share geographical overlap. Lastly, we will analyze the distance between all these dialects, pair by pair.\n\nOur analysis is based on the phonetic representations of these dialects (languages), and we will calculate various linguistic distances between the pairs. By examining the data and discussing the results, we aim to draw conclusions on this matter."}, "2401.14718": {"original_text": "  Future Frame Synthesis (FFS) aims to enable models to generate sequences of\nfuture frames based on existing content. This survey comprehensively reviews\nhistorical and contemporary works in FFS, including widely used datasets and\nalgorithms. It scrutinizes the challenges and the evolving landscape of FFS\nwithin computer vision, with a focus on the transition from deterministic to\ngenerative synthesis methodologies. Our taxonomy highlights the significant\nadvancements and shifts in approach, underscoring the growing importance of\ngenerative models in achieving realistic and diverse future frame predictions.\n", "rewritten_text": "Future Frame Synthesis (FFS) aims to empower models to generate sequences of future frames by leveraging existing content. This survey provides a comprehensive review of both historical and contemporary works in FFS, encompassing widely used datasets and algorithms. It delves into the challenges and the changing landscape of FFS within computer vision, particularly focusing on the shift from deterministic to generative synthesis methodologies. Our taxonomy emphasizes the notable advancements and changes in approach, highlighting the increasing significance of generative models in producing realistic and diverse future frame predictions."}, "1808.0841": {"original_text": "  Training a good deep learning model often requires a lot of annotated data.\nAs a large amount of labeled data is typically difficult to collect and even\nmore difficult to annotate, data augmentation and data generation are widely\nused in the process of training deep neural networks. However, there is no\nclear common understanding on how much labeled data is needed to get\nsatisfactory performance. In this paper, we try to address such a question\nusing vehicle license plate character recognition as an example application. We\napply computer graphic scripts and Generative Adversarial Networks to generate\nand augment a large number of annotated, synthesized license plate images with\nrealistic colors, fonts, and character composition from a small number of real,\nmanually labeled license plate images. Generated and augmented data are mixed\nand used as training data for the license plate recognition network modified\nfrom DenseNet. The experimental results show that the model trained from the\ngenerated mixed training data has good generalization ability, and the proposed\napproach achieves a new state-of-the-art accuracy on Dataset-1 and AOLP, even\nwith a very limited number of original real license plates. In addition, the\naccuracy improvement caused by data generation becomes more significant when\nthe number of labeled images is reduced. Data augmentation also plays a more\nsignificant role when the number of labeled images is increased.\n", "rewritten_text": "Training a high-quality deep learning model often necessitates a substantial amount of annotated data. Given the challenges associated with collecting and annotating a large volume of labeled data, data augmentation and generation techniques are commonly employed during the training of deep neural networks. However, there remains a lack of consensus regarding the optimal amount of labeled data required to achieve satisfactory performance. This study aims to address this issue by focusing on vehicle license plate character recognition as a case study. By leveraging computer graphic scripts and Generative Adversarial Networks, we generated and augmented a diverse set of annotated license plate images featuring realistic colors, fonts, and character compositions using a limited number of manually labeled real license plate images. The combined generated and augmented data were utilized as training data for a license plate recognition network based on DenseNet. Our experimental findings demonstrate that the model trained on the mixed training data exhibits strong generalization capabilities, leading to a new state-of-the-art accuracy on Dataset-1 and AOLP, even when working with a small number of original real license plates. Notably, the impact of data generation on accuracy improvement becomes more pronounced as the number of labeled images decreases, while data augmentation becomes increasingly influential as the number of labeled images rises."}, "2401.03183": {"original_text": "  Defeasibility in causal reasoning implies that the causal relationship\nbetween cause and effect can be strengthened or weakened. Namely, the causal\nstrength between cause and effect should increase or decrease with the\nincorporation of strengthening arguments (supporters) or weakening arguments\n(defeaters), respectively. However, existing works ignore defeasibility in\ncausal reasoning and fail to evaluate existing causal strength metrics in\ndefeasible settings. In this work, we present $\\delta$-CAUSAL, the first\nbenchmark dataset for studying defeasibility in causal reasoning.\n$\\delta$-CAUSAL includes around 11K events spanning ten domains, featuring\ndefeasible causality pairs, i.e., cause-effect pairs accompanied by supporters\nand defeaters. We further show current causal strength metrics fail to reflect\nthe change of causal strength with the incorporation of supporters or defeaters\nin $\\delta$-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation\nwith Attention Rating), a metric that measures causal strength based on\ntoken-level causal relationships. CESAR achieves a significant 69.7% relative\nimprovement over existing metrics, increasing from 47.2% to 80.1% in capturing\nthe causal strength change brought by supporters and defeaters. We further\ndemonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and\n10.7 points behind humans in generating supporters and defeaters, emphasizing\nthe challenge posed by $\\delta$-CAUSAL.\n", "rewritten_text": "Defeasibility in causal reasoning refers to the idea that the strength of the causal relationship between a cause and its effect can be either reinforced or weakened. Specifically, the causal connection between cause and effect should either increase or decrease when additional supporting arguments (supporters) or opposing arguments (defeaters) are introduced. Despite this important aspect of causal reasoning, current research overlooks defeasibility and does not adequately assess the effectiveness of existing metrics for measuring causal strength in situations where defeasibility is present.\n\nIn this study, we introduce $\\delta$-CAUSAL, the first benchmark dataset designed to investigate defeasibility in causal reasoning. $\\delta$-CAUSAL comprises approximately 11,000 events across ten different domains, each featuring pairs of causality that are defeasible, meaning they are accompanied by both supporters and defeaters. Our analysis reveals that existing causal strength metrics do not accurately capture the changes in causal strength that occur when supporters or defeaters are introduced in $\\delta$-CAUSAL.\n\nTo address this gap, we propose CESAR (Causal Embedding aSsociation with Attention Rating), a metric that evaluates causal strength based on token-level causal relationships. CESAR demonstrates a significant 69.7% improvement over existing metrics, increasing its accuracy from 47.2% to 80.1% in capturing the changes in causal strength resulting from the inclusion of supporters and defeaters. Furthermore, we find that even advanced Large Language Models (LLMs) such as GPT-3.5 still fall short by 4.5 and 10.7 points compared to human performance in generating supporters and defeaters, underscoring the challenges presented by $\\delta$-CAUSAL."}, "1310.5767": {"original_text": "  Salient object detection aims to locate objects that capture human attention\nwithin images. Previous approaches often pose this as a problem of image\ncontrast analysis. In this work, we model an image as a hypergraph that\nutilizes a set of hyperedges to capture the contextual properties of image\npixels or regions. As a result, the problem of salient object detection becomes\none of finding salient vertices and hyperedges in the hypergraph. The main\nadvantage of hypergraph modeling is that it takes into account each pixel's (or\nregion's) affinity with its neighborhood as well as its separation from image\nbackground. Furthermore, we propose an alternative approach based on\ncenter-versus-surround contextual contrast analysis, which performs salient\nobject detection by optimizing a cost-sensitive support vector machine (SVM)\nobjective function. Experimental results on four challenging datasets\ndemonstrate the effectiveness of the proposed approaches against the\nstate-of-the-art approaches to salient object detection.\n", "rewritten_text": "Salient object detection aims to identify objects that attract human attention in images. Previous methods have typically approached this task by analyzing image contrast. In this study, we present a novel approach where we represent an image as a hypergraph that uses a set of hyperedges to capture the contextual relationships among image pixels or regions. This formulation transforms the task of salient object detection into identifying salient vertices and hyperedges within the hypergraph. The key advantage of hypergraph modeling is its ability to consider the affinity of each pixel (or region) with its surroundings and its distinction from the image background.\n\nAdditionally, we introduce an alternative method based on center-versus-surround contextual contrast analysis. This approach performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of our proposed approaches compared to the current state-of-the-art methods for salient object detection."}, "2111.02394": {"original_text": "  We propose an accurate and efficient scene text detection framework, termed\nFAST (i.e., faster arbitrarily-shaped text detector). Different from recent\nadvanced text detectors that used complicated post-processing and hand-crafted\nnetwork architectures, resulting in low inference speed, FAST has two new\ndesigns. (1) We design a minimalist kernel representation (only has 1-channel\noutput) to model text with arbitrary shape, as well as a GPU-parallel\npost-processing to efficiently assemble text lines with a negligible time\noverhead. (2) We search the network architecture tailored for text detection,\nleading to more powerful features than most networks that are searched for\nimage classification. Benefiting from these two designs, FAST achieves an\nexcellent trade-off between accuracy and efficiency on several challenging\ndatasets, including Total Text, CTW1500, ICDAR 2015, and MSRA-TD500. For\nexample, FAST-T yields 81.6% F-measure at 152 FPS on Total-Text, outperforming\nthe previous fastest method by 1.7 points and 70 FPS in terms of accuracy and\nspeed. With TensorRT optimization, the inference speed can be further\naccelerated to over 600 FPS. Code and models will be released at\nhttps://github.com/czczup/FAST.\n", "rewritten_text": "We present FAST (Faster Arbitrarily-Shaped Text Detector), a scene text detection framework that is both accurate and efficient. Unlike recent advanced text detectors that rely on complex post-processing and hand-crafted network architectures, resulting in slow inference speeds, FAST introduces two innovative designs. Firstly, we employ a minimalist kernel representation with only a 1-channel output to model text of arbitrary shapes. Additionally, we implement a GPU-parallel post-processing method to efficiently assemble text lines with minimal time overhead. Secondly, we have developed a network architecture specifically tailored for text detection, which generates more powerful features compared to most networks designed for image classification. These two design elements enable FAST to strike an excellent balance between accuracy and efficiency across various challenging datasets, such as Total Text, CTW1500, ICDAR 2015, and MSRA-TD500. For instance, FAST-T achieves an F-measure of 81.6% at 152 FPS on Total-Text, surpassing the previous fastest method by 1.7 points and 70 FPS in terms of accuracy and speed. By leveraging TensorRT optimization, the inference speed can be further boosted to over 600 FPS. The code and models will be made available at https://github.com/czczup/FAST."}, "2403.17856": {"original_text": "  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.\n", "rewritten_text": "Lexical-syntactic flexibility, demonstrated through conversion (or zero-derivation), is a defining feature of English morphology. Conversion involves taking a word of one part of speech and placing it in a context where it is compelled to function as if it were a different part of speech. Despite its widespread impact on the English lexicon, there has been limited research on how well language models capture this type of linguistic generalization. This paper presents the initial investigation into the behavior of large language models concerning conversion. We have developed a task to assess lexical-syntactic flexibility, specifically examining the models' ability to generalize across words in a construction with a non-prototypical part of speech. This task is integrated within a natural language inference framework. We evaluate the performance of five language models: two proprietary models (GPT-3.5 and GPT-4) and three open-source models (Mistral 7B, Falcon 40B, and Llama 2 70B). Our findings indicate that GPT-4 excels in this task, followed by GPT-3.5, while the open-source models also demonstrate competence. Notably, Mistral 7B, with its 7B parameters, shows minimal deviation between its baseline performance on natural language inference and the non-prototypical syntactic category task, comparable to the larger GPT-4."}, "1712.05277": {"original_text": "  Depth cameras allow to set up reliable solutions for people monitoring and\nbehavior understanding, especially when unstable or poor illumination\nconditions make unusable common RGB sensors. Therefore, we propose a complete\nframework for the estimation of the head and shoulder pose based on depth\nimages only. A head detection and localization module is also included, in\norder to develop a complete end-to-end system. The core element of the\nframework is a Convolutional Neural Network, called POSEidon+, that receives as\ninput three types of images and provides the 3D angles of the pose as output.\nMoreover, a Face-from-Depth component based on a Deterministic Conditional GAN\nmodel is able to hallucinate a face from the corresponding depth image. We\nempirically demonstrate that this positively impacts the system performances.\nWe test the proposed framework on two public datasets, namely Biwi Kinect Head\nPose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by\nthe automotive setup. Experimental results show that our method overcomes\nseveral recent state-of-art works based on both intensity and depth input data,\nrunning in real-time at more than 30 frames per second.\n", "rewritten_text": "Depth cameras enable the establishment of reliable solutions for monitoring people and understanding behavior, particularly in situations where unstable or poor lighting conditions render common RGB sensors ineffective. As such, we present a comprehensive framework for estimating head and shoulder poses solely based on depth images. This framework includes a module for head detection and localization to facilitate the development of an end-to-end system. At the core of the framework lies a Convolutional Neural Network named POSEidon+, which takes three types of images as input and outputs the 3D angles of the pose. Additionally, a Face-from-Depth component, utilizing a Deterministic Conditional GAN model, is capable of generating a facial image from the corresponding depth image, with empirical evidence showing a positive impact on system performance. The effectiveness of the proposed framework is evaluated on two public datasets, Biwi Kinect Head Pose and ICT-3DHP, as well as on Pandora, a novel and challenging dataset primarily inspired by automotive settings. Experimental results demonstrate that our method surpasses several recent state-of-the-art approaches utilizing both intensity and depth input data, achieving real-time performance of over 30 frames per second."}, "2404.19287": {"original_text": "  Pretrained vision-language models (VLMs) like CLIP exhibit exceptional\ngeneralization across diverse downstream tasks. While recent studies reveal\ntheir vulnerability to adversarial attacks, research to date has primarily\nfocused on enhancing the robustness of image encoders against image-based\nattacks, with defenses against text-based and multimodal attacks remaining\nlargely unexplored. To this end, this work presents the first comprehensive\nstudy on improving the adversarial robustness of VLMs against attacks targeting\nimage, text, and multimodal inputs. This is achieved by proposing multimodal\ncontrastive adversarial training (MMCoA). Such an approach strengthens the\nrobustness of both image and text encoders by aligning the clean text\nembeddings with adversarial image embeddings, and adversarial text embeddings\nwith clean image embeddings. The robustness of the proposed MMCoA is examined\nagainst existing defense methods over image, text, and multimodal attacks on\nthe CLIP model. Extensive experiments on 15 datasets across two tasks reveal\nthe characteristics of different adversarial defense methods under distinct\ndistribution shifts and dataset complexities across the three attack types.\nThis paves the way for a unified framework of adversarial robustness against\ndifferent modality attacks, opening up new possibilities for securing VLMs\nagainst multimodal attacks. The code is available at\nhttps://github.com/ElleZWQ/MMCoA.git.\n", "rewritten_text": "Pretrained vision-language models (VLMs) such as CLIP demonstrate exceptional generalization capabilities across a wide range of downstream tasks. While recent studies have highlighted their susceptibility to adversarial attacks, existing research has predominantly concentrated on fortifying the robustness of image encoders against image-based attacks, leaving defenses against text-based and multimodal attacks largely unexplored. In light of this, this study introduces the first comprehensive examination of enhancing the adversarial resilience of VLMs against attacks that target image, text, and multimodal inputs. This is accomplished through the proposal of multimodal contrastive adversarial training (MMCoA), a method that bolsters the robustness of both image and text encoders by aligning clean text embeddings with adversarial image embeddings, and adversarial text embeddings with clean image embeddings. The effectiveness of MMCoA in enhancing robustness is evaluated against existing defense strategies across image, text, and multimodal attacks on the CLIP model. Through extensive experiments conducted on 15 datasets spanning two tasks, the study elucidates the performance of various adversarial defense methods under different distribution shifts and dataset complexities across the three types of attacks. This sets the stage for a unified framework for enhancing adversarial resilience against attacks of different modalities, thereby introducing new avenues for safeguarding VLMs against multimodal attacks. The code for MMCoA can be accessed at https://github.com/ElleZWQ/MMCoA.git."}, "2211.09469": {"original_text": "  Generating consecutive descriptions for videos, i.e., Video Captioning,\nrequires taking full advantage of visual representation along with the\ngeneration process. Existing video captioning methods focus on making an\nexploration of spatial-temporal representations and their relationships to\nproduce inferences. However, such methods only exploit the superficial\nassociation contained in the video itself without considering the intrinsic\nvisual commonsense knowledge that existed in a video dataset, which may hinder\ntheir capabilities of knowledge cognitive to reason accurate descriptions. To\naddress this problem, we propose a simple yet effective method, called Visual\nCommonsense-aware Representation Network (VCRN), for video captioning.\nSpecifically, we construct a Video Dictionary, a plug-and-play component,\nobtained by clustering all video features from the total dataset into multiple\nclustered centers without additional annotation. Each center implicitly\nrepresents a visual commonsense concept in the video domain, which is utilized\nin our proposed Visual Concept Selection (VCS) to obtain a video-related\nconcept feature. Next, a Conceptual Integration Generation (CIG) is proposed to\nenhance the caption generation. Extensive experiments on three publicly video\ncaptioning benchmarks: MSVD, MSR-VTT, and VATEX, demonstrate that our method\nreaches state-of-the-art performance, indicating the effectiveness of our\nmethod. In addition, our approach is integrated into the existing method of\nvideo question answering and improves this performance, further showing the\ngeneralization of our method. Source code has been released at\nhttps://github.com/zchoi/VCRN.\n", "rewritten_text": "Generating consecutive descriptions for videos, known as Video Captioning, requires leveraging visual representation in conjunction with the generation process. Current video captioning methods primarily focus on exploring spatial-temporal representations and their interrelationships to draw inferences. However, these methods often rely solely on the surface-level associations present within the video itself, neglecting the intrinsic visual commonsense knowledge inherent in a video dataset. This oversight may limit their ability to generate accurate descriptions based on cognitive reasoning.\n\nTo tackle this challenge, we introduce a straightforward yet powerful approach named the Visual Commonsense-aware Representation Network (VCRN) for video captioning. Specifically, we introduce a Video Dictionary, a versatile component derived by clustering all video features from the entire dataset into multiple clustered centers without requiring additional annotations. Each center implicitly embodies a visual commonsense concept within the video domain, which is harnessed in our proposed Visual Concept Selection (VCS) to extract a video-relevant concept feature. Subsequently, we introduce a Conceptual Integration Generation (CIG) to enhance the caption generation process.\n\nExtensive experiments conducted on three widely recognized video captioning benchmarks\u2014MSVD, MSR-VTT, and VATEX\u2014demonstrate that our method achieves state-of-the-art performance, underscoring its effectiveness. Furthermore, our approach seamlessly integrates with existing video question answering methods, leading to performance enhancements and showcasing the versatility of our approach. The source code is publicly available at https://github.com/zchoi/VCRN."}, "2203.12788": {"original_text": "  A fundamental characteristic of natural language is the high rate at which\nspeakers produce novel expressions. Because of this novelty, a heavy-tail of\nrare events accounts for a significant amount of the total probability mass of\ndistributions in language (Baayen, 2001). Standard language modeling metrics\nsuch as perplexity quantify the performance of language models (LM) in\naggregate. As a result, we have relatively little understanding of whether\nneural LMs accurately estimate the probability of sequences in this heavy-tail\nof rare events. To address this gap, we develop a controlled evaluation scheme\nwhich uses generative models trained on natural data as artificial languages\nfrom which we can exactly compute sequence probabilities. Training LMs on\ngenerations from these artificial languages, we compare the sequence-level\nprobability estimates given by LMs to the true probabilities in the target\nlanguage. Our experiments reveal that LSTM and Transformer language models (i)\nsystematically underestimate the probability of sequences drawn from the target\nlanguage, and (ii) do so more severely for less-probable sequences.\nInvestigating where this probability mass went, (iii) we find that LMs tend to\noverestimate the probability of ill formed (perturbed) sequences. In addition,\nwe find that this underestimation behaviour (iv) is weakened, but not\neliminated by greater amounts of training data, and (v) is exacerbated for\ntarget distributions with lower entropy.\n", "rewritten_text": "A key characteristic of natural language is the frequent production of new expressions by speakers. This novelty leads to a heavy-tail distribution of rare events that contribute significantly to the overall probability mass in language (Baayen, 2001). Common language modeling metrics like perplexity assess the overall performance of language models (LM). However, there is limited understanding of whether neural LMs accurately estimate the probabilities of sequences in this heavy-tail of rare events. To bridge this gap, we introduce a controlled evaluation approach that utilizes generative models trained on natural data to create artificial languages, enabling exact computation of sequence probabilities. By training LMs on sequences generated from these artificial languages, we compare the probability estimates at the sequence level provided by LMs with the actual probabilities in the target language. Our experiments demonstrate that LSTM and Transformer language models (i) consistently underestimate the probabilities of sequences from the target language, particularly for less probable sequences. Upon investigating the discrepancy in probability estimation, (ii) we observe that LMs tend to overestimate the probabilities of malformed (perturbed) sequences. Furthermore, we discover that this underestimation tendency (iii) persists even with increased amounts of training data, and (iv) is more pronounced for target distributions with lower entropy."}, "2207.09884": {"original_text": "  We present a Momentum Re-identification (MoReID) framework that can leverage\na very large number of negative samples in training for general\nre-identification task. The design of this framework is inspired by Momentum\nContrast (MoCo), which uses a dictionary to store current and past batches to\nbuild a large set of encoded samples. As we find it less effective to use past\npositive samples which may be highly inconsistent to the encoded feature\nproperty formed with the current positive samples, MoReID is designed to use\nonly a large number of negative samples stored in the dictionary. However, if\nwe train the model using the widely used Triplet loss that uses only one sample\nto represent a set of positive/negative samples, it is hard to effectively\nleverage the enlarged set of negative samples acquired by the MoReID framework.\nTo maximize the advantage of using the scaled-up negative sample set, we newly\nintroduce Hard-distance Elastic loss (HE loss), which is capable of using more\nthan one hard sample to represent a large number of samples. Our experiments\ndemonstrate that a large number of negative samples provided by MoReID\nframework can be utilized at full capacity only with the HE loss, achieving the\nstate-of-the-art accuracy on three re-ID benchmarks, VeRi-776, Market-1501, and\nVeRi-Wild.\n", "rewritten_text": "We introduce the Momentum Re-identification (MoReID) framework, which effectively utilizes a vast number of negative samples during training for general re-identification tasks. Inspired by Momentum Contrast (MoCo), our framework employs a dictionary to store current and past batches, creating a large set of encoded samples. Unlike using past positive samples, which may not align well with current positive samples, MoReID focuses solely on the large number of negative samples stored in the dictionary. When training with the commonly used Triplet loss, which represents a set of positive/negative samples with only one sample, it becomes challenging to leverage the expanded negative sample set obtained by MoReID effectively.\n\nTo fully exploit the benefits of the increased negative sample set, we introduce the Hard-distance Elastic loss (HE loss), allowing for the use of multiple hard samples to represent numerous samples. Our experiments show that the MoReID framework's abundance of negative samples can be maximized with the HE loss, leading to state-of-the-art accuracy on three re-ID benchmarks: VeRi-776, Market-1501, and VeRi-Wild."}, "2410.12158": {"original_text": "  Foundation models have significantly enhanced 2D task performance, and recent\nworks like Bridge3D have successfully applied these models to improve 3D scene\nunderstanding through knowledge distillation, marking considerable\nadvancements. Nonetheless, challenges such as the misalignment between 2D and\n3D representations and the persistent long-tail distribution in 3D datasets\nstill restrict the effectiveness of knowledge distillation from 2D to 3D using\nfoundation models. To tackle these issues, we introduce a novel SAM-guided\ntokenization method that seamlessly aligns 3D transformer structures with\nregion-level knowledge distillation, replacing the traditional KNN-based\ntokenization techniques. Additionally, we implement a group-balanced\nre-weighting strategy to effectively address the long-tail problem in knowledge\ndistillation. Furthermore, inspired by the recent success of masked feature\nprediction, our framework incorporates a two-stage masked token prediction\nprocess in which the student model predicts both the global embeddings and the\ntoken-wise local embeddings derived from the teacher models trained in the\nfirst stage. Our methodology has been validated across multiple datasets,\nincluding SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and\nsemantic segmentation. The results demonstrate significant improvements over\ncurrent State-of-the-art self-supervised methods, establishing new benchmarks\nin this field.\n", "rewritten_text": "Foundation models have significantly improved 2D task performance. Recent works, such as Bridge3D, have successfully utilized these models to enhance 3D scene understanding through knowledge distillation, leading to notable advancements. However, challenges persist, such as the discrepancy between 2D and 3D representations and the prevalent long-tail distribution in 3D datasets, which hinder the efficacy of knowledge distillation from 2D to 3D using foundation models.\n\nTo address these challenges, we propose a novel SAM-guided tokenization method that aligns 3D transformer structures with region-level knowledge distillation, replacing traditional KNN-based tokenization techniques. Additionally, we introduce a group-balanced re-weighting strategy to effectively tackle the long-tail problem in knowledge distillation. Drawing inspiration from the success of masked feature prediction, our framework incorporates a two-stage masked token prediction process. In this process, the student model predicts both the global embeddings and the token-wise local embeddings derived from the teacher models trained in the initial stage.\n\nOur methodology has been validated across various datasets, including SUN RGB-D, ScanNet, and S3DIS, for tasks such as 3D object detection and semantic segmentation. The results demonstrate significant improvements over current state-of-the-art self-supervised methods, setting new benchmarks in this field."}, "2307.09892": {"original_text": "  We propose 3Deformer, a general-purpose framework for interactive 3D shape\nediting. Given a source 3D mesh with semantic materials, and a user-specified\nsemantic image, 3Deformer can accurately edit the source mesh following the\nshape guidance of the semantic image, while preserving the source topology as\nrigid as possible. Recent studies of 3D shape editing mostly focus on learning\nneural networks to predict 3D shapes, which requires high-cost 3D training\ndatasets and is limited to handling objects involved in the datasets. Unlike\nthese studies, our 3Deformer is a non-training and common framework, which only\nrequires supervision of readily-available semantic images, and is compatible\nwith editing various objects unlimited by datasets. In 3Deformer, the source\nmesh is deformed utilizing the differentiable renderer technique, according to\nthe correspondences between semantic images and mesh materials. However,\nguiding complex 3D shapes with a simple 2D image incurs extra challenges, that\nis, the deform accuracy, surface smoothness, geometric rigidity, and global\nsynchronization of the edited mesh should be guaranteed. To address these\nchallenges, we propose a hierarchical optimization architecture to balance the\nglobal and local shape features, and propose further various strategies and\nlosses to improve properties of accuracy, smoothness, rigidity, and so on.\nExtensive experiments show that our 3Deformer is able to produce impressive\nresults and reaches the state-of-the-art level.\n", "rewritten_text": "We introduce 3Deformer, a versatile framework designed for interactive 3D shape editing. By utilizing a source 3D mesh with semantic materials and a user-defined semantic image, 3Deformer can precisely modify the source mesh based on the shape guidance provided by the semantic image, while striving to maintain the original topology as rigid as possible.\n\nRecent research on 3D shape editing has predominantly focused on training neural networks to predict 3D shapes, necessitating expensive 3D training datasets and being limited to objects present in those datasets. In contrast, our 3Deformer framework is non-training and universal, requiring only supervision from easily accessible semantic images, and is capable of editing a wide range of objects without being constrained by specific datasets.\n\nWithin 3Deformer, the deformation of the source mesh is achieved through the use of differentiable renderer techniques, aligning with the correspondences between semantic images and mesh materials. However, guiding intricate 3D shapes with a simple 2D image presents additional challenges, such as ensuring deformation accuracy, surface smoothness, geometric rigidity, and overall synchronization of the edited mesh.\n\nTo tackle these challenges, we propose a hierarchical optimization architecture that balances global and local shape features, along with various strategies and loss functions to enhance properties like accuracy, smoothness, rigidity, and more. Extensive experiments demonstrate that our 3Deformer framework consistently delivers impressive results, reaching a state-of-the-art level in 3D shape editing."}, "2105.03571": {"original_text": "  Dialogue state tracking (DST) plays a key role in task-oriented dialogue\nsystems to monitor the user's goal. In general, there are two strategies to\ntrack a dialogue state: predicting it from scratch and updating it from\nprevious state. The scratch-based strategy obtains each slot value by inquiring\nall the dialogue history, and the previous-based strategy relies on the current\nturn dialogue to update the previous dialogue state. However, it is hard for\nthe scratch-based strategy to correctly track short-dependency dialogue state\nbecause of noise; meanwhile, the previous-based strategy is not very useful for\nlong-dependency dialogue state tracking. Obviously, it plays different roles\nfor the context information of different granularity to track different kinds\nof dialogue states. Thus, in this paper, we will study and discuss how the\ncontext information of different granularity affects dialogue state tracking.\nFirst, we explore how greatly different granularities affect dialogue state\ntracking. Then, we further discuss how to combine multiple granularities for\ndialogue state tracking. Finally, we apply the findings about context\ngranularity to few-shot learning scenario. Besides, we have publicly released\nall codes.\n", "rewritten_text": "Dialogue state tracking (DST) is crucial in task-oriented dialogue systems for monitoring the user's goal. Generally, there are two strategies for tracking dialogue state: predicting it from scratch and updating it from the previous state. The scratch-based strategy involves obtaining each slot value by examining the entire dialogue history, while the previous-based strategy relies on the current turn dialogue to update the previous dialogue state. However, the scratch-based strategy faces challenges in accurately tracking short-dependency dialogue state due to noise, whereas the previous-based strategy may not be as effective for long-dependency dialogue state tracking. It is evident that different roles are played by context information of varying granularity in tracking different types of dialogue states. Therefore, this paper aims to investigate and discuss the impact of context information of different granularity on dialogue state tracking. Initially, we explore the significant influence of different granularities on dialogue state tracking. Subsequently, we delve into the integration of multiple granularities for dialogue state tracking. Lastly, we apply the insights on context granularity to a few-shot learning scenario. Additionally, we have made all codes publicly available."}, "2405.10474": {"original_text": "  Over the last decade, a wide range of training and deployment strategies for\nLarge Language Models (LLMs) have emerged. Among these, the prompting paradigms\nof Auto-regressive LLMs (AR-LLMs) have catalyzed a significant surge in\nArtificial Intelligence (AI). This paper aims to emphasize the significance of\nutilizing free-form modalities (forms of input and output) and verbal free-form\ncontexts as user-directed channels (methods for transforming modalities) for\ndownstream deployment. Specifically, we analyze the structure of modalities\nwithin both two types of LLMs and six task-specific channels during deployment.\nFrom the perspective of users, our analysis introduces and applies the\nanalytical metrics of task customizability, transparency, and complexity to\ngauge their usability, highlighting the superior nature of AR-LLMs' prompting\nparadigms. Moreover, we examine the stimulation of diverse cognitive behaviors\nin LLMs through the adoption of free-form text and verbal contexts, mirroring\nhuman linguistic expressions of such behaviors. We then detail four common\ncognitive behaviors to underscore how AR-LLMs' prompting successfully imitate\nhuman-like behaviors using this free-form modality and channel. Lastly, the\npotential for improving LLM deployment, both as autonomous agents and within\nmulti-agent systems, is identified via cognitive behavior concepts and\nprinciples.\n", "rewritten_text": "In the past decade, a variety of training and deployment strategies have emerged for Large Language Models (LLMs). Among these, the prompting paradigms of Auto-regressive LLMs (AR-LLMs) have sparked a significant increase in Artificial Intelligence (AI). This paper aims to highlight the importance of utilizing free-form modalities (input and output forms) and verbal free-form contexts as user-directed channels for downstream deployment. Specifically, we examine the modalities' structure in both types of LLMs and six task-specific channels during deployment. Our analysis introduces and applies analytical metrics such as task customizability, transparency, and complexity from the users' perspective to assess their usability, emphasizing the superior nature of AR-LLMs' prompting paradigms. Furthermore, we explore how diverse cognitive behaviors in LLMs can be stimulated by incorporating free-form text and verbal contexts, reflecting human linguistic expressions of such behaviors. We then outline four common cognitive behaviors to demonstrate how AR-LLMs' prompting successfully mimics human-like behaviors using this free-form modality and channel. Finally, we identify the potential for enhancing LLM deployment, both as autonomous agents and within multi-agent systems, through cognitive behavior concepts and principles."}, "2212.04214": {"original_text": "  In a citation graph, adjacent paper nodes share related scientific terms and\ntopics. The graph thus conveys unique structure information of document-level\nrelatedness that can be utilized in the paper summarization task, for exploring\nbeyond the intra-document information. In this work, we focus on leveraging\ncitation graphs to improve scientific paper extractive summarization under\ndifferent settings. We first propose a Multi-granularity Unsupervised\nSummarization model (MUS) as a simple and low-cost solution to the task. MUS\nfinetunes a pre-trained encoder model on the citation graph by link prediction\ntasks. Then, the abstract sentences are extracted from the corresponding paper\nconsidering multi-granularity information. Preliminary results demonstrate that\ncitation graph is helpful even in a simple unsupervised framework. Motivated by\nthis, we next propose a Graph-based Supervised Summarization model (GSS) to\nachieve more accurate results on the task when large-scale labeled data are\navailable. Apart from employing the link prediction as an auxiliary task, GSS\nintroduces a gated sentence encoder and a graph information fusion module to\ntake advantage of the graph information to polish the sentence representation.\nExperiments on a public benchmark dataset show that MUS and GSS bring\nsubstantial improvements over the prior state-of-the-art model.\n", "rewritten_text": "In a citation graph, adjacent paper nodes share related scientific terms and topics, conveying unique structural information on document-level relatedness that can be utilized in the task of paper summarization to explore beyond intra-document information. This work focuses on leveraging citation graphs to enhance scientific paper extractive summarization under various settings. Initially, a Multi-granularity Unsupervised Summarization model (MUS) is proposed as a straightforward and cost-effective solution to the task. MUS fine-tunes a pre-trained encoder model on the citation graph through link prediction tasks. Subsequently, abstract sentences are extracted from the corresponding paper, considering multi-granularity information. Preliminary results indicate that the citation graph is beneficial even within a simple unsupervised framework. Building on this, a Graph-based Supervised Summarization model (GSS) is introduced to achieve more precise results on the task when ample labeled data is available. In addition to utilizing link prediction as an auxiliary task, GSS incorporates a gated sentence encoder and a graph information fusion module to leverage the graph information for refining the sentence representation. Experiments conducted on a public benchmark dataset demonstrate that both MUS and GSS yield significant improvements over the previous state-of-the-art model."}, "2310.14561": {"original_text": "  Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by\nwell-designed perturbations. This could lead to disastrous results on critical\napplications such as self-driving cars, surveillance security, and medical\ndiagnosis. At present, adversarial training is one of the most effective\ndefenses against adversarial examples. However, traditional adversarial\ntraining makes it difficult to achieve a good trade-off between clean accuracy\nand robustness since spurious features are still learned by DNNs. The intrinsic\nreason is that traditional adversarial training makes it difficult to fully\nlearn core features from adversarial examples when adversarial noise and clean\nexamples cannot be disentangled. In this paper, we disentangle the adversarial\nexamples into natural and perturbed patterns by bit-plane slicing. We assume\nthe higher bit-planes represent natural patterns and the lower bit-planes\nrepresent perturbed patterns, respectively. We propose a Feature-Focusing\nAdversarial Training (F$^2$AT), which differs from previous work in that it\nenforces the model to focus on the core features from natural patterns and\nreduce the impact of spurious features from perturbed patterns. The\nexperimental results demonstrated that F$^2$AT outperforms state-of-the-art\nmethods in clean accuracy and adversarial robustness.\n", "rewritten_text": "Deep neural networks (DNNs) are susceptible to adversarial examples created through well-designed perturbations, posing significant risks in critical applications like self-driving cars, surveillance security, and medical diagnosis. Currently, adversarial training stands out as one of the most effective defenses against such examples. However, traditional adversarial training struggles to strike a balance between clean accuracy and robustness due to DNNs still learning spurious features. This challenge arises from the difficulty in fully capturing core features from adversarial examples when distinguishing between adversarial noise and clean examples is complex.\n\nIn this study, we address this issue by disentangling adversarial examples into natural and perturbed patterns using bit-plane slicing. Specifically, we assign higher bit-planes to natural patterns and lower bit-planes to perturbed patterns. Introducing Feature-Focusing Adversarial Training (F$^2$AT), our approach differs from prior methods by directing the model to concentrate on core features from natural patterns while minimizing the influence of spurious features from perturbed patterns. Experimental results showcase that F$^2$AT surpasses existing techniques in both clean accuracy and adversarial robustness."}, "2308.04782": {"original_text": "  Point cloud registration is a task to estimate the rigid transformation\nbetween two unaligned scans, which plays an important role in many computer\nvision applications. Previous learning-based works commonly focus on supervised\nregistration, which have limitations in practice. Recently, with the advance of\ninexpensive RGB-D sensors, several learning-based works utilize RGB-D data to\nachieve unsupervised registration. However, most of existing unsupervised\nmethods follow a cascaded design or fuse RGB-D data in a unidirectional manner,\nwhich do not fully exploit the complementary information in the RGB-D data. To\nleverage the complementary information more effectively, we propose a network\nimplementing multi-scale bidirectional fusion between RGB images and point\nclouds generated from depth images. By bidirectionally fusing visual and\ngeometric features in multi-scales, more distinctive deep features for\ncorrespondence estimation can be obtained, making our registration more\naccurate. Extensive experiments on ScanNet and 3DMatch demonstrate that our\nmethod achieves new state-of-the-art performance. Code will be released at\nhttps://github.com/phdymz/PointMBF\n", "rewritten_text": "Point cloud registration involves estimating the rigid transformation between two unaligned scans, a crucial task in various computer vision applications. While previous learning-based approaches have typically focused on supervised registration, they are limited in practical applications. Recently, leveraging the advancements in affordable RGB-D sensors, some learning-based methods have utilized RGB-D data for unsupervised registration. However, many existing unsupervised techniques adopt a cascaded design or unidirectional fusion of RGB-D data, failing to fully exploit the complementary information within the RGB-D data.\n\nTo address this limitation and enhance the utilization of complementary information, we propose a network that implements multi-scale bidirectional fusion between RGB images and point clouds derived from depth images. By integrating visual and geometric features bidirectionally across multiple scales, our approach generates more distinctive deep features for accurate correspondence estimation, thereby improving the registration accuracy. Extensive experiments conducted on ScanNet and 3DMatch datasets demonstrate that our method achieves state-of-the-art performance. The code for our approach will be made available at https://github.com/phdymz/PointMBF."}, "2404.02573": {"original_text": "  Knowledge distillation (KD) is a promising yet challenging model compression\ntechnique that transfers rich learning representations from a well-performing\nbut cumbersome teacher model to a compact student model. Previous methods for\nimage super-resolution (SR) mostly compare the feature maps directly or after\nstandardizing the dimensions with basic algebraic operations (e.g. average,\ndot-product). However, the intrinsic semantic differences among feature maps\nare overlooked, which are caused by the disparate expressive capacity between\nthe networks. This work presents MiPKD, a multi-granularity mixture of prior KD\nframework, to facilitate efficient SR model through the feature mixture in a\nunified latent space and stochastic network block mixture. Extensive\nexperiments demonstrate the effectiveness of the proposed MiPKD method.\n", "rewritten_text": "Knowledge distillation (KD) is a promising yet challenging technique for compressing models, wherein rich learning representations are transferred from a high-performing but bulky teacher model to a more compact student model. Previous methods for image super-resolution (SR) have primarily focused on comparing feature maps directly or after standardizing dimensions using basic algebraic operations (such as averaging or dot-product). However, these methods often overlook the intrinsic semantic differences among feature maps, which stem from variations in expressive capacity between networks. This study introduces MiPKD, a multi-granularity mixture of prior KD framework, designed to enhance the efficiency of SR models by blending features in a unified latent space and employing stochastic network block mixture. Extensive experiments validate the effectiveness of the proposed MiPKD method."}, "2307.08198": {"original_text": "  We introduce the notion of point affiliation into feature upsampling. By\nabstracting a feature map into non-overlapped semantic clusters formed by\npoints of identical semantic meaning, feature upsampling can be viewed as point\naffiliation -- designating a semantic cluster for each upsampled point. In the\nframework of kernel-based dynamic upsampling, we show that an upsampled point\ncan resort to its low-res decoder neighbors and high-res encoder point to\nreason the affiliation, conditioned on the mutual similarity between them. We\ntherefore present a generic formulation for generating similarity-aware\nupsampling kernels and prove that such kernels encourage not only semantic\nsmoothness but also boundary sharpness. This formulation constitutes a novel,\nlightweight, and universal upsampling solution, Similarity-Aware Point\nAffiliation (SAPA). We show its working mechanism via our preliminary designs\nwith window-shape kernel. After probing the limitations of the designs on\nobject detection, we reveal additional insights for upsampling, leading to SAPA\nwith the dynamic kernel shape. Extensive experiments demonstrate that SAPA\noutperforms prior upsamplers and invites consistent performance improvements on\na number of dense prediction tasks, including semantic segmentation, object\ndetection, instance segmentation, panoptic segmentation, image matting, and\ndepth estimation. Code is made available at: https://github.com/tiny-smart/sapa\n", "rewritten_text": "We introduce the concept of point affiliation in the context of feature upsampling. By organizing a feature map into distinct semantic clusters based on points with similar semantic meanings, feature upsampling can be seen as assigning each upsampled point to a specific semantic cluster. Within the framework of kernel-based dynamic upsampling, we demonstrate that an upsampled point can leverage information from neighboring low-resolution decoder points and high-resolution encoder points to determine its affiliation, taking into account their mutual similarity. We propose a general formulation for creating similarity-aware upsampling kernels, showing that these kernels not only promote semantic coherence but also enhance boundary sharpness. This formulation presents a new, efficient, and versatile upsampling approach called Similarity-Aware Point Affiliation (SAPA). We illustrate its operation through initial designs using window-shaped kernels. By identifying the limitations of these designs in object detection, we introduce further insights into upsampling, culminating in SAPA with dynamic kernel shapes. Extensive experiments demonstrate that SAPA surpasses existing upsampling methods and consistently enhances performance across various dense prediction tasks, such as semantic segmentation, object detection, instance segmentation, panoptic segmentation, image matting, and depth estimation. The code for SAPA is accessible at: https://github.com/tiny-smart/sapa."}, "2410.16646": {"original_text": "  Diffusion models excel at creating visually impressive images but often\nstruggle to generate images with a specified topology. The Betti number, which\nrepresents the number of structures in an image, is a fundamental measure in\ntopology. Yet, diffusion models fail to satisfy even this basic constraint.\nThis limitation restricts their utility in applications requiring exact\ncontrol, like robotics and environmental modeling. To address this, we propose\nTopoDiffusionNet (TDN), a novel approach that enforces diffusion models to\nmaintain the desired topology. We leverage tools from topological data\nanalysis, particularly persistent homology, to extract the topological\nstructures within an image. We then design a topology-based objective function\nto guide the denoising process, preserving intended structures while\nsuppressing noisy ones. Our experiments across four datasets demonstrate\nsignificant improvements in topological accuracy. TDN is the first to integrate\ntopology with diffusion models, opening new avenues of research in this area.\n", "rewritten_text": "Diffusion models are known for their ability to create visually stunning images, but they often struggle to produce images with specific topologies. The Betti number, which indicates the number of structures in an image, is a crucial measure in topology. However, diffusion models frequently fail to meet even this basic requirement, limiting their usefulness in applications that demand precise control, such as robotics and environmental modeling. \n\nTo address this issue, we introduce TopoDiffusionNet (TDN), a novel approach that enforces diffusion models to adhere to the desired topology. By utilizing tools from topological data analysis, particularly persistent homology, we extract the topological structures present in an image. Subsequently, we develop an objective function based on topology to guide the denoising process, preserving the intended structures while suppressing noise. Our experiments across four datasets demonstrate significant enhancements in topological accuracy. TDN represents the first integration of topology with diffusion models, paving the way for new research opportunities in this field."}, "1603.01684": {"original_text": "  In this paper, we propose an improved mechanism for saliency detection.\nFirstly,based on a neoteric background prior selecting four corners of an image\nas background,we use color and spatial contrast with each superpixel to obtain\na salinecy map(CBP). Inspired by reverse-measurement methods to improve the\naccuracy of measurement in Engineering,we employ the Objectness labels as\nforeground prior based on part of information of CBP to construct a\nmap(OFP).Further,an original energy function is applied to optimize both of\nthem respectively and a single-layer saliency map(SLP)is formed by merging the\nabove twos.Finally,to deal with the scale problem,we obtain our multi-layer\nmap(MLP) by presenting an integration algorithm to take advantage of multiple\nsaliency maps. Quantitative and qualitative experiments on three datasets\ndemonstrate that our method performs favorably against the state-of-the-art\nalgorithm.\n", "rewritten_text": "In this paper, we propose an improved mechanism for saliency detection. Firstly, we utilize a novel background prior to select four corners of an image as the background. We then use color and spatial contrast with each superpixel to generate a saliency map (CBP). Drawing inspiration from reverse-measurement methods used in Engineering to enhance measurement accuracy, we incorporate Objectness labels as a foreground prior based on a portion of the information from CBP to create a map (OFP). Subsequently, we apply an original energy function to optimize both CBP and OFP individually, resulting in a single-layer saliency map (SLP) formed by merging the two. To address the scale issue, we develop a multi-layer map (MLP) by implementing an integration algorithm that leverages multiple saliency maps. Quantitative and qualitative experiments conducted on three datasets demonstrate that our method outperforms the state-of-the-art algorithm."}, "2403.16998": {"original_text": "  Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly.\n", "rewritten_text": "Large Language Models (LLMs) have enabled recent approaches to achieve outstanding performance on long-video understanding benchmarks. We investigate how the extensive world knowledge and strong reasoning skills of underlying LLMs influence this remarkable performance. Surprisingly, we find that LLM-based approaches can achieve high accuracy on long-video tasks even with limited or no video-specific information. Building on this discovery, we explore the integration of video-specific information into an LLM-based framework. We employ off-the-shelf vision tools to extract three object-centric information modalities from videos and then use natural language as a medium to fuse this information. Our resulting Multimodal Video Understanding (MVU) framework showcases state-of-the-art performance across various video understanding benchmarks. Its strong performance on tasks in the robotics domain further establishes its broad applicability. We will release our code publicly."}, "2006.01372": {"original_text": "  In general, the labels used in sequence labeling consist of different types\nof elements. For example, IOB-format entity labels, such as B-Person and\nI-Person, can be decomposed into span (B and I) and type information (Person).\nHowever, while most sequence labeling models do not consider such label\ncomponents, the shared components across labels, such as Person, can be\nbeneficial for label prediction. In this work, we propose to integrate label\ncomponent information as embeddings into models. Through experiments on English\nand Japanese fine-grained named entity recognition, we demonstrate that the\nproposed method improves performance, especially for instances with\nlow-frequency labels.\n", "rewritten_text": "In sequence labeling, the labels typically consist of various elements. For instance, entity labels in IOB format, like B-Person and I-Person, can be broken down into span (B and I) and type information (Person). While many sequence labeling models do not take into account these label components, leveraging shared components like Person can enhance label prediction. Our approach involves integrating label component information as embeddings into models. Through experiments on English and Japanese fine-grained named entity recognition, we show that this method enhances performance, particularly for labels with low frequencies."}, "2410.09140": {"original_text": "  The remarkable development of text-to-image generation models has raised\nnotable security concerns, such as the infringement of portrait rights and the\ngeneration of inappropriate content. Concept erasure has been proposed to\nremove the model's knowledge about protected and inappropriate concepts.\nAlthough many methods have tried to balance the efficacy (erasing target\nconcepts) and specificity (retaining irrelevant concepts), they can still\ngenerate abundant erasure concepts under the steering of semantically related\ninputs. In this work, we propose RealEra to address this \"concept residue\"\nissue. Specifically, we first introduce the mechanism of neighbor-concept\nmining, digging out the associated concepts by adding random perturbation into\nthe embedding of erasure concept, thus expanding the erasing range and\neliminating the generations even through associated concept inputs.\nFurthermore, to mitigate the negative impact on the generation of irrelevant\nconcepts caused by the expansion of erasure scope, RealEra preserves the\nspecificity through the beyond-concept regularization. This makes irrelevant\nconcepts maintain their corresponding spatial position, thereby preserving\ntheir normal generation performance. We also employ the closed-form solution to\noptimize weights of U-Net for the cross-attention alignment, as well as the\nprediction noise alignment with the LoRA module. Extensive experiments on\nmultiple benchmarks demonstrate that RealEra outperforms previous concept\nerasing methods in terms of superior erasing efficacy, specificity, and\ngenerality. More details are available on our project page\nhttps://realerasing.github.io/RealEra/ .\n", "rewritten_text": "The significant advancement of text-to-image generation models has brought about notable security concerns, including potential violations of portrait rights and the creation of inappropriate content. To address these issues, the concept of erasure has been introduced to eliminate the model's knowledge of protected and inappropriate concepts. While various methods have attempted to balance effectiveness (erasing target concepts) and specificity (retaining irrelevant concepts), they often produce numerous erasure concepts influenced by semantically related inputs.\n\nIn this study, we present RealEra as a solution to the problem of \"concept residue.\" Our approach involves the introduction of neighbor-concept mining, which involves uncovering associated concepts by introducing random perturbations into the embedding of the erasure concept. This process expands the erasure range and prevents the generation of content influenced by associated concepts. Additionally, to counteract the negative impact on generating irrelevant concepts due to the expanded erasure scope, RealEra maintains specificity through beyond-concept regularization. This approach ensures that irrelevant concepts retain their spatial positions, thereby preserving their normal generation performance.\n\nRealEra also utilizes a closed-form solution to optimize the weights of U-Net for cross-attention alignment and incorporates prediction noise alignment with the LoRA module. Extensive experiments conducted on multiple benchmarks demonstrate that RealEra surpasses previous concept erasing methods in terms of erasing efficacy, specificity, and generality. For more information, please visit our project page at https://realerasing.github.io/RealEra/."}, "2210.02843": {"original_text": "  Focusing on the issue of how to effectively capture and utilize\ncross-modality information in RGB-D salient object detection (SOD) task, we\npresent a convolutional neural network (CNN) model, named CIR-Net, based on the\nnovel cross-modality interaction and refinement. For the cross-modality\ninteraction, 1) a progressive attention guided integration unit is proposed to\nsufficiently integrate RGB-D feature representations in the encoder stage, and\n2) a convergence aggregation structure is proposed, which flows the RGB and\ndepth decoding features into the corresponding RGB-D decoding streams via an\nimportance gated fusion unit in the decoder stage. For the cross-modality\nrefinement, we insert a refinement middleware structure between the encoder and\nthe decoder, in which the RGB, depth, and RGB-D encoder features are further\nrefined by successively using a self-modality attention refinement unit and a\ncross-modality weighting refinement unit. At last, with the gradually refined\nfeatures, we predict the saliency map in the decoder stage. Extensive\nexperiments on six popular RGB-D SOD benchmarks demonstrate that our network\noutperforms the state-of-the-art saliency detectors both qualitatively and\nquantitatively.\n", "rewritten_text": "In addressing the effective capture and utilization of cross-modality information in RGB-D salient object detection (SOD), we introduce a convolutional neural network (CNN) model called CIR-Net. This model is based on innovative cross-modality interaction and refinement techniques. \n\nTo enhance cross-modality interaction, we propose two key components: \n1. A progressive attention-guided integration unit in the encoder stage to effectively combine RGB-D feature representations.\n2. A convergence aggregation structure in the decoder stage that channels RGB and depth decoding features into their respective RGB-D decoding streams through an importance-gated fusion unit.\n\nFor cross-modality refinement, we introduce a refinement middleware structure positioned between the encoder and decoder. This structure refines RGB, depth, and RGB-D encoder features by sequentially employing a self-modality attention refinement unit and a cross-modality weighting refinement unit. Ultimately, the refined features are used to predict the saliency map in the decoder stage.\n\nExtensive experiments conducted on six prominent RGB-D SOD benchmarks demonstrate that our network surpasses state-of-the-art saliency detectors in both qualitative and quantitative performance metrics."}, "1703.01515": {"original_text": "  Temporal action localization is an important yet challenging problem. Given a\nlong, untrimmed video consisting of multiple action instances and complex\nbackground contents, we need not only to recognize their action categories, but\nalso to localize the start time and end time of each instance. Many\nstate-of-the-art systems use segment-level classifiers to select and rank\nproposal segments of pre-determined boundaries. However, a desirable model\nshould move beyond segment-level and make dense predictions at a fine\ngranularity in time to determine precise temporal boundaries. To this end, we\ndesign a novel Convolutional-De-Convolutional (CDC) network that places CDC\nfilters on top of 3D ConvNets, which have been shown to be effective for\nabstracting action semantics but reduce the temporal length of the input data.\nThe proposed CDC filter performs the required temporal upsampling and spatial\ndownsampling operations simultaneously to predict actions at the frame-level\ngranularity. It is unique in jointly modeling action semantics in space-time\nand fine-grained temporal dynamics. We train the CDC network in an end-to-end\nmanner efficiently. Our model not only achieves superior performance in\ndetecting actions in every frame, but also significantly boosts the precision\nof localizing temporal boundaries. Finally, the CDC network demonstrates a very\nhigh efficiency with the ability to process 500 frames per second on a single\nGPU server. We will update the camera-ready version and publish the source\ncodes online soon.\n", "rewritten_text": "Temporal action localization presents a significant challenge, as it involves identifying action categories and pinpointing the start and end times of each action instance within a long, untrimmed video containing various action instances and complex background elements. While many cutting-edge systems rely on segment-level classifiers to identify and rank proposal segments with predetermined boundaries, an ideal model should go beyond segment-level analysis and provide dense predictions at a fine temporal granularity to accurately determine temporal boundaries. To address this, we introduce a novel Convolutional-De-Convolutional (CDC) network that integrates CDC filters with 3D ConvNets, known for effectively capturing action semantics while reducing the temporal length of input data.\n\nThe CDC filter in our proposed model conducts temporal upsampling and spatial downsampling operations simultaneously, enabling frame-level action prediction. This approach is distinctive in its ability to jointly model action semantics in space-time and fine-grained temporal dynamics. The CDC network is trained end-to-end efficiently, achieving superior performance in action detection at every frame and significantly enhancing the precision of temporal boundary localization. Notably, the CDC network exhibits high efficiency, capable of processing 500 frames per second on a single GPU server. We plan to update the camera-ready version and release the source codes online soon."}, "2305.0541": {"original_text": "  The medical conversational question answering (CQA) system aims at providing\na series of professional medical services to improve the efficiency of medical\ncare. Despite the success of large language models (LLMs) in complex reasoning\ntasks in various fields, such as mathematics, logic, and commonsense QA, they\nstill need to improve with the increased complexity and specialization of the\nmedical field. This is because medical CQA tasks require not only strong\nmedical reasoning, but also the ability to think broadly and deeply. In this\npaper, to address these challenges in medical CQA tasks that need to be\nconsidered and understood in many aspects, we propose the Holistically Thought\n(HoT) method, which is designed to guide the LLMs to perform the diffused and\nfocused thinking for generating high-quality medical responses. The proposed\nHoT method has been evaluated through automated and manual assessments in three\ndifferent medical CQA datasets containing the English and Chinese languages.\nThe extensive experimental results show that our method can produce more\ncorrectness, professional, and considerate answers than several\nstate-of-the-art (SOTA) methods, manifesting its effectiveness. Our code in\nhttps://github.com/WENGSYX/HoT.\n", "rewritten_text": "The medical conversational question answering (CQA) system aims to provide a range of professional medical services to enhance the efficiency of medical care. While large language models (LLMs) have shown success in complex reasoning tasks across various fields like mathematics, logic, and commonsense QA, they still require improvement to meet the growing complexity and specialization of the medical domain. This is because medical CQA tasks demand not only strong medical reasoning but also the capacity for broad and deep thinking.\n\nIn this paper, we introduce the Holistically Thought (HoT) method to tackle the challenges in medical CQA tasks, which necessitate a comprehensive understanding from multiple perspectives. The HoT method is designed to guide LLMs in conducting both diffuse and focused thinking to generate high-quality medical responses. We evaluated the proposed HoT method through automated and manual assessments on three distinct medical CQA datasets in English and Chinese languages.\n\nThe extensive experimental results demonstrate that our method can deliver more accurate, professional, and thoughtful answers compared to several state-of-the-art (SOTA) methods, showcasing its effectiveness. Our code can be found at https://github.com/WENGSYX/HoT."}, "2110.02204": {"original_text": "  Contextualised word embeddings generated from Neural Language Models (NLMs),\nsuch as BERT, represent a word with a vector that considers the semantics of\nthe target word as well its context. On the other hand, static word embeddings\nsuch as GloVe represent words by relatively low-dimensional, memory- and\ncompute-efficient vectors but are not sensitive to the different senses of the\nword. We propose Context Derived Embeddings of Senses (CDES), a method that\nextracts sense related information from contextualised embeddings and injects\nit into static embeddings to create sense-specific static embeddings.\nExperimental results on multiple benchmarks for word sense disambiguation and\nsense discrimination tasks show that CDES can accurately learn sense-specific\nstatic embeddings reporting comparable performance to the current\nstate-of-the-art sense embeddings.\n", "rewritten_text": "Contextualized word embeddings, generated from Neural Language Models (NLMs) like BERT, represent a word using a vector that takes into account both the semantics of the target word and its context. In contrast, static word embeddings such as GloVe represent words using low-dimensional, memory- and compute-efficient vectors but do not capture the various senses of the word. Our proposed method, Context Derived Embeddings of Senses (CDES), extracts sense-related information from contextualized embeddings and integrates it into static embeddings to produce sense-specific static embeddings. Experimental results across multiple benchmarks for word sense disambiguation and sense discrimination tasks demonstrate that CDES effectively learns sense-specific static embeddings, achieving performance comparable to the current state-of-the-art sense embeddings."}, "1804.00247": {"original_text": "  This article describes our experiments in neural machine translation using\nthe recent Tensor2Tensor framework and the Transformer sequence-to-sequence\nmodel (Vaswani et al., 2017). We examine some of the critical parameters that\naffect the final translation quality, memory usage, training stability and\ntraining time, concluding each experiment with a set of recommendations for\nfellow researchers. In addition to confirming the general mantra \"more data and\nlarger models\", we address scaling to multiple GPUs and provide practical tips\nfor improved training regarding batch size, learning rate, warmup steps,\nmaximum sentence length and checkpoint averaging. We hope that our observations\nwill allow others to get better results given their particular hardware and\ndata constraints.\n", "rewritten_text": "This article presents our experiments in neural machine translation utilizing the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We analyze key parameters that impact the translation quality, memory usage, training stability, and duration, concluding each experiment with a series of recommendations for fellow researchers. Alongside affirming the common principle of \"more data and larger models,\" we delve into scaling for multiple GPUs and offer practical advice for enhanced training, covering aspects such as batch size, learning rate, warmup steps, maximum sentence length, and checkpoint averaging. We aim for our insights to assist others in achieving improved outcomes based on their specific hardware and data limitations."}, "2402.08874": {"original_text": "  While large language models (LLMs) excel at understanding and generating\nplain text, they are not tailored to handle hierarchical text structures or\ndirectly predict task-specific properties such as text rating. In fact,\nselectively and repeatedly grasping the hierarchical structure of large-scale\ntext is pivotal for deciphering its essence. To this end, we propose a novel\nframework for hierarchical text rating utilizing LLMs, which incorporates\nRecurrent Alignment with Hard Attention (RAHA). Particularly, hard attention\nmechanism prompts a frozen LLM to selectively focus on pertinent leaf texts\nassociated with the root text and generate symbolic representations of their\nrelationships. Inspired by the gradual stabilization of the Markov Chain,\nrecurrent alignment strategy involves feeding predicted ratings iteratively\nback into the prompts of another trainable LLM, aligning it to progressively\napproximate the desired target. Experimental results demonstrate that RAHA\noutperforms existing state-of-the-art methods on three hierarchical text rating\ndatasets. Theoretical and empirical analysis confirms RAHA's ability to\ngradually converge towards the underlying target through multiple inferences.\nAdditional experiments on plain text rating datasets verify the effectiveness\nof this Markov-like alignment. Our data and code can be available in\nhttps://github.com/ECNU-Text-Computing/Markov-LLM.\n", "rewritten_text": "Large language models (LLMs) excel at understanding and generating plain text, but they are not specifically designed to handle hierarchical text structures or predict task-specific properties such as text rating directly. Understanding the hierarchical structure of large-scale text is crucial for deciphering its essence. To address this, we introduce a novel framework for hierarchical text rating that leverages LLMs and incorporates Recurrent Alignment with Hard Attention (RAHA). The hard attention mechanism enables a frozen LLM to focus selectively on relevant leaf texts connected to the root text, generating symbolic representations of their relationships. Drawing inspiration from the gradual stabilization of a Markov Chain, the recurrent alignment strategy involves iteratively feeding predicted ratings back into the prompts of another trainable LLM, aligning it to progressively approximate the desired target. Experimental results demonstrate that RAHA surpasses existing state-of-the-art methods on three hierarchical text rating datasets. Theoretical and empirical analyses confirm RAHA's ability to converge gradually towards the target through multiple inferences. Additional experiments on plain text rating datasets validate the effectiveness of this Markov-like alignment approach. Our data and code are available at https://github.com/ECNU-Text-Computing/Markov-LLM."}, "2410.10227": {"original_text": "  Few-Shot Learning (FSL) aims to recognize new classes with limited labeled\ndata. Recent studies have attempted to address the challenge of rare samples\nwith textual prompts to modulate visual features. However, they usually\nstruggle to capture complex semantic relationships between textual and visual\nfeatures. Moreover, vanilla self-attention is heavily affected by useless\ninformation in images, severely constraining the potential of semantic priors\nin FSL due to the confusion of numerous irrelevant tokens during interaction.\nTo address these aforementioned issues, a K-NN Transformer with Pyramid Prompts\n(KTPP) is proposed to select discriminative information with K-NN Context\nAttention (KCA) and adaptively modulate visual features with Pyramid\nCross-modal Prompts (PCP). First, for each token, the KCA only selects the K\nmost relevant tokens to compute the self-attention matrix and incorporates the\nmean of all tokens as the context prompt to provide the global context in three\ncascaded stages. As a result, irrelevant tokens can be progressively\nsuppressed. Secondly, pyramid prompts are introduced in the PCP to emphasize\nvisual features via interactions between text-based class-aware prompts and\nmulti-scale visual features. This allows the ViT to dynamically adjust the\nimportance weights of visual features based on rich semantic information at\ndifferent scales, making models robust to spatial variations. Finally,\naugmented visual features and class-aware prompts are interacted via the KCA to\nextract class-specific features. Consequently, our model further enhances\nnoise-free visual representations via deep cross-modal interactions, extracting\ngeneralized visual representation in scenarios with few labeled samples.\nExtensive experiments on four benchmark datasets demonstrate the effectiveness\nof our method.\n", "rewritten_text": "Few-Shot Learning (FSL) aims to recognize new classes with limited labeled data. Recent studies have attempted to address the challenge of rare samples by using textual prompts to modulate visual features. However, these studies often struggle to capture complex semantic relationships between textual and visual features. Additionally, vanilla self-attention is heavily influenced by irrelevant information in images, significantly limiting the potential of semantic priors in FSL due to the confusion caused by numerous irrelevant tokens during interaction.\n\nTo tackle these issues, we propose a K-NN Transformer with Pyramid Prompts (KTPP) that selects discriminative information using K-NN Context Attention (KCA) and adaptively modulates visual features with Pyramid Cross-modal Prompts (PCP). The KCA selects the K most relevant tokens for each token to compute the self-attention matrix and incorporates the mean of all tokens as the context prompt, providing global context in three cascaded stages to progressively suppress irrelevant tokens. \n\nFurthermore, pyramid prompts are introduced in the PCP to highlight visual features through interactions between text-based class-aware prompts and multi-scale visual features. This enables the ViT to dynamically adjust the importance weights of visual features based on rich semantic information at different scales, enhancing the model's robustness to spatial variations. \n\nFinally, augmented visual features and class-aware prompts are interacted via the KCA to extract class-specific features, thereby enhancing noise-free visual representations through deep cross-modal interactions and extracting generalized visual representations in scenarios with few labeled samples. Extensive experiments on four benchmark datasets demonstrate the effectiveness of our method."}, "2005.00692": {"original_text": "  Cross-lingual Entity Linking (XEL), the problem of grounding mentions of\nentities in a foreign language text into an English knowledge base such as\nWikipedia, has seen a lot of research in recent years, with a range of\npromising techniques. However, current techniques do not rise to the challenges\nintroduced by text in low-resource languages (LRL) and, surprisingly, fail to\ngeneralize to text not taken from Wikipedia, on which they are usually trained.\n  This paper provides a thorough analysis of low-resource XEL techniques,\nfocusing on the key step of identifying candidate English Wikipedia titles that\ncorrespond to a given foreign language mention. Our analysis indicates that\ncurrent methods are limited by their reliance on Wikipedia's interlanguage\nlinks and thus suffer when the foreign language's Wikipedia is small. We\nconclude that the LRL setting requires the use of outside-Wikipedia\ncross-lingual resources and present a simple yet effective zero-shot XEL\nsystem, QuEL, that utilizes search engines query logs. With experiments on 25\nlanguages, QuEL~shows an average increase of 25\\% in gold candidate recall and\nof 13\\% in end-to-end linking accuracy over state-of-the-art baselines.\n", "rewritten_text": "Cross-lingual Entity Linking (XEL) is the process of connecting references to entities in a foreign language text to an English knowledge base like Wikipedia. This area has been the focus of extensive research in recent years, resulting in a variety of promising techniques. However, current methods face challenges when dealing with texts in low-resource languages (LRL) and struggle to adapt to texts not sourced from Wikipedia, where they are typically trained.\n\nThis paper conducts a comprehensive examination of low-resource XEL techniques, with a specific emphasis on the crucial task of identifying potential English Wikipedia titles corresponding to a given mention in a foreign language. Our analysis reveals that existing approaches are constrained by their dependence on Wikipedia's interlanguage links, leading to limitations when dealing with small Wikipedia editions in foreign languages. We argue that addressing the LRL scenario necessitates leveraging cross-lingual resources beyond Wikipedia and introduce a straightforward yet efficient zero-shot XEL system called QuEL, which leverages search engine query logs.\n\nThrough experiments conducted across 25 languages, QuEL demonstrates an average increase of 25% in gold candidate recall and 13% in end-to-end linking accuracy compared to current state-of-the-art methods."}, "2103.12462": {"original_text": "  Person ReID methods always learn through a stationary domain that is fixed by\nthe choice of a given dataset. In many contexts (e.g., lifelong learning),\nthose methods are ineffective because the domain is continually changing in\nwhich case incremental learning over multiple domains is required potentially.\nIn this work we explore a new and challenging ReID task, namely lifelong person\nre-identification (LReID), which enables to learn continuously across multiple\ndomains and even generalise on new and unseen domains. Following the cognitive\nprocesses in the human brain, we design an Adaptive Knowledge Accumulation\n(AKA) framework that is endowed with two crucial abilities: knowledge\nrepresentation and knowledge operation. Our method alleviates catastrophic\nforgetting on seen domains and demonstrates the ability to generalize to unseen\ndomains. Correspondingly, we also provide a new and large-scale benchmark for\nLReID. Extensive experiments demonstrate our method outperforms other\ncompetitors by a margin of 5.8% mAP in generalising evaluation.\n", "rewritten_text": "Person ReID methods typically operate within a fixed domain determined by the dataset used. However, in scenarios such as lifelong learning, where the domain is constantly evolving, these methods may prove ineffective. In such cases, there is a need for incremental learning across multiple domains. This study delves into a novel and challenging ReID task known as lifelong person re-identification (LReID), which facilitates continuous learning across diverse domains and the ability to generalize to new and unseen domains. Inspired by the cognitive processes of the human brain, we introduce an Adaptive Knowledge Accumulation (AKA) framework that possesses two key capabilities: knowledge representation and knowledge operation. Our approach mitigates catastrophic forgetting in familiar domains and showcases the capacity to generalize to unfamiliar domains. Additionally, we introduce a new, large-scale benchmark for LReID. Through extensive experiments, our method demonstrates superior performance compared to other competitors, achieving a 5.8% higher mean Average Precision (mAP) in generalization evaluation."}, "1705.07426": {"original_text": "  While the research community appears to have developed a consensus on the\nmethods of acquiring annotated data, design and training of CNNs, many\nquestions still remain to be answered. In this paper, we explore the following\nquestions that are critical to face recognition research: (i) Can we train on\nstill images and expect the systems to work on videos? (ii) Are deeper datasets\nbetter than wider datasets? (iii) Does adding label noise lead to improvement\nin performance of deep networks? (iv) Is alignment needed for face recognition?\nWe address these questions by training CNNs using CASIA-WebFace, UMDFaces, and\na new video dataset and testing on YouTube- Faces, IJB-A and a disjoint portion\nof UMDFaces datasets. Our new data set, which will be made publicly available,\nhas 22,075 videos and 3,735,476 human annotated frames extracted from them.\n", "rewritten_text": "The research community has largely reached a consensus on the methods for acquiring annotated data, designing, and training Convolutional Neural Networks (CNNs). However, many questions still remain unanswered. This paper delves into critical questions in face recognition research, including: \n\n1. Can systems be trained on still images and effectively work on videos?\n2. Are deeper datasets more effective than wider datasets?\n3. Does introducing label noise enhance the performance of deep networks?\n4. Is alignment necessary for face recognition?\n\nTo address these questions, we trained CNNs using CASIA-WebFace, UMDFaces, and a new video dataset, and tested them on YouTube-Faces, IJB-A, and a separate portion of UMDFaces datasets. Our new dataset, comprising 22,075 videos and 3,735,476 human-annotated frames, will be publicly available."}, "2010.06363": {"original_text": "  Lip motion reflects behavior characteristics of speakers, and thus can be\nused as a new kind of biometrics in speaker recognition. In the literature,\nlots of works used two-dimensional (2D) lip images to recognize speaker in a\ntextdependent context. However, 2D lip easily suffers from various face\norientations. To this end, in this work, we present a novel end-to-end 3D lip\nmotion Network (3LMNet) by utilizing the sentence-level 3D lip motion (S3DLM)\nto recognize speakers in both the text-independent and text-dependent contexts.\nA new regional feedback module (RFM) is proposed to obtain attentions in\ndifferent lip regions. Besides, prior knowledge of lip motion is investigated\nto complement RFM, where landmark-level and frame-level features are merged to\nform a better feature representation. Moreover, we present two methods, i.e.,\ncoordinate transformation and face posture correction to pre-process the LSD-AV\ndataset, which contains 68 speakers and 146 sentences per speaker. The\nevaluation results on this dataset demonstrate that our proposed 3LMNet is\nsuperior to the baseline models, i.e., LSTM, VGG-16 and ResNet-34, and\noutperforms the state-of-the-art using 2D lip image as well as the 3D face. The\ncode of this work is released at\nhttps://github.com/wutong18/Three-Dimensional-Lip-\nMotion-Network-for-Text-Independent-Speaker-Recognition.\n", "rewritten_text": "Lip motion is a reflection of speakers' behavioral characteristics, making it a potential biometric for speaker recognition. Previous studies have utilized two-dimensional (2D) lip images to identify speakers in text-dependent scenarios. However, 2D lip recognition is susceptible to challenges posed by varying face orientations. In this study, we introduce a novel end-to-end 3D lip motion Network (3LMNet) that leverages sentence-level 3D lip motion (S3DLM) for speaker recognition in both text-independent and text-dependent contexts. A new regional feedback module (RFM) is proposed to capture attention in different lip regions. Additionally, we explore the integration of prior knowledge of lip motion to enhance RFM, combining landmark-level and frame-level features for improved representation. Furthermore, we introduce two preprocessing methods, namely coordinate transformation and face posture correction, to prepare the LSD-AV dataset, which comprises 68 speakers and 146 sentences per speaker. Evaluation results on this dataset demonstrate the superiority of our proposed 3LMNet over baseline models such as LSTM, VGG-16, and ResNet-34, as well as outperforming state-of-the-art approaches using 2D lip images and 3D face data. The code for this work is available at https://github.com/wutong18/Three-Dimensional-Lip-Motion-Network-for-Text-Independent-Speaker-Recognition."}, "2210.09345": {"original_text": "  Relation Extraction (RE) has attracted increasing attention, but current RE\nevaluation is limited to in-domain evaluation setups. Little is known on how\nwell a RE system fares in challenging, but realistic out-of-distribution\nevaluation setups. To address this gap, we propose CrossRE, a new,\nfreely-available cross-domain benchmark for RE, which comprises six distinct\ntext domains and includes multi-label annotations. An additional innovation is\nthat we release meta-data collected during annotation, to include explanations\nand flags of difficult instances. We provide an empirical evaluation with a\nstate-of-the-art model for relation classification. As the meta-data enables us\nto shed new light on the state-of-the-art model, we provide a comprehensive\nanalysis on the impact of difficult cases and find correlations between model\nand human annotations. Overall, our empirical investigation highlights the\ndifficulty of cross-domain RE. We release our dataset, to spur more research in\nthis direction.\n", "rewritten_text": "Relation Extraction (RE) has been garnering increasing attention, yet current evaluation methods are confined to in-domain setups. There is a lack of understanding regarding the performance of RE systems in challenging, yet realistic out-of-distribution scenarios. To bridge this gap, we introduce CrossRE, a novel cross-domain benchmark for RE that encompasses six distinct text domains and features multi-label annotations. A key aspect of our approach is the release of meta-data gathered during annotation, which includes explanations and flags for challenging instances. We conduct an empirical evaluation using a cutting-edge model for relation classification. Leveraging the meta-data allows us to offer fresh insights into the state-of-the-art model, leading to a comprehensive analysis of the impact of challenging cases and identifying correlations between model predictions and human annotations. Our empirical study underscores the complexity of cross-domain RE. We make our dataset publicly available to encourage further research in this area."}, "1006.2734": {"original_text": "  A difficult problem in clustering is how to handle data with a manifold\nstructure, i.e. data that is not shaped in the form of compact clouds of\npoints, forming arbitrary shapes or paths embedded in a high-dimensional space.\nIn this work we introduce the Penalized k-Nearest-Neighbor-Graph (PKNNG) based\nmetric, a new tool for evaluating distances in such cases. The new metric can\nbe used in combination with most clustering algorithms. The PKNNG metric is\nbased on a two-step procedure: first it constructs the k-Nearest-Neighbor-Graph\nof the dataset of interest using a low k-value and then it adds edges with an\nexponentially penalized weight for connecting the sub-graphs produced by the\nfirst step. We discuss several possible schemes for connecting the different\nsub-graphs. We use three artificial datasets in four different embedding\nsituations to evaluate the behavior of the new metric, including a comparison\namong different clustering methods. We also evaluate the new metric in a real\nworld application, clustering the MNIST digits dataset. In all cases the PKNNG\nmetric shows promising clustering results.\n", "rewritten_text": "Clustering poses a challenge when dealing with data exhibiting a manifold structure, characterized by points forming arbitrary shapes or paths within a high-dimensional space, rather than compact clouds. This study introduces the Penalized k-Nearest-Neighbor-Graph (PKNNG) based metric as a novel tool for assessing distances in such scenarios. The PKNNG metric can be seamlessly integrated with various clustering algorithms. It operates through a two-step process: initially constructing the k-Nearest-Neighbor-Graph of the dataset using a low k-value, followed by the addition of edges with exponentially penalized weights to connect the sub-graphs generated in the first step. Various approaches for linking the distinct sub-graphs are explored. To assess the performance of the new metric, three artificial datasets are utilized across four different embedding scenarios, comparing its efficacy with different clustering methods. Furthermore, the PKNNG metric is evaluated in a practical setting by clustering the MNIST digits dataset, demonstrating promising results across all cases."}, "1804.01422": {"original_text": "  In this paper, we propose a simple but effective semantic-based aggregation\n(SBA) method. The proposed SBA utilizes the discriminative filters of deep\nconvolutional layers as semantic detectors. Moreover, we propose the effective\nunsupervised strategy to select some semantic detectors to generate the\n\"probabilistic proposals\", which highlight certain discriminative pattern of\nobjects and suppress the noise of background. The final global SBA\nrepresentation could then be acquired by aggregating the regional\nrepresentations weighted by the selected \"probabilistic proposals\"\ncorresponding to various semantic content. Our unsupervised SBA is easy to\ngeneralize and achieves excellent performance on various tasks. We conduct\ncomprehensive experiments and show that our unsupervised SBA outperforms the\nstate-of-the-art unsupervised and supervised aggregation methods on image\nretrieval, place recognition and cloud classification.\n", "rewritten_text": "In this paper, we introduce a straightforward yet powerful semantic-based aggregation (SBA) method. The proposed SBA leverages the discriminative filters found in deep convolutional layers as semantic detectors. Additionally, we present an effective unsupervised strategy for selecting specific semantic detectors to create \"probabilistic proposals.\" These proposals emphasize distinctive object patterns while reducing background noise.\n\nThe ultimate global SBA representation is obtained by combining regional representations, weighted by the chosen \"probabilistic proposals\" that correspond to different semantic content. Our unsupervised SBA is easily adaptable and demonstrates outstanding performance across various tasks. Through comprehensive experiments, we demonstrate that our unsupervised SBA surpasses both unsupervised and supervised aggregation methods in image retrieval, place recognition, and cloud classification."}, "2109.00881": {"original_text": "  There is a growing interest in product aesthetics analytics and design.\nHowever, the lack of available large-scale data that covers various variables\nand information is one of the biggest challenges faced by analysts and\nresearchers. In this paper, we present our multidisciplinary initiative of\ndeveloping a comprehensive automotive dataset from different online sources and\nformats. Specifically, the created dataset contains 1.4 million images from 899\ncar models and their corresponding model specifications and sales information\nover more than ten years in the UK market. Our work makes significant\ncontributions to: (i) research and applications in the automotive industry;\n(ii) big data creation and sharing; (iii) database design; and (iv) data\nfusion. Apart from our motivation, technical details and data structure, we\nfurther present three simple examples to demonstrate how our data can be used\nin business research and applications.\n", "rewritten_text": "There is a growing interest in product aesthetics analytics and design. However, one of the biggest challenges faced by analysts and researchers is the lack of available large-scale data that covers various variables and information. In this paper, we introduce our multidisciplinary initiative to develop a comprehensive automotive dataset sourced from different online platforms and formats. The dataset we have created comprises 1.4 million images representing 899 car models, along with their corresponding model specifications and sales information spanning over a decade in the UK market. Our work contributes significantly to: (i) research and applications in the automotive industry; (ii) big data creation and sharing; (iii) database design; and (iv) data fusion. In addition to outlining our motivation, technical details, and data structure, we provide three simple examples to illustrate how our data can be utilized in business research and applications."}, "1611.09007": {"original_text": "  Hyperspectral imaging sensors are becoming increasingly popular in robotics\napplications such as agriculture and mining, and allow per-pixel thematic\nclassification of materials in a scene based on their unique spectral\nsignatures. Recently, convolutional neural networks have shown remarkable\nperformance for classification tasks, but require substantial amounts of\nlabelled training data. This data must sufficiently cover the variability\nexpected to be encountered in the environment. For hyperspectral data, one of\nthe main variations encountered outdoors is due to incident illumination, which\ncan change in spectral shape and intensity depending on the scene geometry. For\nexample, regions occluded from the sun have a lower intensity and their\nincident irradiance skewed towards shorter wavelengths.\n  In this work, a data augmentation strategy based on relighting is used during\ntraining of a hyperspectral convolutional neural network. It allows training to\noccur in the outdoor environment given only a small labelled region, which does\nnot need to sufficiently represent the geometric variability of the entire\nscene. This is important for applications where obtaining large amounts of\ntraining data is labourious, hazardous or difficult, such as labelling pixels\nwithin shadows. Radiometric normalisation approaches for pre-processing the\nhyperspectral data are analysed and it is shown that methods based on the raw\npixel data are sufficient to be used as input for the classifier. This removes\nthe need for external hardware such as calibration boards, which can restrict\nthe application of hyperspectral sensors in robotics applications. Experiments\nto evaluate the classification system are carried out on two datasets captured\nfrom a field-based platform.\n", "rewritten_text": "Hyperspectral imaging sensors are increasingly popular in robotics applications, particularly in fields like agriculture and mining. These sensors enable per-pixel thematic classification of materials in a scene based on their unique spectral signatures. Convolutional neural networks have recently demonstrated exceptional performance in classification tasks, but they require significant amounts of labeled training data. This data must adequately cover the expected variability in the environment. In the case of hyperspectral data, one of the primary variations encountered outdoors is due to incident illumination, which can vary in spectral shape and intensity based on the scene's geometry. For instance, areas shielded from direct sunlight exhibit lower intensity and a spectral distribution skewed towards shorter wavelengths.\n\nThis study introduces a data augmentation strategy utilizing relighting during the training of a hyperspectral convolutional neural network. This approach enables training in outdoor environments with only a small labeled region, eliminating the need to represent the entire scene's geometric variability comprehensively. This is particularly valuable for applications where acquiring large amounts of training data is challenging, hazardous, or labor-intensive, such as labeling pixels within shadows. The study also examines radiometric normalization methods for preprocessing hyperspectral data, demonstrating that techniques based on raw pixel data are adequate for input into the classifier. This eliminates the necessity for external hardware like calibration boards, which can limit the use of hyperspectral sensors in robotics applications. The classification system's performance is evaluated through experiments conducted on two datasets collected from a field-based platform."}, "1905.00996": {"original_text": "  In this work, we propose a novel framework named Region-Aware Network\n(RANet), which learns the ability of anti-confusing in case of heavy occlusion,\nnearby person and symmetric appearance, for human pose estimation.\nSpecifically, the proposed method addresses three key aspects, i.e., data\naugmentation, feature learning and prediction fusion, respectively. First, we\npropose Parsing-based Data Augmentation (PDA) to generate abundant data that\nsynthesizes confusing textures. Second, we not only propose a Feature Pyramid\nStem (FPS) to learn stronger low-level features in lower stage; but also\nincorporate an Effective Region Extraction (ERE) module to excavate better\ntarget-specific features. Third, we introduce Cascade Voting Fusion (CVF) to\nexplicitly exclude the inferior predictions and fuse the rest effective\npredictions for the final pose estimation. Extensive experimental results on\ntwo popular benchmarks, i.e. MPII and LSP, demonstrate the effectiveness of our\nmethod against the state-of-the-art competitors. Especially on\neasily-confusable joints, our method makes significant improvement.\n", "rewritten_text": "In this study, we introduce a new framework called Region-Aware Network (RANet) designed to enhance human pose estimation by learning to distinguish between confusing scenarios such as heavy occlusion, nearby individuals, and symmetric appearances. The proposed method focuses on three main aspects: data augmentation, feature learning, and prediction fusion. \n\nTo begin with, we present Parsing-based Data Augmentation (PDA) as a method to generate diverse data that simulates complex textures. Additionally, we introduce the Feature Pyramid Stem (FPS) to enhance the learning of robust low-level features at earlier stages. Furthermore, we incorporate an Effective Region Extraction (ERE) module to extract more precise target-specific features. \n\nLastly, we introduce Cascade Voting Fusion (CVF) to eliminate unreliable predictions and combine the remaining accurate predictions for the final pose estimation. Our method is evaluated on two widely-used benchmarks, MPII and LSP, showcasing its superiority over existing state-of-the-art methods. Notably, our approach demonstrates significant enhancements, particularly in scenarios involving easily-confused joints."}, "2304.00025": {"original_text": "  After the pandemic, artificial intelligence (AI) powered support for mental\nhealth care has become increasingly important. The breadth and complexity of\nsignificant challenges required to provide adequate care involve: (a)\nPersonalized patient understanding, (b) Safety-constrained and medically\nvalidated chatbot patient interactions, and (c) Support for continued\nfeedback-based refinements in design using chatbot-patient interactions. We\npropose Alleviate, a chatbot designed to assist patients suffering from mental\nhealth challenges with personalized care and assist clinicians with\nunderstanding their patients better. Alleviate draws from an array of publicly\navailable clinically valid mental-health texts and databases, allowing\nAlleviate to make medically sound and informed decisions. In addition,\nAlleviate's modular design and explainable decision-making lends itself to\nrobust and continued feedback-based refinements to its design. In this paper,\nwe explain the different modules of Alleviate and submit a short video\ndemonstrating Alleviate's capabilities to help patients and clinicians\nunderstand each other better to facilitate optimal care strategies.\n", "rewritten_text": "After the pandemic, the importance of artificial intelligence (AI) powered support for mental health care has significantly increased. Addressing the breadth and complexity of significant challenges in providing adequate care involves: (a) personalized patient understanding, (b) safety-constrained and medically validated chatbot patient interactions, and (c) support for ongoing feedback-based refinements in design through chatbot-patient interactions. \n\nWe introduce Alleviate, a chatbot specifically designed to aid patients facing mental health challenges by providing personalized care and assisting clinicians in better understanding their patients. Alleviate utilizes a range of publicly available clinically valid mental health texts and databases to ensure that it makes medically sound and informed decisions. Furthermore, Alleviate's modular design and explainable decision-making process allow for robust and continuous feedback-based refinements to its design.\n\nIn this paper, we detail the various modules of Alleviate and present a short video showcasing its capabilities in helping patients and clinicians better understand each other to facilitate the development of optimal care strategies."}, "1204.2294": {"original_text": "  This paper present our new intensity chromaticity space-based feature\ndetection and matching algorithm. This approach utilizes hybridization of\nwireless local area network and camera internal sensor which to receive signal\nstrength from a access point and the same time retrieve interest point\ninformation from hallways. This information is combined by model fitting\napproach in order to find the absolute of user target position. No conventional\nsearching algorithm is required, thus it is expected reducing the computational\ncomplexity. Finally we present pre-experimental results to illustrate the\nperformance of the localization system for an indoor environment set-up.\n", "rewritten_text": "This paper presents our new intensity chromaticity space-based feature detection and matching algorithm. The approach utilizes a hybridization of a wireless local area network and camera internal sensor to receive signal strength from an access point and simultaneously retrieve interest point information from hallways. This information is combined using a model fitting approach to determine the absolute position of the user target. No conventional searching algorithm is required, thereby reducing computational complexity. Finally, pre-experimental results are presented to illustrate the performance of the localization system in an indoor environment setup."}, "2410.09595": {"original_text": "  Deformable image registration remains a fundamental task in clinical\npractice, yet solving registration problems involving complex deformations\nremains challenging. Current deep learning-based registration methods employ\ncontinuous deformation to model large deformations, which often suffer from\naccumulated registration errors and interpolation inaccuracies. Moreover,\nachieving satisfactory results with these frameworks typically requires a large\nnumber of cascade stages, demanding substantial computational resources.\nTherefore, we propose a novel approach, the field refinement framework\n(FiRework), tailored for unsupervised deformable registration, aiming to\naddress these challenges. In FiRework, we redesign the continuous deformation\nframework to mitigate the aforementioned errors. Notably, our FiRework requires\nonly one level of recursion during training and supports continuous inference,\noffering improved efficacy compared to continuous deformation frameworks. We\nconducted experiments on two brain MRI datasets, enhancing two existing\ndeformable registration networks with FiRework. The experimental results\ndemonstrate the superior performance of our proposed framework in deformable\nregistration. The code is publicly available at\nhttps://github.com/ZAX130/FiRework.\n", "rewritten_text": "Deformable image registration is a crucial task in clinical practice, but solving registration problems involving complex deformations remains a challenge. Current deep learning-based registration methods use continuous deformation to model large deformations, but they often encounter issues such as accumulated registration errors and interpolation inaccuracies. Additionally, achieving satisfactory results with these methods typically requires multiple cascade stages, leading to high computational demands.\n\nTo address these challenges, we introduce a novel approach called the Field Refinement Framework (FiRework) tailored for unsupervised deformable registration. In FiRework, we have redesigned the continuous deformation framework to reduce the aforementioned errors. Notably, our FiRework only requires one level of recursion during training and supports continuous inference, resulting in improved efficacy compared to continuous deformation frameworks.\n\nWe conducted experiments on two brain MRI datasets by enhancing two existing deformable registration networks with FiRework. The experimental results demonstrate the superior performance of our proposed framework in deformable registration. The code for FiRework is publicly available at https://github.com/ZAX130/FiRework."}, "2401.07770": {"original_text": "  Computer vision tasks typically involve describing what is present in an\nimage (e.g. classification, detection, segmentation, and captioning). We study\na visual common sense task that requires understanding what is not present.\nSpecifically, given an image (e.g. of a living room) and name of an object\n(\"cushion\"), a vision system is asked to predict semantically-meaningful\nregions (masks or bounding boxes) in the image where that object could be\nplaced or is likely be placed by humans (e.g. on the sofa). We call this task:\nSemantic Placement (SP) and believe that such common-sense visual understanding\nis critical for assitive robots (tidying a house), and AR devices\n(automatically rendering an object in the user's space). Studying the invisible\nis hard. Datasets for image description are typically constructed by curating\nrelevant images and asking humans to annotate the contents of the image;\nneither of those two steps are straightforward for objects not present in the\nimage. We overcome this challenge by operating in the opposite direction: we\nstart with an image of an object in context from web, and then remove that\nobject from the image via inpainting. This automated pipeline converts\nunstructured web data into a dataset comprising pairs of images with/without\nthe object. Using this, we collect a novel dataset, with ${\\sim}1.3$M images\nacross $9$ object categories, and train a SP prediction model called CLIP-UNet.\nCLIP-UNet outperforms existing VLMs and baselines that combine semantic priors\nwith object detectors on real-world and simulated images. In our user studies,\nwe find that the SP masks predicted by CLIP-UNet are favored $43.7\\%$ and\n$31.3\\%$ times when comparing against the $4$ SP baselines on real and\nsimulated images. In addition, we demonstrate leveraging SP mask predictions\nfrom CLIP-UNet enables downstream applications like building tidying robots in\nindoor environments.\n", "rewritten_text": "Computer vision tasks typically involve describing the contents of an image, such as classification, detection, segmentation, and captioning. In this study, we focus on a visual common sense task that requires understanding what is absent in an image. Specifically, when given an image, like a living room, and the name of an object (e.g., \"cushion\"), a vision system is tasked with predicting semantically meaningful regions (masks or bounding boxes) in the image where that object could be placed or is likely to be placed by humans (e.g., on the sofa). This task is referred to as Semantic Placement (SP), and we believe that such common-sense visual understanding is crucial for assistive robots (e.g., tidying a house) and augmented reality (AR) devices (e.g., automatically rendering an object in the user's space).\n\nStudying what is not visible in an image presents challenges. Typically, datasets for image description are created by selecting relevant images and having humans annotate the image contents; however, these steps are not straightforward for objects that are not present in the image. To address this challenge, we take a different approach: we begin with an image of an object in context from the web and then remove that object from the image using inpainting. This automated process transforms unstructured web data into a dataset consisting of pairs of images with and without the object. Leveraging this method, we compile a novel dataset containing approximately 1.3 million images across nine object categories and develop a SP prediction model named CLIP-UNet.\n\nCLIP-UNet surpasses existing Vision-Language Models (VLMs) and baseline models that combine semantic priors with object detectors on both real-world and simulated images. Through user studies, we observe that the SP masks predicted by CLIP-UNet are preferred 43.7% and 31.3% of the time when compared to the four SP baselines on real and simulated images. Furthermore, we demonstrate that utilizing SP mask predictions from CLIP-UNet enables downstream applications, such as developing tidying robots for indoor environments."}, "2112.11454": {"original_text": "  Generating digital humans that move realistically has many applications and\nis widely studied, but existing methods focus on the major limbs of the body,\nignoring the hands and head. Hands have been separately studied, but the focus\nhas been on generating realistic static grasps of objects. To synthesize\nvirtual characters that interact with the world, we need to generate full-body\nmotions and realistic hand grasps simultaneously. Both sub-problems are\nchallenging on their own and, together, the state-space of poses is\nsignificantly larger, the scales of hand and body motions differ, and the\nwhole-body posture and the hand grasp must agree, satisfy physical constraints,\nand be plausible. Additionally, the head is involved because the avatar must\nlook at the object to interact with it. For the first time, we address the\nproblem of generating full-body, hand and head motions of an avatar grasping an\nunknown object. As input, our method, called GOAL, takes a 3D object, its\nposition, and a starting 3D body pose and shape. GOAL outputs a sequence of\nwhole-body poses using two novel networks. First, GNet generates a goal\nwhole-body grasp with a realistic body, head, arm, and hand pose, as well as\nhand-object contact. Second, MNet generates the motion between the starting and\ngoal pose. This is challenging, as it requires the avatar to walk towards the\nobject with foot-ground contact, orient the head towards it, reach out, and\ngrasp it with a realistic hand pose and hand-object contact. To achieve this,\nthe networks exploit a representation that combines SMPL-X body parameters and\n3D vertex offsets. We train and evaluate GOAL, both qualitatively and\nquantitatively, on the GRAB dataset. Results show that GOAL generalizes well to\nunseen objects, outperforming baselines. GOAL takes a step towards synthesizing\nrealistic full-body object grasping.\n", "rewritten_text": "The generation of digital humans that move realistically has numerous applications and is a subject of widespread study. However, current methods primarily focus on the major limbs of the body while neglecting the hands and head. Although hands have been studied independently, the emphasis has been on creating realistic static grasps of objects. To effectively synthesize virtual characters that interact with their environment, it is essential to simultaneously generate full-body motions and realistic hand grasps. Both of these sub-problems present significant challenges individually, and when combined, the complexity of the pose state-space increases substantially. The scales of hand and body motions differ, requiring the whole-body posture and hand grasp to align, adhere to physical constraints, and appear plausible.\n\nMoreover, the involvement of the head is crucial as the avatar must orient itself towards objects to interact with them. In a groundbreaking approach, we tackle the task of generating full-body, hand, and head motions of an avatar grasping an unknown object. Our method, named GOAL, takes as input a 3D object, its position, and an initial 3D body pose and shape. GOAL then produces a sequence of whole-body poses through the utilization of two innovative networks.\n\nThe first network, GNet, generates a comprehensive whole-body grasp featuring realistic body, head, arm, and hand poses, along with hand-object contact. Subsequently, the second network, MNet, generates the motion between the initial and goal poses. This task is particularly challenging as it necessitates the avatar to walk towards the object with foot-ground contact, orient its head towards the object, reach out, and grasp it with a realistic hand pose and hand-object contact. To accomplish this, the networks leverage a representation that combines SMPL-X body parameters and 3D vertex offsets.\n\nWe have trained and evaluated GOAL, both qualitatively and quantitatively, on the GRAB dataset. The results demonstrate that GOAL exhibits strong generalization capabilities to unseen objects, surpassing baseline methods. GOAL represents a significant advancement towards the synthesis of realistic full-body object grasping scenarios."}, "2103.05861": {"original_text": "  Neural network pruning is an essential approach for reducing the\ncomputational complexity of deep models so that they can be well deployed on\nresource-limited devices. Compared with conventional methods, the recently\ndeveloped dynamic pruning methods determine redundant filters variant to each\ninput instance which achieves higher acceleration. Most of the existing methods\ndiscover effective sub-networks for each instance independently and do not\nutilize the relationship between different inputs. To maximally excavate\nredundancy in the given network architecture, this paper proposes a new\nparadigm that dynamically removes redundant filters by embedding the manifold\ninformation of all instances into the space of pruned networks (dubbed as\nManiDP). We first investigate the recognition complexity and feature similarity\nbetween images in the training set. Then, the manifold relationship between\ninstances and the pruned sub-networks will be aligned in the training\nprocedure. The effectiveness of the proposed method is verified on several\nbenchmarks, which shows better performance in terms of both accuracy and\ncomputational cost compared to the state-of-the-art methods. For example, our\nmethod can reduce 55.3% FLOPs of ResNet-34 with only 0.57% top-1 accuracy\ndegradation on ImageNet.\n", "rewritten_text": "Neural network pruning is a crucial technique for reducing the computational complexity of deep models, enabling their efficient deployment on resource-limited devices. Unlike traditional methods, dynamic pruning methods have emerged as a more effective approach by identifying redundant filters specific to each input instance, resulting in significant acceleration. However, most existing methods focus on identifying effective sub-networks for individual instances without considering the relationships between different inputs.\n\nTo fully exploit redundancy within a network architecture, this paper introduces a novel approach called ManiDP, which dynamically removes redundant filters by incorporating manifold information from all instances into the pruned network space. The method begins by analyzing the recognition complexity and feature similarity among images in the training set. Subsequently, the manifold relationships between instances and pruned sub-networks are aligned during the training process.\n\nExperimental results demonstrate the effectiveness of the proposed method across various benchmarks, showcasing superior performance in terms of both accuracy and computational cost compared to state-of-the-art techniques. For instance, our method achieves a 55.3% reduction in FLOPs for ResNet-34 on ImageNet with only a 0.57% decrease in top-1 accuracy."}, "2002.01127": {"original_text": "  How to generate descriptions from structured data organized in tables?\nExisting approaches using neural encoder-decoder models often suffer from\nlacking diversity. We claim that an open set of templates is crucial for\nenriching the phrase constructions and realizing varied generations. Learning\nsuch templates is prohibitive since it often requires a large paired <table,\ndescription> corpus, which is seldom available. This paper explores the problem\nof automatically learning reusable \"templates\" from paired and non-paired data.\nWe propose the variational template machine (VTM), a novel method to generate\ntext descriptions from data tables. Our contributions include: a) we carefully\ndevise a specific model architecture and losses to explicitly disentangle text\ntemplate and semantic content information, in the latent spaces, and b)we\nutilize both small parallel data and large raw text without aligned tables to\nenrich the template learning. Experiments on datasets from a variety of\ndifferent domains show that VTM is able to generate more diversely while\nkeeping a good fluency and quality.\n", "rewritten_text": "How can descriptions be generated from structured data organized in tables? Existing approaches using neural encoder-decoder models often lack diversity. We argue that having an open set of templates is essential for enhancing phrase constructions and achieving varied generations. Learning such templates can be challenging as it typically necessitates a large paired <table, description> corpus, which is rarely available. This paper addresses the issue of automatically learning reusable \"templates\" from both paired and non-paired data. We introduce the variational template machine (VTM), a novel method for generating text descriptions from data tables. Our contributions are twofold: a) we meticulously design a specific model architecture and losses to explicitly separate text template and semantic content information in the latent spaces, and b) we leverage both limited parallel data and extensive raw text without aligned tables to enhance template learning. Experiments conducted on datasets from various domains demonstrate that VTM can generate more diverse outputs while maintaining good fluency and quality."}, "2010.03725": {"original_text": "  We present a new benchmark dataset called PARADE for paraphrase\nidentification that requires specialized domain knowledge. PARADE contains\nparaphrases that overlap very little at the lexical and syntactic level but are\nsemantically equivalent based on computer science domain knowledge, as well as\nnon-paraphrases that overlap greatly at the lexical and syntactic level but are\nnot semantically equivalent based on this domain knowledge. Experiments show\nthat both state-of-the-art neural models and non-expert human annotators have\npoor performance on PARADE. For example, BERT after fine-tuning achieves an F1\nscore of 0.709, which is much lower than its performance on other paraphrase\nidentification datasets. PARADE can serve as a resource for researchers\ninterested in testing models that incorporate domain knowledge. We make our\ndata and code freely available.\n", "rewritten_text": "We introduce a novel benchmark dataset named PARADE for paraphrase identification, which necessitates specialized domain knowledge. PARADE comprises paraphrases that exhibit minimal overlap at the lexical and syntactic levels but are semantically equivalent based on domain-specific computer science knowledge. Additionally, it includes non-paraphrases that display significant overlap at the lexical and syntactic levels but are not semantically equivalent within this domain. Experimental results demonstrate that both cutting-edge neural models and non-expert human annotators exhibit subpar performance on PARADE. For instance, even after fine-tuning, BERT achieves an F1 score of 0.709, significantly lower than its performance on other paraphrase identification datasets. PARADE can serve as a valuable resource for researchers interested in evaluating models that integrate domain knowledge. We provide our data and code freely accessible to the public."}, "2307.12067": {"original_text": "  We introduce Replay, a collection of multi-view, multi-modal videos of humans\ninteracting socially. Each scene is filmed in high production quality, from\ndifferent viewpoints with several static cameras, as well as wearable action\ncameras, and recorded with a large array of microphones at different positions\nin the room. Overall, the dataset contains over 4000 minutes of footage and\nover 7 million timestamped high-resolution frames annotated with camera poses\nand partially with foreground masks. The Replay dataset has many potential\napplications, such as novel-view synthesis, 3D reconstruction, novel-view\nacoustic synthesis, human body and face analysis, and training generative\nmodels. We provide a benchmark for training and evaluating novel-view\nsynthesis, with two scenarios of different difficulty. Finally, we evaluate\nseveral baseline state-of-the-art methods on the new benchmark.\n", "rewritten_text": "Introducing Replay, a collection of multi-view, multi-modal videos showcasing human interactions in social settings. Each scene is captured in high production quality, utilizing various viewpoints from static cameras and wearable action cameras. The recordings are complemented by a diverse array of microphones strategically placed throughout the room. The dataset comprises over 4000 minutes of footage, encompassing more than 7 million timestamped high-resolution frames annotated with camera poses and, to some extent, foreground masks.\n\nThe Replay dataset holds significant potential for a range of applications, including novel-view synthesis, 3D reconstruction, novel-view acoustic synthesis, human body and face analysis, and the training of generative models. To facilitate research and development in novel-view synthesis, we present a benchmark featuring two distinct difficulty scenarios. Furthermore, we assess the performance of various cutting-edge methods against this new benchmark."}, "1703.10645": {"original_text": "  We propose a novel method called the Relevance Subject Machine (RSM) to solve\nthe person re-identification (re-id) problem. RSM falls under the category of\nBayesian sparse recovery algorithms and uses the sparse representation of the\ninput video under a pre-defined dictionary to identify the subject in the\nvideo. Our approach focuses on the multi-shot re-id problem, which is the\nprevalent problem in many video analytics applications. RSM captures the\nessence of the multi-shot re-id problem by constraining the support of the\nsparse codes for each input video frame to be the same. Our proposed approach\nis also robust enough to deal with time varying outliers and occlusions by\nintroducing a sparse, non-stationary noise term in the model error. We provide\na novel Variational Bayesian based inference procedure along with an intuitive\ninterpretation of the proposed update rules. We evaluate our approach over\nseveral commonly used re-id datasets and show superior performance over current\nstate-of-the-art algorithms. Specifically, for ILIDS-VID, a recent large scale\nre-id dataset, RSM shows significant improvement over all published approaches,\nachieving an 11.5% (absolute) improvement in rank 1 accuracy over the closest\ncompeting algorithm considered.\n", "rewritten_text": "We introduce a novel method known as the Relevance Subject Machine (RSM) to address the person re-identification (re-id) challenge. RSM belongs to the category of Bayesian sparse recovery algorithms and leverages the sparse representation of the input video based on a predefined dictionary to identify the subject in the video. Our approach is tailored towards tackling the multi-shot re-id problem, which is a common issue in various video analytics applications. RSM effectively captures the essence of the multi-shot re-id problem by enforcing that the support of the sparse codes for each input video frame remains consistent. Additionally, our proposed approach demonstrates robustness in handling time-varying outliers and occlusions by incorporating a sparse, non-stationary noise term in the model error. We present a novel Variational Bayesian-based inference procedure along with a straightforward interpretation of the proposed update rules. Through evaluations on multiple widely-used re-id datasets, we showcase superior performance compared to current state-of-the-art algorithms. Notably, on the ILIDS-VID dataset, a recent large-scale re-id dataset, RSM exhibits a significant improvement over all existing approaches, achieving an 11.5% (absolute) enhancement in rank 1 accuracy compared to the closest competing algorithm."}, "1908.08498": {"original_text": "  We focus on multi-modal fusion for egocentric action recognition, and propose\na novel architecture for multi-modal temporal-binding, i.e. the combination of\nmodalities within a range of temporal offsets. We train the architecture with\nthree modalities -- RGB, Flow and Audio -- and combine them with mid-level\nfusion alongside sparse temporal sampling of fused representations. In contrast\nwith previous works, modalities are fused before temporal aggregation, with\nshared modality and fusion weights over time. Our proposed architecture is\ntrained end-to-end, outperforming individual modalities as well as late-fusion\nof modalities.\n  We demonstrate the importance of audio in egocentric vision, on per-class\nbasis, for identifying actions as well as interacting objects. Our method\nachieves state of the art results on both the seen and unseen test sets of the\nlargest egocentric dataset: EPIC-Kitchens, on all metrics using the public\nleaderboard.\n", "rewritten_text": "Our focus is on multi-modal fusion for egocentric action recognition. We introduce a novel architecture for multi-modal temporal binding, which involves combining modalities within a range of temporal offsets. The architecture is trained using three modalities - RGB, Flow, and Audio - and integrates them through mid-level fusion with sparse temporal sampling of fused representations. Unlike previous approaches, our method fuses modalities before temporal aggregation, utilizing shared modality and fusion weights over time. The proposed architecture is trained end-to-end and surpasses individual modalities and late-fusion approaches in performance.\n\nWe highlight the significance of audio in egocentric vision for identifying actions and interacting objects on a per-class basis. Our method achieves state-of-the-art results on both the seen and unseen test sets of the largest egocentric dataset, EPIC-Kitchens, across all metrics on the public leaderboard."}, "1611.09813": {"original_text": "  We propose a CNN-based approach for 3D human body pose estimation from single\nRGB images that addresses the issue of limited generalizability of models\ntrained solely on the starkly limited publicly available 3D pose data. Using\nonly the existing 3D pose data and 2D pose data, we show state-of-the-art\nperformance on established benchmarks through transfer of learned features,\nwhile also generalizing to in-the-wild scenes. We further introduce a new\ntraining set for human body pose estimation from monocular images of real\nhumans that has the ground truth captured with a multi-camera marker-less\nmotion capture system. It complements existing corpora with greater diversity\nin pose, human appearance, clothing, occlusion, and viewpoints, and enables an\nincreased scope of augmentation. We also contribute a new benchmark that covers\noutdoor and indoor scenes, and demonstrate that our 3D pose dataset shows\nbetter in-the-wild performance than existing annotated data, which is further\nimproved in conjunction with transfer learning from 2D pose data. All in all,\nwe argue that the use of transfer learning of representations in tandem with\nalgorithmic and data contributions is crucial for general 3D body pose\nestimation.\n", "rewritten_text": "We present a CNN-based approach for estimating 3D human body pose from single RGB images, aiming to overcome the challenge of limited generalizability in models trained solely on the scarce publicly available 3D pose data. By leveraging both existing 3D pose data and 2D pose data, we achieve state-of-the-art performance on established benchmarks through the transfer of learned features, while also demonstrating effectiveness in diverse real-world scenarios. \n\nAdditionally, we introduce a novel training set for human body pose estimation using monocular images of real humans, capturing ground truth with a multi-camera marker-less motion capture system. This new dataset enhances existing collections by offering greater diversity in pose, human appearance, clothing, occlusion, and viewpoints, thereby enabling broader augmentation possibilities. \n\nFurthermore, we contribute a new benchmark encompassing both outdoor and indoor scenes, showcasing superior performance in real-world settings compared to existing annotated data. This performance is further enhanced through transfer learning from 2D pose data. \n\nIn conclusion, we advocate for the essential role of transfer learning in conjunction with algorithmic advancements and enriched data contributions for advancing general 3D body pose estimation."}, "2109.12338": {"original_text": "  Model binarization is an effective method of compressing neural networks and\naccelerating their inference process. However, a significant performance gap\nstill exists between the 1-bit model and the 32-bit one. The empirical study\nshows that binarization causes a great loss of information in the forward and\nbackward propagation. We present a novel Distribution-sensitive Information\nRetention Network (DIR-Net) that retains the information in the forward and\nbackward propagation by improving internal propagation and introducing external\nrepresentations. The DIR-Net mainly relies on three technical contributions:\n(1) Information Maximized Binarization (IMB): minimizing the information loss\nand the binarization error of weights/activations simultaneously by weight\nbalance and standardization; (2) Distribution-sensitive Two-stage Estimator\n(DTE): retaining the information of gradients by distribution-sensitive soft\napproximation by jointly considering the updating capability and accurate\ngradient; (3) Representation-align Binarization-aware Distillation (RBD):\nretaining the representation information by distilling the representations\nbetween full-precision and binarized networks. The DIR-Net investigates both\nforward and backward processes of BNNs from the unified information\nperspective, thereby providing new insight into the mechanism of network\nbinarization. The three techniques in our DIR-Net are versatile and effective\nand can be applied in various structures to improve BNNs. Comprehensive\nexperiments on the image classification and objective detection tasks show that\nour DIR-Net consistently outperforms the state-of-the-art binarization\napproaches under mainstream and compact architectures, such as ResNet, VGG,\nEfficientNet, DARTS, and MobileNet. Additionally, we conduct our DIR-Net on\nreal-world resource-limited devices which achieves 11.1x storage saving and\n5.4x speedup.\n", "rewritten_text": "Model binarization is a proven method for compressing neural networks and speeding up their inference process. Despite its effectiveness, a notable performance gap persists between 1-bit and 32-bit models. Empirical studies indicate that binarization leads to significant information loss during both forward and backward propagation. To address this issue, we introduce the Distribution-sensitive Information Retention Network (DIR-Net), designed to preserve information in both propagation directions through enhanced internal propagation and the introduction of external representations.\n\nThe DIR-Net relies on three key technical contributions: \n1. Information Maximized Binarization (IMB): minimizing information loss and binarization errors in weights/activations simultaneously through weight balance and standardization.\n2. Distribution-sensitive Two-stage Estimator (DTE): retaining gradient information through distribution-sensitive soft approximation, considering both updating capability and accurate gradient estimation.\n3. Representation-align Binarization-aware Distillation (RBD): preserving representation information by distilling representations between full-precision and binarized networks.\n\nBy examining both forward and backward processes of Binary Neural Networks (BNNs) from a unified information perspective, DIR-Net offers new insights into the mechanism of network binarization. The versatility and effectiveness of the three techniques in DIR-Net enable their application across various structures to enhance BNNs. Extensive experiments on image classification and object detection tasks demonstrate that DIR-Net consistently outperforms state-of-the-art binarization methods across popular architectures like ResNet, VGG, EfficientNet, DARTS, and MobileNet. Furthermore, deploying DIR-Net on resource-limited devices results in significant storage savings of 11.1x and a speedup of 5.4x."}, "2310.07343": {"original_text": "  Although large language models (LLMs) are impressive in solving various\ntasks, they can quickly be outdated after deployment. Maintaining their\nup-to-date status is a pressing concern in the current era. This paper provides\na comprehensive review of recent advances in aligning LLMs with the\never-changing world knowledge without re-training from scratch. We categorize\nresearch works systemically and provide in-depth comparisons and discussion. We\nalso discuss existing challenges and highlight future directions to facilitate\nresearch in this field. We release the paper list at\nhttps://github.com/hyintell/awesome-refreshing-llms\n", "rewritten_text": "Large language models (LLMs) are impressive in solving various tasks; however, they can quickly become outdated after deployment, making maintaining their up-to-date status a pressing concern in the current era. This paper offers a comprehensive review of recent advances in aligning LLMs with the ever-changing world knowledge without requiring re-training from scratch. The research works are systematically categorized, and in-depth comparisons and discussions are provided. Existing challenges are also discussed, and future directions are highlighted to facilitate research in this field. The paper list is available at https://github.com/hyintell/awesome-refreshing-llms."}, "2301.05792": {"original_text": "  Class-Incremental Learning (CIL) [40] trains classifiers under a strict\nmemory budget: in each incremental phase, learning is done for new data, most\nof which is abandoned to free space for the next phase. The preserved data are\nexemplars used for replaying. However, existing methods use a static and ad hoc\nstrategy for memory allocation, which is often sub-optimal. In this work, we\npropose a dynamic memory management strategy that is optimized for the\nincremental phases and different object classes. We call our method reinforced\nmemory management (RMM), leveraging reinforcement learning. RMM training is not\nnaturally compatible with CIL as the past, and future data are strictly\nnon-accessible during the incremental phases. We solve this by training the\npolicy function of RMM on pseudo CIL tasks, e.g., the tasks built on the data\nof the 0-th phase, and then applying it to target tasks. RMM propagates two\nlevels of actions: Level-1 determines how to split the memory between old and\nnew classes, and Level-2 allocates memory for each specific class. In essence,\nit is an optimizable and general method for memory management that can be used\nin any replaying-based CIL method. For evaluation, we plug RMM into two\ntop-performing baselines (LUCIR+AANets and POD+AANets [30]) and conduct\nexperiments on three benchmarks (CIFAR-100, ImageNet-Subset, and\nImageNet-Full). Our results show clear improvements, e.g., boosting POD+AANets\nby 3.6%, 4.4%, and 1.9% in the 25-Phase settings of the above benchmarks,\nrespectively.\n", "rewritten_text": "Class-Incremental Learning (CIL) [40] is a method that trains classifiers within a strict memory budget. In each incremental phase, new data is learned, with most of the data being discarded to make room for the next phase. The retained data, known as exemplars, are used for replaying. However, existing methods typically employ a static and ad hoc memory allocation strategy, which is often suboptimal. \n\nIn this study, we introduce a dynamic memory management strategy specifically optimized for incremental phases and different object classes. This strategy, named reinforced memory management (RMM), utilizes reinforcement learning. Integrating RMM training with CIL poses a challenge as past and future data are inaccessible during the incremental phases. To address this, we train the policy function of RMM on pseudo CIL tasks, such as tasks constructed using data from the initial phase, before applying it to target tasks.\n\nRMM involves two levels of actions: Level-1 determines the allocation of memory between old and new classes, while Level-2 allocates memory for each specific class. Essentially, RMM is an adaptable and versatile method for memory management that can be implemented in any replaying-based CIL approach.\n\nTo evaluate the effectiveness of RMM, we incorporate it into two leading baselines (LUCIR+AANets and POD+AANets [30]) and conduct experiments on three benchmarks (CIFAR-100, ImageNet-Subset, and ImageNet-Full). Our results demonstrate significant improvements, such as enhancing POD+AANets performance by 3.6%, 4.4%, and 1.9% in the 25-Phase settings across the aforementioned benchmarks."}, "1711.01362": {"original_text": "  An Unreliable news is any piece of information which is false or misleading,\ndeliberately spread to promote political, ideological and financial agendas.\nRecently the problem of unreliable news has got a lot of attention as the\nnumber instances of using news and social media outlets for propaganda have\nincreased rapidly. This poses a serious threat to society, which calls for\ntechnology to automatically and reliably identify unreliable news sources. This\npaper is an effort made in this direction to build systems for detecting\nunreliable news articles. In this paper, various NLP algorithms were built and\nevaluated on Unreliable News Data 2017 dataset. Variants of hierarchical\nattention networks (HAN) are presented for encoding and classifying news\narticles which achieve the best results of 0.944 ROC-AUC. Finally, Attention\nlayer weights are visualized to understand and give insight into the decisions\nmade by HANs. The results obtained are very promising and encouraging to deploy\nand use these systems in the real world to mitigate the problem of unreliable\nnews.\n", "rewritten_text": "Unreliable news refers to any information that is false or misleading, intentionally disseminated to advance political, ideological, or financial agendas. The issue of unreliable news has garnered significant attention due to the escalating use of news and social media platforms for propaganda. This trend poses a grave threat to society, necessitating the development of technology capable of automatically and accurately identifying unreliable news sources. This paper represents a step in that direction by proposing systems for detecting unreliable news articles. The study involved constructing and assessing various Natural Language Processing (NLP) algorithms using the Unreliable News Data 2017 dataset. Different versions of hierarchical attention networks (HAN) were introduced for encoding and categorizing news articles, achieving outstanding results with a 0.944 ROC-AUC score. Additionally, the visualization of attention layer weights offers insights into the decision-making process of HANs. The promising outcomes obtained suggest the potential for deploying these systems in real-world scenarios to combat the issue of unreliable news effectively."}, "1803.10464": {"original_text": "  The deficiency of segmentation labels is one of the main obstacles to\nsemantic segmentation in the wild. To alleviate this issue, we present a novel\nframework that generates segmentation labels of images given their image-level\nclass labels. In this weakly supervised setting, trained models have been known\nto segment local discriminative parts rather than the entire object area. Our\nsolution is to propagate such local responses to nearby areas which belong to\nthe same semantic entity. To this end, we propose a Deep Neural Network (DNN)\ncalled AffinityNet that predicts semantic affinity between a pair of adjacent\nimage coordinates. The semantic propagation is then realized by random walk\nwith the affinities predicted by AffinityNet. More importantly, the supervision\nemployed to train AffinityNet is given by the initial discriminative part\nsegmentation, which is incomplete as a segmentation annotation but sufficient\nfor learning semantic affinities within small image areas. Thus the entire\nframework relies only on image-level class labels and does not require any\nextra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with\nsegmentation labels generated by our method outperforms previous models trained\nwith the same level of supervision, and is even as competitive as those relying\non stronger supervision.\n", "rewritten_text": "The lack of segmentation labels poses a significant challenge for semantic segmentation in uncontrolled environments. To address this issue, we introduce a novel framework that produces segmentation labels for images based on their image-level class labels. In this weakly supervised scenario, models trained in the past have tended to focus on segmenting specific local features rather than the entire object region. Our approach involves extending these local responses to neighboring areas that are part of the same semantic entity. To achieve this, we introduce a Deep Neural Network (DNN) named AffinityNet, which predicts the semantic affinity between adjacent pairs of image coordinates. The propagation of semantics is then carried out through a random walk using the affinities predicted by AffinityNet. Crucially, the training supervision for AffinityNet is derived from the initial segmentation of discriminative parts, which, while incomplete as a segmentation annotation, is adequate for learning semantic affinities within small image regions. Consequently, the entire framework relies solely on image-level class labels and does not necessitate additional data or annotations. On the PASCAL VOC 2012 dataset, a DNN trained with segmentation labels generated by our method surpasses previous models trained under similar levels of supervision and performs comparably to models relying on more robust supervision."}, "1902.07938": {"original_text": "  Named entity recognition (NER) is an important task in NLP, which is all the\nmore challenging in conversational domain with their noisy facets. Moreover,\nconversational texts are often available in limited amount, making supervised\ntasks infeasible. To learn from small data, strong inductive biases are\nrequired. Previous work relied on hand-crafted features to encode these biases\nuntil transfer learning emerges. Here, we explore a transfer learning method,\nnamely language model pretraining, on NER task in Indonesian conversational\ntexts. We utilize large unlabeled data (generic domain) to be transferred to\nconversational texts, enabling supervised training on limited in-domain data.\nWe report two transfer learning variants, namely supervised model fine-tuning\nand unsupervised pretrained LM fine-tuning. Our experiments show that both\nvariants outperform baseline neural models when trained on small data (100\nsentences), yielding an absolute improvement of 32 points of test F1 score.\nFurthermore, we find that the pretrained LM encodes part-of-speech information\nwhich is a strong predictor for NER.\n", "rewritten_text": "Named entity recognition (NER) is a crucial task in natural language processing (NLP), particularly challenging in conversational domains due to their noisy nature. Conversational texts are often limited in quantity, making supervised tasks impractical. To effectively learn from small datasets, strong inductive biases are necessary. Previous approaches relied on hand-crafted features to incorporate these biases until the emergence of transfer learning. In this study, we investigate a transfer learning technique, specifically language model pretraining, for the NER task in Indonesian conversational texts. By leveraging large amounts of unlabeled data from a generic domain, we transfer knowledge to conversational texts, enabling supervised training on a limited in-domain dataset. We present two transfer learning approaches: supervised model fine-tuning and unsupervised pretrained language model fine-tuning. Our experiments demonstrate that both approaches outperform baseline neural models when trained on a small dataset of 100 sentences, resulting in a significant absolute improvement of 32 points in test F1 score. Additionally, we observe that the pretrained language model encodes part-of-speech information, which serves as a strong predictor for NER."}, "1504.0234": {"original_text": "  In this paper, we focus on the two key aspects of multiple target tracking\nproblem: 1) designing an accurate affinity measure to associate detections and\n2) implementing an efficient and accurate (near) online multiple target\ntracking algorithm. As the first contribution, we introduce a novel Aggregated\nLocal Flow Descriptor (ALFD) that encodes the relative motion pattern between a\npair of temporally distant detections using long term interest point\ntrajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust\naffinity measure for estimating the likelihood of matching detections\nregardless of the application scenarios. As another contribution, we present a\nNear-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is\nformulated as a data-association between targets and detections in a temporal\nwindow, that is performed repeatedly at every frame. While being efficient,\nNOMT achieves robustness via integrating multiple cues including ALFD metric,\ntarget dynamics, appearance similarity, and long term trajectory regularization\ninto the model. Our ablative analysis verifies the superiority of the ALFD\nmetric over the other conventional affinity metrics. We run a comprehensive\nexperimental evaluation on two challenging tracking datasets, KITTI and MOT\ndatasets. The NOMT method combined with ALFD metric achieves the best accuracy\nin both datasets with significant margins (about 10% higher MOTA) over the\nstate-of-the-arts.\n", "rewritten_text": "This paper focuses on two key aspects of the multiple target tracking problem: designing an accurate affinity measure to associate detections and implementing an efficient and accurate (near) online multiple target tracking algorithm. \n\nAs a primary contribution, a novel Aggregated Local Flow Descriptor (ALFD) is introduced to encode the relative motion pattern between a pair of temporally distant detections using long-term interest point trajectories (IPTs). Leveraging the IPTs, the ALFD provides a robust affinity measure for estimating the likelihood of matching detections across various application scenarios. \n\nAdditionally, a Near-Online Multi-target Tracking (NOMT) algorithm is presented. The tracking problem is formulated as a data-association between targets and detections in a temporal window, which is performed repeatedly at every frame. While maintaining efficiency, NOMT achieves robustness by integrating multiple cues, including the ALFD metric, target dynamics, appearance similarity, and long-term trajectory regularization into the model. \n\nAblative analysis confirms the superiority of the ALFD metric over other conventional affinity metrics. Comprehensive experimental evaluations are conducted on two challenging tracking datasets, KITTI and MOT datasets. The NOMT method combined with the ALFD metric demonstrates the highest accuracy in both datasets, surpassing the state-of-the-art methods by significant margins (approximately 10% higher MOTA)."}, "2405.07399": {"original_text": "  Weeds present a significant challenge in agriculture, causing yield loss and\nrequiring expensive control measures. Automatic weed detection using computer\nvision and deep learning offers a promising solution. However, conventional\ndeep learning methods often require large amounts of labelled training data,\nwhich can be costly and time-consuming to acquire. This paper introduces a\nnovel method for semi-supervised weed detection, comprising two main\ncomponents. Firstly, a multi-scale feature representation technique is employed\nto capture distinctive weed features across different scales. Secondly, we\npropose an adaptive pseudo-label assignment strategy, leveraging a small set of\nlabelled images during training. This strategy dynamically assigns confidence\nscores to pseudo-labels generated from unlabeled data. Additionally, our\napproach integrates epoch-corresponding and mixed pseudo-labels to further\nenhance the learning process. Experimental results on the COCO dataset and five\nprominent weed datasets -- CottonWeedDet12, CropAndWeed, Palmer amaranth,\nRadishWheat, and RoboWeedMap -- illustrate that our method achieves\nstate-of-the-art performance in weed detection, even with significantly less\nlabelled data compared to existing techniques. This approach holds the\npotential to alleviate the labelling burden and enhance the feasibility and\ndeployment speed of deep learning for weed detection in real-world agricultural\nscenarios.\n", "rewritten_text": "Weeds pose a significant challenge in agriculture by causing yield loss and necessitating costly control measures. The use of computer vision and deep learning for automatic weed detection presents a promising solution. However, traditional deep learning methods often demand large amounts of labeled training data, which can be expensive and time-consuming to obtain. This paper introduces a new approach for semi-supervised weed detection, consisting of two main components. Firstly, a multi-scale feature representation technique is utilized to capture distinct weed features across various scales. Secondly, an adaptive pseudo-label assignment strategy is proposed, utilizing a small set of labeled images during training. This strategy dynamically assigns confidence scores to pseudo-labels generated from unlabeled data. Furthermore, our method integrates epoch-corresponding and mixed pseudo-labels to further enhance the learning process. Experimental results on the COCO dataset and five prominent weed datasets -- CottonWeedDet12, CropAndWeed, Palmer amaranth, RadishWheat, and RoboWeedMap -- demonstrate that our method achieves state-of-the-art performance in weed detection, even with significantly less labeled data compared to existing techniques. This approach has the potential to reduce the labeling burden and improve the feasibility and deployment speed of deep learning for weed detection in real-world agricultural settings."}, "2204.12785": {"original_text": "  Language models (LMs) have shown great potential as implicit knowledge bases\n(KBs). And for their practical use, knowledge in LMs need to be updated\nperiodically. However, existing tasks to assess LMs' efficacy as KBs do not\nadequately consider multiple large-scale updates. To this end, we first propose\na novel task--Continuously-updated QA (CuQA)--in which multiple large-scale\nupdates are made to LMs, and the performance is measured with respect to the\nsuccess in adding and updating knowledge while retaining existing knowledge. We\nthen present LMs with plug-in modules that effectively handle the updates.\nExperiments conducted on zsRE QA and NQ datasets show that our method\noutperforms existing approaches. We find that our method is 4x more effective\nin terms of updates/forgets ratio, compared to a fine-tuning baseline.\n", "rewritten_text": "Language models (LMs) have demonstrated significant potential as implicit knowledge bases (KBs). For practical applications, it is essential to periodically update the knowledge within LMs. However, current methods for evaluating the effectiveness of LMs as KBs do not adequately account for multiple large-scale updates. To address this gap, we introduce a new task called Continuously-updated QA (CuQA). In CuQA, LMs undergo multiple large-scale updates, and their performance is assessed based on their ability to incorporate new knowledge while maintaining existing knowledge. We also introduce plug-in modules to assist LMs in handling these updates effectively.\n\nExperimental results on zsRE QA and NQ datasets demonstrate that our approach outperforms existing methods. Specifically, our method is found to be 4 times more effective in terms of the updates/forgets ratio compared to a fine-tuning baseline."}, "2406.08215": {"original_text": "  Extractive summarization is a task of highlighting the most important parts\nof the text. We introduce a new approach to extractive summarization task using\nhidden clustering structure of the text. Experimental results on CNN/DailyMail\ndemonstrate that our approach generates more accurate summaries than both\nextractive and abstractive methods, achieving state-of-the-art results in terms\nof ROUGE-2 metric exceeding the previous approaches by 10%. Additionally, we\nshow that hidden structure of the text could be interpreted as aspects.\n", "rewritten_text": "Extractive summarization involves highlighting the most crucial aspects of a text. In this study, we propose a novel method for extractive summarization that leverages the hidden clustering structure within the text. Our experimental findings on the CNN/DailyMail dataset reveal that our approach produces more precise summaries compared to both extractive and abstractive techniques, surpassing previous methods by 10% in terms of the ROUGE-2 metric. Furthermore, we demonstrate that the hidden structure of the text can be interpreted as distinct aspects."}, "1205.3183": {"original_text": "  Existing probabilistic scanners and parsers impose hard constraints on the\nway lexical and syntactic ambiguities can be resolved. Furthermore, traditional\ngrammar-based parsing tools are limited in the mechanisms they allow for taking\ncontext into account. In this paper, we propose a model-driven tool that allows\nfor statistical language models with arbitrary probability estimators. Our work\non model-driven probabilistic parsing is built on top of ModelCC, a model-based\nparser generator, and enables the probabilistic interpretation and resolution\nof anaphoric, cataphoric, and recursive references in the disambiguation of\nabstract syntax graphs. In order to prove the expression power of ModelCC, we\ndescribe the design of a general-purpose natural language parser.\n", "rewritten_text": "In this paper, we introduce a model-driven tool that offers flexibility in resolving lexical and syntactic ambiguities by incorporating statistical language models with various probability estimators. Existing probabilistic scanners and parsers typically have rigid constraints on ambiguity resolution, while traditional grammar-based parsing tools often lack mechanisms to consider context effectively. Our approach, based on ModelCC, a model-based parser generator, facilitates probabilistic parsing by enabling the interpretation and resolution of anaphoric, cataphoric, and recursive references within abstract syntax graphs. To demonstrate the capabilities of ModelCC, we present the design of a versatile natural language parser."}, "1607.06408": {"original_text": "  Action recognition has received increasing attention from the computer vision\nand machine learning communities in the last decade. To enable the study of\nthis problem, there exist a vast number of action datasets, which are recorded\nunder controlled laboratory settings, real-world surveillance environments, or\ncrawled from the Internet. Apart from the \"in-the-wild\" datasets, the training\nand test split of conventional datasets often possess similar environments\nconditions, which leads to close to perfect performance on constrained\ndatasets. In this paper, we introduce a new dataset, namely Multi-Camera Action\nDataset (MCAD), which is designed to evaluate the open view classification\nproblem under the surveillance environment. In total, MCAD contains 14,298\naction samples from 18 action categories, which are performed by 20 subjects\nand independently recorded with 5 cameras. Inspired by the well received\nevaluation approach on the LFW dataset, we designed a standard evaluation\nprotocol and benchmarked MCAD under several scenarios. The benchmark shows that\nwhile an average of 85% accuracy is achieved under the closed-view scenario,\nthe performance suffers from a significant drop under the cross-view scenario.\nIn the worst case scenario, the performance of 10-fold cross validation drops\nfrom 87.0% to 47.4%.\n", "rewritten_text": "Over the past decade, action recognition has garnered increasing attention from the computer vision and machine learning communities. To facilitate research in this area, a wide range of action datasets have been developed. These datasets are collected in controlled laboratory settings, real-world surveillance environments, or sourced from the Internet. While traditional datasets typically exhibit similar environmental conditions in their training and test splits, resulting in near-perfect performance on constrained datasets, there is a growing need for more challenging \"in-the-wild\" datasets.\n\nThis paper introduces a new dataset called the Multi-Camera Action Dataset (MCAD), specifically designed to address the open-view classification problem within a surveillance environment. MCAD comprises 14,298 action samples across 18 action categories, performed by 20 subjects and captured independently by 5 cameras. Drawing inspiration from the evaluation methodology used for the LFW dataset, a standard evaluation protocol was devised to benchmark MCAD across various scenarios.\n\nThe benchmark results reveal that while the closed-view scenario achieves an average accuracy of 85%, performance significantly declines in the cross-view scenario. In the most challenging scenario, the accuracy drops from 87.0% to 47.4% in a 10-fold cross-validation setup."}, "2007.06233": {"original_text": "  In the majority of object detection frameworks, the confidence of instance\nclassification is used as the quality criterion of predicted bounding boxes,\nlike the confidence-based ranking in non-maximum suppression (NMS). However,\nthe quality of bounding boxes, indicating the spatial relations, is not only\ncorrelated with the classification scores. Compared with the region proposal\nnetwork (RPN) based detectors, single-shot object detectors suffer the box\nquality as there is a lack of pre-selection of box proposals. In this paper, we\naim at single-shot object detectors and propose a location-aware anchor-based\nreasoning (LAAR) for the bounding boxes. LAAR takes both the location and\nclassification confidences into consideration for the quality evaluation of\nbounding boxes. We introduce a novel network block to learn the relative\nlocation between the anchors and the ground truths, denoted as a localization\nscore, which acts as a location reference during the inference stage. The\nproposed localization score leads to an independent regression branch and\ncalibrates the bounding box quality by scoring the predicted localization score\nso that the best-qualified bounding boxes can be picked up in NMS. Experiments\non MS COCO and PASCAL VOC benchmarks demonstrate that the proposed\nlocation-aware framework enhances the performances of current anchor-based\nsingle-shot object detection frameworks and yields consistent and robust\ndetection results.\n", "rewritten_text": "In most object detection frameworks, the confidence of instance classification serves as the primary quality criterion for predicted bounding boxes, such as the confidence-based ranking in non-maximum suppression (NMS). However, the quality of bounding boxes, which indicates spatial relations, is not solely dependent on classification scores. Single-shot object detectors, in contrast to region proposal network (RPN) based detectors, face challenges in box quality due to the absence of pre-selection of box proposals. This paper focuses on single-shot object detectors and introduces a location-aware anchor-based reasoning (LAAR) approach for bounding boxes. LAAR considers both location and classification confidences in evaluating bounding box quality. A novel network block is proposed to learn the relative location between anchors and ground truths, referred to as a localization score, which serves as a location reference during the inference stage. The introduced localization score leads to an independent regression branch and adjusts bounding box quality by scoring the predicted localization score, enabling the selection of the best-qualified bounding boxes in NMS. Experiments conducted on MS COCO and PASCAL VOC benchmarks demonstrate that the proposed location-aware framework enhances the performance of current anchor-based single-shot object detection frameworks, resulting in consistent and robust detection outcomes."}, "2308.09475": {"original_text": "  Robot-assisted surgery has made significant progress, with instrument\nsegmentation being a critical factor in surgical intervention quality. It\nserves as the building block to facilitate surgical robot navigation and\nsurgical education for the next generation of operating intelligence. Although\nexisting methods have achieved accurate instrument segmentation results, they\nsimultaneously generate segmentation masks for all instruments, without the\ncapability to specify a target object and allow an interactive experience. This\nwork explores a new task of Referring Surgical Video Instrument Segmentation\n(RSVIS), which aims to automatically identify and segment the corresponding\nsurgical instruments based on the given language expression. To achieve this,\nwe devise a novel Video-Instrument Synergistic Network (VIS-Net) to learn both\nvideo-level and instrument-level knowledge to boost performance, while previous\nwork only used video-level information. Meanwhile, we design a Graph-based\nRelation-aware Module (GRM) to model the correlation between multi-modal\ninformation (i.e., textual description and video frame) to facilitate the\nextraction of instrument-level information. We are also the first to produce\ntwo RSVIS datasets to promote related research. Our method is verified on these\ndatasets, and experimental results exhibit that the VIS-Net can significantly\noutperform existing state-of-the-art referring segmentation methods. Our code\nand our datasets will be released upon the publication of this work.\n", "rewritten_text": "Robot-assisted surgery has made significant progress, with instrument segmentation playing a crucial role in ensuring the quality of surgical interventions. It serves as the foundation for enhancing surgical robot navigation and advancing surgical education for the next generation of operating intelligence. While current methods have achieved precise instrument segmentation results, they lack the ability to target specific objects and provide an interactive experience. This study introduces a new task called Referring Surgical Video Instrument Segmentation (RSVIS), which aims to automatically identify and segment surgical instruments based on provided language expressions. To accomplish this, a novel Video-Instrument Synergistic Network (VIS-Net) is developed to incorporate both video-level and instrument-level knowledge, unlike previous approaches that only utilize video-level information. Additionally, a Graph-based Relation-aware Module (GRM) is designed to capture the correlation between different modalities (textual descriptions and video frames) to enhance the extraction of instrument-level information. Furthermore, two RSVIS datasets are created to support further research in this area. The effectiveness of our method is demonstrated on these datasets, showing that VIS-Net significantly outperforms existing state-of-the-art referring segmentation methods. The code and datasets will be made available upon the publication of this work."}, "1401.119": {"original_text": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n", "rewritten_text": "Extracting and recognizing Bangla text from video frame images poses a challenge due to factors such as complex color backgrounds and low resolution. In this study, we present an algorithm designed for the extraction and recognition of Bangla text from video frames featuring complex backgrounds. Our approach involves a two-step process. Initially, the text line is segmented into words utilizing information derived from line contours. The first-order gradient value of the text blocks is then utilized to identify word gaps. Subsequently, a local binarization technique is applied to each word, and the text line is reconstructed using these words. In the second step, the binarized text block is passed to an Optical Character Recognition (OCR) system for recognition purposes."}, "1502.06807": {"original_text": "  We introduce and evaluate several architectures for Convolutional Neural\nNetworks to predict the 3D joint locations of a hand given a depth map. We\nfirst show that a prior on the 3D pose can be easily introduced and\nsignificantly improves the accuracy and reliability of the predictions. We also\nshow how to use context efficiently to deal with ambiguities between fingers.\nThese two contributions allow us to significantly outperform the\nstate-of-the-art on several challenging benchmarks, both in terms of accuracy\nand computation times.\n", "rewritten_text": "We present and assess multiple architectures for Convolutional Neural Networks designed to predict the 3D joint locations of a hand based on a depth map. Initially, we demonstrate that incorporating a prior on the 3D pose can be seamlessly achieved, leading to a substantial enhancement in the accuracy and dependability of the predictions. Additionally, we illustrate an efficient method for leveraging context to effectively address ambiguities related to finger positioning. These dual contributions enable us to surpass the current state-of-the-art performance on various demanding benchmarks, excelling in both accuracy and computational efficiency."}, "1812.01855": {"original_text": "  We aim to dismantle the prevalent black-box neural architectures used in\ncomplex visual reasoning tasks, into the proposed eXplainable and eXplicit\nNeural Modules (XNMs), which advance beyond existing neural module networks\ntowards using scene graphs --- objects as nodes and the pairwise relationships\nas edges --- for explainable and explicit reasoning with structured knowledge.\nXNMs allow us to pay more attention to teach machines how to \"think\",\nregardless of what they \"look\". As we will show in the paper, by using scene\ngraphs as an inductive bias, 1) we can design XNMs in a concise and flexible\nfashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce\nthe number of parameters by 10 to 100 times, and 2) we can explicitly trace the\nreasoning-flow in terms of graph attentions. XNMs are so generic that they\nsupport a wide range of scene graph implementations with various qualities. For\nexample, when the graphs are detected perfectly, XNMs achieve 100% accuracy on\nboth CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound\nfor visual reasoning; when the graphs are noisily detected from real-world\nimages, XNMs are still robust to achieve a competitive 67.5% accuracy on\nVQAv2.0, surpassing the popular bag-of-objects attention models without graph\nstructures.\n", "rewritten_text": "Our goal is to deconstruct the commonly used black-box neural architectures in complex visual reasoning tasks and introduce the eXplainable and eXplicit Neural Modules (XNMs). These modules go beyond existing neural module networks by incorporating scene graphs, where objects are represented as nodes and their relationships as edges. This approach enables explainable and explicit reasoning based on structured knowledge. XNMs allow us to focus on teaching machines how to \"think\" rather than how they \"look\". By leveraging scene graphs as an inductive bias, we demonstrate in our paper that: 1) XNMs can be designed concisely and flexibly, with only 4 meta-types, leading to a significant reduction in parameters by 10 to 100 times; and 2) the reasoning flow can be explicitly traced through graph attentions. XNMs are highly versatile, supporting a wide range of scene graph implementations with varying qualities. For instance, when the graphs are perfectly detected, XNMs achieve 100% accuracy on both CLEVR and CLEVR CoGenT, setting an empirical performance upper bound for visual reasoning. Even when the graphs are detected noisily from real-world images, XNMs remain robust and achieve a competitive 67.5% accuracy on VQAv2.0, outperforming popular bag-of-objects attention models that lack graph structures."}, "2401.03340": {"original_text": "  This paper introduces the CowStallNumbers dataset, a collection of images\nextracted from videos focusing on cow teats, designed to advance the field of\ncow stall number detection. The dataset comprises 1042 training images and 261\ntest images, featuring stall numbers ranging from 0 to 60. To enhance the\ndataset, we performed fine-tuning on a YOLO model and applied data augmentation\ntechniques, including random crop, center crop, and random rotation. The\nexperimental outcomes demonstrate a notable 95.4\\% accuracy in recognizing\nstall numbers.\n", "rewritten_text": "This paper presents the CowStallNumbers dataset, which consists of images extracted from videos that specifically focus on cow teats. The dataset is intended to advance the field of cow stall number detection and includes 1042 training images and 261 test images. These images showcase stall numbers ranging from 0 to 60. To improve the dataset, fine-tuning was conducted on a YOLO model, and various data augmentation techniques such as random crop, center crop, and random rotation were applied. The experimental results show a significant 95.4% accuracy in identifying stall numbers."}, "2307.05158": {"original_text": "  Predicting where a person is looking is a complex task, requiring to\nunderstand not only the person's gaze and scene content, but also the 3D scene\nstructure and the person's situation (are they manipulating? interacting or\nobserving others? attentive?) to detect obstructions in the line of sight or\napply attention priors that humans typically have when observing others. In\nthis paper, we hypothesize that identifying and leveraging such priors can be\nbetter achieved through the exploitation of explicitly derived multimodal cues\nsuch as depth and pose. We thus propose a modular multimodal architecture\nallowing to combine these cues using an attention mechanism. The architecture\ncan naturally be exploited in privacy-sensitive situations such as surveillance\nand health, where personally identifiable information cannot be released. We\nperform extensive experiments on the GazeFollow and VideoAttentionTarget public\ndatasets, obtaining state-of-the-art performance and demonstrating very\ncompetitive results in the privacy setting case.\n", "rewritten_text": "Predicting where a person is looking is a complex task that requires an understanding of not only the person's gaze and scene content, but also the 3D scene structure and the person's situation (such as whether they are manipulating, interacting, or observing others and their level of attentiveness). This understanding is crucial for detecting obstructions in the line of sight or applying attention priors that humans typically have when observing others. In this paper, we propose that identifying and leveraging such priors can be more effectively achieved by exploiting explicitly derived multimodal cues, such as depth and pose.\n\nTo this end, we introduce a modular multimodal architecture that allows for the combination of these cues using an attention mechanism. This architecture can be particularly useful in privacy-sensitive situations, such as surveillance and health applications, where personally identifiable information cannot be disclosed. We conducted extensive experiments on the GazeFollow and VideoAttentionTarget public datasets, achieving state-of-the-art performance and demonstrating highly competitive results in the privacy setting."}, "2302.07027": {"original_text": "  Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.\n", "rewritten_text": "Pretrained language models (PLMs) are trained on massive corpora, but often require specialization for specific domains. A parameter-efficient adaptation method proposes training an adapter for each domain in the context of language modeling. While this approach yields high in-domain scores, it may not be practical in domain- or resource-constrained scenarios. To address this challenge, a viable solution involves utilizing a related-domain adapter for a new domain during testing. \n\nIn this paper, we introduce AdapterSoup, a novel approach that involves weight-space averaging of adapters trained across different domains. Our method is embarrassingly parallel: initially, a set of domain-specific adapters is trained, followed by determining which adapters should be averaged for each new domain during testing. Through extensive experiments, we demonstrate that AdapterSoup consistently enhances performance on new domains without requiring additional training. \n\nFurthermore, we investigate the efficacy of averaging adapters trained on the same domain but with varying hyperparameters. Our findings indicate that this approach maintains the performance of a PLM on new domains while achieving robust in-domain results. We explore different strategies for selecting which adapters to combine, including text clustering and semantic similarity. Our results show that employing clustering yields the most competitive outcomes on novel domains."}, "1706.09147": {"original_text": "  We address the task of Named Entity Disambiguation (NED) for noisy text. We\npresent WikilinksNED, a large-scale NED dataset of text fragments from the web,\nwhich is significantly noisier and more challenging than existing news-based\ndatasets. To capture the limited and noisy local context surrounding each\nmention, we design a neural model and train it with a novel method for sampling\ninformative negative examples. We also describe a new way of initializing word\nand entity embeddings that significantly improves performance. Our model\nsignificantly outperforms existing state-of-the-art methods on WikilinksNED\nwhile achieving comparable performance on a smaller newswire dataset.\n", "rewritten_text": "We focus on Named Entity Disambiguation (NED) in noisy text by introducing WikilinksNED, a large-scale dataset comprising text fragments from the web. This dataset is notably noisier and more challenging compared to existing news-based datasets. In order to capture the limited and noisy local context surrounding each entity mention, we have developed a neural model and trained it using a unique method for sampling informative negative examples. Additionally, we have devised a novel approach for initializing word and entity embeddings, resulting in a significant improvement in performance. Our model demonstrates superior performance on the WikilinksNED dataset compared to existing state-of-the-art methods, while also achieving comparable results on a smaller newswire dataset."}, "2312.16274": {"original_text": "  Recent progress in multi-modal conditioned face synthesis has enabled the\ncreation of visually striking and accurately aligned facial images. Yet,\ncurrent methods still face issues with scalability, limited flexibility, and a\none-size-fits-all approach to control strength, not accounting for the\ndiffering levels of conditional entropy, a measure of unpredictability in data\ngiven some condition, across modalities. To address these challenges, we\nintroduce a novel uni-modal training approach with modal surrogates, coupled\nwith an entropy-aware modal-adaptive modulation, to support flexible, scalable,\nand scalable multi-modal conditioned face synthesis network. Our uni-modal\ntraining with modal surrogate that only leverage uni-modal data, use modal\nsurrogate to decorate condition with modal-specific characteristic and serve as\nlinker for inter-modal collaboration , fully learns each modality control in\nface synthesis process as well as inter-modal collaboration. The entropy-aware\nmodal-adaptive modulation finely adjust diffusion noise according to\nmodal-specific characteristics and given conditions, enabling well-informed\nstep along denoising trajectory and ultimately leading to synthesis results of\nhigh fidelity and quality. Our framework improves multi-modal face synthesis\nunder various conditions, surpassing current methods in image quality and\nfidelity, as demonstrated by our thorough experimental results.\n", "rewritten_text": "Recent advancements in multi-modal conditioned face synthesis have allowed for the creation of visually striking and accurately aligned facial images. However, current methods are still facing challenges related to scalability, limited flexibility, and a one-size-fits-all approach to controlling strength, without considering the varying levels of conditional entropy \u2013 a measure of unpredictability in data given certain conditions \u2013 across different modalities. To tackle these issues, we propose a novel uni-modal training approach utilizing modal surrogates, combined with an entropy-aware modal-adaptive modulation. This approach aims to support a flexible, scalable, and efficient multi-modal conditioned face synthesis network.\n\nOur uni-modal training approach with modal surrogates leverages only uni-modal data, using modal surrogates to enhance conditions with modal-specific characteristics and act as a bridge for inter-modal collaboration. This method fully comprehends each modality's control in the face synthesis process, as well as the collaboration between different modalities. The entropy-aware modal-adaptive modulation finely adjusts diffusion noise based on modal-specific characteristics and given conditions, facilitating informed steps along the denoising trajectory and ultimately resulting in high-fidelity and high-quality synthesis outcomes.\n\nOur framework enhances multi-modal face synthesis across various conditions, outperforming current methods in terms of image quality and fidelity, as evidenced by our comprehensive experimental results."}, "2407.06938": {"original_text": "  We present RodinHD, which can generate high-fidelity 3D avatars from a\nportrait image. Existing methods fail to capture intricate details such as\nhairstyles which we tackle in this paper. We first identify an overlooked\nproblem of catastrophic forgetting that arises when fitting triplanes\nsequentially on many avatars, caused by the MLP decoder sharing scheme. To\novercome this issue, we raise a novel data scheduling strategy and a weight\nconsolidation regularization term, which improves the decoder's capability of\nrendering sharper details. Additionally, we optimize the guiding effect of the\nportrait image by computing a finer-grained hierarchical representation that\ncaptures rich 2D texture cues, and injecting them to the 3D diffusion model at\nmultiple layers via cross-attention. When trained on 46K avatars with a noise\nschedule optimized for triplanes, the resulting model can generate 3D avatars\nwith notably better details than previous methods and can generalize to\nin-the-wild portrait input.\n", "rewritten_text": "Introducing RodinHD, a cutting-edge technology capable of producing high-fidelity 3D avatars from a single portrait image. Unlike existing methods, our approach excels at capturing intricate details, including hairstyles, a challenge we address in this study. We have identified a previously overlooked issue of catastrophic forgetting that occurs when fitting triplanes sequentially to numerous avatars, stemming from the shared scheme of the MLP decoder. To combat this problem, we introduce a novel data scheduling strategy and a weight consolidation regularization term, enhancing the decoder's ability to render finer details. Furthermore, we enhance the guidance provided by the portrait image by creating a more detailed hierarchical representation that captures rich 2D texture cues. These cues are then integrated into the 3D diffusion model at various layers through cross-attention mechanisms. By training on a dataset of 46,000 avatars with a noise schedule optimized for triplanes, our model produces 3D avatars with significantly improved details compared to previous methods. Importantly, our model demonstrates the ability to generalize to diverse portrait inputs encountered in real-world scenarios."}, "2311.08107": {"original_text": "  Large Language Models (LLMs) can justify or critique their predictions\nthrough discussions with other models or humans, thereby enriching their\nintrinsic understanding of instances. While proactive discussions in the\ninference phase have been shown to boost performance, such interactions have\nnot been extensively explored during the training phase. We hypothesize that\nincorporating interactive discussions into the training process can enhance the\nmodels' understanding and improve their reasoning and verbal expression\nabilities during inference. This work introduces the SAIE framework, which\nfacilitates supportive and adversarial discussions between learner and partner\nmodels. The learner model receives responses from the partner, and its\nparameters are then updated based on this discussion. This dynamic adjustment\nprocess continues throughout the training phase, responding to the evolving\noutputs of the learner model. Our empirical evaluation across various tasks,\nincluding math problems, commonsense reasoning, and multi-domain knowledge,\ndemonstrates that models fine-tuned with the SAIE framework outperform those\ntrained with conventional fine-tuning approaches. Furthermore, our method\nenhances the models' reasoning capabilities, improving both individual and\nmulti-agent inference performance.\n", "rewritten_text": "Large Language Models (LLMs) can justify or critique their predictions by engaging in discussions with other models or humans, thereby enhancing their intrinsic understanding of specific instances. While proactive discussions during the inference phase have been proven to enhance performance, the exploration of such interactions during the training phase has been limited. We propose that integrating interactive discussions into the training process can improve the models' comprehension and enhance their reasoning and verbal expression skills during inference.\n\nThis study introduces the SAIE framework, which enables supportive and adversarial discussions between the learner model and partner models. The learner model receives feedback from the partner, and its parameters are adjusted based on these discussions. This dynamic adjustment process continues throughout the training phase, adapting to the changing outputs of the learner model.\n\nOur empirical evaluation across various tasks, such as math problems, commonsense reasoning, and multi-domain knowledge, shows that models fine-tuned using the SAIE framework outperform those trained with traditional fine-tuning methods. Additionally, our approach enhances the models' reasoning abilities, leading to improved performance in both individual and multi-agent inference tasks."}, "1704.05831": {"original_text": "  We propose a hierarchical approach for making long-term predictions of future\nframes. To avoid inherent compounding errors in recursive pixel-level\nprediction, we propose to first estimate high-level structure in the input\nframes, then predict how that structure evolves in the future, and finally by\nobserving a single frame from the past and the predicted high-level structure,\nwe construct the future frames without having to observe any of the pixel-level\npredictions. Long-term video prediction is difficult to perform by recurrently\nobserving the predicted frames because the small errors in pixel space\nexponentially amplify as predictions are made deeper into the future. Our\napproach prevents pixel-level error propagation from happening by removing the\nneed to observe the predicted frames. Our model is built with a combination of\nLSTM and analogy based encoder-decoder convolutional neural networks, which\nindependently predict the video structure and generate the future frames,\nrespectively. In experiments, our model is evaluated on the Human3.6M and Penn\nAction datasets on the task of long-term pixel-level video prediction of humans\nperforming actions and demonstrate significantly better results than the\nstate-of-the-art.\n", "rewritten_text": "We propose a hierarchical approach for making long-term predictions of future frames. To avoid compounding errors inherent in recursive pixel-level prediction, we suggest first estimating the high-level structure in the input frames. Next, we predict how that structure evolves in the future. Finally, by observing a single frame from the past and the predicted high-level structure, we construct the future frames without needing to observe any pixel-level predictions. Long-term video prediction is challenging when relying on recurrently observing predicted frames, as small errors in pixel space exponentially amplify with deeper future predictions. Our approach prevents the propagation of pixel-level errors by eliminating the need to observe predicted frames. Our model combines LSTM and analogy-based encoder-decoder convolutional neural networks, which independently predict the video structure and generate the future frames, respectively. In experiments, we evaluate our model on the Human3.6M and Penn Action datasets for the task of long-term pixel-level video prediction of humans performing actions. The results demonstrate significant improvements over the state-of-the-art methods."}, "1606.05706": {"original_text": "  We study the problem of agreement and disagreement detection in online\ndiscussions. An isotonic Conditional Random Fields (isotonic CRF) based\nsequential model is proposed to make predictions on sentence- or segment-level.\nWe automatically construct a socially-tuned lexicon that is bootstrapped from\nexisting general-purpose sentiment lexicons to further improve the performance.\nWe evaluate our agreement and disagreement tagging model on two disparate\nonline discussion corpora -- Wikipedia Talk pages and online debates. Our model\nis shown to outperform the state-of-the-art approaches in both datasets. For\nexample, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for\nagreement and disagreement detection, when a linear chain CRF obtains 0.58 and\n0.56 for the discussions on Wikipedia Talk pages.\n", "rewritten_text": "We address the issue of detecting agreement and disagreement in online discussions by proposing an isotonic Conditional Random Fields (isotonic CRF) based sequential model for making predictions at the sentence- or segment-level. To enhance performance, we automatically generate a socially-tuned lexicon derived from existing general-purpose sentiment lexicons. Our agreement and disagreement tagging model is evaluated on two distinct online discussion corpora: Wikipedia Talk pages and online debates. Results show that our model surpasses state-of-the-art approaches in both datasets. For instance, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for agreement and disagreement detection, outperforming a linear chain CRF which obtains 0.58 and 0.56 for discussions on Wikipedia Talk pages."}, "2401.13011": {"original_text": "  This paper presents a novel generative model, Collaborative Competitive\nAgents (CCA), which leverages the capabilities of multiple Large Language\nModels (LLMs) based agents to execute complex tasks. Drawing inspiration from\nGenerative Adversarial Networks (GANs), the CCA system employs two equal-status\ngenerator agents and a discriminator agent. The generators independently\nprocess user instructions and generate results, while the discriminator\nevaluates the outputs, and provides feedback for the generator agents to\nfurther reflect and improve the generation results. Unlike the previous\ngenerative model, our system can obtain the intermediate steps of generation.\nThis allows each generator agent to learn from other successful executions due\nto its transparency, enabling a collaborative competition that enhances the\nquality and robustness of the system's results. The primary focus of this study\nis image editing, demonstrating the CCA's ability to handle intricate\ninstructions robustly. The paper's main contributions include the introduction\nof a multi-agent-based generative model with controllable intermediate steps\nand iterative optimization, a detailed examination of agent relationships, and\ncomprehensive experiments on image editing. Code is available at\n\\href{https://github.com/TiankaiHang/CCA}{https://github.com/TiankaiHang/CCA}.\n", "rewritten_text": "This paper introduces a novel generative model called Collaborative Competitive Agents (CCA), which harnesses the capabilities of multiple Large Language Models (LLMs) to perform complex tasks. Inspired by Generative Adversarial Networks (GANs), the CCA system consists of two generator agents and a discriminator agent of equal status. The generators independently process user instructions and generate results, while the discriminator evaluates the outputs and provides feedback to the generator agents for further refinement. Unlike previous generative models, our system allows for the capture of intermediate generation steps. This transparency enables each generator agent to learn from successful executions by others, fostering a collaborative competition that enhances the quality and robustness of the system's results. The primary focus of this study is on image editing, showcasing the CCA's ability to handle intricate instructions effectively. Key contributions of the paper include the introduction of a multi-agent-based generative model with controllable intermediate steps and iterative optimization, a detailed exploration of agent relationships, and comprehensive experiments on image editing. The code for the system is available at \\href{https://github.com/TiankaiHang/CCA}{https://github.com/TiankaiHang/CCA}."}, "2309.08644": {"original_text": "  3D pose estimation is an invaluable task in computer vision with various\npractical applications. Especially, 3D pose estimation for multi-person from a\nmonocular video (3DMPPE) is particularly challenging and is still largely\nuncharted, far from applying to in-the-wild scenarios yet. We pose three\nunresolved issues with the existing methods: lack of robustness on unseen views\nduring training, vulnerability to occlusion, and severe jittering in the\noutput. As a remedy, we propose POTR-3D, the first realization of a\nsequence-to-sequence 2D-to-3D lifting model for 3DMPPE, powered by a novel\ngeometry-aware data augmentation strategy, capable of generating unbounded data\nwith a variety of views while caring about the ground plane and occlusions.\nThrough extensive experiments, we verify that the proposed model and data\naugmentation robustly generalizes to diverse unseen views, robustly recovers\nthe poses against heavy occlusions, and reliably generates more natural and\nsmoother outputs. The effectiveness of our approach is verified not only by\nachieving the state-of-the-art performance on public benchmarks, but also by\nqualitative results on more challenging in-the-wild videos. Demo videos are\navailable at https://www.youtube.com/@potr3d.\n", "rewritten_text": "3D pose estimation is a crucial task in computer vision with numerous practical applications. Specifically, 3D pose estimation for multiple individuals from a monocular video (3DMPPE) presents significant challenges and remains largely unexplored in real-world scenarios. We identify three unresolved issues with current methods: a lack of robustness when faced with unseen views during training, susceptibility to occlusion, and noticeable jittering in the output. To address these challenges, we introduce POTR-3D, the first implementation of a sequence-to-sequence 2D-to-3D lifting model for 3DMPPE. This model is enhanced by a novel geometry-aware data augmentation strategy that can generate diverse data with various views, taking into account the ground plane and occlusions. Extensive experiments demonstrate that our proposed model and data augmentation approach effectively generalize to unseen views, successfully recover poses in the presence of heavy occlusions, and produce more natural and smoother outputs. Our approach not only achieves state-of-the-art performance on public benchmarks but also demonstrates qualitative results on more challenging real-world videos. Demo videos showcasing our work can be found at https://www.youtube.com/@potr3d."}, "2008.08735": {"original_text": "  Computer vision (CV) has achieved great success in interpreting semantic\nmeanings from images, yet CV algorithms can be brittle for tasks with adverse\nvision conditions and the ones suffering from data/label pair limitation. One\nof this tasks is in-bed human pose estimation, which has significant values in\nmany healthcare applications. In-bed pose monitoring in natural settings could\ninvolve complete darkness or full occlusion. Furthermore, the lack of publicly\navailable in-bed pose datasets hinders the use of many successful pose\nestimation algorithms for this task. In this paper, we introduce our\nSimultaneously-collected multimodal Lying Pose (SLP) dataset, which includes\nin-bed pose images from 109 participants captured using multiple imaging\nmodalities including RGB, long wave infrared, depth, and pressure map. We also\npresent a physical hyper parameter tuning strategy for ground truth pose label\ngeneration under extreme conditions such as lights off and being fully covered\nby a sheet/blanket. SLP design is compatible with the mainstream human pose\ndatasets, therefore, the state-of-the-art 2D pose estimation models can be\ntrained effectively with SLP data with promising performance as high as 95% at\nPCKh@0.5 on a single modality. The pose estimation performance can be further\nimproved by including additional modalities through collaboration.\n", "rewritten_text": "Computer vision (CV) has made significant strides in extracting semantic meanings from images. However, CV algorithms can be fragile when faced with challenging vision conditions or limited data/label pairs. One such challenging task is in-bed human pose estimation, which holds great importance in various healthcare applications. Monitoring poses in bed in real-world scenarios may involve scenarios like complete darkness or full occlusion. Additionally, the scarcity of publicly available in-bed pose datasets impedes the application of successful pose estimation algorithms to this task.\n\nIn this study, we introduce the Simultaneously-collected multimodal Lying Pose (SLP) dataset, comprising in-bed pose images from 109 participants captured using multiple imaging modalities such as RGB, long wave infrared, depth, and pressure map. We also propose a physical hyperparameter tuning strategy for generating ground truth pose labels under extreme conditions, such as darkness and full coverage by a sheet/blanket. The SLP design is compatible with mainstream human pose datasets, enabling effective training of state-of-the-art 2D pose estimation models using SLP data, achieving promising performance levels of up to 95% at PCKh@0.5 on a single modality. Furthermore, incorporating additional modalities through collaboration can further enhance pose estimation performance."}, "2206.04846": {"original_text": "  Deep neural networks are capable of learning powerful representations to\ntackle complex vision tasks but expose undesirable properties like the\nover-fitting issue. To this end, regularization techniques like image\naugmentation are necessary for deep neural networks to generalize well.\nNevertheless, most prevalent image augmentation recipes confine themselves to\noff-the-shelf linear transformations like scale, flip, and colorjitter. Due to\ntheir hand-crafted property, these augmentations are insufficient to generate\ntruly hard augmented examples. In this paper, we propose a novel perspective of\naugmentation to regularize the training process. Inspired by the recent success\nof applying masked image modeling to self-supervised learning, we adopt the\nself-supervised masked autoencoder to generate the distorted view of the input\nimages. We show that utilizing such model-based nonlinear transformation as\ndata augmentation can improve high-level recognition tasks. We term the\nproposed method as \\textbf{M}ask-\\textbf{R}econstruct \\textbf{A}ugmentation\n(MRA). The extensive experiments on various image classification benchmarks\nverify the effectiveness of the proposed augmentation. Specifically, MRA\nconsistently enhances the performance on supervised, semi-supervised as well as\nfew-shot classification. The code will be available at\n\\url{https://github.com/haohang96/MRA}.\n", "rewritten_text": "Deep neural networks have the ability to learn powerful representations for addressing complex vision tasks, but they also exhibit undesirable properties such as the issue of over-fitting. In order to address this, regularization techniques like image augmentation are essential for deep neural networks to generalize effectively. However, many common image augmentation methods are limited to basic linear transformations like scaling, flipping, and color jittering. These hand-crafted augmentations may not be sufficient to create truly challenging augmented examples.\n\nIn this paper, we introduce a new perspective on augmentation to enhance the training process. Drawing inspiration from the success of applying masked image modeling in self-supervised learning, we propose the use of a self-supervised masked autoencoder to generate distorted versions of input images. We demonstrate that incorporating such model-based nonlinear transformations as data augmentation can lead to improvements in high-level recognition tasks. Our proposed method is named Mask-Reconstruct Augmentation (MRA).\n\nExtensive experiments conducted on various image classification benchmarks validate the effectiveness of the proposed augmentation technique. Specifically, MRA consistently boosts performance in supervised, semi-supervised, and few-shot classification scenarios. The code for implementing MRA can be accessed at \\url{https://github.com/haohang96/MRA}."}, "1803.10547": {"original_text": "  Text articles with false claims, especially news, have recently become\naggravating for the Internet users. These articles are in wide circulation and\nreaders face difficulty discerning fact from fiction. Previous work on\ncredibility assessment has focused on factual analysis and linguistic features.\nThe task's main challenge is the distinction between the features of true and\nfalse articles. In this paper, we propose a novel approach called Credibility\nOutcome (CREDO) which aims at scoring the credibility of an article in an open\ndomain setting.\n  CREDO consists of different modules for capturing various features\nresponsible for the credibility of an article. These features includes\ncredibility of the article's source and author, semantic similarity between the\narticle and related credible articles retrieved from a knowledge base, and\nsentiments conveyed by the article. A neural network architecture learns the\ncontribution of each of these modules to the overall credibility of an article.\nExperiments on Snopes dataset reveals that CREDO outperforms the\nstate-of-the-art approaches based on linguistic features.\n", "rewritten_text": "Recently, text articles containing false claims, particularly news articles, have become increasingly frustrating for Internet users. These articles are widely circulated, making it challenging for readers to distinguish between fact and fiction. Previous research on credibility assessment has primarily focused on factual analysis and linguistic features, with the main challenge being the differentiation between true and false articles. In this paper, we introduce a novel approach called Credibility Outcome (CREDO), designed to evaluate the credibility of articles in an open domain setting.\n\nCREDO comprises various modules that capture different features influencing an article's credibility. These features include the credibility of the article's source and author, the semantic similarity between the article and credible articles obtained from a knowledge base, and the sentiments expressed in the article. A neural network architecture is utilized to learn the impact of each module on the overall credibility of an article. Experiments conducted on the Snopes dataset demonstrate that CREDO surpasses state-of-the-art approaches based on linguistic features."}, "1704.01926": {"original_text": "  This paper tackles the problem of semi-supervised video object segmentation,\nthat is, segmenting an object in a sequence given its mask in the first frame.\nOne of the main challenges in this scenario is the change of appearance of the\nobjects of interest. Their semantics, on the other hand, do not vary. This\npaper investigates how to take advantage of such invariance via the\nintroduction of a semantic prior that guides the appearance model.\nSpecifically, given the segmentation mask of the first frame of a sequence, we\nestimate the semantics of the object of interest, and propagate that knowledge\nthroughout the sequence to improve the results based on an appearance model. We\npresent Semantically-Guided Video Object Segmentation (SGV), which improves\nresults over previous state of the art on two different datasets using a\nvariety of evaluation metrics, while running in half a second per frame.\n", "rewritten_text": "This paper addresses the issue of semi-supervised video object segmentation, which involves segmenting an object in a sequence based on its mask in the initial frame. A key challenge in this scenario is the changing appearance of the objects of interest, while their semantics remain consistent. The study explores how to leverage this semantic invariance by introducing a semantic prior that informs the appearance model. Specifically, by utilizing the segmentation mask from the first frame of a sequence, we estimate the semantics of the object of interest and propagate this information throughout the sequence to enhance results using an appearance model. The proposed approach, Semantically-Guided Video Object Segmentation (SGV), demonstrates improved performance compared to previous state-of-the-art methods on two distinct datasets across various evaluation metrics, all while achieving real-time processing at half a second per frame."}, "1902.06557": {"original_text": "  We propose a novel biophysical and dichromatic reflectance model that\nefficiently characterises spectral skin reflectance. We show how to fit the\nmodel to multispectral face images enabling high quality estimation of diffuse\nand specular shading as well as biophysical parameter maps (melanin and\nhaemoglobin). Our method works from a single image without requiring complex\ncontrolled lighting setups yet provides quantitatively accurate reconstructions\nand qualitatively convincing decomposition and editing.\n", "rewritten_text": "We present a new biophysical and dichromatic reflectance model that effectively characterizes spectral skin reflectance. Our model can be fitted to multispectral face images to accurately estimate diffuse and specular shading, as well as biophysical parameter maps for melanin and hemoglobin. This method operates using only a single image, eliminating the need for complex controlled lighting setups. Despite this simplicity, it delivers precise reconstructions and visually compelling decomposition and editing results."}, "2211.09379": {"original_text": "  In dialogue state tracking (DST), labeling the dataset involves considerable\nhuman labor. We propose a new self-training framework for few-shot generative\nDST that utilize unlabeled data. Our self-training method iteratively improves\nthe model by pseudo labeling and employs Purpose Preserving Augmentation\n(PPAug) to prevent overfitting. We increaese the few-shot 10% performance by\napproximately 4% on MultiWOZ 2.1 and enhances the slot-recall 8.34% for unseen\nvalues compared to baseline.\n", "rewritten_text": "In the field of dialogue state tracking (DST), labeling the dataset requires significant human effort. We introduce a novel self-training framework for few-shot generative DST that leverages unlabeled data. Our self-training approach enhances the model iteratively through pseudo labeling and incorporates Purpose Preserving Augmentation (PPAug) to mitigate overfitting. We achieve a 4% increase in performance for few-shot scenarios on MultiWOZ 2.1 and improve slot-recall by 8.34% for unseen values compared to the baseline."}, "1902.11268": {"original_text": "  Deep neural networks (DNNs), especially deep convolutional neural networks\n(CNNs), have emerged as the powerful technique in various machine learning\napplications. However, the large model sizes of DNNs yield high demands on\ncomputation resource and weight storage, thereby limiting the practical\ndeployment of DNNs. To overcome these limitations, this paper proposes to\nimpose the circulant structure to the construction of convolutional layers, and\nhence leads to circulant convolutional layers (CircConvs) and circulant CNNs.\nThe circulant structure and models can be either trained from scratch or\nre-trained from a pre-trained non-circulant model, thereby making it very\nflexible for different training environments. Through extensive experiments,\nsuch strong structure-imposing approach is proved to be able to substantially\nreduce the number of parameters of convolutional layers and enable significant\nsaving of computational cost by using fast multiplication of the circulant\ntensor.\n", "rewritten_text": "Deep neural networks (DNNs), particularly deep convolutional neural networks (CNNs), have emerged as a powerful technique in various machine learning applications. However, the large model sizes of DNNs create high demands on computational resources and weight storage, thereby limiting the practical deployment of DNNs. To address these limitations, this paper proposes incorporating a circulant structure into the construction of convolutional layers, resulting in circulant convolutional layers (CircConvs) and circulant CNNs. These circulant structures and models can be trained from scratch or re-trained from a pre-trained non-circulant model, making them highly adaptable to different training environments. Extensive experiments demonstrate that this approach of imposing a strong structure can significantly reduce the number of parameters in convolutional layers and enable substantial savings in computational costs through the use of fast multiplication of the circulant tensor."}, "1704.00447": {"original_text": "  Purpose: To allow fast and high-quality reconstruction of clinical\naccelerated multi-coil MR data by learning a variational network that combines\nthe mathematical structure of variational models with deep learning.\n  Theory and Methods: Generalized compressed sensing reconstruction formulated\nas a variational model is embedded in an unrolled gradient descent scheme. All\nparameters of this formulation, including the prior model defined by filter\nkernels and activation functions as well as the data term weights, are learned\nduring an offline training procedure. The learned model can then be applied\nonline to previously unseen data.\n  Results: The variational network approach is evaluated on a clinical knee\nimaging protocol. The variational network reconstructions outperform standard\nreconstruction algorithms in terms of image quality and residual artifacts for\nall tested acceleration factors and sampling patterns.\n  Conclusion: Variational network reconstructions preserve the natural\nappearance of MR images as well as pathologies that were not included in the\ntraining data set. Due to its high computational performance, i.e.,\nreconstruction time of 193 ms on a single graphics card, and the omission of\nparameter tuning once the network is trained, this new approach to image\nreconstruction can easily be integrated into clinical workflow.\n", "rewritten_text": "Purpose: The purpose of this study is to enable rapid and high-quality reconstruction of clinical accelerated multi-coil MR data through the utilization of a variational network that merges the mathematical structure of variational models with deep learning.\n\nTheory and Methods: The study formulates generalized compressed sensing reconstruction as a variational model integrated within an unrolled gradient descent scheme. All parameters of this formulation, such as the prior model defined by filter kernels and activation functions, along with the data term weights, are acquired through an offline training process. Subsequently, the trained model can be applied online to previously unseen data.\n\nResults: The effectiveness of the variational network approach is assessed using a clinical knee imaging protocol. The variational network reconstructions surpass standard reconstruction algorithms in terms of image quality and residual artifacts across all tested acceleration factors and sampling patterns.\n\nConclusion: Variational network reconstructions maintain the natural appearance of MR images and accurately represent pathologies not present in the training dataset. With its high computational efficiency, exemplified by a reconstruction time of 193 ms on a single graphics card, and the elimination of parameter tuning post-training, this novel image reconstruction approach can be seamlessly integrated into clinical workflows."}, "2104.1269": {"original_text": "  Data is the engine of modern computer vision, which necessitates collecting\nlarge-scale datasets. This is expensive, and guaranteeing the quality of the\nlabels is a major challenge. In this paper, we investigate efficient annotation\nstrategies for collecting multi-class classification labels for a large\ncollection of images. While methods that exploit learnt models for labeling\nexist, a surprisingly prevalent approach is to query humans for a fixed number\nof labels per datum and aggregate them, which is expensive. Building on prior\nwork on online joint probabilistic modeling of human annotations and\nmachine-generated beliefs, we propose modifications and best practices aimed at\nminimizing human labeling effort. Specifically, we make use of advances in\nself-supervised learning, view annotation as a semi-supervised learning\nproblem, identify and mitigate pitfalls and ablate several key design choices\nto propose effective guidelines for labeling. Our analysis is done in a more\nrealistic simulation that involves querying human labelers, which uncovers\nissues with evaluation using existing worker simulation methods. Simulated\nexperiments on a 125k image subset of the ImageNet100 show that it can be\nannotated to 80% top-1 accuracy with 0.35 annotations per image on average, a\n2.7x and 6.7x improvement over prior work and manual annotation, respectively.\nProject page: https://fidler-lab.github.io/efficient-annotation-cookbook\n", "rewritten_text": "Data serves as the driving force behind modern computer vision, necessitating the collection of large-scale datasets. However, this process can be costly, and ensuring the quality of the labels presents a significant challenge. In this study, we explore efficient annotation strategies for gathering multi-class classification labels for a vast array of images. While methods leveraging learned models for labeling do exist, a commonly used approach involves soliciting a fixed number of labels from humans per data point and then aggregating them, which can be expensive.\n\nBuilding upon previous research on online joint probabilistic modeling of human annotations and machine-generated beliefs, we introduce modifications and best practices aimed at reducing the human labeling effort. Specifically, we leverage advancements in self-supervised learning, treating annotation as a semi-supervised learning problem. We also identify and address potential pitfalls and eliminate several critical design choices to propose effective guidelines for labeling.\n\nOur analysis is conducted through a more realistic simulation that involves querying human labelers, revealing issues with evaluating using existing worker simulation methods. Through simulated experiments on a subset of 125,000 images from ImageNet100, we demonstrate that these images can be annotated with an 80% top-1 accuracy using an average of 0.35 annotations per image. This represents a 2.7x and 6.7x improvement over previous methods and manual annotation, respectively.\n\nFor more information, please visit our project page at: https://fidler-lab.github.io/efficient-annotation-cookbook"}, "2206.00923": {"original_text": "  We present a method that achieves state-of-the-art results on challenging\n(few-shot) layout-to-image generation tasks by accurately modeling textures,\nstructures and relationships contained in a complex scene. After compressing\nRGB images into patch tokens, we propose the Transformer with Focal Attention\n(TwFA) for exploring dependencies of object-to-object, object-to-patch and\npatch-to-patch. Compared to existing CNN-based and Transformer-based generation\nmodels that entangled modeling on pixel-level&patch-level and\nobject-level&patch-level respectively, the proposed focal attention predicts\nthe current patch token by only focusing on its highly-related tokens that\nspecified by the spatial layout, thereby achieving disambiguation during\ntraining. Furthermore, the proposed TwFA largely increases the data efficiency\nduring training, therefore we propose the first few-shot complex scene\ngeneration strategy based on the well-trained TwFA. Comprehensive experiments\nshow the superiority of our method, which significantly increases both\nquantitative metrics and qualitative visual realism with respect to\nstate-of-the-art CNN-based and transformer-based methods. Code is available at\nhttps://github.com/JohnDreamer/TwFA.\n", "rewritten_text": "We introduce a method that achieves state-of-the-art results in challenging few-shot layout-to-image generation tasks by accurately modeling textures, structures, and relationships within complex scenes. Our approach involves compressing RGB images into patch tokens and utilizing the Transformer with Focal Attention (TwFA) to explore dependencies between objects, patches, and pixels. In contrast to existing CNN-based and Transformer-based models that combine modeling at the pixel-level and patch-level, as well as object-level and patch-level, our focal attention mechanism predicts the current patch token by focusing solely on highly-related tokens specified by the spatial layout. This approach aids in disambiguation during training and significantly increases data efficiency. As a result, we propose a novel few-shot complex scene generation strategy based on the well-trained TwFA. Extensive experiments demonstrate the superiority of our method, showcasing improvements in both quantitative metrics and qualitative visual realism compared to state-of-the-art CNN-based and Transformer-based approaches. The code is available at https://github.com/JohnDreamer/TwFA."}, "2204.09841": {"original_text": "  Information from an image occurs over multiple and distinct spatial scales.\nImage pyramid multiresolution representations are a useful data structure for\nimage analysis and manipulation over a spectrum of spatial scales. This paper\nemploys the Gaussian-Laplacian pyramid to treat different spatial frequency\nbands of a texture separately. First, we generate three images corresponding to\nthree levels of the Gaussian-Laplacian pyramid for an input image to capture\nintrinsic details. Then we aggregate features extracted from gray and color\ntexture images using bio-inspired texture descriptors, information-theoretic\nmeasures, gray-level co-occurrence matrix features, and Haralick statistical\nfeatures into a single feature vector. Such an aggregation aims at producing\nfeatures that characterize textures to their maximum extent, unlike employing\neach descriptor separately, which may lose some relevant textural information\nand reduce the classification performance. The experimental results on texture\nand histopathologic image datasets have shown the advantages of the proposed\nmethod compared to state-of-the-art approaches. Such findings emphasize the\nimportance of multiscale image analysis and corroborate that the descriptors\nmentioned above are complementary.\n", "rewritten_text": "Information in an image is present across various spatial scales. Utilizing image pyramid multiresolution representations proves to be a valuable data structure for analyzing and manipulating images across a range of spatial scales. This study utilizes the Gaussian-Laplacian pyramid to address distinct spatial frequency bands of a texture individually. Initially, three images are generated to represent three levels of the Gaussian-Laplacian pyramid for an input image, capturing intrinsic details. Subsequently, features extracted from grayscale and color texture images are combined using bio-inspired texture descriptors, information-theoretic measures, gray-level co-occurrence matrix features, and Haralick statistical features to form a unified feature vector. This aggregation aims to create features that fully characterize textures, as opposed to using each descriptor independently, which may result in the loss of relevant textural information and a decrease in classification performance. Experimental results on texture and histopathologic image datasets demonstrate the advantages of the proposed method over current approaches. These findings underscore the significance of multiscale image analysis and confirm that the aforementioned descriptors complement each other."}, "2207.00744": {"original_text": "  Spatial-Temporal Video Grounding (STVG) is a challenging task which aims to\nlocalize the spatio-temporal tube of the interested object semantically\naccording to a natural language query. Most previous works not only severely\nrely on the anchor boxes extracted by Faster R-CNN, but also simply regard the\nvideo as a series of individual frames, thus lacking their temporal modeling.\nInstead, in this paper, we are the first to propose an anchor-free framework\nfor STVG, called Gaussian Kernel-based Cross Modal Network (GKCMN).\nSpecifically, we utilize the learned Gaussian Kernel-based heatmaps of each\nvideo frame to locate the query-related object. A mixed serial and parallel\nconnection network is further developed to leverage both spatial and temporal\nrelations among frames for better grounding. Experimental results on VidSTG\ndataset demonstrate the effectiveness of our proposed GKCMN.\n", "rewritten_text": "Spatial-Temporal Video Grounding (STVG) presents a challenging task that involves localizing the spatio-temporal tube of a specific object based on a natural language query. Previous studies have heavily relied on anchor boxes extracted by Faster R-CNN and treated videos as a sequence of individual frames, neglecting temporal modeling. In contrast, this paper introduces a novel anchor-free framework for STVG, known as the Gaussian Kernel-based Cross Modal Network (GKCMN). The GKCMN framework utilizes Gaussian Kernel-based heatmaps learned from each video frame to pinpoint the object relevant to the query. To enhance grounding, a mixed serial and parallel connection network is developed to capture spatial and temporal relationships among frames. Experimental results on the VidSTG dataset validate the effectiveness of the proposed GKCMN approach."}, "2209.00232": {"original_text": "  Capsule networks (CapsNets) aim to parse images into a hierarchy of objects,\nparts, and their relations using a two-step process involving part-whole\ntransformation and hierarchical component routing. However, this hierarchical\nrelationship modeling is computationally expensive, which has limited the wider\nuse of CapsNet despite its potential advantages. The current state of CapsNet\nmodels primarily focuses on comparing their performance with capsule baselines,\nfalling short of achieving the same level of proficiency as deep CNN variants\nin intricate tasks. To address this limitation, we present an efficient\napproach for learning capsules that surpasses canonical baseline models and\neven demonstrates superior performance compared to high-performing convolution\nmodels. Our contribution can be outlined in two aspects: firstly, we introduce\na group of subcapsules onto which an input vector is projected. Subsequently,\nwe present the Hybrid Gromov-Wasserstein framework, which initially quantifies\nthe dissimilarity between the input and the components modeled by the\nsubcapsules, followed by determining their alignment degree through optimal\ntransport. This innovative mechanism capitalizes on new insights into defining\nalignment between the input and subcapsules, based on the similarity of their\nrespective component distributions. This approach enhances CapsNets' capacity\nto learn from intricate, high-dimensional data while retaining their\ninterpretability and hierarchical structure. Our proposed model offers two\ndistinct advantages: (i) its lightweight nature facilitates the application of\ncapsules to more intricate vision tasks, including object detection; (ii) it\noutperforms baseline approaches in these demanding tasks.\n", "rewritten_text": "Capsule networks, also known as CapsNets, are designed to analyze images by organizing them into a hierarchy of objects, parts, and their relationships through a two-step process involving part-whole transformation and hierarchical component routing. However, the computational complexity of this hierarchical relationship modeling has hindered the widespread adoption of CapsNet despite its potential benefits. Current CapsNet models primarily focus on comparing their performance with capsule baselines, falling short of achieving the same level of proficiency as deep CNN variants in complex tasks.\n\nTo overcome this limitation, we propose an efficient approach for learning capsules that outperforms canonical baseline models and even demonstrates superior performance compared to high-performing convolution models. Our contribution can be summarized in two key aspects: firstly, we introduce a group of subcapsules onto which an input vector is projected. Subsequently, we introduce the Hybrid Gromov-Wasserstein framework, which quantifies the dissimilarity between the input and the components modeled by the subcapsules, and then determines their alignment degree through optimal transport.\n\nThis innovative mechanism leverages new insights into defining alignment between the input and subcapsules based on the similarity of their respective component distributions. This approach enhances CapsNets' ability to learn from complex, high-dimensional data while maintaining interpretability and a hierarchical structure. Our proposed model offers two main advantages: (i) its lightweight nature enables the application of capsules to more complex vision tasks, such as object detection; (ii) it surpasses baseline approaches in these challenging tasks."}, "2311.12076": {"original_text": "  Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof open-world intelligent systems. Despite the notable advancements in existing\nOOD detection methodologies, our study identifies a significant performance\ndrop under the scarcity of training samples. In this context, we introduce a\nnovel few-shot OOD detection benchmark, carefully constructed to address this\ngap. Our empirical analysis reveals the superiority of ParameterEfficient\nFine-Tuning (PEFT) strategies, such as visual prompt tuning and visual adapter\ntuning, over conventional techniques, including fully fine-tuning and linear\nprobing tuning in the few-shot OOD detection task. Recognizing some crucial\ninformation from the pre-trained model, which is pivotal for OOD detection, may\nbe lost during the fine-tuning process, we propose a method termed\nDomainSpecific and General Knowledge Fusion (DSGF). This approach is designed\nto be compatible with diverse fine-tuning frameworks. Our experiments show that\nthe integration of DSGF significantly enhances the few-shot OOD detection\ncapabilities across various methods and fine-tuning methodologies, including\nfully fine-tuning, visual adapter tuning, and visual prompt tuning. The code\nwill be released.\n", "rewritten_text": "Out-of-distribution (OOD) detection is crucial for ensuring the reliability of open-world intelligent systems. Despite notable advancements in existing OOD detection methodologies, our study has identified a significant performance drop when training samples are scarce. To address this gap, we have introduced a novel few-shot OOD detection benchmark. Our empirical analysis demonstrates the superiority of ParameterEfficient Fine-Tuning (PEFT) strategies, such as visual prompt tuning and visual adapter tuning, over conventional techniques like fully fine-tuning and linear probing tuning in the few-shot OOD detection task.\n\nRecognizing that crucial information from the pre-trained model, which is essential for OOD detection, may be lost during the fine-tuning process, we propose a method called Domain-Specific and General Knowledge Fusion (DSGF). This approach is designed to be compatible with various fine-tuning frameworks. Our experiments show that integrating DSGF significantly enhances few-shot OOD detection capabilities across different methods and fine-tuning methodologies, including fully fine-tuning, visual adapter tuning, and visual prompt tuning. The code will be released."}, "2104.10117": {"original_text": "  We present a novel deep learning-based framework to generate embedding\nrepresentations of fine-grained emotions that can be used to computationally\ndescribe psychological models of emotions. Our framework integrates a\ncontextualized embedding encoder with a multi-head probing model that enables\nto interpret dynamically learned representations optimized for an emotion\nclassification task. Our model is evaluated on the Empathetic Dialogue dataset\nand shows the state-of-the-art result for classifying 32 emotions. Our layer\nanalysis can derive an emotion graph to depict hierarchical relations among the\nemotions. Our emotion representations can be used to generate an emotion wheel\ndirectly comparable to the one from Plutchik's\\LN model, and also augment the\nvalues of missing emotions in the PAD emotional state model.\n", "rewritten_text": "We introduce a novel deep learning-based framework for generating embedding representations of fine-grained emotions. These representations can be utilized to computationally describe psychological models of emotions. Our framework combines a contextualized embedding encoder with a multi-head probing model, allowing for the dynamic interpretation of learned representations optimized for emotion classification tasks. Evaluating our model on the Empathetic Dialogue dataset demonstrates its state-of-the-art performance in classifying 32 emotions. Through layer analysis, we construct an emotion graph that illustrates hierarchical relationships among the emotions. The generated emotion representations enable the creation of an emotion wheel, comparable to Plutchik's model, and enhance the values of missing emotions in the PAD emotional state model."}, "1912.06842": {"original_text": "  The main requisite for fine-grained recognition task is to focus on subtle\ndiscriminative details that make the subordinate classes different from each\nother. We note that existing methods implicitly address this requirement and\nleave it to a data-driven pipeline to figure out what makes a subordinate class\ndifferent from the others. This results in two major limitations: First, the\nnetwork focuses on the most obvious distinctions between classes and overlooks\nmore subtle inter-class variations. Second, the chance of misclassifying a\ngiven sample in any of the negative classes is considered equal, while in fact,\nconfusions generally occur among only the most similar classes. Here, we\npropose to explicitly force the network to find the subtle differences among\nclosely related classes. In this pursuit, we introduce two key novelties that\ncan be easily plugged into existing end-to-end deep learning pipelines. On one\nhand, we introduce diversification block which masks the most salient features\nfor an input to force the network to use more subtle cues for its correct\nclassification. Concurrently, we introduce a gradient-boosting loss function\nthat focuses only on the confusing classes for each sample and therefore moves\nswiftly along the direction on the loss surface that seeks to resolve these\nambiguities. The synergy between these two blocks helps the network to learn\nmore effective feature representations. Comprehensive experiments are performed\non five challenging datasets. Our approach outperforms existing methods using\nsimilar experimental setting on all five datasets.\n", "rewritten_text": "The key requirement for a fine-grained recognition task is to focus on subtle discriminative details that differentiate subordinate classes from each other. Existing methods tend to address this requirement implicitly, leaving it to a data-driven pipeline to determine the distinguishing features of a subordinate class. However, this approach has two significant limitations. Firstly, the network tends to emphasize the most obvious differences between classes, often overlooking more nuanced inter-class variations. Secondly, misclassification errors are assumed to be equally likely across all negative classes, whereas in reality, confusions typically occur among the most similar classes.\n\nTo address these limitations, we propose explicitly guiding the network to identify subtle differences among closely related classes. To achieve this goal, we introduce two key innovations that can be seamlessly integrated into existing end-to-end deep learning pipelines. Firstly, we introduce a diversification block that masks the most prominent features of an input, encouraging the network to rely on more subtle cues for accurate classification. Additionally, we introduce a gradient-boosting loss function that specifically targets the confusing classes for each sample, facilitating rapid progress towards resolving classification ambiguities.\n\nThe combination of these two components enhances the network's ability to learn more effective feature representations. We conducted comprehensive experiments on five challenging datasets, demonstrating that our approach outperforms existing methods under similar experimental conditions across all five datasets."}, "1310.7447": {"original_text": "  A new method for removing impulse noise from speech in the wavelet transform\ndomain is proposed. The method utilizes the multiresolution property of the\nwavelet transform, which provides finer time resolution at the higher\nfrequencies than the short-time Fourier transform (STFT), to effectively\nidentify and remove impulse noise. It uses two features of speech to\ndiscriminate speech from impulse noise: one is the slow time-varying nature of\nspeech and the other is the Lipschitz regularity of the speech components. On\nthe basis of these features, an algorithm has been developed to identify and\nsuppress wavelet coefficients that correspond to impulse noise. Experiment\nresults show that the new method is able to significantly reduce impulse noise\nwithout degrading the quality of the speech signal or introducing any audible\nartifacts.\n", "rewritten_text": "A novel method is proposed for removing impulse noise from speech in the wavelet transform domain. This method leverages the multiresolution property of the wavelet transform, which offers finer time resolution at higher frequencies compared to the short-time Fourier transform (STFT). By effectively utilizing this property, the method can accurately identify and eliminate impulse noise. It relies on two key features of speech to distinguish speech from impulse noise: the gradual time-varying nature of speech and the Lipschitz regularity of speech components. Building upon these features, an algorithm has been developed to detect and suppress wavelet coefficients associated with impulse noise. Experimental results demonstrate that this new method significantly reduces impulse noise without compromising the quality of the speech signal or introducing any audible artifacts."}, "2108.06771": {"original_text": "  Quantification of uncertainty in deep-neural-networks (DNN) based image\nregistration algorithms plays a critical role in the deployment of image\nregistration algorithms for clinical applications such as surgical planning,\nintraoperative guidance, and longitudinal monitoring of disease progression or\ntreatment efficacy as well as in research-oriented processing pipelines.\nCurrently available approaches for uncertainty estimation in DNN-based image\nregistration algorithms may result in sub-optimal clinical decision making due\nto potentially inaccurate estimation of the uncertainty of the registration\nstems for the assumed parametric distribution of the registration latent space.\nWe introduce NPBDREG, a fully non-parametric Bayesian framework for uncertainty\nestimation in DNN-based deformable image registration by combining an Adam\noptimizer with stochastic gradient Langevin dynamics (SGLD) to characterize the\nunderlying posterior distribution through posterior sampling. Thus, it has the\npotential to provide uncertainty estimates that are highly correlated with the\npresence of out of distribution data. We demonstrated the added-value of\nNPBDREG, compared to the baseline probabilistic VoxelMorph model (PrVXM), on\nbrain MRI image registration using $390$ image pairs from four publicly\navailable databases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a\nbetter correlation of the predicted uncertainty with out-of-distribution data\n($r>0.95$ vs. $r<0.5$) as well as a 7.3%improvement in the registration\naccuracy (Dice score, $0.74$ vs. $0.69$, $p \\ll 0.01$), and 18% improvement in\nregistration smoothness (percentage of folds in the deformation field, 0.014\nvs. 0.017, $p \\ll 0.01$). Finally, NPBDREG demonstrated a better generalization\ncapability for data corrupted by a mixed structure noise (Dice score of $0.73$\nvs. $0.69$, $p \\ll 0.01$) compared to the baseline PrVXM approach.\n", "rewritten_text": "Quantifying uncertainty in deep neural network (DNN)-based image registration algorithms is crucial for their effective deployment in clinical applications such as surgical planning, intraoperative guidance, and longitudinal monitoring of disease progression or treatment efficacy, as well as in research-oriented processing pipelines. Existing methods for estimating uncertainty in DNN-based image registration algorithms may lead to suboptimal clinical decision-making due to potentially inaccurate estimation of registration uncertainty stemming from the assumed parametric distribution of the registration latent space.\n\nWe present NPBDREG, a fully non-parametric Bayesian framework for uncertainty estimation in DNN-based deformable image registration. NPBDREG combines an Adam optimizer with stochastic gradient Langevin dynamics (SGLD) to characterize the underlying posterior distribution through posterior sampling. This approach has the potential to provide uncertainty estimates that are highly correlated with the presence of out-of-distribution data.\n\nTo demonstrate the effectiveness of NPBDREG, we compared it to the baseline probabilistic VoxelMorph model (PrVXM) in brain MRI image registration using 390 image pairs from four publicly available databases: MGH10, CMUC12, ISBR18, and LPBA40. NPBDREG exhibited a stronger correlation of predicted uncertainty with out-of-distribution data (r > 0.95 vs. r < 0.5), along with a 7.3% improvement in registration accuracy (Dice score: 0.74 vs. 0.69, p < 0.01) and an 18% enhancement in registration smoothness (percentage of folds in the deformation field: 0.014 vs. 0.017, p < 0.01). Furthermore, NPBDREG demonstrated superior generalization capabilities for data corrupted by mixed structural noise, achieving a Dice score of 0.73 compared to 0.69 with the baseline PrVXM approach (p < 0.01)."}, "2308.13678": {"original_text": "  Reconstructing and tracking deformable surface with little or no texture has\nposed long-standing challenges. Fundamentally, the challenges stem from\ntextureless surfaces lacking features for establishing cross-image\ncorrespondences. In this work, we present a novel type of markers to\nproactively enrich the object's surface features, and thereby ease the 3D\nsurface reconstruction and correspondence tracking. Our markers are made of\nfluorescent dyes, visible only under the ultraviolet (UV) light and invisible\nunder regular lighting condition. Leveraging the markers, we design a\nmulti-camera system that captures surface deformation under the UV light and\nthe visible light in a time multiplexing fashion. Under the UV light, markers\non the object emerge to enrich its surface texture, allowing high-quality 3D\nshape reconstruction and tracking. Under the visible light, markers become\ninvisible, allowing us to capture the object's original untouched appearance.\nWe perform experiments on various challenging scenes, including hand gestures,\nfacial expressions, waving cloth, and hand-object interaction. In all these\ncases, we demonstrate that our system is able to produce robust, high-quality\n3D reconstruction and tracking.\n", "rewritten_text": "Reconstructing and tracking deformable surfaces with little or no texture has presented long-standing challenges. The fundamental issue arises from the lack of features on textureless surfaces to establish cross-image correspondences. In this study, we introduce a new type of markers to enhance the object's surface features proactively, thereby facilitating 3D surface reconstruction and correspondence tracking. These markers are composed of fluorescent dyes that are visible only under ultraviolet (UV) light and invisible under regular lighting conditions. By utilizing these markers, we have developed a multi-camera system that captures surface deformation under both UV light and visible light in a time-multiplexing manner. When illuminated by UV light, the markers on the object enhance its surface texture, enabling high-quality 3D shape reconstruction and tracking. Conversely, under visible light, the markers become invisible, allowing us to capture the object's original appearance without any alterations. Our experiments encompass various challenging scenarios, such as hand gestures, facial expressions, waving cloth, and hand-object interactions. Across all these cases, our system consistently demonstrates the ability to generate robust, high-quality 3D reconstructions and tracking results."}, "2408.12100": {"original_text": "  In recent years Plug-and-Play (PnP) methods have achieved state-of-the-art\nperformance in inverse imaging problems by replacing proximal operators with\ndenoisers. Based on the proximal gradient method, some theoretical results of\nPnP have appeared, where appropriate step size is crucial for convergence\nanalysis. However, in practical applications, applying PnP methods with\ntheoretically guaranteed step sizes is difficult, and these algorithms are\nlimited to Gaussian noise. In this paper,from a perspective of split convex\nfeasibility problems (SCFP), an adaptive PnP algorithm with Projected Landweber\nOperator (PnP-PLO) is proposed to address these issues. Numerical experiments\non image deblurring, super-resolution, and compressed sensing MRI experiments\nillustrate that PnP-PLO with theoretical guarantees outperforms\nstate-of-the-art methods such as RED and RED-PRO.\n", "rewritten_text": "In recent years, Plug-and-Play (PnP) methods have achieved state-of-the-art performance in solving inverse imaging problems by replacing proximal operators with denoisers. Theoretical results of PnP based on the proximal gradient method have been presented, emphasizing the importance of an appropriate step size for convergence analysis. However, in practical applications, implementing PnP methods with theoretically guaranteed step sizes is challenging, and these algorithms are typically limited to handling Gaussian noise. \n\nThis paper introduces an adaptive PnP algorithm with Projected Landweber Operator (PnP-PLO) from the perspective of split convex feasibility problems (SCFP) to address these challenges. Numerical experiments conducted on image deblurring, super-resolution, and compressed sensing MRI demonstrate that PnP-PLO, with its theoretical guarantees, outperforms state-of-the-art methods like RED and RED-PRO."}, "2210.13077": {"original_text": "  Novel-view synthesis (NVS) can be tackled through different approaches,\ndepending on the general setting: a single source image to a short video\nsequence, exact or noisy camera pose information, 3D-based information such as\npoint clouds etc. The most challenging scenario, the one where we stand in this\nwork, only considers a unique source image to generate a novel one from another\nviewpoint. However, in such a tricky situation, the latest learning-based\nsolutions often struggle to integrate the camera viewpoint transformation.\nIndeed, the extrinsic information is often passed as-is, through a\nlow-dimensional vector. It might even occur that such a camera pose, when\nparametrized as Euler angles, is quantized through a one-hot representation.\nThis vanilla encoding choice prevents the learnt architecture from inferring\nnovel views on a continuous basis (from a camera pose perspective). We claim it\nexists an elegant way to better encode relative camera pose, by leveraging\n3D-related concepts such as the epipolar constraint. We, therefore, introduce\nan innovative method that encodes the viewpoint transformation as a 2D feature\nimage. Such a camera encoding strategy gives meaningful insights to the network\nregarding how the camera has moved in space between the two views. By encoding\nthe camera pose information as a finite number of coloured epipolar lines, we\ndemonstrate through our experiments that our strategy outperforms vanilla\nencoding.\n", "rewritten_text": "Novel-view synthesis (NVS) can be approached in various ways depending on the specific context, such as a single source image to a short video sequence, precise or noisy camera pose information, or 3D-based data like point clouds. The focus of our work lies in the most challenging scenario, which involves generating a novel image from a different viewpoint using only a single source image. In such complex situations, current learning-based solutions often struggle to effectively incorporate the camera viewpoint transformation. Typically, extrinsic information is transmitted in a low-dimensional vector without proper integration. In some cases, the camera pose may even be quantized into a one-hot representation when parametrized as Euler angles. This simplistic encoding approach hinders the model from inferring novel views continuously based on the camera pose. We propose a more sophisticated method for encoding relative camera pose by leveraging 3D concepts like the epipolar constraint. Our innovative approach encodes the viewpoint transformation as a 2D feature image, providing valuable insights to the network on how the camera has moved between the views in space. By representing the camera pose information as colored epipolar lines, we demonstrate through experiments that our strategy surpasses the performance of traditional encoding methods."}, "2106.00184": {"original_text": "  Encouraging progress in few-shot semantic segmentation has been made by\nleveraging features learned upon base classes with sufficient training data to\nrepresent novel classes with few-shot examples. However, this feature sharing\nmechanism inevitably causes semantic aliasing between novel classes when they\nhave similar compositions of semantic concepts. In this paper, we reformulate\nfew-shot segmentation as a semantic reconstruction problem, and convert base\nclass features into a series of basis vectors which span a class-level semantic\nspace for novel class reconstruction. By introducing contrastive loss, we\nmaximize the orthogonality of basis vectors while minimizing semantic aliasing\nbetween classes. Within the reconstructed representation space, we further\nsuppress interference from other classes by projecting query features to the\nsupport vector for precise semantic activation. Our proposed approach, referred\nto as anti-aliasing semantic reconstruction (ASR), provides a systematic yet\ninterpretable solution for few-shot learning problems. Extensive experiments on\nPASCAL VOC and MS COCO datasets show that ASR achieves strong results compared\nwith the prior works.\n", "rewritten_text": "Significant progress has been achieved in few-shot semantic segmentation by utilizing features learned from base classes with ample training data to represent novel classes with only a few examples. However, this feature sharing approach inevitably leads to semantic aliasing between novel classes that share similar compositions of semantic concepts. In this study, we redefine few-shot segmentation as a semantic reconstruction problem. We transform base class features into a set of basis vectors that cover a class-level semantic space for reconstructing novel classes. By incorporating a contrastive loss, we aim to enhance the orthogonality of basis vectors while reducing semantic aliasing between classes. In the reconstructed representation space, we further mitigate interference from other classes by mapping query features to the support vector for precise semantic activation. Our proposed method, known as anti-aliasing semantic reconstruction (ASR), offers a systematic and interpretable solution to few-shot learning challenges. Extensive experiments conducted on the PASCAL VOC and MS COCO datasets demonstrate that ASR outperforms previous approaches, yielding robust results."}, "2310.00274": {"original_text": "  Africa has a very low doctor-to-patient ratio. At very busy clinics, doctors\ncould see 30+ patients per day -- a heavy patient burden compared with\ndeveloped countries -- but productivity tools such as clinical automatic speech\nrecognition (ASR) are lacking for these overworked clinicians. However,\nclinical ASR is mature, even ubiquitous, in developed nations, and\nclinician-reported performance of commercial clinical ASR systems is generally\nsatisfactory. Furthermore, the recent performance of general domain ASR is\napproaching human accuracy. However, several gaps exist. Several publications\nhave highlighted racial bias with speech-to-text algorithms and performance on\nminority accents lags significantly. To our knowledge, there is no publicly\navailable research or benchmark on accented African clinical ASR, and speech\ndata is non-existent for the majority of African accents. We release\nAfriSpeech, 200hrs of Pan-African English speech, 67,577 clips from 2,463\nunique speakers across 120 indigenous accents from 13 countries for clinical\nand general domain ASR, a benchmark test set, with publicly available\npre-trained models with SOTA performance on the AfriSpeech benchmark.\n", "rewritten_text": "Africa has a significantly low doctor-to-patient ratio, with doctors in busy clinics often seeing over 30 patients per day. This patient burden is much higher compared to developed countries. Unfortunately, these overworked clinicians lack productivity tools such as clinical automatic speech recognition (ASR). In contrast, clinical ASR is well-established and widely used in developed nations, with satisfactory performance reported by clinicians using commercial systems.\n\nWhile general domain ASR has made significant advancements, there are still gaps to address. Some publications have pointed out racial bias in speech-to-text algorithms, and performance on minority accents remains a challenge. Surprisingly, there is a lack of publicly available research or benchmarks on accented African clinical ASR, and data on African accents is scarce.\n\nTo address this gap, we have introduced AfriSpeech, which includes 200 hours of Pan-African English speech. This dataset comprises 67,577 clips from 2,463 unique speakers representing 120 indigenous accents from 13 countries. It serves as a benchmark test set for both clinical and general domain ASR, with publicly available pre-trained models that demonstrate state-of-the-art performance on the AfriSpeech benchmark."}, "2306.02351": {"original_text": "  We present the RSSOD-Bench dataset for salient object detection (SOD) in\noptical remote sensing imagery. While SOD has achieved success in natural scene\nimages with deep learning, research in SOD for remote sensing imagery (RSSOD)\nis still in its early stages. Existing RSSOD datasets have limitations in terms\nof scale, and scene categories, which make them misaligned with real-world\napplications. To address these shortcomings, we construct the RSSOD-Bench\ndataset, which contains images from four different cities in the USA. The\ndataset provides annotations for various salient object categories, such as\nbuildings, lakes, rivers, highways, bridges, aircraft, ships, athletic fields,\nand more. The salient objects in RSSOD-Bench exhibit large-scale variations,\ncluttered backgrounds, and different seasons. Unlike existing datasets,\nRSSOD-Bench offers uniform distribution across scene categories. We benchmark\n23 different state-of-the-art approaches from both the computer vision and\nremote sensing communities. Experimental results demonstrate that more research\nefforts are required for the RSSOD task.\n", "rewritten_text": "We introduce the RSSOD-Bench dataset for salient object detection (SOD) in optical remote sensing imagery. While SOD has shown success in natural scene images through deep learning, research in SOD for remote sensing imagery (RSSOD) is still in its nascent stage. Existing RSSOD datasets have limitations in terms of scale and scene categories, which do not align well with real-world applications. To address these limitations, we have developed the RSSOD-Bench dataset, comprising images from four different cities in the USA. This dataset includes annotations for various salient object categories, such as buildings, lakes, rivers, highways, bridges, aircraft, ships, athletic fields, and more. The salient objects in RSSOD-Bench exhibit significant variations in scale, cluttered backgrounds, and seasonal changes. Unlike current datasets, RSSOD-Bench ensures a uniform distribution across scene categories. We have evaluated 23 different state-of-the-art approaches from both the computer vision and remote sensing communities. The experimental results highlight the need for further research efforts in the RSSOD domain."}, "2303.12946": {"original_text": "  We have made a dataset of camouflage object detection mainly for complex\nseabed scenes, and named it UnderWater RGB&Sonar,or UW-RS for short. The UW-RS\ndataset contains a total of 1972 image data. The dataset mainly consists of two\nparts, namely underwater optical data part (UW-R dataset) and underwater sonar\ndata part (UW-S dataset).\n", "rewritten_text": "We have created a dataset for detecting camouflage objects in complex seabed scenes, which we named UnderWater RGB&Sonar (UW-RS). The UW-RS dataset comprises a total of 1972 image data. It is divided into two main parts: the underwater optical data section (UW-R dataset) and the underwater sonar data section (UW-S dataset)."}, "2308.02738": {"original_text": "  Recently, visual-language learning has shown great potential in enhancing\nvisual-based person re-identification (ReID). Existing visual-language\nlearning-based ReID methods often focus on whole-body scale image-text feature\nalignment, while neglecting supervisions on fine-grained part features. This\nchoice simplifies the learning process but cannot guarantee within-part feature\nsemantic consistency thus hindering the final performance. Therefore, we\npropose to enhance fine-grained visual features with part-informed language\nsupervision for ReID tasks. The proposed method, named Part-Informed\nVisual-language Learning ($\\pi$-VL), suggests that (i) a human parsing-guided\nprompt tuning strategy and (ii) a hierarchical fusion-based visual-language\nalignment paradigm play essential roles in ensuring within-part feature\nsemantic consistency. Specifically, we combine both identity labels and parsing\nmaps to constitute pixel-level text prompts and fuse multi-stage visual\nfeatures with a light-weight auxiliary head to perform fine-grained image-text\nalignment. As a plug-and-play and inference-free solution, our $\\pi$-VL\nachieves substantial improvements over previous state-of-the-arts on four\ncommon-used ReID benchmarks, especially reporting 90.3% Rank-1 and 76.5% mAP\nfor the most challenging MSMT17 database without bells and whistles.\n", "rewritten_text": "Recently, there has been significant progress in utilizing visual-language learning to enhance visual-based person re-identification (ReID). While existing methods in this area typically focus on aligning image-text features at the whole-body scale, they often overlook supervising fine-grained part features. This simplification in the learning process may compromise the semantic consistency of within-part features, ultimately impacting the overall performance. To address this limitation, we propose a method to enrich fine-grained visual features through part-informed language supervision for ReID tasks. Our approach, named Part-Informed Visual-language Learning ($\\pi$-VL), emphasizes the importance of (i) a human parsing-guided prompt tuning strategy and (ii) a hierarchical fusion-based visual-language alignment paradigm to ensure semantic consistency within part features. Specifically, we leverage both identity labels and parsing maps to create pixel-level text prompts and integrate multi-stage visual features with a lightweight auxiliary head for precise image-text alignment at a fine-grained level. As a user-friendly and inference-free solution, our $\\pi$-VL method demonstrates significant performance enhancements compared to previous state-of-the-art approaches on four commonly used ReID benchmarks, notably achieving a remarkable 90.3% Rank-1 accuracy and 76.5% mean Average Precision (mAP) on the challenging MSMT17 database without any additional complexities."}, "1812.05785": {"original_text": "  It is prohibitively expensive to annotate a large-scale video-based person\nre-identification (re-ID) dataset, which makes fully supervised methods\ninapplicable to real-world deployment. How to maximally reduce the annotation\ncost while retaining the re-ID performance becomes an interesting problem. In\nthis paper, we address this problem by integrating an active learning scheme\ninto a deep learning framework. Noticing that the truly matched tracklet-pairs,\nalso denoted as true positives (TP), are the most informative samples for our\nre-ID model, we propose a sampling criterion to choose the most TP-likely\ntracklet-pairs for annotation. A view-aware sampling strategy considering\nview-specific biases is designed to facilitate candidate selection, followed by\nan adaptive resampling step to leave out the selected candidates that are\nunnecessary to annotate. Our method learns the re-ID model and updates the\nannotation set iteratively. The re-ID model is supervised by the tracklets'\npesudo labels that are initialized by treating each tracklet as a distinct\nclass. With the gained annotations of the actively selected candidates, the\ntracklets' pesudo labels are updated by label merging and further used to\nre-train our re-ID model. While being simple, the proposed method demonstrates\nits effectiveness on three video-based person re-ID datasets. Experimental\nresults show that less than 3\\% pairwise annotations are needed for our method\nto reach comparable performance with the fully-supervised setting.\n", "rewritten_text": "Annotating a large-scale video-based person re-identification (re-ID) dataset is prohibitively expensive, rendering fully supervised methods impractical for real-world deployment. The challenge of minimizing annotation costs while maintaining re-ID performance is a compelling issue. This paper tackles this challenge by incorporating an active learning scheme into a deep learning framework. Recognizing that truly matched tracklet-pairs, referred to as true positives (TP), are the most informative samples for the re-ID model, we propose a sampling criterion to select the most TP-likely tracklet-pairs for annotation. A view-aware sampling strategy, which accounts for view-specific biases, is devised to aid in candidate selection. This is followed by an adaptive resampling step to exclude unnecessary candidates from annotation. Our method iteratively learns the re-ID model and updates the annotation set. The re-ID model is supervised by pseudo labels assigned to tracklets, treating each tracklet as a distinct class. Upon obtaining annotations from the actively selected candidates, the pseudo labels of tracklets are updated through label merging and utilized for retraining the re-ID model. Despite its simplicity, our proposed method demonstrates effectiveness across three video-based person re-ID datasets. Experimental results indicate that our method achieves comparable performance to the fully-supervised setting with less than 3% pairwise annotations required."}, "2202.05457": {"original_text": "  Sentiment Analysis typically refers to using natural language processing,\ntext analysis and computational linguistics to extract affect and emotion based\ninformation from text data. Our work explores how we can effectively use deep\nneural networks in transfer learning and joint dual input learning settings to\neffectively classify sentiments and detect hate speech in Hindi and Bengali\ndata. We start by training Word2Vec word embeddings for Hindi \\textbf{HASOC\ndataset} and Bengali hate speech and then train LSTM and subsequently, employ\nparameter sharing based transfer learning to Bengali sentiment classifiers by\nreusing and fine-tuning the trained weights of Hindi classifiers with both\nclassifier being used as baseline in our study. Finally, we use BiLSTM with\nself attention in joint dual input learning setting where we train a single\nneural network on Hindi and Bengali dataset simultaneously using their\nrespective embeddings.\n", "rewritten_text": "Sentiment Analysis typically involves using natural language processing, text analysis, and computational linguistics to extract affect and emotion-based information from text data. Our research delves into the effective utilization of deep neural networks in transfer learning and joint dual input learning scenarios to classify sentiments and detect hate speech in Hindi and Bengali text data. \n\nWe begin by training Word2Vec word embeddings for the Hindi HASOC dataset and Bengali hate speech. Subsequently, we train LSTM models and then apply parameter sharing-based transfer learning to Bengali sentiment classifiers. This involves reusing and fine-tuning the trained weights of Hindi classifiers, with both classifiers serving as baselines in our study. \n\nFinally, we employ BiLSTM with self-attention in a joint dual input learning setting. This approach entails training a single neural network on both Hindi and Bengali datasets simultaneously using their respective embeddings."}, "1210.0115": {"original_text": "  A framework of demosaicing and superresolution for color filter array (CFA)\nvia residual image reconstruction and sparse representation is presented.Given\nthe intermediate image produced by certain demosaicing and interpolation\ntechnique, a residual image between the final reconstruction image and the\nintermediate image is reconstructed using sparse representation.The final\nreconstruction image has richer edges and details than that of the intermediate\nimage. Specifically, a generic dictionary is learned from a large set of\ncomposite training data composed of intermediate data and residual data. The\nlearned dictionary implies a mapping between the two data. A specific\ndictionary adaptive to the input CFA is learned thereafter. Using the adaptive\ndictionary, the sparse coefficients of intermediate data are computed and\ntransformed to predict residual image. The residual image is added back into\nthe intermediate image to obtain the final reconstruction image. Experimental\nresults demonstrate the state-of-the-art performance in terms of PSNR and\nsubjective visual perception.\n", "rewritten_text": "Presented is a framework for demosaicing and superresolution of color filter arrays (CFA) utilizing residual image reconstruction and sparse representation. The process involves reconstructing a residual image between the final reconstruction image and an intermediate image generated by a demosaicing and interpolation technique. The final reconstruction image exhibits enhanced edges and details compared to the intermediate image.\n\nInitially, a generic dictionary is acquired from a vast collection of composite training data, comprising both intermediate and residual data. This learned dictionary establishes a correlation between the two types of data. Subsequently, a specific dictionary tailored to the input CFA is developed. Utilizing this adaptive dictionary, the sparse coefficients of the intermediate data are calculated and transformed to predict the residual image. The residual image is then combined with the intermediate image to yield the final reconstruction image.\n\nExperimental results showcase the cutting-edge performance in terms of Peak Signal-to-Noise Ratio (PSNR) and subjective visual perception."}, "2406.01300": {"original_text": "  Text-guided image generation enables the creation of visual content from\ntextual descriptions. However, certain visual concepts cannot be effectively\nconveyed through language alone. This has sparked a renewed interest in\nutilizing the CLIP image embedding space for more visually-oriented tasks\nthrough methods such as IP-Adapter. Interestingly, the CLIP image embedding\nspace has been shown to be semantically meaningful, where linear operations\nwithin this space yield semantically meaningful results. Yet, the specific\nmeaning of these operations can vary unpredictably across different images. To\nharness this potential, we introduce pOps, a framework that trains specific\nsemantic operators directly on CLIP image embeddings. Each pOps operator is\nbuilt upon a pretrained Diffusion Prior model. While the Diffusion Prior model\nwas originally trained to map between text embeddings and image embeddings, we\ndemonstrate that it can be tuned to accommodate new input conditions, resulting\nin a diffusion operator. Working directly over image embeddings not only\nimproves our ability to learn semantic operations but also allows us to\ndirectly use a textual CLIP loss as an additional supervision when needed. We\nshow that pOps can be used to learn a variety of photo-inspired operators with\ndistinct semantic meanings, highlighting the semantic diversity and potential\nof our proposed approach.\n", "rewritten_text": "Text-guided image generation allows for the creation of visual content based on textual descriptions. However, some visual concepts are challenging to convey through language alone. This has led to a renewed interest in leveraging the CLIP image embedding space for tasks that are more visually oriented, such as through methods like IP-Adapter. It is noteworthy that the CLIP image embedding space has demonstrated semantic meaningfulness, with linear operations within this space producing semantically meaningful outcomes. However, the specific interpretations of these operations can vary unpredictably across different images.\n\nTo capitalize on this potential, we introduce pOps, a framework that trains specific semantic operators directly on CLIP image embeddings. Each pOps operator is constructed on a pretrained Diffusion Prior model. While the Diffusion Prior model was initially trained to map between text embeddings and image embeddings, we illustrate that it can be fine-tuned to adapt to new input conditions, resulting in a diffusion operator. By operating directly on image embeddings, we not only enhance our capacity to learn semantic operations but also enable the direct utilization of a textual CLIP loss as additional supervision when necessary.\n\nOur study demonstrates that pOps can be utilized to acquire a range of photo-inspired operators with distinct semantic meanings, underscoring the semantic diversity and potential of our proposed approach."}, "1805.07477": {"original_text": "  Augmenting neural networks with skip connections, as introduced in the\nso-called ResNet architecture, surprised the community by enabling the training\nof networks of more than 1,000 layers with significant performance gains. This\npaper deciphers ResNet by analyzing the effect of skip connections, and puts\nforward new theoretical results on the advantages of identity skip connections\nin neural networks. We prove that the skip connections in the residual blocks\nfacilitate preserving the norm of the gradient, and lead to stable\nback-propagation, which is desirable from optimization perspective. We also\nshow that, perhaps surprisingly, as more residual blocks are stacked, the\nnorm-preservation of the network is enhanced. Our theoretical arguments are\nsupported by extensive empirical evidence. Can we push for extra\nnorm-preservation? We answer this question by proposing an efficient method to\nregularize the singular values of the convolution operator and making the\nResNet's transition layers extra norm-preserving. Our numerical investigations\ndemonstrate that the learning dynamics and the classification performance of\nResNet can be improved by making it even more norm preserving. Our results and\nthe introduced modification for ResNet, referred to as Procrustes ResNets, can\nbe used as a guide for training deeper networks and can also inspire new deeper\narchitectures.\n", "rewritten_text": "The ResNet architecture, which incorporates skip connections to augment neural networks, has surprised the community by enabling the training of networks with over 1,000 layers while achieving significant performance gains. This paper aims to decode ResNet by examining the impact of skip connections and presenting new theoretical findings on the benefits of identity skip connections in neural networks. Our analysis demonstrates that the skip connections within the residual blocks aid in preserving the gradient norm, resulting in stable back-propagation, a desirable trait from an optimization standpoint. Furthermore, we establish that the norm-preservation of the network is enhanced as more residual blocks are added. These theoretical insights are substantiated by extensive empirical evidence.\n\nTo explore the potential for further norm-preservation, we propose an efficient method to regularize the singular values of the convolution operator and enhance the norm-preserving properties of ResNet's transition layers. Our numerical experiments reveal that by increasing norm preservation, the learning dynamics and classification performance of ResNet can be enhanced. The modifications introduced in this study, known as Procrustes ResNets, offer a roadmap for training deeper networks and may serve as a source of inspiration for developing new, more profound architectures."}, "2104.08668": {"original_text": "  Communicating new research ideas involves highlighting similarities and\ndifferences with past work. Authors write fluent, often long sections to survey\nthe distinction of a new paper with related work. In this work we model\ngenerating related work sections while being cognisant of the motivation behind\nciting papers. Our content planning model generates a tree of cited papers\nbefore a surface realization model lexicalizes this skeleton. Our model\noutperforms several strong state-of-the-art summarization and multi-document\nsummarization models on generating related work on an ACL Anthology (AA) based\ndataset which we contribute.\n", "rewritten_text": "In the process of communicating new research ideas, it is important to emphasize both the similarities and differences with previous work. Authors often write detailed sections to compare a new paper with existing research. In this study, we propose a model for creating sections on related work that takes into account the reasons for citing specific papers. Our approach involves generating a tree structure of cited papers using a content planning model, followed by lexicalizing this structure using a surface realization model. Our model has shown superior performance compared to several state-of-the-art summarization and multi-document summarization models when generating related work sections on a dataset from the ACL Anthology (AA) that we have contributed to."}, "1608.08021": {"original_text": "  This paper presents how we can achieve the state-of-the-art accuracy in\nmulti-category object detection task while minimizing the computational cost by\nadapting and combining recent technical innovations. Following the common\npipeline of \"CNN feature extraction + region proposal + RoI classification\", we\nmainly redesign the feature extraction part, since region proposal part is not\ncomputationally expensive and classification part can be efficiently compressed\nwith common techniques like truncated SVD. Our design principle is \"less\nchannels with more layers\" and adoption of some building blocks including\nconcatenated ReLU, Inception, and HyperNet. The designed network is deep and\nthin and trained with the help of batch normalization, residual connections,\nand learning rate scheduling based on plateau detection. We obtained solid\nresults on well-known object detection benchmarks: 83.8% mAP (mean average\nprecision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only\n750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA\nTitan X GPU. Theoretically, our network requires only 12.3% of the\ncomputational cost compared to ResNet-101, the winner on VOC2012.\n", "rewritten_text": "This paper demonstrates how state-of-the-art accuracy in multi-category object detection tasks can be achieved while minimizing computational costs through the adaptation and combination of recent technical innovations. The approach follows the common pipeline of \"CNN feature extraction + region proposal + RoI classification.\" The focus is primarily on redesigning the feature extraction component, as the region proposal part is not computationally intensive, and the classification part can be efficiently compressed using techniques like truncated SVD.\n\nThe design principle revolves around \"less channels with more layers,\" incorporating building blocks such as concatenated ReLU, Inception, and HyperNet. The resulting network is deep and thin, trained with the assistance of batch normalization, residual connections, and learning rate scheduling based on plateau detection. The achieved results on established object detection benchmarks are notable: 83.8% mAP (mean average precision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place). The computational efficiency is also impressive, with processing times of only 750ms/image on an Intel i7-6700K CPU with a single core and 46ms/image on an NVIDIA Titan X GPU. The network's computational cost is theoretically just 12.3% compared to ResNet-101, the winner on VOC2012."}, "2210.05738": {"original_text": "  In this work, we propose to explicitly use the landmarks of prostate to guide\nthe MR-TRUS image registration. We first train a deep neural network to\nautomatically localize a set of meaningful landmarks, and then directly\ngenerate the affine registration matrix from the location of these landmarks.\nFor landmark localization, instead of directly training a network to predict\nthe landmark coordinates, we propose to regress a full-resolution distance map\nof the landmark, which is demonstrated effective in avoiding statistical bias\nto unsatisfactory performance and thus improving performance. We then use the\npredicted landmarks to generate the affine transformation matrix, which\noutperforms the clinicians' manual rigid registration by a significant margin\nin terms of TRE.\n", "rewritten_text": "In this study, we propose a method that utilizes prostate landmarks to guide MR-TRUS image registration explicitly. Initially, a deep neural network is trained to automatically identify significant landmarks within the prostate. Subsequently, an affine registration matrix is directly generated based on the positions of these identified landmarks. Instead of training the network to predict landmark coordinates directly, we suggest regressing a full-resolution distance map of the landmarks. This approach has been shown to be effective in mitigating statistical bias, leading to improved performance. The predicted landmarks are then used to create the affine transformation matrix, which significantly outperforms manual rigid registration by clinicians in terms of Target Registration Error (TRE)."}, "2006.08844": {"original_text": "  We tackle the problem of establishing dense pixel-wise correspondences\nbetween a pair of images. In this work, we introduce Dual-Resolution\nCorrespondence Networks (DualRC-Net), to obtain pixel-wise correspondences in a\ncoarse-to-fine manner. DualRC-Net extracts both coarse- and fine- resolution\nfeature maps. The coarse maps are used to produce a full but coarse 4D\ncorrelation tensor, which is then refined by a learnable neighbourhood\nconsensus module. The fine-resolution feature maps are used to obtain the final\ndense correspondences guided by the refined coarse 4D correlation tensor. The\nselected coarse-resolution matching scores allow the fine-resolution features\nto focus only on a limited number of possible matches with high confidence. In\nthis way, DualRC-Net dramatically increases matching reliability and\nlocalisation accuracy, while avoiding to apply the expensive 4D convolution\nkernels on fine-resolution feature maps. We comprehensively evaluate our method\non large-scale public benchmarks including HPatches, InLoc, and Aachen\nDay-Night. It achieves the state-of-the-art results on all of them.\n", "rewritten_text": "In this study, we address the challenge of establishing dense pixel-wise correspondences between a pair of images. Our approach, Dual-Resolution Correspondence Networks (DualRC-Net), is introduced to achieve pixel-wise correspondences in a coarse-to-fine fashion. DualRC-Net leverages both coarse- and fine-resolution feature maps. The coarse maps are utilized to generate a complete yet coarse 4D correlation tensor, which is subsequently refined by a trainable neighborhood consensus module. Meanwhile, the fine-resolution feature maps are employed to derive the ultimate dense correspondences under the guidance of the refined coarse 4D correlation tensor. By utilizing selected coarse-resolution matching scores, the fine-resolution features are able to concentrate solely on a limited set of potential matches with high confidence. This approach significantly enhances matching reliability and localization accuracy of DualRC-Net, while circumventing the need for costly 4D convolution kernels on fine-resolution feature maps. Our method is thoroughly evaluated on prominent public benchmarks such as HPatches, InLoc, and Aachen Day-Night, where it outperforms existing techniques and achieves state-of-the-art results across all benchmarks."}, "1704.00405": {"original_text": "  As for semantic role labeling (SRL) task, when it comes to utilizing parsing\ninformation, both traditional methods and recent recurrent neural network (RNN)\nbased methods use the feature engineering way. In this paper, we propose Syntax\nAware Long Short Time Memory(SA-LSTM). The structure of SA-LSTM modifies\naccording to dependency parsing information in order to model parsing\ninformation directly in an architecture engineering way instead of feature\nengineering way. We experimentally demonstrate that SA-LSTM gains more\nimprovement from the model architecture. Furthermore, SA-LSTM outperforms the\nstate-of-the-art on CPB 1.0 significantly according to Student t-test\n($p<0.05$).\n", "rewritten_text": "In the realm of semantic role labeling (SRL) tasks, both traditional methods and more recent recurrent neural network (RNN)-based approaches rely on feature engineering when incorporating parsing information. This paper introduces Syntax-Aware Long Short-Term Memory (SA-LSTM), a novel model that adapts its structure based on dependency parsing information to directly integrate parsing details into the architecture, rather than through feature engineering. Our experimental results show that SA-LSTM achieves greater improvements through its architectural design. Moreover, SA-LSTM surpasses the current state-of-the-art performance on CPB 1.0 significantly, as confirmed by a Student t-test ($p<0.05$)."}, "2304.08481": {"original_text": "  High-definition (HD) semantic maps are crucial in enabling autonomous\nvehicles to navigate urban environments. The traditional method of creating\noffline HD maps involves labor-intensive manual annotation processes, which are\nnot only costly but also insufficient for timely updates. Recent studies have\nproposed an alternative approach that generates local maps using online sensor\nobservations. However, this approach is limited by the sensor's perception\nrange and its susceptibility to occlusions. In this study, we propose Neural\nMap Prior (NMP), a neural representation of global maps. This representation\nautomatically updates itself and improves the performance of local map\ninference. Specifically, we utilize two approaches to achieve this. Firstly, to\nintegrate a strong map prior into local map inference, we apply\ncross-attention, a mechanism that dynamically identifies correlations between\ncurrent and prior features. Secondly, to update the global neural map prior, we\nutilize a learning-based fusion module that guides the network in fusing\nfeatures from previous traversals. Our experimental results, based on the\nnuScenes dataset, demonstrate that our framework is highly compatible with\nvarious map segmentation and detection architectures. It significantly improves\nmap prediction performance, even in challenging weather conditions and\nsituations with a longer perception range. To the best of our knowledge, this\nis the first learning-based system for creating a global map prior.\n", "rewritten_text": "High-definition (HD) semantic maps play a crucial role in enabling autonomous vehicles to navigate urban environments. The traditional method of creating offline HD maps involves labor-intensive manual annotation processes, which are not only costly but also inadequate for timely updates. Recent studies have introduced an alternative approach that generates local maps using online sensor observations. However, this method is constrained by the sensor's perception range and its susceptibility to occlusions.\n\nIn this study, we introduce Neural Map Prior (NMP), a neural representation of global maps. This representation autonomously updates itself and enhances the performance of local map inference. Our approach involves two key strategies. Firstly, to incorporate a robust map prior into local map inference, we employ cross-attention, a mechanism that dynamically identifies correlations between current and prior features. Secondly, to update the global neural map prior, we utilize a learning-based fusion module that guides the network in merging features from previous traversals.\n\nOur experimental results, conducted on the nuScenes dataset, illustrate that our framework is highly compatible with various map segmentation and detection architectures. It notably enhances map prediction performance, even in challenging weather conditions and situations with an extended perception range. To the best of our knowledge, this represents the first learning-based system for establishing a global map prior."}, "2202.10108": {"original_text": "  Vision transformers have shown great potential in various computer vision\ntasks owing to their strong capability to model long-range dependency using the\nself-attention mechanism. Nevertheless, they treat an image as a 1D sequence of\nvisual tokens, lacking an intrinsic inductive bias (IB) in modeling local\nvisual structures and dealing with scale variance, which is instead learned\nimplicitly from large-scale training data with longer training schedules. In\nthis paper, we propose a Vision Transformer Advanced by Exploring intrinsic IB\nfrom convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid\nreduction modules to downsample and embed the input image into tokens with rich\nmulti-scale context using multiple convolutions with different dilation rates.\nIn this way, it acquires an intrinsic scale invariance IB and can learn robust\nfeature representation for objects at various scales. Moreover, in each\ntransformer layer, ViTAE has a convolution block parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. The proposed two kinds\nof cells are stacked in both isotropic and multi-stage manners to formulate two\nfamilies of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on\nthe ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and\nAP10K datasets validate the superiority of our models over the baseline\ntransformer models and concurrent works. Besides, we scale up our ViTAE model\nto 644M parameters and obtain the state-of-the-art classification performance,\ni.e., 88.5% Top-1 classification accuracy on ImageNet validation set and the\nbest 91.2% Top-1 accuracy on ImageNet real validation set, without using extra\nprivate data.\n", "rewritten_text": "Vision transformers have demonstrated significant potential in various computer vision tasks due to their strong ability to model long-range dependencies using the self-attention mechanism. However, they view an image as a 1D sequence of visual tokens, lacking an inherent inductive bias (IB) in capturing local visual structures and addressing scale variance, which is instead learned implicitly from extensive training data with extended training schedules. \n\nIn this study, we introduce ViTAE (Vision Transformer Advanced by Exploring intrinsic IB from convolutions), which incorporates several spatial pyramid reduction modules to downsample and embed the input image into tokens with diverse multi-scale context using multiple convolutions with varying dilation rates. This approach enables ViTAE to acquire an intrinsic scale invariance IB and to learn robust feature representations for objects at different scales. \n\nFurthermore, within each transformer layer, ViTAE includes a convolution block running in parallel to the multi-head self-attention module, with their features fused and fed into the feed-forward network. As a result, ViTAE possesses an inherent locality IB and can effectively learn both local features and global dependencies in a collaborative manner. \n\nThe proposed ViTAE architecture consists of two types of cells stacked in isotropic and multi-stage configurations, leading to the creation of two ViTAE model families: vanilla ViTAE and ViTAEv2. Experimental results on the ImageNet dataset and downstream tasks on the MS COCO, ADE20K, and AP10K datasets confirm the superior performance of our models compared to baseline transformer models and existing works. \n\nMoreover, by scaling up our ViTAE model to 644M parameters, we achieve state-of-the-art classification results, specifically 88.5% Top-1 classification accuracy on the ImageNet validation set and the highest 91.2% Top-1 accuracy on the ImageNet real validation set, all without the use of additional private data."}, "1809.08371": {"original_text": "  Human pose estimation is an essential yet challenging task in computer\nvision. One of the reasons for this difficulty is that there are many redundant\nregions in the images. In this work, we proposed a convolutional network\narchitecture combined with the novel attention model. We named it attention\nconvolutional neural network (ACNN). ACNN learns to focus on specific regions\nof different input features. It's a multi-stage architecture. Early stages\nfiltrate the \"nothing-regions\", such as background and redundant body parts.\nAnd then, they submit the important regions which contain the joints of the\nhuman body to the following stages to get a more accurate result. What's more,\nit does not require extra manual annotations and self-learning is one of our\nintentions. We separately trained the network because the attention learning\ntask and the pose estimation task are not independent. State-of-the-art\nperformance is obtained on the MPII benchmarks.\n", "rewritten_text": "Human pose estimation presents a crucial yet challenging task in computer vision due to the presence of numerous redundant regions in images. In this study, we introduce a novel approach combining a convolutional network architecture with an attention model, resulting in the attention convolutional neural network (ACNN). The ACNN is designed to learn to focus on specific regions within different input features. This multi-stage architecture first filters out \"nothing-regions,\" such as background and redundant body parts, in the early stages. Subsequently, it identifies and prioritizes important regions containing the joints of the human body, which are then passed on to the following stages for a more precise estimation. Notably, our approach eliminates the need for additional manual annotations, emphasizing self-learning as a primary objective. We trained the network separately, recognizing that the attention learning task and pose estimation task are interdependent. Our method achieves state-of-the-art performance on the MPII benchmarks."}, "1310.0302": {"original_text": "  Surface registration is a technique that is used in various areas such as\nobject recognition and 3D model reconstruction. Problem of surface registration\ncan be analyzed as an optimization problem of seeking a rigid motion between\ntwo different views. Genetic algorithms can be used for solving this\noptimization problem, both for obtaining the robust parameter estimation and\nfor its fine-tuning. The main drawback of genetic algorithms is that they are\ntime consuming which makes them unsuitable for online applications. Modern\nacquisition systems enable the implementation of the solutions that would\nimmediately give the information on the rotational angles between the different\nviews, thus reducing the dimension of the optimization problem. The paper gives\nan analysis of the genetic algorithm implemented in the conditions when the\nrotation matrix is known and a comparison of these results with results when\nthis information is not available.\n", "rewritten_text": "Surface registration is a technique utilized in various fields such as object recognition and 3D model reconstruction. The problem of surface registration can be viewed as an optimization challenge involving the search for a rigid motion between two distinct perspectives. Genetic algorithms offer a solution to this optimization problem, aiding in robust parameter estimation and fine-tuning. However, a notable drawback of genetic algorithms is their time-consuming nature, rendering them unsuitable for real-time applications. With advancements in modern acquisition systems, it is now possible to implement solutions that provide immediate information on rotational angles between different perspectives, thereby reducing the complexity of the optimization problem. This paper presents an analysis of a genetic algorithm implementation under the condition of a known rotation matrix, along with a comparison of results obtained when this information is unavailable."}, "2008.0194": {"original_text": "  While the progress of machine translation of written text has come far in the\npast several years thanks to the increasing availability of parallel corpora\nand corpora-based training technologies, automatic translation of spoken text\nand dialogues remains challenging even for modern systems. In this paper, we\naim to boost the machine translation quality of conversational texts by\nintroducing a newly constructed Japanese-English business conversation parallel\ncorpus. A detailed analysis of the corpus is provided along with challenging\nexamples for automatic translation. We also experiment with adding the corpus\nin a machine translation training scenario and show how the resulting system\nbenefits from its use.\n", "rewritten_text": "In recent years, significant progress has been made in machine translation of written text, largely due to the increased availability of parallel corpora and corpora-based training technologies. However, automatic translation of spoken text and dialogues remains a challenge for modern systems. This paper focuses on enhancing the quality of machine translation for conversational texts by introducing a newly developed Japanese-English business conversation parallel corpus. The paper includes a detailed analysis of the corpus, presents challenging examples for automatic translation, and explores the impact of incorporating the corpus into machine translation training. The results demonstrate the benefits of using this corpus in improving translation systems."}, "2204.08308": {"original_text": "  With the rapid development of multimedia technology, Augmented Reality (AR)\nhas become a promising next-generation mobile platform. The primary theory\nunderlying AR is human visual confusion, which allows users to perceive the\nreal-world scenes and augmented contents (virtual-world scenes) simultaneously\nby superimposing them together. To achieve good Quality of Experience (QoE), it\nis important to understand the interaction between two scenarios, and\nharmoniously display AR contents. However, studies on how this superimposition\nwill influence the human visual attention are lacking. Therefore, in this\npaper, we mainly analyze the interaction effect between background (BG) scenes\nand AR contents, and study the saliency prediction problem in AR. Specifically,\nwe first construct a Saliency in AR Dataset (SARD), which contains 450 BG\nimages, 450 AR images, as well as 1350 superimposed images generated by\nsuperimposing BG and AR images in pair with three mixing levels. A large-scale\neye-tracking experiment among 60 subjects is conducted to collect eye movement\ndata. To better predict the saliency in AR, we propose a vector quantized\nsaliency prediction method and generalize it for AR saliency prediction. For\ncomparison, three benchmark methods are proposed and evaluated together with\nour proposed method on our SARD. Experimental results demonstrate the\nsuperiority of our proposed method on both of the common saliency prediction\nproblem and the AR saliency prediction problem over benchmark methods. Our\ndataset and code are available at: https://github.com/DuanHuiyu/ARSaliency.\n", "rewritten_text": "With the rapid development of multimedia technology, Augmented Reality (AR) has emerged as a promising next-generation mobile platform. The primary theory underlying AR is human visual fusion, enabling users to simultaneously perceive real-world scenes and augmented contents (virtual-world scenes) by superimposing them. To ensure a high Quality of Experience (QoE), it is crucial to comprehend the interaction between these two scenarios and seamlessly display AR contents. However, there is a lack of research on how this superimposition affects human visual attention. Therefore, this paper focuses on analyzing the interaction effect between background (BG) scenes and AR contents, as well as studying the saliency prediction issue in AR.\n\nTo address this, we have developed a Saliency in AR Dataset (SARD) comprising 450 BG images, 450 AR images, and 1350 superimposed images created by pairing BG and AR images at three mixing levels. A comprehensive eye-tracking experiment involving 60 subjects was conducted to gather eye movement data. To enhance saliency prediction in AR, we introduce a vector quantized saliency prediction method tailored for AR saliency prediction. Additionally, three benchmark methods are proposed and evaluated alongside our method using the SARD dataset. The experimental results showcase the superiority of our proposed method in addressing both common saliency prediction challenges and the AR saliency prediction problem compared to the benchmark methods.\n\nFor those interested, our dataset and code can be accessed at: https://github.com/DuanHuiyu/ARSaliency."}, "2210.16865": {"original_text": "  Explicit decomposition modeling, which involves breaking down complex tasks\ninto more straightforward and often more interpretable sub-tasks, has long been\na central theme in developing robust and interpretable NLU systems. However,\ndespite the many datasets and resources built as part of this effort, the\nmajority have small-scale annotations and limited scope, which is insufficient\nto solve general decomposition tasks. In this paper, we look at large-scale\nintermediate pre-training of decomposition-based transformers using distant\nsupervision from comparable texts, particularly large-scale parallel news. We\nshow that with such intermediate pre-training, developing robust\ndecomposition-based models for a diverse range of tasks becomes more feasible.\nFor example, on semantic parsing, our model, DecompT5, improves 20% to 30% on\ntwo datasets, Overnight and TORQUE, over the baseline language model. We\nfurther use DecompT5 to build a novel decomposition-based QA system named\nDecompEntail, improving over state-of-the-art models, including GPT-3, on both\nHotpotQA and StrategyQA by 8% and 4%, respectively.\n", "rewritten_text": "Explicit decomposition modeling, which involves breaking down complex tasks into simpler and more interpretable sub-tasks, has been a fundamental concept in the development of robust and interpretable NLU systems for a long time. Despite the creation of numerous datasets and resources in pursuit of this goal, many of them have limited annotations and narrow scopes, rendering them inadequate for addressing general decomposition tasks. \n\nIn this paper, we focus on the large-scale intermediate pre-training of decomposition-based transformers by leveraging distant supervision from comparable texts, particularly from extensive parallel news sources. We demonstrate that through such intermediate pre-training, the creation of robust decomposition-based models for a wide range of tasks becomes more achievable. For instance, in the domain of semantic parsing, our model, DecompT5, exhibits a 20% to 30% improvement on two datasets, Overnight and TORQUE, compared to the baseline language model.\n\nFurthermore, we utilize DecompT5 to construct a novel decomposition-based QA system called DecompEntail, which outperforms state-of-the-art models, including GPT-3, on both HotpotQA and StrategyQA by 8% and 4%, respectively."}, "2103.16364": {"original_text": "  Unsupervised person re-identification (ReID) aims at learning discriminative\nidentity features without annotations. Recently, self-supervised contrastive\nlearning has gained increasing attention for its effectiveness in unsupervised\nrepresentation learning. The main idea of instance contrastive learning is to\nmatch a same instance in different augmented views. However, the relationship\nbetween different instances has not been fully explored in previous contrastive\nmethods, especially for instance-level contrastive loss. To address this issue,\nwe propose Inter-instance Contrastive Encoding (ICE) that leverages\ninter-instance pairwise similarity scores to boost previous class-level\ncontrastive ReID methods. We first use pairwise similarity ranking as one-hot\nhard pseudo labels for hard instance contrast, which aims at reducing\nintra-class variance. Then, we use similarity scores as soft pseudo labels to\nenhance the consistency between augmented and original views, which makes our\nmodel more robust to augmentation perturbations. Experiments on several\nlarge-scale person ReID datasets validate the effectiveness of our proposed\nunsupervised method ICE, which is competitive with even supervised methods.\nCode is made available at https://github.com/chenhao2345/ICE.\n", "rewritten_text": "Unsupervised person re-identification (ReID) aims to learn discriminative identity features without annotations. Recently, self-supervised contrastive learning has garnered increasing attention for its effectiveness in unsupervised representation learning. The primary concept of instance contrastive learning involves matching the same instance in different augmented views. However, the relationship between different instances has not been thoroughly explored in previous contrastive methods, particularly for instance-level contrastive loss. To tackle this issue, we introduce Inter-instance Contrastive Encoding (ICE), which utilizes inter-instance pairwise similarity scores to enhance existing class-level contrastive ReID methods. Initially, we employ pairwise similarity ranking as one-hot hard pseudo labels for hard instance contrast, with the goal of reducing intra-class variance. Subsequently, we utilize similarity scores as soft pseudo labels to improve the consistency between augmented and original views, thereby enhancing the robustness of our model against augmentation perturbations. Experiments conducted on several large-scale person ReID datasets confirm the effectiveness of our proposed unsupervised method ICE, demonstrating competitiveness even with supervised methods. The code is accessible at https://github.com/chenhao2345/ICE."}, "1607.07604": {"original_text": "  The interpretation and analysis of the wireless capsule endoscopy recording\nis a complex task which requires sophisticated computer aided decision (CAD)\nsystems in order to help physicians with the video screening and, finally, with\nthe diagnosis. Most of the CAD systems in the capsule endoscopy share a common\nsystem design, but use very different image and video representations. As a\nresult, each time a new clinical application of WCE appears, new CAD system has\nto be designed from scratch. This characteristic makes the design of new CAD\nsystems a very time consuming. Therefore, in this paper we introduce a system\nfor small intestine motility characterization, based on Deep Convolutional\nNeural Networks, which avoids the laborious step of designing specific features\nfor individual motility events. Experimental results show the superiority of\nthe learned features over alternative classifiers constructed by using state of\nthe art hand-crafted features. In particular, it reaches a mean classification\naccuracy of 96% for six intestinal motility events, outperforming the other\nclassifiers by a large margin (a 14% relative performance increase).\n", "rewritten_text": "The analysis and interpretation of wireless capsule endoscopy recordings is a complex task that necessitates sophisticated computer-aided decision (CAD) systems to assist physicians in video screening and diagnosis. While most CAD systems in capsule endoscopy share a common system design, they utilize varying image and video representations. Consequently, with each new clinical application of Wireless Capsule Endoscopy (WCE), a new CAD system must be developed from scratch, making the process time-consuming. In this paper, we present a system for characterizing small intestine motility based on Deep Convolutional Neural Networks. This system eliminates the need for designing specific features for individual motility events, streamlining the process. Experimental results demonstrate the superior performance of the learned features compared to alternative classifiers that rely on state-of-the-art hand-crafted features. Specifically, the system achieves a mean classification accuracy of 96% for six intestinal motility events, surpassing other classifiers by a significant margin (a 14% relative performance increase)."}, "2206.12262": {"original_text": "  Microblogs have become a social platform for people to express their emotions\nin real-time, and it is a trend to analyze user emotional tendencies from the\ninformation on Microblogs. The dynamic features of emojis can affect the\nsentiment polarity of microblog texts. Since existing models seldom consider\nthe diversity of emoji sentiment polarity,the paper propose a microblog\nsentiment classification model based on ALBERT-FAET. We obtain text embedding\nvia ALBERT pretraining model and learn the inter-emoji embedding with an\nattention-based LSTM network. In addition, a fine-grained attention mechanism\nis proposed to capture the word-level interactions between plain text and\nemoji. Finally, we concatenate these features and feed them into a CNN\nclassifier to predict the sentiment labels of the microblogs. To verify the\neffectiveness of the model and the fine-grained attention network, we conduct\ncomparison experiments and ablation experiments. The comparison experiments\nshow that the model outperforms previous methods in three evaluation indicators\n(accuracy, precision, and recall) and the model can significantly improve\nsentiment classification. The ablation experiments show that compared with\nALBERT-AET, the proposed model ALBERT-FAET is better in the metrics, indicating\nthat the fine-grained attention network can understand the diversified\ninformation of emoticons.\n", "rewritten_text": "Microblogs have evolved into a social platform where individuals can express their emotions in real-time, leading to a growing trend of analyzing user emotional tendencies through the content shared on these platforms. The dynamic nature of emojis plays a significant role in influencing the sentiment polarity of microblog texts. However, existing models often overlook the diverse sentiment polarities associated with emojis. In this paper, we introduce a microblog sentiment classification model based on ALBERT-FAET. Our approach involves obtaining text embeddings using the ALBERT pretraining model and learning inter-emoji embeddings through an attention-based LSTM network. Furthermore, we propose a fine-grained attention mechanism to capture word-level interactions between plain text and emojis. These features are then concatenated and input into a CNN classifier to predict sentiment labels for microblogs. To validate the effectiveness of our model and the fine-grained attention network, we conduct comparison and ablation experiments. The comparison experiments demonstrate that our model surpasses previous methods in terms of accuracy, precision, and recall, significantly enhancing sentiment classification. The ablation experiments reveal that, compared to ALBERT-AET, our proposed model ALBERT-FAET performs better across metrics, indicating that the fine-grained attention network effectively comprehends the diverse information conveyed by emoticons."}, "1910.07481": {"original_text": "  In Machine Translation, considering the document as a whole can help to\nresolve ambiguities and inconsistencies. In this paper, we propose a simple yet\npromising approach to add contextual information in Neural Machine Translation.\nWe present a method to add source context that capture the whole document with\naccurate boundaries, taking every word into account. We provide this additional\ninformation to a Transformer model and study the impact of our method on three\nlanguage pairs. The proposed approach obtains promising results in the\nEnglish-German, English-French and French-English document-level translation\ntasks. We observe interesting cross-sentential behaviors where the model learns\nto use document-level information to improve translation coherence.\n", "rewritten_text": "In the field of Machine Translation, considering the document as a whole can help resolve ambiguities and inconsistencies. This paper introduces a simple yet promising approach to incorporating contextual information in Neural Machine Translation. The method proposed involves adding source context that captures the entire document with precise boundaries, taking into account every word. This additional information is provided to a Transformer model, and the impact of this method is studied on three language pairs. The approach yields promising results in document-level translation tasks for English-German, English-French, and French-English pairs. Notably, the model demonstrates intriguing cross-sentential behaviors, indicating its ability to leverage document-level information to enhance translation coherence."}, "2007.02424": {"original_text": "  Recent advances in unsupervised domain adaptation for semantic segmentation\nhave shown great potentials to relieve the demand of expensive per-pixel\nannotations. However, most existing works address the domain discrepancy by\naligning the data distributions of two domains at a global image level whereas\nthe local consistencies are largely neglected. This paper presents an\ninnovative local contextual-relation consistent domain adaptation (CrCDA)\ntechnique that aims to achieve local-level consistencies during the\nglobal-level alignment. The idea is to take a closer look at region-wise\nfeature representations and align them for local-level consistencies.\nSpecifically, CrCDA learns and enforces the prototypical local\ncontextual-relations explicitly in the feature space of a labelled source\ndomain while transferring them to an unlabelled target domain via\nbackpropagation-based adversarial learning. An adaptive entropy max-min\nadversarial learning scheme is designed to optimally align these hundreds of\nlocal contextual-relations across domain without requiring discriminator or\nextra computation overhead. The proposed CrCDA has been evaluated extensively\nover two challenging domain adaptive segmentation tasks (e.g., GTA5 to\nCityscapes and SYNTHIA to Cityscapes), and experiments demonstrate its superior\nsegmentation performance as compared with state-of-the-art methods.\n", "rewritten_text": "Recent advancements in unsupervised domain adaptation for semantic segmentation have demonstrated significant potential in reducing the need for costly per-pixel annotations. However, most existing studies focus on addressing domain differences by aligning the data distributions of two domains at a global image level, while overlooking local consistencies. This paper introduces an innovative technique called local contextual-relation consistent domain adaptation (CrCDA) that aims to achieve local-level consistencies alongside global-level alignment. The core concept involves examining region-wise feature representations and aligning them to ensure local-level consistencies. Specifically, CrCDA learns and enforces prototypical local contextual-relations explicitly in the feature space of a labeled source domain, then transfers them to an unlabeled target domain through backpropagation-based adversarial learning. An adaptive entropy max-min adversarial learning scheme is devised to effectively align hundreds of local contextual-relations across domains without the need for a discriminator or additional computational overhead. The proposed CrCDA is extensively evaluated on two challenging domain adaptive segmentation tasks (e.g., GTA5 to Cityscapes and SYNTHIA to Cityscapes), with experiments showcasing its superior segmentation performance compared to state-of-the-art methods."}, "1805.10047": {"original_text": "  Neural machine translation (NMT) has a drawback in that can generate only\nhigh-frequency words owing to the computational costs of the softmax function\nin the output layer.\n  In Japanese-English NMT, Japanese predicate conjugation causes an increase in\nvocabulary size. For example, one verb can have as many as 19 surface\nvarieties. In this research, we focus on predicate conjugation for compressing\nthe vocabulary size in Japanese. The vocabulary list is filled with the various\nforms of verbs. We propose methods using predicate conjugation information\nwithout discarding linguistic information. The proposed methods can generate\nlow-frequency words and deal with unknown words. Two methods were considered to\nintroduce conjugation information: the first considers it as a token\n(conjugation token) and the second considers it as an embedded vector\n(conjugation feature).\n  The results using these methods demonstrate that the vocabulary size can be\ncompressed by approximately 86.1% (Tanaka corpus) and the NMT models can output\nthe words not in the training data set. Furthermore, BLEU scores improved by\n0.91 points in Japanese-to-English translation, and 0.32 points in\nEnglish-to-Japanese translation with ASPEC.\n", "rewritten_text": "Neural machine translation (NMT) faces a limitation in that it can only generate high-frequency words due to the computational costs associated with the softmax function in the output layer. In Japanese-English NMT, the conjugation of Japanese predicates leads to an expansion in vocabulary size. For instance, a single verb can manifest in up to 19 different surface forms. This study concentrates on predicate conjugation as a means to reduce the overall vocabulary size in Japanese. The vocabulary list encompasses various verb forms. We propose techniques that leverage predicate conjugation information while retaining linguistic nuances, enabling the generation of low-frequency and unknown words. Two approaches were explored for incorporating conjugation information: one treats it as a token (conjugation token), while the other treats it as an embedded vector (conjugation feature).\n\nThe outcomes obtained through these methodologies reveal that the vocabulary size can be reduced by approximately 86.1% (Tanaka corpus), and the NMT models can produce words not present in the training dataset. Additionally, there was an enhancement of 0.91 points in BLEU scores for Japanese-to-English translation and 0.32 points for English-to-Japanese translation using ASPEC."}, "1703.01028": {"original_text": "  Outlier detection and cluster number estimation is an important issue for\nclustering real data. This paper focuses on spectral clustering, a time-tested\nclustering method, and reveals its important properties related to outliers.\nThe highlights of this paper are the following two mathematical observations:\nfirst, spectral clustering's intrinsic property of an outlier cluster\nformation, and second, the singularity of an outlier cluster with a valid\ncluster number. Based on these observations, we designed a function that\nevaluates clustering and outlier detection results. In experiments, we prepared\ntwo scenarios, face clustering in photo album and person re-identification in a\ncamera network. We confirmed that the proposed method detects outliers and\nestimates the number of clusters properly in both problems. Our method\noutperforms state-of-the-art methods in both the 128-dimensional sparse space\nfor face clustering and the 4,096-dimensional non-sparse space for person\nre-identification.\n", "rewritten_text": "Outlier detection and estimation of cluster numbers are crucial issues in clustering real data. This paper delves into spectral clustering, a well-established clustering method, and uncovers its significant properties in relation to outliers. The key points of this paper revolve around two mathematical observations: firstly, the inherent property of spectral clustering to form outlier clusters, and secondly, the singularity of an outlier cluster with a valid cluster number. Building upon these observations, we have devised a function to assess clustering and outlier detection outcomes. In our experiments, we considered two scenarios: face clustering in a photo album and person re-identification in a camera network. Our findings confirm that the proposed method effectively detects outliers and accurately estimates the number of clusters in both scenarios. Furthermore, our method surpasses state-of-the-art techniques in both the 128-dimensional sparse space for face clustering and the 4,096-dimensional non-sparse space for person re-identification."}, "2402.19122": {"original_text": "  Gait recognition stands as one of the most pivotal remote identification\ntechnologies and progressively expands across research and industry\ncommunities. However, existing gait recognition methods heavily rely on\ntask-specific upstream driven by supervised learning to provide explicit gait\nrepresentations like silhouette sequences, which inevitably introduce expensive\nannotation costs and potential error accumulation. Escaping from this trend,\nthis work explores effective gait representations based on the all-purpose\nknowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a\nsimple yet efficient gait framework, termed BigGait. Specifically, the Gait\nRepresentation Extractor (GRE) within BigGait draws upon design principles from\nestablished gait representations, effectively transforming all-purpose\nknowledge into implicit gait representations without requiring third-party\nsupervision signals. Experiments on CCPG, CAISA-B* and SUSTech1K indicate that\nBigGait significantly outperforms the previous methods in both within-domain\nand cross-domain tasks in most cases, and provides a more practical paradigm\nfor learning the next-generation gait representation. Finally, we delve into\nprospective challenges and promising directions in LVMs-based gait recognition,\naiming to inspire future work in this emerging topic. The source code is\navailable at https://github.com/ShiqiYu/OpenGait.\n", "rewritten_text": "Gait recognition is considered a crucial technology for remote identification, with increasing adoption in research and industry communities. However, current gait recognition methods heavily depend on task-specific approaches driven by supervised learning to generate explicit gait representations such as silhouette sequences. This reliance leads to high annotation costs and the potential for error accumulation. To break away from this pattern, this study explores effective gait representations based on the general knowledge generated by task-agnostic Large Vision Models (LVMs). It introduces a straightforward yet efficient gait framework called BigGait.\n\nThe Gait Representation Extractor (GRE) in BigGait incorporates design principles from established gait representations to convert general knowledge into implicit gait representations without the need for external supervision signals. Experimental results on CCPG, CAISA-B*, and SUSTech1K datasets demonstrate that BigGait consistently outperforms previous methods in both within-domain and cross-domain tasks, offering a more practical approach to learning the next-generation gait representation.\n\nFurthermore, the study discusses potential challenges and future directions in LVMs-based gait recognition, aiming to inspire further research in this emerging field. The source code for this work can be accessed at https://github.com/ShiqiYu/OpenGait."}, "1808.03959": {"original_text": "  Deep Learning based stereo matching methods have shown great successes and\nachieved top scores across different benchmarks. However, like most data-driven\nmethods, existing deep stereo matching networks suffer from some well-known\ndrawbacks such as requiring large amount of labeled training data, and that\ntheir performances are fundamentally limited by the generalization ability. In\nthis paper, we propose a novel Recurrent Neural Network (RNN) that takes a\ncontinuous (possibly previously unseen) stereo video as input, and directly\npredicts a depth-map at each frame without a pre-training process, and without\nthe need of ground-truth depth-maps as supervision. Thanks to the recurrent\nnature (provided by two convolutional-LSTM blocks), our network is able to\nmemorize and learn from its past experiences, and modify its inner parameters\n(network weights) to adapt to previously unseen or unfamiliar environments.\nThis suggests a remarkable generalization ability of the net, making it\napplicable in an {\\em open world} setting. Our method works robustly with\nchanges in scene content, image statistics, and lighting and season conditions\n{\\em etc}. By extensive experiments, we demonstrate that the proposed method\nseamlessly adapts between different scenarios. Equally important, in terms of\nthe stereo matching accuracy, it outperforms state-of-the-art deep stereo\napproaches on standard benchmark datasets such as KITTI and Middlebury stereo.\n", "rewritten_text": "Deep learning-based stereo matching methods have demonstrated significant success and have achieved top scores across various benchmarks. However, like most data-driven approaches, current deep stereo matching networks suffer from well-known drawbacks, such as the need for a large amount of labeled training data and limitations in their performance due to generalization ability constraints.\n\nIn this paper, we introduce a novel Recurrent Neural Network (RNN) that can process continuous stereo video input, potentially unseen before, and directly generate a depth map for each frame without requiring a pre-training phase or ground-truth depth maps for supervision. Leveraging the recurrent nature of the network, facilitated by two convolutional-LSTM blocks, our model can retain and learn from past experiences, adjusting its internal parameters (network weights) to adapt to new or unfamiliar environments. This characteristic highlights the network's exceptional generalization capability, enabling its application in an \"open world\" context.\n\nOur method exhibits robust performance in varying scenarios, including changes in scene content, image statistics, lighting conditions, and seasonal variations. Through extensive experimentation, we illustrate the seamless adaptability of the proposed approach across different scenarios. Notably, in terms of stereo matching accuracy, our method surpasses state-of-the-art deep stereo techniques on established benchmark datasets like KITTI and Middlebury stereo."}, "2206.12571": {"original_text": "  This competition focus on Urban-Sense Segmentation based on the vehicle\ncamera view. Class highly unbalanced Urban-Sense images dataset challenge the\nexisting solutions and further studies. Deep Conventional neural network-based\nsemantic segmentation methods such as encoder-decoder architecture and\nmulti-scale and pyramid-based approaches become flexible solutions applicable\nto real-world applications. In this competition, we mainly review the\nliterature and conduct experiments on transformer-driven methods especially\nSegFormer, to achieve an optimal trade-off between performance and efficiency.\nFor example, SegFormer-B0 achieved 74.6% mIoU with the smallest FLOPS, 15.6G,\nand the largest model, SegFormer- B5 archived 80.2% mIoU. According to multiple\nfactors, including individual case failure analysis, individual class\nperformance, training pressure and efficiency estimation, the final candidate\nmodel for the competition is SegFormer- B2 with 50.6 GFLOPS and 78.5% mIoU\nevaluated on the testing set. Checkout our code implementation at\nhttps://vmv.re/cv3315.\n", "rewritten_text": "This competition focuses on Urban-Sense Segmentation using vehicle camera views. The highly unbalanced Urban-Sense image dataset challenges existing solutions and prompts further studies. Deep Convolutional Neural Network-based semantic segmentation methods, such as encoder-decoder architecture and multi-scale and pyramid-based approaches, have emerged as flexible solutions applicable to real-world scenarios. In this competition, we primarily review the literature and conduct experiments on transformer-driven methods, particularly SegFormer, to achieve an optimal balance between performance and efficiency. For instance, SegFormer-B0 achieved a 74.6% mIoU with the smallest FLOPS at 15.6G, while the largest model, SegFormer-B5, reached an 80.2% mIoU. Considering various factors, including individual case failure analysis, performance of individual classes, training pressure, and efficiency estimation, the selected model for the competition is SegFormer-B2, with 50.6 GFLOPS and a 78.5% mIoU on the testing set. You can find our code implementation at https://vmv.re/cv3315."}, "2204.11994": {"original_text": "  Digital pathological analysis is run as the main examination used for cancer\ndiagnosis. Recently, deep learning-driven feature extraction from pathology\nimages is able to detect genetic variations and tumor environment, but few\nstudies focus on differential gene expression in tumor cells. In this paper, we\npropose a self-supervised contrastive learning framework, HistCode, to infer\ndifferential gene expressions from whole slide images (WSIs). We leveraged\ncontrastive learning on large-scale unannotated WSIs to derive slide-level\nhistopathological feature in latent space, and then transfer it to tumor\ndiagnosis and prediction of differentially expressed cancer driver genes. Our\nextensive experiments showed that our method outperformed other\nstate-of-the-art models in tumor diagnosis tasks, and also effectively\npredicted differential gene expressions. Interestingly, we found the higher\nfold-changed genes can be more precisely predicted. To intuitively illustrate\nthe ability to extract informative features from pathological images, we\nspatially visualized the WSIs colored by the attentive scores of image tiles.\nWe found that the tumor and necrosis areas were highly consistent with the\nannotations of experienced pathologists. Moreover, the spatial heatmap\ngenerated by lymphocyte-specific gene expression patterns was also consistent\nwith the manually labeled WSI.\n", "rewritten_text": "Digital pathological analysis serves as the primary method for diagnosing cancer. While recent advancements in deep learning have enabled the extraction of features from pathology images to detect genetic variations and tumor environments, there has been limited focus on identifying differential gene expression in tumor cells. This paper introduces HistCode, a self-supervised contrastive learning framework designed to infer differential gene expressions from whole slide images (WSIs). By utilizing contrastive learning on a large scale of unannotated WSIs, we extract slide-level histopathological features in a latent space and apply them to tumor diagnosis and the prediction of differentially expressed cancer driver genes.\n\nOur comprehensive experiments demonstrate that our method surpasses other state-of-the-art models in tumor diagnosis tasks and effectively predicts differential gene expressions. Notably, we observed that genes with higher fold changes can be predicted more accurately. To visually showcase the capability of extracting informative features from pathological images, we spatially visualize the WSIs colored by the attentive scores of image tiles. Our findings reveal a high consistency between the tumor and necrosis areas identified by our method and the annotations made by experienced pathologists. Additionally, the spatial heatmap generated based on lymphocyte-specific gene expression patterns aligns well with manually labeled WSIs."}, "2309.07509": {"original_text": "  Generating realistic talking faces is a complex and widely discussed task\nwith numerous applications. In this paper, we present DiffTalker, a novel model\ndesigned to generate lifelike talking faces through audio and landmark\nco-driving. DiffTalker addresses the challenges associated with directly\napplying diffusion models to audio control, which are traditionally trained on\ntext-image pairs. DiffTalker consists of two agent networks: a\ntransformer-based landmarks completion network for geometric accuracy and a\ndiffusion-based face generation network for texture details. Landmarks play a\npivotal role in establishing a seamless connection between the audio and image\ndomains, facilitating the incorporation of knowledge from pre-trained diffusion\nmodels. This innovative approach efficiently produces articulate-speaking\nfaces. Experimental results showcase DiffTalker's superior performance in\nproducing clear and geometrically accurate talking faces, all without the need\nfor additional alignment between audio and image features.\n", "rewritten_text": "Generating realistic talking faces is a complex and widely discussed task with numerous applications. This paper introduces DiffTalker, a novel model designed to generate lifelike talking faces by utilizing audio and landmark co-driving. DiffTalker tackles the challenges associated with applying diffusion models directly to audio control, which are typically trained on text-image pairs. The model comprises two agent networks: a transformer-based landmarks completion network for geometric accuracy and a diffusion-based face generation network for texture details. Landmarks play a crucial role in establishing a seamless connection between the audio and image domains, enabling the integration of knowledge from pre-trained diffusion models. This innovative approach efficiently produces articulate-speaking faces. Experimental results demonstrate DiffTalker's superior performance in creating clear and geometrically accurate talking faces, all without requiring additional alignment between audio and image features."}, "2210.09071": {"original_text": "  Monocular Depth Estimation (MDE) aims to predict pixel-wise depth given a\nsingle RGB image. For both, the convolutional as well as the recent\nattention-based models, encoder-decoder-based architectures have been found to\nbe useful due to the simultaneous requirement of global context and pixel-level\nresolution. Typically, a skip connection module is used to fuse the encoder and\ndecoder features, which comprises of feature map concatenation followed by a\nconvolution operation. Inspired by the demonstrated benefits of attention in a\nmultitude of computer vision problems, we propose an attention-based fusion of\nencoder and decoder features. We pose MDE as a pixel query refinement problem,\nwhere coarsest-level encoder features are used to initialize pixel-level\nqueries, which are then refined to higher resolutions by the proposed Skip\nAttention Module (SAM). We formulate the prediction problem as ordinal\nregression over the bin centers that discretize the continuous depth range and\nintroduce a Bin Center Predictor (BCP) module that predicts bins at the\ncoarsest level using pixel queries. Apart from the benefit of image adaptive\ndepth binning, the proposed design helps learn improved depth embedding in\ninitial pixel queries via direct supervision from the ground truth. Extensive\nexperiments on the two canonical datasets, NYUV2 and KITTI, show that our\narchitecture outperforms the state-of-the-art by 5.3% and 3.9%, respectively,\nalong with an improved generalization performance by 9.4% on the SUNRGBD\ndataset. Code is available at https://github.com/ashutosh1807/PixelFormer.git.\n", "rewritten_text": "Monocular Depth Estimation (MDE) aims to predict pixel-wise depth from a single RGB image. Both convolutional and recent attention-based models have found encoder-decoder-based architectures to be useful, as they require global context and pixel-level resolution simultaneously. Typically, a skip connection module is utilized to merge encoder and decoder features, involving feature map concatenation followed by a convolution operation. Drawing inspiration from the demonstrated advantages of attention in various computer vision problems, we introduce an attention-based fusion of encoder and decoder features.\n\nWe frame MDE as a pixel query refinement problem, where the coarsest-level encoder features initialize pixel-level queries, refined to higher resolutions by the proposed Skip Attention Module (SAM). The prediction problem is formulated as ordinal regression over bin centers that discretize the continuous depth range. We introduce a Bin Center Predictor (BCP) module that predicts bins at the coarsest level using pixel queries. In addition to the benefits of image-adaptive depth binning, this design facilitates learning improved depth embedding in initial pixel queries through direct supervision from the ground truth.\n\nExtensive experiments on the NYUV2 and KITTI datasets demonstrate that our architecture outperforms the state-of-the-art by 5.3% and 3.9%, respectively, with an enhanced generalization performance by 9.4% on the SUNRGBD dataset. The code is available at https://github.com/ashutosh1807/PixelFormer.git."}, "2312.11716": {"original_text": "  Demand for efficient onboard object detection is increasing due to its key\nrole in autonomous navigation. However, deploying object detection models such\nas YOLO on resource constrained edge devices is challenging due to the high\ncomputational requirements of such models. In this paper, an compressed object\ndetection model named Squeezed Edge YOLO is examined. This model is compressed\nand optimized to kilobytes of parameters in order to fit onboard such edge\ndevices. To evaluate Squeezed Edge YOLO, two use cases - human and shape\ndetection - are used to show the model accuracy and performance. Moreover, the\nmodel is deployed onboard a GAP8 processor with 8 RISC-V cores and an NVIDIA\nJetson Nano with 4GB of memory. Experimental results show Squeezed Edge YOLO\nmodel size is optimized by a factor of 8x which leads to 76% improvements in\nenergy efficiency and 3.3x faster throughout.\n", "rewritten_text": "The demand for efficient onboard object detection is increasing due to its crucial role in autonomous navigation. However, deploying object detection models like YOLO on resource-constrained edge devices poses challenges because of the high computational requirements of such models. This paper examines a compressed object detection model called Squeezed Edge YOLO. This model has been compressed and optimized to kilobytes of parameters to be suitable for onboard deployment on edge devices. To evaluate Squeezed Edge YOLO, two use cases - human and shape detection - are utilized to demonstrate the model's accuracy and performance. Furthermore, the model is deployed on a GAP8 processor with 8 RISC-V cores and an NVIDIA Jetson Nano with 4GB of memory. Experimental results indicate that the Squeezed Edge YOLO model size has been optimized by a factor of 8x, resulting in a 76% improvement in energy efficiency and 3.3x faster throughput."}, "2409.15939": {"original_text": "  3D shape completion is traditionally solved using supervised training or by\ndistribution learning on complete shape examples. Recently self-supervised\nlearning approaches that do not require any complete 3D shape examples have\ngained more interests. In this paper, we propose a non-adversarial\nself-supervised approach for the shape completion task. Our first finding is\nthat completion problems can be formulated as an involutory function trivially,\nwhich implies a special constraint on the completion function G, such that\nG(G(X)) = X. Our second constraint on self-supervised shape completion relies\non the fact that shape completion becomes easier to solve with correspondences\nand similarly, completion can simplify the correspondences problem. We\nformulate a consistency measure in the canonical space in order to supervise\nthe completion function. We efficiently optimize the completion and\ncorrespondence modules using \"freeze and alternate\" strategy. The overall\napproach performs well for rigid shapes in a category as well as dynamic\nnon-rigid shapes. We ablate our design choices and compare our solution against\nstate-of-the-art methods, showing remarkable accuracy approaching supervised\naccuracy in some cases.\n", "rewritten_text": "The traditional approach to solving 3D shape completion involves supervised training or distribution learning on complete shape examples. However, there is a growing interest in self-supervised learning methods that do not rely on complete 3D shape examples. In this paper, we introduce a non-adversarial self-supervised approach for the shape completion task. Our key insight is that completion problems can be viewed as involutory functions, leading to a special constraint on the completion function G, such that G(G(X)) = X. Additionally, we leverage the relationship between shape completion and correspondences, showing that solving one can simplify the other. To supervise the completion function, we introduce a consistency measure in the canonical space. We optimize the completion and correspondence modules efficiently using a \"freeze and alternate\" strategy. Our approach demonstrates strong performance for both rigid shapes within a category and dynamic non-rigid shapes. We conduct ablation studies on our design choices and compare our method with state-of-the-art approaches, achieving accuracy levels close to supervised methods in certain cases."}, "1807.10552": {"original_text": "  Histology imaging is an essential diagnosis method to finalize the grade and\nstage of cancer of different tissues, especially for breast cancer diagnosis.\nSpecialists often disagree on the final diagnosis on biopsy tissue due to the\ncomplex morphological variety. Although convolutional neural networks (CNN)\nhave advantages in extracting discriminative features in image classification,\ndirectly training a CNN on high resolution histology images is computationally\ninfeasible currently. Besides, inconsistent discriminative features often\ndistribute over the whole histology image, which incurs challenges in\npatch-based CNN classification method. In this paper, we propose a novel\narchitecture for automatic classification of high resolution histology images.\nFirst, an adapted residual network is employed to explore hierarchical features\nwithout attenuation. Second, we develop a robust deep fusion network to utilize\nthe spatial relationship between patches and learn to correct the prediction\nbias generated from inconsistent discriminative feature distribution. The\nproposed method is evaluated using 10-fold cross-validation on 400 high\nresolution breast histology images with balanced labels and reports 95%\naccuracy on 4-class classification and 98.5% accuracy, 99.6% AUC on 2-class\nclassification (carcinoma and non-carcinoma), which substantially outperforms\nprevious methods and close to pathologist performance.\n", "rewritten_text": "Histology imaging plays a crucial role in diagnosing cancer by determining the grade and stage of various tissues, particularly in cases of breast cancer. Due to the intricate morphological variations, specialists often face challenges in reaching a unanimous diagnosis based on biopsy tissue samples. While convolutional neural networks (CNNs) excel in extracting distinctive features for image classification, training them directly on high-resolution histology images is currently computationally unfeasible. Moreover, the inconsistent distribution of discriminative features across the entire histology image poses difficulties for patch-based CNN classification methods.\n\nIn this study, we introduce a novel architecture for automatically classifying high-resolution histology images. Firstly, we utilize an adapted residual network to extract hierarchical features without loss of information. Secondly, we introduce a robust deep fusion network to leverage the spatial relationships between patches and address prediction biases resulting from the inconsistent distribution of discriminative features. Our proposed method is evaluated through 10-fold cross-validation on 400 high-resolution breast histology images with balanced labels. The results demonstrate a remarkable 95% accuracy in 4-class classification and 98.5% accuracy with 99.6% AUC in 2-class classification (carcinoma vs. non-carcinoma). These findings significantly outperform previous methods and approach the performance levels of experienced pathologists."}, "2205.11395": {"original_text": "  Hyperspectral anomalous change detection has been a challenging task for its\nemphasis on the dynamics of small and rare objects against the prevalent\nchanges. In this paper, we have proposed a Multi-Temporal spatial-spectral\nComparison Network for hyperspectral anomalous change detection (MTC-NET). The\nwhole model is a deep siamese network, aiming at learning the prevalent\nspectral difference resulting from the complex imaging conditions from the\nhyperspectral images by contrastive learning. A three-dimensional spatial\nspectral attention module is designed to effectively extract the spatial\nsemantic information and the key spectral differences. Then the gaps between\nthe multi-temporal features are minimized, boosting the alignment of the\nsemantic and spectral features and the suppression of the multi-temporal\nbackground spectral difference. The experiments on the \"Viareggio 2013\"\ndatasets demonstrate the effectiveness of proposed MTC-NET.\n", "rewritten_text": "Detecting anomalous changes in hyperspectral imagery has long been a challenging task due to the need to identify small and rare objects amidst common changes. This paper introduces the Multi-Temporal Spatial-Spectral Comparison Network (MTC-NET) for hyperspectral anomalous change detection. The model, a deep siamese network, focuses on learning the prevalent spectral differences caused by complex imaging conditions in hyperspectral images through contrastive learning. A three-dimensional spatial-spectral attention module is developed to extract spatial semantic information and key spectral variances effectively. By minimizing the gaps between multi-temporal features, the model enhances the alignment of semantic and spectral features while suppressing multi-temporal background spectral differences. Experimental results on the \"Viareggio 2013\" dataset validate the effectiveness of the proposed MTC-NET."}, "2109.01211": {"original_text": "  Reproducibility has become an intensely debated topic in NLP and ML over\nrecent years, but no commonly accepted way of assessing reproducibility, let\nalone quantifying it, has so far emerged. The assumption has been that wider\nscientific reproducibility terminology and definitions are not applicable to\nNLP/ML, with the result that many different terms and definitions have been\nproposed, some diametrically opposed. In this paper, we test this assumption,\nby taking the standard terminology and definitions from metrology and applying\nthem directly to NLP/ML. We find that we are able to straightforwardly derive a\npractical framework for assessing reproducibility which has the desirable\nproperty of yielding a quantified degree of reproducibility that is comparable\nacross different reproduction studies.\n", "rewritten_text": "Reproducibility has been a highly debated topic in the fields of Natural Language Processing (NLP) and Machine Learning (ML) in recent years. However, a universally accepted method for evaluating reproducibility, let alone quantifying it, has yet to be established. The prevailing belief has been that the broader scientific reproducibility concepts and definitions do not directly apply to NLP/ML. Consequently, numerous conflicting terms and definitions have been put forward. This paper challenges this assumption by adopting standard terminology and definitions from metrology and applying them to NLP/ML. The study demonstrates that by doing so, a practical framework for assessing reproducibility can be derived straightforwardly. This framework offers the advantage of providing a quantifiable measure of reproducibility that can be compared across various replication studies."}, "2101.1065": {"original_text": "  In machine translation field, in both academia and industry, there is a\ngrowing interest in increasingly powerful systems, using corpora of several\nhundred million to several billion examples. These systems represent the\nstate-of-the-art. Here we defend the idea of developing in parallel <<frugal>>\nbilingual translation systems, trained with relatively small corpora. Based on\nthe observation of a standard human professional translator, we estimate that\nthe corpora should be composed at maximum of a monolingual sub-corpus of 75\nmillion examples for the source language, a second monolingual sub-corpus of 6\nmillion examples for the target language, and an aligned bilingual sub-corpus\nof 6 million bi-examples. A less desirable alternative would be an aligned\nbilingual corpus of 47.5 million bi-examples.\n", "rewritten_text": "In the field of machine translation, there is a growing interest in developing increasingly powerful systems in both academia and industry. These systems utilize corpora containing several hundred million to several billion examples, representing the current state-of-the-art. In this context, we propose the concept of creating \"frugal\" bilingual translation systems in parallel, which are trained using relatively small corpora.\n\nDrawing inspiration from the practices of professional human translators, we suggest that these frugal systems should ideally be trained with a maximum of 75 million examples for the source language, 6 million examples for the target language, and 6 million aligned bilingual examples. A less optimal option would involve using an aligned bilingual corpus containing 47.5 million examples."}, "2211.10104": {"original_text": "  Stereo images, containing left and right view images with disparity, are\nutilized in solving low-vision tasks recently, e.g., rain removal and\nsuper-resolution. Stereo image restoration methods usually obtain better\nperformance than monocular methods by learning the disparity between dual views\neither implicitly or explicitly. However, existing stereo rain removal methods\nstill cannot make full use of the complementary information between two views,\nand we find it is because: 1) the rain streaks have more complex distributions\nin directions and densities, which severely damage the complementary\ninformation and pose greater challenges; 2) the disparity estimation is not\naccurate enough due to the imperfect fusion mechanism for the features between\ntwo views. To overcome such limitations, we propose a new \\underline{Stereo}\n\\underline{I}mage \\underline{R}ain \\underline{R}emoval method (StereoIRR) via\nsufficient interaction between two views, which incorporates: 1) a new\nDual-view Mutual Attention (DMA) mechanism which generates mutual attention\nmaps by taking left and right views as key information for each other to\nfacilitate cross-view feature fusion; 2) a long-range and cross-view\ninteraction, which is constructed with basic blocks and dual-view mutual\nattention, can alleviate the adverse effect of rain on complementary\ninformation to help the features of stereo images to get long-range and\ncross-view interaction and fusion. Notably, StereoIRR outperforms other related\nmonocular and stereo image rain removal methods on several datasets. Our codes\nand datasets will be released.\n", "rewritten_text": "Recently, stereo images, which consist of left and right view images with disparity, have been employed in addressing low-vision tasks such as rain removal and super-resolution. Stereo image restoration methods typically yield superior performance compared to monocular methods by learning the disparity between the dual views either implicitly or explicitly. However, existing stereo rain removal techniques still struggle to fully leverage the complementary information between the two views. This limitation arises from two main factors: 1) the complex distributions of rain streaks in terms of directions and densities, which significantly impair the complementary information and present greater challenges; and 2) the inaccurate disparity estimation resulting from an imperfect fusion mechanism for the features between the two views.\n\nTo address these limitations, we introduce a novel Stereo Image Rain Removal method (StereoIRR) that emphasizes sufficient interaction between the two views. This method incorporates: 1) a new Dual-view Mutual Attention (DMA) mechanism that generates mutual attention maps by utilizing the left and right views as key information for each other to enhance cross-view feature fusion; and 2) a long-range and cross-view interaction, which is established using basic blocks and dual-view mutual attention, to mitigate the negative impact of rain on complementary information. This approach aids in enabling the features of stereo images to engage in long-range and cross-view interaction and fusion effectively. Notably, StereoIRR surpasses other related monocular and stereo image rain removal methods across multiple datasets. Our codes and datasets will be made available for public access."}, "1704.07293": {"original_text": "  The accuracy of object detectors and trackers is most commonly evaluated by\nthe Intersection over Union (IoU) criterion. To date, most approaches are\nrestricted to axis-aligned or oriented boxes and, as a consequence, many\ndatasets are only labeled with boxes. Nevertheless, axis-aligned or oriented\nboxes cannot accurately capture an object's shape. To address this, a number of\ndensely segmented datasets has started to emerge in both the object detection\nand the object tracking communities. However, evaluating the accuracy of object\ndetectors and trackers that are restricted to boxes on densely segmented data\nis not straightforward. To close this gap, we introduce the relative\nIntersection over Union (rIoU) accuracy measure. The measure normalizes the IoU\nwith the optimal box for the segmentation to generate an accuracy measure that\nranges between 0 and 1 and allows a more precise measurement of accuracies.\nFurthermore, it enables an efficient and easy way to understand scenes and the\nstrengths and weaknesses of an object detection or tracking approach. We\ndisplay how the new measure can be efficiently calculated and present an\neasy-to-use evaluation framework. The framework is tested on the DAVIS and the\nVOT2016 segmentations and has been made available to the community.\n", "rewritten_text": "Object detectors and trackers are commonly assessed for accuracy using the Intersection over Union (IoU) criterion. Currently, most methods are limited to axis-aligned or oriented boxes, resulting in many datasets being labeled with only boxes. However, these boxes may not accurately represent an object's shape. To address this limitation, there has been a rise in densely segmented datasets within the object detection and tracking communities. Evaluating the accuracy of detectors and trackers that are confined to boxes on densely segmented data poses challenges. To bridge this gap, we propose the relative Intersection over Union (rIoU) accuracy measure. This measure normalizes the IoU with the optimal box for segmentation, providing an accuracy measure ranging from 0 to 1 for a more precise assessment. Additionally, it offers an efficient way to comprehend scenes and identify the strengths and weaknesses of object detection or tracking methods. We demonstrate the efficient calculation of the new measure and introduce a user-friendly evaluation framework. The framework has been tested on DAVIS and VOT2016 segmentations and is now available to the community."}, "2306.02819": {"original_text": "  Natural language understanding (NLU) is an essential branch of natural\nlanguage processing, which relies on representations generated by pre-trained\nlanguage models (PLMs). However, PLMs primarily focus on acquiring\nlexico-semantic information, while they may be unable to adequately handle the\nmeaning of constructions. To address this issue, we introduce construction\ngrammar (CxG), which highlights the pairings of form and meaning, to enrich\nlanguage representation. We adopt usage-based construction grammar as the basis\nof our work, which is highly compatible with statistical models such as PLMs.\nThen a HyCxG framework is proposed to enhance language representation through a\nthree-stage solution. First, all constructions are extracted from sentences via\na slot-constraints approach. As constructions can overlap with each other,\nbringing redundancy and imbalance, we formulate the conditional max coverage\nproblem for selecting the discriminative constructions. Finally, we propose a\nrelational hypergraph attention network to acquire representation from\nconstructional information by capturing high-order word interactions among\nconstructions. Extensive experiments demonstrate the superiority of the\nproposed model on a variety of NLU tasks.\n", "rewritten_text": "Natural Language Understanding (NLU) is a crucial aspect of natural language processing, relying on representations generated by pre-trained language models (PLMs). While PLMs primarily focus on acquiring lexico-semantic information, they may struggle to effectively interpret constructions. To tackle this challenge, we introduce Construction Grammar (CxG), emphasizing the relationship between form and meaning to enhance language representation. Our approach is grounded in usage-based construction grammar, which aligns well with statistical models like PLMs. We propose a HyCxG framework to improve language representation through a three-stage process. Initially, constructions are extracted from sentences using a slot-constraints method. Given that constructions can overlap, leading to redundancy and imbalance, we address this by formulating the conditional max coverage problem to select discriminative constructions. Finally, we introduce a relational hypergraph attention network to extract representation from constructional information by capturing high-order word interactions among constructions. Extensive experiments validate the effectiveness of our model across various NLU tasks."}, "1309.4573": {"original_text": "  This paper is based on an application of smoothing of 3D face images followed\nby feature detection i.e. detecting the nose tip. The present method uses a\nweighted mesh median filtering technique for smoothing. In this present\nsmoothing technique we have built the neighborhood surrounding a particular\npoint in 3D face and replaced that with the weighted value of the surrounding\npoints in 3D face image. After applying the smoothing technique to the 3D face\nimages our experimental results show that we have obtained considerable\nimprovement as compared to the algorithm without smoothing. We have used here\nthe maximum intensity algorithm for detecting the nose-tip and this method\ncorrectly detects the nose-tip in case of any pose i.e. along X, Y, and Z axes.\nThe present technique gave us worked successfully on 535 out of 542 3D face\nimages as compared to the method without smoothing which worked only on 521 3D\nface images out of 542 face images. Thus we have obtained a 98.70% performance\nrate over 96.12% performance rate of the algorithm without smoothing. All the\nexperiments have been performed on the FRAV3D database.\n", "rewritten_text": "This paper presents an application of smoothing 3D face images followed by feature detection, specifically detecting the nose tip. The method utilizes a weighted mesh median filtering technique for smoothing. In this technique, the neighborhood surrounding a particular point in the 3D face is built and replaced with the weighted value of the surrounding points in the 3D face image. Experimental results after applying the smoothing technique to the 3D face images demonstrate a significant improvement compared to the algorithm without smoothing. The nose-tip detection employs the maximum intensity algorithm, accurately identifying the nose-tip regardless of the pose along the X, Y, and Z axes. The method successfully detected the nose-tip in 535 out of 542 3D face images, in contrast to the method without smoothing, which only worked on 521 out of 542 face images. This represents a performance rate of 98.70% with the present technique, surpassing the 96.12% performance rate of the algorithm without smoothing. All experiments were conducted using the FRAV3D database."}, "2306.16682": {"original_text": "  Egocentric action anticipation aims to predict the future actions the camera\nwearer will perform from the observation of the past. While predictions about\nthe future should be available before the predicted events take place, most\napproaches do not pay attention to the computational time required to make such\npredictions. As a result, current evaluation schemes assume that predictions\nare available right after the input video is observed, i.e., presuming a\nnegligible runtime, which may lead to overly optimistic evaluations. We propose\na streaming egocentric action evaluation scheme which assumes that predictions\nare performed online and made available only after the model has processed the\ncurrent input segment, which depends on its runtime. To evaluate all models\nconsidering the same prediction horizon, we hence propose that slower models\nshould base their predictions on temporal segments sampled ahead of time. Based\non the observation that model runtime can affect performance in the considered\nstreaming evaluation scenario, we further propose a lightweight action\nanticipation model based on feed-forward 3D CNNs which is optimized using\nknowledge distillation techniques with a novel past-to-future distillation\nloss. Experiments on the three popular datasets EPIC-KITCHENS-55,\nEPIC-KITCHENS-100 and EGTEA Gaze+ show that (i) the proposed evaluation scheme\ninduces a different ranking on state-of-the-art methods as compared to classic\nevaluations, (ii) lightweight approaches tend to outmatch more computationally\nexpensive ones, and (iii) the proposed model based on feed-forward 3D CNNs and\nknowledge distillation outperforms current art in the streaming egocentric\naction anticipation scenario.\n", "rewritten_text": "The goal of egocentric action anticipation is to predict the future actions of the camera wearer based on past observations. While it is important for predictions to be made before the actual events occur, many existing approaches do not take into account the computational time required for such predictions. This oversight can lead to overly optimistic evaluations, as current evaluation schemes assume that predictions are immediately available after observing the input video, without considering runtime constraints.\n\nTo address this issue, we propose a streaming egocentric action evaluation scheme that performs predictions in real-time, making them available only after the model has processed the current input segment, taking into account its runtime. In order to ensure a fair comparison among models with the same prediction horizon, we suggest that slower models should base their predictions on temporal segments sampled in advance.\n\nRecognizing the impact of model runtime on performance in a streaming evaluation scenario, we introduce a lightweight action anticipation model based on feed-forward 3D CNNs. This model is optimized using knowledge distillation techniques, incorporating a novel past-to-future distillation loss. Our experiments on three popular datasets - EPIC-KITCHENS-55, EPIC-KITCHENS-100, and EGTEA Gaze+ - demonstrate that our proposed evaluation scheme yields different rankings for state-of-the-art methods compared to traditional evaluations. Additionally, we find that lightweight approaches tend to outperform more computationally intensive ones, and our model based on feed-forward 3D CNNs and knowledge distillation surpasses current state-of-the-art methods in the streaming egocentric action anticipation scenario."}, "1508.04458": {"original_text": "  Three-dimensional x-ray CT image reconstruction in baggage scanning in\nsecurity applications is an important research field. The variety of materials\nto be reconstructed is broader than medical x-ray imaging. Presence of high\nattenuating materials such as metal may cause artifacts if analytical\nreconstruction methods are used. Statistical modeling and the resultant\niterative algorithms are known to reduce these artifacts and present good\nquantitative accuracy in estimates of linear attenuation coefficients. However,\niterative algorithms may require computations in order to achieve\nquantitatively accurate results. For the case of baggage scanning, in order to\nprovide fast accurate inspection throughput, they must be accelerated\ndrastically. There are many approaches proposed in the literature to increase\nspeed of convergence. This paper presents a new method that estimates the\nwavelet coefficients of the images in the discrete wavelet transform domain\ninstead of the image space itself. Initially, surrogate functions are created\naround approximation coefficients only. As the iterations proceed, the wavelet\ntree on which the updates are made is expanded based on a criterion and detail\ncoefficients at each level are updated and the tree is expanded this way. For\nexample, in the smooth regions of the image the detail coefficients are not\nupdated while the coefficients that represent the high-frequency component\naround edges are being updated, thus saving time by focusing computations where\nthey are needed. This approach is implemented on real data from a SureScan (TM)\nx1000 Explosive Detection System and compared to straightforward implementation\nof the unregularized alternating minimization of O'Sullivan and Benac [1].\n", "rewritten_text": "The field of three-dimensional x-ray CT image reconstruction in baggage scanning for security applications is a crucial area of research. The range of materials to be reconstructed is wider compared to medical x-ray imaging, with the presence of high attenuating materials like metal potentially causing artifacts when using analytical reconstruction methods. To address this issue, statistical modeling and iterative algorithms have been developed to reduce these artifacts and provide accurate estimates of linear attenuation coefficients. However, these iterative algorithms often require extensive computations to achieve quantitative accuracy, which can hinder the speed of baggage scanning inspections.\n\nIn order to enhance the efficiency of baggage scanning inspections, various approaches have been proposed in the literature to accelerate the convergence speed of iterative algorithms. This paper introduces a novel method that estimates the wavelet coefficients of images in the discrete wavelet transform domain rather than directly in the image space. Initially, surrogate functions are created around the approximation coefficients only. As the iterations progress, the wavelet tree is expanded based on a specific criterion, and detail coefficients at each level are updated accordingly. This iterative process focuses on updating coefficients in high-frequency components around edges, while leaving coefficients in smooth regions unchanged, thereby optimizing computational resources where they are most needed.\n\nThis approach is applied to real data obtained from a SureScan\u2122 x1000 Explosive Detection System and compared against a straightforward implementation of the unregularized alternating minimization method proposed by O'Sullivan and Benac [1]."}, "2301.00184": {"original_text": "  Most existing text-video retrieval methods focus on cross-modal matching\nbetween the visual content of videos and textual query sentences. However, in\nreal-world scenarios, online videos are often accompanied by relevant text\ninformation such as titles, tags, and even subtitles, which can be utilized to\nmatch textual queries. This insight has motivated us to propose a novel\napproach to text-video retrieval, where we directly generate associated\ncaptions from videos using zero-shot video captioning with knowledge from\nweb-scale pre-trained models (e.g., CLIP and GPT-2). Given the generated\ncaptions, a natural question arises: what benefits do they bring to text-video\nretrieval? To answer this, we introduce Cap4Video, a new framework that\nleverages captions in three ways: i) Input data: video-caption pairs can\naugment the training data. ii) Intermediate feature interaction: we perform\ncross-modal feature interaction between the video and caption to produce\nenhanced video representations. iii) Output score: the Query-Caption matching\nbranch can complement the original Query-Video matching branch for text-video\nretrieval. We conduct comprehensive ablation studies to demonstrate the\neffectiveness of our approach. Without any post-processing, Cap4Video achieves\nstate-of-the-art performance on four standard text-video retrieval benchmarks:\nMSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%). The code is\navailable at https://github.com/whwu95/Cap4Video .\n", "rewritten_text": "Most current methods for text-video retrieval primarily focus on cross-modal matching between the visual content of videos and textual query sentences. However, in real-world scenarios, online videos often come with relevant text information such as titles, tags, and subtitles, which can be leveraged to match textual queries. This realization has inspired us to propose a novel approach to text-video retrieval. In this approach, we directly generate associated captions from videos using zero-shot video captioning with knowledge from web-scale pre-trained models like CLIP and GPT-2.\n\nUpon generating captions, a natural question arises: what advantages do they offer for text-video retrieval? To address this question, we introduce Cap4Video, a new framework that utilizes captions in three key ways: \n\ni) Input data: video-caption pairs can enhance the training data.\nii) Intermediate feature interaction: we conduct cross-modal feature interaction between the video and caption to create improved video representations.\niii) Output score: the Query-Caption matching branch can supplement the original Query-Video matching branch for text-video retrieval.\n\nWe perform comprehensive ablation studies to showcase the effectiveness of our approach. Cap4Video achieves state-of-the-art performance on four standard text-video retrieval benchmarks - MSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%) - without requiring any post-processing. The code for Cap4Video is accessible at https://github.com/whwu95/Cap4Video."}, "1907.08854": {"original_text": "  Document Grounded Conversations is a task to generate dialogue responses when\nchatting about the content of a given document. Obviously, document knowledge\nplays a critical role in Document Grounded Conversations, while existing\ndialogue models do not exploit this kind of knowledge effectively enough. In\nthis paper, we propose a novel Transformer-based architecture for multi-turn\ndocument grounded conversations. In particular, we devise an Incremental\nTransformer to encode multi-turn utterances along with knowledge in related\ndocuments. Motivated by the human cognitive process, we design a two-pass\ndecoder (Deliberation Decoder) to improve context coherence and knowledge\ncorrectness. Our empirical study on a real-world Document Grounded Dataset\nproves that responses generated by our model significantly outperform\ncompetitive baselines on both context coherence and knowledge relevance.\n", "rewritten_text": "The task of Document Grounded Conversations involves generating dialogue responses based on the content of a given document. Document knowledge is crucial in these conversations, yet current dialogue models do not effectively utilize this knowledge. This paper introduces a new Transformer-based architecture for multi-turn document grounded conversations. Specifically, we introduce an Incremental Transformer to encode multi-turn utterances and document-related knowledge. Inspired by human cognitive processes, we develop a two-pass decoder (Deliberation Decoder) to enhance context coherence and knowledge accuracy. Our empirical study on a real-world Document Grounded Dataset demonstrates that responses generated by our model outperform existing baselines in terms of both context coherence and knowledge relevance."}, "2406.05023": {"original_text": "  Generative adversarial networks (GANs) are machine learning models that are\nused to estimate the underlying statistical structure of a given dataset and as\na result can be used for a variety of tasks such as image generation or anomaly\ndetection. Despite their initial simplicity, designing an effective loss\nfunction for training GANs remains challenging, and various loss functions have\nbeen proposed aiming to improve the performance and stability of the generative\nmodels. In this study, loss function design for GANs is presented as an\noptimization problem solved using the genetic programming (GP) approach.\nInitial experiments were carried out using small Deep Convolutional GAN (DCGAN)\nmodel and the MNIST dataset, in order to search experimentally for an improved\nloss function. The functions found were evaluated on CIFAR10, with the best\nfunction, named GANetic loss, showing exceptionally better performance and\nstability compared to the losses commonly used for GAN training. To further\nevalute its general applicability on more challenging problems, GANetic loss\nwas applied for two medical applications: image generation and anomaly\ndetection. Experiments were performed with histopathological, gastrointestinal\nor glaucoma images to evaluate the GANetic loss in medical image generation,\nresulting in improved image quality compared to the baseline models. The\nGANetic Loss used for polyp and glaucoma images showed a strong improvement in\nthe detection of anomalies. In summary, the GANetic loss function was evaluated\non multiple datasets and applications where it consistently outperforms\nalternative loss functions. Moreover, GANetic loss leads to stable training and\nreproducible results, a known weak spot of GANs.\n", "rewritten_text": "Generative adversarial networks (GANs) are machine learning models used to estimate the underlying statistical structure of a dataset. They can be applied to various tasks such as image generation and anomaly detection. Despite their apparent simplicity, designing an effective loss function for training GANs remains a challenge. Several loss functions have been proposed to enhance the performance and stability of generative models. This study presents the design of loss functions for GANs as an optimization problem solved using the genetic programming (GP) approach.\n\nInitial experiments were conducted using a small Deep Convolutional GAN (DCGAN) model with the MNIST dataset to experimentally search for an improved loss function. The identified functions were then evaluated on CIFAR10. The best-performing function, named GANetic loss, demonstrated significantly better performance and stability compared to commonly used loss functions for GAN training. To assess its applicability to more complex problems, GANetic loss was applied to two medical applications: image generation and anomaly detection.\n\nExperiments were carried out using histopathological, gastrointestinal, and glaucoma images to evaluate the performance of GANetic loss in medical image generation. The results showed improved image quality compared to baseline models. Specifically, GANetic loss applied to polyp and glaucoma images exhibited enhanced anomaly detection capabilities. In summary, GANetic loss was evaluated across multiple datasets and applications, consistently outperforming alternative loss functions. Furthermore, GANetic loss facilitated stable training and reproducible results, addressing a common challenge in GANs."}, "2208.01633": {"original_text": "  We present UnrealEgo, i.e., a new large-scale naturalistic dataset for\negocentric 3D human pose estimation. UnrealEgo is based on an advanced concept\nof eyeglasses equipped with two fisheye cameras that can be used in\nunconstrained environments. We design their virtual prototype and attach them\nto 3D human models for stereo view capture. We next generate a large corpus of\nhuman motions. As a consequence, UnrealEgo is the first dataset to provide\nin-the-wild stereo images with the largest variety of motions among existing\negocentric datasets. Furthermore, we propose a new benchmark method with a\nsimple but effective idea of devising a 2D keypoint estimation module for\nstereo inputs to improve 3D human pose estimation. The extensive experiments\nshow that our approach outperforms the previous state-of-the-art methods\nqualitatively and quantitatively. UnrealEgo and our source codes are available\non our project web page.\n", "rewritten_text": "Introducing UnrealEgo, a novel large-scale naturalistic dataset created for egocentric 3D human pose estimation. UnrealEgo is centered around an innovative concept involving eyeglasses equipped with two fisheye cameras, designed for use in diverse environments. By developing a virtual prototype of these glasses and integrating them with 3D human models for stereo view capture, we have produced a substantial collection of human motions. As a result, UnrealEgo stands out as the first dataset to offer in-the-wild stereo images showcasing a wide range of motions compared to existing egocentric datasets. Additionally, we introduce a new benchmark method that leverages a straightforward yet powerful approach of implementing a 2D keypoint estimation module for stereo inputs to enhance 3D human pose estimation. Through comprehensive experiments, we demonstrate that our methodology surpasses previous state-of-the-art techniques both qualitatively and quantitatively. For those interested, UnrealEgo and our source codes can be accessed on our project website."}, "2011.131": {"original_text": "  To assess the knowledge proficiency of a learner, multiple choice question is\nan efficient and widespread form in standard tests. However, the composition of\nthe multiple choice question, especially the construction of distractors is\nquite challenging. The distractors are required to both incorrect and plausible\nenough to confuse the learners who did not master the knowledge. Currently, the\ndistractors are generated by domain experts which are both expensive and\ntime-consuming. This urges the emergence of automatic distractor generation,\nwhich can benefit various standard tests in a wide range of domains. In this\npaper, we propose a question and answer guided distractor generation (EDGE)\nframework to automate distractor generation. EDGE consists of three major\nmodules: (1) the Reforming Question Module and the Reforming Passage Module\napply gate layers to guarantee the inherent incorrectness of the generated\ndistractors; (2) the Distractor Generator Module applies attention mechanism to\ncontrol the level of plausibility. Experimental results on a large-scale public\ndataset demonstrate that our model significantly outperforms existing models\nand achieves a new state-of-the-art.\n", "rewritten_text": "Assessing a learner's knowledge proficiency often involves using multiple-choice questions, a common and efficient format in standard tests. However, crafting effective multiple-choice questions, particularly in terms of creating distractors, can be quite challenging. Distractors must be both incorrect and plausible enough to mislead learners who have not fully grasped the material. Currently, distractors are typically created by domain experts, a process that is both costly and time-consuming. This has led to a growing need for automatic distractor generation, which could benefit a wide range of standard tests across various domains.\n\nIn this paper, we introduce the Question and Answer Guided Distractor Generation (EDGE) framework, designed to automate the process of generating distractors. EDGE comprises three main modules: \n1. The Reforming Question Module and the Reforming Passage Module, which utilize gate layers to ensure the inherent incorrectness of the generated distractors.\n2. The Distractor Generator Module, which employs an attention mechanism to control the plausibility level of the distractors.\n\nExperimental results using a large-scale public dataset demonstrate that our model significantly outperforms existing models and establishes a new state-of-the-art in automatic distractor generation."}, "2308.14082": {"original_text": "  Reconstructing interacting hands from monocular images is indispensable in\nAR/VR applications. Most existing solutions rely on the accurate localization\nof each skeleton joint. However, these methods tend to be unreliable due to the\nsevere occlusion and confusing similarity among adjacent hand parts. This also\ndefies human perception because humans can quickly imitate an interaction\npattern without localizing all joints. Our key idea is to first construct a\ntwo-hand interaction prior and recast the interaction reconstruction task as\nthe conditional sampling from the prior. To expand more interaction states, a\nlarge-scale multimodal dataset with physical plausibility is proposed. Then a\nVAE is trained to further condense these interaction patterns as latent codes\nin a prior distribution. When looking for image cues that contribute to\ninteraction prior sampling, we propose the interaction adjacency heatmap (IAH).\nCompared with a joint-wise heatmap for localization, IAH assigns denser visible\nfeatures to those invisible joints. Compared with an all-in-one visible\nheatmap, it provides more fine-grained local interaction information in each\ninteraction region. Finally, the correlations between the extracted features\nand corresponding interaction codes are linked by the ViT module. Comprehensive\nevaluations on benchmark datasets have verified the effectiveness of this\nframework. The code and dataset are publicly available at\nhttps://github.com/binghui-z/InterPrior_pytorch\n", "rewritten_text": "Reconstructing interacting hands from monocular images is crucial for AR/VR applications. Many current solutions rely on accurately localizing each skeleton joint, but these methods can be unreliable due to occlusion and the similarity between adjacent hand parts. This also goes against human perception, as humans can quickly mimic interaction patterns without needing to pinpoint every joint. Our main approach involves creating a prior for two-hand interactions and reframing the reconstruction task as sampling from this prior. To enhance the variety of interaction states, we introduce a large-scale multimodal dataset that emphasizes physical plausibility. Additionally, a VAE is trained to condense these interaction patterns into latent codes within a prior distribution. To identify image cues that aid in sampling the interaction prior, we introduce the interaction adjacency heatmap (IAH). Unlike a joint-wise heatmap used for localization, the IAH highlights denser visible features for invisible joints. In comparison to an all-in-one visible heatmap, the IAH offers more detailed local interaction information within each interaction region. Finally, the ViT module establishes connections between the extracted features and corresponding interaction codes. Extensive evaluations on benchmark datasets have confirmed the effectiveness of our framework. The code and dataset are publicly accessible at https://github.com/binghui-z/InterPrior_pytorch."}, "1910.10223": {"original_text": "  Deep generative models come with the promise to learn an explainable\nrepresentation for visual objects that allows image sampling, synthesis, and\nselective modification. The main challenge is to learn to properly model the\nindependent latent characteristics of an object, especially its appearance and\npose. We present a novel approach that learns disentangled representations of\nthese characteristics and explains them individually. Training requires only\npairs of images depicting the same object appearance, but no pose annotations.\nWe propose an additional classifier that estimates the minimal amount of\nregularization required to enforce disentanglement. Thus both representations\ntogether can completely explain an image while being independent of each other.\nPrevious methods based on adversarial approaches fail to enforce this\nindependence, while methods based on variational approaches lead to\nuninformative representations. In experiments on diverse object categories, the\napproach successfully recombines pose and appearance to reconstruct and\nretarget novel synthesized images. We achieve significant improvements over\nstate-of-the-art methods which utilize the same level of supervision, and reach\nperformances comparable to those of pose-supervised approaches. However, we can\nhandle the vast body of articulated object classes for which no pose\nmodels/annotations are available.\n", "rewritten_text": "Deep generative models offer the potential to acquire an interpretable representation for visual objects, enabling image sampling, synthesis, and selective modification. The primary challenge lies in effectively capturing the independent latent characteristics of an object, particularly its appearance and pose. Our novel approach aims to learn disentangled representations of these characteristics and explain them individually. Training only necessitates pairs of images showing the same object appearance, without the need for pose annotations. We introduce an additional classifier that estimates the minimal regularization needed to ensure disentanglement. Consequently, both representations can jointly account for an image while remaining independent of each other.\n\nExisting methods relying on adversarial approaches struggle to enforce this independence, while those based on variational approaches often yield uninformative representations. Through experiments across various object categories, our approach successfully combines pose and appearance to reconstruct and retarget novel synthesized images. We achieve significant advancements over state-of-the-art methods that operate with the same level of supervision, approaching the performance levels of pose-supervised approaches. Notably, our method can handle a wide range of articulated object classes for which pose models or annotations are unavailable."}, "2405.17765": {"original_text": "  Video quality assessment (VQA) is a challenging problem due to the numerous\nfactors that can affect the perceptual quality of a video, \\eg, content\nattractiveness, distortion type, motion pattern, and level. However, annotating\nthe Mean opinion score (MOS) for videos is expensive and time-consuming, which\nlimits the scale of VQA datasets, and poses a significant obstacle for deep\nlearning-based methods. In this paper, we propose a VQA method named PTM-VQA,\nwhich leverages PreTrained Models to transfer knowledge from models pretrained\non various pre-tasks, enabling benefits for VQA from different aspects.\n  Specifically, we extract features of videos from different pretrained models\nwith frozen weights and integrate them to generate representation. Since these\nmodels possess various fields of knowledge and are often trained with labels\nirrelevant to quality, we propose an Intra-Consistency and Inter-Divisibility\n(ICID) loss to impose constraints on features extracted by multiple pretrained\nmodels. The intra-consistency constraint ensures that features extracted by\ndifferent pretrained models are in the same unified quality-aware latent space,\nwhile the inter-divisibility introduces pseudo clusters based on the annotation\nof samples and tries to separate features of samples from different clusters.\nFurthermore, with a constantly growing number of pretrained models, it is\ncrucial to determine which models to use and how to use them. To address this\nproblem, we propose an efficient scheme to select suitable candidates. Models\nwith better clustering performance on VQA datasets are chosen to be our\ncandidates. Extensive experiments demonstrate the effectiveness of the proposed\nmethod.\n", "rewritten_text": "Video quality assessment (VQA) presents a challenging problem due to the multitude of factors that can impact the perceptual quality of a video, such as content attractiveness, distortion type, motion pattern, and level. However, annotating the Mean Opinion Score (MOS) for videos is both costly and time-consuming, thereby restricting the scale of VQA datasets and creating a significant barrier for deep learning-based methods. In this paper, we introduce a VQA approach called PTM-VQA, which utilizes PreTrained Models to transfer knowledge from models pretrained on various tasks, offering advantages for VQA from diverse perspectives.\n\nSpecifically, we extract video features from different pretrained models with fixed weights and combine them to generate a unified representation. As these models encompass a range of knowledge domains and are typically trained with labels unrelated to quality, we introduce an Intra-Consistency and Inter-Divisibility (ICID) loss to impose constraints on features extracted by multiple pretrained models. The intra-consistency constraint ensures that features extracted by different pretrained models reside in a unified quality-aware latent space, while the inter-divisibility introduces pseudo clusters based on sample annotations to segregate features from distinct clusters.\n\nMoreover, given the increasing number of pretrained models, it is essential to determine which models to utilize and how to leverage them effectively. To tackle this challenge, we propose an efficient method to select suitable candidates. Models exhibiting superior clustering performance on VQA datasets are identified as potential candidates. Extensive experiments validate the efficacy of the proposed approach."}, "2306.00246": {"original_text": "  The monetary value of a given piece of real estate, a parcel, is often\nreadily available from a geographic information system. However, for many\napplications, such as insurance and urban planning, it is useful to have\nestimates of property value at much higher spatial resolutions. We propose a\nmethod to estimate the distribution over property value at the pixel level from\nremote sensing imagery. We evaluate on a real-world dataset of a major urban\narea. Our results show that the proposed approaches are capable of generating\nfine-level estimates of property values, significantly improving upon a diverse\ncollection of baseline approaches.\n", "rewritten_text": "The value of a specific piece of real estate, such as a parcel, can typically be easily obtained from a geographic information system. However, in various scenarios like insurance and urban planning, having property value estimates at more detailed spatial resolutions is beneficial. Our method suggests estimating the distribution of property values at the pixel level using remote sensing imagery. We tested this approach on a dataset from a significant urban area and found that our method can provide precise estimates of property values at a fine level, surpassing various baseline approaches."}, "1606.02147": {"original_text": "  The ability to perform pixel-wise semantic segmentation in real-time is of\nparamount importance in mobile applications. Recent deep neural networks aimed\nat this task have the disadvantage of requiring a large number of floating\npoint operations and have long run-times that hinder their usability. In this\npaper, we propose a novel deep neural network architecture named ENet\n(efficient neural network), created specifically for tasks requiring low\nlatency operation. ENet is up to 18$\\times$ faster, requires 75$\\times$ less\nFLOPs, has 79$\\times$ less parameters, and provides similar or better accuracy\nto existing models. We have tested it on CamVid, Cityscapes and SUN datasets\nand report on comparisons with existing state-of-the-art methods, and the\ntrade-offs between accuracy and processing time of a network. We present\nperformance measurements of the proposed architecture on embedded systems and\nsuggest possible software improvements that could make ENet even faster.\n", "rewritten_text": "Performing pixel-wise semantic segmentation in real-time is crucial for mobile applications. Existing deep neural networks designed for this task often require a high number of floating-point operations and have long run-times, limiting their practicality. In this paper, we introduce a new deep neural network architecture called ENet (efficient neural network), specifically optimized for low-latency operations. ENet is up to 18 times faster, requires 75 times fewer FLOPs, has 79 times fewer parameters, and delivers comparable or superior accuracy compared to current models. Our evaluation on CamVid, Cityscapes, and SUN datasets includes comparisons with state-of-the-art methods, highlighting the accuracy-processing time trade-offs. We present performance metrics of ENet on embedded systems and propose potential software enhancements to further boost its speed."}, "2201.11374": {"original_text": "  In this work, we focus on low-resource dependency parsing for multiple\nlanguages. Several strategies are tailored to enhance performance in\nlow-resource scenarios. While these are well-known to the community, it is not\ntrivial to select the best-performing combination of these strategies for a\nlow-resource language that we are interested in, and not much attention has\nbeen given to measuring the efficacy of these strategies. We experiment with 5\nlow-resource strategies for our ensembled approach on 7 Universal Dependency\n(UD) low-resource languages. Our exhaustive experimentation on these languages\nsupports the effective improvements for languages not covered in pretrained\nmodels. We show a successful application of the ensembled system on a truly\nlow-resource language Sanskrit. The code and data are available at:\nhttps://github.com/Jivnesh/SanDP\n", "rewritten_text": "This work focuses on low-resource dependency parsing for multiple languages, with several strategies tailored to enhance performance in such scenarios. While these strategies are well-known within the community, selecting the best-performing combination for a specific low-resource language of interest is not a trivial task, and there has been limited attention given to measuring their efficacy. \n\nWe conducted experiments using five low-resource strategies in an ensembled approach on seven Universal Dependency (UD) low-resource languages. Our exhaustive experimentation on these languages demonstrates effective improvements for languages not covered in pretrained models. \n\nWe showcase a successful application of the ensembled system on the truly low-resource language Sanskrit. For access to the code and data, please visit: https://github.com/Jivnesh/SanDP"}, "2407.03841": {"original_text": "  Large Language Models (LLMs) have showcased remarkable capabilities in\nvarious Natural Language Processing tasks. For automatic open-domain dialogue\nevaluation in particular, LLMs have been seamlessly integrated into evaluation\nframeworks, and together with human evaluation, compose the backbone of most\nevaluations. However, existing evaluation benchmarks often rely on outdated\ndatasets and evaluate aspects like Fluency and Relevance, which fail to\nadequately capture the capabilities and limitations of state-of-the-art chatbot\nmodels.\n  This paper critically examines current evaluation benchmarks, highlighting\nthat the use of older response generators and quality aspects fail to\naccurately reflect modern chatbot capabilities. A small annotation experiment\non a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as\nGPT-4 struggle to detect actual deficiencies in dialogues generated by current\nLLM chatbots.\n", "rewritten_text": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various Natural Language Processing tasks. In the context of automatic open-domain dialogue evaluation, LLMs have been seamlessly integrated into evaluation frameworks, forming the backbone of most evaluations alongside human assessment. However, current evaluation benchmarks often rely on outdated datasets and assess factors such as Fluency and Relevance, which do not fully capture the capabilities and limitations of state-of-the-art chatbot models.\n\nThis paper critically examines existing evaluation benchmarks, emphasizing that the use of older response generators and quality metrics does not accurately represent the modern capabilities of chatbots. An annotation experiment conducted on a recent LLM-generated dataset (SODA) reveals that LLM evaluators like GPT-4 struggle to identify actual deficiencies in dialogues produced by current LLM chatbots."}, "1502.00377": {"original_text": "  In order to track the moving objects in long range against occlusion,\ninterruption, and background clutter, this paper proposes a unified approach\nfor global trajectory analysis. Instead of the traditional frame-by-frame\ntracking, our method recovers target trajectories based on a short sequence of\nvideo frames, e.g. $15$ frames. We initially calculate a foreground map at each\nframe, as obtained from a state-of-the-art background model. An attribute graph\nis then extracted from the foreground map, where the graph vertices are image\nprimitives represented by the composite features. With this graph\nrepresentation, we pose trajectory analysis as a joint task of spatial graph\npartitioning and temporal graph matching. The task can be formulated by\nmaximizing a posteriori under the Bayesian framework, in which we integrate the\nspatio-temporal contexts and the appearance models. The probabilistic inference\nis achieved by a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given a\nperoid of observed frames, the algorithm simulates a ergodic and aperiodic\nMarkov Chain, and it visits a sequence of solution states in the joint space of\nspatial graph partitioning and temporal graph matching. In the experiments, our\nmethod is tested on several challenging videos from the public datasets of\nvisual surveillance, and it outperforms the state-of-the-art methods.\n", "rewritten_text": "This paper proposes a unified approach for global trajectory analysis to track moving objects in long range against occlusion, interruption, and background clutter. Instead of traditional frame-by-frame tracking, our method recovers target trajectories based on a short sequence of video frames, typically 15 frames. Initially, a foreground map is calculated at each frame using a state-of-the-art background model. An attribute graph is then extracted from the foreground map, with graph vertices representing image primitives through composite features. This graph representation frames trajectory analysis as a joint task of spatial graph partitioning and temporal graph matching. The task is formulated by maximizing a posteriori under the Bayesian framework, integrating spatio-temporal contexts and appearance models. Probabilistic inference is achieved through a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given a period of observed frames, the algorithm simulates an ergodic and aperiodic Markov Chain, visiting a sequence of solution states in the joint space of spatial graph partitioning and temporal graph matching. In experiments, our method is tested on challenging videos from public datasets of visual surveillance, demonstrating superior performance compared to state-of-the-art methods."}, "1607.02204": {"original_text": "  In this paper we introduce a method to overcome one of the main challenges of\nperson re-identification in multi-camera networks, namely cross-view appearance\nchanges. The proposed solution addresses the extreme variability of person\nappearance in different camera views by exploiting multiple feature\nrepresentations. For each feature, Kernel Canonical Correlation Analysis (KCCA)\nwith different kernels is exploited to learn several projection spaces in which\nthe appearance correlation between samples of the same person observed from\ndifferent cameras is maximized. An iterative logistic regression is finally\nused to select and weigh the contributions of each feature projections and\nperform the matching between the two views. Experimental evaluation shows that\nthe proposed solution obtains comparable performance on VIPeR and PRID 450s\ndatasets and improves on PRID and CUHK01 datasets with respect to the state of\nthe art.\n", "rewritten_text": "This paper introduces a method to address a key challenge in person re-identification within multi-camera networks: cross-view appearance changes. The proposed solution tackles the significant variability in a person's appearance across different camera views by leveraging multiple feature representations. Kernel Canonical Correlation Analysis (KCCA) is utilized with various kernels to learn multiple projection spaces that maximize the appearance correlation between samples of the same individual captured from different cameras. An iterative logistic regression is then employed to choose and assign weights to the contributions of each feature projection, facilitating the matching between the two views. Experimental evaluation demonstrates that the proposed solution achieves comparable performance on the VIPeR and PRID 450s datasets, while outperforming existing methods on the PRID and CUHK01 datasets."}, "2208.03207": {"original_text": "  Learning with noisy labels (LNL) aims at designing strategies to improve\nmodel performance and generalization by mitigating the effects of model\noverfitting to noisy labels. The key success of LNL lies in identifying as many\nclean samples as possible from massive noisy data, while rectifying the wrongly\nassigned noisy labels. Recent advances employ the predicted label distributions\nof individual samples to perform noise verification and noisy label correction,\neasily giving rise to confirmation bias. To mitigate this issue, we propose\nNeighborhood Collective Estimation, in which the predictive reliability of a\ncandidate sample is re-estimated by contrasting it against its feature-space\nnearest neighbors. Specifically, our method is divided into two steps: 1)\nNeighborhood Collective Noise Verification to separate all training samples\ninto a clean or noisy subset, 2) Neighborhood Collective Label Correction to\nrelabel noisy samples, and then auxiliary techniques are used to assist further\nmodel optimization. Extensive experiments on four commonly used benchmark\ndatasets, i.e., CIFAR-10, CIFAR-100, Clothing-1M and Webvision-1.0, demonstrate\nthat our proposed method considerably outperforms state-of-the-art methods.\n", "rewritten_text": "Learning with noisy labels (LNL) aims to enhance model performance and generalization by addressing model overfitting to noisy labels. The key to the success of LNL is the identification of clean samples within large noisy datasets, while correcting mislabeled instances. Recent advancements leverage predicted label distributions of individual samples for noise verification and label correction, which can lead to confirmation bias. To address this issue, we introduce Neighborhood Collective Estimation. This approach re-evaluates the predictive reliability of a candidate sample by comparing it with its nearest neighbors in feature space. Our method consists of two main steps: 1) Neighborhood Collective Noise Verification, which categorizes training samples into clean or noisy subsets, and 2) Neighborhood Collective Label Correction, which corrects labels of noisy samples. Additional techniques are then employed to further optimize the model. Extensive experiments conducted on four widely used benchmark datasets - CIFAR-10, CIFAR-100, Clothing-1M, and Webvision-1.0 - demonstrate the superior performance of our proposed method compared to state-of-the-art techniques."}, "2305.03112": {"original_text": "  A surge of interest has emerged in weakly supervised semantic segmentation\ndue to its remarkable efficiency in recent years. Existing approaches based on\ntransformers mainly focus on exploring the affinity matrix to boost CAMs with\nglobal relationships. While in this work, we first perform a scrupulous\nexamination towards the impact of successive affinity matrices and discover\nthat they possess an inclination toward sparsification as the network\napproaches convergence, hence disclosing a manifestation of over-smoothing.\nBesides, it has been observed that enhanced attention maps tend to evince a\nsubstantial amount of extraneous background noise in deeper layers. Drawing\nupon this, we posit a daring conjecture that the undisciplined over-smoothing\nphenomenon introduces a noteworthy quantity of semantically irrelevant\nbackground noise, causing performance degradation. To alleviate this issue, we\npropose a novel perspective that highlights the objects of interest by\ninvestigating the regions of the trait, thereby fostering an extensive\ncomprehension of the successive affinity matrix. Consequently, we suggest an\nadaptive re-activation mechanism (AReAM) that alleviates the issue of\nincomplete attention within the object and the unbounded background noise.\nAReAM accomplishes this by supervising high-level attention with shallow\naffinity matrices, yielding promising results. Exhaustive experiments conducted\non the commonly used dataset manifest that segmentation results can be greatly\nimproved through our proposed AReAM, which imposes restrictions on each\naffinity matrix in deep layers to make it attentive to semantic regions.\n", "rewritten_text": "In recent years, there has been a surge of interest in weakly supervised semantic segmentation due to its remarkable efficiency. Existing approaches based on transformers have primarily focused on exploring the affinity matrix to enhance Class Activation Maps (CAMs) with global relationships. However, in this study, we conducted a thorough investigation into the impact of successive affinity matrices and found that they tend to become sparse as the network converges, revealing a phenomenon of over-smoothing.\n\nFurthermore, it has been observed that improved attention maps often exhibit significant background noise in deeper layers. Building on this observation, we propose a bold hypothesis that the uncontrolled over-smoothing leads to a considerable amount of semantically irrelevant background noise, resulting in performance degradation. To address this issue, we introduce a novel perspective that emphasizes the regions of interest by analyzing the characteristics, thereby promoting a comprehensive understanding of the successive affinity matrix.\n\nAs a solution, we present an Adaptive Re-activation Mechanism (AReAM) to mitigate the problem of incomplete attention within the object and excessive background noise. AReAM achieves this by guiding high-level attention using shallow affinity matrices, yielding promising outcomes. Extensive experiments conducted on a widely used dataset demonstrate that our proposed AReAM significantly enhances segmentation results by constraining each affinity matrix in deep layers to focus on semantic regions."}, "2010.10894": {"original_text": "  This paper aims to enhance the few-shot relation classification especially\nfor sentences that jointly describe multiple relations. Due to the fact that\nsome relations usually keep high co-occurrence in the same context, previous\nfew-shot relation classifiers struggle to distinguish them with few annotated\ninstances. To alleviate the above relation confusion problem, we propose CTEG,\na model equipped with two mechanisms to learn to decouple these easily-confused\nrelations. On the one hand, an Entity-Guided Attention (EGA) mechanism, which\nleverages the syntactic relations and relative positions between each word and\nthe specified entity pair, is introduced to guide the attention to filter out\ninformation causing confusion. On the other hand, a Confusion-Aware Training\n(CAT) method is proposed to explicitly learn to distinguish relations by\nplaying a pushing-away game between classifying a sentence into a true relation\nand its confusing relation. Extensive experiments are conducted on the FewRel\ndataset, and the results show that our proposed model achieves comparable and\neven much better results to strong baselines in terms of accuracy. Furthermore,\nthe ablation test and case study verify the effectiveness of our proposed EGA\nand CAT, especially in addressing the relation confusion problem.\n", "rewritten_text": "This paper aims to improve few-shot relation classification, particularly for sentences that describe multiple relations simultaneously. Some relations often co-occur in the same context, making it challenging for existing few-shot relation classifiers to differentiate them with limited annotated instances. To address this issue of relation confusion, we introduce CTEG, a model that incorporates two mechanisms to effectively separate these closely related relations.\n\nFirstly, we propose an Entity-Guided Attention (EGA) mechanism, which utilizes syntactic relations and relative positions between each word and the specified entity pair to guide attention and filter out confusing information. Secondly, we introduce a Confusion-Aware Training (CAT) method that explicitly trains the model to distinguish between relations by engaging in a \"pushing-away\" game, where the model learns to classify a sentence into its true relation while pushing away from its confusing relation.\n\nExtensive experiments are conducted on the FewRel dataset, demonstrating that our proposed model achieves comparable or even superior results compared to strong baselines in terms of accuracy. Additionally, ablation tests and case studies confirm the effectiveness of our EGA and CAT mechanisms, particularly in addressing the challenge of relation confusion."}, "1903.05396": {"original_text": "  This paper introduces improved methods for sub-event detection in social\nmedia streams, by applying neural sequence models not only on the level of\nindividual posts, but also directly on the stream level. Current approaches to\nidentify sub-events within a given event, such as a goal during a soccer match,\nessentially do not exploit the sequential nature of social media streams. We\naddress this shortcoming by framing the sub-event detection problem in social\nmedia streams as a sequence labeling task and adopt a neural sequence\narchitecture that explicitly accounts for the chronological order of posts.\nSpecifically, we (i) establish a neural baseline that outperforms a graph-based\nstate-of-the-art method for binary sub-event detection (2.7% micro-F1\nimprovement), as well as (ii) demonstrate superiority of a recurrent neural\nnetwork model on the posts sequence level for labeled sub-events (2.4%\nbin-level F1 improvement over non-sequential models).\n", "rewritten_text": "This paper presents enhanced methods for detecting sub-events in social media streams by utilizing neural sequence models not only at the individual post level but also directly at the stream level. Current approaches for identifying sub-events within a given event, such as a goal during a soccer match, often fail to leverage the sequential nature of social media streams. To address this limitation, we reframe the sub-event detection problem in social media streams as a sequence labeling task and employ a neural sequence architecture that explicitly considers the chronological order of posts. Our contributions include: (i) establishing a neural baseline that outperforms a graph-based state-of-the-art method for binary sub-event detection (with a 2.7% micro-F1 improvement), and (ii) demonstrating the superiority of a recurrent neural network model at the post sequence level for labeled sub-events (achieving a 2.4% bin-level F1 improvement over non-sequential models)."}, "2212.03496": {"original_text": "  Script event prediction aims to predict the subsequent event given the\ncontext. This requires the capability to infer the correlations between events.\nRecent works have attempted to improve event correlation reasoning by using\npretrained language models and incorporating external knowledge~(e.g.,\ndiscourse relations). Though promising results have been achieved, some\nchallenges still remain. First, the pretrained language models adopted by\ncurrent works ignore event-level knowledge, resulting in an inability to\ncapture the correlations between events well. Second, modeling correlations\nbetween events with discourse relations is limited because it can only capture\nexplicit correlations between events with discourse markers, and cannot capture\nmany implicit correlations. To this end, we propose a novel generative approach\nfor this task, in which a pretrained language model is fine-tuned with an\nevent-centric pretraining objective and predicts the next event within a\ngenerative paradigm. Specifically, we first introduce a novel event-level blank\ninfilling strategy as the learning objective to inject event-level knowledge\ninto the pretrained language model, and then design a likelihood-based\ncontrastive loss for fine-tuning the generative model. Instead of using an\nadditional prediction layer, we perform prediction by using sequence\nlikelihoods generated by the generative model. Our approach models correlations\nbetween events in a soft way without any external knowledge. The\nlikelihood-based prediction eliminates the need to use additional networks to\nmake predictions and is somewhat interpretable since it scores each word in the\nevent. Experimental results on the multi-choice narrative cloze~(MCNC) task\ndemonstrate that our approach achieves better results than other\nstate-of-the-art baselines. Our code will be available at\nhttps://github.com/zhufq00/mcnc.\n", "rewritten_text": "The goal of script event prediction is to forecast the subsequent event based on the context, requiring the ability to deduce correlations between events. Recent studies have sought to enhance event correlation reasoning by leveraging pretrained language models and integrating external knowledge, such as discourse relations. While these efforts have shown promising outcomes, certain challenges persist. Firstly, existing works utilizing pretrained language models overlook event-level knowledge, leading to a limited capacity to effectively capture event correlations. Secondly, modeling event correlations through discourse relations has limitations as it can only identify explicit correlations marked by discourse indicators, failing to capture numerous implicit correlations.\n\nIn response to these challenges, we propose a novel generative approach for this task. In this approach, a pretrained language model is fine-tuned with an event-centric pretraining objective to predict the next event within a generative framework. Initially, we introduce a unique event-level blank filling strategy as the learning objective to embed event-level knowledge into the pretrained language model. Subsequently, we devise a likelihood-based contrastive loss for fine-tuning the generative model. Instead of employing an additional prediction layer, predictions are made using sequence likelihoods generated by the generative model. Our approach soft models correlations between events without relying on external knowledge. The likelihood-based prediction eliminates the necessity for additional networks to make predictions and offers interpretability by scoring each word in the event.\n\nExperimental results on the multi-choice narrative cloze (MCNC) task demonstrate that our approach outperforms other state-of-the-art baselines. Our code will be accessible at https://github.com/zhufq00/mcnc."}, "1911.12982": {"original_text": "  Recently, Chinese word segmentation (CWS) methods using neural networks have\nmade impressive progress. Most of them regard the CWS as a sequence labeling\nproblem which construct models based on local features rather than considering\nglobal information of input sequence. In this paper, we cast the CWS as a\nsequence translation problem and propose a novel sequence-to-sequence CWS model\nwith an attention-based encoder-decoder framework. The model captures the\nglobal information from the input and directly outputs the segmented sequence.\nIt can also tackle other NLP tasks with CWS jointly in an end-to-end mode.\nExperiments on Weibo, PKU and MSRA benchmark datasets show that our approach\nhas achieved competitive performances compared with state-of-the-art methods.\nMeanwhile, we successfully applied our proposed model to jointly learning CWS\nand Chinese spelling correction, which demonstrates its applicability of\nmulti-task fusion.\n", "rewritten_text": "Recently, there have been significant advancements in Chinese word segmentation (CWS) methods utilizing neural networks. The majority of these methods treat CWS as a sequence labeling problem, constructing models based on local features rather than considering the global information of the input sequence. In this study, we reframe CWS as a sequence translation problem and introduce a novel sequence-to-sequence CWS model employing an attention-based encoder-decoder framework. This model effectively captures global information from the input and directly generates the segmented sequence. Furthermore, it can address other NLP tasks alongside CWS in an end-to-end manner. Experimental results on Weibo, PKU, and MSRA benchmark datasets demonstrate that our approach achieves competitive performance compared to state-of-the-art methods. Additionally, we successfully applied our proposed model to simultaneously learn CWS and Chinese spelling correction, showcasing its versatility in multi-task fusion."}, "2308.14018": {"original_text": "  Few-shot font generation is challenging, as it needs to capture the\nfine-grained stroke styles from a limited set of reference glyphs, and then\ntransfer to other characters, which are expected to have similar styles.\nHowever, due to the diversity and complexity of Chinese font styles, the\nsynthesized glyphs of existing methods usually exhibit visible artifacts, such\nas missing details and distorted strokes. In this paper, we propose a\nVQGAN-based framework (i.e., VQ-Font) to enhance glyph fidelity through token\nprior refinement and structure-aware enhancement. Specifically, we pre-train a\nVQGAN to encapsulate font token prior within a codebook. Subsequently, VQ-Font\nrefines the synthesized glyphs with the codebook to eliminate the domain gap\nbetween synthesized and real-world strokes. Furthermore, our VQ-Font leverages\nthe inherent design of Chinese characters, where structure components such as\nradicals and character components are combined in specific arrangements, to\nrecalibrate fine-grained styles based on references. This process improves the\nmatching and fusion of styles at the structure level. Both modules collaborate\nto enhance the fidelity of the generated fonts. Experiments on a collected font\ndataset show that our VQ-Font outperforms the competing methods both\nquantitatively and qualitatively, especially in generating challenging styles.\n", "rewritten_text": "Generating fonts with few-shot learning is a challenging task, requiring the capture of intricate stroke styles from a limited set of reference glyphs and their transfer to other characters with similar styles. The diversity and complexity of Chinese font styles often lead to visible artifacts in synthesized glyphs produced by existing methods, such as missing details and distorted strokes. In this study, we introduce a VQGAN-based framework called VQ-Font to enhance glyph fidelity through token prior refinement and structure-aware enhancement.\n\nTo begin, we pre-train a VQGAN to encode font token priors within a codebook. Subsequently, VQ-Font refines the synthesized glyphs using the codebook to bridge the domain gap between synthesized and real-world strokes. Moreover, our approach leverages the inherent design of Chinese characters, where structure components like radicals and character elements are arranged in specific configurations, to recalibrate fine-grained styles based on references. This process enhances the matching and fusion of styles at the structural level, ultimately improving the fidelity of the generated fonts. The collaboration of both modules results in enhanced font fidelity.\n\nExperimental results on a curated font dataset demonstrate that our VQ-Font surpasses competing methods in both quantitative and qualitative evaluations, particularly in generating challenging styles."}, "2403.19836": {"original_text": "  Identifying the targets of hate speech is a crucial step in grasping the\nnature of such speech and, ultimately, in improving the detection of offensive\nposts on online forums. Much harmful content on online platforms uses implicit\nlanguage especially when targeting vulnerable and protected groups such as\nusing stereotypical characteristics instead of explicit target names, making it\nharder to detect and mitigate the language. In this study, we focus on\nidentifying implied targets of hate speech, essential for recognizing subtler\nhate speech and enhancing the detection of harmful content on digital\nplatforms. We define a new task aimed at identifying the targets even when they\nare not explicitly stated. To address that task, we collect and annotate target\nspans in three prominent implicit hate speech datasets: SBIC, DynaHate, and\nIHC. We call the resulting merged collection Implicit-Target-Span. The\ncollection is achieved using an innovative pooling method with matching scores\nbased on human annotations and Large Language Models (LLMs). Our experiments\nindicate that Implicit-Target-Span provides a challenging test bed for target\nspan detection methods.\n", "rewritten_text": "Identifying the targets of hate speech is a crucial step in understanding the nature of such speech and, ultimately, improving the detection of offensive posts on online forums. Much harmful content on online platforms utilizes implicit language, particularly when targeting vulnerable and protected groups. This involves using stereotypical characteristics instead of explicit target names, which makes it more challenging to detect and address the harmful language. \n\nIn this study, our focus is on identifying implied targets of hate speech, which is essential for recognizing subtler forms of hate speech and enhancing the detection of harmful content on digital platforms. We introduce a new task that aims to identify the targets even when they are not explicitly stated. To tackle this task, we gather and annotate target spans from three prominent implicit hate speech datasets: SBIC, DynaHate, and IHC. The resulting merged collection is named Implicit-Target-Span, and it is created using an innovative pooling method that incorporates matching scores based on human annotations and Large Language Models (LLMs). \n\nOur experiments demonstrate that Implicit-Target-Span serves as a challenging test bed for target span detection methods."}, "2303.05955": {"original_text": "  Remote photoplethysmography (rPPG) technology has drawn increasing attention\nin recent years. It can extract Blood Volume Pulse (BVP) from facial videos,\nmaking many applications like health monitoring and emotional analysis more\naccessible. However, as the BVP signal is easily affected by environmental\nchanges, existing methods struggle to generalize well for unseen domains. In\nthis paper, we systematically address the domain shift problem in the rPPG\nmeasurement task. We show that most domain generalization methods do not work\nwell in this problem, as domain labels are ambiguous in complicated\nenvironmental changes. In light of this, we propose a domain-label-free\napproach called NEuron STructure modeling (NEST). NEST improves the\ngeneralization capacity by maximizing the coverage of feature space during\ntraining, which reduces the chance for under-optimized feature activation\nduring inference. Besides, NEST can also enrich and enhance domain invariant\nfeatures across multi-domain. We create and benchmark a large-scale domain\ngeneralization protocol for the rPPG measurement task. Extensive experiments\nshow that our approach outperforms the state-of-the-art methods on both\ncross-dataset and intra-dataset settings.\n", "rewritten_text": "In recent years, remote photoplethysmography (rPPG) technology has garnered increasing attention for its ability to extract Blood Volume Pulse (BVP) from facial videos, enabling various applications such as health monitoring and emotional analysis to become more accessible. However, the BVP signal is susceptible to environmental influences, posing challenges for existing methods to generalize effectively across different domains. \n\nThis paper systematically addresses the domain shift issue in the rPPG measurement task. It is observed that conventional domain generalization methods struggle due to the ambiguity of domain labels in complex environmental conditions. To tackle this, we introduce a domain-label-free approach named NEuron STructure modeling (NEST). NEST enhances generalization by maximizing feature space coverage during training, thereby reducing the likelihood of suboptimal feature activation during inference. Additionally, NEST can enhance domain-invariant features across multiple domains.\n\nA comprehensive domain generalization protocol is developed and evaluated for the rPPG measurement task. Extensive experiments demonstrate that our approach surpasses state-of-the-art methods in both cross-dataset and intra-dataset scenarios."}, "2401.06957": {"original_text": "  As virtual environments continue to advance, the demand for immersive and\nemotionally engaging experiences has grown. Addressing this demand, we\nintroduce Emotion enabled Virtual avatar mapping using Optimized KnowledgE\ndistillation (EVOKE), a lightweight emotion recognition framework designed for\nthe seamless integration of emotion recognition into 3D avatars within virtual\nenvironments. Our approach leverages knowledge distillation involving\nmulti-label classification on the publicly available DEAP dataset, which covers\nvalence, arousal, and dominance as primary emotional classes. Remarkably, our\ndistilled model, a CNN with only two convolutional layers and 18 times fewer\nparameters than the teacher model, achieves competitive results, boasting an\naccuracy of 87% while demanding far less computational resources. This\nequilibrium between performance and deployability positions our framework as an\nideal choice for virtual environment systems. Furthermore, the multi-label\nclassification outcomes are utilized to map emotions onto custom-designed 3D\navatars.\n", "rewritten_text": "As virtual environments continue to advance, the demand for immersive and emotionally engaging experiences has increased. To address this demand, we present EVOKE (Emotion enabled Virtual Avatar Mapping using Optimized Knowledge Distillation), a lightweight emotion recognition framework designed for seamlessly integrating emotion recognition into 3D avatars within virtual environments. Our approach utilizes knowledge distillation through multi-label classification on the publicly available DEAP dataset, which includes valence, arousal, and dominance as primary emotional classes. Notably, our distilled model, a CNN with only two convolutional layers and 18 times fewer parameters than the teacher model, achieves competitive results with an accuracy of 87%, while requiring significantly fewer computational resources. This balance between performance and deployability positions our framework as an ideal choice for virtual environment systems. Additionally, the outcomes of multi-label classification are used to map emotions onto custom-designed 3D avatars."}, "2108.07638": {"original_text": "  Affective Computing is the study of how computers can recognize, interpret\nand simulate human affects. Sentiment Analysis is a common task inNLP related\nto this topic, but it focuses only on emotion valence (positive, negative,\nneutral). An emerging approach in NLP is Emotion Recognition, which relies on\nfined-grained classification. This research describes an approach to create a\nlexical-based weakly supervised corpus for fine-grained emotion in Portuguese.\nWe evaluated our dataset by fine-tuning a transformer-based language model\n(BERT) and validating it on a Gold Standard annotated validation set. Our\nresults (F1-score=.64) suggest lexical-based weak supervision as an appropriate\nstrategy for initial work in low resourced environment.\n", "rewritten_text": "Affective Computing is the study of how computers can recognize, interpret, and simulate human emotions. Sentiment Analysis, a common task in NLP related to this topic, focuses solely on emotion valence (positive, negative, neutral). An emerging approach in NLP is Emotion Recognition, which relies on fine-grained classification. This research presents a method for creating a lexical-based weakly supervised corpus for fine-grained emotion analysis in Portuguese. The dataset was evaluated by fine-tuning a transformer-based language model (BERT) and validating it on a Gold Standard annotated validation set. The results, with an F1-score of 0.64, suggest that lexical-based weak supervision is a suitable strategy for initial work in low-resource environments."}, "2408.01653": {"original_text": "  We introduce Multi-Cylindrical Panoramic Depth Estimation (MCPDepth), a\ntwo-stage framework for omnidirectional depth estimation via stereo matching\nbetween multiple cylindrical panoramas. MCPDepth uses cylindrical panoramas for\ninitial stereo matching and then fuses the resulting depth maps across views. A\ncircular attention module is employed to overcome the distortion along the\nvertical axis. MCPDepth exclusively utilizes standard network components,\nsimplifying deployment to embedded devices and outperforming previous methods\nthat require custom kernels. We theoretically and experimentally compare\nspherical and cylindrical projections for stereo matching, highlighting the\nadvantages of the cylindrical projection. MCPDepth achieves state-of-the-art\nperformance with an 18.8% reduction in mean absolute error (MAE) for depth on\nthe outdoor synthetic dataset Deep360 and a 19.9% reduction on the indoor\nreal-scene dataset 3D60.\n", "rewritten_text": "Introducing Multi-Cylindrical Panoramic Depth Estimation (MCPDepth), a two-stage framework designed for omnidirectional depth estimation through stereo matching across multiple cylindrical panoramas. MCPDepth leverages cylindrical panoramas for initial stereo matching and subsequently integrates the resulting depth maps from various perspectives. To address distortion along the vertical axis, a circular attention module is incorporated. Notably, MCPDepth relies solely on standard network components, facilitating deployment on embedded devices and surpassing previous methods that necessitate custom kernels.\n\nWe conduct a theoretical and experimental comparison between spherical and cylindrical projections for stereo matching, underscoring the benefits of the cylindrical projection. Notably, MCPDepth attains state-of-the-art performance, achieving an 18.8% reduction in mean absolute error (MAE) for depth on the outdoor synthetic dataset Deep360 and a 19.9% reduction on the indoor real-scene dataset 3D60."}, "2111.09091": {"original_text": "  Monitoring behaviour in smart homes using sensors can offer insights into\nchanges in the independent ability and long-term health of residents. Passive\nInfrared motion sensors (PIRs) are standard, however may not accurately track\nthe full duration of movement. They also require line-of-sight to detect motion\nwhich can restrict performance and ensures they must be visible to residents.\nChannel State Information (CSI) is a low cost, unintrusive form of radio\nsensing which can monitor movement but also offers opportunities to generate\nrich data. We have developed a novel, self-calibrating motion detection system\nwhich uses CSI data collected and processed on a stock Raspberry Pi 4. This\nsystem exploits the correlation between CSI frames, on which we perform\nvariance analysis using our algorithm to accurately measure the full period of\na resident's movement. We demonstrate the effectiveness of this approach in\nseveral real-world environments. Experiments conducted demonstrate that\nactivity start and end time can be accurately detected for motion examples of\ndifferent intensities at different locations.\n", "rewritten_text": "Monitoring behavior in smart homes using sensors can provide valuable insights into changes in residents' independent abilities and long-term health. While Passive Infrared motion sensors (PIRs) are commonly used, they may not always accurately capture the complete duration of movement. Additionally, PIRs require a line of sight to detect motion, which can limit their effectiveness and necessitates visibility to residents.\n\nChannel State Information (CSI) presents a cost-effective and non-intrusive radio sensing method that not only monitors movement but also offers the potential to gather comprehensive data. Our team has developed an innovative, self-calibrating motion detection system that leverages CSI data processed on a standard Raspberry Pi 4. This system capitalizes on the correlation between CSI frames, utilizing our algorithm to conduct variance analysis and precisely measure the entirety of a resident's movement period.\n\nOur approach has been successfully tested in various real-world settings, demonstrating its efficacy in accurately detecting the start and end times of activities across different motion intensities and locations."}, "2405.02296": {"original_text": "  Perspective distortion (PD) causes unprecedented changes in shape, size,\norientation, angles, and other spatial relationships of visual concepts in\nimages. Precisely estimating camera intrinsic and extrinsic parameters is a\nchallenging task that prevents synthesizing perspective distortion.\nNon-availability of dedicated training data poses a critical barrier to\ndeveloping robust computer vision methods. Additionally, distortion correction\nmethods make other computer vision tasks a multi-step approach and lack\nperformance. In this work, we propose mitigating perspective distortion (MPD)\nby employing a fine-grained parameter control on a specific family of M\\\"obius\ntransform to model real-world distortion without estimating camera intrinsic\nand extrinsic parameters and without the need for actual distorted data. Also,\nwe present a dedicated perspectively distorted benchmark dataset, ImageNet-PD,\nto benchmark the robustness of deep learning models against this new dataset.\nThe proposed method outperforms existing benchmarks, ImageNet-E and ImageNet-X.\nAdditionally, it significantly improves performance on ImageNet-PD while\nconsistently performing on standard data distribution. Notably, our method\nshows improved performance on three PD-affected real-world applications crowd\ncounting, fisheye image recognition, and person re-identification and one\nPD-affected challenging CV task: object detection. The source code, dataset,\nand models are available on the project webpage at\nhttps://prakashchhipa.github.io/projects/mpd.\n", "rewritten_text": "Perspective distortion (PD) causes significant alterations in the shape, size, orientation, angles, and other spatial relationships of visual concepts within images. The precise estimation of camera intrinsic and extrinsic parameters poses a challenging task, hindering the synthesis of perspective distortion. The lack of dedicated training data presents a critical obstacle to the development of robust computer vision methods. Furthermore, existing distortion correction methods necessitate a multi-step approach and exhibit suboptimal performance in other computer vision tasks.\n\nIn this study, we introduce a novel approach, termed Mitigating Perspective Distortion (MPD), which involves fine-grained parameter control using a specific family of M\u00f6bius transforms to model real-world distortion. This method eliminates the need to estimate camera intrinsic and extrinsic parameters and does not require actual distorted data. Additionally, we introduce a dedicated benchmark dataset for perspectively distorted images, ImageNet-PD, to evaluate the resilience of deep learning models against this new dataset.\n\nOur proposed method surpasses existing benchmarks, namely ImageNet-E and ImageNet-X, and demonstrates significant performance enhancements on ImageNet-PD while maintaining consistent performance on standard data distributions. Notably, our method exhibits improved performance across three real-world applications affected by perspective distortion: crowd counting, fisheye image recognition, and person re-identification, as well as on the challenging computer vision task of object detection.\n\nFor further details, including the source code, dataset, and models, please visit the project webpage at https://prakashchhipa.github.io/projects/mpd."}, "1705.08479": {"original_text": "  This paper introduces a new architectural framework, known as input\nfast-forwarding, that can enhance the performance of deep networks. The main\nidea is to incorporate a parallel path that sends representations of input\nvalues forward to deeper network layers. This scheme is substantially different\nfrom \"deep supervision\" in which the loss layer is re-introduced to earlier\nlayers. The parallel path provided by fast-forwarding enhances the training\nprocess in two ways. First, it enables the individual layers to combine\nhigher-level information (from the standard processing path) with lower-level\ninformation (from the fast-forward path). Second, this new architecture reduces\nthe problem of vanishing gradients substantially because the fast-forwarding\npath provides a shorter route for gradient backpropagation. In order to\nevaluate the utility of the proposed technique, a Fast-Forward Network (FFNet),\nwith 20 convolutional layers along with parallel fast-forward paths, has been\ncreated and tested. The paper presents empirical results that demonstrate\nimproved learning capacity of FFNet due to fast-forwarding, as compared to\nGoogLeNet (with deep supervision) and CaffeNet, which are 4x and 18x larger in\nsize, respectively. All of the source code and deep learning models described\nin this paper will be made available to the entire research community\n", "rewritten_text": "This paper introduces a novel architectural framework called input fast-forwarding, designed to enhance the performance of deep networks. The key concept involves integrating a parallel path that forwards representations of input values to deeper network layers. This approach differs significantly from \"deep supervision,\" where the loss layer is reintroduced to earlier layers. The parallel path introduced by fast-forwarding improves the training process in two key ways. Firstly, it allows individual layers to merge higher-level information from the standard processing path with lower-level information from the fast-forward path. Secondly, this new architecture effectively mitigates the issue of vanishing gradients by providing a shorter route for gradient backpropagation through the fast-forwarding path.\n\nTo assess the effectiveness of the proposed technique, a Fast-Forward Network (FFNet) comprising 20 convolutional layers with parallel fast-forward paths was developed and tested. The paper presents empirical results demonstrating the enhanced learning capacity of FFNet due to fast-forwarding, in comparison to GoogLeNet (with deep supervision) and CaffeNet, which are 4x and 18x larger in size, respectively. The source code and deep learning models described in this paper will be made available to the entire research community."}, "2002.06515": {"original_text": "  Automatic analysis of highly crowded people has attracted extensive attention\nfrom computer vision research. Previous approaches for crowd counting have\nalready achieved promising performance across various benchmarks. However, to\ndeal with the real situation, we hope the model run as fast as possible while\nkeeping accuracy. In this paper, we propose a compact convolutional neural\nnetwork for crowd counting which learns a more efficient model with a small\nnumber of parameters. With three parallel filters executing the convolutional\noperation on the input image simultaneously at the front of the network, our\nmodel could achieve nearly real-time speed and save more computing resources.\nExperiments on two benchmarks show that our proposed method not only takes a\nbalance between performance and efficiency which is more suitable for actual\nscenes but also is superior to existing light-weight models in speed.\n", "rewritten_text": "The automatic analysis of densely populated areas has garnered significant attention in computer vision research. Previous crowd counting methods have shown promising performance on various benchmarks. However, in practical scenarios, there is a need for models to run with high speed while maintaining accuracy. In this study, we introduce a compact convolutional neural network for crowd counting that focuses on learning an efficient model with a reduced number of parameters. By employing three parallel filters to perform convolution operations simultaneously on the input image at the network's forefront, our model achieves nearly real-time processing speed and conserves computational resources. Experimental results on two benchmarks demonstrate that our proposed approach strikes a balance between performance and efficiency, making it well-suited for real-world applications. Furthermore, our method outperforms existing lightweight models in terms of speed."}, "1711.03278": {"original_text": "  Convolution Neural Networks (CNN), known as ConvNets are widely used in many\nvisual imagery application, object classification, speech recognition. After\nthe implementation and demonstration of the deep convolution neural network in\nImagenet classification in 2012 by krizhevsky, the architecture of deep\nConvolution Neural Network is attracted many researchers. This has led to the\nmajor development in Deep learning frameworks such as Tensorflow, caffe, keras,\ntheno. Though the implementation of deep learning is quite possible by\nemploying deep learning frameworks, mathematical theory and concepts are harder\nto understand for new learners and practitioners. This article is intended to\nprovide an overview of ConvNets architecture and to explain the mathematical\ntheory behind it including activation function, loss function, feedforward and\nbackward propagation. In this article, grey scale image is taken as input\ninformation image, ReLU and Sigmoid activation function are considered for\ndeveloping the architecture and cross-entropy loss function are used for\ncomputing the difference between predicted value and actual value. The\narchitecture is developed in such a way that it can contain one convolution\nlayer, one pooling layer, and multiple dense layers\n", "rewritten_text": "Convolutional Neural Networks (CNNs), also known as ConvNets, are widely utilized in various visual imagery applications, including object classification and speech recognition. The deep convolutional neural network showcased in the ImageNet classification in 2012 by Krizhevsky sparked significant interest among researchers in the architecture of deep CNNs. This, in turn, has driven major advancements in deep learning frameworks such as TensorFlow, Caffe, Keras, and Theano. While implementing deep learning using these frameworks is feasible, understanding the underlying mathematical theories and concepts can be challenging for newcomers and practitioners.\n\nThis article aims to provide an overview of ConvNets' architecture and elucidate the mathematical theories behind them, encompassing activation functions, loss functions, feedforward, and backward propagation. In this context, grayscale images serve as the input information, with ReLU and Sigmoid activation functions considered for architecture development, and cross-entropy loss functions employed to compute the variance between predicted and actual values. The architecture is structured to accommodate a convolutional layer, a pooling layer, and multiple dense layers."}, "1610.07995": {"original_text": "  In this paper, the task of recognizing signs made by hearing impaired people\nat sentence level has been addressed. A novel method of extracting spatial\nfeatures to capture hand movements of a signer has been proposed. Frames of a\ngiven video of a sign are preprocessed to extract face and hand components of a\nsigner. The local centroids of the extracted components along with the global\ncentroid are exploited to extract spatial features. The concept of interval\nvalued type symbolic data has been explored to capture variations in the same\nsign made by the different signers at different instances of time. A suitable\nsymbolic similarity measure is studied to establish matching between test and\nreference signs and a simple nearest neighbour classifier is used to recognize\nan unknown sign as one among the known signs by specifying a desired level of\nthreshold. An extensive experimentation is conducted on a considerably large\ndatabase of signs created by us during the course of research work in order to\nevaluate the performance of the proposed system\n", "rewritten_text": "This paper addresses the task of recognizing signs made by hearing-impaired individuals at the sentence level. A novel method is proposed for extracting spatial features to capture hand movements of a signer. The frames of a given sign video are preprocessed to extract the face and hand components of the signer. The local centroids of the extracted components, along with the global centroid, are utilized to extract spatial features. The concept of interval-valued symbolic data is explored to capture variations in the same sign made by different signers at different instances of time. A suitable symbolic similarity measure is studied to establish a match between test and reference signs, and a simple nearest neighbor classifier is employed to recognize an unknown sign as one among the known signs by specifying a desired threshold level. Extensive experimentation is conducted on a significantly large database of signs created during the research work to evaluate the performance of the proposed system."}, "1908.04501": {"original_text": "  When a deep neural network is trained on data with only image-level labeling,\nthe regions activated in each image tend to identify only a small region of the\ntarget object. We propose a method of using videos automatically harvested from\nthe web to identify a larger region of the target object by using temporal\ninformation, which is not present in the static image. The temporal variations\nin a video allow different regions of the target object to be activated. We\nobtain an activated region in each frame of a video, and then aggregate the\nregions from successive frames into a single image, using a warping technique\nbased on optical flow. The resulting localization maps cover more of the target\nobject, and can then be used as proxy ground-truth to train a segmentation\nnetwork. This simple approach outperforms existing methods under the same level\nof supervision, and even approaches relying on extra annotations. Based on\nVGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4,\nrespectively, on PASCAL VOC 2012 test images, which represents a new\nstate-of-the-art.\n", "rewritten_text": "When training a deep neural network on data with only image-level labeling, the activated regions in each image typically identify only a small portion of the target object. To address this limitation, we propose a novel method that leverages videos automatically collected from the web to identify a larger region of the target object by incorporating temporal information, which is absent in static images. The temporal variations present in videos enable different regions of the target object to be activated. Our approach involves extracting an activated region in each frame of a video and then combining these regions from consecutive frames into a single image using a warping technique based on optical flow. The resulting localization maps cover a greater area of the target object and can serve as proxy ground-truth data to train a segmentation network. Remarkably, this straightforward technique surpasses existing methods with the same level of supervision and even rivals approaches that rely on additional annotations. Utilizing VGG-16 and ResNet 101 backbones, our method achieves mIoU scores of 65.0 and 67.4, respectively, on PASCAL VOC 2012 test images, establishing a new state-of-the-art performance level."}, "2203.01532": {"original_text": "  Recently, contrastive learning-based image translation methods have been\nproposed, which contrasts different spatial locations to enhance the spatial\ncorrespondence. However, the methods often ignore the diverse semantic relation\nwithin the images. To address this, here we propose a novel semantic relation\nconsistency (SRC) regularization along with the decoupled contrastive learning,\nwhich utilize the diverse semantics by focusing on the heterogeneous semantics\nbetween the image patches of a single image. To further improve the\nperformance, we present a hard negative mining by exploiting the semantic\nrelation. We verified our method for three tasks: single-modal and multi-modal\nimage translations, and GAN compression task for image translation.\nExperimental results confirmed the state-of-art performance of our method in\nall the three tasks.\n", "rewritten_text": "Recently, there have been advancements in contrastive learning-based image translation methods that aim to enhance spatial correspondence by contrasting different spatial locations. However, these methods often overlook the diverse semantic relationships present within images. To address this limitation, we introduce a novel approach called Semantic Relation Consistency (SRC) regularization in conjunction with decoupled contrastive learning. This approach leverages diverse semantics by focusing on the heterogeneous semantic relationships between image patches within a single image.\n\nTo further enhance performance, we introduce a hard negative mining technique that exploits semantic relationships. Our method was evaluated across three tasks: single-modal and multi-modal image translations, as well as GAN compression for image translation. Experimental results demonstrate the state-of-the-art performance of our approach across all three tasks."}, "2310.03414": {"original_text": "  Multi-document summarization is a challenging task due to its inherent\nsubjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4\namong DUC-2004 reference summaries. In this work, we aim to enhance the\nobjectivity of news summarization by focusing on the main event of a group of\nrelated news documents and presenting it coherently with sufficient context.\nOur primary objective is to succinctly report the main event, ensuring that the\nsummary remains objective and informative. To achieve this, we employ an\nextract-rewrite approach that incorporates a main-event biased\nmonotone-submodular function for content selection. This enables us to extract\nthe most crucial information related to the main event from the document\ncluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for\nrewriting the extracted content into a coherent text. The evaluation using\nobjective metrics and human evaluators confirms the effectiveness of our\napproach, as it surpasses potential baselines, demonstrating excellence in both\ncontent coverage, coherence, and informativeness.\n", "rewritten_text": "Multi-document summarization presents a significant challenge due to its inherent subjective bias, as evidenced by the low inter-annotator ROUGE-1 score of 0.4 in DUC-2004 reference summaries. This study aims to enhance the objectivity of news summarization by focusing on the primary event within a collection of related news documents and presenting it cohesively with ample context. The main goal is to concisely report the central event while maintaining objectivity and providing valuable information. To achieve this, an extract-rewrite approach is employed, incorporating a main-event biased monotone-submodular function for content selection. This method allows for the extraction of essential information pertaining to the main event from the document cluster. To ensure coherence, a fine-tuned Language Model (LLM) is utilized to rewrite the extracted content into a coherent text. Evaluation using objective metrics and human evaluators validates the effectiveness of this approach, surpassing potential baselines and demonstrating excellence in content coverage, coherence, and informativeness."}, "1608.04314": {"original_text": "  We present a technique for weakly supervised object localization (WSOL),\nbuilding on the observation that WSOL algorithms usually work better on images\nwith bigger objects. Instead of training the object detector on the entire\ntraining set at the same time, we propose a curriculum learning strategy to\nfeed training images into the WSOL learning loop in an order from images\ncontaining bigger objects down to smaller ones. To automatically determine the\norder, we train a regressor to estimate the size of the object given the whole\nimage as input. Furthermore, we use these size estimates to further improve the\nre-localization step of WSOL by assigning weights to object proposals according\nto how close their size matches the estimated object size. We demonstrate the\neffectiveness of using size order and size weighting on the challenging PASCAL\nVOC 2007 dataset, where we achieve a significant improvement over existing\nstate-of-the-art WSOL techniques.\n", "rewritten_text": "We introduce a technique for weakly supervised object localization (WSOL) that capitalizes on the observation that WSOL algorithms tend to perform better on images featuring larger objects. Rather than training the object detector on the entire training set simultaneously, we propose a curriculum learning approach. This strategy involves feeding training images into the WSOL learning loop in a sequence starting from images containing larger objects and progressing to smaller ones.\n\nTo automate the determination of this sequence, we train a regressor to predict the object size based on the entire image input. Additionally, we leverage these size predictions to enhance the re-localization step of WSOL by assigning weights to object proposals based on their proximity to the estimated object size. By incorporating size order and size weighting, we demonstrate notable improvements on the challenging PASCAL VOC 2007 dataset compared to existing state-of-the-art WSOL techniques."}, "2402.00160": {"original_text": "  In this work, we introduce the Multiple Embedding Model for EHR (MEME), an\napproach that serializes multimodal EHR tabular data into text using\npseudo-notes, mimicking clinical text generation. This conversion not only\npreserves better representations of categorical data and learns contexts but\nalso enables the effective employment of pretrained foundation models for rich\nfeature representation. To address potential issues with context length, our\nframework encodes embeddings for each EHR modality separately. We demonstrate\nthe effectiveness of MEME by applying it to several decision support tasks\nwithin the Emergency Department across multiple hospital systems. Our findings\nindicate that MEME outperforms traditional machine learning, EHR-specific\nfoundation models, and general LLMs, highlighting its potential as a general\nand extendible EHR representation strategy.\n", "rewritten_text": "In this study, we present the Multiple Embedding Model for Electronic Health Records (MEME), a method that transforms multimodal EHR tabular data into text using pseudo-notes to simulate clinical text generation. This conversion not only enhances the representation of categorical data and captures contextual information, but also facilitates the utilization of pretrained foundation models for comprehensive feature representation. To mitigate potential challenges related to context length, our framework encodes embeddings for each EHR modality separately. By applying MEME to various decision support tasks in Emergency Departments across multiple hospital systems, we demonstrate its effectiveness. Our results show that MEME surpasses traditional machine learning approaches, EHR-specific foundation models, and general Large Language Models (LLMs), underscoring its potential as a versatile and scalable EHR representation strategy."}, "2305.11853": {"original_text": "  Large language models (LLMs) with in-context learning have demonstrated\nremarkable capability in the text-to-SQL task. Previous research has prompted\nLLMs with various demonstration-retrieval strategies and intermediate reasoning\nsteps to enhance the performance of LLMs. However, those works often employ\nvaried strategies when constructing the prompt text for text-to-SQL inputs,\nsuch as databases and demonstration examples. This leads to a lack of\ncomparability in both the prompt constructions and their primary contributions.\nFurthermore, selecting an effective prompt construction has emerged as a\npersistent problem for future research. To address this limitation, we\ncomprehensively investigate the impact of prompt constructions across various\nsettings and provide insights into prompt constructions for future text-to-SQL\nstudies.\n", "rewritten_text": "Large language models (LLMs) with in-context learning have shown remarkable capability in the text-to-SQL task. Previous research has explored different demonstration-retrieval strategies and intermediate reasoning steps to improve LLM performance. However, these studies often use diverse strategies in constructing prompt texts for text-to-SQL inputs, including databases and demonstration examples. This results in a lack of comparability in prompt constructions and their main contributions. Additionally, the selection of an effective prompt construction remains a persistent challenge for future research. To address this limitation, we conduct a comprehensive investigation into the impact of prompt constructions across various settings and offer insights into prompt constructions for future text-to-SQL studies."}, "1007.0621": {"original_text": "  In this paper fusion of visual and thermal images in wavelet transformed\ndomain has been presented. Here, Daubechies wavelet transform, called as D2,\ncoefficients from visual and corresponding coefficients computed in the same\nmanner from thermal images are combined to get fused coefficients. After\ndecomposition up to fifth level (Level 5) fusion of coefficients is done.\nInverse Daubechies wavelet transform of those coefficients gives us fused face\nimages. The main advantage of using wavelet transform is that it is well-suited\nto manage different image resolution and allows the image decomposition in\ndifferent kinds of coefficients, while preserving the image information. Fused\nimages thus found are passed through Principal Component Analysis (PCA) for\nreduction of dimensions and then those reduced fused images are classified\nusing a multi-layer perceptron. For experiments IRIS Thermal/Visual Face\nDatabase was used. Experimental results show that the performance of the\napproach presented here achieves maximum success rate of 100% in many cases.\n", "rewritten_text": "This paper presents the fusion of visual and thermal images in the wavelet transformed domain. The Daubechies wavelet transform, specifically the D2 wavelet, is utilized to combine coefficients from visual images with corresponding coefficients from thermal images. The fused coefficients are obtained after decomposition up to the fifth level (Level 5). By performing the inverse Daubechies wavelet transform on these coefficients, fused face images are generated. The key advantage of employing wavelet transform lies in its ability to handle various image resolutions and conduct image decomposition into different types of coefficients while preserving image information.\n\nThe resulting fused images undergo Principal Component Analysis (PCA) for dimensionality reduction. Subsequently, the reduced fused images are classified using a multi-layer perceptron. The experiments were conducted using the IRIS Thermal/Visual Face Database. The experimental results demonstrate that the proposed approach achieves a maximum success rate of 100% in many instances."}, "2210.11725": {"original_text": "  We present AROS, a one-shot learning approach that uses an explicit\nrepresentation of interactions between highly-articulated human poses and 3D\nscenes. The approach is one-shot as the method does not require re-training to\nadd new affordance instances. Furthermore, only one or a small handful of\nexamples of the target pose are needed to describe the interaction. Given a 3D\nmesh of a previously unseen scene, we can predict affordance locations that\nsupport the interactions and generate corresponding articulated 3D human bodies\naround them. We evaluate on three public datasets of scans of real environments\nwith varied degrees of noise. Via rigorous statistical analysis of crowdsourced\nevaluations, results show that our one-shot approach outperforms data-intensive\nbaselines by up to 80\\%.\n", "rewritten_text": "We introduce AROS, a one-shot learning approach that utilizes an explicit representation of interactions between highly-articulated human poses and 3D scenes. This approach is considered one-shot because it does not necessitate re-training to incorporate new affordance instances. Additionally, only one or a small number of examples of the target pose are required to describe the interaction. When provided with a 3D mesh of a scene that has not been encountered before, we can anticipate affordance locations that facilitate the interactions and create corresponding articulated 3D human bodies around them. Our evaluation is conducted on three public datasets containing scans of real environments with varying levels of noise. Through thorough statistical analysis of crowdsourced evaluations, the results demonstrate that our one-shot approach surpasses data-intensive baselines by up to 80%."}, "1604.06646": {"original_text": "  In this paper we introduce a new method for text detection in natural images.\nThe method comprises two contributions: First, a fast and scalable engine to\ngenerate synthetic images of text in clutter. This engine overlays synthetic\ntext to existing background images in a natural way, accounting for the local\n3D scene geometry. Second, we use the synthetic images to train a\nFully-Convolutional Regression Network (FCRN) which efficiently performs text\ndetection and bounding-box regression at all locations and multiple scales in\nan image. We discuss the relation of FCRN to the recently-introduced YOLO\ndetector, as well as other end-to-end object detection systems based on deep\nlearning. The resulting detection network significantly out performs current\nmethods for text detection in natural images, achieving an F-measure of 84.2%\non the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per\nsecond on a GPU.\n", "rewritten_text": "This paper introduces a novel method for detecting text in natural images. The method consists of two main components. Firstly, there is a fast and scalable engine that generates synthetic images of text within cluttered backgrounds. This engine seamlessly overlays synthetic text onto existing background images, taking into account the local 3D scene geometry. Secondly, the synthetic images are utilized to train a Fully-Convolutional Regression Network (FCRN). The FCRN efficiently performs text detection and bounding-box regression at various locations and scales within an image.\n\nThe paper also explores the relationship between FCRN and the YOLO detector, as well as other end-to-end object detection systems based on deep learning. The resulting detection network surpasses current methods for text detection in natural images, achieving an impressive F-measure of 84.2% on the standard ICDAR 2013 benchmark. Additionally, it can process 15 images per second on a GPU."}, "2204.06028": {"original_text": "  In this paper, we describe our submission to the Simultaneous Speech\nTranslation at IWSLT 2022. We explore strategies to utilize an offline model in\na simultaneous setting without the need to modify the original model. In our\nexperiments, we show that our onlinization algorithm is almost on par with the\noffline setting while being $3\\times$ faster than offline in terms of latency\non the test set. We also show that the onlinized offline model outperforms the\nbest IWSLT2021 simultaneous system in medium and high latency regimes and is\nalmost on par in the low latency regime. We make our system publicly available.\n", "rewritten_text": "In this paper, we present our submission to the Simultaneous Speech Translation at IWSLT 2022. We investigate strategies for utilizing an offline model in a simultaneous setting without requiring modifications to the original model. Through our experiments, we demonstrate that our onlinization algorithm performs nearly as well as the offline setting, while achieving a latency on the test set that is three times faster than offline. Furthermore, we illustrate that the onlinized offline model surpasses the top IWSLT2021 simultaneous system in medium and high latency scenarios, and is nearly equivalent in the low latency scenario. Our system is now available to the public."}, "2304.02715": {"original_text": "  This paper presents an unsupervised approach that leverages raw aerial videos\nto learn to estimate planar homographic transformation between consecutive\nvideo frames. Previous learning-based estimators work on pairs of images to\nestimate their planar homographic transformations but suffer from severe\nover-fitting issues, especially when applying over aerial videos. To address\nthis concern, we develop a sequential estimator that directly processes a\nsequence of video frames and estimates their pairwise planar homographic\ntransformations in batches. We also incorporate a set of spatial-temporal\nknowledge to regularize the learning of such a sequence-to-sequence model. We\ncollect a set of challenging aerial videos and compare the proposed method to\nthe alternative algorithms. Empirical studies suggest that our sequential model\nachieves significant improvement over alternative image-based methods and the\nknowledge-rich regularization further boosts our system performance. Our codes\nand dataset could be found at https://github.com/Paul-LiPu/DeepVideoHomography\n", "rewritten_text": "This paper introduces an unsupervised approach that utilizes raw aerial videos to learn how to estimate planar homographic transformations between consecutive video frames. Previous learning-based estimators typically work on pairs of images to estimate their planar homographic transformations, but they often face significant overfitting challenges, particularly when applied to aerial videos. To tackle this issue, we have developed a sequential estimator that processes a sequence of video frames directly and estimates their pairwise planar homographic transformations in batches. Additionally, we have integrated a spatial-temporal knowledge set to regularize the learning process of this sequence-to-sequence model. We have gathered a collection of challenging aerial videos and compared our proposed method with alternative algorithms. Empirical studies indicate that our sequential model demonstrates a notable improvement over alternative image-based methods, and the incorporation of knowledge-rich regularization further enhances our system's performance. Our code and dataset can be accessed at https://github.com/Paul-LiPu/DeepVideoHomography."}, "2306.16925": {"original_text": "  Pretraining with large-scale 3D volumes has a potential for improving the\nsegmentation performance on a target medical image dataset where the training\nimages and annotations are limited. Due to the high cost of acquiring\npixel-level segmentation annotations on the large-scale pretraining dataset,\npretraining with unannotated images is highly desirable. In this work, we\npropose a novel self-supervised learning strategy named Volume Fusion (VF) for\npretraining 3D segmentation models. It fuses several random patches from a\nforeground sub-volume to a background sub-volume based on a predefined set of\ndiscrete fusion coefficients, and forces the model to predict the fusion\ncoefficient of each voxel, which is formulated as a self-supervised\nsegmentation task without manual annotations. Additionally, we propose a novel\nnetwork architecture based on parallel convolution and transformer blocks that\nis suitable to be transferred to different downstream segmentation tasks with\nvarious scales of organs and lesions. The proposed model was pretrained with\n110k unannotated 3D CT volumes, and experiments with different downstream\nsegmentation targets including head and neck organs, thoracic/abdominal organs\nshowed that our pretrained model largely outperformed training from scratch and\nseveral state-of-the-art self-supervised training methods and segmentation\nmodels. The code and pretrained model are available at\nhttps://github.com/openmedlab/MIS-FM.\n", "rewritten_text": "Pretraining with large-scale 3D volumes has the potential to enhance segmentation performance on a target medical image dataset with limited training images and annotations. Given the high cost of acquiring pixel-level segmentation annotations for the large-scale pretraining dataset, pretraining with unannotated images is highly desirable. In this study, we introduce a novel self-supervised learning strategy called Volume Fusion (VF) for pretraining 3D segmentation models. VF involves fusing random patches from a foreground sub-volume to a background sub-volume using a predefined set of discrete fusion coefficients. This approach compels the model to predict the fusion coefficient of each voxel, framing it as a self-supervised segmentation task without manual annotations.\n\nFurthermore, we propose a novel network architecture that incorporates parallel convolution and transformer blocks, making it suitable for transfer to various downstream segmentation tasks involving organs and lesions of different scales. The model was pretrained using 110k unannotated 3D CT volumes. Experimental results on different downstream segmentation targets, such as head and neck organs, and thoracic/abdominal organs, demonstrate that our pretrained model significantly outperforms training from scratch and several state-of-the-art self-supervised training methods and segmentation models. The code and pretrained model can be accessed at https://github.com/openmedlab/MIS-FM."}, "2102.0932": {"original_text": "  Event cameras are novel vision sensors that report per-pixel brightness\nchanges as a stream of asynchronous \"events\". They offer significant advantages\ncompared to standard cameras due to their high temporal resolution, high\ndynamic range and lack of motion blur. However, events only measure the varying\ncomponent of the visual signal, which limits their ability to encode scene\ncontext. By contrast, standard cameras measure absolute intensity frames, which\ncapture a much richer representation of the scene. Both sensors are thus\ncomplementary. However, due to the asynchronous nature of events, combining\nthem with synchronous images remains challenging, especially for learning-based\nmethods. This is because traditional recurrent neural networks (RNNs) are not\ndesigned for asynchronous and irregular data from additional sensors. To\naddress this challenge, we introduce Recurrent Asynchronous Multimodal (RAM)\nnetworks, which generalize traditional RNNs to handle asynchronous and\nirregular data from multiple sensors. Inspired by traditional RNNs, RAM\nnetworks maintain a hidden state that is updated asynchronously and can be\nqueried at any time to generate a prediction. We apply this novel architecture\nto monocular depth estimation with events and frames where we show an\nimprovement over state-of-the-art methods by up to 30% in terms of mean\nabsolute depth error. To enable further research on multimodal learning with\nevents, we release EventScape, a new dataset with events, intensity frames,\nsemantic labels, and depth maps recorded in the CARLA simulator.\n", "rewritten_text": "Event cameras are innovative vision sensors that detect per-pixel brightness changes through a stream of asynchronous \"events.\" They offer significant advantages over standard cameras, thanks to their high temporal resolution, wide dynamic range, and absence of motion blur. However, events only capture the changing aspect of the visual signal, limiting their ability to encode scene context. In contrast, standard cameras record absolute intensity frames, providing a more detailed representation of the scene. Both types of sensors are thus complementary. Nevertheless, integrating asynchronous events with synchronous images poses a challenge, particularly for learning-based methods, due to the asynchronous nature of events. Traditional recurrent neural networks (RNNs) are not optimized for handling asynchronous and irregular data from additional sensors. To tackle this issue, we introduce Recurrent Asynchronous Multimodal (RAM) networks, which extend traditional RNNs to manage asynchronous and irregular data from multiple sensors. Drawing inspiration from traditional RNNs, RAM networks maintain a hidden state that is updated asynchronously and can be accessed at any time to make predictions. We apply this innovative architecture to monocular depth estimation using events and frames, demonstrating an improvement of up to 30% in mean absolute depth error compared to state-of-the-art methods. To facilitate further research on multimodal learning with events, we introduce EventScape, a new dataset containing events, intensity frames, semantic labels, and depth maps captured in the CARLA simulator."}, "2410.05963": {"original_text": "  Existing perception models achieve great success by learning from large\namounts of labeled data, but they still struggle with open-world scenarios. To\nalleviate this issue, researchers introduce open-set perception tasks to detect\nor segment unseen objects in the training set. However, these models require\npredefined object categories as inputs during inference, which are not\navailable in real-world scenarios. Recently, researchers pose a new and more\npractical problem, \\textit{i.e.}, open-ended object detection, which discovers\nunseen objects without any object categories as inputs. In this paper, we\npresent VL-SAM, a training-free framework that combines the generalized object\nrecognition model (\\textit{i.e.,} Vision-Language Model) with the generalized\nobject localization model (\\textit{i.e.,} Segment-Anything Model), to address\nthe open-ended object detection and segmentation task. Without additional\ntraining, we connect these two generalized models with attention maps as the\nprompts. Specifically, we design an attention map generation module by\nemploying head aggregation and a regularized attention flow to aggregate and\npropagate attention maps across all heads and layers in VLM, yielding\nhigh-quality attention maps. Then, we iteratively sample positive and negative\npoints from the attention maps with a prompt generation module and send the\nsampled points to SAM to segment corresponding objects. Experimental results on\nthe long-tail instance segmentation dataset (LVIS) show that our method\nsurpasses the previous open-ended method on the object detection task and can\nprovide additional instance segmentation masks. Besides, VL-SAM achieves\nfavorable performance on the corner case object detection dataset (CODA),\ndemonstrating the effectiveness of VL-SAM in real-world applications. Moreover,\nVL-SAM exhibits good model generalization that can incorporate various VLMs and\nSAMs.\n", "rewritten_text": "Current perception models have achieved significant success through learning from extensive amounts of labeled data, yet they encounter challenges in open-world scenarios. To address this issue, researchers have introduced open-set perception tasks aimed at detecting or segmenting unseen objects not present in the training set. However, these models rely on predefined object categories during inference, which are often unavailable in real-world settings. Recently, researchers have introduced a new and more practical problem known as open-ended object detection, which aims to identify unseen objects without the need for predefined object categories.\n\nIn this paper, we introduce VL-SAM, a training-free framework that combines the generalized object recognition model (Vision-Language Model) with the generalized object localization model (Segment-Anything Model) to tackle the open-ended object detection and segmentation task. Without the need for additional training, we connect these two generalized models using attention maps as prompts. Specifically, we have designed an attention map generation module that utilizes head aggregation and a regularized attention flow to aggregate and propagate attention maps across all heads and layers in the Vision-Language Model, resulting in high-quality attention maps.\n\nWe then iteratively sample positive and negative points from the attention maps using a prompt generation module and feed these sampled points to the Segment-Anything Model to segment the corresponding objects. Experimental results on the long-tail instance segmentation dataset (LVIS) demonstrate that our method outperforms previous open-ended approaches in object detection and provides additional instance segmentation masks. Additionally, VL-SAM shows promising performance on the corner case object detection dataset (CODA), showcasing its effectiveness in real-world applications. Furthermore, VL-SAM exhibits strong model generalization capabilities, allowing for integration with various Vision-Language Models and Segment-Anything Models."}, "1805.05421": {"original_text": "  Deep learning has made significant improvements at many image processing\ntasks in recent years, such as image classification, object recognition and\nobject detection. Convolutional neural networks (CNN), which is a popular deep\nlearning architecture designed to process data in multiple array form, show\ngreat success to almost all detection \\& recognition problems and computer\nvision tasks. However, the number of parameters in a CNN is too high such that\nthe computers require more energy and larger memory size. In order to solve\nthis problem, we propose a novel energy efficient model Binary Weight and\nHadamard-transformed Image Network (BWHIN), which is a combination of Binary\nWeight Network (BWN) and Hadamard-transformed Image Network (HIN). It is\nobserved that energy efficiency is achieved with a slight sacrifice at\nclassification accuracy. Among all energy efficient networks, our novel\nensemble model outperforms other energy efficient models.\n", "rewritten_text": "In recent years, deep learning has significantly enhanced various image processing tasks, including image classification, object recognition, and object detection. Convolutional neural networks (CNN), a popular deep learning architecture designed to process data in multiple array forms, have demonstrated great success in addressing detection and recognition challenges as well as computer vision tasks. However, the high number of parameters in a CNN results in increased energy consumption and larger memory requirements for computers. To address this issue, we propose a novel energy-efficient model called Binary Weight and Hadamard-transformed Image Network (BWHIN), which combines Binary Weight Network (BWN) and Hadamard-transformed Image Network (HIN). Our research indicates that energy efficiency is achieved with a minor trade-off in classification accuracy. Among various energy-efficient networks, our innovative ensemble model surpasses other energy-efficient models."}, "2001.11561": {"original_text": "  We consider referring image segmentation. It is a problem at the intersection\nof computer vision and natural language understanding. Given an input image and\na referring expression in the form of a natural language sentence, the goal is\nto segment the object of interest in the image referred by the linguistic\nquery. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to\ntackle this problem. Our model consists of an encoder network and a decoder\nnetwork, where ConvLSTM is used in both encoder and decoder networks to capture\nspatial and sequential information. The encoder network extracts visual and\nlinguistic features for each word in the expression sentence, and adopts an\nattention mechanism to focus on words that are more informative in the\nmultimodal interaction. The decoder network integrates the features generated\nby the encoder network at multiple levels as its input and produces the final\nprecise segmentation mask. Experimental results on four challenging datasets\ndemonstrate that the proposed network achieves superior segmentation\nperformance compared with other state-of-the-art methods.\n", "rewritten_text": "We are exploring image segmentation, a problem that lies at the intersection of computer vision and natural language understanding. The objective is to segment the object of interest in an input image based on a referring expression provided in the form of a natural language sentence. To address this challenge, we introduce a dual convolutional LSTM (ConvLSTM) network. Our model comprises an encoder network and a decoder network, both utilizing ConvLSTM to capture spatial and sequential information. The encoder network extracts visual and linguistic features for each word in the referring expression, employing an attention mechanism to prioritize informative words in the multimodal interaction. The decoder network integrates features from the encoder network at various levels to generate the final precise segmentation mask. Experimental results on four challenging datasets demonstrate that our proposed network outperforms other state-of-the-art methods in segmentation performance."}, "1903.07224": {"original_text": "  This work develops a novel end-to-end deep unsupervised learning method based\non convolutional neural network (CNN) with pseudo-classes for remote sensing\nscene representation. First, we introduce center points as the centers of the\npseudo classes and the training samples can be allocated with pseudo labels\nbased on the center points. Therefore, the CNN model, which is used to extract\nfeatures from the scenes, can be trained supervised with the pseudo labels.\nMoreover, a pseudo-center loss is developed to decrease the variance between\nthe samples and the corresponding pseudo center point. The pseudo-center loss\nis important since it can update both the center points with the training\nsamples and the CNN model with the center points in the training process\nsimultaneously. Finally, joint learning of the pseudo-center loss and the\npseudo softmax loss which is formulated with the samples and the pseudo labels\nis developed for unsupervised remote sensing scene representation to obtain\ndiscriminative representations from the scenes. Experiments are conducted over\ntwo commonly used remote sensing scene datasets to validate the effectiveness\nof the proposed method and the experimental results show the superiority of the\nproposed method when compared with other state-of-the-art methods.\n", "rewritten_text": "This study presents a novel end-to-end deep unsupervised learning approach utilizing a convolutional neural network (CNN) with pseudo-classes for remote sensing scene representation. Initially, center points are introduced as the focal points of the pseudo classes, enabling the allocation of training samples with pseudo labels based on these center points. Consequently, the CNN model, employed for feature extraction from the scenes, can be supervisedly trained using the pseudo labels. Additionally, a pseudo-center loss is devised to minimize the variance between the samples and their corresponding pseudo center points. This loss is crucial as it facilitates the simultaneous updating of both the center points with the training samples and the CNN model with the center points during the training process. Subsequently, a joint learning approach is proposed, combining the pseudo-center loss and the pseudo softmax loss, which is formulated using the samples and pseudo labels, to achieve unsupervised remote sensing scene representation and derive discriminative representations from the scenes. Experimental evaluations are conducted on two widely used remote sensing scene datasets to validate the efficacy of the proposed method. The results of the experiments demonstrate the superiority of the proposed method over other state-of-the-art methods."}, "2203.073": {"original_text": "  Current mobile user authentication systems based on PIN codes, fingerprint,\nand face recognition have several shortcomings. Such limitations have been\naddressed in the literature by exploring the feasibility of passive\nauthentication on mobile devices through behavioral biometrics. In this line of\nresearch, this work carries out a comparative analysis of unimodal and\nmultimodal behavioral biometric traits acquired while the subjects perform\ndifferent activities on the phone such as typing, scrolling, drawing a number,\nand tapping on the screen, considering the touchscreen and the simultaneous\nbackground sensor data (accelerometer, gravity sensor, gyroscope, linear\naccelerometer, and magnetometer). Our experiments are performed over HuMIdb,\none of the largest and most comprehensive freely available mobile user\ninteraction databases to date. A separate Recurrent Neural Network (RNN) with\ntriplet loss is implemented for each single modality. Then, the weighted fusion\nof the different modalities is carried out at score level. In our experiments,\nthe most discriminative background sensor is the magnetometer, whereas among\ntouch tasks the best results are achieved with keystroke in a fixed-text\nscenario. In all cases, the fusion of modalities is very beneficial, leading to\nEqual Error Rates (EER) ranging from 4% to 9% depending on the modality\ncombination in a 3-second interval.\n", "rewritten_text": "Current mobile user authentication systems, which rely on PIN codes, fingerprint scans, and face recognition, exhibit various shortcomings. These limitations have been addressed in the literature by investigating the potential of passive authentication on mobile devices using behavioral biometrics. This study conducts a comparative analysis of unimodal and multimodal behavioral biometric traits collected while subjects engage in various activities on their phones, such as typing, scrolling, drawing numbers, and tapping on the screen. The analysis takes into account both the touchscreen data and the concurrent background sensor data, including the accelerometer, gravity sensor, gyroscope, linear accelerometer, and magnetometer.\n\nThe experiments are conducted using HuMIdb, one of the most extensive and comprehensive freely available databases of mobile user interactions. For each individual modality, a separate Recurrent Neural Network (RNN) with triplet loss is implemented. Subsequently, a weighted fusion of the different modalities is performed at the score level. The results indicate that the magnetometer is the most discriminative background sensor, while keystroke tasks in a fixed-text scenario yield the best outcomes among touch-related activities. Overall, the fusion of modalities proves highly beneficial, resulting in Equal Error Rates (EER) ranging from 4% to 9% depending on the combination of modalities within a 3-second timeframe."}, "2305.14722": {"original_text": "  Most contemporary supervised Remote Sensing (RS) image Change Detection (CD)\napproaches are customized for equal-resolution bitemporal images. Real-world\napplications raise the need for cross-resolution change detection, aka, CD\nbased on bitemporal images with different spatial resolutions. Given training\nsamples of a fixed bitemporal resolution difference (ratio) between the\nhigh-resolution (HR) image and the low-resolution (LR) one, current\ncross-resolution methods may fit a certain ratio but lack adaptation to other\nresolution differences. Toward continuous cross-resolution CD, we propose\nscale-invariant learning to enforce the model consistently predicting HR\nresults given synthesized samples of varying resolution differences.\nConcretely, we synthesize blurred versions of the HR image by random\ndownsampled reconstructions to reduce the gap between HR and LR images. We\nintroduce coordinate-based representations to decode per-pixel predictions by\nfeeding the coordinate query and corresponding multi-level embedding features\ninto an MLP that implicitly learns the shape of land cover changes, therefore\nbenefiting recognizing blurred objects in the LR image. Moreover, considering\nthat spatial resolution mainly affects the local textures, we apply\nlocal-window self-attention to align bitemporal features during the early\nstages of the encoder. Extensive experiments on two synthesized and one\nreal-world different-resolution CD datasets verify the effectiveness of the\nproposed method. Our method significantly outperforms several vanilla CD\nmethods and two cross-resolution CD methods on the three datasets both in\nin-distribution and out-of-distribution settings. The empirical results suggest\nthat our method could yield relatively consistent HR change predictions\nregardless of varying bitemporal resolution ratios. Our code is available at\n\\url{https://github.com/justchenhao/SILI_CD}.\n", "rewritten_text": "Most contemporary supervised Remote Sensing (RS) image Change Detection (CD) approaches are tailored for bitemporal images with equal resolutions. However, real-world applications necessitate cross-resolution change detection, which involves analyzing bitemporal images with different spatial resolutions. Existing cross-resolution methods are typically designed to handle a specific resolution ratio between the high-resolution (HR) and low-resolution (LR) images, lacking adaptability to other resolution differences.\n\nTo address the need for continuous cross-resolution CD, we propose scale-invariant learning. This approach aims to ensure that the model consistently predicts HR results by training on synthesized samples with varying resolution differences. Specifically, we generate blurred versions of the HR image through random downsampling reconstructions to bridge the gap between HR and LR images. Additionally, we introduce coordinate-based representations to facilitate per-pixel predictions by feeding coordinate queries and corresponding multi-level embedding features into a Multi-Layer Perceptron (MLP). This MLP implicitly learns the shape of land cover changes, aiding in the recognition of blurred objects in the LR image.\n\nRecognizing that spatial resolution primarily impacts local textures, we incorporate local-window self-attention to align bitemporal features in the early stages of the encoder. Extensive experiments conducted on two synthesized and one real-world dataset with varying resolutions validate the effectiveness of our proposed method. Our approach significantly outperforms several conventional CD methods and two cross-resolution CD methods across all datasets, demonstrating superior performance in both in-distribution and out-of-distribution scenarios.\n\nThe empirical results indicate that our method consistently delivers accurate HR change predictions irrespective of varying bitemporal resolution ratios. For those interested, our code is accessible at \\url{https://github.com/justchenhao/SILI_CD}."}, "1704.08763": {"original_text": "  We present GazeDirector, a new approach for eye gaze redirection that uses\nmodel-fitting. Our method first tracks the eyes by fitting a multi-part eye\nregion model to video frames using analysis-by-synthesis, thereby recovering\neye region shape, texture, pose, and gaze simultaneously. It then redirects\ngaze by 1) warping the eyelids from the original image using a model-derived\nflow field, and 2) rendering and compositing synthesized 3D eyeballs onto the\noutput image in a photorealistic manner. GazeDirector allows us to change where\npeople are looking without person-specific training data, and with full\narticulation, i.e. we can precisely specify new gaze directions in 3D.\nQuantitatively, we evaluate both model-fitting and gaze synthesis, with\nexperiments for gaze estimation and redirection on the Columbia gaze dataset.\nQualitatively, we compare GazeDirector against recent work on gaze redirection,\nshowing better results especially for large redirection angles. Finally, we\ndemonstrate gaze redirection on YouTube videos by introducing new 3D gaze\ntargets and by manipulating visual behavior.\n", "rewritten_text": "Introducing GazeDirector, a novel approach for redirecting eye gaze through model-fitting. Our method begins by tracking the eyes, fitting a multi-part eye region model to video frames using analysis-by-synthesis. This process allows for the simultaneous recovery of eye region shape, texture, pose, and gaze. Subsequently, gaze redirection is achieved by: 1) warping the eyelids from the original image using a model-derived flow field, and 2) rendering and compositing synthesized 3D eyeballs onto the output image in a photorealistic manner. GazeDirector enables the alteration of individuals' gaze direction without the need for person-specific training data, offering full articulation to precisely specify new gaze directions in 3D. \n\nQuantitatively, we assess both model-fitting and gaze synthesis through experiments on gaze estimation and redirection using the Columbia gaze dataset. In a qualitative comparison against recent work on gaze redirection, GazeDirector demonstrates superior results, particularly for large redirection angles. Furthermore, we showcase the application of gaze redirection on YouTube videos by introducing new 3D gaze targets and manipulating visual behavior."}, "2110.13385": {"original_text": "  Recently, Transformer-based networks have shown great promise on\nskeleton-based action recognition tasks. The ability to capture global and\nlocal dependencies is the key to success while it also brings quadratic\ncomputation and memory cost. Another problem is that previous studies mainly\nfocus on the relationships among individual joints, which often suffers from\nthe noisy skeleton joints introduced by the noisy inputs of sensors or\ninaccurate estimations. To address the above issues, we propose a novel\nTransformer-based network (IIP-Transformer). Instead of exploiting interactions\namong individual joints, our IIP-Transformer incorporates body joints and parts\ninteractions simultaneously and thus can capture both joint-level (intra-part)\nand part-level (inter-part) dependencies efficiently and effectively. From the\ndata aspect, we introduce a part-level skeleton data encoding that\nsignificantly reduces the computational complexity and is more robust to\njoint-level skeleton noise. Besides, a new part-level data augmentation is\nproposed to improve the performance of the model. On two large-scale datasets,\nNTU-RGB+D 60 and NTU RGB+D 120, the proposed IIP-Transformer achieves\nthe-state-of-art performance with more than 8x less computational complexity\nthan DSTA-Net, which is the SOTA Transformer-based method.\n", "rewritten_text": "Recently, Transformer-based networks have demonstrated significant potential in skeleton-based action recognition tasks. The key to their success lies in their ability to capture both global and local dependencies, albeit at the cost of quadratic computation and memory usage. A notable challenge is that prior research has primarily focused on the relationships among individual joints, which often suffer from noisy inputs generated by sensors or inaccurate estimations.\n\nTo address these challenges, we introduce a novel Transformer-based network called IIP-Transformer. Unlike previous approaches that emphasize interactions among individual joints, our IIP-Transformer integrates interactions among body joints and parts concurrently. This design enables the model to efficiently and effectively capture dependencies at both the joint-level (intra-part) and part-level (inter-part).\n\nFrom a data perspective, we propose a part-level skeleton data encoding method that significantly reduces computational complexity and enhances robustness against joint-level skeleton noise. Additionally, we introduce a novel part-level data augmentation technique to enhance the model's performance. On two large-scale datasets, NTU-RGB+D 60 and NTU RGB+D 120, our proposed IIP-Transformer achieves state-of-the-art performance with over 8 times less computational complexity compared to DSTA-Net, the current state-of-the-art Transformer-based method."}, "2307.01200": {"original_text": "  Learning-based approaches to monocular motion capture have recently shown\npromising results by learning to regress in a data-driven manner. However, due\nto the challenges in data collection and network designs, it remains\nchallenging for existing solutions to achieve real-time full-body capture while\nbeing accurate in world space. In this work, we introduce ProxyCap, a\nhuman-centric proxy-to-motion learning scheme to learn world-space motions from\na proxy dataset of 2D skeleton sequences and 3D rotational motions. Such proxy\ndata enables us to build a learning-based network with accurate world-space\nsupervision while also mitigating the generalization issues. For more accurate\nand physically plausible predictions in world space, our network is designed to\nlearn human motions from a human-centric perspective, which enables the\nunderstanding of the same motion captured with different camera trajectories.\nMoreover, a contact-aware neural motion descent module is proposed in our\nnetwork so that it can be aware of foot-ground contact and motion misalignment\nwith the proxy observations. With the proposed learning-based solution, we\ndemonstrate the first real-time monocular full-body capture system with\nplausible foot-ground contact in world space even using hand-held moving\ncameras. Our project page is https://zhangyux15.github.io/ProxyCapV2.\n", "rewritten_text": "Recently, learning-based approaches to monocular motion capture have shown promising results by utilizing data-driven regression techniques. However, challenges in data collection and network designs have made it difficult for existing solutions to achieve real-time full-body capture with high accuracy in world space. In this study, we present ProxyCap, a human-centric proxy-to-motion learning scheme that learns world-space motions from a proxy dataset containing 2D skeleton sequences and 3D rotational motions. This proxy data allows us to construct a learning-based network with precise world-space supervision, addressing generalization issues.\n\nTo enhance the accuracy and physical realism of predictions in world space, our network is designed to learn human motions from a human-centric perspective. This approach enables the network to comprehend the same motion captured from various camera trajectories. Additionally, we introduce a contact-aware neural motion descent module in our network to account for foot-ground contact and motion misalignment based on proxy observations.\n\nThrough our proposed learning-based solution, we have developed the first real-time monocular full-body capture system with realistic foot-ground contact in world space, even when using hand-held moving cameras. For more information, please visit our project page at https://zhangyux15.github.io/ProxyCapV2."}, "1410.7484": {"original_text": "  Stochastic sampling based trackers have shown good performance for abrupt\nmotion tracking so that they have gained popularity in recent years. However,\nconventional methods tend to use a two-stage sampling paradigm, in which the\nsearch space needs to be uniformly explored with an inefficient preliminary\nsampling phase. In this paper, we propose a novel sampling-based method in the\nBayesian filtering framework to address the problem. Within the framework,\nnearest neighbor field estimation is utilized to compute the importance\nproposal probabilities, which guide the Markov chain search towards promising\nregions and thus enhance the sampling efficiency; given the motion priors, a\nsmoothing stochastic sampling Monte Carlo algorithm is proposed to approximate\nthe posterior distribution through a smoothing weight-updating scheme.\nMoreover, to track the abrupt and the smooth motions simultaneously, we develop\nan abrupt-motion detection scheme which can discover the presence of abrupt\nmotions during online tracking. Extensive experiments on challenging image\nsequences demonstrate the effectiveness and the robustness of our algorithm in\nhandling the abrupt motions.\n", "rewritten_text": "Stochastic sampling-based trackers have demonstrated strong performance in tracking abrupt motion, leading to increased popularity in recent years. However, traditional methods typically employ a two-stage sampling approach, requiring uniform exploration of the search space through an inefficient preliminary sampling phase. This paper introduces a novel sampling-based method within the Bayesian filtering framework to address this issue. \n\nWithin this framework, nearest neighbor field estimation is used to calculate importance proposal probabilities, directing the Markov chain search towards promising regions and improving sampling efficiency. Leveraging motion priors, a smoothing stochastic sampling Monte Carlo algorithm is proposed to approximate the posterior distribution via a smoothing weight-updating scheme. \n\nFurthermore, to simultaneously track abrupt and smooth motions, an abrupt-motion detection scheme is developed to identify abrupt motions during online tracking. Extensive experiments on challenging image sequences showcase the effectiveness and robustness of our algorithm in handling abrupt motions."}, "2010.06944": {"original_text": "  We present a formulation of the relative depth estimation from a single image\nproblem, as a ranking problem. By reformulating the problem this way, we were\nable to utilize literature on the ranking problem, and apply the existing\nknowledge to achieve better results. To this end, we have introduced a listwise\nranking loss borrowed from ranking literature, weighted ListMLE, to the\nrelative depth estimation problem. We have also brought a new metric which\nconsiders pixel depth ranking accuracy, on which our method is stronger.\n", "rewritten_text": "We introduce a formulation of the relative depth estimation problem from a single image as a ranking problem. By reframing the problem in this manner, we leveraged existing literature on ranking problems to improve our results. In pursuit of this goal, we introduced a listwise ranking loss, specifically Weighted ListMLE, borrowed from ranking literature, to address the relative depth estimation problem. Additionally, we developed a new metric that evaluates pixel depth ranking accuracy, in which our method excels."}, "2012.02951": {"original_text": "  Visual scene understanding is the core task in making any crucial decision in\nany computer vision system. Although popular computer vision datasets like\nCityscapes, MS-COCO, PASCAL provide good benchmarks for several tasks (e.g.\nimage classification, segmentation, object detection), these datasets are\nhardly suitable for post disaster damage assessments. On the other hand,\nexisting natural disaster datasets include mainly satellite imagery which have\nlow spatial resolution and a high revisit period. Therefore, they do not have a\nscope to provide quick and efficient damage assessment tasks. Unmanned Aerial\nVehicle(UAV) can effortlessly access difficult places during any disaster and\ncollect high resolution imagery that is required for aforementioned tasks of\ncomputer vision. To address these issues we present a high resolution UAV\nimagery, FloodNet, captured after the hurricane Harvey. This dataset\ndemonstrates the post flooded damages of the affected areas. The images are\nlabeled pixel-wise for semantic segmentation task and questions are produced\nfor the task of visual question answering. FloodNet poses several challenges\nincluding detection of flooded roads and buildings and distinguishing between\nnatural water and flooded water. With the advancement of deep learning\nalgorithms, we can analyze the impact of any disaster which can make a precise\nunderstanding of the affected areas. In this paper, we compare and contrast the\nperformances of baseline methods for image classification, semantic\nsegmentation, and visual question answering on our dataset.\n", "rewritten_text": "Visual scene understanding is a fundamental task in decision-making within computer vision systems. While widely-used computer vision datasets such as Cityscapes, MS-COCO, and PASCAL offer robust benchmarks for various tasks like image classification, segmentation, and object detection, they are not well-suited for assessing post-disaster damage. Conversely, current natural disaster datasets primarily consist of satellite imagery with limited spatial resolution and infrequent revisits, thus lacking the capability to efficiently conduct damage assessments. Unmanned Aerial Vehicles (UAVs) can access challenging disaster areas and capture high-resolution imagery essential for computer vision tasks. To tackle these challenges, we introduce FloodNet, a high-resolution UAV imagery dataset captured post-hurricane Harvey, showcasing flood-related damages in affected regions. The dataset includes pixel-wise labels for semantic segmentation and generates questions for visual question answering tasks. FloodNet presents various challenges, such as identifying flooded roads and buildings and distinguishing between natural water bodies and flooded areas. Leveraging deep learning algorithms enables precise analysis of disaster impacts and a comprehensive understanding of affected regions. This study compares and contrasts the performance of baseline methods for image classification, semantic segmentation, and visual question answering on our FloodNet dataset."}, "1905.1062": {"original_text": "  Large-scale face recognition in-the-wild has been recently achieved matured\nperformance in many real work applications. However, such systems are built on\nGPU platforms and mostly deploy heavy deep network architectures. Given a\nhigh-performance heavy network as a teacher, this work presents a simple and\nelegant teacher-student learning paradigm, namely ShrinkTeaNet, to train a\nportable student network that has significantly fewer parameters and\ncompetitive accuracy against the teacher network. Far apart from prior\nteacher-student frameworks mainly focusing on accuracy and compression ratios\nin closed-set problems, our proposed teacher-student network is proved to be\nmore robust against open-set problem, i.e. large-scale face recognition. In\naddition, this work introduces a novel Angular Distillation Loss for distilling\nthe feature direction and the sample distributions of the teacher's hypersphere\nto its student. Then ShrinkTeaNet framework can efficiently guide the student's\nlearning process with the teacher's knowledge presented in both intermediate\nand last stages of the feature embedding. Evaluations on LFW, CFP-FP, AgeDB,\nIJB-B and IJB-C Janus, and MegaFace with one million distractors have\ndemonstrated the efficiency of the proposed approach to learn robust student\nnetworks which have satisfying accuracy and compact sizes. Our ShrinkTeaNet is\nable to support the light-weight architecture achieving high performance with\n99.77% on LFW and 95.64% on large-scale Megaface protocols.\n", "rewritten_text": "Large-scale face recognition in the wild has recently achieved mature performance in many real-world applications. However, these systems are typically built on GPU platforms and predominantly utilize complex deep network architectures. This study introduces a straightforward and refined teacher-student learning paradigm, known as ShrinkTeaNet, to train a compact student network with significantly fewer parameters while maintaining competitive accuracy compared to the teacher network. Unlike previous teacher-student frameworks that primarily focus on accuracy and compression ratios in closed-set problems, our proposed teacher-student network demonstrates enhanced robustness against open-set problems, specifically in large-scale face recognition scenarios.\n\nMoreover, this research introduces a novel Angular Distillation Loss for distilling the feature direction and sample distributions of the teacher's hypersphere to its student. The ShrinkTeaNet framework effectively guides the student's learning process by leveraging the teacher's knowledge at both intermediate and final stages of feature embedding. Evaluations conducted on various datasets, including LFW, CFP-FP, AgeDB, IJB-B, IJB-C Janus, and MegaFace with one million distractors, have showcased the efficiency of our approach in training resilient student networks that exhibit satisfactory accuracy and compact sizes.\n\nShrinkTeaNet supports a lightweight architecture that achieves high performance, achieving 99.77% accuracy on LFW and 95.64% on large-scale MegaFace protocols."}, "1601.0763": {"original_text": "  We propose a method that integrates two widely available data sources,\nbuilding footprints from 2D maps and street level images, to derive valuable\ninformation that is generally difficult to acquire -- building heights and\nbuilding facade masks in images. Building footprints are elevated in world\ncoordinates and projected onto images. Building heights are estimated by\nscoring projected footprints based on their alignment with building features in\nimages. Building footprints with estimated heights can be converted to simple\n3D building models, which are projected back to images to identify buildings.\nIn this procedure, accurate camera projections are critical. However, camera\nposition errors inherited from external sensors commonly exist, which adversely\naffect results. We derive a solution to precisely locate cameras on maps using\ncorrespondence between image features and building footprints. Experiments on\nreal-world datasets show the promise of our method.\n", "rewritten_text": "We propose a method that integrates two widely available data sources: building footprints from 2D maps and street-level images. This integration allows us to derive valuable information that is typically challenging to obtain, such as building heights and building facade masks in images. The process involves elevating building footprints in world coordinates and projecting them onto images. Building heights are then estimated by evaluating the alignment of projected footprints with building features in the images. Subsequently, building footprints with estimated heights can be transformed into simple 3D building models, which are then projected back onto the images to identify buildings.\n\nAccurate camera projections are crucial in this procedure. However, camera position errors from external sensors are common and can negatively impact the results. To address this issue, we have developed a solution to precisely locate cameras on maps by establishing correspondence between image features and building footprints. Our experiments conducted on real-world datasets demonstrate the potential effectiveness of our method."}, "2112.12141": {"original_text": "  3D human pose estimation (HPE) in autonomous vehicles (AV) differs from other\nuse cases in many factors, including the 3D resolution and range of data,\nabsence of dense depth maps, failure modes for LiDAR, relative location between\nthe camera and LiDAR, and a high bar for estimation accuracy. Data collected\nfor other use cases (such as virtual reality, gaming, and animation) may\ntherefore not be usable for AV applications. This necessitates the collection\nand annotation of a large amount of 3D data for HPE in AV, which is\ntime-consuming and expensive. In this paper, we propose one of the first\napproaches to alleviate this problem in the AV setting. Specifically, we\npropose a multi-modal approach which uses 2D labels on RGB images as weak\nsupervision to perform 3D HPE. The proposed multi-modal architecture\nincorporates LiDAR and camera inputs with an auxiliary segmentation branch. On\nthe Waymo Open Dataset, our approach achieves a 22% relative improvement over\ncamera-only 2D HPE baseline, and 6% improvement over LiDAR-only model. Finally,\ncareful ablation studies and parts based analysis illustrate the advantages of\neach of our contributions.\n", "rewritten_text": "3D human pose estimation (HPE) in autonomous vehicles (AV) presents unique challenges compared to other applications due to various factors, such as differences in 3D resolution and data range, the absence of dense depth maps, failure modes specific to LiDAR, the relative positioning of the camera and LiDAR, and the high accuracy requirements for estimation. Data gathered for other purposes like virtual reality, gaming, and animation may not be suitable for AV applications. Consequently, there is a need to acquire and annotate a significant volume of 3D data for HPE in AV, a process that is both time-consuming and costly.\n\nIn this study, we introduce one of the initial strategies to address this challenge within the AV context. Specifically, we propose a multi-modal approach that leverages 2D labels on RGB images as weak supervision for conducting 3D HPE. Our proposed multi-modal architecture integrates inputs from LiDAR and the camera, along with an auxiliary segmentation branch. When tested on the Waymo Open Dataset, our approach demonstrates a 22% relative enhancement over a baseline model that relies solely on 2D HPE from the camera and a 6% improvement over a model using only LiDAR data.\n\nFurthermore, through meticulous ablation studies and a parts-based analysis, we highlight the distinct advantages of each component of our methodology."}, "2306.02182": {"original_text": "  Indian court legal texts and processes are essential towards the integrity of\nthe judicial system and towards maintaining the social and political order of\nthe nation. Due to the increase in number of pending court cases, there is an\nurgent need to develop tools to automate many of the legal processes with the\nknowledge of artificial intelligence. In this paper, we employ knowledge\nextraction techniques, specially the named entity extraction of legal entities\nwithin court case judgements. We evaluate several state of the art\narchitectures in the realm of sequence labeling using models trained on a\ncurated dataset of legal texts. We observe that a Bi-LSTM model trained on\nFlair Embeddings achieves the best results, and we also publish the BIO\nformatted dataset as part of this paper.\n", "rewritten_text": "Indian court legal texts and processes play a crucial role in upholding the integrity of the judicial system and maintaining the social and political order of the nation. With the rising number of pending court cases, there is an urgent necessity to develop tools that can automate many legal processes using artificial intelligence. This paper focuses on employing knowledge extraction techniques, specifically named entity extraction of legal entities within court case judgments. We assess various state-of-the-art architectures in the field of sequence labeling by utilizing models trained on a carefully curated dataset of legal texts. Our findings indicate that a Bi-LSTM model trained on Flair Embeddings yields the most favorable results. Additionally, we are sharing the BIO formatted dataset as a part of this paper."}, "2011.09563": {"original_text": "  Unsupervised domain adaptation (UDA) is widely used to transfer knowledge\nfrom a labeled source domain to an unlabeled target domain with different data\ndistribution. While extensive studies attested that deep learning models are\nvulnerable to adversarial attacks, the adversarial robustness of models in\ndomain adaptation application has largely been overlooked. This paper points\nout that the inevitable domain distribution deviation in UDA is a critical\nbarrier to model robustness on the target domain. To address the problem, we\npropose a novel Class-consistent Unsupervised Robust Domain Adaptation (CURDA)\nframework for training robust UDA models. With the introduced contrastive\nrobust training and source anchored adversarial contrastive losses, our\nproposed CURDA framework can effectively robustify UDA models by simultaneously\nminimizing the data distribution deviation and the distance between target\ndomain clean-adversarial pairs without creating classification confusion.\nExperiments on several public benchmarks show that CURDA can significantly\nimprove model robustness in the target domain with only minor cost of accuracy\non the clean samples.\n", "rewritten_text": "Unsupervised domain adaptation (UDA) is a widely used technique for transferring knowledge from a labeled source domain to an unlabeled target domain with a different data distribution. While numerous studies have shown that deep learning models are susceptible to adversarial attacks, the adversarial robustness of models in domain adaptation applications has largely been overlooked. This paper highlights that the inevitable deviation in domain distribution in UDA poses a significant barrier to model robustness on the target domain. To tackle this issue, we introduce a novel framework called Class-consistent Unsupervised Robust Domain Adaptation (CURDA) for training robust UDA models. By incorporating contrastive robust training and source-anchored adversarial contrastive losses, our proposed CURDA framework effectively enhances the robustness of UDA models by simultaneously reducing the data distribution deviation and the distance between target domain clean-adversarial pairs without causing classification confusion. Experimental results on various public benchmarks demonstrate that CURDA can substantially enhance model robustness in the target domain with only a minimal decrease in accuracy on clean samples."}, "1907.00184": {"original_text": "  Since Bahdanau et al. [1] first introduced attention for neural machine\ntranslation, most sequence-to-sequence models made use of attention mechanisms\n[2, 3, 4]. While they produce soft-alignment matrices that could be interpreted\nas alignment between target and source languages, we lack metrics to quantify\ntheir quality, being unclear which approach produces the best alignments. This\npaper presents an empirical evaluation of 3 main sequence-to-sequence models\n(CNN, RNN and Transformer-based) for word discovery from unsegmented phoneme\nsequences. This task consists in aligning word sequences in a source language\nwith phoneme sequences in a target language, inferring from it word\nsegmentation on the target side [5]. Evaluating word segmentation quality can\nbe seen as an extrinsic evaluation of the soft-alignment matrices produced\nduring training. Our experiments in a low-resource scenario on Mboshi and\nEnglish languages (both aligned to French) show that RNNs surprisingly\noutperform CNNs and Transformer for this task. Our results are confirmed by an\nintrinsic evaluation of alignment quality through the use of Average Normalized\nEntropy (ANE). Lastly, we improve our best word discovery model by using an\nalignment entropy confidence measure that accumulates ANE over all the\noccurrences of a given alignment pair in the collection.\n", "rewritten_text": "Since Bahdanau et al. (2014) first introduced attention mechanisms for neural machine translation, the majority of sequence-to-sequence models have incorporated attention mechanisms. These models generate soft-alignment matrices that can be interpreted as alignments between target and source languages. However, there is a lack of metrics to quantify the quality of these alignments, making it unclear which approach yields the most optimal alignments. This study conducts an empirical evaluation of three primary sequence-to-sequence models (CNN, RNN, and Transformer-based) for word discovery from unsegmented phoneme sequences. The objective of this task is to align word sequences in a source language with phoneme sequences in a target language, thereby deducing word segmentation on the target side. Assessing the quality of word segmentation serves as an extrinsic evaluation of the soft-alignment matrices generated during training. Our experiments, conducted in a low-resource setting using Mboshi and English languages (both aligned to French), reveal that RNNs surprisingly outperform CNNs and Transformers for this task. These findings are further supported by an intrinsic evaluation of alignment quality using the Average Normalized Entropy (ANE) metric. Finally, we enhance our top-performing word discovery model by incorporating an alignment entropy confidence measure that aggregates ANE values across all instances of a specific alignment pair in the dataset."}, "1812.09877": {"original_text": "  In multimodal unsupervised image-to-image translation tasks, the goal is to\ntranslate an image from the source domain to many images in the target domain.\nWe present a simple method that produces higher quality images than current\nstate-of-the-art while maintaining the same amount of multimodal diversity.\nPrevious methods follow the unconditional approach of trying to map the latent\ncode directly to a full-size image. This leads to complicated network\narchitectures with several introduced hyperparameters to tune. By treating the\nlatent code as a modifier of the convolutional filters, we produce multimodal\noutput while maintaining the traditional Generative Adversarial Network (GAN)\nloss and without additional hyperparameters. The only tuning required by our\nmethod controls the tradeoff between variability and quality of generated\nimages. Furthermore, we achieve disentanglement between source domain content\nand target domain style for free as a by-product of our formulation. We perform\nqualitative and quantitative experiments showing the advantages of our method\ncompared with the state-of-the art on multiple benchmark image-to-image\ntranslation datasets.\n", "rewritten_text": "In tasks involving multimodal unsupervised image-to-image translation, the objective is to convert an image from the source domain into multiple images within the target domain. We introduce a straightforward method that generates higher-quality images compared to current state-of-the-art techniques, while preserving the same level of multimodal diversity. Previous approaches have typically taken an unconditional route by attempting to directly map the latent code to a full-size image. This often results in complex network architectures with numerous hyperparameters that need fine-tuning. Our method, on the other hand, treats the latent code as a modifier of the convolutional filters, enabling the production of multimodal outputs without the need for additional hyperparameters, all while adhering to the conventional Generative Adversarial Network (GAN) loss. The only adjustment required by our method involves balancing the tradeoff between the variability and quality of the generated images. Moreover, our formulation naturally achieves disentanglement between the content of the source domain and the style of the target domain, as a beneficial by-product. Through qualitative and quantitative experiments, we demonstrate the advantages of our approach over the current state-of-the-art on various benchmark image-to-image translation datasets."}, "1807.04047": {"original_text": "  Document image quality assessment (DIQA) is an important and challenging\nproblem in real applications. In order to predict the quality scores of\ndocument images, this paper proposes a novel no-reference DIQA method based on\ncharacter gradient, where the OCR accuracy is used as a ground-truth quality\nmetric. Character gradient is computed on character patches detected with the\nmaximally stable extremal regions (MSER) based method. Character patches are\nessentially significant to character recognition and therefore suitable for use\nin estimating document image quality. Experiments on a benchmark dataset show\nthat the proposed method outperforms the state-of-the-art methods in estimating\nthe quality score of document images.\n", "rewritten_text": "Document image quality assessment (DIQA) presents a significant and challenging issue in practical applications. This paper introduces a novel no-reference DIQA method that relies on character gradient to predict the quality scores of document images. The method utilizes OCR accuracy as a benchmark quality metric. Character gradient is calculated on character patches identified using the maximally stable extremal regions (MSER) method. Character patches play a crucial role in character recognition and are therefore well-suited for estimating document image quality. Experimental results on a benchmark dataset demonstrate that the proposed method surpasses state-of-the-art techniques in accurately estimating the quality score of document images."}, "2406.01171": {"original_text": "  The concept of persona, originally adopted in dialogue literature, has\nre-surged as a promising framework for tailoring large language models (LLMs)\nto specific context (e.g., personalized search, LLM-as-a-judge). However, the\ngrowing research on leveraging persona in LLMs is relatively disorganized and\nlacks a systematic taxonomy. To close the gap, we present a comprehensive\nsurvey to categorize the current state of the field. We identify two lines of\nresearch, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and\n(2) LLM Personalization, where LLMs take care of user personas. Additionally,\nwe introduce existing methods for LLM personality evaluation. To the best of\nour knowledge, we present the first survey for role-playing and personalization\nin LLMs under the unified view of persona. We continuously maintain a paper\ncollection to foster future endeavors:\nhttps://github.com/MiuLab/PersonaLLM-Survey\n", "rewritten_text": "The concept of persona, originally utilized in dialogic literature, has experienced a resurgence as a promising framework for customizing large language models (LLMs) to specific contexts, such as personalized search and LLM-as-a-judge scenarios. However, the expanding research on incorporating persona into LLMs lacks organization and a systematic taxonomy. To address this gap, we have conducted a comprehensive survey to categorize the current state of the field. Our survey identifies two main lines of research: (1) LLM Role-Playing, involving assigning personas to LLMs, and (2) LLM Personalization, where LLMs cater to user personas. Furthermore, we introduce existing methods for evaluating personality in LLMs. To the best of our knowledge, this survey is the first to explore role-playing and personalization in LLMs from a unified persona perspective. We are committed to maintaining a collection of relevant papers to support future research efforts: https://github.com/MiuLab/PersonaLLM-Survey"}, "1804.02745": {"original_text": "  Dynamic contrast-enhanced (DCE) MRI is an evolving imaging technique that\nprovides a quantitative measure of pharmacokinetic (PK) parameters in body\ntissues, in which series of T1-weighted images are collected following the\nadministration of a paramagnetic contrast agent. Unfortunately, in many\napplications, conventional clinical DCE-MRI suffers from low spatiotemporal\nresolution and insufficient volume coverage. In this paper, we propose a novel\ndeep learning based approach to directly estimate the PK parameters from\nundersampled DCE-MRI data. Specifically, we design a custom loss function where\nwe incorporate a forward physical model that relates the PK parameters to\ncorrupted image-time series obtained due to subsampling in k-space. This allows\nthe network to directly exploit the knowledge of true contrast agent kinetics\nin the training phase, and hence provide more accurate restoration of PK\nparameters. Experiments on clinical brain DCE datasets demonstrate the efficacy\nof our approach in terms of fidelity of PK parameter reconstruction and\nsignificantly faster parameter inference compared to a model-based iterative\nreconstruction method.\n", "rewritten_text": "Dynamic contrast-enhanced (DCE) MRI is an advancing imaging technique that quantitatively measures pharmacokinetic (PK) parameters in body tissues. This involves collecting a series of T1-weighted images after administering a paramagnetic contrast agent. However, conventional clinical DCE-MRI often suffers from low spatiotemporal resolution and limited volume coverage in many applications.\n\nIn this study, we introduce a novel deep learning-based approach to directly estimate PK parameters from undersampled DCE-MRI data. Our method involves designing a custom loss function that incorporates a forward physical model linking PK parameters to corrupted image-time series resulting from subsampling in k-space. By integrating this physical model into the training phase, the network can leverage knowledge of true contrast agent kinetics, leading to more accurate restoration of PK parameters.\n\nExperiments conducted on clinical brain DCE datasets showcase the effectiveness of our approach in terms of the faithfulness of PK parameter reconstruction and the significantly faster parameter inference compared to a model-based iterative reconstruction method."}, "2207.01164": {"original_text": "  Neural Radiance Field (NeRF) regresses a neural parameterized scene by\ndifferentially rendering multi-view images with ground-truth supervision.\nHowever, when interpolating novel views, NeRF often yields inconsistent and\nvisually non-smooth geometric results, which we consider as a generalization\ngap between seen and unseen views. Recent advances in convolutional neural\nnetworks have demonstrated the promise of advanced robust data augmentations,\neither random or learned, in enhancing both in-distribution and\nout-of-distribution generalization. Inspired by that, we propose Augmented NeRF\n(Aug-NeRF), which for the first time brings the power of robust data\naugmentations into regularizing the NeRF training. Particularly, our proposal\nlearns to seamlessly blend worst-case perturbations into three distinct levels\nof the NeRF pipeline with physical grounds, including (1) the input\ncoordinates, to simulate imprecise camera parameters at image capture; (2)\nintermediate features, to smoothen the intrinsic feature manifold; and (3)\npre-rendering output, to account for the potential degradation factors in the\nmulti-view image supervision. Extensive results demonstrate that Aug-NeRF\neffectively boosts NeRF performance in both novel view synthesis (up to 1.5dB\nPSNR gain) and underlying geometry reconstruction. Furthermore, thanks to the\nimplicit smooth prior injected by the triple-level augmentations, Aug-NeRF can\neven recover scenes from heavily corrupted images, a highly challenging setting\nuntackled before. Our codes are available in\nhttps://github.com/VITA-Group/Aug-NeRF.\n", "rewritten_text": "The Neural Radiance Field (NeRF) technique involves regressing a neural parameterized scene by differentially rendering multi-view images under ground-truth supervision. However, NeRF often produces inconsistent and visually non-smooth geometric results when interpolating novel views, indicating a generalization gap between seen and unseen views. Recent advancements in convolutional neural networks have shown the potential of robust data augmentations, whether random or learned, in improving generalization within and outside the training distribution.\n\nMotivated by these advancements, we introduce Augmented NeRF (Aug-NeRF), which integrates robust data augmentations into the NeRF training process for the first time. Our approach focuses on seamlessly incorporating worst-case perturbations at three distinct levels of the NeRF pipeline, grounded in physical principles. These levels include adjusting (1) input coordinates to simulate imprecise camera parameters during image capture, (2) intermediate features to enhance the smoothness of the feature manifold, and (3) pre-rendering output to address potential degradation factors in multi-view image supervision.\n\nExtensive experimental results demonstrate that Aug-NeRF significantly enhances NeRF performance in novel view synthesis (achieving up to a 1.5dB PSNR gain) and improves the accuracy of underlying geometry reconstruction. Moreover, due to the implicit smooth prior introduced by the triple-level augmentations, Aug-NeRF exhibits the capability to reconstruct scenes from heavily corrupted images, a challenging scenario that has not been addressed previously. Our code is publicly available at https://github.com/VITA-Group/Aug-NeRF."}, "1511.05296": {"original_text": "  In this paper, we propose a method for ranking fashion images to find the\nones which might be liked by more people. We collect two new datasets from\nimage sharing websites (Pinterest and Polyvore). We represent fashion images\nbased on attributes: semantic attributes and data-driven attributes. To learn\nsemantic attributes from limited training data, we use an algorithm on\nmulti-task convolutional neural networks to share visual knowledge among\ndifferent semantic attribute categories. To discover data-driven attributes\nunsupervisedly, we propose an algorithm to simultaneously discover visual\nclusters and learn fashion-specific feature representations. Given attributes\nas representations, we propose to learn a ranking SPN (sum product networks) to\nrank pairs of fashion images. The proposed ranking SPN can capture the\nhigh-order correlations of the attributes. We show the effectiveness of our\nmethod on our two newly collected datasets.\n", "rewritten_text": "This paper introduces a method for ranking fashion images to identify those likely to be favored by a larger audience. Two new datasets are gathered from image sharing platforms (Pinterest and Polyvore). Fashion images are characterized based on semantic attributes and data-driven attributes. To address the challenge of learning semantic attributes from limited training data, a multi-task convolutional neural network algorithm is employed to share visual knowledge across various semantic attribute categories. For the unsupervised discovery of data-driven attributes, an algorithm is proposed to simultaneously identify visual clusters and acquire fashion-specific feature representations. Utilizing attributes as representations, a ranking Sum Product Network (SPN) is trained to rank pairs of fashion images. The proposed ranking SPN is designed to capture high-order correlations among attributes. The effectiveness of the method is demonstrated on the newly collected datasets."}, "1901.00049": {"original_text": "  We introduce a new silhouette-based representation for modeling clothed human\nbodies using deep generative models. Our method can reconstruct a complete and\ntextured 3D model of a person wearing clothes from a single input picture.\nInspired by the visual hull algorithm, our implicit representation uses 2D\nsilhouettes and 3D joints of a body pose to describe the immense shape\ncomplexity and variations of clothed people. Given a segmented 2D silhouette of\na person and its inferred 3D joints from the input picture, we first synthesize\nconsistent silhouettes from novel view points around the subject. The\nsynthesized silhouettes which are the most consistent with the input\nsegmentation are fed into a deep visual hull algorithm for robust 3D shape\nprediction. We then infer the texture of the subject's back view using the\nfrontal image and segmentation mask as input to a conditional generative\nadversarial network. Our experiments demonstrate that our silhouette-based\nmodel is an effective representation and the appearance of the back view can be\npredicted reliably using an image-to-image translation network. While classic\nmethods based on parametric models often fail for single-view images of\nsubjects with challenging clothing, our approach can still produce successful\nresults, which are comparable to those obtained from multi-view input.\n", "rewritten_text": "We present a novel silhouette-based representation for modeling clothed human bodies using deep generative models. Our method is capable of reconstructing a complete and textured 3D model of a person wearing clothes from a single input picture. Drawing inspiration from the visual hull algorithm, our implicit representation leverages 2D silhouettes and 3D joints of a body pose to effectively capture the intricate shape complexity and variations of clothed individuals.\n\nGiven a segmented 2D silhouette of a person and the inferred 3D joints from the input picture, our approach initially synthesizes consistent silhouettes from various viewpoints around the subject. These synthesized silhouettes, which best match the input segmentation, are then inputted into a deep visual hull algorithm for robust 3D shape prediction. Subsequently, we predict the texture of the subject's back view by utilizing the frontal image and segmentation mask as inputs to a conditional generative adversarial network.\n\nOur experiments validate the effectiveness of our silhouette-based model as a representation, showcasing the reliable prediction of the appearance of the back view through an image-to-image translation network. In contrast to traditional methods based on parametric models that often struggle with single-view images of subjects in challenging clothing, our approach consistently delivers successful results comparable to those achieved with multi-view inputs."}, "2308.07837": {"original_text": "  In this paper, we present a novel shape reconstruction method leveraging\ndiffusion model to generate 3D sparse point cloud for the object captured in a\nsingle RGB image. Recent methods typically leverage global embedding or local\nprojection-based features as the condition to guide the diffusion model.\nHowever, such strategies fail to consistently align the denoised point cloud\nwith the given image, leading to unstable conditioning and inferior\nperformance. In this paper, we present CCD-3DR, which exploits a novel centered\ndiffusion probabilistic model for consistent local feature conditioning. We\nconstrain the noise and sampled point cloud from the diffusion model into a\nsubspace where the point cloud center remains unchanged during the forward\ndiffusion process and reverse process. The stable point cloud center further\nserves as an anchor to align each point with its corresponding local\nprojection-based features. Extensive experiments on synthetic benchmark\nShapeNet-R2N2 demonstrate that CCD-3DR outperforms all competitors by a large\nmargin, with over 40% improvement. We also provide results on real-world\ndataset Pix3D to thoroughly demonstrate the potential of CCD-3DR in real-world\napplications. Codes will be released soon\n", "rewritten_text": "This paper introduces a novel shape reconstruction method that utilizes a diffusion model to generate a 3D sparse point cloud for an object captured in a single RGB image. Recent methods have typically relied on global embedding or local projection-based features as conditions to guide the diffusion model. However, these strategies often fail to consistently align the denoised point cloud with the given image, resulting in unstable conditioning and subpar performance.\n\nIn this study, we propose CCD-3DR, which leverages a novel centered diffusion probabilistic model for consistent local feature conditioning. We constrain the noise and sampled point cloud from the diffusion model to a subspace where the point cloud center remains unchanged throughout the forward and reverse diffusion processes. This stable point cloud center acts as an anchor to align each point with its corresponding local projection-based features.\n\nExtensive experiments conducted on the synthetic benchmark ShapeNet-R2N2 demonstrate that CCD-3DR significantly outperforms all competitors, showing an improvement of over 40%. Additionally, results from the real-world dataset Pix3D are presented to showcase the potential of CCD-3DR in practical applications. The code for CCD-3DR will be made available soon."}, "2312.10610": {"original_text": "  A number of tasks have been proposed recently to facilitate easy access to\ncharts such as chart QA and summarization. The dominant paradigm to solve these\ntasks has been to fine-tune a pretrained model on the task data. However, this\napproach is not only expensive but also not generalizable to unseen tasks. On\nthe other hand, large language models (LLMs) have shown impressive\ngeneralization capabilities to unseen tasks with zero- or few-shot prompting.\nHowever, their application to chart-related tasks is not trivial as these tasks\ntypically involve considering not only the underlying data but also the visual\nfeatures in the chart image. We propose PromptChart, a multimodal few-shot\nprompting framework with LLMs for chart-related applications. By analyzing the\ntasks carefully, we have come up with a set of prompting guidelines for each\ntask to elicit the best few-shot performance from LLMs. We further propose a\nstrategy to inject visual information into the prompts. Our experiments on\nthree different chart-related information consumption tasks show that with\nproperly designed prompts LLMs can excel on the benchmarks, achieving\nstate-of-the-art.\n", "rewritten_text": "Recently, a variety of tasks have been suggested to enhance the accessibility of charts, such as chart quality assurance and summarization. The prevailing approach to addressing these tasks has been fine-tuning a pretrained model using task-specific data. However, this method is not only costly but also lacks generalizability to new tasks. Conversely, large language models (LLMs) have demonstrated remarkable ability to generalize to new tasks with minimal or no prior training through prompting. Yet, applying LLMs to chart-related tasks presents challenges due to the need to consider both the data and visual elements in chart images.\n\nTo address this, we introduce PromptChart, a multimodal few-shot prompting framework utilizing LLMs for chart-related applications. Through careful task analysis, we have developed a set of prompting guidelines for each task to optimize the few-shot performance of LLMs. Additionally, we propose a method to incorporate visual information into the prompts. Our experiments on three distinct chart-related information consumption tasks reveal that well-crafted prompts enable LLMs to outperform existing benchmarks, achieving state-of-the-art results."}, "1906.03657": {"original_text": "  Group convolution works well with many deep convolutional neural networks\n(CNNs) that can effectively compress the model by reducing the number of\nparameters and computational cost. Using this operation, feature maps of\ndifferent group cannot communicate, which restricts their representation\ncapability. To address this issue, in this work, we propose a novel operation\nnamed Hierarchical Group Convolution (HGC) for creating computationally\nefficient neural networks. Different from standard group convolution which\nblocks the inter-group information exchange and induces the severe performance\ndegradation, HGC can hierarchically fuse the feature maps from each group and\nleverage the inter-group information effectively. Taking advantage of the\nproposed method, we introduce a family of compact networks called HGCNets.\nCompared to networks using standard group convolution, HGCNets have a huge\nimprovement in accuracy at the same model size and complexity level. Extensive\nexperimental results on the CIFAR dataset demonstrate that HGCNets obtain\nsignificant reduction of parameters and computational cost to achieve\ncomparable performance over the prior CNN architectures designed for mobile\ndevices such as MobileNet and ShuffleNet.\n", "rewritten_text": "Group convolution is effective with many deep convolutional neural networks (CNNs) as it can compress the model by reducing the number of parameters and computational cost. However, a limitation of this operation is that feature maps from different groups are unable to communicate, thereby restricting their representation capability. To overcome this limitation, we introduce a novel operation called Hierarchical Group Convolution (HGC) in this study to create computationally efficient neural networks.\n\nUnlike standard group convolution, which hinders inter-group information exchange and leads to performance degradation, HGC can hierarchically fuse feature maps from each group and effectively leverage inter-group information. By utilizing this approach, we present a series of compact networks known as HGCNets. Compared to networks utilizing standard group convolution, HGCNets exhibit a significant improvement in accuracy at the same model size and complexity level.\n\nExtensive experimental results on the CIFAR dataset demonstrate that HGCNets achieve a notable reduction in parameters and computational cost while delivering comparable performance to previous CNN architectures designed for mobile devices, such as MobileNet and ShuffleNet."}, "2304.01534": {"original_text": "  Bird's eye view (BEV) perception is becoming increasingly important in the\nfield of autonomous driving. It uses multi-view camera data to learn a\ntransformer model that directly projects the perception of the road environment\nonto the BEV perspective. However, training a transformer model often requires\na large amount of data, and as camera data for road traffic are often private,\nthey are typically not shared. Federated learning offers a solution that\nenables clients to collaborate and train models without exchanging data but\nmodel parameters. In this paper, we introduce FedBEVT, a federated transformer\nlearning approach for BEV perception. In order to address two common data\nheterogeneity issues in FedBEVT: (i) diverse sensor poses, and (ii) varying\nsensor numbers in perception systems, we propose two approaches -- Federated\nLearning with Camera-Attentive Personalization (FedCaP) and Adaptive\nMulti-Camera Masking (AMCM), respectively. To evaluate our method in real-world\nsettings, we create a dataset consisting of four typical federated use cases.\nOur findings suggest that FedBEVT outperforms the baseline approaches in all\nfour use cases, demonstrating the potential of our approach for improving BEV\nperception in autonomous driving.\n", "rewritten_text": "Bird's eye view (BEV) perception is increasingly crucial in the realm of autonomous driving. It leverages multi-view camera data to train a transformer model that directly maps the road environment perception onto the BEV perspective. However, training a transformer model typically necessitates a substantial amount of data. Since camera data related to road traffic is often proprietary and not readily shared, a solution is needed. Federated learning presents an opportunity for clients to collaborate and train models without sharing data, only model parameters.\n\nThis paper introduces FedBEVT, a federated transformer learning approach tailored for BEV perception. To tackle two prevalent data heterogeneity challenges in FedBEVT\u2014diverse sensor poses and varying sensor numbers in perception systems\u2014we propose two strategies: Federated Learning with Camera-Attentive Personalization (FedCaP) and Adaptive Multi-Camera Masking (AMCM), respectively. To assess the effectiveness of our method in practical scenarios, we curated a dataset comprising four typical federated use cases.\n\nOur results indicate that FedBEVT surpasses the baseline approaches across all four use cases, underscoring the potential of our approach to enhance BEV perception in autonomous driving."}, "1911.10248": {"original_text": "  The rapid development of inexpensive commodity depth sensors has made\nkeypoint detection and matching in the depth image modality an important\nproblem in computer vision. Despite great improvements in recent RGB local\nfeature learning methods, adapting them directly in the depth modality leads to\nunsatisfactory performance. Most of these methods do not explicitly reason\nbeyond the visible pixels in the images. To address the limitations of these\nmethods, we propose a framework ViewSynth, to jointly learn: (1) viewpoint\ninvariant keypoint-descriptor from depth images using a proposed Contrastive\nMatching Loss, and (2) view synthesis of depth images from different viewpoints\nusing the proposed View Synthesis Module and View Synthesis Loss. By learning\nview synthesis, we explicitly encourage the feature extractor to encode\ninformation about not only the visible, but also the occluded parts of the\nscene. We demonstrate that in the depth modality, ViewSynth outperforms the\nstate-of-the-art depth and RGB local feature extraction techniques in the 3D\nkeypoint matching and camera localization tasks on the RGB-D datasets 7-Scenes,\nTUM RGBD and CoRBS in most scenarios. We also show the generalizability of\nViewSynth in 3D keypoint matching across different datasets.\n", "rewritten_text": "The rapid advancement of affordable commodity depth sensors has elevated the significance of keypoint detection and matching within the depth image modality in the field of computer vision. Despite notable progress in recent RGB local feature learning techniques, directly applying them to the depth modality often results in subpar performance. Many existing methods lack explicit reasoning beyond the visible pixels in images. To overcome these limitations, we introduce a framework called ViewSynth. This framework aims to simultaneously learn: (1) a viewpoint-invariant keypoint-descriptor from depth images utilizing a novel Contrastive Matching Loss, and (2) view synthesis of depth images from various perspectives through the proposed View Synthesis Module and View Synthesis Loss. By incorporating view synthesis into the learning process, we explicitly prompt the feature extractor to encode information not only from visible areas but also from occluded parts of the scene. Our experiments demonstrate that in the depth modality, ViewSynth surpasses state-of-the-art depth and RGB local feature extraction methods in 3D keypoint matching and camera localization tasks across RGB-D datasets such as 7-Scenes, TUM RGBD, and CoRBS in most scenarios. Furthermore, we showcase the versatility of ViewSynth in 3D keypoint matching across diverse datasets."}, "2211.10018": {"original_text": "  Relation extraction has the potential for large-scale knowledge graph\nconstruction, but current methods do not consider the qualifier attributes for\neach relation triplet, such as time, quantity or location. The qualifiers form\nhyper-relational facts which better capture the rich and complex knowledge\ngraph structure. For example, the relation triplet (Leonard Parker, Educated\nAt, Harvard University) can be factually enriched by including the qualifier\n(End Time, 1967). Hence, we propose the task of hyper-relational extraction to\nextract more specific and complete facts from text. To support the task, we\nconstruct HyperRED, a large-scale and general-purpose dataset. Existing models\ncannot perform hyper-relational extraction as it requires a model to consider\nthe interaction between three entities. Hence, we propose CubeRE, a\ncube-filling model inspired by table-filling approaches and explicitly\nconsiders the interaction between relation triplets and qualifiers. To improve\nmodel scalability and reduce negative class imbalance, we further propose a\ncube-pruning method. Our experiments show that CubeRE outperforms strong\nbaselines and reveal possible directions for future research. Our code and data\nare available at github.com/declare-lab/HyperRED.\n", "rewritten_text": "Relation extraction has the potential to facilitate large-scale knowledge graph construction. However, current methods overlook the importance of qualifier attributes associated with each relation triplet, such as time, quantity, or location. These qualifiers represent hyper-relational facts that can enhance the depth and complexity of the knowledge graph structure. For instance, consider the relation triplet (Leonard Parker, Educated At, Harvard University), which can be enriched by including the qualifier (End Time, 1967). \n\nTherefore, we introduce the concept of hyper-relational extraction to extract more specific and comprehensive facts from text. To support this task, we have developed HyperRED, a large-scale and versatile dataset. Existing models are unable to perform hyper-relational extraction effectively due to the necessity of considering the interaction between three entities. To address this challenge, we propose CubeRE, a cube-filling model inspired by table-filling approaches that explicitly accounts for the interplay between relation triplets and qualifiers. \n\nIn order to enhance model scalability and mitigate negative class imbalance, we further introduce a cube-pruning method. Our experimental results demonstrate that CubeRE surpasses strong baseline models and suggest potential avenues for future research. For those interested, our code and data can be accessed at github.com/declare-lab/HyperRED."}, "2210.06246": {"original_text": "  Recently, the community has achieved substantial progress on many commonsense\nreasoning benchmarks. However, it is still unclear what is learned from the\ntraining process: the knowledge, inference capability, or both? We argue that\ndue to the large scale of commonsense knowledge, it is infeasible to annotate a\nlarge enough training set for each task to cover all commonsense for learning.\nThus we should separate the commonsense knowledge acquisition and inference\nover commonsense knowledge as two separate tasks. In this work, we focus on\ninvestigating models' commonsense inference capabilities from two perspectives:\n(1) Whether models can know if the knowledge they have is enough to solve the\ntask; (2) Whether models can develop commonsense inference capabilities that\ngeneralize across commonsense tasks. We first align commonsense tasks with\nrelevant knowledge from commonsense knowledge bases and ask humans to annotate\nwhether the knowledge is enough or not. Then, we convert different commonsense\ntasks into a unified question answering format to evaluate models'\ngeneralization capabilities. We name the benchmark as Commonsense Inference\nwith Knowledge-in-the-loop Question Answering (CIKQA).\n", "rewritten_text": "The community has recently made significant progress on various commonsense reasoning benchmarks. However, there remains uncertainty regarding what is acquired during the training process: knowledge, inference capability, or both. We posit that due to the extensive nature of commonsense knowledge, it is impractical to annotate a sufficiently large training set for each task to encompass all commonsense aspects for learning. Therefore, we advocate for the separation of commonsense knowledge acquisition and inference over commonsense knowledge as distinct tasks. In this study, our focus is on examining models' commonsense inference capabilities from two angles: (1) determining if models can discern whether the knowledge they possess is adequate to solve a given task, and (2) assessing whether models can develop commonsense inference abilities that can be applied across various commonsense tasks. Initially, we align commonsense tasks with pertinent knowledge from commonsense knowledge bases and have humans annotate whether the knowledge suffices. Subsequently, we transform diverse commonsense tasks into a standardized question-answering format to evaluate the models' generalization capabilities. This benchmark is termed Commonsense Inference with Knowledge-in-the-loop Question Answering (CIKQA)."}, "2409.11819": {"original_text": "  6D object pose estimation is the problem of identifying the position and\norientation of an object relative to a chosen coordinate system, which is a\ncore technology for modern XR applications. State-of-the-art 6D object pose\nestimators directly predict an object pose given an object observation. Due to\nthe ill-posed nature of the pose estimation problem, where multiple different\nposes can correspond to a single observation, generating additional plausible\nestimates per observation can be valuable. To address this, we reformulate the\nstate-of-the-art algorithm GDRNPP and introduce EPRO-GDR (End-to-End\nProbabilistic Geometry-Guided Regression). Instead of predicting a single pose\nper detection, we estimate a probability density distribution of the pose.\nUsing the evaluation procedure defined by the BOP (Benchmark for 6D Object Pose\nEstimation) Challenge, we test our approach on four of its core datasets and\ndemonstrate superior quantitative results for EPRO-GDR on LM-O, YCB-V, and\nITODD. Our probabilistic solution shows that predicting a pose distribution\ninstead of a single pose can improve state-of-the-art single-view pose\nestimation while providing the additional benefit of being able to sample\nmultiple meaningful pose candidates.\n", "rewritten_text": "6D object pose estimation involves identifying the position and orientation of an object in relation to a selected coordinate system, which is a fundamental technology for modern XR applications. Current state-of-the-art 6D object pose estimators directly determine an object's pose based on its observation. Given the inherently ambiguous nature of the pose estimation problem, where a single observation can correspond to multiple possible poses, generating multiple plausible estimates per observation can be advantageous. To tackle this challenge, we have redefined the state-of-the-art algorithm GDRNPP and introduced EPRO-GDR (End-to-End Probabilistic Geometry-Guided Regression). Instead of predicting a single pose for each detection, our approach estimates a probability density distribution of the pose. Through the evaluation process outlined by the BOP (Benchmark for 6D Object Pose Estimation) Challenge, we have tested our method on four key datasets and have demonstrated superior quantitative results for EPRO-GDR on LM-O, YCB-V, and ITODD. Our probabilistic approach illustrates that predicting a pose distribution, rather than a single pose, can enhance state-of-the-art single-view pose estimation, while also offering the advantage of being able to generate multiple meaningful pose candidates."}, "2405.18111": {"original_text": "  Large language models (LLMs) are proven to benefit a lot from\nretrieval-augmented generation (RAG) in alleviating hallucinations confronted\nwith knowledge-intensive questions. RAG adopts information retrieval techniques\nto inject external knowledge from semantic-relevant documents as input\ncontexts. However, since today's Internet is flooded with numerous noisy and\nfabricating content, it is inevitable that RAG systems are vulnerable to these\nnoises and prone to respond incorrectly. To this end, we propose to optimize\nthe retrieval-augmented Generator with an Adversarial Tuning Multi-agent system\n(ATM). The ATM steers the Generator to have a robust perspective of useful\ndocuments for question answering with the help of an auxiliary Attacker agent\nthrough adversarially tuning the agents for several iterations. After rounds of\nmulti-agent iterative tuning, the Generator can eventually better discriminate\nuseful documents amongst fabrications. The experimental results verify the\neffectiveness of ATM and we also observe that the Generator can achieve better\nperformance compared to the state-of-the-art baselines.\n", "rewritten_text": "Large language models (LLMs) have been shown to greatly benefit from retrieval-augmented generation (RAG) in addressing hallucinations when faced with knowledge-intensive questions. RAG utilizes information retrieval techniques to incorporate external knowledge from semantically relevant documents as input contexts. However, given the abundance of noisy and fabricated content on the internet today, RAG systems are susceptible to these disturbances and may provide inaccurate responses. \n\nTo address this challenge, we propose optimizing the retrieval-augmented Generator using an Adversarial Tuning Multi-agent system (ATM). The ATM guides the Generator to develop a robust understanding of valuable documents for question answering by employing an auxiliary Attacker agent to adversarially tune the agents through multiple iterations. Following rounds of multi-agent iterative tuning, the Generator becomes more adept at distinguishing useful documents from fabrications. Experimental results confirm the effectiveness of ATM, demonstrating that the Generator can outperform state-of-the-art baselines."}, "2309.00468": {"original_text": "  Dietary assessment is essential to maintaining a healthy lifestyle. Automatic\nimage-based dietary assessment is a growing field of research due to the\nincreasing prevalence of image capturing devices (e.g. mobile phones). In this\nwork, we estimate food energy from a single monocular image, a difficult task\ndue to the limited hard-to-extract amount of energy information present in an\nimage. To do so, we employ an improved encoder-decoder framework for energy\nestimation; the encoder transforms the image into a representation embedded\nwith food energy information in an easier-to-extract format, which the decoder\nthen extracts the energy information from. To implement our method, we compile\na high-quality food image dataset verified by registered dietitians containing\neating scene images, food-item segmentation masks, and ground truth calorie\nvalues. Our method improves upon previous caloric estimation methods by over\n10\\% and 30 kCal in terms of MAPE and MAE respectively.\n", "rewritten_text": "Dietary assessment is crucial for maintaining a healthy lifestyle. The field of automatic image-based dietary assessment is rapidly expanding, driven by the widespread use of image-capturing devices such as mobile phones. In this study, we focus on estimating food energy from a single monocular image, a challenging task due to the limited and difficult-to-extract energy information available in an image. To address this, we have developed an enhanced encoder-decoder framework for energy estimation. The encoder converts the image into a representation that includes food energy information in a more accessible format, which the decoder then extracts. To implement our approach, we have curated a high-quality food image dataset verified by registered dietitians, comprising eating scene images, food-item segmentation masks, and ground truth calorie values. Our method outperforms previous caloric estimation techniques, achieving improvements of over 10% in terms of Mean Absolute Percentage Error (MAPE) and 30 kCal in Mean Absolute Error (MAE)."}, "2411.08196": {"original_text": "  Diffusion Transformers (DiTs) have recently achieved remarkable success in\ntext-guided image generation. In image editing, DiTs project text and image\ninputs to a joint latent space, from which they decode and synthesize new\nimages. However, it remains largely unexplored how multimodal information\ncollectively forms this joint space and how they guide the semantics of the\nsynthesized images. In this paper, we investigate the latent space of DiT\nmodels and uncover two key properties: First, DiT's latent space is inherently\nsemantically disentangled, where different semantic attributes can be\ncontrolled by specific editing directions. Second, consistent semantic editing\nrequires utilizing the entire joint latent space, as neither encoded image nor\ntext alone contains enough semantic information. We show that these editing\ndirections can be obtained directly from text prompts, enabling precise\nsemantic control without additional training or mask annotations. Based on\nthese insights, we propose a simple yet effective Encode-Identify-Manipulate\n(EIM) framework for zero-shot fine-grained image editing. Specifically, we\nfirst encode both the given source image and the text prompt that describes the\nimage, to obtain the joint latent embedding. Then, using our proposed Hessian\nScore Distillation Sampling (HSDS) method, we identify editing directions that\ncontrol specific target attributes while preserving other image features. These\ndirections are guided by text prompts and used to manipulate the latent\nembeddings. Moreover, we propose a new metric to quantify the disentanglement\ndegree of the latent space of diffusion models. Extensive experiment results on\nour new curated benchmark dataset and analysis demonstrate DiT's\ndisentanglement properties and effectiveness of the EIM framework.\n", "rewritten_text": "Recently, Diffusion Transformers (DiTs) have achieved remarkable success in text-guided image generation. In image editing, DiTs project text and image inputs into a joint latent space, from which they decode and synthesize new images. However, there is still much to explore regarding how multimodal information collectively shapes this joint space and influences the semantics of the synthesized images. This paper delves into the latent space of DiT models and uncovers two key properties: Firstly, DiT's latent space is inherently semantically disentangled, allowing for specific editing directions to control different semantic attributes. Secondly, achieving consistent semantic editing necessitates utilizing the entire joint latent space, as neither the encoded image nor text alone contains sufficient semantic information. The study demonstrates that these editing directions can be derived directly from text prompts, enabling precise semantic control without the need for additional training or mask annotations.\n\nBuilding on these findings, a simple yet effective Encode-Identify-Manipulate (EIM) framework is proposed for zero-shot fine-grained image editing. The process involves encoding both the source image and the accompanying text prompt describing the image to obtain a joint latent embedding. Subsequently, utilizing the proposed Hessian Score Distillation Sampling (HSDS) method, editing directions that govern specific target attributes while preserving other image features are identified. These directions, guided by text prompts, are then used to manipulate the latent embeddings. Additionally, a new metric is introduced to quantify the degree of disentanglement in the latent space of diffusion models.\n\nExtensive experimental results on a newly curated benchmark dataset, along with analysis, showcase the disentanglement properties of DiTs and the effectiveness of the EIM framework."}, "2403.06444": {"original_text": "  Estimating reliable geometric model parameters from the data with severe\noutliers is a fundamental and important task in computer vision. This paper\nattempts to sample high-quality subsets and select model instances to estimate\nparameters in the multi-structural data. To address this, we propose an\neffective method called Latent Semantic Consensus (LSC). The principle of LSC\nis to preserve the latent semantic consensus in both data points and model\nhypotheses. Specifically, LSC formulates the model fitting problem into two\nlatent semantic spaces based on data points and model hypotheses, respectively.\nThen, LSC explores the distributions of points in the two latent semantic\nspaces, to remove outliers, generate high-quality model hypotheses, and\neffectively estimate model instances. Finally, LSC is able to provide\nconsistent and reliable solutions within only a few milliseconds for general\nmulti-structural model fitting, due to its deterministic fitting nature and\nefficiency. Compared with several state-of-the-art model fitting methods, our\nLSC achieves significant superiority for the performance of both accuracy and\nspeed on synthetic data and real images. The code will be available at\nhttps://github.com/guobaoxiao/LSC.\n", "rewritten_text": "Estimating reliable geometric model parameters from data containing severe outliers is a crucial task in computer vision. This paper aims to sample high-quality subsets and select model instances to estimate parameters in multi-structural data. To tackle this challenge, we introduce an effective method called Latent Semantic Consensus (LSC). The core principle of LSC is to maintain the latent semantic consensus present in both data points and model hypotheses. Specifically, LSC transforms the model fitting problem into two latent semantic spaces based on data points and model hypotheses, respectively. Subsequently, LSC analyzes the distributions of points in these two latent semantic spaces to eliminate outliers, generate high-quality model hypotheses, and accurately estimate model instances. Ultimately, LSC can deliver consistent and reliable solutions within milliseconds for general multi-structural model fitting, owing to its deterministic fitting approach and efficiency. When compared to various state-of-the-art model fitting methods, our LSC demonstrates significant superiority in terms of both accuracy and speed on synthetic data and real images. The code for LSC will be accessible at https://github.com/guobaoxiao/LSC."}, "1708.0923": {"original_text": "  Named Entity Recognition and Disambiguation (NERD) systems have recently been\nwidely researched to deal with the significant growth of the Web. NERD systems\nare crucial for several Natural Language Processing (NLP) tasks such as\nsummarization, understanding, and machine translation. However, there is no\nstandard interface specification, i.e. these systems may vary significantly\neither for exporting their outputs or for processing the inputs. Thus, when a\ngiven company desires to implement more than one NERD system, the process is\nquite exhaustive and prone to failure. In addition, industrial solutions demand\ncritical requirements, e.g., large-scale processing, completeness, versatility,\nand licenses. Commonly, these requirements impose a limitation, making good\nNERD models to be ignored by companies. This paper presents TANKER, a\ndistributed architecture which aims to overcome scalability, reliability and\nfailure tolerance limitations related to industrial needs by combining NERD\nsystems. To this end, TANKER relies on a micro-services oriented architecture,\nwhich enables agile development and delivery of complex enterprise\napplications. In addition, TANKER provides a standardized API which makes\npossible to combine several NERD systems at once.\n", "rewritten_text": "Named Entity Recognition and Disambiguation (NERD) systems have been extensively researched in response to the rapid expansion of the Web. These systems play a crucial role in various Natural Language Processing (NLP) tasks such as summarization, comprehension, and machine translation. However, a notable challenge lies in the lack of a standardized interface specification, leading to significant variations in how these systems export their outputs or process inputs. Consequently, when a company seeks to implement multiple NERD systems, the process becomes laborious and prone to errors. Moreover, industrial applications necessitate specific requirements such as large-scale processing, completeness, versatility, and licensing, which often result in the neglect of high-quality NERD models by companies.\n\nThis paper introduces TANKER, a distributed architecture designed to address scalability, reliability, and failure tolerance limitations associated with industrial demands by integrating multiple NERD systems. TANKER leverages a micro-services oriented architecture to facilitate agile development and deployment of sophisticated enterprise applications. Additionally, TANKER offers a standardized API that enables the seamless integration of multiple NERD systems simultaneously."}, "1708.01846": {"original_text": "  Low-rank decomposition (LRD) is a state-of-the-art method for visual data\nreconstruction and modelling. However, it is a very challenging problem when\nthe image data contains significant occlusion, noise, illumination variation,\nand misalignment from rotation or viewpoint changes. We leverage the specific\nstructure of data in order to improve the performance of LRD when the data are\nnot ideal. To this end, we propose a new framework that embeds manifold priors\ninto LRD. To implement the framework, we design an alternating direction method\nof multipliers (ADMM) method which efficiently integrates the manifold\nconstraints during the optimization process. The proposed approach is\nsuccessfully used to calculate low-rank models from face images, hand-written\ndigits and planar surface images. The results show a consistent increase of\nperformance when compared to the state-of-the-art over a wide range of\nrealistic image misalignments and corruptions.\n", "rewritten_text": "Low-rank decomposition (LRD) is a cutting-edge method used for reconstructing and modeling visual data. However, it becomes a highly challenging task when the image data includes significant occlusion, noise, illumination variation, and misalignment due to rotation or changes in viewpoint. In order to enhance the performance of LRD under non-ideal data conditions, we exploit the inherent structure of the data. To achieve this, we introduce a novel framework that incorporates manifold priors into LRD. The implementation of this framework involves the development of an alternating direction method of multipliers (ADMM) that effectively integrates manifold constraints throughout the optimization process. Our proposed approach has been successfully applied to generating low-rank models from face images, hand-written digits, and planar surface images. The results demonstrate a consistent performance improvement compared to the current state-of-the-art across a wide range of realistic image misalignments and corruptions."}, "2001.0732": {"original_text": "  It's natural these days for people to know the local events from massive\ndocuments. Many texts contain location information, such as city name or road\nname, which is always incomplete or latent. It's significant to extract the\nadministrative area of the text and organize the hierarchy of area, called\nlocation normalization. Existing detecting location systems either exclude\nhierarchical normalization or present only a few specific regions. We propose a\nsystem named ROIBase that normalizes the text by the Chinese hierarchical\nadministrative divisions. ROIBase adopts a co-occurrence constraint as the\nbasic framework to score the hit of the administrative area, achieves the\ninference by special embeddings, and expands the recall by the ROI (region of\ninterest). It has high efficiency and interpretability because it mainly\nestablishes on the definite knowledge and has less complex logic than the\nsupervised models. We demonstrate that ROIBase achieves better performance\nagainst feasible solutions and is useful as a strong support system for\nlocation normalization.\n", "rewritten_text": "In today's world, it is common for people to gather information about local events from extensive documents. Many texts include location details, such as city or road names, which are often incomplete or ambiguous. Therefore, it is crucial to extract the administrative area from the text and establish a hierarchical structure of areas, a process known as location normalization.\n\nExisting location detection systems typically lack hierarchical normalization or focus on only a limited number of specific regions. In response to this, we introduce a system called ROIBase, which normalizes text based on the hierarchical administrative divisions in China. ROIBase utilizes a co-occurrence constraint as its fundamental framework to assess the relevance of the administrative area, employs special embeddings for inference, and enhances recall through the region of interest (ROI).\n\nROIBase is characterized by its high efficiency and interpretability, as it primarily relies on explicit knowledge and features less complex logic compared to supervised models. Our research demonstrates that ROIBase outperforms other viable solutions and serves as a robust support system for location normalization."}, "1908.02505": {"original_text": "  This paper presents a scoring system that has shown the top result on the\ntext subset of CALL v3 shared task. The presented system is based on text\nembeddings, namely NNLM~\\cite{nnlm} and BERT~\\cite{Bert}. The distinguishing\nfeature of the given approach is that it does not rely on the reference grammar\nfile for scoring. The model is compared against approaches that use the grammar\nfile and proves the possibility to achieve similar and even higher results\nwithout a predefined set of correct answers.\n  The paper describes the model itself and the data preparation process that\nplayed a crucial role in the model training.\n", "rewritten_text": "This paper introduces a scoring system that has achieved the highest performance in the text subset of the CALL v3 shared task. The system presented is built upon text embeddings, specifically NNLM~\\cite{nnlm} and BERT~\\cite{Bert}. A key feature of this approach is its independence from a reference grammar file for scoring. By comparing the model to approaches that utilize the grammar file, it demonstrates the potential to attain comparable or superior results without a predefined set of correct answers. The paper details the model and the essential data preparation process that significantly influenced the model training."}, "1604.0801": {"original_text": "  The purpose of this paper is the detection of salient areas in natural video\nby using the new deep learning techniques. Salient patches in video frames are\npredicted first. Then the predicted visual fixation maps are built upon them.\nWe design the deep architecture on the basis of CaffeNet implemented with Caffe\ntoolkit. We show that changing the way of data selection for optimisation of\nnetwork parameters, we can save computation cost up to 12 times. We extend deep\nlearning approaches for saliency prediction in still images with RGB values to\nspecificity of video using the sensitivity of the human visual system to\nresidual motion. Furthermore, we complete primary colour pixel values by\ncontrast features proposed in classical visual attention prediction models. The\nexperiments are conducted on two publicly available datasets. The first is\nIRCCYN video database containing 31 videos with an overall amount of 7300\nframes and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided\n2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, the\naccuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction of\nsaliency of patches show the improvement up to 2% with regard to RGB use only.\nThe resulting accuracy of 76, 6% is obtained. The AUC metric in comparison of\npredicted saliency maps with visual fixation maps shows the increase up to 16%\non a sample of video clips from this dataset.\n", "rewritten_text": "This paper aims to detect salient areas in natural videos using new deep learning techniques. Initially, salient patches in video frames are predicted, followed by the creation of visual fixation maps based on these predictions. The deep architecture is designed using CaffeNet and implemented with the Caffe toolkit. By altering the data selection method for optimizing network parameters, a computation cost reduction of up to 12 times is achieved. The deep learning approaches for saliency prediction in still images with RGB values are extended to video specificity by leveraging the human visual system's sensitivity to residual motion. Additionally, primary color pixel values are complemented by contrast features proposed in classical visual attention prediction models. \n\nExperiments are conducted on two publicly available datasets. The first dataset is the IRCCYN video database, which includes 31 videos totaling 7300 frames and eye fixations from 37 subjects. The second dataset is HOLLYWOOD2, comprising 2517 movie clips with eye fixations from 19 subjects. The accuracy achieved on the IRCCYN dataset is 89.51%, while on the HOLLYWOOD2 dataset, there is a 2% improvement in saliency prediction of patches compared to using RGB values alone, resulting in an accuracy of 76.6%. The AUC metric demonstrates up to a 16% increase when comparing predicted saliency maps with visual fixation maps on a sample of video clips from the HOLLYWOOD2 dataset."}, "2301.03182": {"original_text": "  Existing deep learning-based shadow removal methods still produce images with\nshadow remnants. These shadow remnants typically exist in homogeneous regions\nwith low-intensity values, making them untraceable in the existing\nimage-to-image mapping paradigm. We observe that shadows mainly degrade images\nat the image-structure level (in which humans perceive object shapes and\ncontinuous colors). Hence, in this paper, we propose to remove shadows at the\nimage structure level. Based on this idea, we propose a novel\nstructure-informed shadow removal network (StructNet) to leverage the\nimage-structure information to address the shadow remnant problem.\nSpecifically, StructNet first reconstructs the structure information of the\ninput image without shadows and then uses the restored shadow-free structure\nprior to guiding the image-level shadow removal. StructNet contains two main\nnovel modules: (1) a mask-guided shadow-free extraction (MSFE) module to\nextract image structural features in a non-shadow-to-shadow directional manner,\nand (2) a multi-scale feature & residual aggregation (MFRA) module to leverage\nthe shadow-free structure information to regularize feature consistency. In\naddition, we also propose to extend StructNet to exploit multi-level structure\ninformation (MStructNet), to further boost the shadow removal performance with\nminimum computational overheads. Extensive experiments on three shadow removal\nbenchmarks demonstrate that our method outperforms existing shadow removal\nmethods, and our StructNet can be integrated with existing methods to improve\nthem further.\n", "rewritten_text": "Current deep learning-based shadow removal methods still leave behind remnants of shadows in the resulting images. These remnants are typically found in uniform areas with low-intensity values, making them difficult to detect within the current image-to-image mapping framework. Our observation indicates that shadows primarily impact images at the level of image structure, affecting how humans perceive object shapes and color continuity. Therefore, in this study, we propose a novel approach to eliminating shadows at the image structure level.\n\nBuilding on this concept, we introduce a new network called StructNet, which is designed to utilize image structure information to address the issue of shadow remnants. StructNet begins by reconstructing the structural details of the input image without shadows, then utilizes this shadow-free structure to guide the removal of shadows at the image level. The network comprises two key components: a mask-guided shadow-free extraction (MSFE) module for extracting image structural features in a direction from non-shadow to shadow, and a multi-scale feature & residual aggregation (MFRA) module for leveraging the shadow-free structure information to ensure feature consistency.\n\nFurthermore, we propose an extension of StructNet, named MStructNet, which incorporates multi-level structure information to enhance shadow removal performance with minimal computational overhead. Through extensive experiments on three shadow removal benchmarks, we demonstrate that our method surpasses existing shadow removal techniques. Additionally, we show that StructNet can be integrated with existing methods to further enhance their performance."}, "1709.0586": {"original_text": "  We present a novel method for cell segmentation in microscopy images which is\ninspired by the Generative Adversarial Neural Network (GAN) approach. Our\nframework is built on a pair of two competitive artificial neural networks,\nwith a unique architecture, termed Rib Cage, which are trained simultaneously\nand together define a min-max game resulting in an accurate segmentation of a\ngiven image. Our approach has two main strengths, similar to the GAN, the\nmethod does not require a formulation of a loss function for the optimization\nprocess. This allows training on a limited amount of annotated data in a weakly\nsupervised manner. Promising segmentation results on real fluorescent\nmicroscopy data are presented. The code is freely available at:\nhttps://github.com/arbellea/DeepCellSeg.git\n", "rewritten_text": "We introduce a novel method for cell segmentation in microscopy images, drawing inspiration from the Generative Adversarial Neural Network (GAN) approach. Our framework is based on a unique architecture called Rib Cage, consisting of a pair of competitive artificial neural networks. These networks are trained simultaneously and engage in a min-max game, leading to precise segmentation of the input image. Our method shares two key strengths with GAN: it eliminates the need for formulating a loss function during optimization, enabling training with limited annotated data in a weakly supervised manner. We demonstrate promising segmentation results on real fluorescent microscopy data. The code is freely accessible at: https://github.com/arbellea/DeepCellSeg.git"}, "2203.14565": {"original_text": "  Domain adaptation (DA) or domain generalization (DG) for face presentation\nattack detection (PAD) has attracted attention recently with its robustness\nagainst unseen attack scenarios. Existing DA/DG-based PAD methods, however,\nhave not yet fully explored the domain-specific style information that can\nprovide knowledge regarding attack styles (e.g., materials, background,\nillumination and resolution). In this paper, we introduce a novel Style-Guided\nDomain Adaptation (SGDA) framework for inference-time adaptive PAD.\nSpecifically, Style-Selective Normalization (SSN) is proposed to explore the\ndomain-specific style information within the high-order feature statistics. The\nproposed SSN enables the adaptation of the model to the target domain by\nreducing the style difference between the target and the source domains.\nMoreover, we carefully design Style-Aware Meta-Learning (SAML) to boost the\nadaptation ability, which simulates the inference-time adaptation with style\nselection process on virtual test domain. In contrast to previous domain\nadaptation approaches, our method does not require either additional auxiliary\nmodels (e.g., domain adaptors) or the unlabeled target domain during training,\nwhich makes our method more practical to PAD task. To verify our experiments,\nwe utilize the public datasets: MSU-MFSD, CASIA-FASD, OULU-NPU and Idiap\nREPLAYATTACK. In most assessments, the result demonstrates a notable gap of\nperformance compared to the conventional DA/DG-based PAD methods.\n", "rewritten_text": "Recently, there has been a growing interest in Domain Adaptation (DA) and Domain Generalization (DG) for face presentation attack detection (PAD) due to their robustness against unseen attack scenarios. However, existing DA/DG-based PAD methods have not fully leveraged domain-specific style information, which can offer insights into attack styles such as materials, background, illumination, and resolution. \n\nThis paper introduces a novel Style-Guided Domain Adaptation (SGDA) framework for adaptive PAD at inference time. Specifically, we propose Style-Selective Normalization (SSN) to extract domain-specific style information from high-order feature statistics. SSN facilitates model adaptation to the target domain by minimizing style differences between the target and source domains. Additionally, we introduce Style-Aware Meta-Learning (SAML) to enhance adaptation capabilities, simulating inference-time adaptation through a style selection process on a virtual test domain.\n\nUnlike previous domain adaptation methods, our approach does not rely on additional auxiliary models (e.g., domain adaptors) or unlabeled target domain data during training, making it more practical for PAD tasks. To validate our experiments, we utilize public datasets such as MSU-MFSD, CASIA-FASD, OULU-NPU, and Idiap REPLAYATTACK. Across various assessments, our results demonstrate a significant performance improvement compared to conventional DA/DG-based PAD methods."}, "1905.08511": {"original_text": "  Question answering (QA) using textual sources for purposes such as reading\ncomprehension (RC) has attracted much attention. This study focuses on the task\nof explainable multi-hop QA, which requires the system to return the answer\nwith evidence sentences by reasoning and gathering disjoint pieces of the\nreference texts. It proposes the Query Focused Extractor (QFE) model for\nevidence extraction and uses multi-task learning with the QA model. QFE is\ninspired by extractive summarization models; compared with the existing method,\nwhich extracts each evidence sentence independently, it sequentially extracts\nevidence sentences by using an RNN with an attention mechanism on the question\nsentence. It enables QFE to consider the dependency among the evidence\nsentences and cover important information in the question sentence.\nExperimental results show that QFE with a simple RC baseline model achieves a\nstate-of-the-art evidence extraction score on HotpotQA. Although designed for\nRC, it also achieves a state-of-the-art evidence extraction score on FEVER,\nwhich is a recognizing textual entailment task on a large textual database.\n", "rewritten_text": "Question answering (QA) using textual sources, such as reading comprehension (RC), has garnered significant attention. This study specifically delves into the realm of explainable multi-hop QA, a task that necessitates the system to provide the answer along with supporting evidence sentences derived from reasoning and gathering disparate segments of the reference texts. To address this challenge, the study introduces the Query Focused Extractor (QFE) model for evidence extraction, employing multi-task learning in conjunction with the QA model.\n\nThe QFE model draws inspiration from extractive summarization models. In contrast to existing methods that extract each evidence sentence independently, QFE adopts a sequential approach to evidence extraction. It utilizes an RNN with an attention mechanism on the question sentence to sequentially extract evidence sentences. This design allows QFE to consider the interdependency among the evidence sentences and encompass crucial information from the question sentence.\n\nExperimental findings demonstrate that QFE, when paired with a simple RC baseline model, achieves a state-of-the-art evidence extraction score on HotpotQA. Notably, despite being designed for RC, QFE also attains a leading evidence extraction score on FEVER, a task centered around recognizing textual entailment within a vast textual database."}, "2310.18205": {"original_text": "  Claim span identification (CSI) is an important step in fact-checking\npipelines, aiming to identify text segments that contain a checkworthy claim or\nassertion in a social media post. Despite its importance to journalists and\nhuman fact-checkers, it remains a severely understudied problem, and the scarce\nresearch on this topic so far has only focused on English. Here we aim to\nbridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K\nreal-world claims collected from numerous social media platforms in five Indian\nlanguages and English. We report strong baselines with state-of-the-art\nencoder-only language models (e.g., XLM-R) and we demonstrate the benefits of\ntraining on multiple languages over alternative cross-lingual transfer methods\nsuch as zero-shot transfer, or training on translated data, from a\nhigh-resource language such as English. We evaluate generative large language\nmodels from the GPT series using prompting methods on the X-CLAIM dataset and\nwe find that they underperform the smaller encoder-only language models for\nlow-resource languages.\n", "rewritten_text": "Claim span identification (CSI) is a crucial step in fact-checking pipelines, with the goal of pinpointing text segments containing checkworthy claims or assertions in social media posts. Despite its significance for journalists and human fact-checkers, this area remains largely unexplored, with limited research focusing solely on English. To address this gap, we introduce a new dataset, X-CLAIM, comprising 7,000 real-world claims gathered from various social media platforms in five Indian languages as well as English. Our study showcases robust baselines using cutting-edge encoder-only language models (e.g., XLM-R) and highlights the advantages of training across multiple languages compared to other cross-lingual transfer methods like zero-shot transfer or training on translated data from a high-resource language like English. We assess the performance of generative large language models from the GPT series on the X-CLAIM dataset using prompting techniques, revealing that they are outperformed by smaller encoder-only language models for low-resource languages."}, "2306.08075": {"original_text": "  Current knowledge distillation approaches in semantic segmentation tend to\nadopt a holistic approach that treats all spatial locations equally. However,\nfor dense prediction, students' predictions on edge regions are highly\nuncertain due to contextual information leakage, requiring higher spatial\nsensitivity knowledge than the body regions. To address this challenge, this\npaper proposes a novel approach called boundary-privileged knowledge\ndistillation (BPKD). BPKD distills the knowledge of the teacher model's body\nand edges separately to the compact student model. Specifically, we employ two\ndistinct loss functions: (i) edge loss, which aims to distinguish between\nambiguous classes at the pixel level in edge regions; (ii) body loss, which\nutilizes shape constraints and selectively attends to the inner-semantic\nregions. Our experiments demonstrate that the proposed BPKD method provides\nextensive refinements and aggregation for edge and body regions. Additionally,\nthe method achieves state-of-the-art distillation performance for semantic\nsegmentation on three popular benchmark datasets, highlighting its\neffectiveness and generalization ability. BPKD shows consistent improvements\nacross a diverse array of lightweight segmentation structures, including both\nCNNs and transformers, underscoring its architecture-agnostic adaptability. The\ncode is available at \\url{https://github.com/AkideLiu/BPKD}.\n", "rewritten_text": "The current approaches to knowledge distillation in semantic segmentation typically take a holistic approach, treating all spatial locations equally. However, in dense prediction tasks, the predictions made by students on edge regions are often highly uncertain due to leakage of contextual information, necessitating a higher spatial sensitivity of knowledge compared to body regions. To tackle this challenge, this paper introduces a novel approach known as boundary-privileged knowledge distillation (BPKD). BPKD involves distilling the knowledge from the teacher model's body and edges separately to a compact student model. This is achieved through the use of two distinct loss functions: an edge loss, which focuses on distinguishing between ambiguous classes at the pixel level in edge regions, and a body loss, which incorporates shape constraints and selectively attends to inner-semantic regions. Experimental results demonstrate that the proposed BPKD method significantly enhances the refinement and aggregation of edge and body regions. Furthermore, the method achieves state-of-the-art distillation performance in semantic segmentation across three popular benchmark datasets, showcasing its effectiveness and generalization capabilities. BPKD consistently improves performance across various lightweight segmentation structures, including both CNNs and transformers, highlighting its adaptability regardless of architecture. The code for BPKD is available at \\url{https://github.com/AkideLiu/BPKD}."}, "2010.05762": {"original_text": "  Scene flow represents the 3D motion of every point in the dynamic\nenvironments. Like the optical flow that represents the motion of pixels in 2D\nimages, 3D motion representation of scene flow benefits many applications, such\nas autonomous driving and service robot. This paper studies the problem of\nscene flow estimation from two consecutive 3D point clouds. In this paper, a\nnovel hierarchical neural network with double attention is proposed for\nlearning the correlation of point features in adjacent frames and refining\nscene flow from coarse to fine layer by layer. The proposed network has a new\nmore-for-less hierarchical architecture. The more-for-less means that the\nnumber of input points is greater than the number of output points for scene\nflow estimation, which brings more input information and balances the precision\nand resource consumption. In this hierarchical architecture, scene flow of\ndifferent levels is generated and supervised respectively. A novel attentive\nembedding module is introduced to aggregate the features of adjacent points\nusing a double attention method in a patch-to-patch manner. The proper layers\nfor flow embedding and flow supervision are carefully considered in our network\ndesignment. Experiments show that the proposed network outperforms the\nstate-of-the-art performance of 3D scene flow estimation on the FlyingThings3D\nand KITTI Scene Flow 2015 datasets. We also apply the proposed network to\nrealistic LiDAR odometry task, which is an key problem in autonomous driving.\nThe experiment results demonstrate that our proposed network can outperform the\nICP-based method and shows the good practical application ability.\n", "rewritten_text": "The concept of scene flow involves capturing the 3D motion of every point within dynamic environments. Similar to optical flow, which tracks the movement of pixels in 2D images, scene flow in 3D has various practical applications, such as autonomous driving and service robots. This study focuses on estimating scene flow from two consecutive 3D point clouds. A novel hierarchical neural network with dual attention mechanisms is proposed in this paper to learn the correlation of point features across frames and refine scene flow from coarse to fine layers. The network's architecture is designed to handle more input points than output points for scene flow estimation, enhancing information input and balancing precision and resource usage. The hierarchical structure generates and supervises scene flow at different levels. An innovative attentive embedding module is introduced to aggregate adjacent point features using a double attention approach in a patch-to-patch manner. The network's design carefully considers the appropriate layers for flow embedding and supervision. Experimental results demonstrate that the proposed network surpasses the current state-of-the-art performance in 3D scene flow estimation on the FlyingThings3D and KITTI Scene Flow 2015 datasets. Additionally, the network is applied to a realistic LiDAR odometry task, a critical issue in autonomous driving, outperforming the ICP-based method and showcasing strong practical application capabilities."}, "2303.14829": {"original_text": "  Generating grammatically and semantically correct captions in video\ncaptioning is a challenging task. The captions generated from the existing\nmethods are either word-by-word that do not align with grammatical structure or\nmiss key information from the input videos. To address these issues, we\nintroduce a novel global-local fusion network, with a Global-Local Fusion Block\n(GLFB) that encodes and fuses features from different parts of speech (POS)\ncomponents with visual-spatial features. We use novel combinations of different\nPOS components - 'determinant + subject', 'auxiliary verb', 'verb', and\n'determinant + object' for supervision of the POS blocks - Det + Subject, Aux\nVerb, Verb, and Det + Object respectively. The novel global-local fusion\nnetwork together with POS blocks helps align the visual features with language\ndescription to generate grammatically and semantically correct captions.\nExtensive qualitative and quantitative experiments on benchmark MSVD and MSRVTT\ndatasets demonstrate that the proposed approach generates more grammatically\nand semantically correct captions compared to the existing methods, achieving\nthe new state-of-the-art. Ablations on the POS blocks and the GLFB demonstrate\nthe impact of the contributions on the proposed method.\n", "rewritten_text": "Generating grammatically and semantically correct captions in video captioning poses a significant challenge. The captions produced by current methods often lack alignment with proper grammatical structure or fail to capture essential information from the input videos. To tackle these issues, we present a novel global-local fusion network featuring a Global-Local Fusion Block (GLFB). This network encodes and merges features from various parts of speech (POS) components along with visual-spatial features.\n\nWe employ unique combinations of different POS components - such as 'determinant + subject', 'auxiliary verb', 'verb', and 'determinant + object' - to supervise the POS blocks: Det + Subject, Aux Verb, Verb, and Det + Object, respectively. The innovative global-local fusion network, in conjunction with the POS blocks, aids in aligning visual features with language descriptions to generate captions that are both grammatically and semantically accurate.\n\nExtensive qualitative and quantitative experiments conducted on benchmark MSVD and MSRVTT datasets showcase that our proposed approach outperforms existing methods in generating captions that are more grammatically and semantically correct, thus achieving a new state-of-the-art. Ablation studies on the POS blocks and the GLFB further illustrate the significant impact of these components on the effectiveness of our proposed method."}, "1805.11788": {"original_text": "  Traditional Convolutional Neural Networks (CNNs) typically use the same\nactivation function (usually ReLU) for all neurons with non-linear mapping\noperations. For example, the deep convolutional architecture Inception-v4 uses\nReLU. To improve the classification performance of traditional CNNs, a new\n\"Multi-function Convolutional Neural Network\" (MCNN) is created by using\ndifferent activation functions for different neurons. For $n$ neurons and $m$\ndifferent activation functions, there are a total of $m^n-m$ MCNNs and only $m$\ntraditional CNNs. Therefore, the best model is very likely to be chosen from\nMCNNs because there are $m^n-2m$ more MCNNs than traditional CNNs. For\nperformance analysis, two different datasets for two applications (classifying\nhandwritten digits from the MNIST database and classifying brain MRI images\ninto one of the four stages of Alzheimer's disease (AD)) are used. For both\napplications, an activation function is randomly selected for each layer of a\nMCNN. For the AD diagnosis application, MCNNs using a newly created\nmulti-function Inception-v4 architecture are constructed. Overall, simulations\nshow that MCNNs can outperform traditional CNNs in terms of multi-class\nclassification accuracy for both applications. An important future research\nwork will be to efficiently select the best MCNN from $m^n-m$ candidate MCNNs.\nCurrent CNN software only provides users with partial functionality of MCNNs\nsince different layers can use different activation functions but not\nindividual neurons in the same layer. Thus, modifying current CNN software\nsystems such as ResNets, DenseNets, and Dual Path Networks by using multiple\nactivation functions and developing more effective and faster MCNN software\nsystems and tools would be very useful to solve difficult practical image\nclassification problems.\n", "rewritten_text": "Traditional Convolutional Neural Networks (CNNs) typically employ the same activation function, usually ReLU, for all neurons to perform non-linear mapping operations. For instance, the deep convolutional architecture Inception-v4 utilizes ReLU. To enhance the classification performance of traditional CNNs, a new model called the \"Multi-function Convolutional Neural Network\" (MCNN) has been developed, which employs different activation functions for different neurons. With $n$ neurons and $m$ distinct activation functions, there exist a total of $m^n-m$ MCNNs compared to only $m$ traditional CNNs. Consequently, the optimal model is more likely to be selected from MCNNs due to the significantly larger number of $m^n-2m$ MCNNs available. \n\nFor performance evaluation, two distinct datasets are utilized for two applications: classifying handwritten digits from the MNIST database and categorizing brain MRI images into one of the four stages of Alzheimer's disease (AD). In both applications, an activation function is randomly assigned to each layer of an MCNN. Specifically for the AD diagnosis application, MCNNs are constructed using a newly devised multi-function Inception-v4 architecture. Simulation results demonstrate that MCNNs can surpass traditional CNNs in terms of multi-class classification accuracy for both applications. \n\nA crucial area for future research will involve efficiently selecting the best MCNN from the pool of $m^n-m$ candidate MCNNs. Current CNN software offers users only limited functionality of MCNNs, as different layers can utilize different activation functions but not individual neurons within the same layer. Therefore, enhancing existing CNN software systems such as ResNets, DenseNets, and Dual Path Networks by incorporating multiple activation functions and developing more efficient and rapid MCNN software systems and tools would be highly beneficial in addressing challenging practical image classification tasks."}, "2108.024": {"original_text": "  Gait data captured by inertial sensors have demonstrated promising results on\nuser authentication. However, most existing approaches stored the enrolled gait\npattern insecurely for matching with the validating pattern, thus, posed\ncritical security and privacy issues. In this study, we present a gait\ncryptosystem that generates from gait data the random key for user\nauthentication, meanwhile, secures the gait pattern. First, we propose a\nrevocable and random binary string extraction method using a deep neural\nnetwork followed by feature-wise binarization. A novel loss function for\nnetwork optimization is also designed, to tackle not only the intrauser\nstability but also the inter-user randomness. Second, we propose a new\nbiometric key generation scheme, namely Irreversible Error Correct and\nObfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme,\nto securely generate from the binary string the random and irreversible key.\nThe model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. We\nshowed that our model could generate the key of 139 bits from 5-second data\nsequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR)\nsmaller than 5.441%. In addition, the security and user privacy analyses showed\nthat our model was secure against existing attacks on biometric template\nprotection, and fulfilled irreversibility and unlinkability.\n", "rewritten_text": "Inertial sensors have shown promising results in user authentication through the capture of gait data. However, most current methods store the enrolled gait pattern insecurely for matching with the validating pattern, leading to critical security and privacy concerns. This study introduces a gait cryptosystem that utilizes gait data to generate a random key for user authentication while ensuring the security of the gait pattern.\n\nTo achieve this, we propose a method for extracting a revocable and random binary string using a deep neural network, followed by feature-wise binarization. A novel loss function is designed for network optimization to address both intrauser stability and inter-user randomness. Additionally, we introduce a new biometric key generation scheme called Irreversible Error Correct and Obfuscate (IECO), an enhancement of the Error Correct and Obfuscate (ECO) scheme, to securely produce a random and irreversible key from the binary string.\n\nThe effectiveness of the model was evaluated using two benchmark datasets, OU-ISIR and whuGAIT. Results demonstrate that our model can generate a 139-bit key from a 5-second data sequence with a zero False Acceptance Rate (FAR) and a False Rejection Rate (FRR) below 5.441%. Furthermore, security and user privacy analyses confirm that our model is resilient against existing attacks on biometric template protection, ensuring irreversibility and unlinkability."}, "2404.03443": {"original_text": "  The goal of occluded person re-identification (ReID) is to retrieve specific\npedestrians in occluded situations. However, occluded person ReID still suffers\nfrom background clutter and low-quality local feature representations, which\nlimits model performance. In our research, we introduce a new framework called\nPAB-ReID, which is a novel ReID model incorporating part-attention mechanisms\nto tackle the aforementioned issues effectively. Firstly, we introduce the\nhuman parsing label to guide the generation of more accurate human part\nattention maps. In addition, we propose a fine-grained feature focuser for\ngenerating fine-grained human local feature representations while suppressing\nbackground interference. Moreover, We also design a part triplet loss to\nsupervise the learning of human local features, which optimizes\nintra/inter-class distance. We conducted extensive experiments on specialized\nocclusion and regular ReID datasets, showcasing that our approach outperforms\nthe existing state-of-the-art methods.\n", "rewritten_text": "The objective of occluded person re-identification (ReID) is to identify specific pedestrians in situations where they are partially hidden. However, occluded person ReID faces challenges such as background clutter and low-quality local feature representations, which hinder the model's performance. In our study, we present a new framework known as PAB-ReID, which is an innovative ReID model that incorporates part-attention mechanisms to effectively address these issues. \n\nTo begin with, we utilize human parsing labels to guide the creation of more precise human part attention maps. Additionally, we introduce a fine-grained feature focuser to generate detailed human local feature representations while minimizing background interference. Furthermore, we have developed a part triplet loss to supervise the learning of human local features, thereby optimizing intra/inter-class distance. \n\nWe conducted extensive experiments on specialized occlusion and standard ReID datasets, demonstrating that our approach surpasses the current state-of-the-art methods."}, "2310.05620": {"original_text": "  General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic.\n", "rewritten_text": "LLMs specializing in general and legal domains have shown strong performance across various tasks in LegalAI. However, the current evaluations of these LLMs in LegalAI are conducted by computer science experts, lacking alignment with the logic of legal practice. This misalignment makes it challenging to assess their practical capabilities accurately.\n\nTo tackle this issue, we have taken the initiative to develop the Chinese legal LLMs benchmark, known as LAiW, which is grounded in the logic of legal practice. In order to mirror the thinking process of legal experts and legal practice (syllogism), we have categorized the legal capabilities of LLMs into three levels of difficulty: basic information retrieval, legal foundation inference, and complex legal application. Each level comprises multiple tasks to ensure a comprehensive evaluation.\n\nOur automated evaluation of current general and legal domain LLMs on our benchmark suggests that these models may not fully adhere to the logic of legal practice. While LLMs appear capable of acquiring complex legal application skills directly, they exhibit weaknesses in basic tasks. These shortcomings could impede their practical application and acceptance by legal experts.\n\nTo further validate the complex legal application capabilities of current LLMs in legal scenarios, we have also included human evaluations by legal experts. The findings indicate that although LLMs may demonstrate strong performance, they still require reinforcement in terms of legal logic."}, "2408.12677": {"original_text": "  Traditional volumetric fusion algorithms preserve the spatial structure of 3D\nscenes, which is beneficial for many tasks in computer vision and robotics.\nHowever, they often lack realism in terms of visualization. Emerging 3D\nGaussian splatting bridges this gap, but existing Gaussian-based reconstruction\nmethods often suffer from artifacts and inconsistencies with the underlying 3D\nstructure, and struggle with real-time optimization, unable to provide users\nwith immediate feedback in high quality. One of the bottlenecks arises from the\nmassive amount of Gaussian parameters that need to be updated during\noptimization. Instead of using 3D Gaussian as a standalone map representation,\nwe incorporate it into a volumetric mapping system to take advantage of\ngeometric information and propose to use a quadtree data structure on images to\ndrastically reduce the number of splats initialized. In this way, we\nsimultaneously generate a compact 3D Gaussian map with fewer artifacts and a\nvolumetric map on the fly. Our method, GSFusion, significantly enhances\ncomputational efficiency without sacrificing rendering quality, as demonstrated\non both synthetic and real datasets. Code will be available at\nhttps://github.com/goldoak/GSFusion.\n", "rewritten_text": "Traditional volumetric fusion algorithms maintain the spatial structure of 3D scenes, which is advantageous for various tasks in computer vision and robotics. However, they often lack realism in terms of visualization. The emerging technique of 3D Gaussian splatting addresses this gap, yet current Gaussian-based reconstruction methods frequently exhibit artifacts and inconsistencies with the underlying 3D structure. Moreover, they struggle with real-time optimization, failing to offer users immediate high-quality feedback. One major bottleneck stems from the substantial number of Gaussian parameters requiring updating during optimization.\n\nTo overcome these challenges, rather than utilizing 3D Gaussian as a standalone map representation, we integrate it into a volumetric mapping system to leverage geometric information. We propose employing a quadtree data structure on images to significantly reduce the number of initialized splats. This approach allows for the simultaneous generation of a concise 3D Gaussian map with fewer artifacts and a dynamic volumetric map. Our method, GSFusion, notably enhances computational efficiency without compromising rendering quality, as evidenced by results on both synthetic and real datasets. The code will be accessible at https://github.com/goldoak/GSFusion."}, "2309.14962": {"original_text": "  All tables can be represented as grids. Based on this observation, we propose\nGridFormer, a novel approach for interpreting unconstrained table structures by\npredicting the vertex and edge of a grid. First, we propose a flexible table\nrepresentation in the form of an MXN grid. In this representation, the vertexes\nand edges of the grid store the localization and adjacency information of the\ntable. Then, we introduce a DETR-style table structure recognizer to\nefficiently predict this multi-objective information of the grid in a single\nshot. Specifically, given a set of learned row and column queries, the\nrecognizer directly outputs the vertexes and edges information of the\ncorresponding rows and columns. Extensive experiments on five challenging\nbenchmarks which include wired, wireless, multi-merge-cell, oriented, and\ndistorted tables demonstrate the competitive performance of our model over\nother methods.\n", "rewritten_text": "All tables can be represented as grids. Building upon this observation, we introduce GridFormer, a novel approach for interpreting unconstrained table structures by predicting the vertices and edges of a grid. Initially, we propose a flexible table representation in the form of an MXN grid. In this representation, the vertices and edges of the grid store the localization and adjacency information of the table. Subsequently, we present a DETR-style table structure recognizer to efficiently predict this multi-objective information of the grid in a single shot. Specifically, by utilizing a set of learned row and column queries, the recognizer directly outputs the vertex and edge information of the corresponding rows and columns. Through extensive experiments on five challenging benchmarks, including wired, wireless, multi-merge-cell, oriented, and distorted tables, we demonstrate the competitive performance of our model compared to other methods."}, "2303.17658": {"original_text": "  Machine learning models deployed in the open world may encounter observations\nthat they were not trained to recognize, and they risk misclassifying such\nobservations with high confidence. Therefore, it is essential that these models\nare able to ascertain what is in-distribution (ID) and out-of-distribution\n(OOD), to avoid this misclassification. In recent years, huge strides have been\nmade in creating models that are robust to this distinction. As a result, the\ncurrent state-of-the-art has reached near perfect performance on relatively\ncoarse-grained OOD detection tasks, such as distinguishing horses from trucks,\nwhile struggling with finer-grained classification, like differentiating models\nof commercial aircraft. In this paper, we describe a new theoretical framework\nfor understanding fine- and coarse-grained OOD detection, we re-conceptualize\nfine grained classification into a three part problem, and we propose a new\nbaseline task for OOD models on two fine-grained hierarchical data sets, two\nnew evaluation methods to differentiate fine- and coarse-grained OOD\nperformance, along with a new loss function for models in this task.\n", "rewritten_text": "Machine learning models deployed in real-world scenarios may encounter unfamiliar observations that were not included in their training data, leading to the risk of misclassifying such observations with high confidence. Therefore, it is crucial for these models to be capable of distinguishing between in-distribution (ID) and out-of-distribution (OOD) data to prevent such misclassifications. Significant progress has been made in recent years in developing models that are resilient to this distinction. Consequently, the current state-of-the-art has achieved nearly perfect performance in coarse-grained OOD detection tasks, such as distinguishing between horses and trucks, while facing challenges in finer-grained classification tasks, such as differentiating between models of commercial aircraft.\n\nThis paper introduces a new theoretical framework for comprehending fine- and coarse-grained OOD detection. It re-conceptualizes fine-grained classification as a three-part problem and proposes a new baseline task for OOD models using two fine-grained hierarchical datasets. Additionally, two new evaluation methods are presented to differentiate between fine- and coarse-grained OOD performance, along with a novel loss function tailored for models in this task."}, "2109.00698": {"original_text": "  While conventional wisdom suggests that more aggressively filtering data from\nlow-quality sources like Common Crawl always monotonically improves the quality\nof training data, we find that aggressive filtering can in fact lead to a\ndecrease in model quality on a wide array of downstream tasks for a GPT-like\nlanguage model. We speculate that this is because optimizing sufficiently\nstrongly for a proxy metric harms performance on the true objective, suggesting\na need for more robust filtering objectives when attempting to filter more\naggressively. We hope this work leads to detailed analysis of the effects of\ndataset filtering design choices on downstream model performance in future\nwork.\n", "rewritten_text": "While conventional wisdom suggests that more aggressively filtering data from low-quality sources, such as Common Crawl, always improves the quality of training data in a consistent manner, our research reveals that aggressive filtering can actually result in a decrease in model quality across a variety of downstream tasks for a GPT-like language model. We hypothesize that this phenomenon occurs because optimizing too strongly for a proxy metric can negatively impact performance on the true objective. This suggests a necessity for more robust filtering objectives when pursuing aggressive filtering strategies. We anticipate that our findings will prompt further in-depth analysis of the impact of dataset filtering design choices on downstream model performance in future research endeavors."}, "1501.03952": {"original_text": "  Domain adaptation techniques aim at adapting a classifier learnt on a source\ndomain to work on the target domain. Exploiting the subspaces spanned by\nfeatures of the source and target domains respectively is one approach that has\nbeen investigated towards solving this problem. These techniques normally\nassume the existence of a single subspace for the entire source / target\ndomain. In this work, we consider the hierarchical organization of the data and\nconsider multiple subspaces for the source and target domain based on the\nhierarchy. We evaluate different subspace based domain adaptation techniques\nunder this setting and observe that using different subspaces based on the\nhierarchy yields consistent improvement over a non-hierarchical baseline\n", "rewritten_text": "Domain adaptation techniques aim to adapt a classifier learned on a source domain to function effectively on the target domain. One approach that has been explored to address this challenge involves leveraging the subspaces formed by the features of the source and target domains. Typically, these techniques assume the presence of a single subspace that encompasses the entire source/target domain.\n\nIn this study, we examine the hierarchical structure of the data and propose considering multiple subspaces for both the source and target domains based on this hierarchy. By evaluating various subspace-based domain adaptation techniques within this framework, we find that utilizing different subspaces according to the hierarchy consistently leads to improvements compared to a non-hierarchical baseline."}, "1707.00569": {"original_text": "  This survey presents a deep analysis of the learning and inference\ncapabilities in nine popular trackers. It is neither intended to study the\nwhole literature nor is it an attempt to review all kinds of neural networks\nproposed for visual tracking. We focus instead on Siamese neural networks which\nare a promising starting point for studying the challenging problem of\ntracking. These networks integrate efficiently feature learning and the\ntemporal matching and have so far shown state-of-the-art performance. In\nparticular, the branches of Siamese networks, their layers connecting these\nbranches, specific aspects of training and the embedding of these networks into\nthe tracker are highlighted. Quantitative results from existing papers are\ncompared with the conclusion that the current evaluation methodology shows\nproblems with the reproducibility and the comparability of results. The paper\nproposes a novel Lisp-like formalism for a better comparison of trackers. This\nassumes a certain functional design and functional decomposition of trackers.\nThe paper tries to give foundation for tracker design by a formulation of the\nproblem based on the theory of machine learning and by the interpretation of a\ntracker as a decision function. The work concludes with promising lines of\nresearch and suggests future work.\n", "rewritten_text": "This survey provides an in-depth analysis of the learning and inference capabilities in nine popular trackers. The intention is not to comprehensively cover all existing literature or review every type of neural network proposed for visual tracking. Instead, the focus is on Siamese neural networks, which serve as a promising starting point for addressing the challenging problem of tracking. These networks effectively integrate feature learning and temporal matching, demonstrating state-of-the-art performance thus far. The survey highlights various aspects of Siamese networks, including their branches, interconnecting layers, specific training methods, and the incorporation of these networks into trackers.\n\nComparative analysis of quantitative results from previous studies reveals issues with the reproducibility and comparability of results within the current evaluation methodology. To address this, the paper introduces a novel Lisp-like formalism aimed at enhancing the comparison of trackers. This formalism is based on a functional design and decomposition of trackers. Furthermore, the paper aims to lay the groundwork for tracker design by formulating the problem within the framework of machine learning theory and interpreting a tracker as a decision function.\n\nIn conclusion, the survey outlines potential avenues for future research and suggests areas for further exploration in the field."}, "2202.04897": {"original_text": "  Knowledge graph embedding (KGE) models learn the representation of entities\nand relations in knowledge graphs. Distance-based methods show promising\nperformance on link prediction task, which predicts the result by the distance\nbetween two entity representations. However, most of these methods represent\nthe head entity and tail entity separately, which limits the model capacity. We\npropose two novel distance-based methods named InterHT and InterHT+ that allow\nthe head and tail entities to interact better and get better entity\nrepresentation. Experimental results show that our proposed method achieves the\nbest results on ogbl-wikikg2 dataset.\n", "rewritten_text": "Knowledge graph embedding (KGE) models aim to learn the representation of entities and relations within knowledge graphs. Distance-based methods have demonstrated promising performance in tasks such as link prediction, where the prediction is based on the distance between two entity representations. However, many existing methods treat the head entity and tail entity as separate entities, which can limit the model's capacity. In this study, we introduce two novel distance-based methods, InterHT and InterHT+, which facilitate better interaction between the head and tail entities, resulting in improved entity representation. Experimental results indicate that our proposed method outperforms others on the ogbl-wikikg2 dataset."}, "1907.0203": {"original_text": "  Factchecking has always been a part of the journalistic process. However with\nnewsroom budgets shrinking it is coming under increasing pressure just as the\namount of false information circulating is on the rise. We therefore propose a\nmethod to increase the efficiency of the factchecking process, using the latest\ndevelopments in Natural Language Processing (NLP). This method allows us to\ncompare incoming claims to an existing corpus and return similar, factchecked,\nclaims in a live system-allowing factcheckers to work simultaneously without\nduplicating their work.\n", "rewritten_text": "Fact-checking has always been an integral part of the journalistic process. However, as newsroom budgets shrink, it is facing mounting pressure at a time when the volume of false information in circulation is increasing. To address this challenge, we propose a method to enhance the efficiency of the fact-checking process by leveraging the latest advancements in Natural Language Processing (NLP). This method enables us to compare incoming claims with an established corpus and promptly provide similar, fact-checked claims in a real-time system. This approach allows fact-checkers to collaborate simultaneously without duplicating their efforts."}, "2311.06377": {"original_text": "  Heaps' law is an empirical relation in text analysis that predicts vocabulary\ngrowth as a function of corpus size. While this law has been validated in\ndiverse human-authored text corpora, its applicability to large language model\ngenerated text remains unexplored. This study addresses this gap, focusing on\nthe emulation of corpora using the suite of GPT-Neo large language models. To\nconduct our investigation, we emulated corpora of PubMed abstracts using three\ndifferent parameter sizes of the GPT-Neo model. Our emulation strategy involved\nusing the initial five words of each PubMed abstract as a prompt and\ninstructing the model to expand the content up to the original abstract's\nlength. Our findings indicate that the generated corpora adhere to Heaps' law.\nInterestingly, as the GPT-Neo model size grows, its generated vocabulary\nincreasingly adheres to Heaps' law as as observed in human-authored text. To\nfurther improve the richness and authenticity of GPT-Neo outputs, future\niterations could emphasize enhancing model size or refining the model\narchitecture to curtail vocabulary repetition.\n", "rewritten_text": "Heaps' law is an empirical relation in text analysis that predicts vocabulary growth based on the size of the corpus. While this law has been confirmed in various human-authored text corpora, its applicability to large language model-generated text has not been thoroughly explored. This study aims to fill this gap by focusing on emulating corpora using the suite of GPT-Neo large language models. To conduct our investigation, we emulated corpora of PubMed abstracts using three different parameter sizes of the GPT-Neo model. Our emulation strategy involved using the first five words of each PubMed abstract as a prompt and instructing the model to expand the content to match the original abstract's length. Our findings suggest that the generated corpora adhere to Heaps' law. Interestingly, as the size of the GPT-Neo model increases, its generated vocabulary increasingly conforms to Heaps' law, similar to what is observed in human-authored text. To enhance the richness and authenticity of GPT-Neo outputs in future iterations, emphasis could be placed on increasing model size or refining the model architecture to reduce vocabulary repetition."}, "1809.07099": {"original_text": "  Deep neural networks have exhibited promising performance in image\nsuper-resolution (SR) due to the power in learning the non-linear mapping from\nlow-resolution (LR) images to high-resolution (HR) images. However, most deep\nlearning methods employ feed-forward architectures, and thus the dependencies\nbetween LR and HR images are not fully exploited, leading to limited learning\nperformance. Moreover, most deep learning based SR methods apply the pixel-wise\nreconstruction error as the loss, which, however, may fail to capture\nhigh-frequency information and produce perceptually unsatisfying results,\nwhilst the recent perceptual loss relies on some pre-trained deep model and\nthey may not generalize well. In this paper, we introduce a mask to separate\nthe image into low- and high-frequency parts based on image gradient magnitude,\nand then devise a gradient sensitive loss to well capture the structures in the\nimage without sacrificing the recovery of low-frequency content. Moreover, by\ninvestigating the duality in SR, we develop a dual reconstruction network (DRN)\nto improve the SR performance. We provide theoretical analysis on the\ngeneralization performance of our method and demonstrate its effectiveness and\nsuperiority with thorough experiments.\n", "rewritten_text": "Deep neural networks have shown promising performance in image super-resolution (SR) by effectively learning the non-linear mapping from low-resolution (LR) images to high-resolution (HR) images. However, many deep learning methods utilize feed-forward architectures, which do not fully exploit the dependencies between LR and HR images, resulting in limited learning performance. Additionally, most deep learning-based SR approaches rely on pixel-wise reconstruction error as the loss function, which may not adequately capture high-frequency information and can lead to perceptually unsatisfactory results. Recent perceptual loss methods depend on pre-trained deep models and may not generalize well.\n\nIn this study, we propose a novel approach where we introduce a mask to separate images into low- and high-frequency components based on image gradient magnitude. Subsequently, we design a gradient-sensitive loss function to effectively capture image structures without compromising the recovery of low-frequency content. Furthermore, by exploring the duality in SR, we develop a dual reconstruction network (DRN) to enhance SR performance. Theoretical analysis of the generalization performance of our method is provided, along with comprehensive experimental demonstrations showcasing its effectiveness and superiority."}, "2310.15052": {"original_text": "  Dataset distillation plays a crucial role in creating compact datasets with\nsimilar training performance compared with original large-scale ones. This is\nessential for addressing the challenges of data storage and training costs.\nPrevalent methods facilitate knowledge transfer by matching the gradients,\nembedding distributions, or training trajectories of synthetic images with\nthose of the sampled original images. Although there are various matching\nobjectives, currently the strategy for selecting original images is limited to\nnaive random sampling. We argue that random sampling overlooks the evenness of\nthe selected sample distribution, which may result in noisy or biased matching\ntargets. Besides, the sample diversity is also not constrained by random\nsampling. Additionally, current methods predominantly focus on\nsingle-dimensional matching, where information is not fully utilized. To\naddress these challenges, we propose a novel matching strategy called Dataset\nDistillation by Bidirectional REpresentAtive Matching (DREAM+), which selects\nrepresentative original images for bidirectional matching. DREAM+ is applicable\nto a variety of mainstream dataset distillation frameworks and significantly\nreduces the number of distillation iterations by more than 15 times without\naffecting performance. Given sufficient training time, DREAM+ can further\nimprove the performance and achieve state-of-the-art results. We have released\nthe code at github.com/NUS-HPC-AI-Lab/DREAM+.\n", "rewritten_text": "Dataset distillation is crucial for creating compact datasets that maintain similar training performance to original large-scale datasets. This is essential for addressing challenges related to data storage and training costs. Current methods facilitate knowledge transfer by aligning gradients, embedding distributions, or training trajectories of synthetic images with those of sampled original images. However, the strategy for selecting original images is currently limited to naive random sampling. We argue that random sampling overlooks the evenness of the sample distribution, potentially leading to noisy or biased matching targets. Furthermore, random sampling does not ensure sample diversity. Additionally, existing methods primarily focus on single-dimensional matching, resulting in underutilization of information.\n\nTo tackle these challenges, we propose a novel matching strategy called Dataset Distillation by Bidirectional REpresentAtive Matching (DREAM+). DREAM+ selects representative original images for bidirectional matching and can be integrated into various mainstream dataset distillation frameworks. Notably, DREAM+ significantly reduces the number of distillation iterations by over 15 times without compromising performance. With sufficient training time, DREAM+ can further enhance performance and achieve state-of-the-art results. The code for DREAM+ is available at github.com/NUS-HPC-AI-Lab/DREAM+."}, "2207.09625": {"original_text": "  Given an image and a reference caption, the image caption editing task aims\nto correct the misalignment errors and generate a refined caption. However, all\nexisting caption editing works are implicit models, ie, they directly produce\nthe refined captions without explicit connections to the reference captions. In\nthis paper, we introduce a new task: Explicit Caption Editing (ECE). ECE models\nexplicitly generate a sequence of edit operations, and this edit operation\nsequence can translate the reference caption into a refined one. Compared to\nthe implicit editing, ECE has multiple advantages: 1) Explainable: it can trace\nthe whole editing path. 2) Editing Efficient: it only needs to modify a few\nwords. 3) Human-like: it resembles the way that humans perform caption editing,\nand tries to keep original sentence structures. To solve this new task, we\npropose the first ECE model: TIger. TIger is a non-autoregressive\ntransformer-based model, consisting of three modules: Tagger_del, Tagger_add,\nand Inserter. Specifically, Tagger_del decides whether each word should be\npreserved or not, Tagger_add decides where to add new words, and Inserter\npredicts the specific word for adding. To further facilitate ECE research, we\npropose two new ECE benchmarks by re-organizing two existing datasets, dubbed\nCOCO-EE and Flickr30K-EE, respectively. Extensive ablations on both two\nbenchmarks have demonstrated the effectiveness of TIger.\n", "rewritten_text": "Given an image and a reference caption, the task of image caption editing aims to correct misalignment errors and generate a refined caption. However, all existing caption editing works are implicit models, meaning they directly produce refined captions without explicit connections to the reference captions. In this paper, we introduce a new task: Explicit Caption Editing (ECE). ECE models explicitly generate a sequence of edit operations, and this edit operation sequence can translate the reference caption into a refined one. Compared to implicit editing, ECE offers multiple advantages: \n\n1) Explainable: it can trace the entire editing path.\n2) Editing Efficiency: it only needs to modify a few words.\n3) Human-like: it resembles the way humans perform caption editing and strives to maintain the original sentence structures.\n\nTo address this new task, we present the first ECE model: TIger. TIger is a non-autoregressive transformer-based model comprising three modules: Tagger_del, Tagger_add, and Inserter. Specifically, Tagger_del determines whether each word should be preserved, Tagger_add decides where to add new words, and Inserter predicts the specific word for addition. \n\nTo further advance ECE research, we propose two new ECE benchmarks by reorganizing two existing datasets, named COCO-EE and Flickr30K-EE, respectively. Extensive ablations on both benchmarks have demonstrated the effectiveness of TIger."}, "1805.05503": {"original_text": "  Human faces are one interesting object class with numerous applications.\nWhile significant progress has been made in the generic deblurring problem,\nexisting methods are less effective for blurry face images. The success of the\nstate-of-the-art image deblurring algorithms stems mainly from implicit or\nexplicit restoration of salient edges for kernel estimation. However, existing\nmethods are less effective as only few edges can be restored from blurry face\nimages for kernel estimation. In this paper, we address the problem of\ndeblurring face images by exploiting facial structures. We propose a deblurring\nalgorithm based on an exemplar dataset without using coarse-to-fine strategies\nor heuristic edge selections. In addition, we develop a convolutional neural\nnetwork to restore sharp edges from blurry images for deblurring. Extensive\nexperiments against the state-of-the-art methods demonstrate the effectiveness\nof the proposed algorithms for deblurring face images. In addition, we show the\nproposed algorithms can be applied to image deblurring for other object\nclasses.\n", "rewritten_text": "Human faces represent an intriguing object class with a wide range of applications. While significant advancements have been achieved in addressing the generic deblurring problem, current methods exhibit limited effectiveness when applied to blurry face images. The success of cutting-edge image deblurring algorithms primarily lies in the implicit or explicit restoration of prominent edges for kernel estimation. However, these methods fall short in restoring only a few edges from blurry face images for accurate kernel estimation.\n\nIn this study, we tackle the challenge of deblurring face images by leveraging facial structures. Our approach introduces a deblurring algorithm based on an exemplar dataset, eliminating the need for coarse-to-fine strategies or heuristic edge selections. Furthermore, we have developed a convolutional neural network specifically designed to recover sharp edges from blurry images for effective deblurring.\n\nExtensive experiments comparing our proposed algorithms with state-of-the-art methods showcase their superior effectiveness in deblurring face images. Moreover, we demonstrate the versatility of our algorithms by applying them to image deblurring for other object classes."}, "2205.09299": {"original_text": "  Convolutional Neural Networks (CNNs) have achieved promising results in\nmedical image segmentation. However, CNNs require lots of training data and are\nincapable of handling pose and deformation of objects. Furthermore, their\npooling layers tend to discard important information such as positions as well\nas CNNs are sensitive to rotation and affine transformation. Capsule network is\na recent new architecture that has achieved better robustness in part-whole\nrepresentation learning by replacing pooling layers with dynamic routing and\nconvolutional strides, which has shown potential results on popular tasks such\nas digit classification and object segmentation. In this paper, we propose a 3D\nencoder-decoder network with Convolutional Capsule Encoder (called 3DConvCaps)\nto learn lower-level features (short-range attention) with convolutional layers\nwhile modeling the higher-level features (long-range dependence) with capsule\nlayers. Our experiments on multiple datasets including iSeg-2017, Hippocampus,\nand Cardiac demonstrate that our 3D 3DConvCaps network considerably outperforms\nprevious capsule networks and 3D-UNets. We further conduct ablation studies of\nnetwork efficiency and segmentation performance under various configurations of\nconvolution layers and capsule layers at both contracting and expanding paths.\n", "rewritten_text": "Convolutional Neural Networks (CNNs) have shown promising results in medical image segmentation. However, CNNs require a large amount of training data and struggle with handling object pose and deformation. Additionally, their pooling layers often discard crucial information, such as object positions, and are sensitive to rotation and affine transformations. \n\nThe Capsule network is a novel architecture that offers improved robustness in part-whole representation learning by replacing pooling layers with dynamic routing and convolutional strides. This approach has demonstrated potential in tasks like digit classification and object segmentation. \n\nIn this study, we introduce a 3D encoder-decoder network featuring a Convolutional Capsule Encoder, referred to as 3DConvCaps. This network aims to capture lower-level features (short-range attention) using convolutional layers while modeling higher-level features (long-range dependence) with capsule layers. Our experiments, conducted on various datasets including iSeg-2017, Hippocampus, and Cardiac, reveal that our 3DConvCaps network significantly outperforms previous capsule networks and 3D-UNets. \n\nFurthermore, we conduct ablation studies to analyze network efficiency and segmentation performance under different configurations of convolution and capsule layers in both contracting and expanding paths."}, "2407.08290": {"original_text": "  Recent advances in mobile mapping systems have greatly enhanced the\nefficiency and convenience of acquiring urban 3D data. These systems utilize\nLiDAR sensors mounted on vehicles to capture vast cityscapes. However, a\nsignificant challenge arises due to occlusions caused by roadside parked\nvehicles, leading to the loss of scene information, particularly on the roads,\nsidewalks, curbs, and the lower sections of buildings. In this study, we\npresent a novel approach that leverages deep neural networks to learn a model\ncapable of filling gaps in urban scenes that are obscured by vehicle occlusion.\nWe have developed an innovative technique where we place virtual vehicle models\nalong road boundaries in the gap-free scene and utilize a ray-casting algorithm\nto create a new scene with occluded gaps. This allows us to generate diverse\nand realistic urban point cloud scenes with and without vehicle occlusion,\nsurpassing the limitations of real-world training data collection and\nannotation. Furthermore, we introduce the Scene Gap Completion Network\n(SGC-Net), an end-to-end model that can generate well-defined shape boundaries\nand smooth surfaces within occluded gaps. The experiment results reveal that\n97.66% of the filled points fall within a range of 5 centimeters relative to\nthe high-density ground truth point cloud scene. These findings underscore the\nefficacy of our proposed model in gap completion and reconstructing urban\nscenes affected by vehicle occlusions.\n", "rewritten_text": "Recent advancements in mobile mapping systems have significantly improved the efficiency and convenience of acquiring urban 3D data. These systems employ LiDAR sensors mounted on vehicles to capture extensive cityscapes. However, a notable challenge arises from occlusions caused by parked vehicles along roadsides, resulting in the loss of scene information, particularly on roads, sidewalks, curbs, and the lower portions of buildings. \n\nIn this study, we introduce a novel approach that utilizes deep neural networks to develop a model capable of filling gaps in urban scenes obscured by vehicle occlusions. Our innovative technique involves placing virtual vehicle models along road boundaries in the gap-free scene and employing a ray-casting algorithm to generate a new scene with occluded gaps filled. This methodology enables the creation of diverse and realistic urban point cloud scenes with and without vehicle occlusions, surpassing the constraints of real-world training data collection and annotation.\n\nAdditionally, we present the Scene Gap Completion Network (SGC-Net), an end-to-end model that can produce well-defined shape boundaries and smooth surfaces within occluded gaps. Experimental results demonstrate that 97.66% of the filled points fall within a 5-centimeter range relative to the high-density ground truth point cloud scene. These findings highlight the effectiveness of our proposed model in completing gaps and reconstructing urban scenes impacted by vehicle occlusions."}, "2004.04699": {"original_text": "  Deep Neural Networks trained in a fully supervised fashion are the dominant\ntechnology in perception-based autonomous driving systems. While collecting\nlarge amounts of unlabeled data is already a major undertaking, only a subset\nof it can be labeled by humans due to the effort needed for high-quality\nannotation. Therefore, finding the right data to label has become a key\nchallenge. Active learning is a powerful technique to improve data efficiency\nfor supervised learning methods, as it aims at selecting the smallest possible\ntraining set to reach a required performance. We have built a scalable\nproduction system for active learning in the domain of autonomous driving. In\nthis paper, we describe the resulting high-level design, sketch some of the\nchallenges and their solutions, present our current results at scale, and\nbriefly describe the open problems and future directions.\n", "rewritten_text": "Deep Neural Networks trained in a fully supervised manner are the predominant technology in perception-based autonomous driving systems. While gathering substantial amounts of unlabeled data is already a significant endeavor, only a portion of it can be annotated by humans due to the considerable effort required for high-quality labeling. Consequently, identifying the appropriate data for annotation has emerged as a critical challenge. Active learning stands out as a potent technique for enhancing data efficiency in supervised learning approaches by focusing on selecting the smallest training set necessary to achieve a desired performance level. We have developed a scalable production system for active learning within the realm of autonomous driving. This paper outlines the resulting high-level design, addresses some of the challenges encountered along with their solutions, showcases our current results at scale, and briefly touches upon the unresolved issues and future directions."}, "1808.01491": {"original_text": "  Single image rain streaks removal has recently witnessed substantial progress\ndue to the development of deep convolutional neural networks. However, existing\ndeep learning based methods either focus on the entrance and exit of the\nnetwork by decomposing the input image into high and low frequency information\nand employing residual learning to reduce the mapping range, or focus on the\nintroduction of cascaded learning scheme to decompose the task of rain streaks\nremoval into multi-stages. These methods treat the convolutional neural network\nas an encapsulated end-to-end mapping module without deepening into the\nrationality and superiority of neural network design. In this paper, we delve\ninto an effective end-to-end neural network structure for stronger feature\nexpression and spatial correlation learning. Specifically, we propose a\nnon-locally enhanced encoder-decoder network framework, which consists of a\npooling indices embedded encoder-decoder network to efficiently learn\nincreasingly abstract feature representation for more accurate rain streaks\nmodeling while perfectly preserving the image detail. The proposed\nencoder-decoder framework is composed of a series of non-locally enhanced dense\nblocks that are designed to not only fully exploit hierarchical features from\nall the convolutional layers but also well capture the long-distance\ndependencies and structural information. Extensive experiments on synthetic and\nreal datasets demonstrate that the proposed method can effectively remove\nrain-streaks on rainy image of various densities while well preserving the\nimage details, which achieves significant improvements over the recent\nstate-of-the-art methods.\n", "rewritten_text": "Recent advancements in single-image rain streak removal have been significant, largely attributed to the development of deep convolutional neural networks. However, current deep learning approaches for this task typically either focus on the input and output of the network by breaking down the image into high and low frequency components and utilizing residual learning to narrow the mapping range, or they employ a cascaded learning scheme to break down rain streak removal into multiple stages. These methods often treat the convolutional neural network as a self-contained end-to-end mapping module without delving deeply into the rationale and effectiveness of neural network design.\n\nIn this study, we explore an efficient end-to-end neural network structure for enhanced feature expression and spatial correlation learning. Specifically, we introduce a non-locally enhanced encoder-decoder network framework, comprising a pooling indices embedded encoder-decoder network that effectively learns increasingly abstract feature representations for more precise rain streak modeling while maintaining image details. The proposed encoder-decoder framework incorporates a series of non-locally enhanced dense blocks, designed to fully leverage hierarchical features from all convolutional layers and capture long-distance dependencies and structural information.\n\nExtensive experiments conducted on synthetic and real datasets demonstrate the effectiveness of our proposed method in removing rain streaks from images with varying densities while preserving image details. Our approach yields significant improvements over recent state-of-the-art methods."}, "2303.15698": {"original_text": "  Standard deep learning models such as convolutional neural networks (CNNs)\nlack the ability of generalizing to domains which have not been seen during\ntraining. This problem is mainly due to the common but often wrong assumption\nof such models that the source and target data come from the same i.i.d.\ndistribution. Recently, Vision Transformers (ViTs) have shown outstanding\nperformance for a broad range of computer vision tasks. However, very few\nstudies have investigated their ability to generalize to new domains. This\npaper presents a first Token-level Feature Stylization (TFS-ViT) approach for\ndomain generalization, which improves the performance of ViTs to unseen data by\nsynthesizing new domains. Our approach transforms token features by mixing the\nnormalization statistics of images from different domains. We further improve\nthis approach with a novel strategy for attention-aware stylization, which uses\nthe attention maps of class (CLS) tokens to compute and mix normalization\nstatistics of tokens corresponding to different image regions. The proposed\nmethod is flexible to the choice of backbone model and can be easily applied to\nany ViT-based architecture with a negligible increase in computational\ncomplexity. Comprehensive experiments show that our approach is able to achieve\nstate-of-the-art performance on five challenging benchmarks for domain\ngeneralization, and demonstrate its ability to deal with different types of\ndomain shifts. The implementation is available at:\nhttps://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.\n", "rewritten_text": "Standard deep learning models, such as convolutional neural networks (CNNs), often struggle to generalize to unseen domains due to the incorrect assumption that the source and target data share the same i.i.d. distribution. Vision Transformers (ViTs) have recently demonstrated exceptional performance across various computer vision tasks. However, limited research has explored their ability to generalize to new domains. This paper introduces the Token-level Feature Stylization (TFS-ViT) approach for domain generalization, enhancing ViTs' performance on unseen data by synthesizing new domains. Our method involves transforming token features through the mixing of normalization statistics from images across different domains. Additionally, we enhance this approach with a novel attention-aware stylization strategy, leveraging attention maps of class (CLS) tokens to calculate and blend normalization statistics of tokens corresponding to distinct image regions. The proposed technique is adaptable to different backbone models and can be seamlessly integrated into any ViT-based architecture with minimal computational overhead. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on five challenging benchmarks for domain generalization, showcasing its effectiveness in handling various types of domain shifts. The implementation can be accessed at: https://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization."}, "1804.09555": {"original_text": "  Outdoor vision-based systems suffer from atmospheric turbulences, and rain is\none of the worst factors for vision degradation. Current rain removal methods\nshow limitations either for complex dynamic scenes, or under torrential rain\nwith opaque occlusions. We propose a novel derain framework which applies\nsuperpixel (SP) segmentation to decompose the scene into depth consistent\nunits. Alignment of scene contents are done at the SP level, which proves to be\nrobust against rain occlusion interferences and fast camera motion. Two\nalignment output tensors, i.e., optimal temporal match tensor and sorted\nspatial-temporal match tensor, provide informative clues for the location of\nrain streaks and the occluded background contents. Different classical and\nnovel methods such as Robust Principle Component Analysis and Convolutional\nNeural Networks are applied and compared for their respective advantages in\nefficiently exploiting the rich spatial-temporal features provided by the two\ntensors. Extensive evaluations show that advantage of up to 5dB is achieved on\nthe scene restoration PSNR over state-of-the-art methods, and the advantage is\nespecially obvious with highly complex and dynamic scenes. Visual evaluations\nshow that the proposed framework is not only able to suppress heavy and opaque\noccluding rain streaks, but also large semi-transparent regional fluctuations\nand distortions.\n", "rewritten_text": "Outdoor vision-based systems are often affected by atmospheric turbulences, with rain being a particularly detrimental factor leading to vision degradation. Existing methods for rain removal have shown limitations in handling either complex dynamic scenes or scenarios with heavy rain and opaque occlusions. In response to these challenges, we introduce a novel derain framework that leverages superpixel (SP) segmentation to break down the scene into depth-consistent units. By aligning the scene contents at the SP level, our approach demonstrates robustness against rain occlusions and rapid camera movements.\n\nThe framework produces two key output tensors: an optimal temporal match tensor and a sorted spatial-temporal match tensor. These tensors offer valuable insights into the identification of rain streaks and the recovery of occluded background details. We explore a range of classical and innovative techniques, including Robust Principle Component Analysis and Convolutional Neural Networks, to effectively utilize the rich spatial-temporal features extracted from the two tensors. Through extensive evaluations, we have achieved a significant improvement of up to 5dB in scene restoration PSNR compared to state-of-the-art methods, particularly benefiting highly complex and dynamic scenes.\n\nVisual assessments confirm that our proposed framework excels in suppressing heavy, opaque rain streaks as well as large semi-transparent regional fluctuations and distortions."}, "2005.00975": {"original_text": "  In the deep learning (DL) era, parsing models are extremely simplified with\nlittle hurt on performance, thanks to the remarkable capability of multi-layer\nBiLSTMs in context representation. As the most popular graph-based dependency\nparser due to its high efficiency and performance, the biaffine parser directly\nscores single dependencies under the arc-factorization assumption, and adopts a\nvery simple local token-wise cross-entropy training loss. This paper for the\nfirst time presents a second-order TreeCRF extension to the biaffine parser.\nFor a long time, the complexity and inefficiency of the inside-outside\nalgorithm hinder the popularity of TreeCRF. To address this issue, we propose\nan effective way to batchify the inside and Viterbi algorithms for direct large\nmatrix operation on GPUs, and to avoid the complex outside algorithm via\nefficient back-propagation. Experiments and analysis on 27 datasets from 13\nlanguages clearly show that techniques developed before the DL era, such as\nstructural learning (global TreeCRF loss) and high-order modeling are still\nuseful, and can further boost parsing performance over the state-of-the-art\nbiaffine parser, especially for partially annotated training data. We release\nour code at https://github.com/yzhangcs/crfpar.\n", "rewritten_text": "In the era of deep learning (DL), parsing models have been greatly simplified without compromising performance, thanks to the impressive capabilities of multi-layer BiLSTMs in context representation. The biaffine parser, known for its high efficiency and performance as the most popular graph-based dependency parser, directly scores single dependencies based on the arc-factorization assumption and utilizes a straightforward local token-wise cross-entropy training loss. This paper introduces a novel second-order TreeCRF extension to the biaffine parser.\n\nHistorically, the complexity and inefficiency of the inside-outside algorithm have impeded the widespread adoption of TreeCRF. To tackle this challenge, we propose a method to batch process the inside and Viterbi algorithms for efficient large matrix operations on GPUs, thereby bypassing the need for the intricate outside algorithm through streamlined back-propagation.\n\nExperiments and analyses conducted on 27 datasets across 13 languages unequivocally demonstrate that pre-DL era techniques, such as structural learning (global TreeCRF loss) and high-order modeling, remain valuable and can significantly enhance parsing performance beyond that of the cutting-edge biaffine parser, particularly when dealing with partially annotated training data. Our code is publicly available at https://github.com/yzhangcs/crfpar."}, "1812.00108": {"original_text": "  With vast amounts of video content being uploaded to the Internet every\nminute, video summarization becomes critical for efficient browsing, searching,\nand indexing of visual content. Nonetheless, the spread of social and\negocentric cameras creates an abundance of sparse scenarios captured by several\ndevices, and ultimately required to be jointly summarized. In this paper, we\ndiscuss the problem of summarizing videos recorded independently by several\ndynamic cameras that intermittently share the field of view. We present a\nrobust framework that (a) identifies a diverse set of important events among\nmoving cameras that often are not capturing the same scene, and (b) selects the\nmost representative view(s) at each event to be included in a universal\nsummary. Due to the lack of an applicable alternative, we collected a new\nmulti-view egocentric dataset, Multi-Ego. Our dataset is recorded\nsimultaneously by three cameras, covering a wide variety of real-life\nscenarios. The footage is annotated by multiple individuals under various\nsummarization configurations, with a consensus analysis ensuring a reliable\nground truth. We conduct extensive experiments on the compiled dataset in\naddition to three other standard benchmarks that show the robustness and the\nadvantage of our approach in both supervised and unsupervised settings.\nAdditionally, we show that our approach learns collectively from data of varied\nnumber-of-views and orthogonal to other summarization methods, deeming it\nscalable and generic.\n", "rewritten_text": "With a vast amount of video content being uploaded to the Internet every minute, video summarization is crucial for efficient browsing, searching, and indexing of visual content. However, the proliferation of social and egocentric cameras has led to an abundance of sparse scenarios captured by multiple devices, necessitating joint summarization. This paper addresses the challenge of summarizing videos recorded independently by dynamic cameras that intermittently share the field of view. A robust framework is presented that (a) identifies a diverse set of important events among moving cameras that may not always capture the same scene, and (b) selects the most representative view(s) at each event to be included in a universal summary.\n\nGiven the lack of a suitable alternative, a new multi-view egocentric dataset, Multi-Ego, was collected. This dataset was simultaneously recorded by three cameras, encompassing a wide range of real-life scenarios. The footage is annotated by multiple individuals under various summarization configurations, with a consensus analysis ensuring a reliable ground truth. Extensive experiments were conducted on the compiled dataset, as well as on three other standard benchmarks, demonstrating the robustness and advantages of the proposed approach in both supervised and unsupervised settings.\n\nFurthermore, it is shown that the approach learns collectively from data with varying numbers of views and is orthogonal to other summarization methods, making it scalable and generic."}, "2012.07791": {"original_text": "  We propose real-time, six degrees of freedom (6DoF), 3D face pose estimation\nwithout face detection or landmark localization. We observe that estimating the\n6DoF rigid transformation of a face is a simpler problem than facial landmark\ndetection, often used for 3D face alignment. In addition, 6DoF offers more\ninformation than face bounding box labels. We leverage these observations to\nmake multiple contributions: (a) We describe an easily trained, efficient,\nFaster R-CNN--based model which regresses 6DoF pose for all faces in the photo,\nwithout preliminary face detection. (b) We explain how pose is converted and\nkept consistent between the input photo and arbitrary crops created while\ntraining and evaluating our model. (c) Finally, we show how face poses can\nreplace detection bounding box training labels. Tests on AFLW2000-3D and BIWI\nshow that our method runs at real-time and outperforms state of the art (SotA)\nface pose estimators. Remarkably, our method also surpasses SotA models of\ncomparable complexity on the WIDER FACE detection benchmark, despite not been\noptimized on bounding box labels.\n", "rewritten_text": "We present a method for real-time, 6 degrees of freedom (6DoF), 3D face pose estimation that does not rely on face detection or landmark localization. Our approach focuses on estimating the 6DoF rigid transformation of a face, which we find to be a simpler task compared to facial landmark detection commonly used for 3D face alignment. Moreover, the 6DoF approach provides more detailed information than face bounding box labels. Leveraging these insights, we make several key contributions:\n\n(a) We introduce an easily trainable and efficient model based on Faster R-CNN that can accurately regress 6DoF pose for all faces in an image without the need for prior face detection.\n\n(b) We describe the process of converting and maintaining pose consistency between the original image and arbitrary crops generated during model training and evaluation.\n\n(c) We demonstrate how face poses can effectively replace detection bounding box labels for training purposes.\n\nOur method has been tested on datasets such as AFLW2000-3D and BIWI, showing real-time performance and superior results compared to state-of-the-art face pose estimators. Notably, our approach also outperforms models of similar complexity on the WIDER FACE detection benchmark, despite not being specifically optimized for bounding box labels."}, "2208.05909": {"original_text": "  Preservation of domain knowledge from the source to target is crucial in any\ntranslation workflow. It is common in the translation industry to receive\nhighly specialized projects, where there is hardly any parallel in-domain data.\nIn such scenarios where there is insufficient in-domain data to fine-tune\nMachine Translation (MT) models, producing translations that are consistent\nwith the relevant context is challenging. In this work, we propose a novel\napproach to domain adaptation leveraging state-of-the-art pretrained language\nmodels (LMs) for domain-specific data augmentation for MT, simulating the\ndomain characteristics of either (a) a small bilingual dataset, or (b) the\nmonolingual source text to be translated. Combining this idea with\nback-translation, we can generate huge amounts of synthetic bilingual in-domain\ndata for both use cases. For our investigation, we use the state-of-the-art\nTransformer architecture. We employ mixed fine-tuning to train models that\nsignificantly improve translation of in-domain texts. More specifically, in\nboth scenarios, our proposed methods achieve improvements of approximately 5-6\nBLEU and 2-3 BLEU, respectively, on the Arabic-to-English and English-to-Arabic\nlanguage pairs. Furthermore, the outcome of human evaluation corroborates the\nautomatic evaluation results.\n", "rewritten_text": "Preserving domain knowledge from the source to the target is crucial in any translation workflow. It is common in the translation industry to receive highly specialized projects where there is little to no parallel in-domain data available. In such scenarios, where there is insufficient in-domain data to fine-tune Machine Translation (MT) models, producing translations that are consistent with the relevant context becomes challenging.\n\nIn this study, we introduce a novel approach to domain adaptation by utilizing state-of-the-art pretrained language models (LMs) for domain-specific data augmentation in MT. This involves simulating the domain characteristics of either a small bilingual dataset or the monolingual source text to be translated. By combining this approach with back-translation, we are able to generate significant amounts of synthetic bilingual in-domain data for both scenarios.\n\nFor our research, we leverage the Transformer architecture, employing mixed fine-tuning to train models that notably enhance the translation of in-domain texts. Specifically, our proposed methods achieve improvements of approximately 5-6 BLEU and 2-3 BLEU on the Arabic-to-English and English-to-Arabic language pairs, respectively. Additionally, the results of human evaluation support the findings of automatic evaluation."}, "1811.09745": {"original_text": "  We propose a new geometric regularization principle for reconstructing vector\nfields based on prior knowledge about their divergence. As one important\nexample of this general idea, we focus on vector fields modelling blood flow\npattern that should be divergent in arteries and convergent in veins. We show\nthat this previously ignored regularization constraint can significantly\nimprove the quality of vessel tree reconstruction particularly around\nbifurcations where non-zero divergence is concentrated. Our divergence prior is\ncritical for resolving (binary) sign ambiguity in flow orientations produced by\nstandard vessel filters, e.g. Frangi. Our vessel tree centerline reconstruction\ncombines divergence constraints with robust curvature regularization. Our\nunsupervised method can reconstruct complete vessel trees with near-capillary\ndetails on synthetic and real 3D volumes.\n", "rewritten_text": "We present a novel geometric regularization principle for reconstructing vector fields by leveraging prior knowledge of their divergence. A key application of this principle involves modeling blood flow patterns, where arteries exhibit divergence and veins exhibit convergence. By incorporating this previously overlooked regularization constraint, we demonstrate a significant enhancement in the accuracy of vessel tree reconstruction, particularly at bifurcations where non-zero divergence is prominent. Our divergence prior plays a crucial role in resolving sign ambiguities in flow orientations generated by standard vessel filters like Frangi. The reconstruction of vessel tree centerlines integrates divergence constraints with robust curvature regularization. Our unsupervised approach enables the reconstruction of complete vessel trees with intricate details down to near-capillary levels in both synthetic and real 3D volumes."}, "2104.08575": {"original_text": "  Super-resolution (SR) is an ill-posed problem, which means that infinitely\nmany high-resolution (HR) images can be degraded to the same low-resolution\n(LR) image. To study the one-to-many stochastic SR mapping, we implicitly\nrepresent the non-local self-similarity of natural images and develop a\nVariational Sparse framework for Super-Resolution (VSpSR) via neural networks.\nSince every small patch of a HR image can be well approximated by the sparse\nrepresentation of atoms in an over-complete dictionary, we design a two-branch\nmodule, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM\nextracts patch-level basis from the LR input, and the other branch infers\npixel-wise variational distributions with respect to the sparse coefficients.\nBy repeatedly sampling coefficients, we could obtain infinite sparse\nrepresentations, and thus generate diverse HR images. According to the\npreliminary results of NTIRE 2021 challenge on learning SR space, our team\n(FudanZmic21) ranks 7-th in terms of released scores. The implementation of\nVSpSR is released at https://zmiclab.github.io/.\n", "rewritten_text": "Super-resolution (SR) is a challenging problem known for its ill-posed nature, where numerous high-resolution (HR) images can be degraded to the same low-resolution (LR) image. To investigate the stochastic SR mapping that leads to multiple potential HR images, we leverage the inherent non-local self-similarity found in natural images and introduce a Variational Sparse framework for Super-Resolution (VSpSR) using neural networks. Given that each small patch of a HR image can be effectively approximated by the sparse representation of atoms in an over-complete dictionary, we introduce a two-branch module, referred to as VSpM, to explore the SR space. Specifically, one branch of VSpM extracts patch-level basis from the LR input, while the other branch infers pixel-wise variational distributions based on the sparse coefficients. By iteratively sampling coefficients, we are able to generate an infinite variety of sparse representations, resulting in diverse HR images. In the recent NTIRE 2021 challenge focusing on learning the SR space, our team (FudanZmic21) achieved the 7th rank based on the released scores. The implementation of VSpSR can be accessed at https://zmiclab.github.io/."}, "2206.12505": {"original_text": "  We propose a novel semi-supervised learning approach for classification of\nhistopathology images. We employ strong supervision with patch-level\nannotations combined with a novel co-training loss to create a semi-supervised\nlearning framework. Co-training relies on multiple conditionally independent\nand sufficient views of the data. We separate the hematoxylin and eosin\nchannels in pathology images using color deconvolution to create two views of\neach slide that can partially fulfill these requirements. Two separate CNNs are\nused to embed the two views into a joint feature space. We use a contrastive\nloss between the views in this feature space to implement co-training. We\nevaluate our approach in clear cell renal cell and prostate carcinomas, and\ndemonstrate improvement over state-of-the-art semi-supervised learning methods.\n", "rewritten_text": "We present a new semi-supervised learning method for classifying histopathology images. Our approach combines strong supervision with patch-level annotations and introduces a novel co-training loss to establish a semi-supervised learning framework. The co-training method leverages multiple conditionally independent and sufficient data views. To achieve this, we separate the hematoxylin and eosin channels in pathology images using color deconvolution, creating two distinct views of each slide that partially meet these criteria. Two separate convolutional neural networks (CNNs) are employed to map the two views into a shared feature space. A contrastive loss is applied between the views in this feature space to facilitate co-training. We assess the effectiveness of our method on clear cell renal cell and prostate carcinomas, showcasing enhancements over existing state-of-the-art semi-supervised learning techniques."}, "2303.08714": {"original_text": "  Adapting the Diffusion Probabilistic Model (DPM) for direct image\nsuper-resolution is wasteful, given that a simple Convolutional Neural Network\n(CNN) can recover the main low-frequency content. Therefore, we present\nResDiff, a novel Diffusion Probabilistic Model based on Residual structure for\nSingle Image Super-Resolution (SISR). ResDiff utilizes a combination of a CNN,\nwhich restores primary low-frequency components, and a DPM, which predicts the\nresidual between the ground-truth image and the CNN predicted image. In\ncontrast to the common diffusion-based methods that directly use LR images to\nguide the noise towards HR space, ResDiff utilizes the CNN's initial prediction\nto direct the noise towards the residual space between HR space and\nCNN-predicted space, which not only accelerates the generation process but also\nacquires superior sample quality. Additionally, a frequency-domain-based loss\nfunction for CNN is introduced to facilitate its restoration, and a\nfrequency-domain guided diffusion is designed for DPM on behalf of predicting\nhigh-frequency details. The extensive experiments on multiple benchmark\ndatasets demonstrate that ResDiff outperforms previous diffusion based methods\nin terms of shorter model convergence time, superior generation quality, and\nmore diverse samples.\n", "rewritten_text": "Adapting the Diffusion Probabilistic Model (DPM) for direct image super-resolution is deemed inefficient, as a simple Convolutional Neural Network (CNN) can effectively recover the primary low-frequency content. Therefore, we introduce ResDiff, a novel Diffusion Probabilistic Model based on a Residual structure for Single Image Super-Resolution (SISR). ResDiff combines a CNN, which restores the main low-frequency components, with a DPM that predicts the residual between the ground-truth image and the CNN-predicted image. Unlike conventional diffusion-based methods that use LR images to guide noise towards HR space, ResDiff leverages the initial prediction of the CNN to direct noise towards the residual space between HR space and the CNN-predicted space. This approach not only speeds up the generation process but also enhances sample quality. Furthermore, a frequency-domain-based loss function is introduced for the CNN to aid in its restoration, and a frequency-domain guided diffusion is devised for the DPM to predict high-frequency details. Extensive experiments on various benchmark datasets demonstrate that ResDiff surpasses previous diffusion-based methods in terms of quicker model convergence, superior generation quality, and a wider range of samples."}, "1607.06797": {"original_text": "  In this paper we proposed an ordered patch based method using Conditional\nRandom Field (CRF) in order to encode local properties and their spatial\nrelationship in images to address texture classification, face recognition, and\nscene classification problems. Typical image classification approaches work\nwithout considering spatial causality among distinctive properties of an image\nfor image representation in feature space. In this method first, each image is\nencoded as a sequence of ordered patches, including local properties. Second,\nthe sequence of these ordered patches is modeled as a probabilistic feature\nvector by CRF to model spatial relationship of these local properties. And\nfinally, image classification is performed on such probabilistic image\nrepresentation. Experimental results on several standard image datasets\nindicate that proposed method outperforms some of existing image classification\nmethods.\n", "rewritten_text": "In this paper, we propose an ordered patch-based method utilizing Conditional Random Field (CRF) to encode local properties and their spatial relationships in images for addressing texture classification, face recognition, and scene classification problems. Traditional image classification approaches often overlook the spatial causality among distinct properties of an image when representing it in feature space. Our method involves encoding each image as a sequence of ordered patches containing local properties. Subsequently, these ordered patches are modeled as a probabilistic feature vector by CRF to capture the spatial relationships among these local properties. Finally, image classification is conducted based on this probabilistic image representation. Experimental results on various standard image datasets demonstrate that our proposed method outperforms some existing image classification techniques."}, "2305.15929": {"original_text": "  Current large language models, such as OpenAI's ChatGPT, have captured the\npublic's attention because how remarkable they are in the use of language.\nHere, I demonstrate that ChatGPT displays phonological biases that are a\nhallmark of human language processing. More concretely, just like humans,\nChatGPT has a consonant bias. That is, the chatbot has a tendency to use\nconsonants over vowels to identify words. This is observed across languages\nthat differ in their relative distribution of consonants and vowels such as\nEnglish and Spanish. Despite the differences in how current artificial\nintelligence language models are trained to process linguistic stimuli and how\nhuman infants acquire language, such training seems to be enough for the\nemergence of a phonological bias in ChatGPT\n", "rewritten_text": "Current large language models, like OpenAI's ChatGPT, have captured the public's attention due to their remarkable proficiency in language use. In this study, I demonstrate that ChatGPT exhibits phonological biases that mirror those found in human language processing. Specifically, similar to humans, ChatGPT shows a preference for consonants. The chatbot tends to favor consonants over vowels when forming words, a pattern observed across languages with varying distributions of consonants and vowels, such as English and Spanish. Despite differences in the training methods of current artificial intelligence language models and the language acquisition process in human infants, it appears that such training is sufficient for the emergence of a phonological bias in ChatGPT."}, "2008.13196": {"original_text": "  The task of spatial-temporal action detection has attracted increasing\nattention among researchers. Existing dominant methods solve this problem by\nrelying on short-term information and dense serial-wise detection on each\nindividual frames or clips. Despite their effectiveness, these methods showed\ninadequate use of long-term information and are prone to inefficiency. In this\npaper, we propose for the first time, an efficient framework that generates\naction tube proposals from video streams with a single forward pass in a\nsparse-to-dense manner. There are two key characteristics in this framework:\n(1) Both long-term and short-term sampled information are explicitly utilized\nin our spatiotemporal network, (2) A new dynamic feature sampling module (DTS)\nis designed to effectively approximate the tube output while keeping the system\ntractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 and\nUCFSports benchmark datasets, achieving promising results that are competitive\nto state-of-the-art methods. The proposed sparse-to-dense strategy rendered our\nframework about 7.6 times more efficient than the nearest competitor.\n", "rewritten_text": "The task of spatial-temporal action detection has garnered increasing attention from researchers. Current predominant methods address this challenge by relying on short-term information and performing dense serial-wise detection on individual frames or clips. While effective, these methods often underutilize long-term information and can be inefficient. In this study, we introduce a novel and efficient framework that generates action tube proposals from video streams in a sparse-to-dense manner with a single forward pass. Two key characteristics of this framework are: (1) explicit utilization of both long-term and short-term sampled information in our spatiotemporal network, and (2) the design of a new dynamic feature sampling module (DTS) to effectively approximate the tube output while maintaining system tractability. We assess the performance of our model on benchmark datasets such as UCF101-24, JHMDB-21, and UCFSports, achieving promising results that are competitive with state-of-the-art methods. Our proposed sparse-to-dense strategy makes our framework approximately 7.6 times more efficient than the closest competitor."}, "2010.07574": {"original_text": "  Evaluation of grammatical error correction (GEC) systems has primarily\nfocused on essays written by non-native learners of English, which however is\nonly part of the full spectrum of GEC applications. We aim to broaden the\ntarget domain of GEC and release CWEB, a new benchmark for GEC consisting of\nwebsite text generated by English speakers of varying levels of proficiency.\nWebsite data is a common and important domain that contains far fewer\ngrammatical errors than learner essays, which we show presents a challenge to\nstate-of-the-art GEC systems. We demonstrate that a factor behind this is the\ninability of systems to rely on a strong internal language model in low error\ndensity domains. We hope this work shall facilitate the development of\nopen-domain GEC models that generalize to different topics and genres.\n", "rewritten_text": "The evaluation of grammatical error correction (GEC) systems has predominantly focused on essays written by non-native English learners, which represents only a portion of the full spectrum of GEC applications. Our goal is to expand the target domain of GEC by introducing CWEB, a new benchmark for GEC that includes website text created by English speakers with varying levels of proficiency. Website data is a common and significant domain that typically contains fewer grammatical errors compared to learner essays. This poses a challenge to state-of-the-art GEC systems, as we demonstrate that one contributing factor is the systems' inability to rely on a robust internal language model in domains with low error density. We anticipate that this research will facilitate the development of open-domain GEC models that can generalize across different topics and genres."}, "2306.0045": {"original_text": "  Semantic segmentation is a crucial task in computer vision that involves\nsegmenting images into semantically meaningful regions at the pixel level.\nHowever, existing approaches often rely on expensive human annotations as\nsupervision for model training, limiting their scalability to large, unlabeled\ndatasets. To address this challenge, we present ZeroSeg, a novel method that\nleverages the existing pretrained vision-language (VL) model (e.g. CLIP) to\ntrain open-vocabulary zero-shot semantic segmentation models. Although acquired\nextensive knowledge of visual concepts, it is non-trivial to exploit knowledge\nfrom these VL models to the task of semantic segmentation, as they are usually\ntrained at an image level. ZeroSeg overcomes this by distilling the visual\nconcepts learned by VL models into a set of segment tokens, each summarizing a\nlocalized region of the target image. We evaluate ZeroSeg on multiple popular\nsegmentation benchmarks, including PASCAL VOC 2012, PASCAL Context, and COCO,\nin a zero-shot manner (i.e., no training or adaption on target segmentation\ndatasets). Our approach achieves state-of-the-art performance when compared to\nother zero-shot segmentation methods under the same training data, while also\nperforming competitively compared to strongly supervised methods. Finally, we\nalso demonstrated the effectiveness of ZeroSeg on open-vocabulary segmentation,\nthrough both human studies and qualitative visualizations.\n", "rewritten_text": "Semantic segmentation is a critical task in computer vision that involves dividing images into semantically meaningful regions at the pixel level. However, current approaches often require costly human annotations for model training, which limits their scalability to large, unlabeled datasets. To tackle this challenge, we introduce ZeroSeg, a novel method that utilizes pretrained vision-language (VL) models (such as CLIP) to train open-vocabulary zero-shot semantic segmentation models. Despite possessing extensive knowledge of visual concepts, leveraging this knowledge from VL models for semantic segmentation is not straightforward, as these models are typically trained at the image level. ZeroSeg overcomes this obstacle by distilling the visual concepts learned by VL models into a set of segment tokens, each summarizing a localized region of the target image. We assess ZeroSeg on various popular segmentation benchmarks, including PASCAL VOC 2012, PASCAL Context, and COCO, in a zero-shot manner (i.e., without training or adaptation on target segmentation datasets). Our approach achieves state-of-the-art performance compared to other zero-shot segmentation methods using the same training data, while also performing competitively against strongly supervised methods. Additionally, we demonstrate the effectiveness of ZeroSeg in open-vocabulary segmentation through both human studies and qualitative visualizations."}, "2401.05166": {"original_text": "  In dyadic interactions, humans communicate their intentions and state of mind\nusing verbal and non-verbal cues, where multiple different facial reactions\nmight be appropriate in response to a specific speaker behaviour. Then, how to\ndevelop a machine learning (ML) model that can automatically generate multiple\nappropriate, diverse, realistic and synchronised human facial reactions from an\npreviously unseen speaker behaviour is a challenging task. Following the\nsuccessful organisation of the first REACT challenge (REACT 2023), this edition\nof the challenge (REACT 2024) employs a subset used by the previous challenge,\nwhich contains segmented 30-secs dyadic interaction clips originally recorded\nas part of the NOXI and RECOLA datasets, encouraging participants to develop\nand benchmark Machine Learning (ML) models that can generate multiple\nappropriate facial reactions (including facial image sequences and their\nattributes) given an input conversational partner's stimulus under various\ndyadic video conference scenarios. This paper presents: (i) the guidelines of\nthe REACT 2024 challenge; (ii) the dataset utilized in the challenge; and (iii)\nthe performance of the baseline systems on the two proposed sub-challenges:\nOffline Multiple Appropriate Facial Reaction Generation and Online Multiple\nAppropriate Facial Reaction Generation, respectively. The challenge baseline\ncode is publicly available at\nhttps://github.com/reactmultimodalchallenge/baseline_react2024.\n", "rewritten_text": "In dyadic interactions, humans communicate their intentions and state of mind through verbal and non-verbal cues. Various facial reactions may be suitable responses to specific speaker behaviors. Developing a machine learning (ML) model capable of automatically generating multiple appropriate, diverse, realistic, and synchronized human facial reactions in response to previously unseen speaker behavior poses a significant challenge.\n\nBuilding on the success of the inaugural REACT challenge (REACT 2023), the current iteration (REACT 2024) utilizes a subset of data from the previous challenge. This subset comprises segmented 30-second dyadic interaction clips originally sourced from the NOXI and RECOLA datasets. Participants are encouraged to create and evaluate Machine Learning (ML) models that can produce multiple appropriate facial reactions, including facial image sequences and their attributes, based on a conversational partner's stimulus in various dyadic video conference scenarios.\n\nThis paper outlines: (i) the guidelines for the REACT 2024 challenge; (ii) details of the dataset employed in the challenge; and (iii) the performance of baseline systems in the two proposed sub-challenges: Offline Multiple Appropriate Facial Reaction Generation and Online Multiple Appropriate Facial Reaction Generation. The baseline code for the challenge is publicly accessible at https://github.com/reactmultimodalchallenge/baseline_react2024."}, "2406.10323": {"original_text": "  Most public instruction finetuning datasets are relatively small compared to\nthe closed source datasets used to train industry models. To study questions\nabout finetuning at scale, such as curricula and learning rate cooldown\nschedules, there is a need for industrial-scale datasets. However, this scale\nnecessitates a data generation process that is almost entirely automated. In\nthis work, we study methods for generating large instruction datasets from a\nsingle prompt. With little human oversight, we get LLMs to write diverse sets\nof instruction examples ranging from simple completion tasks to complex\nmulti-turn dialogs across a variety of subject areas. When finetuning a Llama-3\n8B base model, our dataset meets or exceeds both WizardLM and Ultrachat on both\nknowledge-intensive leaderboard tasks as well as conversational evaluations. We\nrelease our dataset, the \"generator\" prompts that created it, and our finetuned\nmodel checkpoints.\n", "rewritten_text": "Most public instruction finetuning datasets are relatively small compared to the closed-source datasets used to train industry models. To address questions about finetuning at scale, such as curricula and learning rate cooldown schedules, there is a need for industrial-scale datasets. However, achieving this scale requires an almost entirely automated data generation process. In this study, we explore methods for generating large instruction datasets from a single prompt. With minimal human oversight, we leverage Language Model Models (LLMs) to produce diverse sets of instruction examples, ranging from simple completion tasks to complex multi-turn dialogs across various subject areas. When finetuning a Llama-38B base model, our dataset either meets or surpasses both WizardLM and Ultrachat in knowledge-intensive leaderboard tasks and conversational evaluations. We are making our dataset, the \"generator\" prompts used to create it, and our finetuned model checkpoints publicly available."}, "2303.12776": {"original_text": "  One-to-one label assignment in object detection has successfully obviated the\nneed for non-maximum suppression (NMS) as postprocessing and makes the pipeline\nend-to-end. However, it triggers a new dilemma as the widely used sparse\nqueries cannot guarantee a high recall, while dense queries inevitably bring\nmore similar queries and encounter optimization difficulties. As both sparse\nand dense queries are problematic, then what are the expected queries in\nend-to-end object detection? This paper shows that the solution should be Dense\nDistinct Queries (DDQ). Concretely, we first lay dense queries like traditional\ndetectors and then select distinct ones for one-to-one assignments. DDQ blends\nthe advantages of traditional and recent end-to-end detectors and significantly\nimproves the performance of various detectors including FCN, R-CNN, and DETRs.\nMost impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12\nepochs using a ResNet-50 backbone, outperforming all existing detectors in the\nsame setting. DDQ also shares the benefit of end-to-end detectors in crowded\nscenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchers\nto consider the complementarity between traditional methods and end-to-end\ndetectors. The source code can be found at\n\\url{https://github.com/jshilong/DDQ}.\n", "rewritten_text": "The adoption of one-to-one label assignment in object detection has effectively eliminated the necessity for non-maximum suppression (NMS) as a postprocessing step, thereby creating an end-to-end pipeline. However, this approach introduces a new challenge: while widely used sparse queries do not ensure high recall, dense queries lead to an increase in similar queries and encounter optimization complexities. Given the issues with both sparse and dense queries, the question arises: what should the ideal queries be for end-to-end object detection? This study proposes Dense Distinct Queries (DDQ) as the solution. Specifically, the method involves initially deploying dense queries akin to traditional detectors, followed by the selection of distinct queries for one-to-one assignments. DDQ combines the strengths of traditional detectors and recent end-to-end detectors, resulting in significant performance enhancements across various detectors such as FCN, R-CNN, and DETRs. Notably, DDQ-DETR achieves an impressive 52.1 AP on the MS-COCO dataset in just 12 epochs using a ResNet-50 backbone, surpassing all existing detectors in the same configuration. Furthermore, DDQ demonstrates its effectiveness in crowded scenes by achieving a 93.8 AP on CrowdHuman. It is hoped that DDQ will encourage researchers to explore the synergy between traditional methods and end-to-end detectors. The source code is available at \\url{https://github.com/jshilong/DDQ}."}, "2303.1112": {"original_text": "  Positional reasoning is the process of ordering unsorted parts contained in a\nset into a consistent structure. We present Positional Diffusion, a\nplug-and-play graph formulation with Diffusion Probabilistic Models to address\npositional reasoning. We use the forward process to map elements' positions in\na set to random positions in a continuous space. Positional Diffusion learns to\nreverse the noising process and recover the original positions through an\nAttention-based Graph Neural Network. We conduct extensive experiments with\nbenchmark datasets including two puzzle datasets, three sentence ordering\ndatasets, and one visual storytelling dataset, demonstrating that our method\noutperforms long-lasting research on puzzle solving with up to +18% compared to\nthe second-best deep learning method, and performs on par against the\nstate-of-the-art methods on sentence ordering and visual storytelling. Our work\nhighlights the suitability of diffusion models for ordering problems and\nproposes a novel formulation and method for solving various ordering tasks.\nProject website at https://iit-pavis.github.io/Positional_Diffusion/\n", "rewritten_text": "Positional reasoning involves organizing unsorted parts from a set into a coherent structure. In this study, we introduce Positional Diffusion, a graph formulation that incorporates Diffusion Probabilistic Models to tackle positional reasoning. Our approach utilizes a forward process to map the positions of elements within a set to random positions in a continuous space. Through Positional Diffusion, we train the model to reverse the noise introduced during the mapping process and accurately reconstruct the original positions using an Attention-based Graph Neural Network.\n\nExtensive experiments were conducted using benchmark datasets, including two puzzle datasets, three sentence ordering datasets, and one visual storytelling dataset. The results demonstrate that our method surpasses long-standing research in puzzle solving by up to +18% compared to the second-best deep learning method. Additionally, our approach performs comparably to state-of-the-art methods in sentence ordering and visual storytelling tasks.\n\nThis work underscores the effectiveness of diffusion models in addressing ordering problems and introduces a novel formulation and methodology for solving a variety of ordering tasks. For more information, please visit our project website at https://iit-pavis.github.io/Positional_Diffusion/."}, "2311.13735": {"original_text": "  Recent advances in large language models (LLMs) show potential for clinical\napplications, such as clinical decision support and trial recommendations.\nHowever, the GPT-4 LLM predicts an excessive number of ICD codes for medical\ncoding tasks, leading to high recall but low precision. To tackle this\nchallenge, we introduce LLM-codex, a two-stage approach to predict ICD codes\nthat first generates evidence proposals using an LLM and then employs an\nLSTM-based verification stage. The LSTM learns from both the LLM's high recall\nand human expert's high precision, using a custom loss function. Our model is\nthe only approach that simultaneously achieves state-of-the-art results in\nmedical coding accuracy, accuracy on rare codes, and sentence-level evidence\nidentification to support coding decisions without training on human-annotated\nevidence according to experiments on the MIMIC dataset.\n", "rewritten_text": "Recent advancements in large language models (LLMs) have demonstrated potential for clinical applications, such as clinical decision support and trial recommendations. However, the GPT-4 LLM has been observed to predict an excessive number of ICD codes for medical coding tasks, resulting in high recall but low precision. To address this issue, we propose LLM-codex, a two-stage approach for predicting ICD codes. In the first stage, the model generates evidence proposals using an LLM, followed by a verification stage based on LSTM. The LSTM leverages the high recall of the LLM and the high precision of human experts by utilizing a custom loss function. Our model stands out as the sole approach that achieves state-of-the-art results in terms of medical coding accuracy, accuracy in rare codes, and identification of sentence-level evidence to support coding decisions. Notably, our model accomplishes this without the need for training on human-annotated evidence, as demonstrated through experiments conducted on the MIMIC dataset."}, "2209.14575": {"original_text": "  In this paper, we investigate the problem of bit allocation in Neural Video\nCompression (NVC). First, we reveal that a recent bit allocation approach\nclaimed to be optimal is, in fact, sub-optimal due to its implementation.\nSpecifically, we find that its sub-optimality lies in the improper application\nof semi-amortized variational inference (SAVI) on latent with non-factorized\nvariational posterior. Then, we show that the corrected version of SAVI on\nnon-factorized latent requires recursively applying back-propagating through\ngradient ascent, based on which we derive the corrected optimal bit allocation\nalgorithm. Due to the computational in-feasibility of the corrected bit\nallocation, we design an efficient approximation to make it practical.\nEmpirical results show that our proposed correction significantly improves the\nincorrect bit allocation in terms of R-D performance and bitrate error, and\noutperforms all other bit allocation methods by a large margin. The source code\nis provided in the supplementary material.\n", "rewritten_text": "This paper investigates the issue of bit allocation in Neural Video Compression (NVC). Initially, we identify a recent bit allocation method that was claimed to be optimal but is actually sub-optimal due to its implementation. Specifically, we discover that its sub-optimality stems from the improper application of semi-amortized variational inference (SAVI) on latent variables with a non-factorized variational posterior. Subsequently, we demonstrate that the corrected version of SAVI on non-factorized latent variables necessitates recursive application through back-propagation via gradient ascent. This forms the basis for deriving the corrected optimal bit allocation algorithm. Recognizing the computational infeasibility of the corrected bit allocation, we develop an efficient approximation to render it practical. Empirical results indicate that our proposed correction significantly enhances the erroneous bit allocation in terms of R-D performance and bitrate error, surpassing all other bit allocation methods by a considerable margin. The source code is available in the supplementary material."}, "2302.03744": {"original_text": "  The ability to perceive and understand 3D scenes is crucial for many\napplications in computer vision and robotics. Inverse graphics is an appealing\napproach to 3D scene understanding that aims to infer the 3D scene structure\nfrom 2D images. In this paper, we introduce probabilistic modeling to the\ninverse graphics framework to quantify uncertainty and achieve robustness in 6D\npose estimation tasks. Specifically, we propose 3D Neural Embedding Likelihood\n(3DNEL) as a unified probabilistic model over RGB-D images, and develop\nefficient inference procedures on 3D scene descriptions. 3DNEL effectively\ncombines learned neural embeddings from RGB with depth information to improve\nrobustness in sim-to-real 6D object pose estimation from RGB-D images.\nPerformance on the YCB-Video dataset is on par with state-of-the-art yet is\nmuch more robust in challenging regimes. In contrast to discriminative\napproaches, 3DNEL's probabilistic generative formulation jointly models\nmultiple objects in a scene, quantifies uncertainty in a principled way, and\nhandles object pose tracking under heavy occlusion. Finally, 3DNEL provides a\nprincipled framework for incorporating prior knowledge about the scene and\nobjects, which allows natural extension to additional tasks like camera pose\ntracking from video.\n", "rewritten_text": "The ability to perceive and understand 3D scenes is essential for various applications in computer vision and robotics. Inverse graphics presents an attractive approach to 3D scene comprehension by inferring the 3D scene structure from 2D images. This paper introduces probabilistic modeling into the inverse graphics framework to quantify uncertainty and enhance robustness in 6D pose estimation tasks. The proposed approach, 3D Neural Embedding Likelihood (3DNEL), serves as a unified probabilistic model for RGB-D images, incorporating efficient inference procedures for 3D scene descriptions. By combining neural embeddings learned from RGB data with depth information, 3DNEL improves robustness in sim-to-real 6D object pose estimation from RGB-D images. Performance on the YCB-Video dataset is comparable to state-of-the-art methods, yet significantly more robust in challenging scenarios. Unlike discriminative methods, 3DNEL's probabilistic generative formulation jointly models multiple objects in a scene, quantifies uncertainty systematically, and handles object pose tracking even under heavy occlusion. Furthermore, 3DNEL offers a structured framework for integrating prior knowledge about scenes and objects, facilitating seamless extension to tasks such as camera pose tracking from video."}, "2005.04551": {"original_text": "  A common approach to localize 3D human joints in a synchronized and\ncalibrated multi-view setup consists of two-steps: (1) apply a 2D detector\nseparately on each view to localize joints in 2D, and (2) perform robust\ntriangulation on 2D detections from each view to acquire the 3D joint\nlocations. However, in step 1, the 2D detector is limited to solving\nchallenging cases which could potentially be better resolved in 3D, such as\nocclusions and oblique viewing angles, purely in 2D without leveraging any 3D\ninformation. Therefore, we propose the differentiable \"epipolar transformer\",\nwhich enables the 2D detector to leverage 3D-aware features to improve 2D pose\nestimation. The intuition is: given a 2D location p in the current view, we\nwould like to first find its corresponding point p' in a neighboring view, and\nthen combine the features at p' with the features at p, thus leading to a\n3D-aware feature at p. Inspired by stereo matching, the epipolar transformer\nleverages epipolar constraints and feature matching to approximate the features\nat p'. Experiments on InterHand and Human3.6M show that our approach has\nconsistent improvements over the baselines. Specifically, in the condition\nwhere no external data is used, our Human3.6M model trained with ResNet-50\nbackbone and image size 256 x 256 outperforms state-of-the-art by 4.23 mm and\nachieves MPJPE 26.9 mm.\n", "rewritten_text": "A common approach for localizing 3D human joints in a synchronized and calibrated multi-view setup typically involves two steps: first, applying a 2D detector separately on each view to identify joint locations in 2D, and second, performing robust triangulation on the 2D detections from each view to determine the 3D joint locations. However, the 2D detector in the first step is limited in its ability to address challenging scenarios, such as occlusions and oblique viewing angles, which could potentially be better resolved in 3D. This limitation arises from the fact that the detector operates solely in 2D without utilizing any 3D information.\n\nTo address this issue, we propose a novel approach called the differentiable \"epipolar transformer,\" which empowers the 2D detector to leverage 3D-aware features for enhanced 2D pose estimation. The core idea is to identify the corresponding point p' in a neighboring view for a given 2D location p in the current view, and then merge the features at p' with those at p to generate a 3D-aware feature at p. Drawing inspiration from stereo matching, the epipolar transformer utilizes epipolar constraints and feature matching to approximate the features at p'.\n\nExperimental results on InterHand and Human3.6M datasets demonstrate that our approach consistently outperforms baseline methods. Notably, our Human3.6M model, trained with a ResNet-50 backbone and image size of 256 x 256, achieves a Mean Per Joint Position Error (MPJPE) of 26.9 mm, surpassing the state-of-the-art by 4.23 mm in scenarios where no external data is utilized."}, "1912.0643": {"original_text": "  Annotating videos is cumbersome, expensive and not scalable. Yet, many strong\nvideo models still rely on manually annotated data. With the recent\nintroduction of the HowTo100M dataset, narrated videos now offer the\npossibility of learning video representations without manual supervision. In\nthis work we propose a new learning approach, MIL-NCE, capable of addressing\nmisalignments inherent to narrated videos. With this approach we are able to\nlearn strong video representations from scratch, without the need for any\nmanual annotation. We evaluate our representations on a wide range of four\ndownstream tasks over eight datasets: action recognition (HMDB-51, UCF-101,\nKinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization\n(YouTube-8M Segments, CrossTask) and action segmentation (COIN). Our method\noutperforms all published self-supervised approaches for these tasks as well as\nseveral fully supervised baselines.\n", "rewritten_text": "Annotating videos is a cumbersome, expensive, and non-scalable task. Despite these challenges, many robust video models still heavily rely on manually annotated data. The recent introduction of the HowTo100M dataset has opened up new possibilities for learning video representations without the need for manual supervision, particularly in narrated videos. In this study, we present a novel learning approach called MIL-NCE, which effectively addresses the inherent misalignments found in narrated videos. Through this approach, we are able to develop strong video representations from scratch, eliminating the requirement for manual annotation. Our representations are evaluated across a diverse set of four downstream tasks spanning eight datasets: action recognition (HMDB-51, UCF-101, Kinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization (YouTube-8M Segments, CrossTask), and action segmentation (COIN). Our method surpasses all previously published self-supervised approaches for these tasks, as well as several fully supervised baselines."}, "2211.09795": {"original_text": "  Diffusion models have become the go-to method for many generative tasks,\nparticularly for image-to-image generation tasks such as super-resolution and\ninpainting. Current diffusion-based methods do not provide statistical\nguarantees regarding the generated results, often preventing their use in\nhigh-stakes situations. To bridge this gap, we construct a confidence interval\naround each generated pixel such that the true value of the pixel is guaranteed\nto fall within the interval with a probability set by the user. Since diffusion\nmodels parametrize the data distribution, a straightforward way of constructing\nsuch intervals is by drawing multiple samples and calculating their bounds.\nHowever, this method has several drawbacks: i) slow sampling speeds ii)\nsuboptimal bounds iii) requires training a diffusion model per task. To\nmitigate these shortcomings we propose Conffusion, wherein we fine-tune a\npre-trained diffusion model to predict interval bounds in a single forward\npass. We show that Conffusion outperforms the baseline method while being three\norders of magnitude faster.\n", "rewritten_text": "Diffusion models have become the preferred method for many generative tasks, especially for tasks involving image-to-image generation like super-resolution and inpainting. However, current diffusion-based methods lack statistical guarantees on the generated results, limiting their applicability in high-stakes scenarios. To address this limitation, we introduce a novel approach where a confidence interval is established around each generated pixel, ensuring that the true pixel value falls within the interval with a user-defined probability.\n\nGiven that diffusion models parameterize the data distribution, one common method to construct these intervals is by generating multiple samples and determining their bounds. Nevertheless, this approach has notable drawbacks, including slow sampling speeds, suboptimal bounds, and the need to train a diffusion model for each task. To overcome these challenges, we present Conffusion, a technique that fine-tunes a pre-trained diffusion model to predict interval bounds in a single forward pass. Our results demonstrate that Conffusion surpasses the baseline method in performance while achieving a speed improvement of three orders of magnitude."}, "2404.14248": {"original_text": "  This paper reviews the NTIRE 2024 low light image enhancement challenge,\nhighlighting the proposed solutions and results. The aim of this challenge is\nto discover an effective network design or solution capable of generating\nbrighter, clearer, and visually appealing results when dealing with a variety\nof conditions, including ultra-high resolution (4K and beyond), non-uniform\nillumination, backlighting, extreme darkness, and night scenes. A notable total\nof 428 participants registered for the challenge, with 22 teams ultimately\nmaking valid submissions. This paper meticulously evaluates the\nstate-of-the-art advancements in enhancing low-light images, reflecting the\nsignificant progress and creativity in this field.\n", "rewritten_text": "This paper provides a review of the NTIRE 2024 low-light image enhancement challenge, focusing on the proposed solutions and results. The primary goal of this challenge is to identify an effective network design or solution capable of producing brighter, clearer, and visually appealing results across various conditions, such as ultra-high resolution (4K and beyond), non-uniform illumination, backlighting, extreme darkness, and night scenes. A total of 428 participants registered for the challenge, with 22 teams ultimately submitting valid entries. The paper meticulously assesses the state-of-the-art advancements in enhancing low-light images, showcasing significant progress and creativity in this field."}, "2311.01907": {"original_text": "  Automatic simplification can help laypeople to comprehend complex scientific\ntext. Language models are frequently applied to this task by translating from\ncomplex to simple language. In this paper, we describe our system based on\nLlama 2, which ranked first in the PLABA shared task addressing the\nsimplification of biomedical text. We find that the large portion of shared\ntokens between input and output leads to weak training signals and\nconservatively editing models. To mitigate these issues, we propose\nsentence-level and token-level loss weights. They give higher weight to\nmodified tokens, indicated by edit distance and edit operations, respectively.\nWe conduct an empirical evaluation on the PLABA dataset and find that both\napproaches lead to simplifications closer to those created by human annotators\n(+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x /\n1.8x edit distance) compared to the same model fine-tuned with standard cross\nentropy. We furthermore show that the hyperparameter $\\lambda$ in token-level\nloss weights can be used to control the edit distance and the simplicity level\n(FKGL).\n", "rewritten_text": "Automatic simplification can assist non-experts in understanding complex scientific texts. Language models are commonly used for this purpose by converting complex language into simpler terms. In this paper, we present our system based on Llama 2, which achieved the top ranking in the PLABA shared task focused on simplifying biomedical text. We observed that a significant overlap of tokens between the input and output results in weak training signals and models that are overly cautious in making edits. To address these challenges, we introduce sentence-level and token-level loss weights. These weights prioritize modified tokens based on edit distance and edit operations, respectively. Through an empirical evaluation on the PLABA dataset, we demonstrate that both approaches lead to simplifications that closely resemble those made by human annotators (improvements of +1.8% / +3.5% in SARI), simpler language (-1 / -1.1 in FKGL), and more edits (1.6x / 1.8x in edit distance) compared to the same model fine-tuned using standard cross-entropy. Additionally, we show that the hyperparameter $\\lambda$ in token-level loss weights can be adjusted to control the edit distance and the level of simplicity (FKGL)."}, "2402.12923": {"original_text": "  In recent years, 3D point clouds (PCs) have gained significant attention due\nto their diverse applications across various fields, such as computer vision\n(CV), condition monitoring (CM), virtual reality, robotics, autonomous driving,\netc. Deep learning (DL) has proven effective in leveraging 3D PCs to address\nvarious challenges encountered in 2D vision. However, applying deep neural\nnetworks (DNNs) to process 3D PCs presents unique challenges. This paper\nprovides an in-depth review of recent advancements in DL-based industrial CM\nusing 3D PCs, with a specific focus on defect shape classification and\nsegmentation within industrial applications. Recognizing the crucial role of\nthese aspects in industrial maintenance, the paper offers insightful\nobservations on the strengths and limitations of the reviewed DL-based PC\nprocessing methods. This knowledge synthesis aims to contribute to\nunderstanding and enhancing CM processes, particularly within the framework of\nremaining useful life (RUL), in industrial systems.\n", "rewritten_text": "In recent years, there has been a significant increase in the attention given to 3D point clouds (PCs) due to their wide range of applications in fields such as computer vision (CV), condition monitoring (CM), virtual reality, robotics, and autonomous driving. Deep learning (DL) has been shown to be effective in utilizing 3D PCs to tackle challenges that arise in 2D vision. However, the application of deep neural networks (DNNs) to process 3D PCs presents unique challenges. This paper offers a comprehensive review of recent advancements in DL-based industrial CM using 3D PCs, with a specific emphasis on defect shape classification and segmentation within industrial settings. Recognizing the critical importance of these aspects in industrial maintenance, the paper provides valuable insights into the strengths and limitations of the examined DL-based PC processing methods. The aim of this knowledge synthesis is to contribute to the understanding and improvement of CM processes, particularly in the context of remaining useful life (RUL) within industrial systems."}, "2402.02082": {"original_text": "  Speculative decoding is a relatively new decoding framework that leverages\nsmall and efficient draft models to reduce the latency of LLMs. In this study,\nwe introduce GliDe and CaPE, two low-hassle modifications to vanilla\nspeculative decoding to further improve the decoding speed of a frozen LLM.\nSpecifically, GliDe is a modified draft model architecture that reuses the\ncached keys and values from the target LLM, while CaPE is a proposal expansion\nmethod that uses the draft model's confidence scores to help select additional\ncandidate tokens for verification. Extensive experiments on different\nbenchmarks demonstrate that our proposed GliDe draft model significantly\nreduces the expected decoding latency. Additional evaluation using walltime\nreveals that GliDe can accelerate Vicuna models up to 2.17x and further extend\nthe improvement to 2.61x with CaPE. We will release our code, data, and the\ntrained draft models.\n", "rewritten_text": "The study introduces GliDe and CaPE as modifications to speculative decoding, a new framework that utilizes small and efficient draft models to decrease the latency of LLMs. GliDe is a revised draft model architecture that optimizes decoding speed by reusing cached keys and values from the target LLM. On the other hand, CaPE is a proposal expansion technique that leverages the confidence scores of the draft model to assist in selecting additional candidate tokens for verification. Through extensive experiments on various benchmarks, it is demonstrated that GliDe significantly reduces the expected decoding latency. Furthermore, evaluation using walltime indicates that GliDe can enhance Vicuna models by up to 2.17x, with an additional improvement to 2.61x when combined with CaPE. The code, data, and trained draft models will be made available for public access."}, "2302.14354": {"original_text": "  The cultural heritage buildings (CHB), which are part of mankind's history\nand identity, are in constant danger of damage or in extreme situations total\ndestruction. That being said, it's of utmost importance to preserve them by\nidentifying the existent, or presumptive, defects using novel methods so that\nrenovation processes can be done in a timely manner and with higher accuracy.\nThe main goal of this research is to use new deep learning (DL) methods in the\nprocess of preserving CHBs (situated in Iran); a goal that has been neglected\nespecially in developing countries such as Iran, as these countries still\npreserve their CHBs using manual, and even archaic, methods that need direct\nhuman supervision. Having proven their effectiveness and performance when it\ncomes to processing images, the convolutional neural networks (CNN) are a\nstaple in computer vision (CV) literacy and this paper is not exempt. When\nlacking enough CHB images, training a CNN from scratch would be very difficult\nand prone to overfitting; that's why we opted to use a technique called\ntransfer learning (TL) in which we used pre-trained ResNet, MobileNet, and\nInception networks, for classification. Even more, the Grad-CAM was utilized to\nlocalize the defects to some extent. The final results were very favorable\nbased on those of similar research. The final proposed model can pave the way\nfor moving from manual to unmanned CHB conservation, hence an increase in\naccuracy and a decrease in human-induced errors.\n", "rewritten_text": "Cultural heritage buildings (CHBs), integral to mankind's history and identity, are constantly at risk of damage or even total destruction in extreme circumstances. Therefore, it is crucial to preserve them by identifying existing or potential defects using innovative methods. This enables renovation processes to be carried out promptly and with greater precision. The primary objective of this study is to leverage new deep learning (DL) techniques to aid in the preservation of CHBs in Iran, a goal that has been overlooked, particularly in developing countries like Iran. These nations often rely on manual or outdated methods that require direct human oversight for CHB preservation.\n\nConvolutional neural networks (CNNs) have demonstrated their effectiveness in image processing and are fundamental in computer vision (CV) literacy, a concept explored in this paper. Training a CNN from scratch can be challenging and prone to overfitting, especially when there is a scarcity of CHB images. To address this, transfer learning (TL) was employed, utilizing pre-trained ResNet, MobileNet, and Inception networks for classification. Additionally, Grad-CAM was utilized to partially localize defects. The study yielded highly favorable results compared to similar research endeavors.\n\nThe proposed model represents a significant step towards transitioning from manual to unmanned CHB conservation, leading to enhanced accuracy and reduced human-induced errors."}, "2303.0149": {"original_text": "  Language identification is an important first step in many IR and NLP\napplications. Most publicly available language identification datasets,\nhowever, are compiled under the assumption that the gold label of each instance\nis determined by where texts are retrieved from. Research has shown that this\nis a problematic assumption, particularly in the case of very similar languages\n(e.g., Croatian and Serbian) and national language varieties (e.g., Brazilian\nand European Portuguese), where texts may contain no distinctive marker of the\nparticular language or variety. To overcome this important limitation, this\npaper presents DSL True Labels (DSL-TL), the first human-annotated multilingual\ndataset for language variety identification. DSL-TL contains a total of 12,900\ninstances in Portuguese, split between European Portuguese and Brazilian\nPortuguese; Spanish, split between Argentine Spanish and Castilian Spanish; and\nEnglish, split between American English and British English. We trained\nmultiple models to discriminate between these language varieties, and we\npresent the results in detail. The data and models presented in this paper\nprovide a reliable benchmark toward the development of robust and fairer\nlanguage variety identification systems. We make DSL-TL freely available to the\nresearch community.\n", "rewritten_text": "Language identification serves as a crucial initial step in numerous Information Retrieval (IR) and Natural Language Processing (NLP) applications. While many publicly accessible language identification datasets operate on the premise that the gold label for each instance is determined based on the source of the texts, recent research has highlighted the flaws in this assumption. This is particularly evident in scenarios involving closely related languages (such as Croatian and Serbian) and national language variations (like Brazilian and European Portuguese), where texts may lack distinct markers of a specific language or variety.\n\nTo address this significant limitation, this paper introduces DSL True Labels (DSL-TL), the first human-annotated multilingual dataset designed for identifying language varieties. DSL-TL comprises a total of 12,900 instances in Portuguese, divided into European Portuguese and Brazilian Portuguese; Spanish, categorized as Argentine Spanish and Castilian Spanish; and English, segmented into American English and British English. Through the training of multiple models to differentiate between these language variations, detailed results are presented.\n\nThe data and models outlined in this paper establish a dependable benchmark for the advancement of robust and equitable language variety identification systems. We are pleased to offer DSL-TL freely to the research community."}, "2004.0827": {"original_text": "  In this paper, we tackle the task of automatically analyzing 3D volumetric\nscans obtained from computed tomography (CT) devices. In particular, we address\na particular task for which data is very limited: the segmentation of ancient\nEgyptian mummies CT scans. We aim at digitally unwrapping the mummy and\nidentify different segments such as body, bandages and jewelry. The problem is\ncomplex because of the lack of annotated data for the different semantic\nregions to segment, thus discouraging the use of strongly supervised\napproaches. We, therefore, propose a weakly supervised and efficient\ninteractive segmentation method to solve this challenging problem. After\nsegmenting the wrapped mummy from its exterior region using histogram analysis\nand template matching, we first design a voxel distance measure to find an\napproximate solution for the body and bandage segments. Here, we use geodesic\ndistances since voxel features as well as spatial relationship among voxels is\nincorporated in this measure. Next, we refine the solution using a GrabCut\nbased segmentation together with a tracking method on the slices of the scan\nthat assigns labels to different regions in the volume, using limited\nsupervision in the form of scribbles drawn by the user. The efficiency of the\nproposed method is demonstrated using visualizations and validated through\nquantitative measures and qualitative unwrapping of the mummy.\n", "rewritten_text": "This paper addresses the automated analysis of 3D volumetric scans acquired from computed tomography (CT) devices, focusing on the segmentation of ancient Egyptian mummies' CT scans. The objective is to digitally unwrap the mummy and identify various segments such as the body, bandages, and jewelry. The challenge lies in the scarcity of annotated data for the different semantic regions, which hinders the use of strongly supervised methods. To overcome this, we propose a weakly supervised and efficient interactive segmentation approach to tackle this intricate problem.\n\nInitially, the wrapped mummy is segmented from its exterior region using histogram analysis and template matching. A voxel distance measure is then devised to provide an initial solution for the body and bandage segments, leveraging geodesic distances to incorporate voxel features and spatial relationships among voxels. Subsequently, the solution is refined through GrabCut-based segmentation and a tracking method on the scan slices, assigning labels to different regions in the volume with limited supervision in the form of user-drawn scribbles.\n\nThe effectiveness of the proposed method is demonstrated through visualizations and validated using quantitative measures and qualitative unwrapping of the mummy."}, "1607.06871": {"original_text": "  The \"interpretation through synthesis\" approach to analyze face images,\nparticularly Active Appearance Models (AAMs) method, has become one of the most\nsuccessful face modeling approaches over the last two decades. AAM models have\nability to represent face images through synthesis using a controllable\nparameterized Principal Component Analysis (PCA) model. However, the accuracy\nand robustness of the synthesized faces of AAM are highly depended on the\ntraining sets and inherently on the generalizability of PCA subspaces. This\npaper presents a novel Deep Appearance Models (DAMs) approach, an efficient\nreplacement for AAMs, to accurately capture both shape and texture of face\nimages under large variations. In this approach, three crucial components\nrepresented in hierarchical layers are modeled using the Deep Boltzmann\nMachines (DBM) to robustly capture the variations of facial shapes and\nappearances. DAMs are therefore superior to AAMs in inferencing a\nrepresentation for new face images under various challenging conditions. The\nproposed approach is evaluated in various applications to demonstrate its\nrobustness and capabilities, i.e. facial super-resolution reconstruction,\nfacial off-angle reconstruction or face frontalization, facial occlusion\nremoval and age estimation using challenging face databases, i.e. Labeled Face\nParts in the Wild (LFPW), Helen and FG-NET. Comparing to AAMs and other deep\nlearning based approaches, the proposed DAMs achieve competitive results in\nthose applications, thus this showed their advantages in handling occlusions,\nfacial representation, and reconstruction.\n", "rewritten_text": "The \"interpretation through synthesis\" approach for analyzing face images, particularly the Active Appearance Models (AAMs) method, has emerged as one of the most successful face modeling approaches in the past two decades. AAM models possess the capability to represent face images through synthesis by utilizing a controllable parameterized Principal Component Analysis (PCA) model. However, the accuracy and robustness of the synthesized faces generated by AAMs heavily rely on the quality of the training sets and, fundamentally, on the generalizability of PCA subspaces.\n\nThis paper introduces a novel approach called Deep Appearance Models (DAMs) as an efficient alternative to AAMs, aiming to accurately capture both the shape and texture of face images across significant variations. In this approach, three essential components, represented in hierarchical layers, are modeled using Deep Boltzmann Machines (DBM) to effectively capture the variations in facial shapes and appearances. DAMs demonstrate superiority over AAMs in inferring a representation for new face images under diverse challenging conditions.\n\nThe proposed approach is evaluated across various applications to showcase its robustness and capabilities, including facial super-resolution reconstruction, facial off-angle reconstruction or face frontalization, facial occlusion removal, and age estimation using challenging face databases such as Labeled Face Parts in the Wild (LFPW), Helen, and FG-NET. When compared to AAMs and other deep learning-based approaches, the proposed DAMs achieve competitive results in these applications, highlighting their advantages in handling occlusions, facial representation, and reconstruction."}, "2105.11872": {"original_text": "  Despite recent advances, standard sequence labeling systems often fail when\nprocessing noisy user-generated text or consuming the output of an Optical\nCharacter Recognition (OCR) process. In this paper, we improve the noise-aware\ntraining method by proposing an empirical error generation approach that\nemploys a sequence-to-sequence model trained to perform translation from\nerror-free to erroneous text. Using an OCR engine, we generated a large\nparallel text corpus for training and produced several real-world noisy\nsequence labeling benchmarks for evaluation. Moreover, to overcome the data\nsparsity problem that exacerbates in the case of imperfect textual input, we\nlearned noisy language model-based embeddings. Our approach outperformed the\nbaseline noise generation and error correction techniques on the erroneous\nsequence labeling data sets. To facilitate future research on robustness, we\nmake our code, embeddings, and data conversion scripts publicly available.\n", "rewritten_text": "Despite recent advances, standard sequence labeling systems often struggle when processing noisy user-generated text or consuming the output of an Optical Character Recognition (OCR) process. In this paper, we enhance the noise-aware training method by introducing an empirical error generation approach that utilizes a sequence-to-sequence model trained to translate from error-free to erroneous text. By leveraging an OCR engine, we created a substantial parallel text corpus for training and developed multiple real-world noisy sequence labeling benchmarks for evaluation. Additionally, to address the issue of data sparsity that is exacerbated by imperfect textual input, we incorporated noisy language model-based embeddings. Our approach surpassed the baseline noise generation and error correction techniques on the erroneous sequence labeling datasets. To support future research on robustness, we have made our code, embeddings, and data conversion scripts publicly accessible."}, "2102.00062": {"original_text": "  In this paper, we present a method of clothes retargeting; generating the\npotential poses and deformations of a given 3D clothing template model to fit\nonto a person in a single RGB image. The problem is fundamentally ill-posed as\nattaining the ground truth data is impossible, i.e., images of people wearing\nthe different 3D clothing template model at exact same pose. We address this\nchallenge by utilizing large-scale synthetic data generated from physical\nsimulation, allowing us to map 2D dense body pose to 3D clothing deformation.\nWith the simulated data, we propose a semi-supervised learning framework that\nvalidates the physical plausibility of the 3D deformation by matching with the\nprescribed body-to-cloth contact points and clothing silhouette to fit onto the\nunlabeled real images. A new neural clothes retargeting network (CRNet) is\ndesigned to integrate the semi-supervised retargeting task in an end-to-end\nfashion. In our evaluation, we show that our method can predict the realistic\n3D pose and deformation field needed for retargeting clothes models in\nreal-world examples.\n", "rewritten_text": "This paper introduces a method for clothes retargeting, which involves generating potential poses and deformations of a given 3D clothing template model to adapt it onto a person in a single RGB image. The problem at hand is inherently challenging due to the impossibility of obtaining ground truth data, such as images of individuals wearing the same 3D clothing template model in identical poses. To tackle this issue, we leverage large-scale synthetic data produced through physical simulation, enabling us to establish a correlation between 2D dense body pose and 3D clothing deformation.\n\nBy utilizing the simulated data, we propose a semi-supervised learning framework that verifies the physical plausibility of the 3D deformation by aligning it with the specified body-to-cloth contact points and clothing silhouette to seamlessly fit onto unlabeled real images. To facilitate this process, we introduce a novel neural clothes retargeting network (CRNet) designed to integrate the semi-supervised retargeting task in an end-to-end manner. Through our evaluation, we demonstrate the effectiveness of our method in accurately predicting the realistic 3D pose and deformation field necessary for retargeting clothes models in real-world scenarios."}, "1810.0578": {"original_text": "  Pose estimation is a widely explored problem, enabling many robotic tasks\nsuch as grasping and manipulation. In this paper, we tackle the problem of pose\nestimation for objects that exhibit rotational symmetry, which are common in\nman-made and industrial environments. In particular, our aim is to infer poses\nfor objects not seen at training time, but for which their 3D CAD models are\navailable at test time. Previous work has tackled this problem by learning to\ncompare captured views of real objects with the rendered views of their 3D CAD\nmodels, by embedding them in a joint latent space using neural networks. We\nshow that sidestepping the issue of symmetry in this scenario during training\nleads to poor performance at test time. We propose a model that reasons about\nrotational symmetry during training by having access to only a small set of\nsymmetry-labeled objects, whereby exploiting a large collection of unlabeled\nCAD models. We demonstrate that our approach significantly outperforms a\nnaively trained neural network on a new pose dataset containing images of tools\nand hardware.\n", "rewritten_text": "Pose estimation is a well-explored problem that plays a crucial role in various robotic tasks, such as grasping and manipulation. This paper addresses the challenge of pose estimation for objects with rotational symmetry, a common characteristic in man-made and industrial settings. Our specific objective is to determine the poses of objects that were not part of the training data but have their 3D CAD models available during testing.\n\nPrior research has approached this issue by training models to compare real-world object views with their corresponding 3D CAD model views, aligning them in a shared latent space using neural networks. However, we have observed that neglecting the consideration of symmetry during training results in subpar performance during testing. To address this, we propose a model that incorporates rotational symmetry during training by utilizing a small set of symmetry-labeled objects and a large collection of unlabeled CAD models. Our method surpasses the performance of conventionally trained neural networks on a novel pose dataset featuring images of tools and hardware."}, "2012.15832": {"original_text": "  Increasing the input length has been a driver of progress in language\nmodeling with transformers. We identify conditions where shorter inputs are not\nharmful, and achieve perplexity and efficiency improvements through two new\nmethods that decrease input length. First, we show that initially training a\nmodel on short subsequences before moving on to longer ones both reduces\noverall training time and, surprisingly, substantially improves perplexity.\nSecond, we show how to improve the efficiency of recurrence methods in\ntransformers, which let models condition on previously processed tokens when\ngenerating sequences that exceed the maximal length the transformer can handle\nat once. Existing methods require computationally expensive relative position\nembeddings; we introduce a simple alternative of adding absolute position\nembeddings to queries and keys instead of to word embeddings, which efficiently\nproduces superior results. We show that these recurrent models also benefit\nfrom short input lengths. Combining these techniques speeds up training by a\nfactor of 1.65, reduces memory usage, and substantially improves perplexity on\nWikiText-103, without adding any parameters.\n", "rewritten_text": "Increasing the length of input has been a key factor driving progress in language modeling using transformers. We have identified scenarios where shorter inputs do not have a negative impact, and have successfully achieved enhancements in perplexity and efficiency through the implementation of two novel methods aimed at reducing input length.\n\nFirstly, we demonstrate that by initially training a model on shorter subsequences before transitioning to longer ones, we not only decrease the overall training time but also, surprisingly, significantly enhance perplexity. Secondly, we present a method to enhance the efficiency of recurrence techniques in transformers, enabling models to condition on previously processed tokens when generating sequences that exceed the maximum length the transformer can handle in a single instance.\n\nTraditional methods necessitate the use of computationally intensive relative position embeddings. In contrast, we propose a straightforward alternative of incorporating absolute position embeddings into queries and keys instead of word embeddings, resulting in superior outcomes in a more efficient manner. Our findings indicate that these recurrent models also benefit from shorter input lengths.\n\nBy combining these techniques, we accelerate training by a factor of 1.65, reduce memory usage, and notably improve perplexity on WikiText-103, all without introducing any additional parameters."}, "cs/0006021": {"original_text": "  Systems now exist which are able to compile unification grammars into\nlanguage models that can be included in a speech recognizer, but it is so far\nunclear whether non-trivial linguistically principled grammars can be used for\nthis purpose. We describe a series of experiments which investigate the\nquestion empirically, by incrementally constructing a grammar and discovering\nwhat problems emerge when successively larger versions are compiled into finite\nstate graph representations and used as language models for a medium-vocabulary\nrecognition task.\n", "rewritten_text": "There are currently systems available that can compile unification grammars into language models for integration into a speech recognizer. However, it remains uncertain whether complex linguistically principled grammars can effectively serve this purpose. In this study, we conduct a series of experiments to empirically explore this question. We incrementally build a grammar and examine the challenges that arise as we compile progressively larger versions into finite state graph representations. These models are then utilized as language models for a medium-vocabulary recognition task."}, "2209.13351": {"original_text": "  Accurately and timely detecting multiscale small objects that contain tens of\npixels from remote sensing images (RSI) remains challenging. Most of the\nexisting solutions primarily design complex deep neural networks to learn\nstrong feature representations for objects separated from the background, which\noften results in a heavy computation burden. In this article, we propose an\naccurate yet fast object detection method for RSI, named SuperYOLO, which fuses\nmultimodal data and performs high-resolution (HR) object detection on\nmultiscale objects by utilizing the assisted super resolution (SR) learning and\nconsidering both the detection accuracy and computation cost. First, we utilize\na symmetric compact multimodal fusion (MF) to extract supplementary information\nfrom various data for improving small object detection in RSI. Furthermore, we\ndesign a simple and flexible SR branch to learn HR feature representations that\ncan discriminate small objects from vast backgrounds with low-resolution (LR)\ninput, thus further improving the detection accuracy. Moreover, to avoid\nintroducing additional computation, the SR branch is discarded in the inference\nstage, and the computation of the network model is reduced due to the LR input.\nExperimental results show that, on the widely used VEDAI RS dataset, SuperYOLO\nachieves an accuracy of 75.09% (in terms of mAP50 ), which is more than 10%\nhigher than the SOTA large models, such as YOLOv5l, YOLOv5x, and RS designed\nYOLOrs. Meanwhile, the parameter size and GFLOPs of SuperYOLO are about 18\ntimes and 3.8 times less than YOLOv5x. Our proposed model shows a favorable\naccuracy and speed tradeoff compared to the state-of-the-art models. The code\nwill be open-sourced at https://github.com/icey-zhang/SuperYOLO.\n", "rewritten_text": "Accurately and timely detecting multiscale small objects containing tens of pixels from remote sensing images (RSI) remains a challenge. Most existing solutions primarily involve designing complex deep neural networks to learn strong feature representations for objects separated from the background, leading to a heavy computation burden. In this article, we introduce a method for accurate and fast object detection in RSI called SuperYOLO. This method fuses multimodal data and conducts high-resolution (HR) object detection on multiscale objects by leveraging assisted super resolution (SR) learning. It takes into account both detection accuracy and computation cost.\n\nTo enhance small object detection in RSI, we first employ a symmetric compact multimodal fusion (MF) to extract additional information from various data sources. Additionally, we incorporate a simple and flexible SR branch to learn HR feature representations that can distinguish small objects from extensive backgrounds with low-resolution (LR) input, thereby improving detection accuracy. During the inference stage, the SR branch is omitted to prevent additional computation, reducing the network model's computational load due to the LR input.\n\nExperimental results on the widely used VEDAI RS dataset demonstrate that SuperYOLO achieves an accuracy of 75.09% (measured by mAP50), surpassing the performance of state-of-the-art large models like YOLOv5l, YOLOv5x, and RS-designed YOLOrs by more than 10%. Furthermore, SuperYOLO boasts a parameter size and GFLOPs that are approximately 18 times and 3.8 times smaller than YOLOv5x, respectively. Our proposed model strikes a favorable balance between accuracy and speed compared to existing models. The code for SuperYOLO will be made available on https://github.com/icey-zhang/SuperYOLO."}, "2109.01958": {"original_text": "  Transformer-based pre-trained language models boost the performance of\nopen-domain dialogue systems. Prior works leverage Transformer-based\npre-trained language models to generate texts with desired attributes in two\ngeneral approaches: (1) gradient-based methods: updating all latent\nrepresentations of pre-trained models with gradients from attribute models; (2)\nweighted-decoding methods: re-ranking beam candidates from pre-trained models\nwith attribute functions. However, gradient-based methods lead to high\ncomputation cost and can easily get overfitted on small training sets, while\nweighted-decoding methods are inherently constrained by the low-variance\nhigh-bias pre-trained model. In this work, we propose a novel approach to\ncontrol the generation of Transformer-based pre-trained language models: the\nSideControl framework, which leverages a novel control attributes loss to\nincorporate useful control signals, and is shown to perform well with very\nlimited training samples. We evaluate our proposed method on two benchmark\nopen-domain dialogue datasets, and results show that the SideControl framework\nhas better controllability, higher generation quality and better\nsample-efficiency than existing gradient-based and weighted-decoding baselines.\n", "rewritten_text": "Transformer-based pre-trained language models enhance the performance of open-domain dialogue systems. Previous studies have utilized Transformer-based pre-trained language models in two main approaches to generate texts with specific attributes: (1) gradient-based methods involve updating all latent representations of pre-trained models using gradients from attribute models; (2) weighted-decoding methods entail re-ranking beam candidates from pre-trained models with attribute functions. However, gradient-based methods are computationally expensive and prone to overfitting on small training sets, whereas weighted-decoding methods are limited by the low-variance high-bias nature of pre-trained models. In this study, we introduce a novel approach for controlling the generation of Transformer-based pre-trained language models: the SideControl framework. This framework utilizes a unique control attributes loss to integrate valuable control signals and demonstrates strong performance even with minimal training data. Our method is evaluated on two standard open-domain dialogue datasets, revealing that the SideControl framework offers superior controllability, higher generation quality, and improved sample efficiency compared to existing gradient-based and weighted-decoding approaches."}, "2205.02022": {"original_text": "  Recent advances in the pre-training of language models leverage large-scale\ndatasets to create multilingual models. However, low-resource languages are\nmostly left out in these datasets. This is primarily because many widely spoken\nlanguages are not well represented on the web and therefore excluded from the\nlarge-scale crawls used to create datasets. Furthermore, downstream users of\nthese models are restricted to the selection of languages originally chosen for\npre-training. This work investigates how to optimally leverage existing\npre-trained models to create low-resource translation systems for 16 African\nlanguages. We focus on two questions: 1) How can pre-trained models be used for\nlanguages not included in the initial pre-training? and 2) How can the\nresulting translation models effectively transfer to new domains? To answer\nthese questions, we create a new African news corpus covering 16 languages, of\nwhich eight languages are not part of any existing evaluation dataset. We\ndemonstrate that the most effective strategy for transferring both to\nadditional languages and to additional domains is to fine-tune large\npre-trained models on small quantities of high-quality translation data.\n", "rewritten_text": "Recent advancements in language model pre-training have been utilizing large-scale datasets to develop multilingual models. However, low-resource languages are often overlooked in these datasets due to the limited representation of many widely spoken languages on the web, leading to their exclusion from the large-scale crawls used for dataset creation. Consequently, users of these models are constrained to the languages originally included in the pre-training process. This study delves into the optimal utilization of existing pre-trained models to establish low-resource translation systems for 16 African languages. The research focuses on addressing two key inquiries: 1) How can pre-trained models be effectively applied to languages not initially incorporated in the pre-training phase? and 2) How can the resulting translation models seamlessly adapt to new domains? To address these questions, a new African news corpus encompassing 16 languages is curated, with eight of these languages not being part of any existing evaluation dataset. The study demonstrates that the most efficient approach for extending the applicability of pre-trained models to additional languages and domains is through fine-tuning large pre-trained models using small amounts of high-quality translation data."}, "2109.12028": {"original_text": "  Human knowledge is collectively encoded in the roughly 6500 languages spoken\naround the world, but it is not distributed equally across languages. Hence,\nfor information-seeking question answering (QA) systems to adequately serve\nspeakers of all languages, they need to operate cross-lingually. In this work\nwe investigate the capabilities of multilingually pre-trained language models\non cross-lingual QA. We find that explicitly aligning the representations\nacross languages with a post-hoc fine-tuning step generally leads to improved\nperformance. We additionally investigate the effect of data size as well as the\nlanguage choice in this fine-tuning step, also releasing a dataset for\nevaluating cross-lingual QA systems. Code and dataset are publicly available\nhere: https://github.com/ffaisal93/aligned_qa\n", "rewritten_text": "Human knowledge is collectively encoded in the approximately 6500 languages spoken worldwide, yet it is not evenly distributed among these languages. Therefore, in order for information-seeking question answering (QA) systems to effectively cater to speakers of all languages, they must be able to operate across languages. In this study, we explore the capabilities of multilingually pre-trained language models in cross-lingual QA. Our findings suggest that explicitly aligning representations across languages through a post-hoc fine-tuning process generally results in improved performance. Furthermore, we examine the impact of data size and language selection during this fine-tuning stage, and we have also made available a dataset for evaluating cross-lingual QA systems. The code and dataset can be accessed publicly at: https://github.com/ffaisal93/aligned_qa"}, "2010.00363": {"original_text": "  Long Short-Term Memory recurrent neural network (LSTM) is widely used and\nknown to capture informative long-term syntactic dependencies. However, how\nsuch information are reflected in its internal vectors for natural text has not\nyet been sufficiently investigated. We analyze them by learning a language\nmodel where syntactic structures are implicitly given. We empirically show that\nthe context update vectors, i.e. outputs of internal gates, are approximately\nquantized to binary or ternary values to help the language model to count the\ndepth of nesting accurately, as Suzgun et al. (2019) recently show for\nsynthetic Dyck languages. For some dimensions in the context vector, we show\nthat their activations are highly correlated with the depth of phrase\nstructures, such as VP and NP. Moreover, with an $L_1$ regularization, we also\nfound that it can accurately predict whether a word is inside a phrase\nstructure or not from a small number of components of the context vector. Even\nfor the case of learning from raw text, context vectors are shown to still\ncorrelate well with the phrase structures. Finally, we show that natural\nclusters of the functional words and the part of speeches that trigger phrases\nare represented in a small but principal subspace of the context-update vector\nof LSTM.\n", "rewritten_text": "The Long Short-Term Memory recurrent neural network (LSTM) is widely recognized for its ability to capture informative long-term syntactic dependencies. However, the extent to which this information is reflected in its internal vectors for natural text has not been thoroughly investigated. To address this gap, we conducted an analysis by training a language model that implicitly incorporates syntactic structures. Our empirical findings reveal that the context update vectors, which are the outputs of internal gates, tend to be quantized to binary or ternary values. This quantization aids the language model in accurately determining the depth of nesting, as demonstrated by Suzgun et al. (2019) in the context of synthetic Dyck languages.\n\nIn our study, we observed a strong correlation between certain dimensions of the context vector and the depth of phrase structures, such as verb phrases (VP) and noun phrases (NP). Additionally, through the application of $L_1$ regularization, we were able to predict with high accuracy whether a word is located within a phrase structure based on a small subset of components within the context vector. Notably, even when learning from raw text, the context vectors continue to exhibit a strong correlation with phrase structures.\n\nFurthermore, our analysis revealed that natural groupings of functional words and parts of speech that trigger phrases are represented within a compact and essential subspace of the context-update vector of the LSTM model."}, "2005.02877": {"original_text": "  Task-oriented dialog systems rely on dialog state tracking (DST) to monitor\nthe user's goal during the course of an interaction. Multi-domain and\nopen-vocabulary settings complicate the task considerably and demand scalable\nsolutions. In this paper we present a new approach to DST which makes use of\nvarious copy mechanisms to fill slots with values. Our model has no need to\nmaintain a list of candidate values. Instead, all values are extracted from the\ndialog context on-the-fly. A slot is filled by one of three copy mechanisms:\n(1) Span prediction may extract values directly from the user input; (2) a\nvalue may be copied from a system inform memory that keeps track of the\nsystem's inform operations; (3) a value may be copied over from a different\nslot that is already contained in the dialog state to resolve coreferences\nwithin and across domains. Our approach combines the advantages of span-based\nslot filling methods with memory methods to avoid the use of value picklists\naltogether. We argue that our strategy simplifies the DST task while at the\nsame time achieving state of the art performance on various popular evaluation\nsets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55%.\n", "rewritten_text": "Task-oriented dialog systems rely on dialog state tracking (DST) to monitor the user's goal throughout an interaction. The task becomes significantly more complex in multi-domain and open-vocabulary settings, necessitating scalable solutions. This paper introduces a novel approach to DST that leverages various copy mechanisms to populate slots with values. Unlike traditional models that require maintaining a list of candidate values, our model dynamically extracts all values from the dialog context. A slot can be filled using one of three copy mechanisms: (1) Span prediction extracts values directly from user input; (2) a value can be copied from a system inform memory that records the system's inform operations; (3) a value may be copied from a different slot already present in the dialog state to resolve coreferences within and across domains. Our approach combines the benefits of span-based slot filling methods with memory methods, eliminating the need for value picklists. We contend that our strategy streamlines the DST task while achieving state-of-the-art performance on various popular evaluation datasets, including Multiwoz 2.1, where we achieve a joint goal accuracy exceeding 55%."}, "2408.06899": {"original_text": "  We present a novel method for measuring the rate of periodic phenomena (e.g.,\nrotation, flicker, and vibration), by an event camera, a device asynchronously\nreporting brightness changes at independently operating pixels with high\ntemporal resolution. The approach assumes that for a periodic phenomenon, a\nhighly similar set of events is generated within a spatio-temporal window at a\ntime difference corresponding to its period. The sets of similar events are\ndetected by a correlation in the spatio-temporal event stream space. The\nproposed method, EEPPR, is evaluated on a dataset of 12 sequences of periodic\nphenomena, i.e. flashing light and vibration, and periodic motion, e.g.,\nrotation, ranging from 3.2 Hz to 2 kHz (equivalent to 192 - 120 000 RPM). EEPPR\nsignificantly outperforms published methods on this dataset, achieving a mean\nrelative error of 0.1%, setting new state-of-the-art. The dataset and codes are\npublicly available on GitHub.\n", "rewritten_text": "We introduce a new method for measuring the rate of periodic phenomena, such as rotation, flicker, and vibration, using an event camera. This device reports brightness changes asynchronously at individual pixels, offering high temporal resolution. Our approach is based on the assumption that a periodic phenomenon generates a highly similar set of events within a spatio-temporal window at a time interval corresponding to its period. These sets of similar events are identified through correlation in the spatio-temporal event stream space. The proposed method, known as EEPPR, is assessed using a dataset comprising 12 sequences of periodic phenomena, including flashing light, vibration, and rotational motion ranging from 3.2 Hz to 2 kHz (equivalent to 192 - 120,000 RPM). EEPPR demonstrates superior performance compared to existing methods on this dataset, achieving a mean relative error of 0.1% and establishing a new state-of-the-art. The dataset and codes are publicly accessible on GitHub."}, "1703.08448": {"original_text": "  We investigate a principle way to progressively mine discriminative object\nregions using classification networks to address the weakly-supervised semantic\nsegmentation problems. Classification networks are only responsive to small and\nsparse discriminative regions from the object of interest, which deviates from\nthe requirement of the segmentation task that needs to localize dense, interior\nand integral regions for pixel-wise inference. To mitigate this gap, we propose\na new adversarial erasing approach for localizing and expanding object regions\nprogressively. Starting with a single small object region, our proposed\napproach drives the classification network to sequentially discover new and\ncomplement object regions by erasing the current mined regions in an\nadversarial manner. These localized regions eventually constitute a dense and\ncomplete object region for learning semantic segmentation. To further enhance\nthe quality of the discovered regions by adversarial erasing, an online\nprohibitive segmentation learning approach is developed to collaborate with\nadversarial erasing by providing auxiliary segmentation supervision modulated\nby the more reliable classification scores. Despite its apparent simplicity,\nthe proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union\n(mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new\nstate-of-the-arts.\n", "rewritten_text": "We explore a novel approach to incrementally extract discriminative object regions using classification networks to tackle weakly-supervised semantic segmentation challenges. Classification networks typically identify small and sparse discriminative regions within the object of interest, which differs from the segmentation task's requirement of localizing dense, interior, and complete regions for pixel-wise inference. To bridge this disparity, we introduce a new adversarial erasing technique for progressively localizing and expanding object regions. Beginning with a single small object region, our method guides the classification network to sequentially uncover new and complementary object regions by adversarially erasing the currently identified regions. These localized regions eventually form a dense and comprehensive object region for semantic segmentation training. To further improve the quality of the identified regions through adversarial erasing, we develop an online prohibitive segmentation learning strategy to work in conjunction with adversarial erasing by offering auxiliary segmentation guidance adjusted by more reliable classification scores. Despite its apparent simplicity, our proposed approach achieves state-of-the-art mean Intersection-over-Union (mIoU) scores of 55.0% and 55.7% on the PASCAL VOC 2012 validation and test sets."}, "1708.00284": {"original_text": "  Future frame prediction in videos is a promising avenue for unsupervised\nvideo representation learning. Video frames are naturally generated by the\ninherent pixel flows from preceding frames based on the appearance and motion\ndynamics in the video. However, existing methods focus on directly\nhallucinating pixel values, resulting in blurry predictions. In this paper, we\ndevelop a dual motion Generative Adversarial Net (GAN) architecture, which\nlearns to explicitly enforce future-frame predictions to be consistent with the\npixel-wise flows in the video through a dual-learning mechanism. The primal\nfuture-frame prediction and dual future-flow prediction form a closed loop,\ngenerating informative feedback signals to each other for better video\nprediction. To make both synthesized future frames and flows indistinguishable\nfrom reality, a dual adversarial training method is proposed to ensure that the\nfuture-flow prediction is able to help infer realistic future-frames, while the\nfuture-frame prediction in turn leads to realistic optical flows. Our dual\nmotion GAN also handles natural motion uncertainty in different pixel locations\nwith a new probabilistic motion encoder, which is based on variational\nautoencoders. Extensive experiments demonstrate that the proposed dual motion\nGAN significantly outperforms state-of-the-art approaches on synthesizing new\nvideo frames and predicting future flows. Our model generalizes well across\ndiverse visual scenes and shows superiority in unsupervised video\nrepresentation learning.\n", "rewritten_text": "Future frame prediction in videos is a promising avenue for unsupervised video representation learning. Video frames are naturally generated by the inherent pixel flows from preceding frames, based on the appearance and motion dynamics in the video. However, existing methods primarily focus on directly hallucinating pixel values, leading to blurry predictions. In this paper, we introduce a dual motion Generative Adversarial Net (GAN) architecture that aims to explicitly enforce future-frame predictions to align with the pixel-wise flows in the video through a dual-learning mechanism. The primal future-frame prediction and dual future-flow prediction work in tandem, forming a closed loop that generates informative feedback signals to enhance video prediction accuracy. To ensure that both synthesized future frames and flows are indistinguishable from reality, we propose a dual adversarial training method. This method guarantees that the future-flow prediction aids in inferring realistic future-frames, while the future-frame prediction, in turn, results in realistic optical flows. Our dual motion GAN also addresses natural motion uncertainty in various pixel locations by incorporating a new probabilistic motion encoder based on variational autoencoders. Extensive experiments demonstrate that the proposed dual motion GAN outperforms state-of-the-art approaches in synthesizing new video frames and predicting future flows. Our model exhibits strong generalization capabilities across diverse visual scenes and demonstrates superiority in unsupervised video representation learning."}, "1405.6103": {"original_text": "  The Swiss avalanche bulletin is produced twice a day in four languages. Due\nto the lack of time available for manual translation, a fully automated\ntranslation system is employed, based on a catalogue of predefined phrases and\npredetermined rules of how these phrases can be combined to produce sentences.\nThe system is able to automatically translate such sentences from German into\nthe target languages French, Italian and English without subsequent\nproofreading or correction. Our catalogue of phrases is limited to a small\nsublanguage. The reduction of daily translation costs is expected to offset the\ninitial development costs within a few years. After being operational for two\nwinter seasons, we assess here the quality of the produced texts based on an\nevaluation where participants rate real danger descriptions from both origins,\nthe catalogue of phrases versus the manually written and translated texts. With\na mean recognition rate of 55%, users can hardly distinguish between the two\ntypes of texts, and give similar ratings with respect to their language\nquality. Overall, the output from the catalogue system can be considered\nvirtually equivalent to a text written by avalanche forecasters and then\nmanually translated by professional translators. Furthermore, forecasters\ndeclared that all relevant situations were captured by the system with\nsufficient accuracy and within the limited time available.\n", "rewritten_text": "The Swiss avalanche bulletin is generated twice daily in four languages. To streamline the translation process due to time constraints, an automated translation system is utilized. This system relies on a set of predefined phrases and rules for combining them to form sentences. It can automatically translate sentences from German into French, Italian, and English without the need for subsequent proofreading or corrections. The phrase catalog is limited to a specific sublanguage. The reduction in daily translation costs is projected to offset the initial development expenses within a few years. After two winter seasons of operation, an evaluation was conducted to assess the quality of the generated texts. Participants rated real danger descriptions from both the phrase catalog and manually written and translated texts. The results showed that users could hardly differentiate between the two types of texts, with a mean recognition rate of 55%. They also gave similar ratings regarding language quality. Overall, the output from the automated system can be considered nearly equivalent to a text written by avalanche forecasters and then translated manually by professional translators. Additionally, forecasters confirmed that the system accurately captured all relevant situations within the limited timeframe available."}, "2408.15063": {"original_text": "  Although most existing multi-modal salient object detection (SOD) methods\ndemonstrate effectiveness through training models from scratch, the limited\nmulti-modal data hinders these methods from reaching optimality. In this paper,\nwe propose a novel framework to explore and exploit the powerful feature\nrepresentation and zero-shot generalization ability of the pre-trained Segment\nAnything Model (SAM) for multi-modal SOD. Despite serving as a recent vision\nfundamental model, driving the class-agnostic SAM to comprehend and detect\nsalient objects accurately is non-trivial, especially in challenging scenes. To\nthis end, we develop \\underline{SAM} with se\\underline{m}antic\nf\\underline{e}ature fu\\underline{s}ion guidanc\\underline{e} (Sammese), which\nincorporates multi-modal saliency-specific knowledge into SAM to adapt SAM to\nmulti-modal SOD tasks. However, it is difficult for SAM trained on single-modal\ndata to directly mine the complementary benefits of multi-modal inputs and\ncomprehensively utilize them to achieve accurate saliency prediction. To\naddress these issues, we first design a multi-modal complementary fusion module\nto extract robust multi-modal semantic features by integrating information from\nvisible and thermal or depth image pairs. Then, we feed the extracted\nmulti-modal semantic features into both the SAM image encoder and mask decoder\nfor fine-tuning and prompting, respectively. Specifically, in the image\nencoder, a multi-modal adapter is proposed to adapt the single-modal SAM to\nmulti-modal information. In the mask decoder, a semantic-geometric prompt\ngeneration strategy is proposed to produce corresponding embeddings with\nvarious saliency cues. Extensive experiments on both RGB-D and RGB-T SOD\nbenchmarks show the effectiveness of the proposed framework. The code will be\navailable at \\url{https://github.com/Angknpng/Sammese}.\n", "rewritten_text": "While many current multi-modal salient object detection (SOD) methods have shown effectiveness by training models from scratch, the limited availability of multi-modal data hampers their ability to achieve optimality. This paper introduces a novel framework that leverages the powerful feature representation and zero-shot generalization capability of the pre-trained Segment Anything Model (SAM) for multi-modal SOD. Despite SAM being a recent foundational model in computer vision, guiding the class-agnostic SAM to accurately comprehend and detect salient objects, especially in challenging scenes, presents a non-trivial task.\n\nTo address this challenge, we introduce SAM with semantic feature fusion guidance (Sammese), which integrates multi-modal saliency-specific knowledge into SAM to tailor it for multi-modal SOD tasks. However, enabling SAM, trained on single-modal data, to effectively harness the complementary advantages of multi-modal inputs and utilize them for precise saliency prediction is complex. To tackle these issues, we first develop a multi-modal complementary fusion module to extract robust multi-modal semantic features by combining information from visible and thermal or depth image pairs. Subsequently, we input these extracted multi-modal semantic features into both the SAM image encoder and mask decoder for fine-tuning and prompting, respectively.\n\nSpecifically, in the image encoder, we propose a multi-modal adapter to adjust the single-modal SAM for multi-modal information. In the mask decoder, a strategy for generating semantic-geometric prompts is introduced to create corresponding embeddings with diverse saliency cues. Extensive experiments conducted on RGB-D and RGB-T SOD benchmarks demonstrate the effectiveness of the proposed framework. The code for this framework will be accessible at \\url{https://github.com/Angknpng/Sammese}."}, "2210.09563": {"original_text": "  With the continuous development of deep learning in the field of image\ngeneration models, a large number of vivid forged faces have been generated and\nspread on the Internet. These high-authenticity artifacts could grow into a\nthreat to society security. Existing face forgery detection methods directly\nutilize the obtained public shared or centralized data for training but ignore\nthe personal privacy and security issues when personal data couldn't be\ncentralizedly shared in real-world scenarios. Additionally, different\ndistributions caused by diverse artifact types would further bring adverse\ninfluences on the forgery detection task. To solve the mentioned problems, the\npaper proposes a novel generalized residual Federated learning for face Forgery\ndetection (FedForgery). The designed variational autoencoder aims to learn\nrobust discriminative residual feature maps to detect forgery faces (with\ndiverse or even unknown artifact types). Furthermore, the general federated\nlearning strategy is introduced to construct distributed detection model\ntrained collaboratively with multiple local decentralized devices, which could\nfurther boost the representation generalization. Experiments conducted on\npublicly available face forgery detection datasets prove the superior\nperformance of the proposed FedForgery. The designed novel generalized face\nforgery detection protocols and source code would be publicly available.\n", "rewritten_text": "The continuous development of deep learning in image generation models has led to the creation and dissemination of numerous realistic forged faces on the Internet. These highly authentic forgeries pose a potential threat to societal security. Current face forgery detection methods rely on public or centralized data for training, overlooking personal privacy and security concerns in real-world scenarios where centralized data sharing is not feasible. Moreover, the presence of diverse artifact types can complicate forgery detection efforts.\n\nTo address these challenges, this paper introduces a novel approach called Generalized Residual Federated Learning for Face Forgery Detection (FedForgery). The proposed method utilizes a variational autoencoder to learn robust discriminative residual feature maps for detecting forged faces with various or unknown artifact types. Additionally, a federated learning strategy is employed to develop a distributed detection model that is trained collaboratively across multiple local decentralized devices, enhancing representation generalization.\n\nExperimental results on publicly available face forgery detection datasets demonstrate the superior performance of FedForgery. The proposed generalized face forgery detection protocols and accompanying source code will be made publicly accessible."}, "2304.05995": {"original_text": "  In recent years, the success of large-scale vision-language models (VLMs)\nsuch as CLIP has led to their increased usage in various computer vision tasks.\nThese models enable zero-shot inference through carefully crafted instructional\ntext prompts without task-specific supervision. However, the potential of VLMs\nfor generalization tasks in remote sensing (RS) has not been fully realized. To\naddress this research gap, we propose a novel image-conditioned prompt learning\nstrategy called the Visual Attention Parameterized Prompts Learning Network\n(APPLeNet). APPLeNet emphasizes the importance of multi-scale feature learning\nin RS scene classification and disentangles visual style and content primitives\nfor domain generalization tasks. To achieve this, APPLeNet combines visual\ncontent features obtained from different layers of the vision encoder and style\nproperties obtained from feature statistics of domain-specific batches. An\nattention-driven injection module is further introduced to generate visual\ntokens from this information. We also introduce an anti-correlation regularizer\nto ensure discrimination among the token embeddings, as this visual information\nis combined with the textual tokens. To validate APPLeNet, we curated four\navailable RS benchmarks and introduced experimental protocols and datasets for\nthree domain generalization tasks. Our results consistently outperform the\nrelevant literature and code is available at\nhttps://github.com/mainaksingha01/APPLeNet\n", "rewritten_text": "In recent years, the success of large-scale vision-language models (VLMs) like CLIP has led to their increased utilization in various computer vision tasks. These models facilitate zero-shot inference by utilizing carefully crafted instructional text prompts without the need for task-specific supervision. Despite this, the potential of VLMs for generalization tasks in remote sensing (RS) remains largely untapped. To bridge this research gap, we introduce a novel image-conditioned prompt learning strategy known as the Visual Attention Parameterized Prompts Learning Network (APPLeNet).\n\nAPPLeNet underscores the significance of multi-scale feature learning in RS scene classification and separates visual style and content primitives for domain generalization tasks. To accomplish this, APPLeNet integrates visual content features from different layers of the vision encoder and style attributes from feature statistics of domain-specific batches. Additionally, an attention-driven injection module is introduced to generate visual tokens based on this information. Furthermore, an anti-correlation regularizer is implemented to ensure discrimination among the token embeddings, as this visual data is merged with the textual tokens.\n\nTo validate the effectiveness of APPLeNet, we curated four existing RS benchmarks and established experimental protocols and datasets for three domain generalization tasks. Our results consistently outperform existing literature, and the code is accessible at https://github.com/mainaksingha01/APPLeNet."}, "1804.03287": {"original_text": "  Despite the noticeable progress in perceptual tasks like detection, instance\nsegmentation and human parsing, computers still perform unsatisfactorily on\nvisually understanding humans in crowded scenes, such as group behavior\nanalysis, person re-identification and autonomous driving, etc. To this end,\nmodels need to comprehensively perceive the semantic information and the\ndifferences between instances in a multi-human image, which is recently defined\nas the multi-human parsing task. In this paper, we present a new large-scale\ndatabase \"Multi-Human Parsing (MHP)\" for algorithm development and evaluation,\nand advances the state-of-the-art in understanding humans in crowded scenes.\nMHP contains 25,403 elaborately annotated images with 58 fine-grained semantic\ncategory labels, involving 2-26 persons per image and captured in real-world\nscenes from various viewpoints, poses, occlusion, interactions and background.\nWe further propose a novel deep Nested Adversarial Network (NAN) model for\nmulti-human parsing. NAN consists of three Generative Adversarial Network\n(GAN)-like sub-nets, respectively performing semantic saliency prediction,\ninstance-agnostic parsing and instance-aware clustering. These sub-nets form a\nnested structure and are carefully designed to learn jointly in an end-to-end\nway. NAN consistently outperforms existing state-of-the-art solutions on our\nMHP and several other datasets, and serves as a strong baseline to drive the\nfuture research for multi-human parsing.\n", "rewritten_text": "Despite the significant advancements in perceptual tasks such as detection, instance segmentation, and human parsing, computers still struggle to effectively understand humans in crowded scenes. This includes challenges like analyzing group behavior, person re-identification, and autonomous driving. To address this issue, models must be able to accurately interpret semantic information and distinguish between individuals in images containing multiple humans, a task referred to as multi-human parsing. This paper introduces a new extensive database called \"Multi-Human Parsing (MHP)\" for the development and evaluation of algorithms, pushing the boundaries of human understanding in crowded environments.\n\nThe MHP database comprises 25,403 meticulously annotated images with 58 detailed semantic category labels. These images feature 2-26 individuals per image and are captured in real-world settings from various perspectives, poses, occlusions, interactions, and backgrounds. Additionally, a novel deep learning model called Nested Adversarial Network (NAN) is proposed for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-networks, each dedicated to semantic saliency prediction, instance-agnostic parsing, and instance-aware clustering. These sub-networks are intricately designed to work together in an end-to-end manner within a nested structure.\n\nNAN consistently outperforms existing state-of-the-art solutions on the MHP dataset and other datasets, establishing itself as a robust baseline for driving future research in multi-human parsing."}, "2206.10779": {"original_text": "  We propose a large-scale dataset of real-world rainy and clean image pairs\nand a method to remove degradations, induced by rain streaks and rain\naccumulation, from the image. As there exists no real-world dataset for\nderaining, current state-of-the-art methods rely on synthetic data and thus are\nlimited by the sim2real domain gap; moreover, rigorous evaluation remains a\nchallenge due to the absence of a real paired dataset. We fill this gap by\ncollecting a real paired deraining dataset through meticulous control of\nnon-rain variations. Our dataset enables paired training and quantitative\nevaluation for diverse real-world rain phenomena (e.g. rain streaks and rain\naccumulation). To learn a representation robust to rain phenomena, we propose a\ndeep neural network that reconstructs the underlying scene by minimizing a\nrain-robust loss between rainy and clean images. Extensive experiments\ndemonstrate that our model outperforms the state-of-the-art deraining methods\non real rainy images under various conditions. Project website:\nhttps://visual.ee.ucla.edu/gt_rain.htm/.\n", "rewritten_text": "We present a comprehensive dataset comprising real-world pairs of rainy and clean images, along with a methodology designed to eliminate degradations caused by rain streaks and accumulation in images. The lack of a real-world dataset for deraining has led current cutting-edge techniques to rely on synthetic data, resulting in limitations due to the sim2real domain gap. Furthermore, the absence of a genuine paired dataset poses challenges for rigorous evaluation. To address this gap, we have meticulously curated a real paired deraining dataset, carefully controlling for non-rain variations. This dataset facilitates paired training and quantitative assessment of various real-world rain phenomena, such as rain streaks and accumulation. Our approach involves a deep neural network that reconstructs the underlying scene by minimizing a rain-robust loss between rainy and clean images to learn a representation resilient to rain effects. Through extensive experiments, we have demonstrated that our model surpasses state-of-the-art deraining methods when applied to real rainy images across diverse conditions. For more information, please visit our project website at https://visual.ee.ucla.edu/gt_rain.htm/."}, "1812.08115": {"original_text": "  We introduce a model-based deep learning architecture termed MoDL-MUSSELS for\nthe correction of phase errors in multishot diffusion-weighted echo-planar MRI\nimages. The proposed algorithm is a generalization of existing MUSSELS\nalgorithm with similar performance but with significantly reduced computational\ncomplexity. In this work, we show that an iterative re-weighted least-squares\nimplementation of MUSSELS alternates between a multichannel filter bank and the\nenforcement of data consistency. The multichannel filter bank projects the data\nto the signal subspace thus exploiting the phase relations between shots. Due\nto the high computational complexity of self-learned filter bank, we propose to\nreplace it with a convolutional neural network (CNN) whose parameters are\nlearned from exemplary data. The proposed CNN is a hybrid model involving a\nmultichannel CNN in the k-space and another CNN in the image space. The k-space\nCNN exploits the phase relations between the shot images, while the image\ndomain network is used to project the data to an image manifold. The\nexperiments show that the proposed scheme can yield reconstructions that are\ncomparable to state of the art methods while offering several orders of\nmagnitude reduction in run-time.\n", "rewritten_text": "We present a model-based deep learning architecture called MoDL-MUSSELS designed for correcting phase errors in multishot diffusion-weighted echo-planar MRI images. This algorithm is an extension of the existing MUSSELS algorithm, offering similar performance but with significantly reduced computational complexity. Our approach involves an iterative re-weighted least-squares implementation of MUSSELS, which alternates between a multichannel filter bank and enforcing data consistency. The multichannel filter bank projects the data into the signal subspace, leveraging phase relationships between shots.\n\nGiven the high computational complexity of the self-learned filter bank, we propose replacing it with a convolutional neural network (CNN) trained on exemplary data. This CNN model is a hybrid, comprising a multichannel CNN in the k-space and another CNN in the image space. The k-space CNN exploits phase relationships between shot images, while the image domain network projects the data onto an image manifold.\n\nExperimental results demonstrate that our proposed approach can produce reconstructions comparable to state-of-the-art methods while achieving a significant reduction in runtime by several orders of magnitude."}, "1912.10644": {"original_text": "  In spite of the recent progresses on classifying 3D point cloud with deep\nCNNs, large geometric transformations like rotation and translation remain\nchallenging problem and harm the final classification performance. To address\nthis challenge, we propose Geometry Sharing Network (GS-Net) which effectively\nlearns point descriptors with holistic context to enhance the robustness to\ngeometric transformations. Compared with previous 3D point CNNs which perform\nconvolution on nearby points, GS-Net can aggregate point features in a more\nglobal way. Specially, GS-Net consists of Geometry Similarity Connection (GSC)\nmodules which exploit Eigen-Graph to group distant points with similar and\nrelevant geometric information, and aggregate features from nearest neighbors\nin both Euclidean space and Eigenvalue space. This design allows GS-Net to\nefficiently capture both local and holistic geometric features such as\nsymmetry, curvature, convexity and connectivity. Theoretically, we show the\nnearest neighbors of each point in Eigenvalue space are invariant to rotation\nand translation. We conduct extensive experiments on public datasets,\nModelNet40, ShapeNet Part. Experiments demonstrate that GS-Net achieves the\nstate-of-the-art performances on major datasets, 93.3% on ModelNet40, and are\nmore robust to geometric transformations.\n", "rewritten_text": "Despite recent progress in classifying 3D point clouds using deep CNNs, large geometric transformations such as rotation and translation remain challenging and can negatively impact final classification performance. To tackle this issue, we introduce the Geometry Sharing Network (GS-Net), which effectively learns point descriptors with holistic context to improve robustness against geometric transformations. Unlike previous 3D point CNNs that perform convolution on nearby points, GS-Net can aggregate point features in a more global manner. Specifically, GS-Net comprises Geometry Similarity Connection (GSC) modules that leverage Eigen-Graph to group distant points with similar and relevant geometric information, and aggregate features from nearest neighbors in both Euclidean space and Eigenvalue space. This design enables GS-Net to efficiently capture both local and holistic geometric features such as symmetry, curvature, convexity, and connectivity. Theoretically, we demonstrate that the nearest neighbors of each point in Eigenvalue space are invariant to rotation and translation. We conduct extensive experiments on public datasets, including ModelNet40 and ShapeNet Part, which show that GS-Net achieves state-of-the-art performance, with an accuracy of 93.3% on ModelNet40, and exhibits increased robustness to geometric transformations."}, "2111.01515": {"original_text": "  The enormous amount of data being generated on the web and social media has\nincreased the demand for detecting online hate speech. Detecting hate speech\nwill reduce their negative impact and influence on others. A lot of effort in\nthe Natural Language Processing (NLP) domain aimed to detect hate speech in\ngeneral or detect specific hate speech such as religion, race, gender, or\nsexual orientation. Hate communities tend to use abbreviations, intentional\nspelling mistakes, and coded words in their communication to evade detection,\nadding more challenges to hate speech detection tasks. Thus, word\nrepresentation will play an increasingly pivotal role in detecting hate speech.\nThis paper investigates the feasibility of leveraging domain-specific word\nembedding in Bidirectional LSTM based deep model to automatically\ndetect/classify hate speech. Furthermore, we investigate the use of the\ntransfer learning language model (BERT) on hate speech problem as a binary\nclassification task. The experiments showed that domainspecific word embedding\nwith the Bidirectional LSTM based deep model achieved a 93% f1-score while BERT\nachieved up to 96% f1-score on a combined balanced dataset from available hate\nspeech datasets.\n", "rewritten_text": "The proliferation of data on the internet and social media has led to an increased demand for the detection of online hate speech. Identifying hate speech can help mitigate its negative impact and influence on individuals. Significant efforts in the field of Natural Language Processing (NLP) have been directed towards detecting hate speech in general, as well as specific forms targeting religion, race, gender, or sexual orientation. Hate groups often employ tactics such as abbreviations, intentional misspellings, and coded language in their communications to avoid detection, posing additional challenges for hate speech detection tasks. Consequently, word representation is poised to play a crucial role in the detection of hate speech.\n\nThis study explores the feasibility of utilizing domain-specific word embeddings in a Bidirectional LSTM-based deep learning model for the automated detection and classification of hate speech. Additionally, the study examines the application of transfer learning using the BERT language model for hate speech detection as a binary classification task. Experimental results indicate that the combination of domain-specific word embeddings with the Bidirectional LSTM-based deep learning model achieved a 93% f1-score, while BERT achieved a maximum f1-score of 96% on a balanced dataset compiled from various existing hate speech datasets."}, "2409.17432": {"original_text": "  Reducing the atmospheric haze and enhancing image clarity is crucial for\ncomputer vision applications. The lack of real-life hazy ground truth images\nnecessitates synthetic datasets, which often lack diverse haze types, impeding\neffective haze type classification and dehazing algorithm selection. This\nresearch introduces the HazeSpace2M dataset, a collection of over 2 million\nimages designed to enhance dehazing through haze type classification.\nHazeSpace2M includes diverse scenes with 10 haze intensity levels, featuring\nFog, Cloud, and Environmental Haze (EH). Using the dataset, we introduce a\ntechnique of haze type classification followed by specialized dehazers to clear\nhazy images. Unlike conventional methods, our approach classifies haze types\nbefore applying type-specific dehazing, improving clarity in real-life hazy\nimages. Benchmarking with state-of-the-art (SOTA) models, ResNet50 and AlexNet\nachieve 92.75\\% and 92.50\\% accuracy, respectively, against existing synthetic\ndatasets. However, these models achieve only 80% and 70% accuracy,\nrespectively, against our Real Hazy Testset (RHT), highlighting the challenging\nnature of our HazeSpace2M dataset. Additional experiments show that haze type\nclassification followed by specialized dehazing improves results by 2.41% in\nPSNR, 17.14% in SSIM, and 10.2\\% in MSE over general dehazers. Moreover, when\ntesting with SOTA dehazing models, we found that applying our proposed\nframework significantly improves their performance. These results underscore\nthe significance of HazeSpace2M and our proposed framework in addressing\natmospheric haze in multimedia processing. Complete code and dataset is\navailable on \\href{https://github.com/tanvirnwu/HazeSpace2M}\n{\\textcolor{blue}{\\textbf{GitHub}}}.\n", "rewritten_text": "Enhancing image clarity and reducing atmospheric haze are essential for computer vision applications. The absence of real-life hazy ground truth images necessitates the use of synthetic datasets. However, these datasets often lack diverse types of haze, which hinders effective haze type classification and selection of dehazing algorithms. This study introduces the HazeSpace2M dataset, comprising over 2 million images aimed at improving dehazing through haze type classification.\n\nHazeSpace2M features a variety of scenes with 10 levels of haze intensity, including Fog, Cloud, and Environmental Haze (EH). By utilizing this dataset, we propose a method for haze type classification followed by the application of specialized dehazing techniques to enhance clarity in hazy images. Unlike traditional approaches, our method first identifies the type of haze present before employing type-specific dehazing, resulting in improved clarity in real-life hazy images.\n\nBenchmarking against state-of-the-art models such as ResNet50 and AlexNet shows that they achieve 92.75% and 92.50% accuracy, respectively, when tested against existing synthetic datasets. However, when tested against our Real Hazy Testset (RHT), these models only achieve 80% and 70% accuracy, highlighting the challenging nature of the HazeSpace2M dataset. Further experiments demonstrate that combining haze type classification with specialized dehazing leads to improvements of 2.41% in PSNR, 17.14% in SSIM, and 10.2% in MSE compared to general dehazing methods.\n\nAdditionally, when tested with state-of-the-art dehazing models, our proposed framework significantly enhances their performance. These findings emphasize the importance of the HazeSpace2M dataset and our framework in addressing atmospheric haze in multimedia processing. The complete code and dataset are accessible on GitHub at [https://github.com/tanvirnwu/HazeSpace2M](https://github.com/tanvirnwu/HazeSpace2M)."}, "1808.045": {"original_text": "  Medical images with specific pathologies are scarce, but a large amount of\ndata is usually required for a deep convolutional neural network (DCNN) to\nachieve good accuracy. We consider the problem of segmenting the left\nventricular (LV) myocardium on late gadolinium enhancement (LGE) cardiovascular\nmagnetic resonance (CMR) scans of which only some of the scans have scar\ntissue. We propose ScarGAN to simulate scar tissue on healthy myocardium using\nchained generative adversarial networks (GAN). Our novel approach factorizes\nthe simulation process into 3 steps: 1) a mask generator to simulate the shape\nof the scar tissue; 2) a domain-specific heuristic to produce the initial\nsimulated scar tissue from the simulated shape; 3) a refining generator to add\ndetails to the simulated scar tissue. Unlike other approaches that generate\nsamples from scratch, we simulate scar tissue on normal scans resulting in\nhighly realistic samples. We show that experienced radiologists are unable to\ndistinguish between real and simulated scar tissue. Training a U-Net with\nadditional scans with scar tissue simulated by ScarGAN increases the percentage\nof scar pixels correctly included in LV myocardium prediction from 75.9% to\n80.5%.\n", "rewritten_text": "Medical images depicting specific pathologies are often limited in availability, necessitating a substantial volume of data for a deep convolutional neural network (DCNN) to achieve optimal accuracy. In this study, we address the challenge of segmenting the left ventricular (LV) myocardium in late gadolinium enhancement (LGE) cardiovascular magnetic resonance (CMR) scans, where only a subset of scans exhibit scar tissue. To tackle this issue, we introduce ScarGAN, a novel approach that leverages chained generative adversarial networks (GAN) to simulate scar tissue on healthy myocardium.\n\nOur innovative methodology breaks down the simulation process into three distinct steps: firstly, a mask generator is employed to replicate the shape of the scar tissue; secondly, a domain-specific heuristic is utilized to generate the initial simulated scar tissue based on the shape; and finally, a refining generator is applied to enhance the simulated scar tissue with intricate details. Unlike conventional methods that generate samples from scratch, our approach involves simulating scar tissue on normal scans, resulting in remarkably realistic samples. Notably, our experiments demonstrate that even experienced radiologists struggle to differentiate between real and simulated scar tissue.\n\nBy training a U-Net model with additional scans featuring scar tissue simulated by ScarGAN, we observe a notable improvement in the accuracy of LV myocardium prediction. Specifically, the percentage of scar pixels correctly included in the prediction increases from 75.9% to 80.5%."}, "2409.00045": {"original_text": "  Colonoscopy is the primary method for examination, detection, and removal of\npolyps. Regular screening helps detect and prevent colorectal cancer at an\nearly curable stage. However, challenges such as variation among the\nendoscopists' skills, bowel quality preparation, and complex nature of the\nlarge intestine which cause large number of polyp miss-rate. These missed\npolyps can develop into cancer later on, which underscores the importance of\nimproving the detection methods. A computer-aided diagnosis system can support\nphysicians by assisting in detecting overlooked polyps. However, one of the\nimportant challenges for developing novel deep learning models for automatic\npolyp detection and segmentation is the lack of publicly available,\nmulti-center large and diverse datasets. To address this gap, we introduce\nPolypDB, a large scale publicly available dataset that contains 3934 still\npolyp images and their corresponding ground truth from real colonoscopy videos\nto design efficient polyp detection and segmentation architectures. The dataset\nhas been developed and verified by a team of 10 gastroenterologists. PolypDB\ncomprises of images from five modalities: Blue Light Imaging (BLI), Flexible\nImaging Color Enhancement (FICE), Linked Color Imaging (LCI), Narrow Band\nImaging (NBI), and White Light Imaging (WLI) and three medical centers from\nNorway, Sweden and Vietnam. Thus, we split the dataset based on modality and\nmedical center for modality-wise and center-wise analysis. We provide a\nbenchmark on each modality using eight popular segmentation methods and six\nstandard benchmark polyp detection methods. Furthermore, we also provide\nbenchmark on center-wise under federated learning settings. Our dataset is\npublic and can be downloaded at \\url{https://osf.io/pr7ms/}.\n", "rewritten_text": "Colonoscopy is the primary method for examining, detecting, and removing polyps. Regular screening is crucial for detecting and preventing colorectal cancer at an early, curable stage. However, challenges such as variations in endoscopists' skills, bowel preparation quality, and the complex nature of the large intestine contribute to a significant number of missed polyps. These undetected polyps can potentially develop into cancer, highlighting the critical need for improving detection methods.\n\nA computer-aided diagnosis system can assist physicians in identifying overlooked polyps. Nevertheless, a key challenge in developing innovative deep learning models for automatic polyp detection and segmentation is the lack of publicly available, multi-center large and diverse datasets. To bridge this gap, we introduce PolypDB, a comprehensive dataset comprising 3934 still polyp images and their corresponding ground truth extracted from real colonoscopy videos. This dataset was meticulously curated and validated by a team of 10 gastroenterologists.\n\nPolypDB includes images from five modalities: Blue Light Imaging (BLI), Flexible Imaging Color Enhancement (FICE), Linked Color Imaging (LCI), Narrow Band Imaging (NBI), and White Light Imaging (WLI) obtained from three medical centers in Norway, Sweden, and Vietnam. To facilitate in-depth analysis, we have categorized the dataset based on modality and medical center for modality-specific and center-specific evaluations. We present benchmarks for each modality using eight popular segmentation methods and six standard benchmark polyp detection techniques. Additionally, we offer a benchmark for center-specific evaluations under federated learning settings.\n\nOur dataset is publicly available for download at \\url{https://osf.io/pr7ms/}."}, "2201.09724": {"original_text": "  The task of hot-refresh model upgrades of image retrieval systems plays an\nessential role in the industry but has never been investigated in academia\nbefore. Conventional cold-refresh model upgrades can only deploy new models\nafter the gallery is overall backfilled, taking weeks or even months for\nmassive data. In contrast, hot-refresh model upgrades deploy the new model\nimmediately and then gradually improve the retrieval accuracy by backfilling\nthe gallery on-the-fly. Compatible training has made it possible, however, the\nproblem of model regression with negative flips poses a great challenge to the\nstable improvement of user experience. We argue that it is mainly due to the\nfact that new-to-old positive query-gallery pairs may show less similarity than\nnew-to-new negative pairs. To solve the problem, we introduce a\nRegression-Alleviating Compatible Training (RACT) method to properly constrain\nthe feature compatibility while reducing negative flips. The core is to\nencourage the new-to-old positive pairs to be more similar than both the\nnew-to-old negative pairs and the new-to-new negative pairs. An efficient\nuncertainty-based backfilling strategy is further introduced to fasten accuracy\nimprovements. Extensive experiments on large-scale retrieval benchmarks (e.g.,\nGoogle Landmark) demonstrate that our RACT effectively alleviates the model\nregression for one more step towards seamless model upgrades. The code will be\navailable at https://github.com/binjiezhang/RACT_ICLR2022.\n", "rewritten_text": "The task of upgrading image retrieval systems using a hot-refresh model plays a crucial role in the industry, yet it remains unexplored in academia. Traditional cold-refresh model upgrades require the entire gallery to be backfilled before deploying new models, a process that can take weeks or even months for large datasets. In contrast, hot-refresh model upgrades enable immediate deployment of new models, gradually enhancing retrieval accuracy by backfilling the gallery on-the-fly. While compatible training has facilitated this approach, the challenge of model regression with negative flips presents a significant obstacle to ensuring a stable user experience. This issue is primarily attributed to the reduced similarity between new-to-old positive query-gallery pairs compared to new-to-new negative pairs.\n\nTo address this challenge, we propose a Regression-Alleviating Compatible Training (RACT) method that effectively constrains feature compatibility while minimizing negative flips. The key idea is to encourage new-to-old positive pairs to exhibit greater similarity than both new-to-old negative pairs and new-to-new negative pairs. Additionally, we introduce an efficient uncertainty-based backfilling strategy to expedite accuracy improvements. Extensive experiments conducted on large-scale retrieval benchmarks, such as Google Landmark, demonstrate the effectiveness of our RACT method in mitigating model regression and advancing seamless model upgrades. The code for our approach will be accessible at https://github.com/binjiezhang/RACT_ICLR2022."}, "2012.14345": {"original_text": "  Deep Learning (DL) based methods for object detection achieve remarkable\nperformance at the cost of computationally expensive training and extensive\ndata labeling. Robots embodiment can be exploited to mitigate this burden by\nacquiring automatically annotated training data via a natural interaction with\na human showing the object of interest, handheld. However, learning solely from\nthis data may introduce biases (the so-called domain shift), and prevents\nadaptation to novel tasks. While Weakly-supervised Learning (WSL) offers a\nwell-established set of techniques to cope with these problems in\ngeneral-purpose Computer Vision, its adoption in challenging robotic domains is\nstill at a preliminary stage. In this work, we target the scenario of a robot\ntrained in a teacher-learner setting to detect handheld objects. The aim is to\nimprove detection performance in different settings by letting the robot\nexplore the environment with a limited human labeling budget. We compare\nseveral techniques for WSL in detection pipelines to reduce model re-training\ncosts without compromising accuracy, proposing solutions which target the\nconsidered robotic scenario. We show that the robot can improve adaptation to\nnovel domains, either by interacting with a human teacher (Active Learning) or\nwith an autonomous supervision (Semi-supervised Learning). We integrate our\nstrategies into an on-line detection method, achieving efficient model update\ncapabilities with few labels. We experimentally benchmark our method on\nchallenging robotic object detection tasks under domain shift.\n", "rewritten_text": "Deep learning (DL) based methods for object detection have shown remarkable performance, but they come with the drawback of requiring computationally expensive training and extensive data labeling. One potential solution to alleviate this burden is to leverage robots' embodiment to automatically annotate training data through natural interactions with humans, who can show the objects of interest handheld. However, learning solely from this data may introduce biases, known as domain shift, and hinder adaptation to new tasks.\n\nWhile Weakly-supervised Learning (WSL) provides established techniques to address these challenges in general-purpose Computer Vision, its application in complex robotic domains is still in its early stages. This study focuses on a scenario where a robot is trained in a teacher-learner setting to detect handheld objects. The goal is to enhance detection performance across various settings by allowing the robot to explore its environment with a limited human labeling budget.\n\nWe evaluate different WSL techniques in detection pipelines to reduce model re-training costs without sacrificing accuracy, tailoring our solutions to the specific robotic context. Our results demonstrate that the robot can enhance its ability to adapt to new domains, whether through interaction with a human teacher (Active Learning) or autonomous supervision (Semi-supervised Learning). These strategies are integrated into an online detection method, enabling efficient model updates with minimal labels. We conduct experimental evaluations of our approach on challenging robotic object detection tasks affected by domain shift."}, "cs/0310014": {"original_text": "  Spoken Language can be used to provide insights into organisational\nprocesses, unfortunately transcription and coding stages are very time\nconsuming and expensive. The concept of partial transcription and coding is\nproposed in which spoken language is indexed prior to any subsequent\nprocessing. The functional linguistic theory of texture is used to describe the\neffects of partial transcription on observational records. The standard used to\nencode transcript context and metadata is called CHAT, but a previous XML\nschema developed to implement it contains design assumptions that make it\ndifficult to support partial transcription for example. This paper describes a\nmore effective XML schema that overcomes many of these problems and is intended\nfor use in applications that support the rapid development of spoken language\ndeliverables.\n", "rewritten_text": "The use of spoken language can offer valuable insights into organizational processes. However, the transcription and coding stages involved in analyzing spoken language are often time-consuming and costly. To address this issue, the concept of partial transcription and coding is introduced, where spoken language is indexed before further processing takes place. This approach leverages the functional linguistic theory of texture to explain the impact of partial transcription on observational records.\n\nThe standard method for encoding transcript context and metadata is known as CHAT. However, a previous XML schema developed to implement CHAT includes design assumptions that hinder the support of partial transcription. This paper presents a more efficient XML schema that addresses many of these challenges. The new schema is designed for applications that facilitate the swift development of spoken language deliverables."}, "2007.12099": {"original_text": "  Object detection is one of the most important areas in computer vision, which\nplays a key role in various practical scenarios. Due to limitation of hardware,\nit is often necessary to sacrifice accuracy to ensure the infer speed of the\ndetector in practice. Therefore, the balance between effectiveness and\nefficiency of object detector must be considered. The goal of this paper is to\nimplement an object detector with relatively balanced effectiveness and\nefficiency that can be directly applied in actual application scenarios, rather\nthan propose a novel detection model. Considering that YOLOv3 has been widely\nused in practice, we develop a new object detector based on YOLOv3. We mainly\ntry to combine various existing tricks that almost not increase the number of\nmodel parameters and FLOPs, to achieve the goal of improving the accuracy of\ndetector as much as possible while ensuring that the speed is almost unchanged.\nSince all experiments in this paper are conducted based on PaddlePaddle, we\ncall it PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a better\nbalance between effectiveness (45.2% mAP) and efficiency (72.9 FPS), surpassing\nthe existing state-of-the-art detectors such as EfficientDet and YOLOv4.Source\ncode is at https://github.com/PaddlePaddle/PaddleDetection.\n", "rewritten_text": "Object detection is a crucial aspect of computer vision, playing a pivotal role in various practical scenarios. However, due to hardware limitations, there is often a need to compromise on accuracy to ensure the speed of the detector in real-world applications. Therefore, it is essential to strike a balance between the effectiveness and efficiency of an object detector. The objective of this study is to develop an object detector that achieves a relatively balanced level of effectiveness and efficiency, suitable for direct application in practical scenarios, rather than introducing a novel detection model.\n\nGiven the widespread use of YOLOv3 in practice, a new object detector is devised based on YOLOv3 in this research. The primary focus is on integrating various existing techniques that minimally increase the model parameters and Floating Point Operations (FLOPs) to enhance the detector's accuracy while maintaining speed almost constant. Referred to as PP-YOLO, this detector is implemented and evaluated using PaddlePaddle for all experiments.\n\nBy leveraging a combination of multiple techniques, PP-YOLO achieves a superior balance between effectiveness (45.2% mAP) and efficiency (72.9 FPS), outperforming state-of-the-art detectors such as EfficientDet and YOLOv4. The source code for PP-YOLO can be accessed at https://github.com/PaddlePaddle/PaddleDetection."}, "2406.15823": {"original_text": "  Understanding the abilities of LLMs to reason about natural language plans,\nsuch as instructional text and recipes, is critical to reliably using them in\ndecision-making systems. A fundamental aspect of plans is the temporal order in\nwhich their steps needs to be executed, which reflects the underlying causal\ndependencies between them. We introduce CaT-Bench, a benchmark of Step Order\nPrediction questions, which test whether a step must necessarily occur before\nor after another in cooking recipe plans. We use this to evaluate how well\nfrontier LLMs understand causal and temporal dependencies. We find that SOTA\nLLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased\ntowards predicting dependence more often, perhaps relying on temporal order of\nsteps as a heuristic. While prompting for explanations and using few-shot\nexamples improve performance, the best F1 result is only 0.73. Further, human\nevaluation of explanations along with answer correctness show that, on average,\nhumans do not agree with model reasoning. Surprisingly, we also find that\nexplaining after answering leads to better performance than normal\nchain-of-thought prompting, and LLM answers are not consistent across questions\nabout the same step pairs. Overall, results show that LLMs' ability to detect\ndependence between steps has significant room for improvement.\n", "rewritten_text": "Understanding the abilities of large language models (LLMs) to reason about natural language plans, such as instructional text and recipes, is crucial for their reliable use in decision-making systems. A key aspect of plans is the temporal order in which their steps need to be executed, reflecting the underlying causal dependencies between them. To address this, we introduce CaT-Bench, a benchmark of Step Order Prediction questions that assess whether a step must occur before or after another in cooking recipe plans. This benchmark is used to evaluate how well state-of-the-art (SOTA) LLMs comprehend causal and temporal dependencies.\n\nOur findings reveal that current SOTA LLMs fall short in this task, with the best zero-shot performance reaching only 0.59 in F1 score. These models tend to predict dependence more frequently, potentially relying on the temporal order of steps as a heuristic. While soliciting explanations and providing few-shot examples can enhance performance, the highest F1 score achieved is only 0.73. Additionally, human evaluation of explanations, in conjunction with answer correctness, indicates that, on average, humans do not align with the reasoning of the models.\n\nInterestingly, our study shows that explaining after answering yields better performance compared to the conventional chain-of-thought prompting method. Furthermore, LLM responses are inconsistent across questions concerning the same step pairs. Overall, the results underscore the significant room for improvement in LLMs' ability to detect dependencies between steps."}, "1907.06007": {"original_text": "  With the development of deep neural networks, the demand for a significant\namount of annotated training data becomes the performance bottlenecks in many\nfields of research and applications. Image synthesis can generate annotated\nimages automatically and freely, which gains increasing attention recently. In\nthis paper, we propose to synthesize scene text images from the 3D virtual\nworlds, where the precise descriptions of scenes, editable\nillumination/visibility, and realistic physics are provided. Different from the\nprevious methods which paste the rendered text on static 2D images, our method\ncan render the 3D virtual scene and text instances as an entirety. In this way,\nreal-world variations, including complex perspective transformations, various\nilluminations, and occlusions, can be realized in our synthesized scene text\nimages. Moreover, the same text instances with various viewpoints can be\nproduced by randomly moving and rotating the virtual camera, which acts as\nhuman eyes. The experiments on the standard scene text detection benchmarks\nusing the generated synthetic data demonstrate the effectiveness and\nsuperiority of the proposed method. The code and synthetic data is available\nat: https://github.com/MhLiao/SynthText3D\n", "rewritten_text": "The advancement of deep neural networks has led to a growing demand for a substantial amount of annotated training data, which has become a performance bottleneck in various research fields and applications. Recently, there has been increasing attention on image synthesis as a means to automatically and freely generate annotated images. This paper introduces a method for synthesizing scene text images from 3D virtual worlds, where detailed scene descriptions, editable illumination/visibility, and realistic physics are provided. Unlike previous approaches that involve pasting rendered text onto static 2D images, our method renders the 3D virtual scene and text instances as a whole. This approach allows for the realization of real-world variations, such as complex perspective transformations, diverse illuminations, and occlusions, in the synthesized scene text images. Furthermore, by randomly moving and rotating the virtual camera, which simulates human eyes, our method can produce the same text instances from various viewpoints. Experiments conducted on standard scene text detection benchmarks using the generated synthetic data demonstrate the effectiveness and superiority of the proposed method. The code and synthetic data can be accessed at: https://github.com/MhLiao/SynthText3D"}, "2006.13017": {"original_text": "  Recently, 3D convolutional networks (3D ConvNets) yield good performance in\naction recognition. However, optical flow stream is still needed to ensure\nbetter performance, the cost of which is very high. In this paper, we propose a\nfast but effective way to extract motion features from videos utilizing\nresidual frames as the input data in 3D ConvNets. By replacing traditional\nstacked RGB frames with residual ones, 35.6% and 26.6% points improvements over\ntop-1 accuracy can be obtained on the UCF101 and HMDB51 datasets when ResNet-18\nmodels are trained from scratch. And we achieved the state-of-the-art results\nin this training mode. Analysis shows that better motion features can be\nextracted using residual frames compared to RGB counterpart. By combining with\na simple appearance path, our proposal can be even better than some methods\nusing optical flow streams.\n", "rewritten_text": "Recently, 3D convolutional networks (3D ConvNets) have shown promising results in action recognition. However, incorporating an optical flow stream remains crucial for achieving optimal performance, despite its high cost. This paper introduces a novel approach to efficiently extract motion features from videos by utilizing residual frames as input data in 3D ConvNets. By substituting traditional stacked RGB frames with residual frames, significant improvements of 35.6% and 26.6% in top-1 accuracy were achieved on the UCF101 and HMDB51 datasets when training ResNet-18 models from scratch. Our method has set a new benchmark in this training paradigm, demonstrating that residual frames enable the extraction of superior motion features compared to RGB frames. When combined with a simple appearance path, our approach outperforms some existing methods that rely on optical flow streams."}, "2403.15615": {"original_text": "  Conversation is the subject of increasing interest in the social, cognitive,\nand computational sciences. And yet, as conversational datasets continue to\nincrease in size and complexity, researchers lack scalable methods to segment\nspeech-to-text transcripts into conversational turns--the basic building blocks\nof social interaction. We introduce \"NaturalTurn,\" a turn segmentation\nalgorithm designed to accurately capture the dynamics of naturalistic exchange.\nNaturalTurn operates by distinguishing speakers' primary conversational turns\nfrom listeners' secondary utterances, such as backchannels, brief\ninterjections, and other forms of parallel speech that characterize\nconversation. Using data from a large conversation corpus, we show how\nNaturalTurn-derived transcripts demonstrate favorable statistical and\ninferential characteristics compared to transcripts derived from existing\nmethods. The NaturalTurn algorithm represents an improvement in\nmachine-generated transcript processing methods, or \"turn models\" that will\nenable researchers to associate turn-taking dynamics with the broader outcomes\nthat result from social interaction, a central goal of conversation science.\n", "rewritten_text": "Conversation is a topic of growing interest in the fields of social, cognitive, and computational sciences. Despite the increasing size and complexity of conversational datasets, researchers currently lack scalable methods for segmenting speech-to-text transcripts into conversational turns, which are the fundamental units of social interaction. To address this gap, we present \"NaturalTurn,\" an algorithm specifically designed to accurately capture the dynamics of naturalistic exchanges. NaturalTurn distinguishes between primary conversational turns of speakers and secondary utterances of listeners, such as backchannels, brief interjections, and other forms of parallel speech that are characteristic of conversations. Through the analysis of a large conversation corpus, we demonstrate that transcripts generated by NaturalTurn exhibit more favorable statistical and inferential properties compared to those produced by existing methods. The NaturalTurn algorithm represents a significant advancement in machine-generated transcript processing, or \"turn models,\" that will empower researchers to link turn-taking dynamics with broader outcomes resulting from social interactions\u2014a key objective in the field of conversation science."}, "1807.00502": {"original_text": "  The use of deep learning for medical imaging has seen tremendous growth in\nthe research community. One reason for the slow uptake of these systems in the\nclinical setting is that they are complex, opaque and tend to fail silently.\nOutside of the medical imaging domain, the machine learning community has\nrecently proposed several techniques for quantifying model uncertainty (i.e.~a\nmodel knowing when it has failed). This is important in practical settings, as\nwe can refer such cases to manual inspection or correction by humans. In this\npaper, we aim to bring these recent results on estimating uncertainty to bear\non two important outputs in deep learning-based segmentation. The first is\nproducing spatial uncertainty maps, from which a clinician can observe where\nand why a system thinks it is failing. The second is quantifying an image-level\nprediction of failure, which is useful for isolating specific cases and\nremoving them from automated pipelines. We also show that reasoning about\nspatial uncertainty, the first output, is a useful intermediate representation\nfor generating segmentation quality predictions, the second output. We propose\na two-stage architecture for producing these measures of uncertainty, which can\naccommodate any deep learning-based medical segmentation pipeline.\n", "rewritten_text": "The research community has witnessed significant growth in the use of deep learning for medical imaging. However, the adoption of these systems in clinical settings has been slow due to their complexity, opacity, and tendency to fail silently. In contrast, the machine learning community outside of medical imaging has recently introduced various techniques for quantifying model uncertainty, which is crucial for identifying when a model has failed. This capability is essential in practical scenarios, as it allows for cases to be referred for manual inspection or correction by humans.\n\nThis paper aims to leverage these recent advancements in estimating uncertainty to enhance two key outputs in deep learning-based segmentation. The first output involves generating spatial uncertainty maps, enabling clinicians to pinpoint where and why a system may be failing. The second output focuses on quantifying image-level predictions of failure, facilitating the identification and removal of specific problematic cases from automated pipelines. Furthermore, we demonstrate that reasoning about spatial uncertainty, the first output, serves as a valuable intermediate representation for predicting segmentation quality, the second output.\n\nTo achieve these objectives, we propose a two-stage architecture for producing measures of uncertainty that can be integrated into any deep learning-based medical segmentation pipeline."}, "2112.15439": {"original_text": "  This paper aims to conduct a comprehensive study on facial-sketch synthesis\n(FSS). However, due to the high costs of obtaining hand-drawn sketch datasets,\nthere lacks a complete benchmark for assessing the development of FSS\nalgorithms over the last decade. We first introduce a high-quality dataset for\nFSS, named FS2K, which consists of 2,104 image-sketch pairs spanning three\ntypes of sketch styles, image backgrounds, lighting conditions, skin colors,\nand facial attributes. FS2K differs from previous FSS datasets in difficulty,\ndiversity, and scalability and should thus facilitate the progress of FSS\nresearch. Second, we present the largest-scale FSS investigation by reviewing\n89 classical methods, including 25 handcrafted feature-based facial-sketch\nsynthesis approaches, 29 general translation methods, and 35 image-to-sketch\napproaches. Besides, we elaborate comprehensive experiments on the existing 19\ncutting-edge models. Third, we present a simple baseline for FSS, named FSGAN.\nWith only two straightforward components, i.e., facial-aware masking and\nstyle-vector expansion, FSGAN surpasses the performance of all previous\nstate-of-the-art models on the proposed FS2K dataset by a large margin.\nFinally, we conclude with lessons learned over the past years and point out\nseveral unsolved challenges. Our code is available at\nhttps://github.com/DengPingFan/FSGAN.\n", "rewritten_text": "This paper aims to conduct a comprehensive study on facial-sketch synthesis (FSS). However, due to the high costs associated with obtaining hand-drawn sketch datasets, there is a lack of a complete benchmark for assessing the development of FSS algorithms over the last decade. To address this gap, we introduce a high-quality dataset for FSS called FS2K. FS2K comprises 2,104 image-sketch pairs that cover three types of sketch styles, various image backgrounds, lighting conditions, skin colors, and facial attributes. Unlike previous FSS datasets, FS2K stands out in terms of difficulty, diversity, and scalability, thereby facilitating advancements in FSS research.\n\nFurthermore, we present the largest-scale FSS investigation to date by reviewing 89 classical methods, which include 25 handcrafted feature-based facial-sketch synthesis approaches, 29 general translation methods, and 35 image-to-sketch approaches. Additionally, we conduct comprehensive experiments on 19 cutting-edge models currently in existence.\n\nIn this study, we also introduce a simple baseline for FSS, named FSGAN. FSGAN achieves superior performance on the FS2K dataset compared to all previous state-of-the-art models by employing only two straightforward components: facial-aware masking and style-vector expansion.\n\nFinally, we conclude by sharing insights gained over the years and highlighting several unresolved challenges in the field. Our code is accessible at https://github.com/DengPingFan/FSGAN."}, "1804.09803": {"original_text": "  The inference structures and computational complexity of existing deep neural\nnetworks, once trained, are fixed and remain the same for all test images.\nHowever, in practice, it is highly desirable to establish a progressive\nstructure for deep neural networks which is able to adapt its inference process\nand complexity for images with different visual recognition complexity. In this\nwork, we develop a multi-stage progressive structure with integrated confidence\nanalysis and decision policy learning for deep neural networks. This new\nframework consists of a set of network units to be activated in a sequential\nmanner with progressively increased complexity and visual recognition power.\nOur extensive experimental results on the CIFAR-10 and ImageNet datasets\ndemonstrate that the proposed progressive deep neural network is able to obtain\nmore than 10 fold complexity scalability while achieving the state-of-the-art\nperformance using a single network model satisfying different\ncomplexity-accuracy requirements.\n", "rewritten_text": "The inference structures and computational complexity of current deep neural networks are fixed once trained and remain consistent across all test images. However, in practical applications, it is highly desirable to establish a progressive structure for deep neural networks that can adapt its inference process and complexity based on the visual recognition complexity of different images. In this study, we have developed a multi-stage progressive structure that incorporates confidence analysis and decision policy learning into deep neural networks. This innovative framework comprises a series of network units that are activated sequentially, with increasing complexity and visual recognition capabilities. Our extensive experiments on the CIFAR-10 and ImageNet datasets show that the proposed progressive deep neural network can achieve over a 10-fold increase in complexity scalability while maintaining state-of-the-art performance using a single network model that meets various complexity-accuracy requirements."}, "2312.14988": {"original_text": "  Autoregressive and diffusion models drive the recent breakthroughs on\ntext-to-image generation. Despite their huge success of generating\nhigh-realistic images, a common shortcoming of these models is their high\ninference latency - autoregressive models run more than a thousand times\nsuccessively to produce image tokens and diffusion models convert Gaussian\nnoise into images with many hundreds of denoising steps. In this work, we\nexplore non-autoregressive text-to-image models that efficiently generate\nhundreds of image tokens in parallel. We develop many model variations with\ndifferent learning and inference strategies, initialized text encoders, etc.\nCompared with autoregressive baselines that needs to run one thousand times,\nour model only runs 16 times to generate images of competitive quality with an\norder of magnitude lower inference latency. Our non-autoregressive model with\n346M parameters generates an image of 256$\\times$256 with about one second on\none V100 GPU.\n", "rewritten_text": "Recent breakthroughs in text-to-image generation have been driven by autoregressive and diffusion models. While these models have achieved great success in generating highly realistic images, a common drawback is their high inference latency. Autoregressive models require running more than a thousand successive steps to produce image tokens, while diffusion models involve converting Gaussian noise into images through hundreds of denoising steps.\n\nIn this study, we investigate non-autoregressive text-to-image models that can efficiently generate hundreds of image tokens simultaneously. We have developed various model variations with different learning and inference strategies, as well as initialized text encoders. Compared to autoregressive baselines that require a thousand iterations, our model only needs to run 16 times to generate images of comparable quality, significantly reducing the inference latency.\n\nOur non-autoregressive model, with 346 million parameters, can generate a 256x256 image in approximately one second on a single V100 GPU."}, "2106.16038": {"original_text": "  Recent pretraining models in Chinese neglect two important aspects specific\nto the Chinese language: glyph and pinyin, which carry significant syntax and\nsemantic information for language understanding. In this work, we propose\nChineseBERT, which incorporates both the {\\it glyph} and {\\it pinyin}\ninformation of Chinese characters into language model pretraining. The glyph\nembedding is obtained based on different fonts of a Chinese character, being\nable to capture character semantics from the visual features, and the pinyin\nembedding characterizes the pronunciation of Chinese characters, which handles\nthe highly prevalent heteronym phenomenon in Chinese (the same character has\ndifferent pronunciations with different meanings). Pretrained on large-scale\nunlabeled Chinese corpus, the proposed ChineseBERT model yields significant\nperformance boost over baseline models with fewer training steps. The porpsoed\nmodel achieves new SOTA performances on a wide range of Chinese NLP tasks,\nincluding machine reading comprehension, natural language inference, text\nclassification, sentence pair matching, and competitive performances in named\nentity recognition. Code and pretrained models are publicly available at\nhttps://github.com/ShannonAI/ChineseBert.\n", "rewritten_text": "In recent pretraining models for the Chinese language, two crucial aspects have been overlooked: glyph and pinyin. These elements carry substantial syntax and semantic information essential for language comprehension. This study introduces ChineseBERT, a model that integrates both glyph and pinyin information of Chinese characters into the pretraining of language models. The glyph embedding is derived from various fonts of a Chinese character, enabling the capture of character semantics through visual features. On the other hand, the pinyin embedding represents the pronunciation of Chinese characters, addressing the prevalent heteronym phenomenon in Chinese where the same character can have different pronunciations and meanings. Through pretraining on a large-scale unlabeled Chinese corpus, ChineseBERT demonstrates a significant performance improvement compared to baseline models with fewer training iterations. The proposed model achieves state-of-the-art performance across various Chinese NLP tasks, such as machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive results in named entity recognition. The code and pretrained models for ChineseBERT are publicly accessible at https://github.com/ShannonAI/ChineseBert."}, "2103.05342": {"original_text": "  In this paper, we propose a novel data augmentation strategy named\nCut-Thumbnail, that aims to improve the shape bias of the network. We reduce an\nimage to a certain size and replace the random region of the original image\nwith the reduced image. The generated image not only retains most of the\noriginal image information but also has global information in the reduced\nimage. We call the reduced image as thumbnail. Furthermore, we find that the\nidea of thumbnail can be perfectly integrated with Mixed Sample Data\nAugmentation, so we put one image's thumbnail on another image while the ground\ntruth labels are also mixed, making great achievements on various computer\nvision tasks. Extensive experiments show that Cut-Thumbnail works better than\nstate-of-the-art augmentation strategies across classification, fine-grained\nimage classification, and object detection. On ImageNet classification,\nResNet-50 architecture with our method achieves 79.21\\% accuracy, which is more\nthan 2.8\\% improvement on the baseline.\n", "rewritten_text": "In this paper, we introduce a new data augmentation technique called Cut-Thumbnail, designed to enhance the shape bias of the network. The approach involves resizing an image to a specific dimension and replacing a random region of the original image with the resized image. This process results in a generated image that preserves most of the original image information while incorporating global information from the resized image, which we refer to as a thumbnail.\n\nMoreover, we have discovered that the concept of thumbnails can be effectively combined with Mixed Sample Data Augmentation. By overlaying one image's thumbnail onto another image and mixing the ground truth labels, significant progress has been made in various computer vision tasks. Extensive experiments demonstrate that Cut-Thumbnail outperforms state-of-the-art augmentation strategies in tasks such as classification, fine-grained image classification, and object detection.\n\nIn ImageNet classification, utilizing the ResNet-50 architecture with our method achieves an accuracy of 79.21%, representing a notable improvement of over 2.8% compared to the baseline performance."}, "2307.15257": {"original_text": "  The complexity of learning problems, such as Generative Adversarial Network\n(GAN) and its variants, multi-task and meta-learning, hyper-parameter learning,\nand a variety of real-world vision applications, demands a deeper understanding\nof their underlying coupling mechanisms. Existing approaches often address\nthese problems in isolation, lacking a unified perspective that can reveal\ncommonalities and enable effective solutions. Therefore, in this work, we\nproposed a new framework, named Learning with Constraint Learning (LwCL), that\ncan holistically examine challenges and provide a unified methodology to tackle\nall the above-mentioned complex learning and vision problems. Specifically,\nLwCL is designed as a general hierarchical optimization model that captures the\nessence of these diverse learning and vision problems. Furthermore, we develop\na gradient-response based fast solution strategy to overcome optimization\nchallenges of the LwCL framework. Our proposed framework efficiently addresses\na wide range of applications in learning and vision, encompassing three\ncategories and nine different problem types. Extensive experiments on synthetic\ntasks and real-world applications verify the effectiveness of our approach. The\nLwCL framework offers a comprehensive solution for tackling complex machine\nlearning and computer vision problems, bridging the gap between theory and\npractice.\n", "rewritten_text": "The complexity of learning problems, such as Generative Adversarial Networks (GAN) and its variants, multi-task and meta-learning, hyper-parameter learning, and a variety of real-world vision applications, necessitates a deeper understanding of their underlying coupling mechanisms. Existing approaches often address these problems in isolation, lacking a unified perspective that can reveal commonalities and enable effective solutions. Therefore, in this work, we introduce a new framework called Learning with Constraint Learning (LwCL), which aims to holistically examine challenges and provide a unified methodology to address all the aforementioned complex learning and vision problems. \n\nLwCL is designed as a general hierarchical optimization model that captures the essence of these diverse learning and vision problems. Additionally, we have developed a gradient-response-based fast solution strategy to overcome optimization challenges within the LwCL framework. Our proposed framework efficiently addresses a wide range of applications in learning and vision, covering three categories and nine different problem types. Extensive experiments on synthetic tasks and real-world applications validate the effectiveness of our approach. The LwCL framework offers a comprehensive solution for addressing complex machine learning and computer vision problems, bridging the gap between theory and practice."}, "2309.11119": {"original_text": "  A recent sensor fusion in a Bird's Eye View (BEV) space has shown its utility\nin various tasks such as 3D detection, map segmentation, etc. However, the\napproach struggles with inaccurate camera BEV estimation, and a perception of\ndistant areas due to the sparsity of LiDAR points. In this paper, we propose a\nbroad BEV fusion (BroadBEV) that addresses the problems with a spatial\nsynchronization approach of cross-modality. Our strategy aims to enhance camera\nBEV estimation for a broad-sighted perception while simultaneously improving\nthe completion of LiDAR's sparsity in the entire BEV space. Toward that end, we\ndevise Point-scattering that scatters LiDAR BEV distribution to camera depth\ndistribution. The method boosts the learning of depth estimation of the camera\nbranch and induces accurate location of dense camera features in BEV space. For\nan effective BEV fusion between the spatially synchronized features, we suggest\nColFusion that applies self-attention weights of LiDAR and camera BEV features\nto each other. Our extensive experiments demonstrate that BroadBEV provides a\nbroad-sighted BEV perception with remarkable performance gains.\n", "rewritten_text": "A recent sensor fusion technique in Bird's Eye View (BEV) space has demonstrated its utility in various tasks such as 3D detection and map segmentation. However, the approach faces challenges with inaccurate camera BEV estimation and limited perception of distant areas due to sparse LiDAR points. In this paper, we introduce a new method called BroadBEV fusion, which tackles these issues through a spatial synchronization approach across different modalities. Our strategy aims to improve camera BEV estimation for a wider field of view while also enhancing the coverage of LiDAR points throughout the BEV space.\n\nTo achieve this, we introduce Point-scattering, a technique that transforms the LiDAR BEV distribution into a camera depth distribution. This method enhances the camera branch's depth estimation and accurately locates dense camera features in the BEV space. For effective fusion of spatially synchronized features, we propose ColFusion, which utilizes self-attention weights to integrate LiDAR and camera BEV features. Our extensive experiments demonstrate that BroadBEV significantly enhances BEV perception, providing notable performance improvements."}, "2403.17920": {"original_text": "  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.\n", "rewritten_text": "Recent techniques in text-to-4D generation involve synthesizing dynamic 3D scenes with guidance from pre-trained text-to-video models. However, current motion representations, such as deformation models or time-dependent neural representations, have limitations in the extent of motion they can produce, as they are unable to generate motion that extends significantly beyond the bounding box used for volume rendering. This lack of a more adaptable motion model contributes to the disparity in realism between 4D generation methods and recent, nearly photorealistic video generation models.\n\nTo address this gap, we introduce TC4D: trajectory-conditioned text-to-4D generation. This approach divides motion into global and local components. The global motion of a scene's bounding box is represented through rigid transformation along a trajectory defined by a spline. Local deformations that align with the global trajectory are learned with guidance from a text-to-video model. Our method allows for the creation of scenes animated along diverse trajectories, facilitates compositional scene generation, and brings about substantial enhancements in realism and the amount of generated motion. These improvements are evaluated through qualitative analysis and a user study.\n\nFor video demonstrations of our results, please visit our website: https://sherwinbahmani.github.io/tc4d."}, "2407.11144": {"original_text": "  Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25.\n", "rewritten_text": "Even for well-studied sign languages like American Sign Language (ASL), the primary challenge for machine learning research is the lack of data. This issue is even more pronounced for the numerous other sign languages used by Deaf and Hard of Hearing communities worldwide. In this paper, we introduce YouTube-SL-25, a comprehensive multilingual corpus of sign language videos sourced from YouTube, featuring well-aligned captions. With over 3000 hours of content spanning more than 25 sign languages, YouTube-SL-25 surpasses YouTube-ASL in size, stands as the largest parallel sign language dataset to date, and represents the first or largest parallel dataset for many of its included languages. We establish baseline performance for sign-to-text tasks using a unified multilingual multitask model based on T5, and present evaluation scores across four sign languages. Our findings indicate that multilingual transfer learning benefits both high-resource and low-resource sign languages within the YouTube-SL-25 dataset."}, "1702.01776": {"original_text": "  In aspect-based sentiment analysis, most existing methods either focus on\naspect/opinion terms extraction or aspect terms categorization. However, each\ntask by itself only provides partial information to end users. To generate more\ndetailed and structured opinion analysis, we propose a finer-grained problem,\nwhich we call category-specific aspect and opinion terms extraction. This\nproblem involves the identification of aspect and opinion terms within each\nsentence, as well as the categorization of the identified terms. To this end,\nwe propose an end-to-end multi-task attention model, where each task\ncorresponds to aspect/opinion terms extraction for a specific category. Our\nmodel benefits from exploring the commonalities and relationships among\ndifferent tasks to address the data sparsity issue. We demonstrate its\nstate-of-the-art performance on three benchmark datasets.\n", "rewritten_text": "In aspect-based sentiment analysis, most current methods typically focus on extracting aspect/opinion terms or categorizing aspect terms. However, each individual task only offers partial information to end users. To provide a more detailed and structured opinion analysis, we introduce a more refined problem known as category-specific aspect and opinion terms extraction. This problem entails identifying aspect and opinion terms within each sentence and categorizing these terms. To achieve this, we propose an end-to-end multi-task attention model where each task is dedicated to extracting aspect/opinion terms for a specific category. Our model leverages the exploration of commonalities and relationships among various tasks to tackle the issue of data sparsity. We showcase its state-of-the-art performance on three benchmark datasets."}, "2301.00399": {"original_text": "  In the present paper, semantic parsing challenges are briefly introduced and\nQDMR formalism in semantic parsing is implemented using sequence to sequence\nmodel with attention but uses only part of speech(POS) as a representation of\nwords of a sentence to make the training as simple and as fast as possible and\nalso avoiding curse of dimensionality as well as overfitting. It is shown how\nsemantic operator prediction could be augmented with other models like the\nCopyNet model or the recursive neural net model.\n", "rewritten_text": "In this paper, we briefly introduce the challenges of semantic parsing and implement the QDMR formalism in semantic parsing using a sequence-to-sequence model with attention. However, we use only part-of-speech (POS) as a representation of words in a sentence to simplify and expedite the training process, while also avoiding issues such as the curse of dimensionality and overfitting. We demonstrate how semantic operator prediction could be enhanced by incorporating other models, such as the CopyNet model or the recursive neural network model."}, "2411.07075": {"original_text": "  To predict upcoming text, language models must in some cases retrieve\nin-context information verbatim. In this report, we investigated how the\nability of language models to retrieve arbitrary in-context nouns developed\nduring training (across time) and as language models trained on the same\ndataset increase in size (across scale). We then asked whether learning of\nin-context retrieval correlates with learning of more challenging zero-shot\nbenchmarks. Furthermore, inspired by semantic effects in human short-term\nmemory, we evaluated the retrieval with respect to a major semantic component\nof target nouns, namely whether they denote a concrete or abstract entity, as\nrated by humans. We show that verbatim in-context retrieval developed in a\nsudden transition early in the training process, after about 1% of the training\ntokens. This was observed across model sizes (from 14M and up to 12B\nparameters), and the transition occurred slightly later for the two smallest\nmodels. We further found that the development of verbatim in-context retrieval\nis positively correlated with the learning of zero-shot benchmarks. Around the\ntransition point, all models showed the advantage of retrieving concrete nouns\nas opposed to abstract nouns. In all but two smallest models, the advantage\ndissipated away toward the end of training.\n", "rewritten_text": "In this report, we explored how language models develop the ability to retrieve specific in-context information, such as nouns, during training over time and as the models increase in size. We also investigated whether the proficiency in in-context retrieval is linked to the performance on more challenging zero-shot benchmarks. Drawing inspiration from human short-term memory effects, we assessed the retrieval of nouns based on whether they represent concrete or abstract entities, as determined by human ratings.\n\nOur findings reveal that the capability for verbatim in-context retrieval undergoes a sudden improvement early in the training process, typically after approximately 1% of the training tokens have been processed. This improvement is consistent across various model sizes, ranging from 14M to 12B parameters, with the transition occurring slightly later for the smaller models. Additionally, we observed a positive correlation between the development of in-context retrieval and the mastery of zero-shot benchmarks.\n\nNotably, around the transition point, all models demonstrated a preference for retrieving concrete nouns over abstract nouns. However, this advantage diminished towards the end of training in all models except the two smallest ones."}, "1911.11351": {"original_text": "  Recently, Human Attribute Recognition (HAR) has become a hot topic due to its\nscientific challenges and application potentials, where localizing attributes\nis a crucial stage but not well handled. In this paper, we propose a novel deep\nlearning approach to HAR, namely Distraction-aware HAR (Da-HAR). It enhances\ndeep CNN feature learning by improving attribute localization through a\ncoarse-to-fine attention mechanism. At the coarse step, a self-mask block is\nbuilt to roughly discriminate and reduce distractions, while at the fine step,\na masked attention branch is applied to further eliminate irrelevant regions.\nThanks to this mechanism, feature learning is more accurate, especially when\nheavy occlusions and complex backgrounds exist. Extensive experiments are\nconducted on the WIDER-Attribute and RAP databases, and state-of-the-art\nresults are achieved, demonstrating the effectiveness of the proposed approach.\n", "rewritten_text": "Recently, Human Attribute Recognition (HAR) has emerged as a prominent topic due to its scientific challenges and potential applications. However, the localization of attributes, a crucial stage in HAR, has not been effectively addressed. In this paper, we introduce a novel deep learning approach to HAR called Distraction-aware HAR (Da-HAR). This approach enhances deep CNN feature learning by refining attribute localization through a coarse-to-fine attention mechanism. \n\nIn the coarse step, a self-mask block is implemented to roughly differentiate and reduce distractions. Subsequently, in the fine step, a masked attention branch is utilized to further eliminate irrelevant regions. This mechanism improves the accuracy of feature learning, particularly in scenarios with heavy occlusions and complex backgrounds. \n\nExtensive experiments were conducted on the WIDER-Attribute and RAP databases, leading to the achievement of state-of-the-art results. These outcomes underscore the effectiveness of the proposed approach."}, "2310.08487": {"original_text": "  While multi-modal models have successfully integrated information from image,\nvideo, and audio modalities, integrating graph modality into large language\nmodels (LLMs) remains unexplored. This discrepancy largely stems from the\ninherent divergence between structured graph data and unstructured text data.\nIncorporating graph knowledge provides a reliable source of information,\nenabling potential solutions to address issues in text generation, e.g.,\nhallucination, and lack of domain knowledge. To evaluate the integration of\ngraph knowledge into language models, a dedicated dataset is needed. However,\nthere is currently no benchmark dataset specifically designed for multimodal\ngraph-language models. To address this gap, we propose GraphextQA, a question\nanswering dataset with paired subgraphs, retrieved from Wikidata, to facilitate\nthe evaluation and future development of graph-language models. Additionally,\nwe introduce a baseline model called CrossGNN, which conditions answer\ngeneration on the paired graphs by cross-attending question-aware graph\nfeatures at decoding. The proposed dataset is designed to evaluate\ngraph-language models' ability to understand graphs and make use of it for\nanswer generation. We perform experiments with language-only models and the\nproposed graph-language model to validate the usefulness of the paired graphs\nand to demonstrate the difficulty of the task.\n", "rewritten_text": "While multi-modal models have successfully integrated information from image, video, and audio modalities, the integration of the graph modality into large language models (LLMs) remains unexplored. This discrepancy largely arises from the inherent divergence between structured graph data and unstructured text data. The incorporation of graph knowledge provides a reliable source of information, enabling potential solutions to address issues in text generation such as hallucination and lack of domain knowledge. To evaluate the integration of graph knowledge into language models, a dedicated dataset is necessary. However, there is currently no benchmark dataset specifically designed for multimodal graph-language models. To bridge this gap, we propose GraphextQA, a question-answering dataset with paired subgraphs retrieved from Wikidata, aimed at facilitating the evaluation and future development of graph-language models. Additionally, we introduce a baseline model called CrossGNN, which conditions answer generation on the paired graphs by cross-attending question-aware graph features during decoding. The proposed dataset is designed to assess the ability of graph-language models to comprehend graphs and utilize them for answer generation. We conduct experiments with language-only models and the proposed graph-language model to validate the utility of the paired graphs and to demonstrate the complexity of the task."}, "cs/0010012": {"original_text": "  We describe a new framework for distilling information from word lattices to\nimprove the accuracy of speech recognition and obtain a more perspicuous\nrepresentation of a set of alternative hypotheses. In the standard MAP decoding\napproach the recognizer outputs the string of words corresponding to the path\nwith the highest posterior probability given the acoustics and a language\nmodel. However, even given optimal models, the MAP decoder does not necessarily\nminimize the commonly used performance metric, word error rate (WER). We\ndescribe a method for explicitly minimizing WER by extracting word hypotheses\nwith the highest posterior probabilities from word lattices. We change the\nstandard problem formulation by replacing global search over a large set of\nsentence hypotheses with local search over a small set of word candidates. In\naddition to improving the accuracy of the recognizer, our method produces a new\nrepresentation of the set of candidate hypotheses that specifies the sequence\nof word-level confusions in a compact lattice format. We study the properties\nof confusion networks and examine their use for other tasks, such as lattice\ncompression, word spotting, confidence annotation, and reevaluation of\nrecognition hypotheses using higher-level knowledge sources.\n", "rewritten_text": "We present a novel framework for extracting information from word lattices to enhance the precision of speech recognition and achieve a clearer representation of a range of potential hypotheses. In the conventional MAP decoding method, the recognizer generates the word sequence corresponding to the path with the highest posterior probability based on the acoustics and a language model. However, even with optimal models, the MAP decoder may not necessarily minimize the widely used performance metric, word error rate (WER). Our approach involves a technique to explicitly minimize WER by identifying word hypotheses with the highest posterior probabilities from word lattices. This method alters the traditional problem formulation by substituting global search across a vast array of sentence hypotheses with local search among a limited set of word candidates. Apart from enhancing the accuracy of the recognizer, our method yields a fresh representation of the candidate hypotheses set that outlines the sequence of word-level confusions in a concise lattice format. We delve into the characteristics of confusion networks and explore their utility for various tasks, including lattice compression, word spotting, confidence annotation, and reassessment of recognition hypotheses using higher-level knowledge sources."}, "2111.03993": {"original_text": "  Skeleton data is of low dimension. However, there is a trend of using very\ndeep and complicated feedforward neural networks to model the skeleton sequence\nwithout considering the complexity in recent year. In this paper, a simple yet\neffective multi-scale semantics-guided neural network (MS-SGN) is proposed for\nskeleton-based action recognition. We explicitly introduce the high level\nsemantics of joints (joint type and frame index) into the network to enhance\nthe feature representation capability of joints. Moreover, a multi-scale\nstrategy is proposed to be robust to the temporal scale variations. In\naddition, we exploit the relationship of joints hierarchically through two\nmodules, i.e., a joint-level module for modeling the correlations of joints in\nthe same frame and a frame-level module for modeling the temporal dependencies\nof frames. With an order of magnitude smaller model size than most previous\nmethods, MSSGN achieves the state-of-the-art performance on the NTU60, NTU120,\nand SYSU datasets.\n", "rewritten_text": "Skeleton data typically has low dimensionality. Despite this, there has been a recent trend of utilizing very deep and complex feedforward neural networks to model skeleton sequences without taking into account the associated complexity. This paper introduces a simple yet effective neural network called Multi-Scale Semantics-Guided Neural Network (MS-SGN) for skeleton-based action recognition. The network explicitly incorporates high-level semantics of joints (joint type and frame index) to enhance the feature representation capability of joints. Additionally, a multi-scale strategy is proposed to ensure robustness to temporal scale variations. Furthermore, the relationship of joints is explored hierarchically through two modules: a joint-level module for modeling correlations of joints within the same frame, and a frame-level module for modeling temporal dependencies of frames. Despite having a significantly smaller model size compared to most previous methods, MSSGN achieves state-of-the-art performance on the NTU60, NTU120, and SYSU datasets."}, "1808.06428": {"original_text": "  This paper presents an approach for automatic detection of Munro's\nMicroabscess in stratum corneum (SC) of human skin biopsy in order to realize a\nmachine assisted diagnosis of Psoriasis. The challenge of detecting neutrophils\nin presence of nucleated cells is solved using the recent advances of deep\nlearning algorithms. Separation of SC layer, extraction of patches from the\nlayer followed by classification of patches with respect to presence or absence\nof neutrophils form the basis of the overall approach which is effected through\nan integration of a U-Net based segmentation network and a capsule network for\nclassification. The novel design of the present capsule net leads to a drastic\nreduction in the number of parameters without any noticeable compromise in the\noverall performance. The research further addresses the challenge of dealing\nwith Mega-pixel images (in 10X) vis-a-vis Giga-pixel ones (in 40X). The\npromising result coming out of an experiment on a dataset consisting of 273\nreal-life images shows that a practical system is possible based on the present\nresearch. The implementation of our system is available at\nhttps://github.com/Anabik/CapsDeMM.\n", "rewritten_text": "This paper introduces an approach for automatically detecting Munro's Microabscess in the stratum corneum (SC) of human skin biopsies to facilitate machine-assisted diagnosis of Psoriasis. The challenge of identifying neutrophils in the presence of nucleated cells is addressed using advanced deep learning algorithms. The methodology involves separating the SC layer, extracting patches from the layer, and classifying the patches based on the presence or absence of neutrophils. This approach combines a U-Net based segmentation network with a capsule network for classification. The unique design of the capsule network significantly reduces the number of parameters without compromising performance. The study also tackles the issue of handling Mega-pixel images (at 10X) compared to Giga-pixel images (at 40X). Results from experiments conducted on a dataset of 273 real-life images demonstrate the feasibility of a practical system based on this research. The system implementation is accessible at https://github.com/Anabik/CapsDeMM."}, "1302.1422": {"original_text": "  The variation of word meaning according to the context leads us to enrich the\ntype system of our syntactical and semantic analyser of French based on\ncategorial grammars and Montague semantics (or lambda-DRT). The main advantage\nof a deep semantic analyse is too represent meaning by logical formulae that\ncan be easily used e.g. for inferences. Determiners and quantifiers play a\nfundamental role in the construction of those formulae. But in our rich type\nsystem the usual semantic terms do not work. We propose a solution ins- pired\nby the tau and epsilon operators of Hilbert, kinds of generic elements and\nchoice functions. This approach unifies the treatment of the different determi-\nners and quantifiers as well as the dynamic binding of pronouns. Above all,\nthis fully computational view fits in well within the wide coverage parser\nGrail, both from a theoretical and a practical viewpoint.\n", "rewritten_text": "The variability of word meanings based on context prompts us to enhance the type system of our syntactic and semantic analyzer for French, which is grounded in categorial grammars and Montague semantics (or lambda-DRT). A key benefit of conducting a thorough semantic analysis is the ability to represent meaning through logical formulas that can be readily utilized for making inferences. Determiners and quantifiers play a crucial role in formulating these formulas. However, the standard semantic terms are inadequate in our comprehensive type system. To address this, we propose a solution inspired by Hilbert's tau and epsilon operators, which involve generic elements and choice functions. This approach harmonizes the treatment of various determiners and quantifiers, as well as the dynamic binding of pronouns. Importantly, this computational perspective aligns seamlessly with the extensive coverage parser Grail, both theoretically and practically."}, "2012.07315": {"original_text": "  The categorical distribution is a natural representation of uncertainty in\nmulti-class segmentations. In the two-class case the categorical distribution\nreduces to the Bernoulli distribution, for which grayscale morphology provides\na range of useful operations. In the general case, applying morphological\noperations on uncertain multi-class segmentations is not straightforward as an\nimage of categorical distributions is not a complete lattice. Although\nmorphology on color images has received wide attention, this is not so for\ncolor-coded or categorical images and even less so for images of categorical\ndistributions. In this work, we establish a set of requirements for morphology\non categorical distributions by combining classic morphology with a\nprobabilistic view. We then define operators respecting these requirements,\nintroduce protected operations on categorical distributions and illustrate the\nutility of these operators on two example tasks: modeling annotator bias in\nbrain tumor segmentations and segmenting vesicle instances from the predictions\nof a multi-class U-Net.\n", "rewritten_text": "The categorical distribution serves as a natural representation of uncertainty in multi-class segmentations. In the case of two classes, the categorical distribution simplifies to the Bernoulli distribution, which grayscale morphology offers a variety of useful operations for. However, in the general scenario, applying morphological operations to uncertain multi-class segmentations is not straightforward due to the fact that an image of categorical distributions does not form a complete lattice. While morphology on color images has garnered significant attention, the same cannot be said for color-coded or categorical images, and even less so for images of categorical distributions.\n\nIn this study, we establish a set of requirements for morphology on categorical distributions by integrating classic morphology with a probabilistic perspective. Subsequently, we define operators that adhere to these requirements, introduce protected operations on categorical distributions, and demonstrate the effectiveness of these operators through two specific tasks: modeling annotator bias in brain tumor segmentations and segmenting vesicle instances from the predictions of a multi-class U-Net."}, "2410.24219": {"original_text": "  Despite advancements in Text-to-Video (T2V) generation, producing videos with\nrealistic motion remains challenging. Current models often yield static or\nminimally dynamic outputs, failing to capture complex motions described by\ntext. This issue stems from the internal biases in text encoding, which\noverlooks motions, and inadequate conditioning mechanisms in T2V generation\nmodels. To address this, we propose a novel framework called DEcomposed MOtion\n(DEMO), which enhances motion synthesis in T2V generation by decomposing both\ntext encoding and conditioning into content and motion components. Our method\nincludes a content encoder for static elements and a motion encoder for\ntemporal dynamics, alongside separate content and motion conditioning\nmechanisms. Crucially, we introduce text-motion and video-motion supervision to\nimprove the model's understanding and generation of motion. Evaluations on\nbenchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench\ndemonstrate DEMO's superior ability to produce videos with enhanced motion\ndynamics while maintaining high visual quality. Our approach significantly\nadvances T2V generation by integrating comprehensive motion understanding\ndirectly from textual descriptions. Project page:\nhttps://PR-Ryan.github.io/DEMO-project/\n", "rewritten_text": "Despite advancements in Text-to-Video (T2V) generation, creating videos with realistic motion remains a challenge. Current models often produce static or minimally dynamic outputs, failing to accurately depict the complex motions described in text. This issue arises from internal biases in text encoding that overlook motions, as well as inadequate conditioning mechanisms in T2V generation models. To tackle this problem, we introduce a novel framework called DEcomposed MOtion (DEMO), which aims to enhance motion synthesis in T2V generation by breaking down text encoding and conditioning into content and motion components. Our method incorporates a content encoder for static elements and a motion encoder for temporal dynamics, along with separate content and motion conditioning mechanisms. Importantly, we introduce text-motion and video-motion supervision to enhance the model's comprehension and generation of motion. Evaluations conducted on benchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench demonstrate DEMO's superior capability to generate videos with improved motion dynamics while maintaining high visual quality. Our approach represents a significant advancement in T2V generation by integrating a comprehensive understanding of motion directly from textual descriptions. For more information, please visit our project page at: https://PR-Ryan.github.io/DEMO-project/"}, "2406.09386": {"original_text": "  Controllable synthetic data generation can substantially lower the annotation\ncost of training data. Prior works use diffusion models to generate driving\nimages conditioned on the 3D object layout. However, those models are trained\non small-scale datasets like nuScenes, which lack appearance and layout\ndiversity. Moreover, overfitting often happens, where the trained models can\nonly generate images based on the layout data from the validation set of the\nsame dataset. In this work, we introduce a simulator-conditioned scene\ngeneration framework called SimGen that can learn to generate diverse driving\nscenes by mixing data from the simulator and the real world. It uses a novel\ncascade diffusion pipeline to address challenging sim-to-real gaps and\nmulti-condition conflicts. A driving video dataset DIVA is collected to enhance\nthe generative diversity of SimGen, which contains over 147.5 hours of\nreal-world driving videos from 73 locations worldwide and simulated driving\ndata from the MetaDrive simulator. SimGen achieves superior generation quality\nand diversity while preserving controllability based on the text prompt and the\nlayout pulled from a simulator. We further demonstrate the improvements brought\nby SimGen for synthetic data augmentation on the BEV detection and segmentation\ntask and showcase its capability in safety-critical data generation.\n", "rewritten_text": "Controllable synthetic data generation has the potential to significantly reduce the annotation cost associated with training data. Previous studies have utilized diffusion models to create driving images based on the 3D object layout. However, these models have typically been trained on limited datasets such as nuScenes, which lack diversity in appearance and layout. This often leads to overfitting, where the models can only generate images based on the layout data from the validation set of the same dataset.\n\nIn this study, we present a simulator-conditioned scene generation framework named SimGen, which is designed to produce a wide range of driving scenes by combining data from both a simulator and the real world. SimGen employs a unique cascade diffusion pipeline to address the challenging gaps between simulation and reality, as well as conflicts arising from multiple conditions. To enhance the diversity of generative outputs, we have curated a driving video dataset called DIVA, comprising over 147.5 hours of real-world driving videos from 73 global locations, along with simulated driving data from the MetaDrive simulator.\n\nSimGen demonstrates superior quality and diversity in generating scenes while maintaining controllability based on textual prompts and layout information extracted from a simulator. We further showcase the benefits of SimGen in augmenting synthetic data for tasks such as BEV detection and segmentation, highlighting its effectiveness in generating safety-critical data."}, "2311.02313": {"original_text": "  Large-scale semantic mapping is crucial for outdoor autonomous agents to\nfulfill high-level tasks such as planning and navigation. This paper proposes a\nnovel method for large-scale 3D semantic reconstruction through implicit\nrepresentations from posed LiDAR measurements alone. We first leverage an\noctree-based and hierarchical structure to store implicit features, then these\nimplicit features are decoded to semantic information and signed distance value\nthrough shallow Multilayer Perceptrons (MLPs). We adopt off-the-shelf\nalgorithms to predict the semantic labels and instance IDs of point clouds. We\nthen jointly optimize the feature embeddings and MLPs parameters with a\nself-supervision paradigm for point cloud geometry and a pseudo-supervision\nparadigm for semantic and panoptic labels. Subsequently, categories and\ngeometric structures for novel points are regressed, and marching cubes are\nexploited to subdivide and visualize the scenes in the inferring stage. For\nscenarios with memory constraints, a map stitching strategy is also developed\nto merge sub-maps into a complete map. Experiments on two real-world datasets,\nSemanticKITTI and SemanticPOSS, demonstrate the superior segmentation\nefficiency and mapping effectiveness of our framework compared to current\nstate-of-the-art 3D LiDAR mapping methods.\n", "rewritten_text": "Large-scale semantic mapping is essential for outdoor autonomous agents to carry out high-level tasks such as planning and navigation. This paper introduces a new method for large-scale 3D semantic reconstruction using implicit representations derived solely from LiDAR measurements. Initially, we utilize an octree-based hierarchical structure to store implicit features, which are then decoded into semantic information and signed distance values through shallow Multilayer Perceptrons (MLPs). Standard algorithms are employed to predict the semantic labels and instance IDs of point clouds. Subsequently, we simultaneously optimize the feature embeddings and MLP parameters using a self-supervision approach for point cloud geometry and a pseudo-supervision approach for semantic and panoptic labels. This process involves regressing categories and geometric structures for new points, followed by the utilization of marching cubes to subdivide and visualize scenes during the inference stage. Additionally, a map stitching strategy is developed for scenarios with memory constraints to merge sub-maps into a complete map. Experimental results on two real-world datasets, SemanticKITTI and SemanticPOSS, showcase the superior segmentation efficiency and mapping effectiveness of our framework compared to current state-of-the-art 3D LiDAR mapping methods."}, "2203.09333": {"original_text": "  Perceiving the similarity between images has been a long-standing and\nfundamental problem underlying various visual generation tasks. Predominant\napproaches measure the inter-image distance by computing pointwise absolute\ndeviations, which tends to estimate the median of instance distributions and\nleads to blurs and artifacts in the generated images. This paper presents\nMoNCE, a versatile metric that introduces image contrast to learn a calibrated\nmetric for the perception of multifaceted inter-image distances. Unlike vanilla\ncontrast which indiscriminately pushes negative samples from the anchor\nregardless of their similarity, we propose to re-weight the pushing force of\nnegative samples adaptively according to their similarity to the anchor, which\nfacilitates the contrastive learning from informative negative samples. Since\nmultiple patch-level contrastive objectives are involved in image distance\nmeasurement, we introduce optimal transport in MoNCE to modulate the pushing\nforce of negative samples collaboratively across multiple contrastive\nobjectives. Extensive experiments over multiple image translation tasks show\nthat the proposed MoNCE outperforms various prevailing metrics substantially.\nThe code is available at https://github.com/fnzhan/MoNCE.\n", "rewritten_text": "Perceiving similarities between images has long been a fundamental challenge in various visual generation tasks. Traditional methods for measuring inter-image distance typically involve calculating pointwise absolute deviations, which often result in estimating the median of instance distributions and can lead to blurriness and artifacts in the generated images. This paper introduces MoNCE, a versatile metric that incorporates image contrast to develop a calibrated metric for assessing multifaceted inter-image distances. Unlike basic contrast methods that indiscriminately push negative samples away from the anchor regardless of their similarity, our approach involves adaptively re-weighting the pushing force of negative samples based on their similarity to the anchor. This adaptive re-weighting facilitates contrastive learning from informative negative samples. Given that multiple patch-level contrastive objectives are utilized in measuring image distance, we incorporate optimal transport in MoNCE to collaboratively modulate the pushing force of negative samples across these objectives. Extensive experiments across various image translation tasks demonstrate that MoNCE significantly outperforms existing metrics. The code for MoNCE can be accessed at https://github.com/fnzhan/MoNCE."}, "2402.17292": {"original_text": "  Text-to-Avatar generation has recently made significant strides due to\nadvancements in diffusion models. However, most existing work remains\nconstrained by limited diversity, producing avatars with subtle differences in\nappearance for a given text prompt. We design DivAvatar, a novel framework that\ngenerates diverse avatars, empowering 3D creatives with a multitude of distinct\nand richly varied 3D avatars from a single text prompt. Different from most\nexisting work that exploits scene-specific 3D representations such as NeRF,\nDivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse\navatar generation from simply noise sampling in inference time. DivAvatar has\ntwo key designs that help achieve generation diversity and visual quality. The\nfirst is a noise sampling technique during training phase which is critical in\ngenerating diverse appearances. The second is a semantic-aware zoom mechanism\nand a novel depth loss, the former producing appearances of high textual\nfidelity by separate fine-tuning of specific body parts and the latter\nimproving geometry quality greatly by smoothing the generated mesh in the\nfeatures space. Extensive experiments show that DivAvatar is highly versatile\nin generating avatars of diverse appearances.\n", "rewritten_text": "Recent advancements in diffusion models have propelled the field of Text-to-Avatar generation forward. Despite this progress, existing methods are often limited in diversity, resulting in avatars that exhibit only subtle variations in appearance based on a given text prompt. In response to this challenge, we introduce DivAvatar, a groundbreaking framework designed to produce a wide array of distinct and richly varied 3D avatars from a single text input. Unlike conventional approaches that rely on scene-specific 3D representations like NeRF, DivAvatar leverages a 3D generative model (EVA3D) that can be fine-tuned to enable the generation of diverse avatars through noise sampling during inference. \n\nDivAvatar incorporates two key components to enhance generation diversity and visual quality. Firstly, a noise sampling technique is employed during the training phase to facilitate the creation of varied appearances. Secondly, the framework features a semantic-aware zoom mechanism and a novel depth loss function. The former enables the fine-tuning of specific body parts to enhance textual fidelity, while the latter significantly improves geometry quality by smoothing the generated mesh in the feature space. Extensive experiments demonstrate the versatility of DivAvatar in generating avatars with diverse appearances."}, "2208.13078": {"original_text": "  Owing to the lack of corpora for low-resource languages, current works on\ndialogue generation have mainly focused on English. In this paper, we present\nmDIA, the first large-scale multilingual benchmark for dialogue generation\nacross low- to high-resource languages. It covers real-life conversations in 46\nlanguages across 19 language families. We present baseline results obtained by\nfine-tuning the multilingual, non-dialogue-focused pre-trained model mT5 as\nwell as English-centric, dialogue-focused pre-trained chatbot DialoGPT. The\nresults show that mT5-based models perform better on sacreBLEU and BertScore\nbut worse on diversity. Even though promising results are found in few-shot and\nzero-shot scenarios, there is a large gap between the generation quality in\nEnglish and other languages. We hope that the release of mDIA could encourage\nmore works on multilingual dialogue generation to promote language diversity.\n", "rewritten_text": "Due to the lack of corpora for low-resource languages, current research on dialogue generation has predominantly focused on English. This paper introduces mDIA, the first extensive multilingual benchmark for dialogue generation spanning low- to high-resource languages. It encompasses real-life conversations in 46 languages from 19 language families. Baseline results are presented by fine-tuning the multilingual, non-dialogue-focused pre-trained model mT5, as well as the English-centric, dialogue-focused pre-trained chatbot DialoGPT. The findings indicate that mT5-based models outperform in sacreBLEU and BertScore metrics but lag in diversity. While promising results are observed in few-shot and zero-shot scenarios, a significant disparity exists in generation quality between English and other languages. The release of mDIA aims to inspire further research in multilingual dialogue generation to foster language diversity."}, "2310.08908": {"original_text": "  The large language model (LLM) has garnered significant attention due to its\nin-context learning mechanisms and emergent capabilities. The research\ncommunity has conducted several pilot studies to apply LLMs to machine\ntranslation tasks and evaluate their performance from diverse perspectives.\nHowever, previous research has primarily focused on the LLM itself and has not\nexplored human intervention in the inference process of LLM. The\ncharacteristics of LLM, such as in-context learning and prompt engineering,\nclosely mirror human cognitive abilities in language tasks, offering an\nintuitive solution for human-in-the-loop generation. In this study, we propose\na human-in-the-loop pipeline that guides LLMs to produce customized outputs\nwith revision instructions. The pipeline initiates by prompting the LLM to\nproduce a draft translation, followed by the utilization of automatic retrieval\nor human feedback as supervision signals to enhance the LLM's translation\nthrough in-context learning. The human-machine interactions generated in this\npipeline are also stored in an external database to expand the in-context\nretrieval database, enabling us to leverage human supervision in an offline\nsetting. We evaluate the proposed pipeline using GPT-3.5-turbo API on five\ndomain-specific benchmarks for German-English translation. The results\ndemonstrate the effectiveness of the pipeline in tailoring in-domain\ntranslations and improving translation performance compared to direct\ntranslation. Additionally, we discuss the results from the following\nperspectives: 1) the effectiveness of different in-context retrieval methods;\n2) the construction of a retrieval database under low-resource scenarios; 3)\nthe observed domains differences; 4) the quantitative analysis of linguistic\nstatistics; and 5) the qualitative analysis of translation cases. The code and\ndata are available at https://github.com/NLP2CT/HIL-MT/.\n", "rewritten_text": "The Large Language Model (LLM) has attracted significant attention due to its in-context learning mechanisms and emergent capabilities. The research community has conducted several pilot studies to apply LLMs to machine translation tasks and evaluate their performance from diverse perspectives. However, previous research has primarily focused on the LLM itself and has not explored human intervention in the inference process of LLM. The characteristics of LLM, such as in-context learning and prompt engineering, closely mirror human cognitive abilities in language tasks, offering an intuitive solution for human-in-the-loop generation.\n\nIn this study, we propose a human-in-the-loop pipeline that guides LLMs to produce customized outputs with revision instructions. The pipeline begins by prompting the LLM to generate a draft translation, followed by using automatic retrieval or human feedback as supervision signals to enhance the LLM's translation through in-context learning. The human-machine interactions generated in this pipeline are also stored in an external database to expand the in-context retrieval database, enabling us to leverage human supervision in an offline setting.\n\nWe evaluate the proposed pipeline using the GPT-3.5-turbo API on five domain-specific benchmarks for German-English translation. The results demonstrate the effectiveness of the pipeline in tailoring in-domain translations and improving translation performance compared to direct translation. Additionally, we discuss the results from the following perspectives: 1) the effectiveness of different in-context retrieval methods; 2) the construction of a retrieval database under low-resource scenarios; 3) the observed domain differences; 4) the quantitative analysis of linguistic statistics; and 5) the qualitative analysis of translation cases. The code and data are available at https://github.com/NLP2CT/HIL-MT/."}, "1011.0519": {"original_text": "  It is usual to consider that standards generate mixed feelings among\nscientists. They are often seen as not really reflecting the state of the art\nin a given domain and a hindrance to scientific creativity. Still, scientists\nshould theoretically be at the best place to bring their expertise into\nstandard developments, being even more neutral on issues that may typically be\nrelated to competing industrial interests. Even if it could be thought of as\neven more complex to think about developping standards in the humanities, we\nwill show how this can be made feasible through the experience gained both\nwithin the Text Encoding Initiative consortium and the International\nOrganisation for Standardisation. By taking the specific case of lexical\nresources, we will try to show how this brings about new ideas for designing\nfuture research infrastructures in the human and social sciences.\n", "rewritten_text": "It is common to believe that standards evoke mixed feelings among scientists. They are often perceived as not truly reflecting the cutting-edge advancements in a particular field and as a barrier to scientific innovation. However, scientists are ideally positioned to contribute their expertise to the development of standards, as they can offer a more impartial perspective on issues that may typically involve competing industrial interests. While it may seem more challenging to consider developing standards in the humanities, we will demonstrate how this can be achieved through the knowledge gained from both the Text Encoding Initiative consortium and the International Organization for Standardization. Focusing on lexical resources as a specific example, we will illustrate how this process can inspire new approaches to designing future research infrastructures in the human and social sciences."}, "2305.04451": {"original_text": "  Virtual try-on attracts increasing research attention as a promising way for\nenhancing the user experience for online cloth shopping. Though existing\nmethods can generate impressive results, users need to provide a well-designed\nreference image containing the target fashion clothes that often do not exist.\nTo support user-friendly fashion customization in full-body portraits, we\npropose a multi-modal interactive setting by combining the advantages of both\ntext and texture for multi-level fashion manipulation. With the carefully\ndesigned fashion editing module and loss functions, FashionTex framework can\nsemantically control cloth types and local texture patterns without annotated\npairwise training data. We further introduce an ID recovery module to maintain\nthe identity of input portrait. Extensive experiments have demonstrated the\neffectiveness of our proposed pipeline.\n", "rewritten_text": "Virtual try-on is gaining increasing research attention as a promising method to enhance the user experience in online clothing shopping. While current methods can produce impressive results, users often struggle to provide a well-designed reference image featuring the desired fashion items, which may not always be available. In order to facilitate user-friendly fashion customization in full-body portraits, we propose a multi-modal interactive approach that combines the strengths of both text and texture for multi-level fashion manipulation. Through a carefully designed fashion editing module and loss functions, the FashionTex framework allows for semantic control over clothing types and local texture patterns without the need for annotated pairwise training data. Additionally, we introduce an ID recovery module to preserve the identity of the input portrait. Extensive experiments have validated the effectiveness of our proposed pipeline."}, "2310.05989": {"original_text": "  3D object detection plays a pivotal role in autonomous driving and robotics,\ndemanding precise interpretation of Bird's Eye View (BEV) images. The dynamic\nnature of real-world environments necessitates the use of dynamic query\nmechanisms in 3D object detection to adaptively capture and process the complex\nspatio-temporal relationships present in these scenes. However, prior\nimplementations of dynamic queries have often faced difficulties in effectively\nleveraging these relationships, particularly when it comes to integrating\ntemporal information in a computationally efficient manner. Addressing this\nlimitation, we introduce a framework utilizing dynamic query evolution\nstrategy, harnesses K-means clustering and Top-K attention mechanisms for\nrefined spatio-temporal data processing. By dynamically segmenting the BEV\nspace and prioritizing key features through Top-K attention, our model achieves\na real-time, focused analysis of pertinent scene elements. Our extensive\nevaluation on the nuScenes and Waymo dataset showcases a marked improvement in\ndetection accuracy, setting a new benchmark in the domain of query-based BEV\nobject detection. Our dynamic query evolution strategy has the potential to\npush the boundaries of current BEV methods with enhanced adaptability and\ncomputational efficiency. Project page:\nhttps://github.com/Jiawei-Yao0812/QE-BEV\n", "rewritten_text": "3D object detection plays a crucial role in autonomous driving and robotics by requiring precise interpretation of Bird's Eye View (BEV) images. The dynamic nature of real-world environments necessitates the use of dynamic query mechanisms in 3D object detection to adaptively capture and process the complex spatio-temporal relationships present in these scenes. However, previous implementations of dynamic queries have often struggled to effectively leverage these relationships, especially in integrating temporal information in a computationally efficient manner. To address this limitation, we introduce a framework that utilizes a dynamic query evolution strategy, incorporating K-means clustering and Top-K attention mechanisms for refined spatio-temporal data processing. By dynamically segmenting the BEV space and prioritizing key features through Top-K attention, our model achieves real-time, focused analysis of pertinent scene elements. Our extensive evaluation on the nuScenes and Waymo datasets demonstrates a significant improvement in detection accuracy, setting a new benchmark in the domain of query-based BEV object detection. The dynamic query evolution strategy we propose has the potential to advance current BEV methods by enhancing adaptability and computational efficiency. For more information, please visit our project page at: https://github.com/Jiawei-Yao0812/QE-BEV"}, "1907.03513": {"original_text": "  Keeping up to date on emerging entities that appear every day is\nindispensable for various applications, such as social-trend analysis and\nmarketing research. Previous studies have attempted to detect unseen entities\nthat are not registered in a particular knowledge base as emerging entities and\nconsequently find non-emerging entities since the absence of entities in\nknowledge bases does not guarantee their emergence. We therefore introduce a\nnovel task of discovering truly emerging entities when they have just been\nintroduced to the public through microblogs and propose an effective method\nbased on time-sensitive distant supervision, which exploits distinctive\nearly-stage contexts of emerging entities. Experimental results with a\nlarge-scale Twitter archive show that the proposed method achieves 83.2%\nprecision of the top 500 discovered emerging entities, which outperforms\nbaselines based on unseen entity recognition with burst detection. Besides\nnotable emerging entities, our method can discover massive long-tail and\nhomographic emerging entities. An evaluation of relative recall shows that the\nmethod detects 80.4% emerging entities newly registered in Wikipedia; 92.4% of\nthem are discovered earlier than their registration in Wikipedia, and the\naverage lead-time is more than one year (571 days).\n", "rewritten_text": "Staying current with the emerging entities that arise daily is crucial for various applications, such as social trend analysis and marketing research. Previous studies have made efforts to identify unseen entities that are not listed in a specific knowledge base as emerging entities. This is important because the absence of entities in knowledge bases does not guarantee their emergence. Therefore, we present a new task of identifying truly emerging entities as soon as they are introduced to the public through microblogs. We propose an effective method that relies on time-sensitive distant supervision, leveraging unique early-stage contexts of emerging entities.\n\nResults from experiments conducted on a large-scale Twitter archive demonstrate that our proposed method achieves a precision of 83.2% for the top 500 discovered emerging entities, surpassing baselines that rely on unseen entity recognition with burst detection. In addition to identifying significant emerging entities, our method can uncover numerous lesser-known and homographic emerging entities. An evaluation of relative recall reveals that the method identifies 80.4% of emerging entities that are newly registered in Wikipedia, with 92.4% of them being discovered before their registration on Wikipedia. On average, the method detects emerging entities more than a year in advance, with an average lead-time of 571 days."}, "2312.04567": {"original_text": "  Recent significant advances in text-to-image models unlock the possibility of\ntraining vision systems using synthetic images, potentially overcoming the\ndifficulty of collecting curated data at scale. It is unclear, however, how\nthese models behave at scale, as more synthetic data is added to the training\nset. In this paper we study the scaling laws of synthetic images generated by\nstate of the art text-to-image models, for the training of supervised models:\nimage classifiers with label supervision, and CLIP with language supervision.\nWe identify several factors, including text prompts, classifier-free guidance\nscale, and types of text-to-image models, that significantly affect scaling\nbehavior. After tuning these factors, we observe that synthetic images\ndemonstrate a scaling trend similar to, but slightly less effective than, real\nimages in CLIP training, while they significantly underperform in scaling when\ntraining supervised image classifiers. Our analysis indicates that the main\nreason for this underperformance is the inability of off-the-shelf\ntext-to-image models to generate certain concepts, a limitation that\nsignificantly impairs the training of image classifiers. Our findings also\nsuggest that scaling synthetic data can be particularly effective in scenarios\nsuch as: (1) when there is a limited supply of real images for a supervised\nproblem (e.g., fewer than 0.5 million images in ImageNet), (2) when the\nevaluation dataset diverges significantly from the training data, indicating\nthe out-of-distribution scenario, or (3) when synthetic data is used in\nconjunction with real images, as demonstrated in the training of CLIP models.\n", "rewritten_text": "Recent significant advancements in text-to-image models have unlocked the potential for training vision systems using synthetic images, which could potentially address the challenge of acquiring curated data on a large scale. However, the behavior of these models when scaled up with the addition of more synthetic data to the training set remains unclear. This paper delves into the scaling laws of synthetic images produced by cutting-edge text-to-image models for training supervised models, including image classifiers with label supervision and CLIP with language supervision. Various factors, such as text prompts, guidance scale without classifiers, and types of text-to-image models, have been identified as significantly influencing scaling behavior. Upon fine-tuning these factors, it was observed that synthetic images exhibit a scaling trend similar to real images in CLIP training, albeit slightly less effective, while they notably underperform in scaling when training supervised image classifiers. The analysis suggests that the primary reason for this underperformance lies in the inability of off-the-shelf text-to-image models to generate certain concepts, which significantly hinders the training of image classifiers. Furthermore, the findings indicate that scaling synthetic data can be particularly beneficial in scenarios where there is a scarcity of real images for a supervised problem (e.g., fewer than 0.5 million images in ImageNet), when the evaluation dataset differs significantly from the training data, indicating an out-of-distribution scenario, or when synthetic data is combined with real images, as demonstrated in the training of CLIP models."}, "1909.1244": {"original_text": "  Recently, pre-trained language models have achieved remarkable success in a\nbroad range of natural language processing tasks. However, in multilingual\nsetting, it is extremely resource-consuming to pre-train a deep language model\nover large-scale corpora for each language. Instead of exhaustively\npre-training monolingual language models independently, an alternative solution\nis to pre-train a powerful multilingual deep language model over large-scale\ncorpora in hundreds of languages. However, the vocabulary size for each\nlanguage in such a model is relatively small, especially for low-resource\nlanguages. This limitation inevitably hinders the performance of these\nmultilingual models on tasks such as sequence labeling, wherein in-depth\ntoken-level or sentence-level understanding is essential.\n  In this paper, inspired by previous methods designed for monolingual\nsettings, we investigate two approaches (i.e., joint mapping and mixture\nmapping) based on a pre-trained multilingual model BERT for addressing the\nout-of-vocabulary (OOV) problem on a variety of tasks, including part-of-speech\ntagging, named entity recognition, machine translation quality estimation, and\nmachine reading comprehension. Experimental results show that using mixture\nmapping is more promising. To the best of our knowledge, this is the first work\nthat attempts to address and discuss the OOV issue in multilingual settings.\n", "rewritten_text": "Recently, pre-trained language models have achieved remarkable success in a wide range of natural language processing tasks. However, in a multilingual setting, it is highly resource-intensive to pre-train a deep language model using large-scale corpora for each language. Instead of independently pre-training monolingual language models exhaustively, an alternative approach is to pre-train a robust multilingual deep language model using large-scale corpora across hundreds of languages. Nevertheless, the vocabulary size for each language in such a model is relatively small, particularly for low-resource languages. This limitation inevitably impedes the performance of these multilingual models on tasks like sequence labeling, where a thorough token-level or sentence-level comprehension is crucial.\n\nIn this paper, drawing inspiration from previous methods tailored for monolingual settings, we explore two approaches (joint mapping and mixture mapping) based on a pre-trained multilingual model BERT to tackle the out-of-vocabulary (OOV) issue across various tasks, including part-of-speech tagging, named entity recognition, machine translation quality estimation, and machine reading comprehension. Experimental findings indicate that utilizing mixture mapping shows more promise. To the best of our knowledge, this is the first study that endeavors to address and discuss the OOV problem in multilingual contexts."}, "2311.05821": {"original_text": "  While recent advances have boosted LM proficiency in linguistic benchmarks,\nLMs consistently struggle to reason correctly on complex tasks like\nmathematics. We turn to Reinforcement Learning from Human Feedback (RLHF) as a\nmethod with which to shape model reasoning processes. In particular, we explore\ntwo reward schemes, outcome-supervised reward models (ORMs) and\nprocess-supervised reward models (PRMs), to optimize for logical reasoning. Our\nresults show that the fine-grained reward provided by PRM-based methods\nenhances accuracy on simple mathematical reasoning (GSM8K) while, unexpectedly,\nreducing performance in complex tasks (MATH). Furthermore, we show the critical\nrole reward aggregation functions play in model performance. Providing\npromising avenues for future research, our study underscores the need for\nfurther exploration into fine-grained reward modeling for more reliable\nlanguage models.\n", "rewritten_text": "Recent advancements have improved LM proficiency in linguistic benchmarks, but LMs still face challenges in correctly reasoning on complex tasks such as mathematics. To address this issue, we propose utilizing Reinforcement Learning from Human Feedback (RLHF) as a method to guide model reasoning processes. Specifically, we investigate two reward schemes: outcome-supervised reward models (ORMs) and process-supervised reward models (PRMs), aiming to enhance logical reasoning capabilities.\n\nOur findings reveal that the detailed rewards offered by PRM-based approaches improve accuracy in simple mathematical reasoning (GSM8K), while unexpectedly leading to a decrease in performance on complex tasks (MATH). Additionally, we highlight the crucial role of reward aggregation functions in influencing model performance. This study suggests promising directions for future research and emphasizes the importance of exploring fine-grained reward modeling to enhance the reliability of language models."}, "1309.6379": {"original_text": "  We propose a large deformation diffeomorphic metric mapping algorithm to\nalign multiple b-value diffusion weighted imaging (mDWI) data, specifically\nacquired via hybrid diffusion imaging (HYDI), denoted as LDDMM-HYDI. We then\npropose a Bayesian model for estimating the white matter atlas from HYDIs. We\nadopt the work given in Hosseinbor et al. (2012) and represent the q-space\ndiffusion signal with the Bessel Fourier orientation reconstruction (BFOR)\nsignal basis. The BFOR framework provides the representation of mDWI in the\nq-space and thus reduces memory requirement. In addition, since the BFOR signal\nbasis is orthonormal, the L2 norm that quantifies the differences in the\nq-space signals of any two mDWI datasets can be easily computed as the sum of\nthe squared differences in the BFOR expansion coefficients. In this work, we\nshow that the reorientation of the $q$-space signal due to spatial\ntransformation can be easily defined on the BFOR signal basis. We incorporate\nthe BFOR signal basis into the LDDMM framework and derive the gradient descent\nalgorithm for LDDMM-HYDI with explicit orientation optimization. Additionally,\nwe extend the previous Bayesian atlas estimation framework for scalar-valued\nimages to HYDIs and derive the expectation-maximization algorithm for solving\nthe HYDI atlas estimation problem. Using real HYDI datasets, we show the\nBayesian model generates the white matter atlas with anatomical details.\nMoreover, we show that it is important to consider the variation of mDWI\nreorientation due to a small change in diffeomorphic transformation in the\nLDDMM-HYDI optimization and to incorporate the full information of HYDI for\naligning mDWI.\n", "rewritten_text": "We introduce the LDDMM-HYDI algorithm, which is a large deformation diffeomorphic metric mapping algorithm designed to align multiple b-value diffusion weighted imaging (mDWI) data obtained through hybrid diffusion imaging (HYDI). Subsequently, we propose a Bayesian model for estimating the white matter atlas from HYDIs. Our approach builds upon the methodology presented by Hosseinbor et al. (2012) and utilizes the Bessel Fourier orientation reconstruction (BFOR) signal basis to represent the q-space diffusion signal. The BFOR framework offers a concise representation of mDWI in the q-space, leading to reduced memory requirements. Furthermore, the orthonormal nature of the BFOR signal basis enables straightforward computation of the L2 norm, which quantifies differences in q-space signals between any two mDWI datasets by summing the squared differences in the BFOR expansion coefficients.\n\nIn this study, we demonstrate that reorienting the q-space signal following spatial transformations can be readily defined using the BFOR signal basis. By integrating the BFOR signal basis into the LDDMM framework, we develop a gradient descent algorithm for LDDMM-HYDI with explicit orientation optimization. Additionally, we extend the Bayesian atlas estimation framework for scalar-valued images to HYDIs and derive an expectation-maximization algorithm for solving the HYDI atlas estimation problem. Through experimentation with actual HYDI datasets, we illustrate that the Bayesian model produces a white matter atlas with detailed anatomical information. Furthermore, we emphasize the significance of accounting for variations in mDWI reorientation resulting from minor changes in diffeomorphic transformations during LDDMM-HYDI optimization and stress the importance of leveraging the complete information from HYDI to align mDWI datasets effectively."}, "2101.03848": {"original_text": "  Convolutional neural networks (CNNs) have been widely used in various vision\ntasks, e.g. image classification, semantic segmentation, etc. Unfortunately,\nstandard 2D CNNs are not well suited for spherical signals such as panorama\nimages or spherical projections, as the sphere is an unstructured grid. In this\npaper, we present Spherical Transformer which can transform spherical signals\ninto vectors that can be directly processed by standard CNNs such that many\nwell-designed CNNs architectures can be reused across tasks and datasets by\npretraining. To this end, the proposed method first uses local structured\nsampling methods such as HEALPix to construct a transformer grid by using the\ninformation of spherical points and its adjacent points, and then transforms\nthe spherical signals to the vectors through the grid. By building the\nSpherical Transformer module, we can use multiple CNN architectures directly.\nWe evaluate our approach on the tasks of spherical MNIST recognition, 3D object\nclassification and omnidirectional image semantic segmentation. For 3D object\nclassification, we further propose a rendering-based projection method to\nimprove the performance and a rotational-equivariant model to improve the\nanti-rotation ability. Experimental results on three tasks show that our\napproach achieves superior performance over state-of-the-art methods.\n", "rewritten_text": "Convolutional neural networks (CNNs) have been widely utilized in various vision tasks, such as image classification and semantic segmentation. However, standard 2D CNNs are not ideal for processing spherical signals like panorama images or spherical projections due to the unstructured grid of a sphere. This paper introduces the Spherical Transformer, a method that converts spherical signals into vectors compatible with standard CNNs. This enables the reuse of well-designed CNN architectures across different tasks and datasets through pretraining.\n\nThe proposed approach begins by employing local structured sampling techniques, such as HEALPix, to create a transformer grid based on the information of spherical points and their neighbors. Subsequently, the spherical signals are transformed into vectors using this grid. By incorporating the Spherical Transformer module, multiple CNN architectures can be directly applied.\n\nThe effectiveness of our method is demonstrated through evaluations on tasks including spherical MNIST recognition, 3D object classification, and omnidirectional image semantic segmentation. For 3D object classification, we introduce a rendering-based projection method to enhance performance and a rotational-equivariant model to improve anti-rotation capabilities. Experimental results across these tasks indicate that our approach outperforms state-of-the-art methods."}, "2209.02686": {"original_text": "  Image-to-image translation has played an important role in enabling synthetic\ndata for computer vision. However, if the source and target domains have a\nlarge semantic mismatch, existing techniques often suffer from source content\ncorruption aka semantic flipping. To address this problem, we propose a new\nparadigm for image-to-image translation using Vector Symbolic Architectures\n(VSA), a theoretical framework which defines algebraic operations in a\nhigh-dimensional vector (hypervector) space. We introduce VSA-based constraints\non adversarial learning for source-to-target translations by learning a\nhypervector mapping that inverts the translation to ensure consistency with\nsource content. We show both qualitatively and quantitatively that our method\nimproves over other state-of-the-art techniques.\n", "rewritten_text": "Image-to-image translation has been crucial in facilitating the generation of synthetic data for computer vision. However, when there is a significant semantic mismatch between the source and target domains, existing techniques often encounter issues such as source content corruption, also known as semantic flipping. To tackle this challenge, we propose a novel approach for image-to-image translation utilizing Vector Symbolic Architectures (VSA). VSA is a theoretical framework that delineates algebraic operations within a high-dimensional vector (hypervector) space.\n\nOur method introduces VSA-based constraints on adversarial learning for source-to-target translations by training a hypervector mapping that reverses the translation process to maintain consistency with the source content. Through both qualitative and quantitative analyses, we demonstrate that our approach outperforms other state-of-the-art techniques."}, "2305.14576": {"original_text": "  Pre-trained language models (PLMs) have ignited a surge in demand for\neffective fine-tuning techniques, particularly in low-resource domains and\nlanguages. Active learning (AL), a set of algorithms designed to decrease\nlabeling costs by minimizing label complexity, has shown promise in confronting\nthe labeling bottleneck. In parallel, adapter modules designed for\nparameter-efficient fine-tuning (PEFT) have demonstrated notable potential in\nlow-resource settings. However, the interplay between AL and adapter-based PEFT\nremains unexplored. We present an empirical study of PEFT behavior with AL in\nlow-resource settings for text classification tasks. Our findings affirm the\nsuperiority of PEFT over full-fine tuning (FFT) in low-resource settings and\ndemonstrate that this advantage persists in AL setups. We further examine the\nproperties of PEFT and FFT through the lens of forgetting dynamics and\ninstance-level representations, where we find that PEFT yields more stable\nrepresentations of early and middle layers compared to FFT. Our research\nunderscores the synergistic potential of AL and PEFT in low-resource settings,\npaving the way for advancements in efficient and effective fine-tuning.\n", "rewritten_text": "Pre-trained language models (PLMs) have sparked a surge in demand for effective fine-tuning techniques, especially in low-resource domains and languages. Active learning (AL), a collection of algorithms aimed at reducing labeling costs by minimizing label complexity, has shown promise in addressing the labeling bottleneck. Concurrently, adapter modules designed for parameter-efficient fine-tuning (PEFT) have exhibited significant potential in low-resource environments. However, the interaction between AL and adapter-based PEFT has not been thoroughly explored. In this study, we present empirical findings on the behavior of PEFT when combined with AL in low-resource settings for text classification tasks. Our results confirm the superiority of PEFT over full-fine tuning (FFT) in low-resource scenarios and demonstrate that this advantage persists in AL configurations. Additionally, we analyze the characteristics of PEFT and FFT in terms of forgetting dynamics and instance-level representations, revealing that PEFT produces more stable representations in early and middle layers compared to FFT. Our research highlights the synergistic potential of AL and PEFT in low-resource contexts, paving the way for advancements in efficient and effective fine-tuning."}, "2308.15448": {"original_text": "  The negative effects of online bullying and harassment are increasing with\nInternet popularity, especially in social media. One solution is using natural\nlanguage processing (NLP) and machine learning (ML) methods for the automatic\ndetection of harmful remarks, but these methods are limited in low-resource\nlanguages like the Chittagonian dialect of Bangla.This study focuses on\ndetecting vulgar remarks in social media using supervised ML and deep learning\nalgorithms.Logistic Regression achieved promising accuracy (0.91) while simple\nRNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the\nissue that NN algorithms require more data.\n", "rewritten_text": "The prevalence of online bullying and harassment is on the rise due to the increasing popularity of the Internet, particularly on social media platforms. One potential solution to address this issue is the utilization of natural language processing (NLP) and machine learning (ML) techniques for the automated detection of harmful comments. However, these methods face limitations when applied to low-resource languages such as the Chittagonian dialect of Bangla.\n\nThis study specifically aims to identify offensive comments on social media by employing supervised ML and deep learning algorithms. Logistic Regression demonstrated a high level of accuracy at 0.91, whereas simple RNN models using Word2vec and fastText achieved slightly lower accuracies ranging from 0.84 to 0.90. This underscores the challenge that neural network algorithms face in terms of requiring a larger dataset for optimal performance."}, "2305.06052": {"original_text": "  Quantization is a widely adopted technique for deep neural networks to reduce\nthe memory and computational resources required. However, when quantized, most\nmodels would need a suitable calibration process to keep their performance\nintact, which requires data from the target domain, such as a fraction of the\ndataset used in model training and model validation (i.e. calibration dataset).\n  In this study, we investigate the use of synthetic data as a substitute for\nthe calibration with real data for the quantization method. We propose a data\ngeneration method based on Generative Adversarial Networks that are trained\nprior to the model quantization step. We compare the performance of models\nquantized using data generated by StyleGAN2-ADA and our pre-trained DiStyleGAN,\nwith quantization using real data and an alternative data generation method\nbased on fractal images. Overall, the results of our experiments demonstrate\nthe potential of leveraging synthetic data for calibration during the\nquantization process. In our experiments, the percentage of accuracy\ndegradation of the selected models was less than 0.6%, with our best\nperformance achieved on MobileNetV2 (0.05%). The code is available at:\nhttps://github.com/ThanosM97/gsoc2022-openvino\n", "rewritten_text": "Quantization is a widely adopted technique in deep neural networks to reduce the memory and computational resources required. However, after quantization, most models typically require a suitable calibration process to maintain their performance. This calibration process necessitates data from the target domain, such as a subset of the dataset used during model training and validation (referred to as the calibration dataset).\n\nIn this study, we explore the potential of using synthetic data as a replacement for real data in the quantization process. We introduce a data generation approach based on Generative Adversarial Networks that are trained before the quantization of the model. We conduct a comparative analysis of model performance when quantized using data generated by StyleGAN2-ADA and our pre-trained DiStyleGAN, against quantization using real data and an alternative data generation method based on fractal images. Our experimental results highlight the promise of leveraging synthetic data for calibration during the quantization process. Notably, the accuracy degradation of the selected models in our experiments was less than 0.6%, with the best performance achieved on MobileNetV2 (0.05%).\n\nFor those interested, the code is accessible at: https://github.com/ThanosM97/gsoc2022-openvino"}, "2406.15719": {"original_text": "  Convolutional Neural Networks (CNNs) and vision transformers (ViTs) have\nshown excellent capability in complex hyperspectral image (HSI) classification.\nHowever, these models require a significant number of training data and are\ncomputational resources. On the other hand, modern Multi-Layer Perceptrons\n(MLPs) have demonstrated great classification capability. These modern\nMLP-based models require significantly less training data compared to CNNs and\nViTs, achieving the state-of-the-art classification accuracy. Recently,\nKolmogorov-Arnold Networks (KANs) were proposed as viable alternatives for\nMLPs. Because of their internal similarity to splines and their external\nsimilarity to MLPs, KANs are able to optimize learned features with remarkable\naccuracy in addition to being able to learn new features. Thus, in this study,\nwe assess the effectiveness of KANs for complex HSI data classification.\nMoreover, to enhance the HSI classification accuracy obtained by the KANs, we\ndevelop and propose a Hybrid architecture utilizing 1D, 2D, and 3D KANs. To\ndemonstrate the effectiveness of the proposed KAN architecture, we conducted\nextensive experiments on three newly created HSI benchmark datasets:\nQUH-Pingan, QUH-Tangdaowan, and QUH-Qingyun. The results underscored the\ncompetitive or better capability of the developed hybrid KAN-based model across\nthese benchmark datasets over several other CNN- and ViT-based algorithms,\nincluding 1D-CNN, 2DCNN, 3D CNN, VGG-16, ResNet-50, EfficientNet, RNN, and ViT.\nThe code are publicly available at (https://github.com/aj1365/HSIConvKAN)\n", "rewritten_text": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) have demonstrated excellent capabilities in the classification of complex hyperspectral images (HSI). However, these models require a significant amount of training data and computational resources. On the contrary, modern Multi-Layer Perceptrons (MLPs) have shown great classification performance while requiring significantly less training data compared to CNNs and ViTs, achieving state-of-the-art accuracy.\n\nRecently, Kolmogorov-Arnold Networks (KANs) have emerged as promising alternatives to MLPs. Due to their internal resemblance to splines and external similarity to MLPs, KANs can optimize learned features with remarkable accuracy and adaptability to new features. Therefore, this study aims to evaluate the effectiveness of KANs in complex HSI data classification.\n\nFurthermore, to enhance the classification accuracy of HSI using KANs, we propose a Hybrid architecture that integrates 1D, 2D, and 3D KANs. To showcase the effectiveness of this proposed KAN architecture, extensive experiments were conducted on three newly developed HSI benchmark datasets: QUH-Pingan, QUH-Tangdaowan, and QUH-Qingyun. The results highlight the competitive or superior performance of the hybrid KAN-based model across these benchmark datasets compared to various CNN- and ViT-based algorithms, including 1D-CNN, 2D-CNN, 3D-CNN, VGG-16, ResNet-50, EfficientNet, RNN, and ViT.\n\nThe code is publicly available at (https://github.com/aj1365/HSIConvKAN)."}, "1905.00413": {"original_text": "  We explore the problem of view synthesis from a narrow baseline pair of\nimages, and focus on generating high-quality view extrapolations with plausible\ndisocclusions. Our method builds upon prior work in predicting a multiplane\nimage (MPI), which represents scene content as a set of RGB$\\alpha$ planes\nwithin a reference view frustum and renders novel views by projecting this\ncontent into the target viewpoints. We present a theoretical analysis showing\nhow the range of views that can be rendered from an MPI increases linearly with\nthe MPI disparity sampling frequency, as well as a novel MPI prediction\nprocedure that theoretically enables view extrapolations of up to $4\\times$ the\nlateral viewpoint movement allowed by prior work. Our method ameliorates two\nspecific issues that limit the range of views renderable by prior methods: 1)\nWe expand the range of novel views that can be rendered without depth\ndiscretization artifacts by using a 3D convolutional network architecture along\nwith a randomized-resolution training procedure to allow our model to predict\nMPIs with increased disparity sampling frequency. 2) We reduce the repeated\ntexture artifacts seen in disocclusions by enforcing a constraint that the\nappearance of hidden content at any depth must be drawn from visible content at\nor behind that depth. Please see our results video at:\nhttps://www.youtube.com/watch?v=aJqAaMNL2m4.\n", "rewritten_text": "We investigate the issue of view synthesis using a narrow baseline pair of images, with a focus on generating high-quality view extrapolations that include plausible disocclusions. Our approach builds on previous research in predicting a multiplane image (MPI), which represents scene content as a collection of RGB$\\alpha$ planes within a reference view frustum. This method renders new views by projecting this content into the desired viewpoints. \n\nWe provide a theoretical analysis demonstrating that the range of views that can be generated from an MPI increases linearly with the MPI disparity sampling frequency. Additionally, we introduce a novel MPI prediction technique that theoretically allows for view extrapolations up to $4\\times$ the lateral viewpoint movement permitted by previous methods. Our approach addresses two specific challenges that restrict the range of renderable views in existing methods:\n\n1) We enhance the range of novel views that can be rendered without depth discretization artifacts by employing a 3D convolutional network architecture and a randomized-resolution training procedure. This enables our model to predict MPIs with an increased disparity sampling frequency.\n\n2) We mitigate the repeated texture artifacts observed in disocclusions by imposing a constraint that the appearance of hidden content at any depth must be derived from visible content at or behind that depth.\n\nFor further details, please refer to our results video at: https://www.youtube.com/watch?v=aJqAaMNL2m4."}, "1909.10579": {"original_text": "  Neural language models (LMs) perform well on tasks that require sensitivity\nto syntactic structure. Drawing on the syntactic priming paradigm from\npsycholinguistics, we propose a novel technique to analyze the representations\nthat enable such success. By establishing a gradient similarity metric between\nstructures, this technique allows us to reconstruct the organization of the\nLMs' syntactic representational space. We use this technique to demonstrate\nthat LSTM LMs' representations of different types of sentences with relative\nclauses are organized hierarchically in a linguistically interpretable manner,\nsuggesting that the LMs track abstract properties of the sentence.\n", "rewritten_text": "Neural language models (LMs) excel in tasks that necessitate an understanding of syntactic structure. Leveraging the syntactic priming paradigm from psycholinguistics, we introduce a new method to examine the representations that underlie this success. This method involves establishing a gradient similarity metric between structures, enabling us to reconstruct the organization of the LMs' syntactic representational space. Through this approach, we illustrate that LSTM LMs organize representations of various sentence types with relative clauses in a hierarchical and linguistically interpretable fashion. This suggests that the LMs capture abstract properties of sentences."}, "1804.04213": {"original_text": "  We study how to synthesize novel views of human body from a single image.\nThough recent deep learning based methods work well for rigid objects, they\noften fail on objects with large articulation, like human bodies. The core step\nof existing methods is to fit a map from the observable views to novel views by\nCNNs; however, the rich articulation modes of human body make it rather\nchallenging for CNNs to memorize and interpolate the data well. To address the\nproblem, we propose a novel deep learning based pipeline that explicitly\nestimates and leverages the geometry of the underlying human body. Our new\npipeline is a composition of a shape estimation network and an image generation\nnetwork, and at the interface a perspective transformation is applied to\ngenerate a forward flow for pixel value transportation. Our design is able to\nfactor out the space of data variation and makes learning at each step much\neasier. Empirically, we show that the performance for pose-varying objects can\nbe improved dramatically. Our method can also be applied on real data captured\nby 3D sensors, and the flow generated by our methods can be used for generating\nhigh quality results in higher resolution.\n", "rewritten_text": "We are researching how to generate new perspectives of the human body from a single image. While recent deep learning methods have been successful for rigid objects, they often struggle with objects that have complex articulation, such as human bodies. The main challenge with existing methods is mapping observable views to new views using CNNs, as the intricate articulation of the human body makes it difficult for CNNs to effectively memorize and interpolate the data.\n\nTo tackle this issue, we introduce a novel deep learning pipeline that explicitly estimates and utilizes the geometry of the human body. Our pipeline consists of a shape estimation network and an image generation network, with a perspective transformation applied at the interface to facilitate pixel value transportation. This design helps to isolate the data variations, making learning more manageable at each stage. Through empirical testing, we demonstrate a significant improvement in performance for objects with varying poses. Additionally, our method can be applied to real data captured by 3D sensors, and the flow generated by our approach can produce high-quality results at higher resolutions."}, "2401.04354": {"original_text": "  With the explosive growth of video data in real-world applications, a\ncomprehensive representation of videos becomes increasingly important. In this\npaper, we address the problem of video scene recognition, whose goal is to\nlearn a high-level video representation to classify scenes in videos. Due to\nthe diversity and complexity of video contents in realistic scenarios, this\ntask remains a challenge. Most existing works identify scenes for videos only\nfrom visual or textual information in a temporal perspective, ignoring the\nvaluable information hidden in single frames, while several earlier studies\nonly recognize scenes for separate images in a non-temporal perspective. We\nargue that these two perspectives are both meaningful for this task and\ncomplementary to each other, meanwhile, externally introduced knowledge can\nalso promote the comprehension of videos. We propose a novel two-stream\nframework to model video representations from multiple perspectives, i.e.\ntemporal and non-temporal perspectives, and integrate the two perspectives in\nan end-to-end manner by self-distillation. Besides, we design a\nknowledge-enhanced feature fusion and label prediction method that contributes\nto naturally introducing knowledge into the task of video scene recognition.\nExperiments conducted on a real-world dataset demonstrate the effectiveness of\nour proposed method.\n", "rewritten_text": "The explosive growth of video data in real-world applications highlights the increasing importance of a comprehensive representation of videos. This paper focuses on addressing the challenge of video scene recognition, aiming to develop a high-level video representation for classifying scenes within videos. Given the diverse and complex nature of video content in realistic scenarios, this task presents ongoing challenges.\n\nMany existing works primarily identify scenes in videos based on visual or textual information from a temporal perspective, overlooking valuable information contained within individual frames. Conversely, earlier studies have focused on recognizing scenes in separate images from a non-temporal perspective. We argue that both perspectives are meaningful and complementary to each other for this task, and the incorporation of external knowledge can enhance video comprehension.\n\nTo tackle these challenges, we propose a novel two-stream framework that captures video representations from both temporal and non-temporal perspectives. We integrate these perspectives in an end-to-end manner through self-distillation. Additionally, we introduce a knowledge-enhanced feature fusion and label prediction method to naturally incorporate external knowledge into the video scene recognition task.\n\nExperimental results on a real-world dataset validate the effectiveness of our proposed method in enhancing video scene recognition."}, "2211.08112": {"original_text": "  Active Learning (AL) is a powerful tool for learning with less labeled data,\nin particular, for specialized domains, like legal documents, where unlabeled\ndata is abundant, but the annotation requires domain expertise and is thus\nexpensive. Recent works have shown the effectiveness of AL strategies for\npre-trained language models. However, most AL strategies require a set of\nlabeled samples to start with, which is expensive to acquire. In addition,\npre-trained language models have been shown unstable during fine-tuning with\nsmall datasets, and their embeddings are not semantically meaningful. In this\nwork, we propose a pipeline for effectively using active learning with\npre-trained language models in the legal domain. To this end, we leverage the\navailable unlabeled data in three phases. First, we continue pre-training the\nmodel to adapt it to the downstream task. Second, we use knowledge distillation\nto guide the model's embeddings to a semantically meaningful space. Finally, we\npropose a simple, yet effective, strategy to find the initial set of labeled\nsamples with fewer actions compared to existing methods. Our experiments on\nContract-NLI, adapted to the classification task, and LEDGAR benchmarks show\nthat our approach outperforms standard AL strategies, and is more efficient.\nFurthermore, our pipeline reaches comparable results to the fully-supervised\napproach with a small performance gap, and dramatically reduced annotation\ncost. Code and the adapted data will be made available.\n", "rewritten_text": "Active Learning (AL) is a potent tool for learning with limited labeled data, especially in specialized domains such as legal documents, where there is an abundance of unlabeled data but annotating it requires domain expertise and is therefore costly. Recent studies have demonstrated the efficacy of AL strategies for pre-trained language models. However, most AL strategies necessitate an initial set of labeled samples, which can be expensive to obtain. Moreover, pre-trained language models have been observed to be unstable during fine-tuning with small datasets, and their embeddings lack semantic significance.\n\nIn this study, we introduce a pipeline for effectively utilizing active learning with pre-trained language models in the legal domain. To achieve this, we leverage the available unlabeled data in three phases. Firstly, we continue pre-training the model to tailor it to the downstream task. Secondly, we employ knowledge distillation to steer the model's embeddings towards a semantically meaningful space. Lastly, we propose a straightforward yet efficient strategy to identify the initial set of labeled samples with fewer actions compared to existing methods.\n\nOur experiments on Contract-NLI, adapted for the classification task, and LEDGAR benchmarks reveal that our approach surpasses standard AL strategies in terms of performance and efficiency. Furthermore, our pipeline yields results comparable to the fully-supervised approach with only a slight performance gap, significantly reducing annotation costs. The code and adapted data will be made accessible."}, "2102.03115": {"original_text": "  Object detection in natural scenes can be a challenging task. In many\nreal-life situations, the visible spectrum is not suitable for traditional\ncomputer vision tasks. Moving outside the visible spectrum range, such as the\nthermal spectrum or the near-infrared (NIR) images, is much more beneficial in\nlow visibility conditions, NIR images are very helpful for understanding the\nobject's material quality. In this work, we have taken images with both the\nThermal and NIR spectrum for the object detection task. As multi-spectral data\nwith both Thermal and NIR is not available for the detection task, we needed to\ncollect data ourselves. Data collection is a time-consuming process, and we\nfaced many obstacles that we had to overcome. We train the YOLO v3 network from\nscratch to detect an object from multi-spectral images. Also, to avoid\noverfitting, we have done data augmentation and tune hyperparameters.\n", "rewritten_text": "Detecting objects in natural scenes can pose a significant challenge. In many real-life scenarios, the visible spectrum proves inadequate for traditional computer vision tasks. Expanding beyond the visible spectrum, such as utilizing the thermal spectrum or near-infrared (NIR) images, offers distinct advantages in low visibility conditions. NIR images, in particular, play a crucial role in discerning an object's material quality.\n\nIn this study, we acquired images encompassing both the thermal and NIR spectra for the object detection task. Given the unavailability of multi-spectral data containing both thermal and NIR components for this specific detection task, we undertook the task of data collection ourselves. This process proved time-consuming and presented numerous obstacles that we diligently surmounted.\n\nOur approach involved training the YOLO v3 network from scratch to detect objects within multi-spectral images. To mitigate the risk of overfitting, we implemented data augmentation techniques and fine-tuned hyperparameters."}, "2207.09157": {"original_text": "  Supervised deep learning-based approaches have been applied to task-oriented\ndialog and have proven to be effective for limited domain and language\napplications when a sufficient number of training examples are available. In\npractice, these approaches suffer from the drawbacks of domain-driven design\nand under-resourced languages. Domain and language models are supposed to grow\nand change as the problem space evolves. On one hand, research on transfer\nlearning has demonstrated the cross-lingual ability of multilingual\nTransformers-based models to learn semantically rich representations. On the\nother, in addition to the above approaches, meta-learning have enabled the\ndevelopment of task and language learning algorithms capable of far\ngeneralization. Through this context, this article proposes to investigate the\ncross-lingual transferability of using synergistically few-shot learning with\nprototypical neural networks and multilingual Transformers-based models.\nExperiments in natural language understanding tasks on MultiATIS++ corpus shows\nthat our approach substantially improves the observed transfer learning\nperformances between the low and the high resource languages. More generally\nour approach confirms that the meaningful latent space learned in a given\nlanguage can be can be generalized to unseen and under-resourced ones using\nmeta-learning.\n", "rewritten_text": "Supervised deep learning-based approaches have been successfully applied to task-oriented dialogues, proving effective for limited domain and language applications when a sufficient number of training examples are available. However, in practice, these approaches face challenges related to domain-driven design and under-resourced languages. Domain and language models are expected to evolve and adapt as the problem space changes.\n\nResearch on transfer learning has highlighted the cross-lingual capabilities of multilingual Transformers-based models in learning semantically rich representations. Additionally, meta-learning has facilitated the development of task and language learning algorithms that exhibit enhanced generalization abilities.\n\nIn this context, this article proposes an investigation into the cross-lingual transferability by leveraging synergistic few-shot learning with prototypical neural networks and multilingual Transformers-based models. Experiments conducted on natural language understanding tasks using the MultiATIS++ corpus demonstrate that our approach significantly enhances transfer learning performance between low and high resource languages. Furthermore, our approach validates that the meaningful latent space learned in a specific language can be effectively generalized to unseen and under-resourced languages through meta-learning."}, "2203.1521": {"original_text": "  To learn camera-view invariant features for person Re-IDentification (Re-ID),\nthe cross-camera image pairs of each person play an important role. However,\nsuch cross-view training samples could be unavailable under the ISolated Camera\nSupervised (ISCS) setting, e.g., a surveillance system deployed across distant\nscenes. To handle this challenging problem, a new pipeline is introduced by\nsynthesizing the cross-camera samples in the feature space for model training.\nSpecifically, the feature encoder and generator are end-to-end optimized under\na novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint\nlearning procedure raises concern on the stability of generative model\ntraining. Therefore, a new feature generator, $\\sigma$-Regularized Conditional\nVariational Autoencoder ($\\sigma$-Reg.~CVAE), is proposed with theoretical and\nexperimental analysis on its robustness. Extensive experiments on two ISCS\nperson Re-ID datasets demonstrate the superiority of our CCSFG to the\ncompetitors.\n", "rewritten_text": "In order to learn camera-view invariant features for person re-identification (Re-ID), the cross-camera image pairs of each individual are crucial. However, obtaining such cross-view training samples may be challenging in the Isolated Camera Supervised (ISCS) setting, such as in a surveillance system deployed across distant scenes. To address this issue, a new approach is proposed that involves synthesizing cross-camera samples in the feature space for model training. This new pipeline optimizes the feature encoder and generator in an end-to-end manner using a novel method called Camera-Conditioned Stable Feature Generation (CCSFG). The joint learning process emphasizes the importance of the stability of generative model training. To ensure this stability, a new feature generator, the $\\sigma$-Regularized Conditional Variational Autoencoder ($\\sigma$-Reg.~CVAE), is introduced, supported by theoretical and experimental analyses of its robustness. Extensive experiments conducted on two ISCS person Re-ID datasets demonstrate the superior performance of CCSFG compared to other methods."}, "1812.0957": {"original_text": "  In recent years, we have seen the performance of video-based person\nRe-Identification (ReID) methods have improved considerably. However, most of\nthe work in this area has dealt with videos acquired by fixed cameras with\nwider field of view. Recently, widespread use of wearable cameras and recording\ndevices such as cellphones have opened the door to interesting research in\nfirst-person Point-of-view (POV) videos (egocentric videos). Nonetheless,\nanalysis of such videos is challenging due to factors such as poor video\nquality due to ego-motion, blurriness, severe changes in lighting conditions\nand perspective distortions. To facilitate the research towards conquering\nthese challenges, this paper contributes a new dataset called EgoReID. The\ndataset is captured using 3 mobile cellphones with non-overlapping\nfield-of-view. It contains 900 IDs and around 10,200 tracks with a total of\n176,000 detections. The dataset also contains 12-sensor meta data e.g. camera\norientation pitch and rotation for each video.\n  In addition, we propose a new framework which takes advantage of both visual\nand sensor meta data to successfully perform Person ReID. We extend image-based\nre-ID method employing human body parsing trained on ten datasets to\nvideo-based re-ID. In our method, first frame level local features are\nextracted for each semantic region, then 3D convolutions are applied to encode\nthe temporal information in each sequence of semantic regions. Additionally, we\nemploy sensor meta data to predict targets' next camera and their estimated\ntime of arrival, which considerably improves our ReID performance as it\nsignificantly reduces our search space.\n", "rewritten_text": "In recent years, there has been significant improvement in the performance of video-based person Re-Identification (ReID) methods. However, most of the research in this field has focused on videos captured by fixed cameras with a wider field of view. The increasing use of wearable cameras and recording devices, such as cellphones, has now paved the way for intriguing research in first-person Point-of-view (POV) videos, also known as egocentric videos. Analyzing such videos presents challenges due to factors like poor video quality caused by ego-motion, blurriness, drastic changes in lighting conditions, and perspective distortions.\n\nTo address these challenges and advance research in this area, this paper introduces a new dataset named EgoReID. The dataset is collected using three mobile cellphones with non-overlapping fields of view, comprising 900 IDs and approximately 10,200 tracks with a total of 176,000 detections. Additionally, the dataset includes 12-sensor metadata, such as camera orientation pitch and rotation for each video.\n\nFurthermore, a novel framework is proposed in this paper that leverages both visual data and sensor metadata to effectively conduct Person ReID. The approach extends the image-based re-ID method by incorporating human body parsing trained on ten datasets into video-based re-ID. The method involves extracting frame-level local features for each semantic region, followed by applying 3D convolutions to encode temporal information in sequences of semantic regions. Moreover, the sensor metadata is utilized to predict the targets' next camera and their estimated time of arrival, leading to a significant enhancement in ReID performance by reducing the search space considerably."}, "2112.07928": {"original_text": "  Real-world data often follows a long-tailed distribution, which makes the\nperformance of existing classification algorithms degrade heavily. A key issue\nis that samples in tail categories fail to depict their intra-class diversity.\nHumans can imagine a sample in new poses, scenes, and view angles with their\nprior knowledge even if it is the first time to see this category. Inspired by\nthis, we propose a novel reasoning-based implicit semantic data augmentation\nmethod to borrow transformation directions from other classes. Since the\ncovariance matrix of each category represents the feature transformation\ndirections, we can sample new directions from similar categories to generate\ndefinitely different instances. Specifically, the long-tailed distributed data\nis first adopted to train a backbone and a classifier. Then, a covariance\nmatrix for each category is estimated, and a knowledge graph is constructed to\nstore the relations of any two categories. Finally, tail samples are adaptively\nenhanced via propagating information from all the similar categories in the\nknowledge graph. Experimental results on CIFAR-100-LT, ImageNet-LT, and\niNaturalist 2018 have demonstrated the effectiveness of our proposed method\ncompared with the state-of-the-art methods.\n", "rewritten_text": "Real-world data often exhibits a long-tailed distribution, leading to significant degradation in the performance of existing classification algorithms. A critical issue arises from the fact that samples in the tail categories do not adequately capture their intra-class diversity. Unlike machines, humans can envision samples in new poses, scenes, and viewing angles based on prior knowledge, even when encountering a category for the first time. Drawing inspiration from this human ability, we introduce a novel reasoning-based implicit semantic data augmentation approach that leverages transformation directions borrowed from other classes.\n\nBy utilizing the covariance matrix of each category to represent feature transformation directions, we can sample new directions from similar categories to generate distinctly different instances. The process begins by training a backbone and a classifier on the long-tailed distributed data. Subsequently, a covariance matrix is estimated for each category, and a knowledge graph is constructed to capture relationships between any two categories. Tail samples are then enhanced adaptively by propagating information from all similar categories in the knowledge graph.\n\nExperimental results on CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018 validate the effectiveness of our proposed method compared to state-of-the-art approaches."}, "2208.07291": {"original_text": "  One of the possible dangers that older people face in their daily lives is\nfalling. Occlusion is one of the biggest challenges of vision-based fall\ndetection systems and degrades their detection performance considerably. To\ntackle this problem, we synthesize specifically-designed occluded videos for\ntraining fall detection systems using existing datasets. Then, by defining a\nnew cost function, we introduce a framework for weighted training of fall\ndetection models using occluded and un-occluded videos, which can be applied to\nany learnable fall detection system. Finally, we use both a non-deep and deep\nmodel to evaluate the effect of the proposed weighted training method.\nExperiments show that the proposed method can improve the classification\naccuracy by 36% for a non-deep model and 55% for a deep model in occlusion\nconditions. Moreover, it is shown that the proposed training framework can also\nsignificantly improve the detection performance of a deep network on normal\nun-occluded samples.\n", "rewritten_text": "Older individuals face the potential danger of falling in their daily lives. A significant challenge for vision-based fall detection systems is occlusion, which significantly hampers their detection capabilities. To address this issue, we have developed a method where we create specially designed occluded videos from existing datasets to train fall detection systems. By introducing a new cost function, we have established a framework for weighted training of fall detection models using both occluded and un-occluded videos, which can be implemented across various learnable fall detection systems. We then utilized both non-deep and deep models to assess the impact of this weighted training approach. Our experiments demonstrate that this method enhances classification accuracy by 36% for non-deep models and 55% for deep models under occlusion conditions. Furthermore, our results indicate that this training framework significantly boosts the detection performance of deep networks even in normal un-occluded scenarios."}, "2101.0219": {"original_text": "  Object detection and semantic segmentation are two of the most widely adopted\ndeep learning algorithms in agricultural applications. One of the major sources\nof variability in image quality acquired in the outdoors for such tasks is\nchanging lighting condition that can alter the appearance of the objects or the\ncontents of the entire image. While transfer learning and data augmentation to\nsome extent reduce the need for large amount of data to train deep neural\nnetworks, the large variety of cultivars and the lack of shared datasets in\nagriculture makes wide-scale field deployments difficult. In this paper, we\npresent a high throughput robust active lighting-based camera system that\ngenerates consistent images in all lighting conditions. We detail experiments\nthat show the consistency in images quality leading to relatively fewer images\nto train deep neural networks for the task of object detection. We further\npresent results from field experiment under extreme lighting conditions where\nimages without active lighting significantly lack to provide consistent\nresults. The experimental results show that on average, deep nets for object\ndetection trained on consistent data required nearly four times less data to\nachieve similar level of accuracy. This proposed work could potentially provide\npragmatic solutions to computer vision needs in agriculture.\n", "rewritten_text": "Object detection and semantic segmentation are two of the most widely adopted deep learning algorithms in agricultural applications. One major source of variability in image quality acquired outdoors for such tasks is changing lighting conditions, which can alter the appearance of objects or the contents of the entire image. While transfer learning and data augmentation can reduce the need for a large amount of data to train deep neural networks to some extent, the wide variety of cultivars and the lack of shared datasets in agriculture make wide-scale field deployments challenging.\n\nIn this paper, we introduce a high-throughput, robust active lighting-based camera system that produces consistent images in all lighting conditions. We present experiments demonstrating the consistency in image quality, resulting in the need for relatively fewer images to train deep neural networks for object detection tasks. Additionally, we share results from field experiments conducted under extreme lighting conditions, highlighting the lack of consistency in images without active lighting.\n\nThe experimental findings reveal that, on average, deep neural networks trained on consistent data required nearly four times less data to achieve a similar level of accuracy for object detection. This proposed approach has the potential to offer practical solutions to the computer vision requirements in agriculture."}, "1911.03828": {"original_text": "  Wasserstein autoencoders are effective for text generation. They do not\nhowever provide any control over the style and topic of the generated sentences\nif the dataset has multiple classes and includes different topics. In this\nwork, we present a semi-supervised approach for generating stylized sentences.\nOur model is trained on a multi-class dataset and learns the latent\nrepresentation of the sentences using a mixture of Gaussian prior without any\nadversarial losses. This allows us to generate sentences in the style of a\nspecified class or multiple classes by sampling from their corresponding prior\ndistributions. Moreover, we can train our model on relatively small datasets\nand learn the latent representation of a specified class by adding external\ndata with other styles/classes to our dataset. While a simple WAE or VAE cannot\ngenerate diverse sentences in this case, generated sentences with our approach\nare diverse, fluent, and preserve the style and the content of the desired\nclasses.\n", "rewritten_text": "Wasserstein autoencoders are effective for text generation, but they lack control over the style and topic of generated sentences when the dataset contains multiple classes and diverse topics. In this study, we introduce a semi-supervised method for generating stylized sentences. Our model is trained on a multi-class dataset and captures the latent representation of sentences using a mixture of Gaussian priors without employing adversarial losses. This enables us to generate sentences in the style of a specific class or multiple classes by sampling from their respective prior distributions. Additionally, our model can be trained on small datasets and learn the latent representation of a specific class by incorporating external data with different styles or classes into the dataset. While a basic Wasserstein autoencoder or Variational Autoencoder (VAE) may struggle to produce diverse sentences in such scenarios, our approach generates sentences that are diverse, fluent, and maintain the style and content of the desired classes."}, "2106.01656": {"original_text": "  Many variants of unsupervised domain adaptation (UDA) problems have been\nproposed and solved individually. Its side effect is that a method that works\nfor one variant is often ineffective for or not even applicable to another,\nwhich has prevented practical applications. In this paper, we give a general\nrepresentation of UDA problems, named Generalized Domain Adaptation (GDA). GDA\ncovers the major variants as special cases, which allows us to organize them in\na comprehensive framework. Moreover, this generalization leads to a new\nchallenging setting where existing methods fail, such as when domain labels are\nunknown, and class labels are only partially given to each domain. We propose a\nnovel approach to the new setting. The key to our approach is self-supervised\nclass-destructive learning, which enables the learning of class-invariant\nrepresentations and domain-adversarial classifiers without using any domain\nlabels. Extensive experiments using three benchmark datasets demonstrate that\nour method outperforms the state-of-the-art UDA methods in the new setting and\nthat it is competitive in existing UDA variations as well.\n", "rewritten_text": "Numerous variations of unsupervised domain adaptation (UDA) problems have been proposed and individually addressed. However, a significant challenge arises as methods effective for one variant often prove ineffective or inapplicable to another, hindering practical applications. In this paper, we introduce a comprehensive approach to UDA problems, termed Generalized Domain Adaptation (GDA). GDA encompasses the primary variants as specific instances, facilitating their organization within a unified framework. This generalization introduces a novel and challenging scenario where conventional methods falter, particularly in cases where domain labels are unknown, and class labels are only partially available for each domain. To address this, we present a novel methodology tailored to this new scenario. Central to our approach is self-supervised class-destructive learning, enabling the acquisition of class-invariant representations and domain-adversarial classifiers without reliance on domain labels. Extensive experiments conducted on three benchmark datasets demonstrate that our approach surpasses state-of-the-art UDA methods in the new scenario and remains competitive across existing UDA variations."}, "2305.07152": {"original_text": "  The ability to automatically detect and track surgical instruments in\nendoscopic videos can enable transformational interventions. Assessing surgical\nperformance and efficiency, identifying skilled tool use and choreography, and\nplanning operational and logistical aspects of OR resources are just a few of\nthe applications that could benefit. Unfortunately, obtaining the annotations\nneeded to train machine learning models to identify and localize surgical tools\nis a difficult task. Annotating bounding boxes frame-by-frame is tedious and\ntime-consuming, yet large amounts of data with a wide variety of surgical tools\nand surgeries must be captured for robust training. Moreover, ongoing annotator\ntraining is needed to stay up to date with surgical instrument innovation. In\nrobotic-assisted surgery, however, potentially informative data like timestamps\nof instrument installation and removal can be programmatically harvested. The\nability to rely on tool installation data alone would significantly reduce the\nworkload to train robust tool-tracking models. With this motivation in mind we\ninvited the surgical data science community to participate in the challenge,\nSurgToolLoc 2022. The goal was to leverage tool presence data as weak labels\nfor machine learning models trained to detect tools and localize them in video\nframes with bounding boxes. We present the results of this challenge along with\nmany of the team's efforts. We conclude by discussing these results in the\nbroader context of machine learning and surgical data science. The training\ndata used for this challenge consisting of 24,695 video clips with tool\npresence labels is also being released publicly and can be accessed at\nhttps://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022.\n", "rewritten_text": "The automatic detection and tracking of surgical instruments in endoscopic videos have the potential to revolutionize interventions. This technology can facilitate the assessment of surgical performance and efficiency, identification of skilled tool use and choreography, and planning of operational and logistical aspects of operating room (OR) resources. However, obtaining the necessary annotations to train machine learning models for identifying and localizing surgical tools is a challenging task. Annotating bounding boxes frame-by-frame is laborious and time-consuming, yet it is essential to capture large amounts of data encompassing a wide variety of surgical tools and procedures for effective training.\n\nFurthermore, continuous training of annotators is required to keep pace with the ongoing innovation in surgical instruments. In the realm of robotic-assisted surgery, valuable data such as timestamps of instrument installation and removal can be automatically collected. Leveraging this tool installation data alone could significantly reduce the workload involved in training robust tool-tracking models.\n\nMotivated by these challenges and opportunities, we launched the SurgToolLoc 2022 challenge to engage the surgical data science community. The objective was to utilize tool presence data as weak labels for training machine learning models to detect and localize tools in video frames using bounding boxes. In this report, we present the outcomes of the challenge and highlight the collaborative efforts of the participating teams. We conclude by contextualizing these results within the broader landscape of machine learning and surgical data science.\n\nThe training data utilized for this challenge, comprising 24,695 video clips with tool presence labels, is now publicly available and can be accessed at https://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022."}, "2403.01858": {"original_text": "  We present TMMLU+, a new benchmark designed for Traditional Chinese language\nunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66\nsubjects from elementary to professional level. It is six times larger and\nboasts a more balanced subject distribution than its predecessor, Taiwan\nMassive Multitask Language Understanding (TMMLU). We also benchmark\nclosed-source models and 26 open-weight Chinese large language models (LLMs) of\nparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings reveal\nthat (1.) Traditional Chinese models still trail behind their Simplified\nChinese counterparts, highlighting a need for more focused advancements in LLMs\ncatering to Traditional Chinese. (2.) Current LLMs still fall short of human\nperformance in average scores, indicating a potential need for future research\nto delve deeper into social science and humanities subjects. (3.) Among all the\ntokenization compression metrics examined, we identify that only the fertility\nscore uniquely demonstrates strong correlations with our benchmark results. We\nforesee that TMMLU+ will pinpoint areas for future model improvement, thereby\nnarrowing the gap between machine and human linguistic capabilities and\nsupporting researchers in developing Traditional Chinese LLMs. Our dataset,\nalong with the benchmark source code, is accessible at\nhuggingface.co/datasets/ikala/tmmluplus.\n", "rewritten_text": "Introducing TMMLU+, a new benchmark specifically tailored for Traditional Chinese language comprehension. This multi-choice question-answering dataset comprises 66 subjects spanning from elementary to professional levels. TMMLU+ is six times larger than its predecessor, Taiwan Massive Multitask Language Understanding (TMMLU), and features a more evenly distributed range of subjects.\n\nIn our study, we evaluated closed-source models and 26 open-weight Chinese large language models (LLMs) with parameter sizes ranging from 1.8B to 72B using the TMMLU+ dataset. Our analysis uncovered several key insights: \n\n1. Traditional Chinese models continue to lag behind their Simplified Chinese counterparts, underscoring the necessity for targeted advancements in LLMs tailored to Traditional Chinese.\n2. Current LLMs still fall short of achieving human-level performance in average scores, suggesting a need for further exploration in social science and humanities subjects.\n3. Among the various tokenization compression metrics examined, only the fertility score exhibited a strong correlation with our benchmark results.\n\nWe anticipate that TMMLU+ will serve as a valuable tool for identifying areas for enhancing model performance, bridging the divide between machine and human linguistic capabilities, and aiding researchers in the development of Traditional Chinese LLMs. The dataset, along with the benchmark source code, is accessible at huggingface.co/datasets/ikala/tmmluplus."}, "2005.04621": {"original_text": "  Deep convolutional neural networks generally perform well in underwater\nobject recognition tasks on both optical and sonar images. Many such methods\nrequire hundreds, if not thousands, of images per class to generalize well to\nunseen examples. However, obtaining and labeling sufficiently large volumes of\ndata can be relatively costly and time-consuming, especially when observing\nrare objects or performing real-time operations. Few-Shot Learning (FSL)\nefforts have produced many promising methods to deal with low data\navailability. However, little attention has been given in the underwater\ndomain, where the style of images poses additional challenges for object\nrecognition algorithms. To the best of our knowledge, this is the first paper\nto evaluate and compare several supervised and semi-supervised Few-Shot\nLearning (FSL) methods using underwater optical and side-scan sonar imagery.\nOur results show that FSL methods offer a significant advantage over the\ntraditional transfer learning methods that fine-tune pre-trained models. We\nhope that our work will help apply FSL to autonomous underwater systems and\nexpand their learning capabilities.\n", "rewritten_text": "Deep convolutional neural networks typically excel in underwater object recognition tasks using both optical and sonar images. Many of these methods necessitate hundreds, if not thousands, of images per class to effectively generalize to unseen examples. However, acquiring and labeling large volumes of data can be quite costly and time-consuming, particularly when dealing with rare objects or real-time operations. Few-Shot Learning (FSL) initiatives have yielded numerous promising approaches to address limited data availability. Nonetheless, little attention has been devoted to the underwater domain, where the image style presents additional challenges for object recognition algorithms. To the best of our knowledge, this paper represents the first attempt to assess and compare several supervised and semi-supervised Few-Shot Learning (FSL) techniques using underwater optical and side-scan sonar imagery. Our findings demonstrate that FSL methods offer a significant edge over traditional transfer learning methods that fine-tune pre-trained models. We anticipate that our research will facilitate the application of FSL in autonomous underwater systems and enhance their learning capabilities."}, "1712.01821": {"original_text": "  Factored neural machine translation (FNMT) is founded on the idea of using\nthe morphological and grammatical decomposition of the words (factors) at the\noutput side of the neural network. This architecture addresses two well-known\nproblems occurring in MT, namely the size of target language vocabulary and the\nnumber of unknown tokens produced in the translation. FNMT system is designed\nto manage larger vocabulary and reduce the training time (for systems with\nequivalent target language vocabulary size). Moreover, we can produce\ngrammatically correct words that are not part of the vocabulary. FNMT model is\nevaluated on IWSLT'15 English to French task and compared to the baseline\nword-based and BPE-based NMT systems. Promising qualitative and quantitative\nresults (in terms of BLEU and METEOR) are reported.\n", "rewritten_text": "Factored Neural Machine Translation (FNMT) is based on the concept of utilizing the morphological and grammatical breakdown of words (factors) on the output side of the neural network. This architecture tackles two common issues in machine translation (MT): the size of the target language vocabulary and the generation of unknown tokens during translation. The FNMT system is specifically designed to handle larger vocabularies and reduce training time (compared to systems with similar target language vocabulary sizes). Additionally, it enables the generation of grammatically correct words that may not be part of the vocabulary.\n\nThe FNMT model was assessed in the context of the IWSLT'15 English to French task and was compared against baseline word-based and BPE-based NMT systems. The evaluation yielded promising qualitative and quantitative results, as indicated by metrics such as BLEU and METEOR."}, "2206.01988": {"original_text": "  Automatic generation of ophthalmic reports using data-driven neural networks\nhas great potential in clinical practice. When writing a report,\nophthalmologists make inferences with prior clinical knowledge. This knowledge\nhas been neglected in prior medical report generation methods. To endow models\nwith the capability of incorporating expert knowledge, we propose a Cross-modal\nclinical Graph Transformer (CGT) for ophthalmic report generation (ORG), in\nwhich clinical relation triples are injected into the visual features as prior\nknowledge to drive the decoding procedure. However, two major common Knowledge\nNoise (KN) issues may affect models' effectiveness. 1) Existing general\nbiomedical knowledge bases such as the UMLS may not align meaningfully to the\nspecific context and language of the report, limiting their utility for\nknowledge injection. 2) Incorporating too much knowledge may divert the visual\nfeatures from their correct meaning. To overcome these limitations, we design\nan automatic information extraction scheme based on natural language processing\nto obtain clinical entities and relations directly from in-domain training\nreports. Given a set of ophthalmic images, our CGT first restores a sub-graph\nfrom the clinical graph and injects the restored triples into visual features.\nThen visible matrix is employed during the encoding procedure to limit the\nimpact of knowledge. Finally, reports are predicted by the encoded cross-modal\nfeatures via a Transformer decoder. Extensive experiments on the large-scale\nFFA-IR benchmark demonstrate that the proposed CGT is able to outperform\nprevious benchmark methods and achieve state-of-the-art performances.\n", "rewritten_text": "The automatic generation of ophthalmic reports using data-driven neural networks holds significant potential in clinical practice. When composing a report, ophthalmologists rely on prior clinical knowledge to make inferences. Unfortunately, this valuable knowledge has often been overlooked in previous medical report generation methods. To address this gap and enable models to incorporate expert knowledge, we introduce a Cross-modal Clinical Graph Transformer (CGT) for ophthalmic report generation (ORG). In this framework, clinical relation triples are integrated into visual features as prior knowledge to guide the decoding process.\n\nHowever, two common Knowledge Noise (KN) issues can impact the effectiveness of models. Firstly, existing general biomedical knowledge bases like the UMLS may not align well with the specific context and language of the report, limiting their usefulness for knowledge injection. Secondly, incorporating excessive knowledge may lead to a distortion of the intended meaning of visual features. To overcome these challenges, we develop an automatic information extraction scheme based on natural language processing to extract clinical entities and relations directly from training reports within the domain.\n\nGiven a set of ophthalmic images, our CGT first reconstructs a sub-graph from the clinical graph and integrates the restored triples into visual features. Subsequently, a visibility matrix is utilized during the encoding process to mitigate the impact of knowledge. Finally, reports are generated based on the encoded cross-modal features using a Transformer decoder. Extensive experiments conducted on the large-scale FFA-IR benchmark demonstrate that the proposed CGT surpasses previous benchmark methods and achieves state-of-the-art performance."}, "1602.01125": {"original_text": "  We propose a fully automatic method for fitting a 3D morphable model to\nsingle face images in arbitrary pose and lighting. Our approach relies on\ngeometric features (edges and landmarks) and, inspired by the iterated closest\npoint algorithm, is based on computing hard correspondences between model\nvertices and edge pixels. We demonstrate that this is superior to previous work\nthat uses soft correspondences to form an edge-derived cost surface that is\nminimised by nonlinear optimisation.\n", "rewritten_text": "We present a fully automatic method for fitting a 3D morphable model to single face images in any pose and lighting conditions. Our approach utilizes geometric features such as edges and landmarks. Inspired by the iterated closest point algorithm, our method involves establishing precise correspondences between model vertices and edge pixels. We show that this approach outperforms previous methods that rely on soft correspondences to create an edge-based cost surface, which is then minimized through nonlinear optimization techniques."}, "2405.11862": {"original_text": "  Table structure recognition (TSR) aims to parse the inherent structure of a\ntable from its input image. The `\"split-and-merge\" paradigm is a pivotal\napproach to parse table structure, where the table separation line detection is\ncrucial. However, challenges such as wireless and deformed tables make it\ndemanding. In this paper, we adhere to the \"split-and-merge\" paradigm and\npropose SEMv3 (SEM: Split, Embed and Merge), a method that is both fast and\nrobust for detecting table separation lines. During the split stage, we\nintroduce a Keypoint Offset Regression (KOR) module, which effectively detects\ntable separation lines by directly regressing the offset of each line relative\nto its keypoint proposals. Moreover, in the merge stage, we define a series of\nmerge actions to efficiently describe the table structure based on table grids.\nExtensive ablation studies demonstrate that our proposed KOR module can detect\ntable separation lines quickly and accurately. Furthermore, on public datasets\n(e.g. WTW, ICDAR-2019 cTDaR Historical and iFLYTAB), SEMv3 achieves\nstate-of-the-art (SOTA) performance. The code is available at\nhttps://github.com/Chunchunwumu/SEMv3.\n", "rewritten_text": "Table Structure Recognition (TSR) is the process of parsing the inherent structure of a table from its input image. The \"split-and-merge\" paradigm is a key approach used to analyze table structure, with a particular emphasis on detecting table separation lines. However, challenges arise, such as dealing with wireless and deformed tables, which can make this process demanding. In this study, we adhere to the \"split-and-merge\" paradigm and introduce SEMv3 (SEM: Split, Embed, and Merge), a method designed to be both fast and robust in detecting table separation lines.\n\nDuring the split stage, we incorporate a Keypoint Offset Regression (KOR) module, which effectively identifies table separation lines by directly calculating the offset of each line in relation to its keypoint proposals. Additionally, in the merge stage, we outline a series of merge actions to efficiently represent the table structure based on table grids. Extensive ablation studies confirm that our proposed KOR module can rapidly and accurately detect table separation lines.\n\nFurthermore, on various public datasets such as WTW, ICDAR-2019 cTDaR Historical, and iFLYTAB, SEMv3 demonstrates state-of-the-art (SOTA) performance. For those interested, the code for SEMv3 is accessible at https://github.com/Chunchunwumu/SEMv3."}, "1505.04424": {"original_text": "  In this work, we propose a novel microaneurysm (MA) detection for early\ndiabetic retinopathy screening using color fundus images. Since MA usually the\nfirst lesions to appear as an indicator of diabetic retinopathy, accurate\ndetection of MA is necessary for treatment. Each pixel of the image is\nclassified as either MA or non-MA using a deep neural network with dropout\ntraining procedure using maxout activation function. No preprocessing step or\nmanual feature extraction is required. Substantial improvements over standard\nMA detection method based on the pipeline of preprocessing, feature extraction,\nclassification followed by post processing is achieved. The presented method is\nevaluated in publicly available Retinopathy Online Challenge (ROC) and\nDiaretdb1v2 database and achieved state-of-the-art accuracy.\n", "rewritten_text": "In this study, we introduce a new method for detecting microaneurysms (MAs) in early diabetic retinopathy screening using color fundus images. MAs are typically the first signs of diabetic retinopathy, making their accurate detection crucial for effective treatment. Our approach involves classifying each pixel in the image as either an MA or non-MA using a deep neural network with a dropout training procedure and a maxout activation function. This method eliminates the need for preprocessing steps or manual feature extraction. We have demonstrated significant improvements over the standard MA detection method, which typically involves a pipeline of preprocessing, feature extraction, classification, and post-processing. Our method has been evaluated using the publicly available Retinopathy Online Challenge (ROC) and Diaretdb1v2 databases, achieving state-of-the-art accuracy."}, "2206.13156": {"original_text": "  Transformer has been widely used in histopathology whole slide image (WSI)\nclassification for the purpose of tumor grading, prognosis analysis, etc.\nHowever, the design of token-wise self-attention and positional embedding\nstrategy in the common Transformer limits the effectiveness and efficiency in\nthe application to gigapixel histopathology images. In this paper, we propose a\nkernel attention Transformer (KAT) for histopathology WSI classification. The\ninformation transmission of the tokens is achieved by cross-attention between\nthe tokens and a set of kernels related to a set of positional anchors on the\nWSI. Compared to the common Transformer structure, the proposed KAT can better\ndescribe the hierarchical context information of the local regions of the WSI\nand meanwhile maintains a lower computational complexity. The proposed method\nwas evaluated on a gastric dataset with 2040 WSIs and an endometrial dataset\nwith 2560 WSIs, and was compared with 6 state-of-the-art methods. The\nexperimental results have demonstrated the proposed KAT is effective and\nefficient in the task of histopathology WSI classification and is superior to\nthe state-of-the-art methods. The code is available at\nhttps://github.com/zhengyushan/kat.\n", "rewritten_text": "The Transformer has been widely utilized in the classification of histopathology whole slide images (WSI) for tasks such as tumor grading and prognosis analysis. However, the design of token-wise self-attention and positional embedding in the conventional Transformer framework hinders its effectiveness and efficiency when applied to gigapixel histopathology images. In this study, we introduce a novel approach called Kernel Attention Transformer (KAT) for histopathology WSI classification. The KAT facilitates information transmission among tokens through cross-attention with a set of kernels associated with positional anchors on the WSI. Compared to the standard Transformer architecture, the proposed KAT offers improved representation of hierarchical context information within local regions of the WSI while maintaining lower computational complexity. \n\nWe evaluated the proposed method on two datasets: a gastric dataset comprising 2040 WSIs and an endometrial dataset comprising 2560 WSIs. Comparative analysis with six state-of-the-art methods revealed that the KAT outperforms existing approaches in terms of effectiveness and efficiency for histopathology WSI classification. The experimental results underscore the efficacy of the KAT model. The code for the proposed method can be accessed at https://github.com/zhengyushan/kat."}, "1610.00291": {"original_text": "  We present a novel method for constructing Variational Autoencoder (VAE).\nInstead of using pixel-by-pixel loss, we enforce deep feature consistency\nbetween the input and the output of a VAE, which ensures the VAE's output to\npreserve the spatial correlation characteristics of the input, thus leading the\noutput to have a more natural visual appearance and better perceptual quality.\nBased on recent deep learning works such as style transfer, we employ a\npre-trained deep convolutional neural network (CNN) and use its hidden features\nto define a feature perceptual loss for VAE training. Evaluated on the CelebA\nface dataset, we show that our model produces better results than other methods\nin the literature. We also show that our method can produce latent vectors that\ncan capture the semantic information of face expressions and can be used to\nachieve state-of-the-art performance in facial attribute prediction.\n", "rewritten_text": "We introduce a novel approach for constructing a Variational Autoencoder (VAE). Instead of relying on pixel-by-pixel loss, our method emphasizes deep feature consistency between the input and output of the VAE. This enforcement ensures that the VAE's output maintains the spatial correlation characteristics of the input, resulting in a more natural visual appearance and enhanced perceptual quality. Drawing inspiration from recent advancements in deep learning, such as style transfer, we leverage a pre-trained deep convolutional neural network (CNN) and its hidden features to establish a feature perceptual loss for VAE training. Through evaluation on the CelebA face dataset, we demonstrate that our model outperforms existing methods in the field. Furthermore, we illustrate that our technique generates latent vectors capable of capturing the semantic information of facial expressions, leading to state-of-the-art performance in facial attribute prediction."}, "2206.00227": {"original_text": "  A data augmentation module is utilized in contrastive learning to transform\nthe given data example into two views, which is considered essential and\nirreplaceable. However, the predetermined composition of multiple data\naugmentations brings two drawbacks. First, the artificial choice of\naugmentation types brings specific representational invariances to the model,\nwhich have different degrees of positive and negative effects on different\ndownstream tasks. Treating each type of augmentation equally during training\nmakes the model learn non-optimal representations for various downstream tasks\nand limits the flexibility to choose augmentation types beforehand. Second, the\nstrong data augmentations used in classic contrastive learning methods may\nbring too much invariance in some cases, and fine-grained information that is\nessential to some downstream tasks may be lost. This paper proposes a general\nmethod to alleviate these two problems by considering where and what to\ncontrast in a general contrastive learning framework. We first propose to learn\ndifferent augmentation invariances at different depths of the model according\nto the importance of each data augmentation instead of learning\nrepresentational invariances evenly in the backbone. We then propose to expand\nthe contrast content with augmentation embeddings to reduce the misleading\neffects of strong data augmentations. Experiments based on several baseline\nmethods demonstrate that we learn better representations for various benchmarks\non classification, detection, and segmentation downstream tasks.\n", "rewritten_text": "In contrastive learning, a data augmentation module is used to transform a given data example into two views, which is considered essential and irreplaceable. However, employing a predetermined composition of multiple data augmentations presents two drawbacks. Firstly, the artificial selection of augmentation types introduces specific representational invariances to the model, resulting in varying degrees of positive and negative effects on different downstream tasks. Treating each augmentation type equally during training leads the model to learn suboptimal representations for diverse downstream tasks and restricts the ability to preselect augmentation types. Secondly, the robust data augmentations utilized in traditional contrastive learning methods may introduce excessive invariance in certain cases, potentially causing the loss of fine-grained information crucial for specific downstream tasks.\n\nThis paper introduces a general method to address these issues by considering where and what to contrast within a general contrastive learning framework. Initially, we suggest learning different augmentation invariances at various depths of the model based on the importance of each data augmentation, rather than uniformly learning representational invariances in the backbone. Subsequently, we propose expanding the contrast content with augmentation embeddings to mitigate the distorting effects of strong data augmentations. Experimental results, utilizing several baseline methods, demonstrate that our approach leads to improved representations for various benchmarks in classification, detection, and segmentation downstream tasks."}, "1711.0719": {"original_text": "  We present a stochastic first-order optimization algorithm, named BCSC, that\nadds a cyclic constraint to stochastic block-coordinate descent. It uses\ndifferent subsets of the data to update different subsets of the parameters,\nthus limiting the detrimental effect of outliers in the training set. Empirical\ntests in benchmark datasets show that our algorithm outperforms\nstate-of-the-art optimization methods in both accuracy as well as convergence\nspeed. The improvements are consistent across different architectures, and can\nbe combined with other training techniques and regularization methods.\n", "rewritten_text": "We introduce BCSC, a stochastic first-order optimization algorithm that incorporates a cyclic constraint into stochastic block-coordinate descent. By updating various subsets of parameters with different subsets of data, BCSC mitigates the impact of outliers in the training set. Empirical tests on benchmark datasets demonstrate that our algorithm surpasses state-of-the-art optimization methods in terms of accuracy and convergence speed. These enhancements are consistently observed across various architectures and can be integrated with other training techniques and regularization methods."}, "2008.11872": {"original_text": "  In Viticulture, visual inspection of the plant is a necessary task for\nmeasuring relevant variables. In many cases, these visual inspections are\nsusceptible to automation through computer vision methods. Bud detection is one\nsuch visual task, central for the measurement of important variables such as:\nmeasurement of bud sunlight exposure, autonomous pruning, bud counting,\ntype-of-bud classification, bud geometric characterization, internode length,\nbud area, and bud development stage, among others. This paper presents a\ncomputer method for grapevine bud detection based on a Fully Convolutional\nNetworks MobileNet architecture (FCN-MN). To validate its performance, this\narchitecture was compared in the detection task with a strong method for bud\ndetection, Scanning Windows (SW) based on a patch classifier, showing\nimprovements over three aspects of detection: segmentation, correspondence\nidentification and localization. The best version of FCN-MN showed a detection\nF1-measure of $88.6\\%$ (for true positives defined as detected components whose\nintersection-over-union with the true bud is above $0.5$), and false positives\nthat are small and near the true bud. Splits -- false positives overlapping the\ntrue bud -- showed a mean segmentation precision of $89.3\\% (21.7)$, while\nfalse alarms -- false positives not overlapping the true bud -- showed a mean\npixel area of only $8\\%$ the area of a true bud, and a distance (between mass\ncenters) of $1.1$ true bud diameters. The paper concludes by discussing how\nthese results for FCN-MN would produce sufficiently accurate measurements of\nbud variables such as bud number, bud area, and internode length, suggesting a\ngood performance in a practical setup.\n", "rewritten_text": "In viticulture, visually inspecting plants is essential for measuring relevant variables. Often, these visual inspections can be automated using computer vision methods. Bud detection is a crucial visual task that plays a central role in measuring important variables such as bud sunlight exposure, autonomous pruning, bud counting, bud classification, bud geometric characteristics, internode length, bud area, and bud development stage, among others. This paper introduces a computer method for detecting grapevine buds based on a Fully Convolutional Networks MobileNet architecture (FCN-MN). To assess its performance, this architecture was compared to a robust bud detection method, Scanning Windows (SW), which is based on a patch classifier. The comparison revealed improvements in three key aspects of detection: segmentation, correspondence identification, and localization. The optimal version of FCN-MN achieved a detection F1-measure of 88.6% (considering true positives as detected components with an intersection-over-union above 0.5) and exhibited small false positives that were in close proximity to the true bud. Specifically, splits (false positives overlapping the true bud) demonstrated a mean segmentation precision of 89.3% (21.7), while false alarms (false positives not overlapping the true bud) had a mean pixel area only 8% of the true bud's area and were located at a distance of 1.1 true bud diameters from the mass centers. The paper concludes by discussing how these results indicate that FCN-MN can provide sufficiently accurate measurements of bud variables such as bud number, bud area, and internode length, suggesting strong performance in practical applications."}, "1703.10798": {"original_text": "  We present a system for converting a fully panoramic ($360^\\circ$) video into\na normal field-of-view (NFOV) hyperlapse for an optimal viewing experience. Our\nsystem exploits visual saliency and semantics to non-uniformly sample in space\nand time for generating hyperlapses. In addition, users can optionally choose\nobjects of interest for customizing the hyperlapses. We first stabilize an\ninput $360^\\circ$ video by smoothing the rotation between adjacent frames and\nthen compute regions of interest and saliency scores. An initial hyperlapse is\ngenerated by optimizing the saliency and motion smoothness followed by the\nsaliency-aware frame selection. We further smooth the result using an efficient\n2D video stabilization approach that adaptively selects the motion model to\ngenerate the final hyperlapse. We validate the design of our system by showing\nresults for a variety of scenes and comparing against the state-of-the-art\nmethod through a user study.\n", "rewritten_text": "We introduce a system designed to transform a fully panoramic ($360^\\circ$) video into a normal field-of-view (NFOV) hyperlapse to enhance the viewing experience. Our system leverages visual saliency and semantics to selectively sample in space and time for hyperlapse creation. Additionally, users have the option to select specific objects of interest to personalize the hyperlapses. \n\nThe process begins with stabilizing the input $360^\\circ$ video by smoothing the rotation between consecutive frames, followed by the computation of regions of interest and saliency scores. An initial hyperlapse is then produced by optimizing saliency and motion smoothness, and subsequently refining the frame selection based on saliency. The final hyperlapse is achieved by applying an efficient 2D video stabilization technique that dynamically selects the motion model. \n\nTo validate our system's effectiveness, we showcase results across various scenes and conduct a user study to compare our approach with the current state-of-the-art method."}, "2001.03182": {"original_text": "  Unsupervised domain adaptation algorithms aim to transfer the knowledge\nlearned from one domain to another (e.g., synthetic to real images). The\nadapted representations often do not capture pixel-level domain shifts that are\ncrucial for dense prediction tasks (e.g., semantic segmentation). In this\npaper, we present a novel pixel-wise adversarial domain adaptation algorithm.\nBy leveraging image-to-image translation methods for data augmentation, our key\ninsight is that while the translated images between domains may differ in\nstyles, their predictions for the task should be consistent. We exploit this\nproperty and introduce a cross-domain consistency loss that enforces our\nadapted model to produce consistent predictions. Through extensive experimental\nresults, we show that our method compares favorably against the\nstate-of-the-art on a wide variety of unsupervised domain adaptation tasks.\n", "rewritten_text": "Unsupervised domain adaptation algorithms are designed to transfer knowledge acquired from one domain to another, such as from synthetic to real images. However, the adapted representations often fail to capture pixel-level domain shifts that are essential for dense prediction tasks like semantic segmentation. This paper introduces a novel pixel-wise adversarial domain adaptation algorithm. Our approach leverages image-to-image translation techniques for data augmentation, with a key insight that despite style differences in translated images between domains, their task predictions should remain consistent. Exploiting this insight, we propose a cross-domain consistency loss that compels our adapted model to generate consistent predictions. Extensive experimental results demonstrate that our method outperforms the current state-of-the-art in a wide range of unsupervised domain adaptation tasks."}, "1802.08636": {"original_text": "  Single document summarization is the task of producing a shorter version of a\ndocument while preserving its principal information content. In this paper we\nconceptualize extractive summarization as a sentence ranking task and propose a\nnovel training algorithm which globally optimizes the ROUGE evaluation metric\nthrough a reinforcement learning objective. We use our algorithm to train a\nneural summarization model on the CNN and DailyMail datasets and demonstrate\nexperimentally that it outperforms state-of-the-art extractive and abstractive\nsystems when evaluated automatically and by humans.\n", "rewritten_text": "The task of single document summarization involves creating a condensed version of a document that retains its main information. In this study, we view extractive summarization as a sentence ranking task and introduce a new training algorithm that optimizes the ROUGE evaluation metric globally using a reinforcement learning objective. We apply this algorithm to train a neural summarization model on the CNN and DailyMail datasets. Our experimental results show that our model surpasses existing extractive and abstractive systems in both automatic and human evaluations."}, "2406.11192": {"original_text": "  Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets faces issues due to inconsistent entity definitions and\nredundant data, limiting LLMs to dataset-specific learning and hindering\nout-of-domain generalization. To address this, we present B2NERD, a cohesive\nand efficient dataset for Open NER, normalized from 54 existing English or\nChinese datasets using a two-step approach. First, we detect inconsistent\nentity definitions across datasets and clarify them by distinguishable label\nnames to construct a universal taxonomy of 400+ entity types. Second, we\naddress redundancy using a data pruning strategy that selects fewer samples\nwith greater category and semantic diversity. Comprehensive evaluation shows\nthat B2NERD significantly improves LLMs' generalization on Open NER. Our B2NER\nmodels, trained on B2NERD, outperform GPT-4 by 6.8-12.0 F1 points and surpass\nprevious methods in 3 out-of-domain benchmarks across 15 datasets and 6\nlanguages.\n", "rewritten_text": "Named Entity Recognition (NER) is a challenging task for Large Language Models (LLMs) as it involves identifying various types of entities across different domains. Recent studies have shown that fine-tuning LLMs on extensive NER data can enhance their performance. However, training directly on existing datasets presents challenges due to inconsistent entity definitions and redundant data. This limits the ability of LLMs to generalize beyond specific datasets.\n\nTo tackle these issues, we introduce B2NERD, a comprehensive dataset for Open NER. B2NERD is created by normalizing data from 54 English and Chinese datasets through a two-step process. Firstly, we identify and resolve inconsistent entity definitions by assigning distinct label names, resulting in a universal taxonomy of over 400 entity types. Secondly, we employ a data pruning strategy to reduce redundancy by selecting a smaller set of samples with greater category and semantic diversity.\n\nExtensive evaluation demonstrates that B2NERD significantly enhances the generalization capabilities of LLMs in Open NER tasks. Our B2NER models, trained on the B2NERD dataset, outperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in three out-of-domain benchmarks spanning 15 datasets and six languages."}, "2411.01465": {"original_text": "  Despite the outstanding performance in many individual tasks, deep neural\nnetworks suffer from catastrophic forgetting when learning from continuous data\nstreams in real-world scenarios. Current Non-Exemplar Class-Incremental\nLearning (NECIL) methods mitigate forgetting by storing a single prototype per\nclass, which serves to inject previous information when sequentially learning\nnew classes. However, these stored prototypes or their augmented variants often\nfail to simultaneously capture spatial distribution diversity and precision\nneeded for representing old classes. Moreover, as the model acquires new\nknowledge, these prototypes gradually become outdated, making them less\neffective. To overcome these limitations, we propose a more efficient NECIL\nmethod that replaces prototypes with synthesized retrospective features for old\nclasses. Specifically, we model each old class's feature space using a\nmultivariate Gaussian distribution and generate deep representations by\nsampling from high-likelihood regions. Additionally, we introduce a\nsimilarity-based feature compensation mechanism that integrates generated old\nclass features with similar new class features to synthesize robust\nretrospective representations. These retrospective features are then\nincorporated into our incremental learning framework to preserve the decision\nboundaries of previous classes while learning new ones. Extensive experiments\non CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate that our method\nsignificantly improves the efficiency of non-exemplar class-incremental\nlearning and achieves state-of-the-art performance.\n", "rewritten_text": "Despite excelling in many individual tasks, deep neural networks face the challenge of catastrophic forgetting when learning from continuous data streams in real-world scenarios. Current methods for Non-Exemplar Class-Incremental Learning (NECIL) aim to address this issue by storing a single prototype per class. These prototypes help retain previous information when new classes are sequentially learned. However, the existing prototypes or their variations often struggle to capture both the spatial distribution diversity and precision required to represent old classes effectively. Furthermore, as the model acquires new knowledge, these prototypes gradually become outdated, diminishing their effectiveness.\n\nTo address these limitations, we propose a more efficient NECIL method that replaces prototypes with synthesized retrospective features for old classes. Specifically, we model the feature space of each old class using a multivariate Gaussian distribution and generate deep representations by sampling from regions with high likelihood. Additionally, we introduce a feature compensation mechanism based on similarity. This mechanism integrates generated features from old classes with similar features from new classes to create robust retrospective representations. These retrospective features are then integrated into our incremental learning framework to maintain the decision boundaries of previous classes while learning new ones.\n\nExtensive experiments conducted on CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate that our method significantly enhances the efficiency of non-exemplar class-incremental learning and achieves state-of-the-art performance."}, "2404.03654": {"original_text": "  NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel\nview synthesis and 3D reconstruction, but its performance is sensitive to input\nimage quality, which struggles to achieve high-fidelity rendering when provided\nwith low-quality sparse input viewpoints. Previous methods for NeRF restoration\nare tailored for specific degradation type, ignoring the generality of\nrestoration. To overcome this limitation, we propose a generic radiance fields\nrestoration pipeline, named RaFE, which applies to various types of\ndegradations, such as low resolution, blurriness, noise, compression artifacts,\nor their combinations. Our approach leverages the success of off-the-shelf 2D\nrestoration methods to recover the multi-view images individually. Instead of\nreconstructing a blurred NeRF by averaging inconsistencies, we introduce a\nnovel approach using Generative Adversarial Networks (GANs) for NeRF generation\nto better accommodate the geometric and appearance inconsistencies present in\nthe multi-view images. Specifically, we adopt a two-level tri-plane\narchitecture, where the coarse level remains fixed to represent the low-quality\nNeRF, and a fine-level residual tri-plane to be added to the coarse level is\nmodeled as a distribution with GAN to capture potential variations in\nrestoration. We validate RaFE on both synthetic and real cases for various\nrestoration tasks, demonstrating superior performance in both quantitative and\nqualitative evaluations, surpassing other 3D restoration methods specific to\nsingle task. Please see our project website\nhttps://zkaiwu.github.io/RaFE-Project/.\n", "rewritten_text": "Neural Radiance Fields (NeRF) have shown significant promise in novel view synthesis and 3D reconstruction. However, their performance is highly dependent on the quality of the input images, struggling to achieve high-fidelity rendering when given low-quality sparse viewpoints. Previous methods for restoring NeRF have been designed for specific types of degradation, neglecting the need for a more general restoration approach.\n\nTo address this limitation, we introduce a versatile radiance fields restoration pipeline called RaFE. This pipeline is capable of handling various types of degradations, including low resolution, blurriness, noise, compression artifacts, and their combinations. Our strategy leverages existing 2D restoration techniques to individually recover the multi-view images. Instead of averaging inconsistencies to reconstruct a blurred NeRF, we propose a novel method using Generative Adversarial Networks (GANs) for NeRF generation. This approach better accommodates the geometric and appearance inconsistencies present in the multi-view images.\n\nSpecifically, we employ a two-level tri-plane architecture in RaFE. The coarse level represents the low-quality NeRF, while a fine-level residual tri-plane, modeled as a distribution with GAN, captures potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, showcasing superior performance in both quantitative and qualitative assessments compared to other 3D restoration methods tailored to a single task.\n\nFor more information, please visit our project website at https://zkaiwu.github.io/RaFE-Project/."}, "2107.04217": {"original_text": "  This paper studies joint models for selecting correct answer sentences among\nthe top $k$ provided by answer sentence selection (AS2) modules, which are core\ncomponents of retrieval-based Question Answering (QA) systems. Our work shows\nthat a critical step to effectively exploit an answer set regards modeling the\ninterrelated information between pair of answers. For this purpose, we build a\nthree-way multi-classifier, which decides if an answer supports, refutes, or is\nneutral with respect to another one. More specifically, our neural architecture\nintegrates a state-of-the-art AS2 model with the multi-classifier, and a joint\nlayer connecting all components. We tested our models on WikiQA, TREC-QA, and a\nreal-world dataset. The results show that our models obtain the new state of\nthe art in AS2.\n", "rewritten_text": "This paper examines joint models for selecting the correct answer sentences from the top $k$ options provided by answer sentence selection (AS2) modules, which are essential components of retrieval-based Question Answering (QA) systems. Our research demonstrates that a crucial aspect in effectively leveraging an answer set involves capturing the interconnected information between pairs of answers. To address this, we have developed a three-way multi-classifier that determines whether an answer supports, refutes, or is neutral in relation to another answer. Specifically, our neural architecture combines a cutting-edge AS2 model with the multi-classifier and a joint layer that connects all components. We evaluated our models on WikiQA, TREC-QA, and a real-world dataset, and the results indicate that our models achieve a new state of the art in AS2."}, "2407.05271": {"original_text": "  Name-based gender prediction has traditionally categorized individuals as\neither female or male based on their names, using a binary classification\nsystem. That binary approach can be problematic in the cases of gender-neutral\nnames that do not align with any one gender, among other reasons. Relying\nsolely on binary gender categories without recognizing gender-neutral names can\nreduce the inclusiveness of gender prediction tasks. We introduce an additional\ngender category, i.e., \"neutral\", to study and address potential gender biases\nin Large Language Models (LLMs). We evaluate the performance of several\nfoundational and large language models in predicting gender based on first\nnames only. Additionally, we investigate the impact of adding birth years to\nenhance the accuracy of gender prediction, accounting for shifting associations\nbetween names and genders over time. Our findings indicate that most LLMs\nidentify male and female names with high accuracy (over 80%) but struggle with\ngender-neutral names (under 40%), and the accuracy of gender prediction is\nhigher for English-based first names than non-English names. The experimental\nresults show that incorporating the birth year does not improve the overall\naccuracy of gender prediction, especially for names with evolving gender\nassociations. We recommend using caution when applying LLMs for gender\nidentification in downstream tasks, particularly when dealing with non-binary\ngender labels.\n", "rewritten_text": "Traditional name-based gender prediction has historically classified individuals as either female or male based on their names, utilizing a binary classification system. However, this binary approach can present challenges when dealing with gender-neutral names that do not align with a specific gender. By solely relying on binary gender categories and overlooking gender-neutral names, the inclusivity of gender prediction tasks may be compromised. To address potential gender biases in Large Language Models (LLMs), we propose the introduction of an additional gender category, namely \"neutral\". Our study aims to assess the performance of various foundational and large language models in predicting gender solely based on first names. Furthermore, we explore the impact of incorporating birth years to enhance the accuracy of gender prediction, considering the evolving associations between names and genders over time.\n\nOur research findings reveal that while most LLMs demonstrate high accuracy (over 80%) in identifying male and female names, they struggle with gender-neutral names (below 40%). Additionally, the accuracy of gender prediction is notably higher for English-based first names compared to non-English names. Notably, our experimental results indicate that integrating birth years does not significantly improve the overall accuracy of gender prediction, particularly for names with changing gender associations. Therefore, caution is advised when utilizing LLMs for gender identification in downstream tasks, especially when dealing with non-binary gender labels."}, "2306.09379": {"original_text": "  In this technical report, we briefly introduce the solution of our team\nVIELab-HUST for coded target restoration through atmospheric turbulence in CVPR\n2023 UG$^2$+ Track 2.2. In this task, we propose an efficient multi-stage\nframework to restore a high quality image from distorted frames. Specifically,\neach distorted frame is initially aligned using image registration to suppress\ngeometric distortion. We subsequently select the sharpest set of registered\nframes by employing a frame selection approach based on image sharpness, and\naverage them to produce an image that is largely free of geometric distortion,\nalbeit with blurriness. A learning-based deblurring method is then applied to\nremove the residual blur in the averaged image. Finally, post-processing\ntechniques are utilized to further enhance the quality of the output image. Our\nframework is capable of handling different kinds of coded target dataset\nprovided in the final testing phase, and ranked 1st on the final leaderboard.\nOur code will be available at https://github.com/xsqhust/Turbulence_Removal.\n", "rewritten_text": "In our technical report, we present the solution developed by our team, VIELab-HUST, for restoring coded targets affected by atmospheric turbulence in the CVPR 2023 UG$^2$+ Track 2.2. Our approach involves a multi-stage framework designed to enhance the quality of images from distorted frames. Initially, each distorted frame undergoes alignment through image registration to mitigate geometric distortions. Subsequently, we identify the sharpest set of registered frames using a frame selection method based on image sharpness and average them to generate an image with reduced geometric distortion, albeit with some blurriness. To address the remaining blur, we apply a learning-based deblurring technique to the averaged image. Further enhancement of the output image is achieved through post-processing methods. Our framework is versatile and can handle various coded target datasets in the final testing phase, leading us to secure the top position on the final leaderboard. For those interested, our code will be accessible at https://github.com/xsqhust/Turbulence_Removal."}, "2108.03502": {"original_text": "  Automatic summarization techniques aim to shorten and generalize information\ngiven in the text while preserving its core message and the most relevant\nideas. This task can be approached and treated with a variety of methods,\nhowever, not many attempts have been made to produce solutions specifically for\nthe Russian language despite existing localizations of the state-of-the-art\nmodels. In this paper, we aim to showcase ruGPT3 ability to summarize texts,\nfine-tuning it on the corpora of Russian news with their corresponding\nhuman-generated summaries. Additionally, we employ hyperparameter tuning so\nthat the model's output becomes less random and more tied to the original text.\nWe evaluate the resulting texts with a set of metrics, showing that our\nsolution can surpass the state-of-the-art model's performance without\nadditional changes in architecture or loss function. Despite being able to\nproduce sensible summaries, our model still suffers from a number of flaws,\nnamely, it is prone to altering Named Entities present in the original text\n(such as surnames, places, dates), deviating from facts stated in the given\ndocument, and repeating the information in the summary.\n", "rewritten_text": "Automatic summarization techniques aim to condense and generalize information from text while retaining its core message and key ideas. Various methods can be used to tackle this task, but there have been limited efforts to develop solutions specifically for the Russian language, despite the availability of advanced models localized for this purpose. This study focuses on demonstrating the ability of ruGPT3 to summarize texts by fine-tuning it on a dataset of Russian news articles along with their human-generated summaries. Additionally, hyperparameter tuning is employed to reduce randomness in the model's output and enhance its alignment with the original text. Evaluation of the generated summaries using a set of metrics reveals that our approach can outperform existing state-of-the-art models without requiring changes to the model's architecture or loss function. While our model is capable of producing coherent summaries, it still exhibits certain limitations, including the tendency to modify Named Entities (such as names, locations, and dates) from the source text, straying from the factual content of the original document, and occasionally repeating information in the summary."}, "1901.01574": {"original_text": "  This work systematically analyzes the smoothing effect of vocabulary\nreduction for phrase translation models. We extensively compare various\nword-level vocabularies to show that the performance of smoothing is not\nsignificantly affected by the choice of vocabulary. This result provides\nempirical evidence that the standard phrase translation model is extremely\nsparse. Our experiments also reveal that vocabulary reduction is more effective\nfor smoothing large-scale phrase tables.\n", "rewritten_text": "This study systematically analyzes the impact of vocabulary reduction on the smoothing effect in phrase translation models. We conduct a comprehensive comparison of different word-level vocabularies to demonstrate that the choice of vocabulary does not significantly affect the performance of smoothing. These findings offer empirical support for the notion that the standard phrase translation model is highly sparse. Furthermore, our experiments indicate that vocabulary reduction is particularly effective in smoothing large-scale phrase tables."}, "2210.16579": {"original_text": "  Generating videos is a complex task that is accomplished by generating a set\nof temporally coherent images frame-by-frame. This limits the expressivity of\nvideos to only image-based operations on the individual video frames needing\nnetwork designs to obtain temporally coherent trajectories in the underlying\nimage space. We propose INR-V, a video representation network that learns a\ncontinuous space for video-based generative tasks. INR-V parameterizes videos\nusing implicit neural representations (INRs), a multi-layered perceptron that\npredicts an RGB value for each input pixel location of the video. The INR is\npredicted using a meta-network which is a hypernetwork trained on neural\nrepresentations of multiple video instances. Later, the meta-network can be\nsampled to generate diverse novel videos enabling many downstream video-based\ngenerative tasks. Interestingly, we find that conditional regularization and\nprogressive weight initialization play a crucial role in obtaining INR-V. The\nrepresentation space learned by INR-V is more expressive than an image space\nshowcasing many interesting properties not possible with the existing works.\nFor instance, INR-V can smoothly interpolate intermediate videos between known\nvideo instances (such as intermediate identities, expressions, and poses in\nface videos). It can also in-paint missing portions in videos to recover\ntemporally coherent full videos. In this work, we evaluate the space learned by\nINR-V on diverse generative tasks such as video interpolation, novel video\ngeneration, video inversion, and video inpainting against the existing\nbaselines. INR-V significantly outperforms the baselines on several of these\ndemonstrated tasks, clearly showcasing the potential of the proposed\nrepresentation space.\n", "rewritten_text": "Generating videos is a complex task that involves creating a series of temporally coherent images frame by frame. This approach limits the capabilities of videos to image-based operations on individual video frames, necessitating network designs to achieve temporally coherent trajectories within the image space. To address this challenge, we introduce INR-V, a video representation network designed to learn a continuous space for generative video tasks.\n\nINR-V utilizes implicit neural representations (INRs) to parameterize videos. INRs are implemented as a multi-layered perceptron that predicts an RGB value for each pixel location in the video. The INR is generated by a meta-network, which is a hypernetwork trained on neural representations from multiple video instances. By sampling the meta-network, diverse novel videos can be generated, enabling a wide range of video-based generative tasks.\n\nOur research highlights the importance of conditional regularization and progressive weight initialization in the development of INR-V. The representation space learned by INR-V offers greater expressiveness compared to traditional image spaces, showcasing unique properties not achievable with existing methods. For example, INR-V can smoothly interpolate intermediate videos between known instances, such as identities, expressions, and poses in face videos. It can also inpaint missing sections in videos to reconstruct temporally coherent sequences.\n\nIn our study, we evaluate the effectiveness of the representation space learned by INR-V across various generative tasks, including video interpolation, novel video generation, video inversion, and video inpainting, comparing the results against existing baselines. INR-V consistently outperforms these baselines in multiple tasks, demonstrating the potential of the proposed representation space."}, "2101.01843": {"original_text": "  Autonomous driving applications use two types of sensor systems to identify\nvehicles - depth sensing LiDAR and radiance sensing cameras. We compare the\nperformance (average precision) of a ResNet for vehicle detection in complex,\ndaytime, driving scenes when the input is a depth map (D = d(x,y)), a radiance\nimage (L = r(x,y)), or both [D,L]. (1) When the spatial sampling resolution of\nthe depth map and radiance image are equal to typical camera resolutions, a\nResNet detects vehicles at higher average precision from depth than radiance.\n(2) As the spatial sampling of the depth map declines to the range of current\nLiDAR devices, the ResNet average precision is higher for radiance than depth.\n(3) For a hybrid system that combines a depth map and radiance image, the\naverage precision is higher than using depth or radiance alone. We established\nthese observations in simulation and then confirmed them using realworld data.\nThe advantage of combining depth and radiance can be explained by noting that\nthe two type of information have complementary weaknesses. The radiance data\nare limited by dynamic range and motion blur. The LiDAR data have relatively\nlow spatial resolution. The ResNet combines the two data sources effectively to\nimprove overall vehicle detection.\n", "rewritten_text": "Autonomous driving applications utilize two types of sensor systems to identify vehicles: depth-sensing LiDAR and radiance-sensing cameras. In this study, we evaluate the performance (average precision) of a ResNet model for vehicle detection in complex daytime driving scenes. The input to the model can be a depth map (D = d(x,y)), a radiance image (L = r(x,y)), or a combination of both [D,L].\n\nOur findings are as follows:\n1. When the spatial sampling resolution of the depth map and radiance image matches typical camera resolutions, the ResNet detects vehicles with higher average precision from depth compared to radiance.\n2. As the spatial sampling of the depth map decreases to the level of current LiDAR devices, the ResNet achieves higher average precision for radiance than depth.\n3. For a hybrid system that integrates a depth map and radiance image, the average precision surpasses that of using depth or radiance individually. These observations were initially made through simulation and later validated using real-world data.\n\nThe advantage of combining depth and radiance lies in the complementary weaknesses of the two types of information. Radiance data are constrained by dynamic range and motion blur, while LiDAR data exhibit relatively low spatial resolution. The ResNet effectively merges these data sources to enhance overall vehicle detection accuracy."}, "1511.03328": {"original_text": "  Deep convolutional neural networks (CNNs) are the backbone of state-of-art\nsemantic image segmentation systems. Recent work has shown that complementing\nCNNs with fully-connected conditional random fields (CRFs) can significantly\nenhance their object localization accuracy, yet dense CRF inference is\ncomputationally expensive. We propose replacing the fully-connected CRF with\ndomain transform (DT), a modern edge-preserving filtering method in which the\namount of smoothing is controlled by a reference edge map. Domain transform\nfiltering is several times faster than dense CRF inference and we show that it\nyields comparable semantic segmentation results, accurately capturing object\nboundaries. Importantly, our formulation allows learning the reference edge map\nfrom intermediate CNN features instead of using the image gradient magnitude as\nin standard DT filtering. This produces task-specific edges in an end-to-end\ntrainable system optimizing the target semantic segmentation quality.\n", "rewritten_text": "Deep convolutional neural networks (CNNs) serve as the foundation for cutting-edge semantic image segmentation systems. Recent research has demonstrated that augmenting CNNs with fully-connected conditional random fields (CRFs) can significantly improve object localization accuracy. However, the dense CRF inference process is computationally intensive. To address this issue, we propose a substitution of the fully-connected CRF with domain transform (DT), a contemporary edge-preserving filtering technique that allows for control over the level of smoothing based on a reference edge map. Domain transform filtering operates several times faster than dense CRF inference, while delivering comparable semantic segmentation outcomes that effectively delineate object boundaries. Notably, our approach enables the learning of the reference edge map from intermediate CNN features, rather than relying on the image gradient magnitude as in traditional DT filtering. This methodology generates task-specific edges within an end-to-end trainable system that optimizes the quality of the target semantic segmentation."}, "1610.03155": {"original_text": "  Convolutional Neural Networks (CNN) have demon- strated its successful\napplications in computer vision, speech recognition, and natural language\nprocessing. For object recog- nition, CNNs might be limited by its strict label\nrequirement and an implicit assumption that images are supposed to be target-\nobject-dominated for optimal solutions. However, the labeling procedure,\nnecessitating laying out the locations of target ob- jects, is very tedious,\nmaking high-quality large-scale dataset prohibitively expensive. Data\naugmentation schemes are widely used when deep networks suffer the insufficient\ntraining data problem. All the images produced through data augmentation share\nthe same label, which may be problematic since not all data augmentation\nmethods are label-preserving. In this paper, we propose a weakly supervised CNN\nframework named Multiple Instance Learning Convolutional Neural Networks\n(MILCNN) to solve this problem. We apply MILCNN framework to object recognition\nand report state-of-the-art performance on three benchmark datasets: CIFAR10,\nCIFAR100 and ILSVRC2015 classification dataset.\n", "rewritten_text": "Convolutional Neural Networks (CNN) have demonstrated successful applications in computer vision, speech recognition, and natural language processing. In the realm of object recognition, CNNs may face limitations due to their strict label requirements and the implicit assumption that images should be dominated by the target object for optimal solutions. The labeling process, which involves pinpointing the locations of target objects, can be extremely tedious, resulting in the creation of high-quality large-scale datasets that are prohibitively expensive.\n\nTo address the challenge of insufficient training data, data augmentation techniques are commonly employed with deep networks. However, a potential issue arises when all images generated through data augmentation are assigned the same label, as not all data augmentation methods preserve the original labels.\n\nIn this paper, we introduce a weakly supervised CNN framework called Multiple Instance Learning Convolutional Neural Networks (MILCNN) to tackle this issue. We apply the MILCNN framework to object recognition tasks and present state-of-the-art performance results on three benchmark datasets: CIFAR10, CIFAR100, and ILSVRC2015 classification dataset."}, "1711.06606": {"original_text": "  To realize the full potential of deep learning for medical imaging, large\nannotated datasets are required for training. Such datasets are difficult to\nacquire because labeled medical images are not usually available due to privacy\nissues, lack of experts available for annotation, underrepresentation of rare\nconditions and poor standardization. Lack of annotated data has been addressed\nin conventional vision applications using synthetic images refined via\nunsupervised adversarial training to look like real images. However, this\napproach is difficult to extend to general medical imaging because of the\ncomplex and diverse set of features found in real human tissues. We propose an\nalternative framework that uses a reverse flow, where adversarial training is\nused to make real medical images more like synthetic images, and hypothesize\nthat clinically-relevant features can be preserved via self-regularization.\nThese domain-adapted images can then be accurately interpreted by networks\ntrained on large datasets of synthetic medical images. We test this approach\nfor the notoriously difficult task of depth-estimation from endoscopy. We train\na depth estimator on a large dataset of synthetic images generated using an\naccurate forward model of an endoscope and an anatomically-realistic colon.\nThis network predicts significantly better depths when using synthetic-like\ndomain-adapted images compared to the real images, confirming that the\nclinically-relevant features of depth are preserved.\n", "rewritten_text": "In order to fully harness the potential of deep learning in medical imaging, the availability of large annotated datasets for training is crucial. However, obtaining such datasets is challenging due to various factors, including the limited availability of labeled medical images caused by privacy concerns, a shortage of experts for annotation, the underrepresentation of rare conditions, and a lack of standardization.\n\nWhile traditional methods in computer vision have addressed the issue of limited annotated data by using synthetic images refined through unsupervised adversarial training to mimic real images, this approach is not easily applicable to the complex and diverse features present in actual human tissues in medical imaging. To overcome this challenge, we propose an alternative framework that employs a reverse flow technique. This involves utilizing adversarial training to make real medical images more closely resemble synthetic images, with the hypothesis that clinically relevant features can be preserved through self-regularization.\n\nBy adapting the domain of the images in this manner, we aim to enable networks trained on extensive datasets of synthetic medical images to accurately interpret the domain-adapted real medical images. To validate this approach, we focus on the challenging task of depth estimation from endoscopy. We train a depth estimator using a large dataset of synthetic images generated through a precise forward model of an endoscope and a realistic anatomical representation of a colon.\n\nOur results demonstrate that the depth estimator performs significantly better when utilizing domain-adapted images that resemble synthetic data, as opposed to real images. This outcome confirms that the clinically relevant features related to depth are effectively preserved through this domain adaptation process."}, "2402.17074": {"original_text": "  Digital Image Correlation (DIC) is an optical technique that measures\ndisplacement and strain by tracking pattern movement in a sequence of captured\nimages during testing. DIC has gained recognition in asphalt pavement\nengineering since the early 2000s. However, users often perceive the DIC\ntechnique as an out-of-box tool and lack a thorough understanding of its\noperational and measurement principles. This article presents a state-of-art\nreview of DIC as a crucial tool for laboratory testing of asphalt concrete\n(AC), primarily focusing on the widely utilized 2D-DIC and 3D-DIC techniques.\nTo address frequently asked questions from users, the review thoroughly\nexamines the optimal methods for preparing speckle patterns, configuring\nsingle-camera or dual-camera imaging systems, conducting DIC analyses, and\nexploring various applications. Furthermore, emerging DIC methodologies such as\nDigital Volume Correlation and deep-learning-based DIC are introduced,\nhighlighting their potential for future applications in pavement engineering.\nThe article also provides a comprehensive and reliable flowchart for\nimplementing DIC in AC characterization. Finally, critical directions for\nfuture research are presented.\n", "rewritten_text": "Digital Image Correlation (DIC) is an optical technique used to measure displacement and strain by tracking pattern movement in a sequence of captured images during testing. Since the early 2000s, DIC has gained recognition in asphalt pavement engineering. However, users often view the DIC technique as a plug-and-play tool and lack a thorough understanding of its operational and measurement principles. This article presents a state-of-the-art review of DIC as a crucial tool for laboratory testing of asphalt concrete (AC), with a primary focus on the widely utilized 2D-DIC and 3D-DIC techniques.\n\nTo address common user queries, the review thoroughly examines optimal methods for preparing speckle patterns, configuring single-camera or dual-camera imaging systems, conducting DIC analyses, and exploring various applications. Additionally, emerging DIC methodologies such as Digital Volume Correlation and deep-learning-based DIC are introduced, showcasing their potential for future applications in pavement engineering. The article also offers a comprehensive and reliable flowchart for implementing DIC in AC characterization. Finally, critical directions for future research are presented."}, "1805.06173": {"original_text": "  Existing deep convolutional neural networks have found major success in image\nderaining, but at the expense of an enormous number of parameters. This limits\ntheir potential application, for example in mobile devices. In this paper, we\npropose a lightweight pyramid of networks (LPNet) for single image deraining.\nInstead of designing a complex network structures, we use domain-specific\nknowledge to simplify the learning process. Specifically, we find that by\nintroducing the mature Gaussian-Laplacian image pyramid decomposition\ntechnology to the neural network, the learning problem at each pyramid level is\ngreatly simplified and can be handled by a relatively shallow network with few\nparameters. We adopt recursive and residual network structures to build the\nproposed LPNet, which has less than 8K parameters while still achieving\nstate-of-the-art performance on rain removal. We also discuss the potential\nvalue of LPNet for other low- and high-level vision tasks.\n", "rewritten_text": "Deep convolutional neural networks have achieved significant success in image deraining, albeit with a substantial number of parameters, limiting their applicability, especially on mobile devices. This paper introduces a lightweight pyramid of networks (LPNet) for single image deraining. Instead of complex network structures, we leverage domain-specific knowledge to streamline the learning process. By incorporating Gaussian-Laplacian image pyramid decomposition technology into the neural network, we simplify the learning task at each pyramid level. This approach allows for the use of a relatively shallow network with minimal parameters. The proposed LPNet utilizes recursive and residual network structures, containing fewer than 8,000 parameters, yet still delivering state-of-the-art performance in rain removal. Furthermore, we explore the potential of LPNet for various low- and high-level vision tasks."}, "2208.00919": {"original_text": "  Visual-inertial localization is a key problem in computer vision and robotics\napplications such as virtual reality, self-driving cars, and aerial vehicles.\nThe goal is to estimate an accurate pose of an object when either the\nenvironment or the dynamics are known. Absolute pose regression (APR)\ntechniques directly regress the absolute pose from an image input in a known\nscene using convolutional and spatio-temporal networks. Odometry methods\nperform relative pose regression (RPR) that predicts the relative pose from a\nknown object dynamic (visual or inertial inputs). The localization task can be\nimproved by retrieving information from both data sources for a cross-modal\nsetup, which is a challenging problem due to contradictory tasks. In this work,\nwe conduct a benchmark to evaluate deep multimodal fusion based on pose graph\noptimization and attention networks. Auxiliary and Bayesian learning are\nutilized for the APR task. We show accuracy improvements for the APR-RPR task\nand for the RPR-RPR task for aerial vehicles and hand-held devices. We conduct\nexperiments on the EuRoC MAV and PennCOSYVIO datasets and record and evaluate a\nnovel industry dataset.\n", "rewritten_text": "Visual-inertial localization is a crucial challenge in the fields of computer vision and robotics, with applications spanning virtual reality, self-driving cars, and aerial vehicles. The primary objective is to accurately estimate the pose of an object in scenarios where either the environment or the dynamics are known. Absolute pose regression (APR) techniques involve directly regressing the absolute pose from an image input within a familiar scene, utilizing convolutional and spatio-temporal networks. On the other hand, odometry methods focus on relative pose regression (RPR), predicting the relative pose based on known object dynamics, whether visual or inertial inputs are used.\n\nEnhancing the localization task involves leveraging information from both data sources in a cross-modal setup, a complex issue due to conflicting objectives. In this study, we present a benchmark evaluating deep multimodal fusion through pose graph optimization and attention networks. Auxiliary and Bayesian learning techniques are applied to the APR task, resulting in improved accuracy for both the APR-RPR and RPR-RPR tasks, particularly for aerial vehicles and hand-held devices. Experimental evaluations are conducted on the EuRoC MAV and PennCOSYVIO datasets, as well as a novel industry dataset, to record and assess the outcomes."}, "2107.0984": {"original_text": "  Multilingual pre-trained contextual embedding models (Devlin et al., 2019)\nhave achieved impressive performance on zero-shot cross-lingual transfer tasks.\nFinding the most effective fine-tuning strategy to fine-tune these models on\nhigh-resource languages so that it transfers well to the zero-shot languages is\na non-trivial task. In this paper, we propose a novel meta-optimizer to\nsoft-select which layers of the pre-trained model to freeze during fine-tuning.\nWe train the meta-optimizer by simulating the zero-shot transfer scenario.\nResults on cross-lingual natural language inference show that our approach\nimproves over the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al.,\n2020).\n", "rewritten_text": "The multilingual pre-trained contextual embedding models developed by Devlin et al. (2019) have demonstrated impressive performance in zero-shot cross-lingual transfer tasks. However, determining the most effective fine-tuning strategy to optimize these models for high-resource languages in a way that facilitates successful transfer to zero-shot languages is a complex challenge. In this study, we introduce a novel meta-optimizer designed to intelligently select which layers of the pre-trained model to freeze during the fine-tuning process. The meta-optimizer is trained using simulations of zero-shot transfer scenarios. Our experimental results on cross-lingual natural language inference indicate that our approach outperforms both the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al., 2020)."}, "2010.09298": {"original_text": "  Though deep learning has achieved advanced performance recently, it remains a\nchallenging task in the field of medical imaging, as obtaining reliable labeled\ntraining data is time-consuming and expensive. In this paper, we propose a\ndouble-uncertainty weighted method for semi-supervised segmentation based on\nthe teacher-student model. The teacher model provides guidance for the student\nmodel by penalizing their inconsistent prediction on both labeled and unlabeled\ndata. We train the teacher model using Bayesian deep learning to obtain\ndouble-uncertainty, i.e. segmentation uncertainty and feature uncertainty. It\nis the first to extend segmentation uncertainty estimation to feature\nuncertainty, which reveals the capability to capture information among\nchannels. A learnable uncertainty consistency loss is designed for the\nunsupervised learning process in an interactive manner between prediction and\nuncertainty. With no ground-truth for supervision, it can still incentivize\nmore accurate teacher's predictions and facilitate the model to reduce\nuncertain estimations. Furthermore, our proposed double-uncertainty serves as a\nweight on each inconsistency penalty to balance and harmonize supervised and\nunsupervised training processes. We validate the proposed feature uncertainty\nand loss function through qualitative and quantitative analyses. Experimental\nresults show that our method outperforms the state-of-the-art uncertainty-based\nsemi-supervised methods on two public medical datasets.\n", "rewritten_text": "While deep learning has recently achieved advanced performance, it remains a challenging task in the field of medical imaging due to the time-consuming and expensive nature of obtaining reliable labeled training data. In this paper, we introduce a double-uncertainty weighted method for semi-supervised segmentation based on the teacher-student model. The teacher model guides the student model by penalizing their inconsistent predictions on both labeled and unlabeled data. We train the teacher model using Bayesian deep learning to acquire double-uncertainty, encompassing segmentation uncertainty and feature uncertainty. This is the first method to extend segmentation uncertainty estimation to feature uncertainty, showcasing the ability to capture information across channels. A learnable uncertainty consistency loss is devised for the unsupervised learning process, fostering an interactive relationship between prediction and uncertainty. Even without ground-truth supervision, this loss incentivizes more accurate teacher predictions and aids the model in reducing uncertain estimations. Moreover, our proposed double-uncertainty acts as a weight on each inconsistency penalty to balance and harmonize the supervised and unsupervised training processes. We validate the feature uncertainty and loss function through qualitative and quantitative analyses. Experimental results demonstrate that our method surpasses state-of-the-art uncertainty-based semi-supervised methods on two public medical datasets."}, "2108.11575": {"original_text": "  Spatio-temporal representational learning has been widely adopted in various\nfields such as action recognition, video object segmentation, and action\nanticipation. Previous spatio-temporal representational learning approaches\nprimarily employ ConvNets or sequential models,e.g., LSTM, to learn the\nintra-frame and inter-frame features. Recently, Transformer models have\nsuccessfully dominated the study of natural language processing (NLP), image\nclassification, etc. However, the pure-Transformer based spatio-temporal\nlearning can be prohibitively costly on memory and computation to extract\nfine-grained features from a tiny patch. To tackle the training difficulty and\nenhance the spatio-temporal learning, we construct a shifted chunk Transformer\nwith pure self-attention blocks. Leveraging the recent efficient Transformer\ndesign in NLP, this shifted chunk Transformer can learn hierarchical\nspatio-temporal features from a local tiny patch to a global video clip. Our\nshifted self-attention can also effectively model complicated inter-frame\nvariances. Furthermore, we build a clip encoder based on Transformer to model\nlong-term temporal dependencies. We conduct thorough ablation studies to\nvalidate each component and hyper-parameters in our shifted chunk Transformer,\nand it outperforms previous state-of-the-art approaches on Kinetics-400,\nKinetics-600, UCF101, and HMDB51.\n", "rewritten_text": "Spatio-temporal representational learning is widely utilized in various fields, including action recognition, video object segmentation, and action anticipation. Previous approaches to spatio-temporal representational learning have mainly relied on ConvNets or sequential models, such as LSTM, to capture intra-frame and inter-frame features. Transformer models have recently emerged as dominant in natural language processing (NLP), image classification, and other domains. However, pure Transformer-based spatio-temporal learning can be excessively demanding in terms of memory and computation when extracting detailed features from small patches.\n\nTo address the challenges of training and enhance spatio-temporal learning, we introduce a shifted chunk Transformer with self-attention blocks. Drawing on efficient Transformer designs in NLP, this shifted chunk Transformer can learn hierarchical spatio-temporal features ranging from local patches to global video clips. The shifted self-attention mechanism effectively captures complex inter-frame variations. Additionally, we develop a clip encoder based on Transformer to model long-term temporal dependencies. Through comprehensive ablation studies, we validate each component and hyper-parameter of our shifted chunk Transformer, demonstrating superior performance compared to previous state-of-the-art methods on datasets such as Kinetics-400, Kinetics-600, UCF101, and HMDB51."}, "2102.0208": {"original_text": "  We introduce a top-down approach to discourse parsing that is conceptually\nsimpler than its predecessors (Kobayashi et al., 2020; Zhang et al., 2020). By\nframing the task as a sequence labelling problem where the goal is to\niteratively segment a document into individual discourse units, we are able to\neliminate the decoder and reduce the search space for splitting points. We\nexplore both traditional recurrent models and modern pre-trained transformer\nmodels for the task, and additionally introduce a novel dynamic oracle for\ntop-down parsing. Based on the Full metric, our proposed LSTM model sets a new\nstate-of-the-art for RST parsing.\n", "rewritten_text": "We present a top-down approach to discourse parsing that is conceptually simpler than previous methods (Kobayashi et al., 2020; Zhang et al., 2020). By defining the task as a sequence labeling problem aimed at iteratively segmenting a document into individual discourse units, we can eliminate the need for a decoder and reduce the search space for splitting points. We investigate both traditional recurrent models and modern pre-trained transformer models for this task, and introduce a new dynamic oracle for top-down parsing. Our proposed LSTM model achieves a new state-of-the-art for RST parsing, as measured by the Full metric."}, "1112.2386": {"original_text": "  This paper proposes a new procedure in order to improve the performance of\nblock matching and 3-D filtering (BM3D) image denoising algorithm. It is\ndemonstrated that it is possible to achieve a better performance than that of\nBM3D algorithm in a variety of noise levels. This method changes BM3D algorithm\nparameter values according to noise level, removes prefiltering, which is used\nin high noise level; therefore Peak Signal-to-Noise Ratio (PSNR) and visual\nquality get improved, and BM3D complexities and processing time are reduced.\nThis improved BM3D algorithm is extended and used to denoise satellite and\ncolor filter array (CFA) images. Output results show that the performance has\nupgraded in comparison with current methods of denoising satellite and CFA\nimages. In this regard this algorithm is compared with Adaptive PCA algorithm,\nthat has led to superior performance for denoising CFA images, on the subject\nof PSNR and visual quality. Also the processing time has decreased\nsignificantly.\n", "rewritten_text": "This paper introduces a novel procedure aimed at enhancing the performance of the Block Matching and 3-D Filtering (BM3D) image denoising algorithm. The study demonstrates the potential for achieving superior performance compared to the BM3D algorithm across various noise levels. The proposed method involves adjusting the parameter values of the BM3D algorithm based on the noise level, eliminating prefiltering typically utilized in high noise scenarios. Consequently, the Peak Signal-to-Noise Ratio (PSNR) and visual quality are enhanced, while reducing the complexities and processing time associated with BM3D. The enhanced BM3D algorithm is further applied to denoise satellite and color filter array (CFA) images, yielding improved performance compared to existing denoising methods for these types of images. A comparison with the Adaptive PCA algorithm reveals superior performance in denoising CFA images in terms of PSNR, visual quality, and processing time efficiency."}, "2103.06678": {"original_text": "  In this paper, we explore the effects of language variants, data sizes, and\nfine-tuning task types in Arabic pre-trained language models. To do so, we\nbuild three pre-trained language models across three variants of Arabic: Modern\nStandard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a\nfourth language model which is pre-trained on a mix of the three. We also\nexamine the importance of pre-training data size by building additional models\nthat are pre-trained on a scaled-down set of the MSA variant. We compare our\ndifferent models to each other, as well as to eight publicly available models\nby fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest\nthat the variant proximity of pre-training data to fine-tuning data is more\nimportant than the pre-training data size. We exploit this insight in defining\nan optimized system selection model for the studied tasks.\n", "rewritten_text": "This paper investigates the impact of language variants, data sizes, and fine-tuning task types on Arabic pre-trained language models. To achieve this, we develop three pre-trained language models based on three Arabic variants: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic. Additionally, we create a fourth language model pre-trained on a combination of the three variants. We also assess the significance of pre-training data size by constructing additional models pre-trained on a reduced set of the MSA variant. Comparisons are made among our various models and eight publicly available models through fine-tuning on five NLP tasks across 12 datasets. Our findings indicate that the proximity of the pre-training data variant to the fine-tuning data is more crucial than the size of the pre-training data. Leveraging this insight, we propose an optimized system selection model for the tasks under study."}, "2408.00331": {"original_text": "  Reliably detecting when a deployed machine learning model is likely to fail\non a given input is crucial for ensuring safe operation. In this work, we\npropose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel\napproach that leverages priors from large language models (LLMs) and\nvision-language models (VLMs) to detect failures in image classification\nmodels. DECIDER utilizes LLMs to specify task-relevant core attributes and\nconstructs a ``debiased'' version of the classifier by aligning its visual\nfeatures to these core attributes using a VLM, and detects potential failure by\nmeasuring disagreement between the original and debiased models. In addition to\nproactively identifying samples on which the model would fail, DECIDER also\nprovides human-interpretable explanations for failure through a novel\nattribute-ablation strategy. Through extensive experiments across diverse\nbenchmarks spanning subpopulation shifts (spurious correlations, class\nimbalance) and covariate shifts (synthetic corruptions, domain shifts), DECIDER\nconsistently achieves state-of-the-art failure detection performance,\nsignificantly outperforming baselines in terms of the overall Matthews\ncorrelation coefficient as well as failure and success recall. Our codes can be\naccessed at~\\url{https://github.com/kowshikthopalli/DECIDER/}\n", "rewritten_text": "Detecting potential failures in deployed machine learning models is crucial for ensuring safe operation. In this study, we introduce DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel approach that utilizes priors from large language models (LLMs) and vision-language models (VLMs) to identify failures in image classification models. DECIDER leverages LLMs to define task-relevant core attributes and creates a \"debiased\" version of the classifier by aligning its visual features with these core attributes using a VLM. It identifies potential failures by measuring discrepancies between the original and debiased models. \n\nIn addition to proactively detecting samples where the model may fail, DECIDER also offers human-interpretable explanations for failures through a unique attribute-ablation strategy. Through extensive experiments across various benchmarks covering subpopulation shifts (such as spurious correlations and class imbalances) and covariate shifts (including synthetic corruptions and domain shifts), DECIDER consistently achieves state-of-the-art performance in failure detection. It significantly outperforms baseline models in terms of the overall Matthews correlation coefficient, as well as failure and success recall rates. \n\nAccess our code at: https://github.com/kowshikthopalli/DECIDER/"}, "1708.06126": {"original_text": "  This paper presents a novel application to detect counterfeit identity\ndocuments forged by a scan-printing operation. Texture analysis approaches are\nproposed to extract validation features from security background that is\nusually printed in documents as IDs or banknotes. The main contribution of this\nwork is the end-to-end mobile-server architecture, which provides a service for\nnon-expert users and therefore can be used in several scenarios. The system\nalso provides a crowdsourcing mode so labeled images can be gathered,\ngenerating databases for incremental training of the algorithms.\n", "rewritten_text": "This paper introduces a new application designed to identify counterfeit identity documents created through scan-printing operations. The proposed approach utilizes texture analysis techniques to extract validation features from the security background typically found in documents such as IDs or banknotes. The primary innovation of this study lies in the development of an end-to-end mobile-server architecture, offering a user-friendly service suitable for individuals without expertise in the field, thus making it applicable in various scenarios. Additionally, the system includes a crowdsourcing feature to collect labeled images, enabling the creation of databases for continuous algorithm training."}, "2409.12421": {"original_text": "  Camouflaged object detection (COD) aims to segment camouflaged objects which\nexhibit very similar patterns with the surrounding environment. Recent research\nworks have shown that enhancing the feature representation via the frequency\ninformation can greatly alleviate the ambiguity problem between the foreground\nobjects and the background.With the emergence of vision foundation models, like\nInternImage, Segment Anything Model etc, adapting the pretrained model on COD\ntasks with a lightweight adapter module shows a novel and promising research\ndirection. Existing adapter modules mainly care about the feature adaptation in\nthe spatial domain. In this paper, we propose a novel frequency-guided spatial\nadaptation method for COD task. Specifically, we transform the input features\nof the adapter into frequency domain. By grouping and interacting with\nfrequency components located within non overlapping circles in the spectrogram,\ndifferent frequency components are dynamically enhanced or weakened, making the\nintensity of image details and contour features adaptively adjusted. At the\nsame time, the features that are conducive to distinguishing object and\nbackground are highlighted, indirectly implying the position and shape of\ncamouflaged object. We conduct extensive experiments on four widely adopted\nbenchmark datasets and the proposed method outperforms 26 state-of-the-art\nmethods with large margins. Code will be released.\n", "rewritten_text": "Camouflaged object detection (COD) aims to segment objects that blend into their surrounding environment by utilizing similar patterns. Recent research has demonstrated that improving feature representation through frequency information can effectively reduce ambiguity between foreground objects and the background. The rise of foundational vision models, such as InternImage and Segment Anything Model, has paved the way for adapting pretrained models to COD tasks using a lightweight adapter module, presenting a new and promising research avenue.\n\nWhile existing adapter modules primarily focus on spatial feature adaptation, this paper introduces a novel frequency-guided spatial adaptation method for COD tasks. Specifically, we propose transforming the input features of the adapter into the frequency domain. By grouping and interacting with frequency components within non-overlapping circles in the spectrogram, different frequency components are dynamically adjusted to enhance or weaken, thereby adaptively modifying the intensity of image details and contour features. Simultaneously, features that aid in distinguishing objects from the background are emphasized, indirectly indicating the position and shape of camouflaged objects.\n\nExtensive experiments were conducted on four widely recognized benchmark datasets, and the proposed method surpassed 26 state-of-the-art methods by a significant margin. The code for this method will be made available for public access."}, "1712.03687": {"original_text": "  Because of affected by weather conditions, camera pose and range, etc.\nObjects are usually small, blur, occluded and diverse pose in the images\ngathered from outdoor surveillance cameras or access control system. It is\nchallenging and important to detect faces precisely for face recognition system\nin the field of public security. In this paper, we design a based on context\nmodeling structure named Feature Hierarchy Encoder-Decoder Network for face\ndetection(FHEDN), which can detect small, blur and occluded face with hierarchy\nby hierarchy from the end to the beginning likes encoder-decoder in a single\nnetwork. The proposed network is consist of multiple context modeling and\nprediction modules, which are in order to detect small, blur, occluded and\ndiverse pose faces. In addition, we analyse the influence of distribution of\ntraining set, scale of default box and receipt field size to detection\nperformance in implement stage. Demonstrated by experiments, Our network\nachieves promising performance on WIDER FACE and FDDB benchmarks.\n", "rewritten_text": "Due to weather conditions, camera pose, range, and other factors, objects in images obtained from outdoor surveillance cameras or access control systems are typically small, blurred, occluded, and in diverse poses. Precisely detecting faces is a challenging and crucial task for face recognition systems in the field of public security. In this study, we introduce a context modeling-based structure called the Feature Hierarchy Encoder-Decoder Network for face detection (FHEDN). This network is designed to detect small, blurred, and occluded faces with a hierarchical approach, moving from the end to the beginning similar to an encoder-decoder within a single network. The proposed network comprises multiple context modeling and prediction modules aimed at detecting faces of varying poses and levels of occlusion. Furthermore, we investigate the impact of training set distribution, default box scale, and receptive field size on detection performance during implementation. Through experimental validation, our network demonstrates promising performance on benchmarks such as WIDER FACE and FDDB."}, "2010.01305": {"original_text": "  Street view images classification aiming at urban land use analysis is\ndifficult because the class labels (e.g., commercial area), are concepts with\nhigher abstract level compared to the ones of general visual tasks (e.g.,\npersons and cars). Therefore, classification models using only visual features\noften fail to achieve satisfactory performance. In this paper, a novel approach\nbased on a \"Detector-Encoder-Classifier\" framework is proposed. Instead of\nusing visual features of the whole image directly as common image-level models\nbased on convolutional neural networks (CNNs) do, the proposed framework\nfirstly obtains the bounding boxes of buildings in street view images from a\ndetector. Their contextual information such as the co-occurrence patterns of\nbuilding classes and their layout are then encoded into metadata by the\nproposed algorithm \"CODING\" (Context encOding of Detected buildINGs). Finally,\nthese bounding box metadata are classified by a recurrent neural network (RNN).\nIn addition, we made a dual-labeled dataset named \"BEAUTY\" (Building dEtection\nAnd Urban funcTional-zone portraYing) of 19,070 street view images and 38,857\nbuildings based on the existing BIC GSV [1]. The dataset can be used not only\nfor street view image classification, but also for multi-class building\ndetection. Experiments on \"BEAUTY\" show that the proposed approach achieves a\n12.65% performance improvement on macro-precision and 12% on macro-recall over\nimage-level CNN based models. Our code and dataset are available at\nhttps://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/\n", "rewritten_text": "Classifying street view images for urban land use analysis is challenging due to the abstract nature of class labels, such as \"commercial area,\" compared to more common visual tasks like identifying people and cars. As a result, classification models relying solely on visual features often fall short in performance. This paper introduces a new approach utilizing a \"Detector-Encoder-Classifier\" framework. Instead of directly using visual features of the entire image, as traditional convolutional neural network (CNN) models do, this framework first identifies building bounding boxes in street view images using a detector. The contextual information, including co-occurrence patterns of building classes and their layout, is then encoded into metadata using the proposed \"CODING\" algorithm (Context encOding of Detected buildINGs). Subsequently, a recurrent neural network (RNN) classifies these bounding box metadata. \n\nFurthermore, a dual-labeled dataset called \"BEAUTY\" (Building dEtection And Urban funcTional-zone portraYing) comprising 19,070 street view images and 38,857 buildings, based on the existing BIC GSV [1], was created. This dataset serves not only for street view image classification but also for multi-class building detection. Experiments conducted on the \"BEAUTY\" dataset demonstrate that the proposed approach yields a 12.65% improvement in macro-precision and 12% in macro-recall compared to image-level CNN-based models. The code and dataset are accessible at https://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/."}, "2403.10737": {"original_text": "  Facial action unit (AU) detection is a fundamental block for objective facial\nexpression analysis. Supervised learning approaches require a large amount of\nmanual labeling which is costly. The limited labeled data are also not diverse\nin terms of gender which can affect model fairness. In this paper, we propose\nto use synthetically generated data and multi-source domain adaptation (MSDA)\nto address the problems of the scarcity of labeled data and the diversity of\nsubjects. Specifically, we propose to generate a diverse dataset through\nsynthetic facial expression re-targeting by transferring the expressions from\nreal faces to synthetic avatars. Then, we use MSDA to transfer the AU detection\nknowledge from a real dataset and the synthetic dataset to a target dataset.\nInstead of aligning the overall distributions of different domains, we propose\nPaired Moment Matching (PM2) to align the features of the paired real and\nsynthetic data with the same facial expression. To further improve gender\nfairness, PM2 matches the features of the real data with a female and a male\nsynthetic image. Our results indicate that synthetic data and the proposed\nmodel improve both AU detection performance and fairness across genders,\ndemonstrating its potential to solve AU detection in-the-wild.\n", "rewritten_text": "Facial action unit (AU) detection is a crucial component in the objective analysis of facial expressions. Traditional supervised learning methods rely heavily on manual labeling, which can be both time-consuming and expensive. Moreover, the existing labeled data often lack diversity in terms of gender, potentially leading to biases in the model. \n\nIn this study, we introduce a novel approach that leverages synthetically generated data and multi-source domain adaptation (MSDA) to tackle the challenges posed by the scarcity of labeled data and the need for diverse subject representation. Our method involves creating a diverse dataset through synthetic facial expression re-targeting, where expressions from real faces are transferred to synthetic avatars. Subsequently, we employ MSDA to transfer the AU detection knowledge from both real and synthetic datasets to a target dataset.\n\nRather than focusing on aligning the overall distributions of different domains, we propose a technique called Paired Moment Matching (PM2) to align the features of paired real and synthetic data exhibiting the same facial expression. To enhance gender fairness, PM2 aligns the features of real data with both female and male synthetic images. Our findings demonstrate that the use of synthetic data and the proposed model not only enhance AU detection performance but also promote fairness across genders, showcasing its potential in addressing AU detection challenges in real-world scenarios."}, "2401.03129": {"original_text": "  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n", "rewritten_text": "Recent advancements in Large Language Models (LLMs) have demonstrated remarkable proficiency in various tasks. The potent applications of LLMs in numerous fields have led to a surge in LLM development. A common practice in developing LLMs involves continuous pre-training on previously fine-tuned models. However, this approach can result in catastrophic forgetting. In our study, we explore the phenomenon of forgetting that occurs during continuous pre-training on an existing fine-tuned LLM. We assess the impact of continuous pre-training on the fine-tuned LLM across different dimensions, such as output format, knowledge, and reliability. The experimental results underscore the significant challenge of mitigating catastrophic forgetting during continuous pre-training, particularly the issue of repetition."}, "2105.07197": {"original_text": "  Modern machine learning models for computer vision exceed humans in accuracy\non specific visual recognition tasks, notably on datasets like ImageNet.\nHowever, high accuracy can be achieved in many ways. The particular decision\nfunction found by a machine learning system is determined not only by the data\nto which the system is exposed, but also the inductive biases of the model,\nwhich are typically harder to characterize. In this work, we follow a recent\ntrend of in-depth behavioral analyses of neural network models that go beyond\naccuracy as an evaluation metric by looking at patterns of errors. Our focus is\non comparing a suite of standard Convolutional Neural Networks (CNNs) and a\nrecently-proposed attention-based network, the Vision Transformer (ViT), which\nrelaxes the translation-invariance constraint of CNNs and therefore represents\na model with a weaker set of inductive biases. Attention-based networks have\npreviously been shown to achieve higher accuracy than CNNs on vision tasks, and\nwe demonstrate, using new metrics for examining error consistency with more\ngranularity, that their errors are also more consistent with those of humans.\nThese results have implications both for building more human-like vision\nmodels, as well as for understanding visual object recognition in humans.\n", "rewritten_text": "Modern machine learning models for computer vision now surpass humans in accuracy for specific visual recognition tasks, particularly on datasets such as ImageNet. Achieving high accuracy can be approached in various ways. The decision function of a machine learning system is not solely influenced by the data it is exposed to, but also by the inductive biases of the model, which are often challenging to define. This study aligns with a recent trend of conducting thorough behavioral analyses of neural network models that extend beyond accuracy as the sole evaluation metric, delving into error patterns. Our focus is on comparing a selection of standard Convolutional Neural Networks (CNNs) with a newly proposed attention-based network, the Vision Transformer (ViT). The ViT relaxes the translation-invariance constraint of CNNs, presenting a model with a less rigid set of inductive biases. Previous research has demonstrated that attention-based networks outperform CNNs in vision tasks. By introducing new metrics to analyze error consistency in more detail, we show that the errors of attention-based networks align more closely with those made by humans. These findings have implications for developing vision models that mimic human capabilities and for enhancing our understanding of visual object recognition in humans."}, "1902.01955": {"original_text": "  In conventional speech recognition, phoneme-based models outperform\ngrapheme-based models for non-phonetic languages such as English. The\nperformance gap between the two typically reduces as the amount of training\ndata is increased. In this work, we examine the impact of the choice of\nmodeling unit for attention-based encoder-decoder models. We conduct\nexperiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, using various\ntarget units (phoneme, grapheme, and word-piece); across all tasks, we find\nthat grapheme or word-piece models consistently outperform phoneme-based\nmodels, even though they are evaluated without a lexicon or an external\nlanguage model. We also investigate model complementarity: we find that we can\nimprove WERs by up to 9% relative by rescoring N-best lists generated from a\nstrong word-piece based baseline with either the phoneme or the grapheme model.\nRescoring an N-best list generated by the phonemic system, however, provides\nlimited improvements. Further analysis shows that the word-piece-based models\nproduce more diverse N-best hypotheses, and thus lower oracle WERs, than\nphonemic models.\n", "rewritten_text": "In traditional speech recognition, phoneme-based models tend to outperform grapheme-based models for non-phonetic languages like English. However, as the amount of training data increases, the performance gap between the two models typically narrows. This study investigates the impact of the choice of modeling unit on attention-based encoder-decoder models. Experiments were conducted on the LibriSpeech tasks with durations of 100hr, 460hr, and 960hr, using different target units (phoneme, grapheme, and word-piece). Across all tasks, it was consistently found that grapheme or word-piece models outperformed phoneme-based models, even without the use of a lexicon or external language model.\n\nAdditionally, the study explored model complementarity and discovered that Word Error Rates (WERs) could be improved by up to 9% relative by rescoring N-best lists generated from a strong word-piece based baseline with either the phoneme or grapheme model. However, rescoring an N-best list generated by the phonemic system showed limited improvements. Further analysis revealed that word-piece-based models generated more diverse N-best hypotheses, resulting in lower oracle WERs compared to phonemic models."}, "2012.12482": {"original_text": "  We address the problem of crowd localization, i.e., the prediction of dots\ncorresponding to people in a crowded scene. Due to various challenges, a\nlocalization method is prone to spatial semantic errors, i.e., predicting\nmultiple dots within a same person or collapsing multiple dots in a cluttered\nregion. We propose a topological approach targeting these semantic errors. We\nintroduce a topological constraint that teaches the model to reason about the\nspatial arrangement of dots. To enforce this constraint, we define a\npersistence loss based on the theory of persistent homology. The loss compares\nthe topographic landscape of the likelihood map and the topology of the ground\ntruth. Topological reasoning improves the quality of the localization algorithm\nespecially near cluttered regions. On multiple public benchmarks, our method\noutperforms previous localization methods. Additionally, we demonstrate the\npotential of our method in improving the performance in the crowd counting\ntask.\n", "rewritten_text": "We focus on the challenge of crowd localization, specifically predicting the locations of individuals in a crowded setting. One of the main difficulties faced by localization methods is the occurrence of spatial semantic errors, such as predicting multiple points within the same person or merging multiple points in a congested area. To address this issue, we propose a topological approach that targets these semantic errors.\n\nOur approach involves introducing a topological constraint that guides the model in understanding the spatial distribution of points. This constraint is enforced through a persistence loss, which is based on the theory of persistent homology. The loss function compares the topographic features of the likelihood map with the topology of the ground truth data. By incorporating topological reasoning, we enhance the accuracy of the localization algorithm, particularly in densely populated areas.\n\nThrough evaluations on various public benchmarks, our method surpasses previous localization techniques. Furthermore, we showcase the potential of our approach in enhancing performance in crowd counting tasks."}, "2303.02982": {"original_text": "  Learning from large-scale contrastive language-image pre-training like CLIP\nhas shown remarkable success in a wide range of downstream tasks recently, but\nit is still under-explored on the challenging few-shot action recognition\n(FSAR) task. In this work, we aim to transfer the powerful multimodal knowledge\nof CLIP to alleviate the inaccurate prototype estimation issue due to data\nscarcity, which is a critical problem in low-shot regimes. To this end, we\npresent a CLIP-guided prototype modulating framework called CLIP-FSAR, which\nconsists of two key components: a video-text contrastive objective and a\nprototype modulation. Specifically, the former bridges the task discrepancy\nbetween CLIP and the few-shot video task by contrasting videos and\ncorresponding class text descriptions. The latter leverages the transferable\ntextual concepts from CLIP to adaptively refine visual prototypes with a\ntemporal Transformer. By this means, CLIP-FSAR can take full advantage of the\nrich semantic priors in CLIP to obtain reliable prototypes and achieve accurate\nfew-shot classification. Extensive experiments on five commonly used benchmarks\ndemonstrate the effectiveness of our proposed method, and CLIP-FSAR\nsignificantly outperforms existing state-of-the-art methods under various\nsettings. The source code and models will be publicly available at\nhttps://github.com/alibaba-mmai-research/CLIP-FSAR.\n", "rewritten_text": "Recently, large-scale contrastive language-image pre-training models like CLIP have demonstrated remarkable success across a wide range of downstream tasks. However, their application to the challenging few-shot action recognition (FSAR) task remains relatively unexplored. This study aims to leverage the robust multimodal knowledge of CLIP to address the issue of inaccurate prototype estimation caused by data scarcity, a critical challenge in low-shot scenarios.\n\nTo achieve this goal, we introduce a novel framework called CLIP-FSAR, which is guided by CLIP and focuses on prototype modulation. This framework comprises two key components: a video-text contrastive objective and prototype modulation. The former aims to bridge the gap between CLIP and the few-shot video task by contrasting videos with corresponding class text descriptions. The latter component utilizes transferable textual concepts from CLIP to dynamically refine visual prototypes using a temporal Transformer. Through this approach, CLIP-FSAR effectively harnesses the rich semantic information in CLIP to generate reliable prototypes and achieve accurate few-shot classification.\n\nExtensive experiments conducted on five commonly used benchmarks demonstrate the effectiveness of our proposed method. CLIP-FSAR consistently outperforms existing state-of-the-art methods across various settings. The source code and models for CLIP-FSAR will be made publicly available at https://github.com/alibaba-mmai-research/CLIP-FSAR."}, "1905.07826": {"original_text": "  Multi-instance video object segmentation is to segment specific instances\nthroughout a video sequence in pixel level, given only an annotated first\nframe. In this paper, we implement an effective fully convolutional networks\nwith U-Net similar structure built on top of OSVOS fine-tuned layer. We use\ninstance isolation to transform this multi-instance segmentation problem into\nbinary labeling problem, and use weighted cross entropy loss and dice\ncoefficient loss as our loss function. Our best model achieves F mean of 0.467\nand J mean of 0.424 on DAVIS dataset, which is a comparable performance with\nthe State-of-the-Art approach. But case analysis shows this model can achieve a\nsmoother contour and better instance coverage, meaning it better for recall\nfocused segmentation scenario. We also did experiments on other convolutional\nneural networks, including Seg-Net, Mask R-CNN, and provide insightful\ncomparison and discussion.\n", "rewritten_text": "The aim of multi-instance video object segmentation is to accurately segment specific instances throughout a video sequence at the pixel level, using only an annotated first frame. In this study, we have developed an effective fully convolutional network with a U-Net-like structure, which is constructed on top of the OSVOS fine-tuned layer. To address the multi-instance segmentation challenge, we employ instance isolation to convert it into a binary labeling problem. Our approach utilizes a combination of weighted cross-entropy loss and dice coefficient loss as the loss function. Our top-performing model achieves an F-measure of 0.467 and a Jaccard index of 0.424 on the DAVIS dataset, demonstrating performance comparable to the State-of-the-Art methods. Furthermore, a detailed case analysis reveals that our model produces smoother contours and better instance coverage, making it particularly suitable for recall-focused segmentation scenarios. Additionally, we conducted experiments with other convolutional neural networks, such as Seg-Net and Mask R-CNN, providing valuable comparisons and discussions."}, "2003.14282": {"original_text": "  A wide variety of transition-based algorithms are currently used for\ndependency parsers. Empirical studies have shown that performance varies across\ndifferent treebanks in such a way that one algorithm outperforms another on one\ntreebank and the reverse is true for a different treebank. There is often no\ndiscernible reason for what causes one algorithm to be more suitable for a\ncertain treebank and less so for another. In this paper we shed some light on\nthis by introducing the concept of an algorithm's inherent dependency\ndisplacement distribution. This characterises the bias of the algorithm in\nterms of dependency displacement, which quantify both distance and direction of\nsyntactic relations. We show that the similarity of an algorithm's inherent\ndistribution to a treebank's displacement distribution is clearly correlated to\nthe algorithm's parsing performance on that treebank, specifically with highly\nsignificant and substantial correlations for the predominant sentence lengths\nin Universal Dependency treebanks. We also obtain results which show a more\ndiscrete analysis of dependency displacement does not result in any meaningful\ncorrelations.\n", "rewritten_text": "Various transition-based algorithms are currently utilized for dependency parsers, showcasing a wide range of performance outcomes across different treebanks. It has been observed through empirical studies that while one algorithm may excel on a particular treebank, another algorithm may outperform it on a different treebank. The reasons behind the suitability of a specific algorithm for one treebank over another often remain unclear.\n\nIn this paper, we aim to illuminate this phenomenon by introducing the concept of an algorithm's inherent dependency displacement distribution. This concept serves to characterize the bias of an algorithm in terms of dependency displacement, which encompasses both the distance and direction of syntactic relations. Our research demonstrates a clear correlation between the similarity of an algorithm's inherent distribution and a treebank's displacement distribution with the algorithm's parsing performance on that specific treebank. Particularly noteworthy are the highly significant and substantial correlations observed for predominant sentence lengths in Universal Dependency treebanks.\n\nFurthermore, our findings indicate that a more detailed analysis of dependency displacement does not yield any meaningful correlations."}, "2404.11682": {"original_text": "  Automated methods are becoming increasingly integrated into studies of\nformative feedback on students' science explanation writing. Most of this work,\nhowever, addresses students' responses to short answer questions. We\ninvestigate automated feedback on students' science explanation essays, where\nstudents must articulate multiple ideas. Feedback is based on a rubric that\nidentifies the main ideas students are prompted to include in explanatory\nessays about the physics of energy and mass, given their experiments with a\nsimulated roller coaster. We have found that students generally improve on\nrevised versions of their essays. Here, however, we focus on two factors that\naffect the accuracy of the automated feedback. First, we find that the main\nideas in the rubric differ with respect to how much freedom they afford in\nexplanations of the idea, thus explanation of a natural law is relatively\nconstrained. Students have more freedom in how they explain complex relations\nthey observe in their roller coasters, such as transfer of different forms of\nenergy. Second, by tracing the automated decision process, we can diagnose when\na student's statement lacks sufficient clarity for the automated tool to\nassociate it more strongly with one of the main ideas above all others. This in\nturn provides an opportunity for teachers and peers to help students reflect on\nhow to state their ideas more clearly.\n", "rewritten_text": "Automated methods are increasingly being integrated into studies of formative feedback on students' science explanation writing. While much of this research focuses on students' responses to short answer questions, our study delves into automated feedback on students' science explanation essays. These essays require students to articulate multiple ideas and are evaluated based on a rubric that outlines the main ideas students should include in explanatory essays about the physics of energy and mass, drawing from their experiments with a simulated roller coaster.\n\nOur findings indicate that students generally show improvement in revised versions of their essays. However, our current focus is on two factors that impact the accuracy of automated feedback. Firstly, we observe that the main ideas outlined in the rubric vary in the degree of freedom they allow for explanations, with explanations of natural laws being relatively constrained compared to explanations of complex relations observed in roller coasters, such as the transfer of different forms of energy. Secondly, by examining the automated decision-making process, we can identify instances where a student's statement lacks clarity, making it challenging for the automated tool to associate it with one of the main ideas more prominently than others.\n\nThis analysis presents an opportunity for teachers and peers to assist students in refining their ideas and enhancing the clarity of their statements."}, "2211.08462": {"original_text": "  Recent years have seen an increasing trend in the volume of personal media\ncaptured by users, thanks to the advent of smartphones and smart glasses,\nresulting in large media collections. Despite conversation being an intuitive\nhuman-computer interface, current efforts focus mostly on single-shot natural\nlanguage based media retrieval to aid users query their media and re-live their\nmemories. This severely limits the search functionality as users can neither\nask follow-up queries nor obtain information without first formulating a\nsingle-turn query.\n  In this work, we propose dialogs for connected memories as a powerful tool to\nempower users to search their media collection through a multi-turn,\ninteractive conversation. Towards this, we collect a new task-oriented dialog\ndataset COMET, which contains $11.5k$ user<->assistant dialogs (totaling $103k$\nutterances), grounded in simulated personal memory graphs. We employ a\nresource-efficient, two-phase data collection pipeline that uses: (1) a novel\nmultimodal dialog simulator that generates synthetic dialog flows grounded in\nmemory graphs, and, (2) manual paraphrasing to obtain natural language\nutterances. We analyze COMET, formulate four main tasks to benchmark meaningful\nprogress, and adopt state-of-the-art language models as strong baselines, in\norder to highlight the multimodal challenges captured by our dataset.\n", "rewritten_text": "In recent years, there has been a growing trend in the amount of personal media captured by users, thanks to the introduction of smartphones and smart glasses, leading to the accumulation of large media collections. While conversation serves as an intuitive human-computer interface, current efforts primarily focus on single-shot natural language-based media retrieval to assist users in querying their media and reliving their memories. However, this approach significantly restricts the search functionality since users are unable to ask follow-up questions or obtain information without initially formulating a single-turn query.\n\nIn this study, we propose using dialogs for connected memories as a robust tool to enable users to search their media collection through a multi-turn, interactive conversation. To achieve this, we have created a new task-oriented dialog dataset called COMET, which comprises 11.5k user<->assistant dialogs totaling 103k utterances, all grounded in simulated personal memory graphs. We have implemented a resource-efficient, two-phase data collection process that involves: (1) a unique multimodal dialog simulator that generates synthetic dialog flows based on memory graphs, and (2) manual paraphrasing to generate natural language utterances. We have conducted an analysis of COMET, defined four main tasks to assess meaningful progress, and utilized state-of-the-art language models as strong baselines to showcase the multimodal challenges captured by our dataset."}, "1911.03668": {"original_text": "  Natural Language Inference (NLI) aims to determine the logic relationships\n(i.e., entailment, neutral and contradiction) between a pair of premise and\nhypothesis. Recently, the alignment mechanism effectively helps NLI by\ncapturing the aligned parts (i.e., the similar segments) in the sentence pairs,\nwhich imply the perspective of entailment and contradiction. However, these\naligned parts will sometimes mislead the judgment of neutral relations.\nIntuitively, NLI should rely more on multiple perspectives to form a holistic\nview to eliminate bias. In this paper, we propose the Multi-Perspective\nInferrer (MPI), a novel NLI model that reasons relationships from multiple\nperspectives associated with the three relationships. The MPI determines the\nperspectives of different parts of the sentences via a routing-by-agreement\npolicy and makes the final decision from a holistic view. Additionally, we\nintroduce an auxiliary supervised signal to ensure the MPI to learn the\nexpected perspectives. Experiments on SNLI and MultiNLI show that 1) the MPI\nachieves substantial improvements on the base model, which verifies the\nmotivation of multi-perspective inference; 2) visualized evidence verifies that\nthe MPI learns highly interpretable perspectives as expected; 3) more\nimportantly, the MPI is architecture-free and compatible with the powerful\nBERT.\n", "rewritten_text": "Natural Language Inference (NLI) aims to determine the logical relationships (i.e., entailment, neutral, and contradiction) between a pair of premises and a hypothesis. Recently, the alignment mechanism has proven effective in aiding NLI by identifying aligned parts (i.e., similar segments) in sentence pairs, which indicate entailment and contradiction. However, these aligned parts can sometimes lead to misjudgments in neutral relations. Intuitively, NLI should incorporate multiple perspectives to form a comprehensive view and eliminate bias.\n\nIn this paper, we introduce the Multi-Perspective Inferrer (MPI), a novel NLI model that considers relationships from various perspectives associated with the three types of relationships. The MPI determines different perspectives of sentence parts through a routing-by-agreement policy and makes decisions based on a holistic view. Additionally, we incorporate an auxiliary supervised signal to ensure that the MPI learns the expected perspectives.\n\nExperiments conducted on SNLI and MultiNLI demonstrate the following: 1) the MPI significantly enhances the base model, validating the importance of multi-perspective inference; 2) visual evidence confirms that the MPI learns interpretable perspectives as intended; and 3) notably, the MPI is architecture-free and compatible with the powerful BERT model."}, "2403.04212": {"original_text": "  Providing emotional support through dialogue systems is becoming increasingly\nimportant in today's world, as it can support both mental health and social\ninteractions in many conversation scenarios. Previous works have shown that\nusing persona is effective for generating empathetic and supportive responses.\nThey have often relied on pre-provided persona rather than inferring them\nduring conversations. However, it is not always possible to obtain a user\npersona before the conversation begins. To address this challenge, we propose\nPESS (Persona Extraction through Semantic Similarity), a novel framework that\ncan automatically infer informative and consistent persona from dialogues. We\ndevise completeness loss and consistency loss based on semantic similarity\nscores. The completeness loss encourages the model to generate missing persona\ninformation, and the consistency loss guides the model to distinguish between\nconsistent and inconsistent persona. Our experimental results demonstrate that\nhigh-quality persona information inferred by PESS is effective in generating\nemotionally supportive responses.\n", "rewritten_text": "In today's world, providing emotional support through dialogue systems is increasingly important as it can enhance both mental health and social interactions across various conversation scenarios. Previous studies have highlighted the effectiveness of using personas to generate empathetic and supportive responses. However, these studies have typically relied on predefined personas rather than deriving them dynamically during conversations. Yet, obtaining a user's persona before a conversation commences is not always feasible.\n\nTo tackle this challenge, we introduce PESS (Persona Extraction through Semantic Similarity), a novel framework designed to automatically infer informative and consistent personas from dialogues. We introduce completeness loss and consistency loss metrics based on semantic similarity scores. The completeness loss encourages the model to fill in missing persona details, while the consistency loss aids the model in distinguishing between consistent and inconsistent persona attributes. Our experimental findings illustrate that the high-quality persona information inferred by PESS effectively facilitates the generation of emotionally supportive responses."}, "1810.13391": {"original_text": "  During natural disasters and conflicts, information about what happened is\noften confusing, messy, and distributed across many sources. We would like to\nbe able to automatically identify relevant information and assemble it into\ncoherent narratives of what happened. To make this task accessible to neural\nmodels, we introduce Story Salads, mixtures of multiple documents that can be\ngenerated at scale. By exploiting the Wikipedia hierarchy, we can generate\nsalads that exhibit challenging inference problems. Story salads give rise to a\nnovel, challenging clustering task, where the objective is to group sentences\nfrom the same narratives. We demonstrate that simple bag-of-words similarity\nclustering falls short on this task and that it is necessary to take into\naccount global context and coherence.\n", "rewritten_text": "During natural disasters and conflicts, information regarding the events is often fragmented, confusing, and scattered across various sources. Our goal is to automate the process of identifying pertinent information and organizing it into coherent narratives. To facilitate this task for neural models, we introduce Story Salads, which are amalgamations of multiple documents that can be generated on a large scale. By leveraging the Wikipedia hierarchy, we can create salads that present complex inference challenges. Story salads introduce a new and demanding clustering objective, where the aim is to group sentences from the same narratives. We illustrate that a basic bag-of-words similarity clustering approach is inadequate for this task, emphasizing the importance of considering global context and coherence."}, "2407.02047": {"original_text": "  Multi-view counting (MVC) methods have shown their superiority over\nsingle-view counterparts, particularly in situations characterized by heavy\nocclusion and severe perspective distortions. However, hand-crafted heuristic\nfeatures and identical camera layout requirements in conventional MVC methods\nlimit their applicability and scalability in real-world scenarios.In this work,\nwe propose a concise 3D MVC framework called \\textbf{CountFormer}to elevate\nmulti-view image-level features to a scene-level volume representation and\nestimate the 3D density map based on the volume features. By incorporating a\ncamera encoding strategy, CountFormer successfully embeds camera parameters\ninto the volume query and image-level features, enabling it to handle various\ncamera layouts with significant differences.Furthermore, we introduce a feature\nlifting module capitalized on the attention mechanism to transform image-level\nfeatures into a 3D volume representation for each camera view. Subsequently,\nthe multi-view volume aggregation module attentively aggregates various\nmulti-view volumes to create a comprehensive scene-level volume representation,\nallowing CountFormer to handle images captured by arbitrary dynamic camera\nlayouts. The proposed method performs favorably against the state-of-the-art\napproaches across various widely used datasets, demonstrating its greater\nsuitability for real-world deployment compared to conventional MVC frameworks.\n", "rewritten_text": "The superiority of multi-view counting (MVC) methods over their single-view counterparts has been demonstrated, especially in scenarios with heavy occlusion and severe perspective distortions. However, the reliance on hand-crafted heuristic features and the need for identical camera layouts in traditional MVC methods restrict their applicability and scalability in real-world situations. \n\nIn this study, we present a concise 3D MVC framework named \\textbf{CountFormer} that aims to enhance multi-view image-level features to a scene-level volume representation and estimate the 3D density map based on these volume features. By integrating a camera encoding strategy, CountFormer effectively incorporates camera parameters into the volume query and image-level features, enabling it to accommodate various camera layouts with significant differences. \n\nAdditionally, we introduce a feature lifting module that leverages the attention mechanism to convert image-level features into a 3D volume representation for each camera view. Subsequently, the multi-view volume aggregation module attentively combines different multi-view volumes to generate a comprehensive scene-level volume representation, enabling CountFormer to handle images captured by diverse dynamic camera layouts. \n\nOur proposed method outperforms state-of-the-art approaches across multiple widely used datasets, showcasing its superior suitability for real-world deployment compared to traditional MVC frameworks."}, "2312.07530": {"original_text": "  Weakly supervised 3D object detection aims to learn a 3D detector with lower\nannotation cost, e.g., 2D labels. Unlike prior work which still relies on few\naccurate 3D annotations, we propose a framework to study how to leverage\nconstraints between 2D and 3D domains without requiring any 3D labels.\nSpecifically, we employ visual data from three perspectives to establish\nconnections between 2D and 3D domains. First, we design a feature-level\nconstraint to align LiDAR and image features based on object-aware regions.\nSecond, the output-level constraint is developed to enforce the overlap between\n2D and projected 3D box estimations. Finally, the training-level constraint is\nutilized by producing accurate and consistent 3D pseudo-labels that align with\nthe visual data. We conduct extensive experiments on the KITTI dataset to\nvalidate the effectiveness of the proposed three constraints. Without using any\n3D labels, our method achieves favorable performance against state-of-the-art\napproaches and is competitive with the method that uses 500-frame 3D\nannotations. Code will be made publicly available at\nhttps://github.com/kuanchihhuang/VG-W3D.\n", "rewritten_text": "Weakly supervised 3D object detection aims to develop a 3D detector with reduced annotation costs, such as utilizing 2D labels. In contrast to previous approaches that still depend on limited accurate 3D annotations, our framework introduces a novel method to explore the utilization of constraints between the 2D and 3D domains without the need for any 3D labels. Our approach involves leveraging visual data from three different perspectives to establish connections between the 2D and 3D domains.\n\nFirstly, we implement a feature-level constraint to align LiDAR and image features based on object-aware regions. Secondly, an output-level constraint is devised to ensure the alignment between 2D and projected 3D box estimations. Lastly, a training-level constraint is employed to generate precise and consistent 3D pseudo-labels that correspond with the visual data. Extensive experiments conducted on the KITTI dataset validate the effectiveness of these three constraints. Remarkably, our method achieves competitive performance against state-of-the-art approaches without the use of any 3D labels, and it even rivals methods that rely on 500-frame 3D annotations. The code for our method will be publicly accessible at https://github.com/kuanchihhuang/VG-W3D."}, "2404.09976": {"original_text": "  Recently, diffusion transformers have gained wide attention with its\nexcellent performance in text-to-image and text-to-vidoe models, emphasizing\nthe need for transformers as backbone for diffusion models. Transformer-based\nmodels have shown better generalization capability compared to CNN-based models\nfor general vision tasks. However, much less has been explored in the existing\nliterature regarding the capabilities of transformer-based diffusion backbones\nand expanding their generative prowess to other datasets. This paper focuses on\nenabling a single pre-trained diffusion transformer model to scale across\nmultiple datasets swiftly, allowing for the completion of diverse generative\ntasks using just one model. To this end, we propose DiffScaler, an efficient\nscaling strategy for diffusion models where we train a minimal amount of\nparameters to adapt to different tasks. In particular, we learn task-specific\ntransformations at each layer by incorporating the ability to utilize the\nlearned subspaces of the pre-trained model, as well as the ability to learn\nadditional task-specific subspaces, which may be absent in the pre-training\ndataset. As these parameters are independent, a single diffusion model with\nthese task-specific parameters can be used to perform multiple tasks\nsimultaneously. Moreover, we find that transformer-based diffusion models\nsignificantly outperform CNN-based diffusion models methods while performing\nfine-tuning over smaller datasets. We perform experiments on four unconditional\nimage generation datasets. We show that using our proposed method, a single\npre-trained model can scale up to perform these conditional and unconditional\ntasks, respectively, with minimal parameter tuning while performing as close as\nfine-tuning an entire diffusion model for that particular task.\n", "rewritten_text": "Recently, diffusion transformers have garnered significant attention due to their outstanding performance in text-to-image and text-to-video models, underscoring the importance of transformers as the backbone for diffusion models. Transformer-based models have demonstrated superior generalization capabilities compared to CNN-based models in various vision tasks. However, the existing literature has explored relatively little about the potential of transformer-based diffusion backbones and their ability to extend their generative capacity to diverse datasets.\n\nThis paper focuses on enabling a single pre-trained diffusion transformer model to efficiently scale across multiple datasets, facilitating the completion of a wide range of generative tasks using a unified model. To achieve this goal, we introduce DiffScaler, a streamlined scaling strategy for diffusion models that involves training a minimal number of parameters to adapt to different tasks. Specifically, we incorporate task-specific transformations at each layer by leveraging the learned subspaces of the pre-trained model and acquiring the capability to learn additional task-specific subspaces that may not be present in the pre-training dataset. Since these parameters operate independently, a single diffusion model equipped with these task-specific parameters can concurrently handle multiple tasks.\n\nFurthermore, our research reveals that transformer-based diffusion models outperform CNN-based diffusion models significantly when fine-tuning on smaller datasets. We conduct experiments on four unconditional image generation datasets, demonstrating that our proposed method enables a single pre-trained model to effectively scale for both conditional and unconditional tasks with minimal parameter adjustments, achieving performance comparable to fine-tuning an entire diffusion model for each specific task."}, "2406.06843": {"original_text": "  We introduce a data capture system and a new dataset named HO-Cap that can be\nused to study 3D reconstruction and pose tracking of hands and objects in\nvideos. The capture system uses multiple RGB-D cameras and a HoloLens headset\nfor data collection, avoiding the use of expensive 3D scanners or mocap\nsystems. We propose a semi-automatic method to obtain annotations of shape and\npose of hands and objects in the collected videos, which significantly reduces\nthe required annotation time compared to manual labeling. With this system, we\ncaptured a video dataset of humans using objects to perform different tasks, as\nwell as simple pick-and-place and handover of an object from one hand to the\nother, which can be used as human demonstrations for embodied AI and robot\nmanipulation research. Our data capture setup and annotation framework can be\nused by the community to reconstruct 3D shapes of objects and human hands and\ntrack their poses in videos.\n", "rewritten_text": "We present a new data capture system and dataset called HO-Cap, designed for studying 3D reconstruction and pose tracking of hands and objects in videos. The system utilizes multiple RGB-D cameras and a HoloLens headset for data collection, eliminating the need for expensive 3D scanners or mocap systems. We propose a semi-automatic method for obtaining annotations of hand and object shapes and poses in the recorded videos, significantly reducing the time required for annotation compared to manual labeling. Using this system, we have created a video dataset showcasing humans using objects for various tasks, including simple pick-and-place actions and handovers between hands. These demonstrations can serve as valuable resources for research in embodied AI and robot manipulation. Our data capture setup and annotation framework are available for use by the research community to reconstruct 3D shapes of objects and human hands and track their poses in videos."}, "2407.05769": {"original_text": "  In autonomous driving, LiDAR sensors are vital for acquiring 3D point clouds,\nproviding reliable geometric information. However, traditional sampling methods\nof preprocessing often ignore semantic features, leading to detail loss and\nground point interference in 3D object detection. To address this, we propose a\nmulti-branch two-stage 3D object detection framework using a Semantic-aware\nMulti-branch Sampling (SMS) module and multi-view consistency constraints. The\nSMS module includes random sampling, Density Equalization Sampling (DES) for\nenhancing distant objects, and Ground Abandonment Sampling (GAS) to focus on\nnon-ground points. The sampled multi-view points are processed through a\nConsistent KeyPoint Selection (CKPS) module to generate consistent keypoint\nmasks for efficient proposal sampling. The first-stage detector uses\nmulti-branch parallel learning with multi-view consistency loss for feature\naggregation, while the second-stage detector fuses multi-view data through a\nMulti-View Fusion Pooling (MVFP) module to precisely predict 3D objects. The\nexperimental results on the KITTI dataset and Waymo Open Dataset show that our\nmethod achieves excellent detection performance improvement for a variety of\nbackbones, especially for low-performance backbones with the simple network\nstructures.\n", "rewritten_text": "In the realm of autonomous driving, LiDAR sensors play a crucial role in capturing 3D point clouds to provide accurate geometric information. However, conventional preprocessing methods often overlook semantic features, resulting in a loss of detail and interference from ground points in 3D object detection. To tackle this issue, we introduce a novel multi-branch two-stage framework for 3D object detection that incorporates a Semantic-aware Multi-branch Sampling (SMS) module and multi-view consistency constraints.\n\nThe SMS module integrates random sampling, Density Equalization Sampling (DES) to enhance distant objects, and Ground Abandonment Sampling (GAS) to prioritize non-ground points. The sampled multi-view points undergo processing via a Consistent KeyPoint Selection (CKPS) module to generate consistent keypoint masks for efficient proposal sampling. The first-stage detector employs multi-branch parallel learning with multi-view consistency loss for feature aggregation, while the second-stage detector merges multi-view data using a Multi-View Fusion Pooling (MVFP) module to accurately predict 3D objects.\n\nExperimental results on the KITTI dataset and Waymo Open Dataset demonstrate that our approach significantly enhances detection performance across various backbones, particularly benefiting low-performance backbones with simpler network structures."}, "2211.0089": {"original_text": "  Few-shot learning problem focuses on recognizing unseen classes given a few\nlabeled images. In recent effort, more attention is paid to fine-grained\nfeature embedding, ignoring the relationship among different distance metrics.\nIn this paper, for the first time, we investigate the contributions of\ndifferent distance metrics, and propose an adaptive fusion scheme, bringing\nsignificant improvements in few-shot classification. We start from a naive\nbaseline of confidence summation and demonstrate the necessity of exploiting\nthe complementary property of different distance metrics. By finding the\ncompetition problem among them, built upon the baseline, we propose an Adaptive\nMetrics Module (AMM) to decouple metrics fusion into metric-prediction fusion\nand metric-losses fusion. The former encourages mutual complementary, while the\nlatter alleviates metric competition via multi-task collaborative learning.\nBased on AMM, we design a few-shot classification framework AMTNet, including\nthe AMM and the Global Adaptive Loss (GAL), to jointly optimize the few-shot\ntask and auxiliary self-supervised task, making the embedding features more\nrobust. In the experiment, the proposed AMM achieves 2% higher performance than\nthe naive metrics fusion module, and our AMTNet outperforms the\nstate-of-the-arts on multiple benchmark datasets.\n", "rewritten_text": "The few-shot learning problem focuses on recognizing unseen classes with only a few labeled images. Recent efforts have emphasized fine-grained feature embedding, but have overlooked the relationship among different distance metrics. This paper investigates the contributions of various distance metrics for the first time and introduces an adaptive fusion scheme that leads to significant improvements in few-shot classification. \n\nWe begin by establishing a baseline of confidence summation and demonstrate the importance of leveraging the complementary nature of different distance metrics. By addressing the competition issue among these metrics, we propose an Adaptive Metrics Module (AMM) that separates metrics fusion into metric-prediction fusion and metric-losses fusion. The former promotes mutual complementarity, while the latter mitigates metric competition through multi-task collaborative learning. \n\nBuilding upon the AMM, we develop a few-shot classification framework called AMTNet, which includes the AMM and the Global Adaptive Loss (GAL). This framework jointly optimizes the few-shot task and an auxiliary self-supervised task to enhance the robustness of embedding features. In experiments, the proposed AMM achieves a 2% higher performance compared to the naive metrics fusion module, and our AMTNet surpasses the state-of-the-art methods on multiple benchmark datasets."}, "2407.13520": {"original_text": "  3D deblurring reconstruction techniques have recently seen significant\nadvancements with the development of Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). Although these techniques can recover relatively\nclear 3D reconstructions from blurry image inputs, they still face limitations\nin handling severe blurring and complex camera motion. To address these issues,\nwe propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting\n(EaDeblur-GS), which integrates event camera data to enhance the robustness of\n3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE)\nnetwork to estimate Gaussian center deviations and using novel loss functions,\nEaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating\nperformance comparable to state-of-the-art methods.\n", "rewritten_text": "Recent advancements in 3D deblurring reconstruction techniques have been notable, particularly with the introduction of Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). While these methods show promise in generating clear 3D reconstructions from blurry image inputs, they still encounter challenges when dealing with severe blurring and intricate camera movements. In response to these limitations, we propose the Event-assisted 3D Deblur Reconstruction with Gaussian Splatting (EaDeblur-GS) approach. This method leverages event camera data to enhance the robustness of 3DGS against motion blur. By utilizing an Adaptive Deviation Estimator (ADE) network to predict Gaussian center deviations and incorporating innovative loss functions, EaDeblur-GS achieves high-quality 3D reconstructions in real-time. The performance of this approach is on par with current state-of-the-art methods."}, "2103.13678": {"original_text": "  Domain Adaptation is widely used in practical applications of neural machine\ntranslation, which aims to achieve good performance on both the general-domain\nand in-domain. However, the existing methods for domain adaptation usually\nsuffer from catastrophic forgetting, domain divergence, and model explosion. To\naddress these three problems, we propose a method of \"divide and conquer\" which\nis based on the importance of neurons or parameters in the translation model.\nIn our method, we first prune the model and only keep the important neurons or\nparameters, making them responsible for both general-domain and in-domain\ntranslation. Then we further train the pruned model supervised by the original\nunpruned model with the knowledge distillation method. Last we expand the model\nto the original size and fine-tune the added parameters for the in-domain\ntranslation. We conduct experiments on different languages and domains and the\nresults show that our method can achieve significant improvements compared with\nseveral strong baselines.\n", "rewritten_text": "Domain Adaptation is commonly utilized in practical applications of neural machine translation to achieve optimal performance in both general-domain and in-domain scenarios. However, existing methods for domain adaptation often encounter issues such as catastrophic forgetting, domain divergence, and model explosion. To tackle these challenges, we propose a \"divide and conquer\" approach that focuses on the importance of neurons or parameters in the translation model.\n\nIn our method, we initially prune the model, retaining only the crucial neurons or parameters responsible for general-domain and in-domain translation. Subsequently, we train the pruned model under the supervision of the original unpruned model using knowledge distillation. Finally, we expand the model back to its original size and fine-tune the added parameters for in-domain translation. Experimental evaluations across various languages and domains demonstrate that our method yields significant improvements compared to several strong baselines."}, "2003.11536": {"original_text": "  Estimating the actual head orientation from 2D images, with regard to its\nthree degrees of freedom, is a well known problem that is highly significant\nfor a large number of applications involving head pose knowledge. Consequently,\nthis topic has been tackled by a plethora of methods and algorithms the most\npart of which exploits neural networks. Machine learning methods, indeed,\nachieve accurate head rotation values yet require an adequate training stage\nand, to that aim, a relevant number of positive and negative examples. In this\npaper we take a different approach to this topic by using fractal coding theory\nand particularly Partitioned Iterated Function Systems to extract the fractal\ncode from the input head image and to compare this representation to the\nfractal code of a reference model through Hamming distance. According to\nexperiments conducted on both the BIWI and the AFLW2000 databases, the proposed\nPIFS based head pose estimation method provides accurate yaw/pitch/roll angular\nvalues, with a performance approaching that of state of the art of\nmachine-learning based algorithms and exceeding most of non-training based\napproaches.\n", "rewritten_text": "Estimating the actual head orientation from 2D images, considering its three degrees of freedom, is a well-known and crucial problem for numerous applications requiring knowledge of head pose. As a result, this issue has been addressed by a variety of methods and algorithms, with the majority leveraging neural networks. While machine learning methods can achieve precise head rotation values, they necessitate a thorough training phase and a substantial number of positive and negative examples. \n\nIn this study, we present a novel approach to this problem by utilizing fractal coding theory, specifically Partitioned Iterated Function Systems (PIFS), to extract the fractal code from the input head image. We then compare this representation to the fractal code of a reference model using Hamming distance. Through experiments conducted on both the BIWI and AFLW2000 databases, our proposed PIFS-based head pose estimation method delivers accurate yaw/pitch/roll angular values. Its performance closely rivals that of state-of-the-art machine-learning algorithms and surpasses most non-training-based approaches."}, "1706.02241": {"original_text": "  Analogy completion has been a popular task in recent years for evaluating the\nsemantic properties of word embeddings, but the standard methodology makes a\nnumber of assumptions about analogies that do not always hold, either in recent\nbenchmark datasets or when expanding into other domains. Through an analysis of\nanalogies in the biomedical domain, we identify three assumptions: that of a\nSingle Answer for any given analogy, that the pairs involved describe the Same\nRelationship, and that each pair is Informative with respect to the other. We\npropose modifying the standard methodology to relax these assumptions by\nallowing for multiple correct answers, reporting MAP and MRR in addition to\naccuracy, and using multiple example pairs. We further present BMASS, a novel\ndataset for evaluating linguistic regularities in biomedical embeddings, and\ndemonstrate that the relationships described in the dataset pose significant\nsemantic challenges to current word embedding methods.\n", "rewritten_text": "In recent years, analogy completion has become a popular task for evaluating the semantic properties of word embeddings. However, the standard methodology for this task is based on several assumptions about analogies that may not always hold true, especially when considering recent benchmark datasets or expanding into other domains. \n\nBy analyzing analogies in the biomedical domain, we have identified three key assumptions that the standard methodology makes: the assumption of a single answer for any given analogy, the assumption that the pairs involved describe the same relationship, and the assumption that each pair is informative with respect to the other. \n\nTo address these limitations, we propose modifying the standard methodology by allowing for multiple correct answers, reporting Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) in addition to accuracy, and using multiple example pairs. \n\nFurthermore, we introduce BMASS, a novel dataset designed for evaluating linguistic regularities in biomedical embeddings. Our analysis demonstrates that the relationships described in the BMASS dataset present significant semantic challenges to current word embedding methods."}, "2312.16894": {"original_text": "  The widespread usage of cars and other large, heavy vehicles necessitates the\ndevelopment of an effective parking infrastructure. Additionally, algorithms\nfor detection and recognition of number plates are often used to identify\nautomobiles all around the world where standardized plate sizes and fonts are\nenforced, making recognition an effortless task. As a result, both kinds of\ndata can be combined to develop an intelligent parking system focuses on the\ntechnology of Automatic Number Plate Recognition (ANPR). Retrieving characters\nfrom an inputted number plate image is the sole purpose of ANPR which is a\ncostly procedure. In this article, we propose Chaurah, a minimal cost ANPR\nsystem that relies on a Raspberry Pi 3 that was specifically created for\nparking facilities. The system employs a dual-stage methodology, with the first\nstage being an ANPR system which makes use of two convolutional neural networks\n(CNNs). The primary locates and recognises license plates from a vehicle image,\nwhile the secondary performs Optical Character Recognition (OCR) to identify\nindividualized numbers from the number plate. An application built with Flutter\nand Firebase for database administration and license plate record comparison\nmakes up the second component of the overall solution. The application also\nacts as an user-interface for the billing mechanism based on parking time\nduration resulting in an all-encompassing software deployment of the study.\n", "rewritten_text": "The widespread use of cars and other large, heavy vehicles necessitates the development of an effective parking infrastructure. Algorithms for the detection and recognition of number plates are commonly employed to identify automobiles worldwide, where standardized plate sizes and fonts are enforced, simplifying the recognition process. Consequently, these two types of data can be integrated to create an intelligent parking system that focuses on Automatic Number Plate Recognition (ANPR) technology.\n\nThe primary objective of ANPR is to extract characters from an inputted number plate image, which can be a costly process. In this article, we introduce Chaurah, a low-cost ANPR system designed for parking facilities, utilizing a Raspberry Pi 3. The system operates on a dual-stage methodology, with the first stage employing two convolutional neural networks (CNNs) to locate and recognize license plates in vehicle images. The second stage involves Optical Character Recognition (OCR) to identify individualized numbers from the license plate.\n\nComplementing the ANPR system is an application developed using Flutter and Firebase for database management and license plate record comparison. This application also serves as a user interface for the billing mechanism based on parking duration, resulting in a comprehensive software deployment for the study."}, "2110.13032": {"original_text": "  Though there has been a large body of recent works in language modeling (LM)\nfor high resource languages such as English and Chinese, the area is still\nunexplored for low resource languages like Bengali and Hindi. We propose an end\nto end trainable memory efficient CNN architecture named CoCNN to handle\nspecific characteristics such as high inflection, morphological richness,\nflexible word order and phonetical spelling errors of Bengali and Hindi. In\nparticular, we introduce two learnable convolutional sub-models at word and at\nsentence level that are end to end trainable. We show that state-of-the-art\n(SOTA) Transformer models including pretrained BERT do not necessarily yield\nthe best performance for Bengali and Hindi. CoCNN outperforms pretrained BERT\nwith 16X less parameters, and it achieves much better performance than SOTA\nLSTM models on multiple real-world datasets. This is the first study on the\neffectiveness of different architectures drawn from three deep learning\nparadigms - Convolution, Recurrent, and Transformer neural nets for modeling\ntwo widely used languages, Bengali and Hindi.\n", "rewritten_text": "While there has been a significant amount of recent work in language modeling (LM) for high-resource languages such as English and Chinese, the field remains largely unexplored for low-resource languages like Bengali and Hindi. Our proposal introduces an end-to-end trainable, memory-efficient CNN architecture called CoCNN, designed to address specific characteristics unique to Bengali and Hindi, such as high inflection, morphological richness, flexible word order, and phonetic spelling errors. Specifically, we incorporate two learnable convolutional sub-models at the word and sentence levels, both of which are end-to-end trainable.\n\nOur research demonstrates that state-of-the-art (SOTA) Transformer models, including pretrained BERT, do not consistently deliver optimal performance for Bengali and Hindi. CoCNN surpasses pretrained BERT with 16 times fewer parameters and significantly outperforms SOTA LSTM models across multiple real-world datasets. This study represents the first exploration of the effectiveness of different architectures derived from three deep learning paradigms - Convolutional, Recurrent, and Transformer neural networks - in modeling two widely used languages, Bengali and Hindi."}, "1305.4537": {"original_text": "  We describe a method for visual object detection based on an ensemble of\noptimized decision trees organized in a cascade of rejectors. The trees use\npixel intensity comparisons in their internal nodes and this makes them able to\nprocess image regions very fast. Experimental analysis is provided through a\nface detection problem. The obtained results are encouraging and demonstrate\nthat the method has practical value. Additionally, we analyse its sensitivity\nto noise and show how to perform fast rotation invariant object detection.\nComplete source code is provided at https://github.com/nenadmarkus/pico.\n", "rewritten_text": "We present a method for visual object detection that utilizes an ensemble of optimized decision trees arranged in a cascade of rejectors. These trees employ pixel intensity comparisons in their internal nodes, enabling them to swiftly process image regions. An experimental analysis is conducted using a face detection scenario, yielding promising results that underscore the practical utility of the method. Furthermore, we assess its sensitivity to noise and outline a method for rapid rotation-invariant object detection. The complete source code can be accessed at https://github.com/nenadmarkus/pico."}, "2311.18835": {"original_text": "  Empowering models to dynamically accomplish tasks specified through natural\nlanguage instructions represents a promising path toward more capable and\ngeneral artificial intelligence. In this work, we introduce InstructSeq, an\ninstruction-conditioned multi-modal modeling framework that unifies diverse\nvision tasks through flexible natural language control and handling of both\nvisual and textual data. InstructSeq employs a multimodal transformer\narchitecture encompassing visual, language, and sequential modeling. We utilize\na visual encoder to extract image features and a text encoder to encode\ninstructions. An autoregressive transformer fuses the representations and\ngenerates sequential task outputs. By training with LLM-generated natural\nlanguage instructions, InstructSeq acquires a strong comprehension of free-form\ninstructions for specifying visual tasks. This provides an intuitive interface\nfor directing capabilities using flexible natural instructions. Without any\ntask-specific tuning, InstructSeq achieves compelling performance on semantic\nsegmentation, referring expression segmentation/comprehension, and image\ncaptioning. The flexible control and multi-task unification empower the model\nwith more human-like versatility and generalizability for computer vision. The\ncode will be released soon at https://github.com/rongyaofang/InstructSeq.\n", "rewritten_text": "Empowering models to dynamically accomplish tasks specified through natural language instructions represents a promising path towards more capable and general artificial intelligence. In this study, we introduce InstructSeq, an instruction-conditioned multi-modal modeling framework that unifies diverse vision tasks through flexible natural language control and handling of both visual and textual data. InstructSeq utilizes a multimodal transformer architecture that integrates visual, language, and sequential modeling. A visual encoder is employed to extract image features, while a text encoder encodes instructions. An autoregressive transformer combines the representations and generates sequential task outputs. Through training with LLM-generated natural language instructions, InstructSeq develops a strong understanding of free-form instructions for specifying visual tasks, offering an intuitive interface for directing capabilities using flexible natural instructions. InstructSeq achieves impressive performance on semantic segmentation, referring expression segmentation/comprehension, and image captioning without the need for task-specific tuning. The model's flexible control and multi-task unification enhance its human-like versatility and generalizability for computer vision tasks. The code for InstructSeq will soon be available at https://github.com/rongyaofang/InstructSeq."}, "2008.07018": {"original_text": "  We present AutoPose, a novel neural architecture search(NAS) framework that\nis capable of automatically discovering multiple parallel branches of\ncross-scale connections towards accurate and high-resolution 2D human pose\nestimation. Recently, high-performance hand-crafted convolutional networks for\npose estimation show growing demands on multi-scale fusion and high-resolution\nrepresentations. However, current NAS works exhibit limited flexibility on\nscale searching, they dominantly adopt simplified search spaces of\nsingle-branch architectures. Such simplification limits the fusion of\ninformation at different scales and fails to maintain high-resolution\nrepresentations. The presentedAutoPose framework is able to search for\nmulti-branch scales and network depth, in addition to the cell-level\nmicrostructure. Motivated by the search space, a novel bi-level optimization\nmethod is presented, where the network-level architecture is searched via\nreinforcement learning, and the cell-level search is conducted by the\ngradient-based method. Within 2.5 GPU days, AutoPose is able to find very\ncompetitive architectures on the MS COCO dataset, that are also transferable to\nthe MPII dataset. Our code is available at\nhttps://github.com/VITA-Group/AutoPose.\n", "rewritten_text": "We introduce AutoPose, a cutting-edge neural architecture search (NAS) framework that automatically uncovers multiple parallel branches of cross-scale connections to achieve precise and high-resolution 2D human pose estimation. Recent advancements in hand-crafted convolutional networks for pose estimation have highlighted the need for effective multi-scale fusion and high-resolution representations. However, existing NAS approaches have limited flexibility in scale exploration, primarily focusing on simplified search spaces of single-branch architectures. This simplification hinders the integration of information across different scales and compromises the maintenance of high-resolution representations.\n\nThe AutoPose framework presented here can explore multi-branch scales and network depth, along with cell-level microstructure. Inspired by the search space, we introduce a novel bi-level optimization method. This method involves searching for network-level architecture through reinforcement learning and conducting cell-level search using a gradient-based approach. In just 2.5 GPU days, AutoPose identifies highly competitive architectures on the MS COCO dataset, which can also be applied effectively to the MPII dataset. Our code is accessible at https://github.com/VITA-Group/AutoPose."}, "1902.053": {"original_text": "  Deep learning, due to its unprecedented success in tasks such as image\nclassification, has emerged as a new tool in image reconstruction with\npotential to change the field. In this paper we demonstrate a crucial\nphenomenon: deep learning typically yields unstablemethods for image\nreconstruction. The instabilities usually occur in several forms: (1) tiny,\nalmost undetectable perturbations, both in the image and sampling domain, may\nresult in severe artefacts in the reconstruction, (2) a small structural\nchange, for example a tumour, may not be captured in the reconstructed image\nand (3) (a counterintuitive type of instability) more samples may yield poorer\nperformance. Our new stability test with algorithms and easy to use software\ndetects the instability phenomena. The test is aimed at researchers to test\ntheir networks for instabilities and for government agencies, such as the Food\nand Drug Administration (FDA), to secure safe use of deep learning methods.\n", "rewritten_text": "Deep learning, with its unprecedented success in tasks like image classification, has emerged as a powerful tool in image reconstruction, potentially revolutionizing the field. This paper presents a crucial finding: deep learning often produces unstable methods for image reconstruction. These instabilities manifest in various ways, including: (1) subtle perturbations, both in the image and sampling domains, leading to significant artifacts in the reconstruction, (2) failure to capture small structural changes, such as tumors, in the reconstructed image, and (3) a counterintuitive form of instability where increased samples result in poorer performance. Our novel stability test, along with user-friendly algorithms and software, can detect these instability phenomena. The test is designed for researchers to assess their networks for instabilities and for regulatory bodies like the Food and Drug Administration (FDA) to ensure the safe implementation of deep learning methods."}, "2203.08543": {"original_text": "  Deep Metric Learning (DML) proposes to learn metric spaces which encode\nsemantic similarities as embedding space distances. These spaces should be\ntransferable to classes beyond those seen during training. Commonly, DML\nmethods task networks to solve contrastive ranking tasks defined over binary\nclass assignments. However, such approaches ignore higher-level semantic\nrelations between the actual classes. This causes learned embedding spaces to\nencode incomplete semantic context and misrepresent the semantic relation\nbetween classes, impacting the generalizability of the learned metric space. To\ntackle this issue, we propose a language guidance objective for visual\nsimilarity learning. Leveraging language embeddings of expert- and\npseudo-classnames, we contextualize and realign visual representation spaces\ncorresponding to meaningful language semantics for better semantic consistency.\nExtensive experiments and ablations provide a strong motivation for our\nproposed approach and show language guidance offering significant,\nmodel-agnostic improvements for DML, achieving competitive and state-of-the-art\nresults on all benchmarks. Code available at\nhttps://github.com/ExplainableML/LanguageGuidance_for_DML.\n", "rewritten_text": "Deep Metric Learning (DML) aims to develop metric spaces that capture semantic similarities by representing them as distances in an embedding space. These spaces should be capable of generalizing to classes not encountered during training. Traditional DML methods involve training networks to perform contrastive ranking tasks based on binary class assignments. However, these approaches often overlook the higher-level semantic relationships between the actual classes. As a result, the learned embedding spaces may lack complete semantic context and inaccurately represent the semantic connections between classes, thereby affecting the overall generalizability of the metric space.\n\nTo address this limitation, we introduce a language guidance objective for enhancing visual similarity learning. By utilizing language embeddings of expert- and pseudo-class names, we contextualize and align visual representation spaces according to meaningful language semantics to ensure better semantic consistency. Through extensive experiments and ablations, we demonstrate the effectiveness of our proposed approach and highlight the significant, model-agnostic improvements that language guidance offers for DML. Our method achieves competitive and state-of-the-art results across all benchmarks. The code is available at https://github.com/ExplainableML/LanguageGuidance_for_DML."}, "1411.5057": {"original_text": "  In this paper, we propose a novel algorithm for analysis-based sparsity\nreconstruction. It can solve the generalized problem by structured sparsity\nregularization with an orthogonal basis and total variation regularization. The\nproposed algorithm is based on the iterative reweighted least squares (IRLS)\nmodel, which is further accelerated by the preconditioned conjugate gradient\nmethod. The convergence rate of the proposed algorithm is almost the same as\nthat of the traditional IRLS algorithms, that is, exponentially fast. Moreover,\nwith the specifically devised preconditioner, the computational cost for each\niteration is significantly less than that of traditional IRLS algorithms, which\nenables our approach to handle large scale problems. In addition to the fast\nconvergence, it is straightforward to apply our method to standard sparsity,\ngroup sparsity, overlapping group sparsity and TV based problems. Experiments\nare conducted on a practical application: compressive sensing magnetic\nresonance imaging. Extensive results demonstrate that the proposed algorithm\nachieves superior performance over 14 state-of-the-art algorithms in terms of\nboth accuracy and computational cost.\n", "rewritten_text": "This paper introduces a novel algorithm for analysis-based sparsity reconstruction. The algorithm addresses the generalized problem through structured sparsity regularization using an orthogonal basis and total variation regularization. It is built on the iterative reweighted least squares (IRLS) model, enhanced by the preconditioned conjugate gradient method for faster convergence. The proposed algorithm demonstrates a convergence rate comparable to traditional IRLS algorithms, with exponential speed. Notably, the computational cost per iteration is significantly reduced compared to traditional IRLS algorithms, making it suitable for large-scale problems. The algorithm is versatile, applicable to standard sparsity, group sparsity, overlapping group sparsity, and TV-based problems. Experimental validation is performed on compressive sensing magnetic resonance imaging, showing superior performance in accuracy and computational efficiency compared to 14 state-of-the-art algorithms."}, "2211.09809": {"original_text": "  Animating portraits using speech has received growing attention in recent\nyears, with various creative and practical use cases. An ideal generated video\nshould have good lip sync with the audio, natural facial expressions and head\nmotions, and high frame quality. In this work, we present SPACE, which uses\nspeech and a single image to generate high-resolution, and expressive videos\nwith realistic head pose, without requiring a driving video. It uses a\nmulti-stage approach, combining the controllability of facial landmarks with\nthe high-quality synthesis power of a pretrained face generator. SPACE also\nallows for the control of emotions and their intensities. Our method\noutperforms prior methods in objective metrics for image quality and facial\nmotions and is strongly preferred by users in pair-wise comparisons. The\nproject website is available at https://deepimagination.cc/SPACE/\n", "rewritten_text": "Animating portraits through speech has garnered increasing attention in recent years due to its diverse creative and practical applications. An optimal generated video should exhibit seamless lip sync with the audio, authentic facial expressions and head movements, and high frame quality. In this study, we introduce SPACE, a novel technique that leverages speech and a single image to produce high-resolution, emotionally expressive videos with lifelike head poses, all without the need for a reference video. SPACE employs a multi-stage methodology that merges the controllability of facial landmarks with the robust synthesis capabilities of a pretrained face generator. Furthermore, SPACE enables users to manipulate emotions and their intensities. Our approach surpasses previous methods in objective metrics related to image quality and facial movements, and is notably preferred by users in pairwise comparisons. For more information, please visit the project website at https://deepimagination.cc/SPACE/."}, "2307.03602": {"original_text": "  Stereo vision systems have become popular in computer vision applications,\nsuch as 3D reconstruction, object tracking, and autonomous navigation. However,\ntraditional stereo vision systems that use rectilinear lenses may not be\nsuitable for certain scenarios due to their limited field of view. This has led\nto the popularity of vision systems based on one or multiple fisheye cameras in\ndifferent orientations, which can provide a field of view of 180x180 degrees or\nmore. However, fisheye cameras introduce significant distortion at the edges\nthat affects the accuracy of stereo matching and depth estimation. To overcome\nthese limitations, this paper proposes a method for distortion-removal and\ndepth estimation analysis for stereovision system using orthogonally divergent\nfisheye cameras (ODFC). The proposed method uses two virtual pinhole cameras\n(VPC), each VPC captures a small portion of the original view and presents it\nwithout any lens distortions, emulating the behavior of a pinhole camera. By\ncarefully selecting the captured regions, it is possible to create a stereo\npair using two VPCs. The performance of the proposed method is evaluated in\nboth simulation using virtual environment and experiments using real cameras\nand their results compared to stereo cameras with parallel optical axes. The\nresults demonstrate the effectiveness of the proposed method in terms of\ndistortion removal and depth estimation accuracy.\n", "rewritten_text": "Stereo vision systems have gained popularity in computer vision applications, such as 3D reconstruction, object tracking, and autonomous navigation. However, traditional stereo vision systems utilizing rectilinear lenses may not be suitable for certain scenarios due to their limited field of view. This limitation has led to the rise in popularity of vision systems based on one or multiple fisheye cameras in various orientations, offering a field of view of 180x180 degrees or more. Nevertheless, fisheye cameras introduce significant distortion at the edges, impacting the accuracy of stereo matching and depth estimation.\n\nTo address these challenges, this paper proposes a method for distortion removal and depth estimation analysis for a stereo vision system using orthogonally divergent fisheye cameras (ODFC). The proposed method involves the use of two virtual pinhole cameras (VPC), with each VPC capturing a small portion of the original view and presenting it without any lens distortions, mimicking the behavior of a pinhole camera. By carefully selecting the captured regions, a stereo pair can be created using two VPCs.\n\nThe performance of the proposed method is evaluated through simulations in a virtual environment and experiments using real cameras. The results are then compared to stereo cameras with parallel optical axes. The findings demonstrate the effectiveness of the proposed method in terms of distortion removal and depth estimation accuracy."}, "2410.11816": {"original_text": "  The automatic assembly problem has attracted increasing interest due to its\ncomplex challenges that involve 3D representation. This paper introduces\nJigsaw++, a novel generative method designed to tackle the multifaceted\nchallenges of reconstruction for the reassembly problem. Existing approach\nfocusing primarily on piecewise information for both part and fracture\nassembly, often overlooking the integration of complete object prior. Jigsaw++\ndistinguishes itself by learning a category-agnostic shape prior of complete\nobjects. It employs the proposed \"retargeting\" strategy that effectively\nleverages the output of any existing assembly method to generate complete shape\nreconstructions. This capability allows it to function orthogonally to the\ncurrent methods. Through extensive evaluations on Breaking Bad dataset and\nPartNet, Jigsaw++ has demonstrated its effectiveness, reducing reconstruction\nerrors and enhancing the precision of shape reconstruction, which sets a new\ndirection for future reassembly model developments.\n", "rewritten_text": "The automatic assembly problem has garnered increasing interest due to its complex challenges involving 3D representation. This paper introduces Jigsaw++, a novel generative method designed to address the multifaceted challenges of reconstruction for the reassembly problem. Existing approaches primarily focus on piecewise information for both part and fracture assembly, often neglecting the integration of complete object priors. Jigsaw++ sets itself apart by learning a category-agnostic shape prior of complete objects. It utilizes the proposed \"retargeting\" strategy, which effectively leverages the output of any existing assembly method to generate complete shape reconstructions. This capability enables it to operate independently of current methods. Through extensive evaluations on the Breaking Bad dataset and PartNet, Jigsaw++ has demonstrated its effectiveness in reducing reconstruction errors and enhancing the precision of shape reconstruction, paving the way for future developments in reassembly models."}, "1806.05658": {"original_text": "  Seq2seq learning has produced promising results on summarization. However, in\nmany cases, system summaries still struggle to keep the meaning of the original\nintact. They may miss out important words or relations that play critical roles\nin the syntactic structure of source sentences. In this paper, we present\nstructure-infused copy mechanisms to facilitate copying important words and\nrelations from the source sentence to summary sentence. The approach naturally\ncombines source dependency structure with the copy mechanism of an abstractive\nsentence summarizer. Experimental results demonstrate the effectiveness of\nincorporating source-side syntactic information in the system, and our proposed\napproach compares favorably to state-of-the-art methods.\n", "rewritten_text": "Seq2seq learning has shown promising results in summarization tasks. However, in many instances, system-generated summaries struggle to preserve the original meaning. They often overlook crucial words or relationships that are essential to the syntactic structure of the source sentences. In this paper, we introduce structure-infused copy mechanisms designed to enhance the retention of important words and relationships from the source sentence in the summary. This approach seamlessly integrates the source dependency structure with the copy mechanism of an abstractive sentence summarizer. Experimental results illustrate the effectiveness of incorporating source-side syntactic information into the system, and our proposed approach outperforms state-of-the-art methods."}, "2108.12545": {"original_text": "  Training deep networks for semantic segmentation requires large amounts of\nlabeled training data, which presents a major challenge in practice, as\nlabeling segmentation masks is a highly labor-intensive process. To address\nthis issue, we present a framework for semi-supervised and domain-adaptive\nsemantic segmentation, which is enhanced by self-supervised monocular depth\nestimation (SDE) trained only on unlabeled image sequences.\n  In particular, we utilize SDE as an auxiliary task comprehensively across the\nentire learning framework: First, we automatically select the most useful\nsamples to be annotated for semantic segmentation based on the correlation of\nsample diversity and difficulty between SDE and semantic segmentation. Second,\nwe implement a strong data augmentation by mixing images and labels using the\ngeometry of the scene. Third, we transfer knowledge from features learned\nduring SDE to semantic segmentation by means of transfer and multi-task\nlearning. And fourth, we exploit additional labeled synthetic data with\nCross-Domain DepthMix and Matching Geometry Sampling to align synthetic and\nreal data.\n  We validate the proposed model on the Cityscapes dataset, where all four\ncontributions demonstrate significant performance gains, and achieve\nstate-of-the-art results for semi-supervised semantic segmentation as well as\nfor semi-supervised domain adaptation. In particular, with only 1/30 of the\nCityscapes labels, our method achieves 92% of the fully-supervised baseline\nperformance and even 97% when exploiting additional data from GTA. The source\ncode is available at\nhttps://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth.\n", "rewritten_text": "Training deep networks for semantic segmentation poses a significant challenge due to the need for extensive labeled training data. Labeling segmentation masks is a labor-intensive process, making it difficult to obtain the required data. To tackle this challenge, we introduce a framework for semi-supervised and domain-adaptive semantic segmentation. This framework leverages self-supervised monocular depth estimation (SDE), trained solely on unlabeled image sequences.\n\nOur approach integrates SDE as an auxiliary task throughout the learning process. Firstly, we use SDE to automatically select the most informative samples for annotation in semantic segmentation, based on the correlation of sample diversity and difficulty between SDE and semantic segmentation. Secondly, we enhance data augmentation by blending images and labels using scene geometry. Thirdly, we transfer knowledge from features learned during SDE to semantic segmentation through transfer and multi-task learning. Lastly, we utilize additional labeled synthetic data with Cross-Domain DepthMix and Matching Geometry Sampling to align synthetic and real data.\n\nWe evaluate our model on the Cityscapes dataset, where all these contributions lead to significant performance improvements. Our method achieves state-of-the-art results for semi-supervised semantic segmentation and domain adaptation. Notably, with only 1/30 of the Cityscapes labels, our approach achieves 92% of the fully-supervised baseline performance, increasing to 97% when incorporating additional data from GTA. The source code can be accessed at https://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth."}, "2405.12833": {"original_text": "  Automatic radiology report generation can alleviate the workload for\nphysicians and minimize regional disparities in medical resources, therefore\nbecoming an important topic in the medical image analysis field. It is a\nchallenging task, as the computational model needs to mimic physicians to\nobtain information from multi-modal input data (i.e., medical images, clinical\ninformation, medical knowledge, etc.), and produce comprehensive and accurate\nreports. Recently, numerous works emerged to address this issue using deep\nlearning-based methods, such as transformers, contrastive learning, and\nknowledge-base construction. This survey summarizes the key techniques\ndeveloped in the most recent works and proposes a general workflow for deep\nlearning-based report generation with five main components, including\nmulti-modality data acquisition, data preparation, feature learning, feature\nfusion/interaction, and report generation. The state-of-the-art methods for\neach of these components are highlighted. Additionally, training strategies,\npublic datasets, evaluation methods, current challenges, and future directions\nin this field are summarized. We have also conducted a quantitative comparison\nbetween different methods under the same experimental setting. This is the most\nup-to-date survey that focuses on multi-modality inputs and data fusion for\nradiology report generation. The aim is to provide comprehensive and rich\ninformation for researchers interested in automatic clinical report generation\nand medical image analysis, especially when using multimodal inputs, and assist\nthem in developing new algorithms to advance the field.\n", "rewritten_text": "Automatic radiology report generation is an important topic in the field of medical image analysis, as it can help alleviate physicians' workload and reduce regional disparities in medical resources. This task is challenging because the computational model must replicate physicians' abilities to extract information from various sources, including medical images, clinical data, and medical knowledge, in order to generate accurate and comprehensive reports. Recent advancements in deep learning, such as transformers, contrastive learning, and knowledge-base construction, have been utilized to tackle this issue.\n\nThis survey provides an overview of the key techniques employed in recent works and presents a general workflow for deep learning-based report generation. The workflow consists of five main components: multi-modality data acquisition, data preparation, feature learning, feature fusion/interaction, and report generation. State-of-the-art methods for each component are discussed, along with training strategies, public datasets, evaluation methods, current challenges, and future directions in the field.\n\nFurthermore, a quantitative comparison of different methods under the same experimental conditions is conducted in this survey, making it the most up-to-date resource focusing on multi-modality inputs and data fusion for radiology report generation. The goal of this survey is to offer comprehensive information to researchers interested in automatic clinical report generation and medical image analysis, particularly when dealing with multimodal inputs, to aid them in developing innovative algorithms and advancing the field."}, "1809.10417": {"original_text": "  The tracking-by-detection framework receives growing attentions through the\nintegration with the Convolutional Neural Networks (CNNs). Existing\ntracking-by-detection based methods, however, fail to track objects with severe\nappearance variations. This is because the traditional convolutional operation\nis performed on fixed grids, and thus may not be able to find the correct\nresponse while the object is changing pose or under varying environmental\nconditions. In this paper, we propose a deformable convolution layer to enrich\nthe target appearance representations in the tracking-by-detection framework.\nWe aim to capture the target appearance variations via deformable convolution,\nwhich adaptively enhances its original features. In addition, we also propose a\ngated fusion scheme to control how the variations captured by the deformable\nconvolution affect the original appearance. The enriched feature representation\nthrough deformable convolution facilitates the discrimination of the CNN\nclassifier on the target object and background. Extensive experiments on the\nstandard benchmarks show that the proposed tracker performs favorably against\nstate-of-the-art methods.\n", "rewritten_text": "The tracking-by-detection framework has garnered increasing attention due to its integration with Convolutional Neural Networks (CNNs). However, existing tracking-by-detection methods struggle to track objects with significant appearance variations. This limitation arises from the fact that traditional convolutional operations are conducted on fixed grids, making it challenging to accurately detect objects that change pose or are subject to varying environmental conditions. \n\nIn this study, we introduce a deformable convolution layer to enhance the representation of target appearances within the tracking-by-detection framework. Our goal is to capture the variations in target appearance by utilizing deformable convolution, which dynamically enhances the original features. Additionally, we propose a gated fusion scheme to regulate how the variations identified by the deformable convolution impact the original appearance. \n\nThe enriched feature representation achieved through deformable convolution aids in distinguishing the target object from the background, thereby improving the performance of the CNN classifier. Extensive experiments conducted on standard benchmarks demonstrate that our proposed tracker outperforms current state-of-the-art methods."}, "2111.15603": {"original_text": "  Modern neural networks are able to perform at least as well as humans in\nnumerous tasks involving object classification and image generation. However,\nsmall perturbations which are imperceptible to humans may significantly degrade\nthe performance of well-trained deep neural networks. We provide a\nDistributionally Robust Optimization (DRO) framework which integrates\nhuman-based image quality assessment methods to design optimal attacks that are\nimperceptible to humans but significantly damaging to deep neural networks.\nThrough extensive experiments, we show that our attack algorithm generates\nbetter-quality (less perceptible to humans) attacks than other state-of-the-art\nhuman imperceptible attack methods. Moreover, we demonstrate that DRO training\nusing our optimally designed human imperceptible attacks can improve group\nfairness in image classification. Towards the end, we provide an algorithmic\nimplementation to speed up DRO training significantly, which could be of\nindependent interest.\n", "rewritten_text": "Modern neural networks can match or exceed human performance in various tasks such as object classification and image generation. However, even imperceptible perturbations to humans can significantly impact the performance of well-trained deep neural networks. To address this issue, we introduce a Distributionally Robust Optimization (DRO) framework that incorporates human-based image quality assessment methods to create optimal attacks that are undetectable to humans yet highly detrimental to deep neural networks. Our experimental results demonstrate that our attack algorithm produces attacks of superior quality (less perceptible to humans) compared to other cutting-edge human-imperceptible attack methods. Furthermore, we illustrate that training with our optimally designed human-imperceptible attacks through DRO can enhance group fairness in image classification. Lastly, we present an algorithmic implementation to expedite DRO training significantly, which may be of independent interest."}, "2404.00829": {"original_text": "  Human writers often bookend their writing with ending sentences that relate\nback to the beginning sentences in order to compose a satisfying narrative that\n\"closes the loop.\" Motivated by this observation, we propose RENarGen, a\ncontrollable story-generation paradigm that generates narratives by ensuring\nthe first and last sentences are related and then infilling the middle\nsentences. Our contributions include an initial exploration of how various\nmethods of bookending from Narratology affect language modeling for stories.\nAutomatic and human evaluations indicate RENarGen produces better stories with\nmore narrative closure than current autoregressive models.\n", "rewritten_text": "Human writers frequently conclude their writing by connecting the ending sentences back to the beginning sentences to create a cohesive narrative that \"closes the loop.\" Inspired by this practice, we introduce RENarGen, a controllable story-generation approach that constructs narratives by ensuring a thematic link between the first and last sentences before filling in the middle content. Our research explores the impact of different bookending techniques from Narratology on language modeling for storytelling. Both automatic and human evaluations demonstrate that RENarGen generates superior stories with enhanced narrative closure compared to existing autoregressive models."}, "1711.01062": {"original_text": "  With the development of depth cameras such as Kinect and Intel Realsense,\nRGB-D based human detection receives continuous research attention due to its\nusage in a variety of applications. In this paper, we propose a new\nMulti-Glimpse LSTM (MG-LSTM) network, in which multi-scale contextual\ninformation is sequentially integrated to promote the human detection\nperformance. Furthermore, we propose a feature fusion strategy based on our\nMG-LSTM network to better incorporate the RGB and depth information. To the\nbest of our knowledge, this is the first attempt to utilize LSTM structure for\nRGB-D based human detection. Our method achieves superior performance on two\npublicly available datasets.\n", "rewritten_text": "The development of depth cameras like Kinect and Intel RealSense has sparked ongoing research interest in RGB-D based human detection, given its wide range of applications. This paper introduces a novel Multi-Glimpse LSTM (MG-LSTM) network that sequentially integrates multi-scale contextual information to enhance human detection performance. Additionally, a feature fusion strategy based on the MG-LSTM network is proposed to effectively combine RGB and depth information. To our knowledge, this is the first application of LSTM structure for RGB-D based human detection. Our method demonstrates superior performance on two publicly available datasets."}, "1810.07599": {"original_text": "  As facial appearance is subject to significant intra-class variations caused\nby the aging process over time, age-invariant face recognition (AIFR) remains a\nmajor challenge in face recognition community. To reduce the intra-class\ndiscrepancy caused by the aging, in this paper we propose a novel approach\n(namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep\nface features. Specifically, we decompose deep face features into two\northogonal components to represent age-related and identity-related features.\nAs a result, identity-related features that are robust to aging are then used\nfor AIFR. Besides, for complementing the existing cross-age datasets and\nadvancing the research in this field, we construct a brand-new large-scale\nCross-Age Face dataset (CAF). Extensive experiments conducted on the three\npublic domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have\nshown the effectiveness of the proposed approach and the value of the\nconstructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most\npopular general face recognition (GFR) dataset LFW additionally demonstrates\nthe comparable generalization performance on GFR.\n", "rewritten_text": "Facial appearance undergoes significant intra-class variations due to the aging process, presenting a major challenge for age-invariant face recognition (AIFR) within the face recognition community. In this paper, we introduce a novel approach, Orthogonal Embedding CNNs (OE-CNNs), aimed at mitigating the intra-class discrepancy caused by aging by learning age-invariant deep face features. Our method involves decomposing deep face features into two orthogonal components to capture age-related and identity-related features separately. By focusing on identity-related features that are resilient to aging, we enhance AIFR performance. \n\nFurthermore, to supplement existing cross-age datasets and advance research in this domain, we have developed a new large-scale Cross-Age Face dataset (CAF). Extensive experiments conducted on three public domain face aging datasets (MORPH Album 2, CACD-VS, and FG-NET) demonstrate the effectiveness of our proposed approach and the significance of the CAF dataset for AIFR. Benchmarking our algorithm on the widely-used general face recognition (GFR) dataset LFW also showcases its comparable generalization performance in the realm of GFR."}, "2107.005": {"original_text": "  Driven by recent advances in object detection with deep neural networks, the\ntracking-by-detection paradigm has gained increasing prevalence in the research\ncommunity of multi-object tracking (MOT). It has long been known that\nappearance information plays an essential role in the detection-to-track\nassociation, which lies at the core of the tracking-by-detection paradigm.\nWhile most existing works consider the appearance distances between the\ndetections and the tracks, they ignore the statistical information implied by\nthe historical appearance distance records in the tracks, which can be\nparticularly useful when a detection has similar distances with two or more\ntracks. In this work, we propose a hybrid track association (HTA) algorithm\nthat models the historical appearance distances of a track with an incremental\nGaussian mixture model (IGMM) and incorporates the derived statistical\ninformation into the calculation of the detection-to-track association cost.\nExperimental results on three MOT benchmarks confirm that HTA effectively\nimproves the target identification performance with a small compromise to the\ntracking speed. Additionally, compared to many state-of-the-art trackers, the\nDeepSORT tracker equipped with HTA achieves better or comparable performance in\nterms of the balance of tracking quality and speed.\n", "rewritten_text": "Recent advances in object detection using deep neural networks have driven the increasing prevalence of the tracking-by-detection paradigm in the research community of multi-object tracking (MOT). It has long been recognized that appearance information plays a crucial role in the association of detection-to-track, which is at the core of the tracking-by-detection paradigm. While most existing works focus on the appearance distances between detections and tracks, they often overlook the valuable statistical information contained in the historical appearance distance records within tracks. This information can be particularly beneficial when a detection has similar distances with multiple tracks.\n\nIn this study, we introduce a hybrid track association (HTA) algorithm that leverages an incremental Gaussian mixture model (IGMM) to model the historical appearance distances of a track. The algorithm then integrates this statistical information into the calculation of the detection-to-track association cost. Experimental results on three MOT benchmarks demonstrate that HTA significantly enhances target identification performance with minimal impact on tracking speed. Furthermore, when compared to many state-of-the-art trackers, the DeepSORT tracker enhanced with HTA achieves superior or comparable performance in terms of balancing tracking quality and speed."}, "1711.08879": {"original_text": "  Objects for detection usually have distinct characteristics in different\nsub-regions and different aspect ratios. However, in prevalent two-stage object\ndetection methods, Region-of-Interest (RoI) features are extracted by RoI\npooling with little emphasis on these translation-variant feature components.\nWe present feature selective networks to reform the feature representations of\nRoIs by exploiting their disparities among sub-regions and aspect ratios. Our\nnetwork produces the sub-region attention bank and aspect ratio attention bank\nfor the whole image. The RoI-based sub-region attention map and aspect ratio\nattention map are selectively pooled from the banks, and then used to refine\nthe original RoI features for RoI classification. Equipped with a light-weight\ndetection subnetwork, our network gets a consistent boost in detection\nperformance based on general ConvNet backbones (ResNet-101, GoogLeNet and\nVGG-16). Without bells and whistles, our detectors equipped with ResNet-101\nachieve more than 3% mAP improvement compared to counterparts on PASCAL VOC\n2007, PASCAL VOC 2012 and MS COCO datasets.\n", "rewritten_text": "Objects for detection typically exhibit distinct characteristics across different sub-regions and aspect ratios. However, in conventional two-stage object detection methods, Region-of-Interest (RoI) features are extracted through RoI pooling with minimal consideration for these translation-variant feature components. To address this limitation, we introduce feature selective networks designed to enhance the feature representations of RoIs by leveraging variations among sub-regions and aspect ratios. Our network generates a sub-region attention bank and an aspect ratio attention bank for the entire image. The RoI-based sub-region attention map and aspect ratio attention map are selectively pooled from these banks and utilized to refine the original RoI features for RoI classification. With the integration of a lightweight detection subnetwork, our network consistently enhances detection performance when paired with common ConvNet backbones such as ResNet-101, GoogLeNet, and VGG-16. Notably, our detectors featuring ResNet-101 achieve over a 3% mean Average Precision (mAP) improvement compared to counterparts on the PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO datasets, without any additional embellishments."}, "2402.02145": {"original_text": "  In today's media landscape, where news outlets play a pivotal role in shaping\npublic opinion, it is imperative to address the issue of sentiment manipulation\nwithin news text. News writers often inject their own biases and emotional\nlanguage, which can distort the objectivity of reporting. This paper introduces\na novel approach to tackle this problem by reducing the polarity of latent\nsentiments in news content. Drawing inspiration from adversarial attack-based\nsentence perturbation techniques and a prompt based method using ChatGPT, we\nemploy transformation constraints to modify sentences while preserving their\ncore semantics. Using three perturbation methods: replacement, insertion, and\ndeletion coupled with a context-aware masked language model, we aim to maximize\nthe desired sentiment score for targeted news aspects through a beam search\nalgorithm. Our experiments and human evaluations demonstrate the effectiveness\nof these two models in achieving reduced sentiment polarity with minimal\nmodifications while maintaining textual similarity, fluency, and grammatical\ncorrectness. Comparative analysis confirms the competitive performance of the\nadversarial attack based perturbation methods and prompt-based methods,\noffering a promising solution to foster more objective news reporting and\ncombat emotional language bias in the media.\n", "rewritten_text": "In today's media landscape, news outlets play a crucial role in shaping public opinion. Therefore, addressing the issue of sentiment manipulation within news text is imperative. News writers often inject their own biases and emotional language, which can compromise the objectivity of reporting. This paper presents a new approach to addressing this problem by reducing the polarity of latent sentiments in news content.\n\nDrawing inspiration from adversarial attack-based sentence perturbation techniques and a prompt-based method using ChatGPT, we utilize transformation constraints to modify sentences while preserving their core semantics. By employing three perturbation methods - replacement, insertion, and deletion - in conjunction with a context-aware masked language model, our goal is to maximize the desired sentiment score for targeted news aspects through a beam search algorithm.\n\nOur experiments and human evaluations demonstrate the effectiveness of these two models in achieving reduced sentiment polarity with minimal modifications, all while maintaining textual similarity, fluency, and grammatical correctness. Comparative analysis confirms the competitive performance of the adversarial attack-based perturbation methods and prompt-based methods, offering a promising solution to promote more objective news reporting and combat emotional language bias in the media."}, "2312.10437": {"original_text": "  Tender notices are usually sought by most of the companies at regular\nintervals as a means for obtaining the contracts of various projects. These\nnotices consist of all the required information like description of the work,\nperiod of construction, estimated amount of project, etc. In the context of\nNepal, tender notices are usually published in national as well as local\nnewspapers. The interested bidders should search all the related tender notices\nin newspapers. However, it is very tedious for these companies to manually\nsearch tender notices in every newspaper and figure out which bid is best\nsuited for them. This project is built with the purpose of solving this tedious\ntask of manually searching the tender notices. Initially, the newspapers are\ndownloaded in PDF format using the selenium library of python. After\ndownloading the newspapers, the e-papers are scanned and tender notices are\nautomatically extracted using a neural network. For extraction purposes,\ndifferent architectures of CNN namely ResNet, GoogleNet and Xception are used\nand a model with highest performance has been implemented. Finally, these\nextracted notices are then published on the website and are accessible to the\nusers. This project is helpful for construction companies as well as\ncontractors assuring quality and efficiency. This project has great application\nin the field of competitive bidding as well as managing them in a systematic\nmanner.\n", "rewritten_text": "Most companies regularly seek tender notices to secure contracts for various projects. These notices contain essential information such as work descriptions, construction timelines, estimated project costs, and more. In Nepal, tender notices are typically published in both national and local newspapers. Interested bidders must diligently search for relevant notices in newspapers, which can be a cumbersome task. To address this issue, a project was developed to automate the process of searching for tender notices. Initially, newspapers are downloaded in PDF format using the Selenium library in Python. Subsequently, the e-papers are scanned, and tender notices are automatically extracted using a neural network. Various CNN architectures including ResNet, GoogleNet, and Xception are employed for extraction, with the highest performing model being implemented. The extracted notices are then published on a website for user access. This project benefits construction companies and contractors by enhancing efficiency and ensuring quality. Its practical applications extend to competitive bidding and systematic bid management."}, "2405.18132": {"original_text": "  In recent years, the increasing demand for dynamic 3D assets in design and\ngaming applications has given rise to powerful generative pipelines capable of\nsynthesizing high-quality 4D objects. Previous methods generally rely on score\ndistillation sampling (SDS) algorithm to infer the unseen views and motion of\n4D objects, thus leading to unsatisfactory results with defects like\nover-saturation and Janus problem. Therefore, inspired by recent progress of\nvideo diffusion models, we propose to optimize a 4D representation by\nexplicitly generating multi-view videos from one input image. However, it is\nfar from trivial to handle practical challenges faced by such a pipeline,\nincluding dramatic temporal inconsistency, inter-frame geometry and texture\ndiversity, and semantic defects brought by video generation results. To address\nthese issues, we propose DG4D, a novel multi-stage framework that generates\nhigh-quality and consistent 4D assets without score distillation. Specifically,\ncollaborative techniques and solutions are developed, including an attention\ninjection strategy to synthesize temporal-consistent multi-view videos, a\nrobust and efficient dynamic reconstruction method based on Gaussian Splatting,\nand a refinement stage with diffusion prior for semantic restoration. The\nqualitative results and user preference study demonstrate that our framework\noutperforms the baselines in generation quality by a considerable margin. Code\nwill be released at \\url{https://github.com/jasongzy/EG4D}.\n", "rewritten_text": "In recent years, the growing demand for dynamic 3D assets in design and gaming applications has led to the development of powerful generative pipelines capable of creating high-quality 4D objects. Previous methods typically rely on the score distillation sampling (SDS) algorithm to predict the unseen views and motion of 4D objects, resulting in subpar outcomes with issues such as over-saturation and the Janus problem. Drawing inspiration from advancements in video diffusion models, we propose an optimization approach for 4D representation by directly generating multi-view videos from a single input image. However, effectively managing the practical challenges associated with such a pipeline, such as significant temporal inconsistencies, diverse inter-frame geometry and textures, and semantic defects arising from video generation, is far from straightforward.\n\nTo tackle these challenges, we introduce DG4D, a novel multi-stage framework designed to produce high-quality and coherent 4D assets without relying on score distillation. Our framework incorporates collaborative techniques and solutions, including an attention injection strategy for generating temporally consistent multi-view videos, a robust and efficient dynamic reconstruction method based on Gaussian Splatting, and a refinement stage utilizing diffusion priors for semantic restoration. Qualitative results and a user preference study demonstrate that our framework significantly outperforms baseline methods in terms of generation quality. The code for our framework will be made available at \\url{https://github.com/jasongzy/EG4D}."}, "2101.04929": {"original_text": "  Motivated by applications from computer vision to bioinformatics, the field\nof shape analysis deals with problems where one wants to analyze geometric\nobjects, such as curves, while ignoring actions that preserve their shape, such\nas translations, rotations, or reparametrizations. Mathematical tools have been\ndeveloped to define notions of distances, averages, and optimal deformations\nfor geometric objects. One such framework, which has proven to be successful in\nmany applications, is based on the square root velocity (SRV) transform, which\nallows one to define a computable distance between spatial curves regardless of\nhow they are parametrized. This paper introduces a supervised deep learning\nframework for the direct computation of SRV distances between curves, which\nusually requires an optimization over the group of reparametrizations that act\non the curves. The benefits of our approach in terms of computational speed and\naccuracy are illustrated via several numerical experiments.\n", "rewritten_text": "Motivated by applications ranging from computer vision to bioinformatics, the field of shape analysis addresses problems involving the analysis of geometric objects, such as curves, while disregarding actions that maintain their shape, such as translations, rotations, or reparametrizations. Mathematical tools have been developed to establish concepts of distances, averages, and optimal deformations for geometric objects. One successful framework, based on the square root velocity (SRV) transform, enables the definition of a computable distance between spatial curves regardless of their parametrization. This paper presents a supervised deep learning framework for directly computing SRV distances between curves, a task that typically involves optimization over the group of reparametrizations that affect the curves. The advantages of our approach in terms of computational speed and accuracy are demonstrated through various numerical experiments."}, "1811.07461": {"original_text": "  Humans naturally perceive a 3D scene in front of them through accumulation of\ninformation obtained from multiple interconnected projections of the scene and\nby interpreting their correspondence. This phenomenon has inspired artificial\nintelligence models to extract the depth and view angle of the observed scene\nby modeling the correspondence between different views of that scene. Our paper\nis built upon previous works in the field of unsupervised depth and relative\ncamera pose estimation from temporal consecutive video frames using deep\nlearning (DL) models. Our approach uses a hybrid learning framework introduced\nin a recent work called GeoNet, which leverages geometric constraints in the 3D\nscenes to synthesize a novel view from intermediate DL-based predicted depth\nand relative pose. However, the state-of-the-art unsupervised depth and pose\nestimation DL models are exclusively trained/tested on a few available outdoor\nscene datasets and we have shown they are hardly transferable to new scenes,\nespecially from indoor environments, in which estimation requires higher\nprecision and dealing with probable occlusions. This paper introduces \"Indoor\nGeoNet\", a weakly supervised depth and camera pose estimation model targeted\nfor indoor scenes. In Indoor GeoNet, we take advantage of the availability of\nindoor RGBD datasets collected by human or robot navigators, and added partial\n(i.e. weak) supervision in depth training into the model. Experimental results\nshowed that our model effectively generalizes to new scenes from different\nbuildings. Indoor GeoNet demonstrated significant depth and pose estimation\nerror reduction when compared to the original GeoNet, while showing 3 times\nmore reconstruction accuracy in synthesizing novel views in indoor\nenvironments.\n", "rewritten_text": "Humans naturally perceive a 3D scene in front of them by accumulating information from multiple interconnected projections of the scene and interpreting their correspondence. This natural phenomenon has inspired artificial intelligence models to extract the depth and view angle of observed scenes by modeling the correspondence between different views. Our paper builds upon previous works in the field of unsupervised depth and relative camera pose estimation from temporal consecutive video frames using deep learning (DL) models. Our approach utilizes a hybrid learning framework introduced in a recent work called GeoNet, which leverages geometric constraints in 3D scenes to synthesize a novel view from intermediate DL-based predicted depth and relative pose.\n\nHowever, the current state-of-the-art unsupervised depth and pose estimation DL models are primarily trained and tested on a limited number of outdoor scene datasets, making them challenging to transfer to new scenes, particularly those in indoor environments where estimation requires higher precision and the handling of potential occlusions. This paper introduces \"Indoor GeoNet,\" a weakly supervised depth and camera pose estimation model designed for indoor scenes. In Indoor GeoNet, we capitalize on the availability of indoor RGBD datasets collected by human or robot navigators and incorporate partial (i.e., weak) supervision in depth training into the model.\n\nExperimental results demonstrate that our model effectively generalizes to new scenes from various buildings. Indoor GeoNet exhibits a significant reduction in depth and pose estimation errors compared to the original GeoNet, while achieving three times greater reconstruction accuracy in synthesizing novel views in indoor environments."}, "2208.04254": {"original_text": "  Image captioning models are usually trained according to human annotated\nground-truth captions, which could generate accurate but generic captions. In\nthis paper, we focus on generating distinctive captions that can distinguish\nthe target image from other similar images. To evaluate the distinctiveness of\ncaptions, we introduce a series of metrics that use large-scale vision-language\npre-training model CLIP to quantify the distinctiveness. To further improve the\ndistinctiveness of captioning models, we propose a simple and effective\ntraining strategy that trains the model by comparing target image with similar\nimage group and optimizing the group embedding gap. Extensive experiments are\nconducted on various baseline models to demonstrate the wide applicability of\nour strategy and the consistency of metric results with human evaluation. By\ncomparing the performance of our best model with existing state-of-the-art\nmodels, we claim that our model achieves new state-of-the-art towards\ndistinctiveness objective.\n", "rewritten_text": "Image captioning models are typically trained using human-annotated ground-truth captions, which often result in accurate but generic descriptions. This paper focuses on generating unique captions that can effectively differentiate the target image from similar images. To assess the distinctiveness of these captions, we introduce a set of metrics that leverage the large-scale vision-language pre-training model CLIP. Additionally, to enhance the distinctiveness of captioning models, we propose a straightforward yet efficient training approach that involves comparing the target image with a group of similar images and optimizing the gap in their embeddings.\n\nExtensive experiments were conducted on various baseline models to showcase the broad applicability of our strategy and the consistency of metric results with human evaluation. By comparing the performance of our best model with existing state-of-the-art models, we assert that our model sets a new state-of-the-art benchmark in terms of distinctiveness."}, "2212.07181": {"original_text": "  Neuromorphic vision or event vision is an advanced vision technology, where\nin contrast to the visible camera that outputs pixels, the event vision\ngenerates neuromorphic events every time there is a brightness change which\nexceeds a specific threshold in the field of view (FOV). This study focuses on\nleveraging neuromorphic event data for roadside object detection. This is a\nproof of concept towards building artificial intelligence (AI) based pipelines\nwhich can be used for forward perception systems for advanced vehicular\napplications. The focus is on building efficient state-of-the-art object\ndetection networks with better inference results for fast-moving forward\nperception using an event camera. In this article, the event-simulated A2D2\ndataset is manually annotated and trained on two different YOLOv5 networks\n(small and large variants). To further assess its robustness, single model\ntesting and ensemble model testing are carried out.\n", "rewritten_text": "Neuromorphic vision, also known as event vision, is an advanced technology that differs from traditional visible cameras by generating neuromorphic events in response to brightness changes exceeding a specific threshold within the field of view (FOV), rather than outputting pixels. This study aims to utilize neuromorphic event data for roadside object detection, serving as a proof of concept for developing artificial intelligence (AI)-based pipelines for forward perception systems in advanced vehicular applications. The primary focus is on constructing efficient, cutting-edge object detection networks to achieve superior inference results for rapidly moving forward perception using an event camera. The article details the manual annotation and training of the event-simulated A2D2 dataset on two distinct YOLOv5 networks (small and large variants). Additionally, single model testing and ensemble model testing are conducted to further evaluate the robustness of the system."}, "2402.01217": {"original_text": "  Implicit neural representations, represented by Neural Radiance Fields\n(NeRF), have dominated research in 3D computer vision by virtue of high-quality\nvisual results and data-driven benefits. However, their realistic applications\nare hindered by the need for dense inputs and per-scene optimization. To solve\nthis problem, previous methods implement generalizable NeRFs by extracting\nlocal features from sparse inputs as conditions for the NeRF decoder. However,\nalthough this way can allow feed-forward reconstruction, they suffer from the\ninherent drawback of yielding sub-optimal results caused by erroneous\nreprojected features. In this paper, we focus on this problem and aim to\naddress it by introducing pre-trained generative priors to enable high-quality\ngeneralizable novel view synthesis. Specifically, we propose a novel Indirect\nDiffusion-guided NeRF framework, termed ID-NeRF, which leverages pre-trained\ndiffusion priors as a guide for the reprojected features created by the\nprevious paradigm. Notably, to enable 3D-consistent predictions, the proposed\nID-NeRF discards the way of direct supervision commonly used in prior 3D\ngenerative models and instead adopts a novel indirect prior injection strategy.\nThis strategy is implemented by distilling pre-trained knowledge into an\nimaginative latent space via score-based distillation, and an attention-based\nrefinement module is then proposed to leverage the embedded priors to improve\nreprojected features extracted from sparse inputs. We conduct extensive\nexperiments on multiple datasets to evaluate our method, and the results\ndemonstrate the effectiveness of our method in synthesizing novel views in a\ngeneralizable manner, especially in sparse settings.\n", "rewritten_text": "Implicit neural representations, such as Neural Radiance Fields (NeRF), have been at the forefront of research in 3D computer vision due to their ability to produce high-quality visual results and leverage data-driven advantages. However, their practical applications are limited by the requirement for dense inputs and scene-specific optimization. To address this challenge, previous approaches have sought to create generalizable NeRFs by extracting local features from sparse inputs to inform the NeRF decoder. While this approach enables feed-forward reconstruction, it is prone to suboptimal outcomes due to inaccuracies in the reprojected features.\n\nIn this study, we focus on this issue and propose a solution by introducing pre-trained generative priors to facilitate high-quality, generalizable novel view synthesis. Specifically, we introduce a novel framework called Indirect Diffusion-guided NeRF (ID-NeRF), which utilizes pre-trained diffusion priors to guide the reprojected features generated by the existing paradigm. Notably, to ensure consistent 3D predictions, ID-NeRF diverges from the direct supervision commonly employed in previous 3D generative models and instead adopts an innovative indirect prior injection strategy.\n\nThis strategy involves distilling pre-trained knowledge into a latent space through score-based distillation. Subsequently, an attention-based refinement module is proposed to leverage the embedded priors and enhance the reprojected features derived from sparse inputs. We conducted comprehensive experiments across multiple datasets to evaluate our approach, and the results underscore the efficacy of our method in synthesizing novel views in a generalizable manner, particularly in sparse scenarios."}, "2207.02803": {"original_text": "  Recent advances in face forgery techniques produce nearly visually\nuntraceable deepfake videos, which could be leveraged with malicious\nintentions. As a result, researchers have been devoted to deepfake detection.\nPrevious studies have identified the importance of local low-level cues and\ntemporal information in pursuit to generalize well across deepfake methods,\nhowever, they still suffer from robustness problem against post-processings. In\nthis work, we propose the Local- & Temporal-aware Transformer-based Deepfake\nDetection (LTTD) framework, which adopts a local-to-global learning protocol\nwith a particular focus on the valuable temporal information within local\nsequences. Specifically, we propose a Local Sequence Transformer (LST), which\nmodels the temporal consistency on sequences of restricted spatial regions,\nwhere low-level information is hierarchically enhanced with shallow layers of\nlearned 3D filters. Based on the local temporal embeddings, we then achieve the\nfinal classification in a global contrastive way. Extensive experiments on\npopular datasets validate that our approach effectively spots local forgery\ncues and achieves state-of-the-art performance.\n", "rewritten_text": "Recent advancements in face forgery techniques have led to the creation of deepfake videos that are nearly visually indistinguishable, posing a potential threat for malicious use. Consequently, researchers have focused their efforts on developing methods for detecting deepfakes. While previous studies have recognized the significance of local low-level cues and temporal information in achieving generalization across various deepfake methods, they continue to face challenges in terms of robustness against post-processing techniques. \n\nIn this study, we introduce the Local- & Temporal-aware Transformer-based Deepfake Detection (LTTD) framework, which employs a local-to-global learning approach with a specific emphasis on capturing valuable temporal information within local sequences. Our framework features a Local Sequence Transformer (LST) that captures temporal consistency within sequences of limited spatial regions, enhancing low-level information through shallow layers of learned 3D filters. By leveraging local temporal embeddings, we perform final classification in a global contrastive manner. \n\nExtensive experiments conducted on widely-used datasets demonstrate that our approach effectively identifies local forgery indicators and achieves state-of-the-art performance in deepfake detection."}, "2210.16621": {"original_text": "  Transformer-based architectures like BERT have achieved great success in a\nwide range of Natural Language tasks. Despite their decent performance, the\nmodels still have numerous parameters and high computational complexity,\nimpeding their deployment in resource-constrained environments. Post-Training\nQuantization (PTQ), which enables low-bit computations without extra training,\ncould be a promising tool. In this work, we conduct an empirical evaluation of\nthree PTQ methods on BERT-Base and BERT-Large: Linear Quantization (LQ),\nAnalytical Clipping for Integer Quantization (ACIQ), and Outlier Channel\nSplitting (OCS). OCS theoretically surpasses the others in minimizing the Mean\nSquare quantization Error and avoiding distorting the weights' outliers. That\nis consistent with the evaluation results of most language tasks of GLUE\nbenchmark and a reading comprehension task, SQuAD. Moreover, low-bit quantized\nBERT models could outperform the corresponding 32-bit baselines on several\nsmall language tasks, which we attribute to the alleviation of\nover-parameterization. We further explore the limit of quantization bit and\nshow that OCS could quantize BERT-Base and BERT-Large to 3-bits and retain 98%\nand 96% of the performance on the GLUE benchmark accordingly. Moreover, we\nconduct quantization on the whole BERT family, i.e., BERT models in different\nconfigurations, and comprehensively evaluate their performance on the GLUE\nbenchmark and SQuAD, hoping to provide valuable guidelines for their deployment\nin various computation environments.\n", "rewritten_text": "Transformer-based architectures such as BERT have demonstrated significant success across a wide array of Natural Language tasks. Despite their commendable performance, these models possess a large number of parameters and exhibit high computational complexity, which hinders their deployment in resource-constrained settings. Post-Training Quantization (PTQ) presents a promising solution by enabling low-bit computations without requiring additional training. \n\nIn this study, we empirically evaluate three PTQ methods - Linear Quantization (LQ), Analytical Clipping for Integer Quantization (ACIQ), and Outlier Channel Splitting (OCS) - on both BERT-Base and BERT-Large models. OCS, in theory, outperforms the other methods by minimizing Mean Square quantization Error and preserving the integrity of weight outliers. This finding is consistent with the evaluation results across various language tasks in the GLUE benchmark and a reading comprehension task, SQuAD.\n\nFurthermore, our research indicates that low-bit quantized BERT models can surpass their 32-bit counterparts in performance on several smaller language tasks, attributed to the reduction of over-parameterization. We also investigate the quantization bit limit and demonstrate that OCS can quantize BERT-Base and BERT-Large models to 3 bits while retaining 98% and 96% of their performance on the GLUE benchmark, respectively.\n\nAdditionally, we extend our analysis to encompass the entire BERT family, including models in different configurations, and comprehensively assess their performance on the GLUE benchmark and SQuAD. Our aim is to offer valuable insights for deploying these models in diverse computational environments."}, "2401.10768": {"original_text": "  While large language models (LLMs) have demonstrated exceptional performance\nacross various tasks following human alignment, they may still generate\nresponses that sound plausible but contradict factual knowledge, a phenomenon\nknown as hallucination. In this paper, we demonstrate the feasibility of\nmitigating hallucinations by verifying and minimizing the inconsistency between\nexternal knowledge present in the alignment data and the intrinsic knowledge\nembedded within foundation LLMs. Specifically, we propose a novel approach\ncalled Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM\nto automatically formulate assessments based on external knowledge to evaluate\nthe knowledge boundaries of foundation LLMs. To address knowledge\ninconsistencies in the alignment data, KCA implements several specific\nstrategies to deal with these data instances. We demonstrate the superior\nefficacy of KCA in reducing hallucinations across six benchmarks, utilizing\nfoundation LLMs of varying backbones and scales. This confirms the\neffectiveness of mitigating hallucinations by reducing knowledge inconsistency.\nOur code, model weights, and data are openly accessible at\n\\url{https://github.com/fanqiwan/KCA}.\n", "rewritten_text": "Large language models (LLMs) have shown exceptional performance in various tasks when aligned with human input. However, they can still produce responses that seem plausible but contradict factual knowledge, a phenomenon known as hallucination. This paper demonstrates the feasibility of reducing hallucinations by verifying and minimizing inconsistencies between external knowledge in the alignment data and the inherent knowledge within foundational LLMs. \n\nA novel approach, Knowledge Consistent Alignment (KCA), is proposed to address this issue. KCA utilizes a well-aligned LLM to automatically assess external knowledge and evaluate the knowledge boundaries of foundational LLMs. To tackle knowledge inconsistencies in the alignment data, KCA implements specific strategies for handling such instances. \n\nThe effectiveness of KCA in reducing hallucinations is demonstrated across six benchmarks, using foundational LLMs with different backbones and scales. This highlights the success of mitigating hallucinations by minimizing knowledge inconsistencies. The code, model weights, and data are openly accessible at \\url{https://github.com/fanqiwan/KCA}."}, "2201.01016": {"original_text": "  Recovering detailed facial geometry from a set of calibrated multi-view\nimages is valuable for its wide range of applications. Traditional multi-view\nstereo (MVS) methods adopt an optimization-based scheme to regularize the\nmatching cost. Recently, learning-based methods integrate all these into an\nend-to-end neural network and show superiority of efficiency. In this paper, we\npropose a novel architecture to recover extremely detailed 3D faces within\ndozens of seconds. Unlike previous learning-based methods that regularize the\ncost volume via 3D CNN, we propose to learn an implicit function for regressing\nthe matching cost. By fitting a 3D morphable model from multi-view images, the\nfeatures of multiple images are extracted and aggregated in the mesh-attached\nUV space, which makes the implicit function more effective in recovering\ndetailed facial shape. Our method outperforms SOTA learning-based MVS in\naccuracy by a large margin on the FaceScape dataset. The code and data are\nreleased in https://github.com/zhuhao-nju/mvfr.\n", "rewritten_text": "Recovering detailed facial geometry from a set of calibrated multi-view images is valuable due to its wide range of applications. Traditional multi-view stereo (MVS) methods typically utilize an optimization-based scheme to regularize the matching cost. Recently, learning-based methods have emerged that integrate these processes into an end-to-end neural network, demonstrating superior efficiency. \n\nIn this paper, we introduce a novel architecture designed to recover extremely detailed 3D faces within a matter of seconds. In contrast to previous learning-based methods that regularize the cost volume using 3D CNN, our approach involves learning an implicit function for regressing the matching cost. By fitting a 3D morphable model from multi-view images, we extract and aggregate features from multiple images in the mesh-attached UV space. This enhances the effectiveness of the implicit function in recovering detailed facial shape.\n\nOur method significantly outperforms state-of-the-art learning-based MVS techniques in terms of accuracy on the FaceScape dataset. The code and data for our method are available at https://github.com/zhuhao-nju/mvfr."}, "1809.05752": {"original_text": "  Readmission after discharge from a hospital is disruptive and costly,\nregardless of the reason. However, it can be particularly problematic for\npsychiatric patients, so predicting which patients may be readmitted is\ncritically important but also very difficult. Clinical narratives in\npsychiatric electronic health records (EHRs) span a wide range of topics and\nvocabulary; therefore, a psychiatric readmission prediction model must begin\nwith a robust and interpretable topic extraction component. We created a data\npipeline for using document vector similarity metrics to perform topic\nextraction on psychiatric EHR data in service of our long-term goal of creating\na readmission risk classifier. We show initial results for our topic extraction\nmodel and identify additional features we will be incorporating in the future.\n", "rewritten_text": "Readmission following discharge from a hospital is both disruptive and costly, regardless of the underlying cause. This issue is particularly challenging for psychiatric patients, making it crucial to predict which patients are at risk of readmission. However, this task is inherently complex. Clinical narratives within psychiatric electronic health records (EHRs) cover a broad spectrum of topics and vocabulary. Therefore, the development of a psychiatric readmission prediction model necessitates a strong and easily interpretable topic extraction component.\n\nTo address this need, we have established a data pipeline that utilizes document vector similarity metrics for topic extraction from psychiatric EHR data. This initiative aligns with our overarching objective of creating a classifier to assess readmission risk. Our preliminary findings showcase the effectiveness of our topic extraction model, while also highlighting additional features that we plan to integrate in future iterations."}, "2302.1443": {"original_text": "  3D hand tracking methods based on monocular RGB videos are easily affected by\nmotion blur, while event camera, a sensor with high temporal resolution and\ndynamic range, is naturally suitable for this task with sparse output and low\npower consumption. However, obtaining 3D annotations of fast-moving hands is\ndifficult for constructing event-based hand-tracking datasets. In this paper,\nwe provided an event-based speed adaptive hand tracker (ESAHT) to solve the\nhand tracking problem based on event camera. We enabled a CNN model trained on\na hand tracking dataset with slow motion, which enabled the model to leverage\nthe knowledge of RGB-based hand tracking solutions, to work on fast hand\ntracking tasks. To realize our solution, we constructed the first 3D hand\ntracking dataset captured by an event camera in a real-world environment,\nfigured out two data augment methods to narrow the domain gap between slow and\nfast motion data, developed a speed adaptive event stream segmentation method\nto handle hand movements in different moving speeds, and introduced a new\nevent-to-frame representation method adaptive to event streams with different\nlengths. Experiments showed that our solution outperformed RGB-based as well as\nprevious event-based solutions in fast hand tracking tasks, and our codes and\ndataset will be publicly available.\n", "rewritten_text": "3D hand tracking methods relying on monocular RGB videos are susceptible to motion blur, whereas event cameras, sensors boasting high temporal resolution and dynamic range, are inherently well-suited for this task due to their sparse output and low power consumption. However, acquiring 3D annotations for swiftly moving hands poses a challenge in constructing event-based hand-tracking datasets. In this study, we introduce an event-based speed-adaptive hand tracker (ESAHT) designed to address the hand tracking issue using an event camera. We have trained a CNN model on a hand tracking dataset featuring slow-motion sequences, enabling the model to leverage insights from RGB-based hand tracking solutions for rapid hand tracking tasks. To implement our approach, we have curated the initial 3D hand tracking dataset captured by an event camera in a real-world setting, devised two data augmentation techniques to bridge the gap between slow and fast motion data, devised a speed-adaptive event stream segmentation method to accommodate hand movements at varying speeds, and introduced a novel event-to-frame representation method tailored to event streams of different lengths. Experimental results demonstrate that our solution surpasses both RGB-based and prior event-based approaches in rapid hand tracking tasks, with our codes and dataset set to be made publicly accessible."}, "2407.15153": {"original_text": "  Video generation has drawn significant interest recently, pushing the\ndevelopment of large-scale models capable of producing realistic videos with\ncoherent motion. Due to memory constraints, these models typically generate\nshort video segments that are then combined into long videos. The merging\nprocess poses a significant challenge, as it requires ensuring smooth\ntransitions and overall consistency. In this paper, we introduce Anchored\nDiffusion, a novel method for synthesizing relatively long and seamless videos.\nWe extend Diffusion Transformers (DiTs) to incorporate temporal information,\ncreating our sequence-DiT (sDiT) model for generating short video segments.\nUnlike previous works, we train our model on video sequences with random\nnon-uniform temporal spacing and incorporate temporal information via external\nguidance, increasing flexibility and allowing it to capture both short and\nlong-term relationships. Furthermore, during inference, we leverage the\ntransformer architecture to modify the diffusion process, generating a batch of\nnon-uniform sequences anchored to a common frame, ensuring consistency\nregardless of temporal distance. To demonstrate our method, we focus on face\nreenactment, the task of creating a video from a source image that replicates\nthe facial expressions and movements from a driving video. Through\ncomprehensive experiments, we show our approach outperforms current techniques\nin producing longer consistent high-quality videos while offering editing\ncapabilities.\n", "rewritten_text": "Recently, there has been significant interest in video generation, leading to the development of large-scale models capable of creating realistic videos with coherent motion. These models often face memory constraints, resulting in the generation of short video segments that are later combined to form longer videos. The merging process presents a notable challenge, as it necessitates ensuring smooth transitions and overall consistency.\n\nIn this paper, we introduce Anchored Diffusion, a novel method for synthesizing relatively long and seamless videos. We enhance Diffusion Transformers (DiTs) by integrating temporal information, thereby introducing our sequence-DiT (sDiT) model designed for generating short video segments. Unlike previous approaches, we train our model on video sequences with random non-uniform temporal spacing and incorporate temporal information through external guidance. This approach enhances flexibility, enabling the model to capture both short and long-term relationships.\n\nDuring inference, we utilize the transformer architecture to adjust the diffusion process, generating a batch of non-uniform sequences anchored to a common frame. This ensures consistency regardless of temporal distance. To showcase the effectiveness of our method, we focus on face reenactment, a task involving the creation of a video from a source image that mirrors the facial expressions and movements from a driving video.\n\nThrough extensive experiments, we demonstrate that our approach surpasses current techniques in producing longer, consistent, high-quality videos while also offering editing capabilities."}, "2009.03116": {"original_text": "  This paper presents the first Swedish evaluation benchmark for textual\nsemantic similarity. The benchmark is compiled by simply running the English\nSTS-B dataset through the Google machine translation API. This paper discusses\npotential problems with using such a simple approach to compile a Swedish\nevaluation benchmark, including translation errors, vocabulary variation, and\nproductive compounding. Despite some obvious problems with the resulting\ndataset, we use the benchmark to compare the majority of the currently existing\nSwedish text representations, demonstrating that native models outperform\nmultilingual ones, and that simple bag of words performs remarkably well.\n", "rewritten_text": "This paper introduces the inaugural Swedish evaluation benchmark for textual semantic similarity. The benchmark was created by processing the English STS-B dataset through the Google machine translation API. The paper delves into the potential issues associated with employing this straightforward method to construct a Swedish evaluation benchmark, such as translation inaccuracies, vocabulary discrepancies, and productive compounding. Despite the evident challenges posed by the resultant dataset, we utilize the benchmark to assess the majority of the presently available Swedish text representations. Our analysis reveals that native models surpass multilingual ones and that a simple bag of words approach yields impressive results."}, "2106.16138": {"original_text": "  In this paper, we introduce ELECTRA-style tasks to cross-lingual language\nmodel pre-training. Specifically, we present two pre-training tasks, namely\nmultilingual replaced token detection, and translation replaced token\ndetection. Besides, we pretrain the model, named as XLM-E, on both multilingual\nand parallel corpora. Our model outperforms the baseline models on various\ncross-lingual understanding tasks with much less computation cost. Moreover,\nanalysis shows that XLM-E tends to obtain better cross-lingual transferability.\n", "rewritten_text": "In this paper, we introduce ELECTRA-style tasks for cross-lingual language model pre-training. Specifically, we introduce two pre-training tasks: multilingual replaced token detection and translation replaced token detection. Additionally, we pretrain the model, named XLM-E, using both multilingual and parallel corpora. Our model surpasses the baseline models in various cross-lingual understanding tasks while requiring significantly less computational cost. Furthermore, analysis indicates that XLM-E demonstrates improved cross-lingual transferability."}, "2101.03929": {"original_text": "  Learning to capture dependencies between spatial positions is essential to\nmany visual tasks, especially the dense labeling problems like scene parsing.\nExisting methods can effectively capture long-range dependencies with\nself-attention mechanism while short ones by local convolution. However, there\nis still much gap between long-range and short-range dependencies, which\nlargely reduces the models' flexibility in application to diverse spatial\nscales and relationships in complicated natural scene images. To fill such a\ngap, we develop a Middle-Range (MR) branch to capture middle-range dependencies\nby restricting self-attention into local patches. Also, we observe that the\nspatial regions which have large correlations with others can be emphasized to\nexploit long-range dependencies more accurately, and thus propose a Reweighed\nLong-Range (RLR) branch. Based on the proposed MR and RLR branches, we build an\nOmni-Range Dependencies Network (ORDNet) which can effectively capture short-,\nmiddle- and long-range dependencies. Our ORDNet is able to extract more\ncomprehensive context information and well adapt to complex spatial variance in\nscene images. Extensive experiments show that our proposed ORDNet outperforms\nprevious state-of-the-art methods on three scene parsing benchmarks including\nPASCAL Context, COCO Stuff and ADE20K, demonstrating the superiority of\ncapturing omni-range dependencies in deep models for scene parsing task.\n", "rewritten_text": "Learning to capture dependencies between spatial positions is crucial for various visual tasks, particularly dense labeling issues such as scene parsing. While existing methods can effectively capture long-range dependencies using self-attention mechanisms and short-range dependencies through local convolutions, there remains a significant gap between the two. This gap limits the flexibility of models when applied to diverse spatial scales and relationships within complex natural scene images.\n\nTo address this gap, we introduce a Middle-Range (MR) branch that focuses on capturing middle-range dependencies by confining self-attention to local patches. Additionally, we identify that spatial regions exhibiting strong correlations with others can be highlighted to more accurately leverage long-range dependencies. Consequently, we propose a Reweighed Long-Range (RLR) branch. By combining the MR and RLR branches, we construct an Omni-Range Dependencies Network (ORDNet) capable of effectively capturing short-, middle-, and long-range dependencies.\n\nOur ORDNet enhances the extraction of comprehensive contextual information and adeptly adapts to the intricate spatial variations present in scene images. Extensive experiments demonstrate that our proposed ORDNet surpasses previous state-of-the-art methods across three scene parsing benchmarks, namely PASCAL Context, COCO Stuff, and ADE20K. These results underscore the superiority of incorporating omni-range dependencies into deep models for the scene parsing task."}, "2012.08514": {"original_text": "  In this paper, we propose an end-end model for producing furniture layout for\ninterior scene synthesis from the random vector. This proposed model is aimed\nto support professional interior designers to produce the interior decoration\nsolutions more quickly. The proposed model combines a conditional floor-plan\nmodule of the room, a conditional graphical floor-plan module of the room and a\nconditional layout module. As compared with the prior work on scene synthesis,\nour proposed three modules enhance the ability of auto-layout generation given\nthe dimensional category of the room. We conduct our experiments on the\nproposed real-world interior layout dataset that contains $191208$ designs from\nthe professional designers. Our numerical results demonstrate that the proposed\nmodel yields higher-quality layouts in comparison with the state-of-the-art\nmodel. The dataset and code are released\n\\href{https://github.com/CODE-SUBMIT/dataset3}{Dataset,Code}\n", "rewritten_text": "In this paper, we present an end-to-end model for generating furniture layouts in interior scenes using a random vector. The primary goal of this model is to assist professional interior designers in creating interior decoration solutions more efficiently. The model integrates a conditional floor-plan module, a conditional graphical floor-plan module, and a conditional layout module. Compared to previous research on scene synthesis, our model's three modules improve the automatic layout generation based on the room's dimensional category. \n\nWe conducted experiments using a real-world interior layout dataset comprising 191,208 designs by professional designers. Our numerical results show that our model produces higher-quality layouts compared to the current state-of-the-art model. The dataset and code can be accessed at [Dataset, Code](https://github.com/CODE-SUBMIT/dataset3)."}, "2004.14525": {"original_text": "  Inverted bottleneck layers, which are built upon depthwise convolutions, have\nbeen the predominant building blocks in state-of-the-art object detection\nmodels on mobile devices. In this work, we investigate the optimality of this\ndesign pattern over a broad range of mobile accelerators by revisiting the\nusefulness of regular convolutions. We discover that regular convolutions are a\npotent component to boost the latency-accuracy trade-off for object detection\non accelerators, provided that they are placed strategically in the network via\nneural architecture search. By incorporating regular convolutions in the search\nspace and directly optimizing the network architectures for object detection,\nwe obtain a family of object detection models, MobileDets, that achieve\nstate-of-the-art results across mobile accelerators. On the COCO object\ndetection task, MobileDets outperform MobileNetV3+SSDLite by 1.7 mAP at\ncomparable mobile CPU inference latencies. MobileDets also outperform\nMobileNetV2+SSDLite by 1.9 mAP on mobile CPUs, 3.7 mAP on Google EdgeTPU, 3.4\nmAP on Qualcomm Hexagon DSP and 2.7 mAP on Nvidia Jetson GPU without increasing\nlatency. Moreover, MobileDets are comparable with the state-of-the-art MnasFPN\non mobile CPUs even without using the feature pyramid, and achieve better mAP\nscores on both EdgeTPUs and DSPs with up to 2x speedup. Code and models are\navailable in the TensorFlow Object Detection API:\nhttps://github.com/tensorflow/models/tree/master/research/object_detection.\n", "rewritten_text": "In state-of-the-art object detection models on mobile devices, inverted bottleneck layers, which are constructed using depthwise convolutions, have been the predominant building blocks. This study explores the effectiveness of this design pattern across a wide range of mobile accelerators by reevaluating the utility of regular convolutions. It is revealed that strategically placing regular convolutions in the network through neural architecture search can significantly enhance the trade-off between latency and accuracy in object detection on accelerators.\n\nBy integrating regular convolutions into the search space and optimizing network architectures directly for object detection, a series of object detection models called MobileDets is developed, achieving top-tier performance across various mobile accelerators. On the COCO object detection task, MobileDets surpass MobileNetV3+SSDLite by 1.7 mAP at similar mobile CPU inference latencies. MobileDets also outperform MobileNetV2+SSDLite by 1.9 mAP on mobile CPUs, 3.7 mAP on Google EdgeTPU, 3.4 mAP on Qualcomm Hexagon DSP, and 2.7 mAP on Nvidia Jetson GPU without increasing latency.\n\nFurthermore, MobileDets demonstrate comparable performance to the state-of-the-art MnasFPN on mobile CPUs even without utilizing the feature pyramid. They achieve superior mAP scores on both EdgeTPUs and DSPs with up to a 2x speedup. The code and models for MobileDets are accessible in the TensorFlow Object Detection API at: https://github.com/tensorflow/models/tree/master/research/object_detection."}, "2310.00031": {"original_text": "  Diffusion models are generative models with impressive text-to-image\nsynthesis capabilities and have spurred a new wave of creative methods for\nclassical machine learning tasks. However, the best way to harness the\nperceptual knowledge of these generative models for visual tasks is still an\nopen question. Specifically, it is unclear how to use the prompting interface\nwhen applying diffusion backbones to vision tasks. We find that automatically\ngenerated captions can improve text-image alignment and significantly enhance a\nmodel's cross-attention maps, leading to better perceptual performance. Our\napproach improves upon the current state-of-the-art (SOTA) in diffusion-based\nsemantic segmentation on ADE20K and the current overall SOTA for depth\nestimation on NYUv2. Furthermore, our method generalizes to the cross-domain\nsetting. We use model personalization and caption modifications to align our\nmodel to the target domain and find improvements over unaligned baselines. Our\ncross-domain object detection model, trained on Pascal VOC, achieves SOTA\nresults on Watercolor2K. Our cross-domain segmentation method, trained on\nCityscapes, achieves SOTA results on Dark Zurich-val and Nighttime Driving.\nProject page: https://www.vision.caltech.edu/tadp/. Code:\nhttps://github.com/damaggu/TADP.\n", "rewritten_text": "Diffusion models, which are generative models known for their impressive text-to-image synthesis capabilities, have sparked a new wave of creative approaches for traditional machine learning tasks. However, the optimal method for leveraging the perceptual knowledge embedded in these generative models for visual tasks remains an open question. Specifically, the effective utilization of the prompting interface in conjunction with diffusion backbones for vision tasks is not yet clear.\n\nOur research reveals that automatically generated captions can enhance text-image alignment and significantly improve a model's cross-attention maps, resulting in enhanced perceptual performance. This approach surpasses the current state-of-the-art (SOTA) in diffusion-based semantic segmentation on ADE20K and achieves the current overall SOTA for depth estimation on NYUv2. Moreover, our method demonstrates effectiveness in cross-domain scenarios.\n\nBy employing model personalization and caption adjustments to align our model with the target domain, we observe enhancements over unaligned baselines. For instance, our cross-domain object detection model, trained on Pascal VOC, achieves SOTA results on Watercolor2K. Similarly, our cross-domain segmentation method, trained on Cityscapes, achieves SOTA results on Dark Zurich-val and Nighttime Driving datasets.\n\nFor more information, please visit our project page at https://www.vision.caltech.edu/tadp/. The code for our project can be found at https://github.com/damaggu/TADP."}, "2402.15930": {"original_text": "  The writing examples of English language learners may be different from those\nof native speakers. Given that there is a significant differences in second\nlanguage (L2) learners' error types by their proficiency levels, this paper\nattempts to reduce overcorrection by examining the interaction between LLM's\nperformance and L2 language proficiency. Our method focuses on zero-shot and\nfew-shot prompting and fine-tuning models for GEC for learners of English as a\nforeign language based on the different proficiency. We investigate GEC results\nand find that overcorrection happens primarily in advanced language learners'\nwriting (proficiency C) rather than proficiency A (a beginner level) and\nproficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot\nprompting with writing examples of English learners, actually tend to exhibit\ndecreased recall measures. To make our claim concrete, we conduct a\ncomprehensive examination of GEC outcomes and their evaluation results based on\nlanguage proficiency.\n", "rewritten_text": "The writing examples of English language learners may differ from those of native speakers. This paper aims to reduce overcorrection by exploring the relationship between L2 learners' error types and proficiency levels. Our approach focuses on utilizing zero-shot and few-shot prompting, as well as fine-tuning models for Grammar Error Correction (GEC) tailored to English as a foreign language learners of varying proficiency levels. Our analysis reveals that overcorrection is more prevalent among advanced language learners (proficiency level C) compared to beginners (proficiency level A) and intermediate learners (proficiency level B). Interestingly, fine-tuned Language Model Models (LLMs) and even few-shot prompting with writing examples from English learners show a decrease in recall measures. To support our findings, we conduct a thorough examination of GEC outcomes and evaluations based on language proficiency levels."}, "2306.10963": {"original_text": "  Adversarial patches are still a simple yet powerful white box attack that can\nbe used to fool object detectors by suppressing possible detections. The\npatches of these so-called evasion attacks are computational expensive to\nproduce and require full access to the attacked detector. This paper addresses\nthe problem of computational expensiveness by analyzing 375 generated patches,\ncalculating the principal components of these and show, that linear\ncombinations of the resulting \"eigenpatches\" can be used to fool object\ndetections successfully.\n", "rewritten_text": "Adversarial patches remain a simple yet potent white-box attack capable of deceiving object detectors by suppressing potential detections. These patches, known as evasion attacks, are computationally expensive to create and necessitate complete access to the targeted detector. This study tackles the issue of computational cost by examining 375 generated patches, determining their principal components, and demonstrating that linear combinations of the resulting \"eigenpatches\" can effectively deceive object detectors."}, "1806.03028": {"original_text": "  Vehicle Make and Model Recognition (MMR) systems provide a fully automatic\nframework to recognize and classify different vehicle models. Several\napproaches have been proposed to address this challenge, however they can\nperform in restricted conditions. Here, we formulate the vehicle make and model\nrecognition as a fine-grained classification problem and propose a new\nconfigurable on-road vehicle make and model recognition framework. We benefit\nfrom the unsupervised feature learning methods and in more details we employ\nLocality constraint Linear Coding (LLC) method as a fast feature encoder for\nencoding the input SIFT features. The proposed method can perform in real\nenvironments of different conditions. This framework can recognize fifty models\nof vehicles and has an advantage to classify every other vehicle not belonging\nto one of the specified fifty classes as an unknown vehicle. The proposed MMR\nframework can be configured to become faster or more accurate based on the\napplication domain. The proposed approach is examined on two datasets including\nIranian on-road vehicle dataset and CompuCar dataset. The Iranian on-road\nvehicle dataset contains images of 50 models of vehicles captured in real\nsituations by traffic cameras in different weather and lighting conditions.\nExperimental results show superiority of the proposed framework over the\nstate-of-the-art methods on Iranian on-road vehicle datatset and comparable\nresults on CompuCar dataset with 97.5% and 98.4% accuracies, respectively.\n", "rewritten_text": "The Vehicle Make and Model Recognition (MMR) systems offer a fully automatic framework for recognizing and classifying various vehicle models. While several approaches have been proposed to tackle this challenge, they often operate under restricted conditions. In this study, we present the recognition of vehicle make and model as a fine-grained classification problem and introduce a novel configurable on-road vehicle make and model recognition framework. Leveraging unsupervised feature learning methods, we specifically utilize the Locality-constrained Linear Coding (LLC) method as a rapid feature encoder for encoding the input SIFT features. Our proposed method demonstrates robust performance in diverse real-world environments.\n\nThis framework is capable of recognizing fifty different vehicle models and possesses the ability to classify any vehicle not belonging to the specified fifty classes as an unknown vehicle. Moreover, the MMR framework can be adjusted to prioritize speed or accuracy based on the specific application domain. To evaluate our approach, we conducted experiments on two datasets: the Iranian on-road vehicle dataset and the CompuCar dataset. The Iranian on-road vehicle dataset comprises images of 50 vehicle models captured in real-world scenarios by traffic cameras under varying weather and lighting conditions.\n\nExperimental results showcase the superiority of our proposed framework over state-of-the-art methods on the Iranian on-road vehicle dataset, achieving an accuracy of 97.5%, and comparable results on the CompuCar dataset with an accuracy of 98.4%."}, "2110.10575": {"original_text": "  Recent research in opinion mining proposed word embedding-based topic\nmodeling methods that provide superior coherence compared to traditional topic\nmodeling. In this paper, we demonstrate how these methods can be used to\ndisplay correlated topic models on social media texts using SocialVisTUM, our\nproposed interactive visualization toolkit. It displays a graph with topics as\nnodes and their correlations as edges. Further details are displayed\ninteractively to support the exploration of large text collections, e.g.,\nrepresentative words and sentences of topics, topic and sentiment\ndistributions, hierarchical topic clustering, and customizable, predefined\ntopic labels. The toolkit optimizes automatically on custom data for optimal\ncoherence. We show a working instance of the toolkit on data crawled from\nEnglish social media discussions about organic food consumption. The\nvisualization confirms findings of a qualitative consumer research study.\nSocialVisTUM and its training procedures are accessible online.\n", "rewritten_text": "Recent research in opinion mining has introduced word embedding-based topic modeling methods that demonstrate superior coherence compared to traditional topic modeling. This paper showcases how these methods can be utilized to present correlated topic models in social media texts through SocialVisTUM, an interactive visualization toolkit developed by us. The toolkit showcases a graph with topics as nodes and their correlations as edges. It also provides interactive displays for exploring large text collections, including representative words and sentences of topics, topic and sentiment distributions, hierarchical topic clustering, and customizable predefined topic labels. The toolkit automatically optimizes for optimal coherence on custom data. An example of the toolkit in action is presented using data collected from English social media discussions on organic food consumption, confirming findings from a qualitative consumer research study. SocialVisTUM and its training procedures are accessible online."}, "2404.00851": {"original_text": "  Pre-trained vision-language models have shown impressive success on various\ncomputer vision tasks with their zero-shot generalizability. Recently, prompt\nlearning approaches have been explored to efficiently and effectively adapt the\nvision-language models to a variety of downstream tasks. However, most existing\nprompt learning methods suffer from task overfitting since the general\nknowledge of the pre-trained vision language models is forgotten while the\nprompts are finetuned on a small data set from a specific target task. To\naddress this issue, we propose a Prompt Meta-Regularization (ProMetaR) to\nimprove the generalizability of prompt learning for vision-language models.\nSpecifically, ProMetaR meta-learns both the regularizer and the soft prompts to\nharness the task-specific knowledge from the downstream tasks and task-agnostic\ngeneral knowledge from the vision-language models. Further, ProMetaR augments\nthe task to generate multiple virtual tasks to alleviate the meta-overfitting.\nIn addition, we provide the analysis to comprehend how ProMetaR improves the\ngeneralizability of prompt tuning in the perspective of the gradient alignment.\nOur extensive experiments demonstrate that our ProMetaR improves the\ngeneralizability of conventional prompt learning methods under\nbase-to-base/base-to-new and domain generalization settings. The code of\nProMetaR is available at https://github.com/mlvlab/ProMetaR.\n", "rewritten_text": "Pre-trained vision-language models have demonstrated remarkable success across various computer vision tasks due to their zero-shot generalizability. Recently, there has been exploration into prompt learning approaches aimed at efficiently and effectively adapting these models to a range of downstream tasks. However, many existing prompt learning methods are plagued by task overfitting, as they tend to forget the general knowledge embedded in the pre-trained vision-language models while fine-tuning prompts on small datasets specific to target tasks.\n\nTo tackle this challenge, we introduce Prompt Meta-Regularization (ProMetaR) as a solution to enhance the generalizability of prompt learning for vision-language models. ProMetaR meta-learns both the regularizer and soft prompts to leverage task-specific knowledge from downstream tasks and task-agnostic general knowledge from the vision-language models. Additionally, ProMetaR augments tasks to create multiple virtual tasks, mitigating meta-overfitting.\n\nFurthermore, we conduct an analysis to elucidate how ProMetaR enhances the generalizability of prompt tuning through the lens of gradient alignment. Our extensive experiments showcase that ProMetaR enhances the generalizability of traditional prompt learning methods across base-to-base, base-to-new, and domain generalization scenarios. The code for ProMetaR can be accessed at https://github.com/mlvlab/ProMetaR."}, "2409.12539": {"original_text": "  In radiation therapy (RT), the reliance on pre-treatment computed tomography\n(CT) images encounter challenges due to anatomical changes, necessitating\nadaptive planning. Daily cone-beam CT (CBCT) imaging, pivotal for therapy\nadjustment, falls short in tissue density accuracy. To address this, our\ninnovative approach integrates diffusion models for CT image generation,\noffering precise control over data synthesis. Leveraging a self-training method\nwith knowledge distillation, we maximize CBCT data during therapy, complemented\nby sparse paired fan-beam CTs. This strategy, incorporated into\nstate-of-the-art diffusion-based models, surpasses conventional methods like\nPix2pix and CycleGAN. A meticulously curated dataset of 2800 paired CBCT and CT\nscans, supplemented by 4200 CBCT scans, undergoes preprocessing and teacher\nmodel training, including the Brownian Bridge Diffusion Model (BBDM).\nPseudo-label CT images are generated, resulting in a dataset combining 5600 CT\nimages with corresponding CBCT images. Thorough evaluation using MSE, SSIM,\nPSNR and LPIPS demonstrates superior performance against Pix2pix and CycleGAN.\nOur approach shows promise in generating high-quality CT images from CBCT scans\nin RT.\n", "rewritten_text": "In radiation therapy (RT), challenges arise in relying on pre-treatment computed tomography (CT) images due to anatomical changes, necessitating adaptive planning. Daily cone-beam CT (CBCT) imaging, crucial for therapy adjustments, lacks accuracy in tissue density. To tackle this issue, our innovative approach integrates diffusion models for CT image generation, providing precise control over data synthesis. By employing a self-training method with knowledge distillation, we enhance CBCT data throughout therapy, supported by sparse paired fan-beam CTs. This strategy, embedded in cutting-edge diffusion-based models, outperforms traditional methods such as Pix2pix and CycleGAN.\n\nA meticulously curated dataset comprising 2800 paired CBCT and CT scans, along with 4200 CBCT scans, undergoes preprocessing and teacher model training, incorporating the Brownian Bridge Diffusion Model (BBDM). Pseudo-labeled CT images are produced, resulting in a dataset that combines 5600 CT images with corresponding CBCT images. Comprehensive evaluation using metrics like MSE, SSIM, PSNR, and LPIPS demonstrates superior performance compared to Pix2pix and CycleGAN. Our approach exhibits promise in generating high-quality CT images from CBCT scans in RT."}, "1904.0169": {"original_text": "  We present MonoPSR, a monocular 3D object detection method that leverages\nproposals and shape reconstruction. First, using the fundamental relations of a\npinhole camera model, detections from a mature 2D object detector are used to\ngenerate a 3D proposal per object in a scene. The 3D location of these\nproposals prove to be quite accurate, which greatly reduces the difficulty of\nregressing the final 3D bounding box detection. Simultaneously, a point cloud\nis predicted in an object centered coordinate system to learn local scale and\nshape information. However, the key challenge is how to exploit shape\ninformation to guide 3D localization. As such, we devise aggregate losses,\nincluding a novel projection alignment loss, to jointly optimize these tasks in\nthe neural network to improve 3D localization accuracy. We validate our method\non the KITTI benchmark where we set new state-of-the-art results among\npublished monocular methods, including the harder pedestrian and cyclist\nclasses, while maintaining efficient run-time.\n", "rewritten_text": "We introduce MonoPSR, a monocular 3D object detection approach that utilizes proposals and shape reconstruction. Initially, by applying the fundamental principles of a pinhole camera model, detections generated by a well-established 2D object detector are employed to create a 3D proposal for each object within a scene. The accuracy of the 3D locations of these proposals significantly simplifies the process of predicting the final 3D bounding box detection. Concurrently, a point cloud is forecasted in a coordinate system centered on the object to capture local scale and shape details. Nevertheless, the primary challenge lies in effectively utilizing shape information to enhance 3D localization. To address this, we introduce aggregate losses, including a novel projection alignment loss, to collectively optimize these tasks within the neural network and enhance the accuracy of 3D localization. Our method is validated on the KITTI benchmark, where we achieve new state-of-the-art results compared to other published monocular methods, particularly excelling in the more challenging pedestrian and cyclist classes, all while maintaining efficient run-time performance."}, "2408.02291": {"original_text": "  Unsupervised 3D keypoints estimation from Point Cloud Data (PCD) is a complex\ntask, even more challenging when an object shape is deforming. As keypoints\nshould be semantically and geometrically consistent across all the 3D frames -\neach keypoint should be anchored to a specific part of the deforming shape\nirrespective of intrinsic and extrinsic motion. This paper presents, \"SelfGeo\",\na self-supervised method that computes persistent 3D keypoints of non-rigid\nobjects from arbitrary PCDs without the need of human annotations. The gist of\nSelfGeo is to estimate keypoints between frames that respect invariant\nproperties of deforming bodies. Our main contribution is to enforce that\nkeypoints deform along with the shape while keeping constant geodesic distances\namong them. This principle is then propagated to the design of a set of losses\nwhich minimization let emerge repeatable keypoints in specific semantic\nlocations of the non-rigid shape. We show experimentally that the use of\ngeodesic has a clear advantage in challenging dynamic scenes and with different\nclasses of deforming shapes (humans and animals). Code and data are available\nat: https://github.com/IIT-PAVIS/SelfGeo\n", "rewritten_text": "Estimating 3D keypoints from Point Cloud Data (PCD) without supervision is a complex task, especially when dealing with deforming object shapes. Ensuring that keypoints are both semantically and geometrically consistent across all 3D frames is crucial, with each keypoint needing to be anchored to a specific part of the deforming shape regardless of intrinsic and extrinsic motion. This paper introduces \"SelfGeo,\" a self-supervised method that identifies persistent 3D keypoints on non-rigid objects from arbitrary PCDs without requiring human annotations.\n\nThe essence of SelfGeo lies in computing keypoints between frames that adhere to invariant properties of deforming bodies. Our primary contribution is in ensuring that keypoints deform in conjunction with the shape while maintaining consistent geodesic distances between them. This guiding principle is then applied to the formulation of a series of loss functions, the minimization of which results in the emergence of repeatable keypoints at specific semantic locations on the non-rigid shape.\n\nExperimental results demonstrate the clear advantages of utilizing geodesic distances in challenging dynamic scenes across various classes of deforming shapes, including humans and animals. The code and data for SelfGeo can be accessed at: https://github.com/IIT-PAVIS/SelfGeo"}, "2304.0723": {"original_text": "  Pedestrian attribute recognition (PAR) has received increasing attention\nbecause of its wide application in video surveillance and pedestrian analysis.\nExtracting robust feature representation is one of the key challenges in this\ntask. The existing methods mainly use the convolutional neural network (CNN) as\nthe backbone network to extract features. However, these methods mainly focus\non small discriminative regions while ignoring the global perspective. To\novercome these limitations, we propose a pure transformer-based multi-task PAR\nnetwork named PARFormer, which includes four modules. In the feature extraction\nmodule, we build a transformer-based strong baseline for feature extraction,\nwhich achieves competitive results on several PAR benchmarks compared with the\nexisting CNN-based baseline methods. In the feature processing module, we\npropose an effective data augmentation strategy named batch random mask (BRM)\nblock to reinforce the attentive feature learning of random patches.\nFurthermore, we propose a multi-attribute center loss (MACL) to enhance the\ninter-attribute discriminability in the feature representations. In the\nviewpoint perception module, we explore the impact of viewpoints on pedestrian\nattributes, and propose a multi-view contrastive loss (MCVL) that enables the\nnetwork to exploit the viewpoint information. In the attribute recognition\nmodule, we alleviate the negative-positive imbalance problem to generate the\nattribute predictions. The above modules interact and jointly learn a highly\ndiscriminative feature space, and supervise the generation of the final\nfeatures. Extensive experimental results show that the proposed PARFormer\nnetwork performs well compared to the state-of-the-art methods on several\npublic datasets, including PETA, RAP, and PA100K. Code will be released at\nhttps://github.com/xwf199/PARFormer.\n", "rewritten_text": "Pedestrian attribute recognition (PAR) has garnered increasing attention due to its broad applications in video surveillance and pedestrian analysis. A key challenge in this task is extracting robust feature representations. Current methods primarily rely on convolutional neural networks (CNNs) as the backbone network for feature extraction. However, these approaches tend to focus on small discriminative regions, neglecting the global perspective.\n\nTo address these limitations, we introduce PARFormer, a pure transformer-based multi-task PAR network comprising four modules. In the feature extraction module, we establish a transformer-based strong baseline for feature extraction, yielding competitive results on various PAR benchmarks compared to existing CNN-based methods. The feature processing module introduces a novel data augmentation strategy called batch random mask (BRM) block to enhance attentive feature learning of random patches. Additionally, we propose a multi-attribute center loss (MACL) to boost inter-attribute discriminability in feature representations.\n\nIn the viewpoint perception module, we investigate the influence of viewpoints on pedestrian attributes and introduce a multi-view contrastive loss (MCVL) to enable the network to leverage viewpoint information. The attribute recognition module addresses the negative-positive imbalance issue to generate accurate attribute predictions. These modules collaborate to learn a highly discriminative feature space and supervise the generation of final features.\n\nExtensive experimental results demonstrate that the proposed PARFormer network outperforms state-of-the-art methods on several public datasets, including PETA, RAP, and PA100K. The code will be available at https://github.com/xwf199/PARFormer."}, "2402.09967": {"original_text": "  Large Language Models (LLMs) excel in generating personalized content and\nfacilitating interactive dialogues, showcasing their remarkable aptitude for a\nmyriad of applications. However, their capabilities in reasoning and providing\nexplainable outputs, especially within the context of reasoning abilities,\nremain areas for improvement. In this study, we delve into the reasoning\nabilities of LLMs, highlighting the current challenges and limitations that\nhinder their effectiveness in complex reasoning scenarios.\n", "rewritten_text": "Large Language Models (LLMs) excel in generating personalized content and facilitating interactive dialogues, showcasing their remarkable aptitude for a wide range of applications. However, their capabilities in reasoning and providing explainable outputs, particularly within the context of reasoning abilities, are areas that require improvement. This study delves into the reasoning abilities of LLMs, emphasizing the current challenges and limitations that impede their effectiveness in complex reasoning scenarios."}, "2212.05911": {"original_text": "  Deep learning has emerged as an effective solution for solving the task of\nobject detection in images but at the cost of requiring large labeled datasets.\nTo mitigate this cost, semi-supervised object detection methods, which consist\nin leveraging abundant unlabeled data, have been proposed and have already\nshown impressive results. However, most of these methods require linking a\npseudo-label to a ground-truth object by thresholding. In previous works, this\nthreshold value is usually determined empirically, which is time consuming, and\nonly done for a single data distribution. When the domain, and thus the data\ndistribution, changes, a new and costly parameter search is necessary. In this\nwork, we introduce our method Adaptive Self-Training for Object Detection\n(ASTOD), which is a simple yet effective teacher-student method. ASTOD\ndetermines without cost a threshold value based directly on the ground value of\nthe score histogram. To improve the quality of the teacher predictions, we also\npropose a novel pseudo-labeling procedure. We use different views of the\nunlabeled images during the pseudo-labeling step to reduce the number of missed\npredictions and thus obtain better candidate labels. Our teacher and our\nstudent are trained separately, and our method can be used in an iterative\nfashion by replacing the teacher by the student. On the MS-COCO dataset, our\nmethod consistently performs favorably against state-of-the-art methods that do\nnot require a threshold parameter, and shows competitive results with methods\nthat require a parameter sweep search. Additional experiments with respect to a\nsupervised baseline on the DIOR dataset containing satellite images lead to\nsimilar conclusions, and prove that it is possible to adapt the score threshold\nautomatically in self-training, regardless of the data distribution. The code\nis available at https:// github.com/rvandeghen/ASTOD\n", "rewritten_text": "Deep learning has emerged as an effective solution for object detection in images, albeit at the expense of needing large labeled datasets. To address this issue, semi-supervised object detection methods have been proposed, leveraging abundant unlabeled data to achieve impressive results. However, many of these methods rely on linking a pseudo-label to a ground-truth object through thresholding. Typically, the threshold value is determined empirically in prior works, a time-consuming process that is performed for a single data distribution. Consequently, when the domain changes, necessitating a new and costly parameter search.\n\nIn this study, we introduce Adaptive Self-Training for Object Detection (ASTOD), a straightforward yet efficient teacher-student method. ASTOD determines a threshold value based directly on the ground value of the score histogram, eliminating the need for costly parameter searches. Additionally, we propose a novel pseudo-labeling procedure to enhance the quality of teacher predictions. By utilizing various views of unlabeled images during pseudo-labeling, we reduce missed predictions and improve candidate labels.\n\nOur method involves training the teacher and student separately, allowing for an iterative approach by replacing the teacher with the student. On the MS-COCO dataset, ASTOD consistently outperforms state-of-the-art methods that do not require a threshold parameter and achieves competitive results with methods that do require parameter sweep searches. Further experiments on the DIOR dataset, which contains satellite images, support our findings and demonstrate the automatic adaptation of the score threshold in self-training, irrespective of the data distribution.\n\nThe code for ASTOD is available at https://github.com/rvandeghen/ASTOD."}, "1507.02772": {"original_text": "  Data encoded as symmetric positive definite (SPD) matrices frequently arise\nin many areas of computer vision and machine learning. While these matrices\nform an open subset of the Euclidean space of symmetric matrices, viewing them\nthrough the lens of non-Euclidean Riemannian geometry often turns out to be\nbetter suited in capturing several desirable data properties. However,\nformulating classical machine learning algorithms within such a geometry is\noften non-trivial and computationally expensive. Inspired by the great success\nof dictionary learning and sparse coding for vector-valued data, our goal in\nthis paper is to represent data in the form of SPD matrices as sparse conic\ncombinations of SPD atoms from a learned dictionary via a Riemannian geometric\napproach. To that end, we formulate a novel Riemannian optimization objective\nfor dictionary learning and sparse coding in which the representation loss is\ncharacterized via the affine invariant Riemannian metric. We also present a\ncomputationally simple algorithm for optimizing our model. Experiments on\nseveral computer vision datasets demonstrate superior classification and\nretrieval performance using our approach when compared to sparse coding via\nalternative non-Riemannian formulations.\n", "rewritten_text": "Symmetric positive definite (SPD) matrices are commonly encountered in various fields such as computer vision and machine learning. While these matrices belong to an open subset of the Euclidean space of symmetric matrices, a perspective from non-Euclidean Riemannian geometry often proves more effective in capturing key data properties. However, adapting classical machine learning algorithms to this geometry can be challenging and resource-intensive.\n\nDrawing inspiration from the success of dictionary learning and sparse coding for vector-valued data, the objective of this paper is to represent data as sparse conic combinations of SPD atoms from a learned dictionary using a Riemannian geometric approach. To achieve this, a novel Riemannian optimization objective is formulated for dictionary learning and sparse coding, with the representation loss characterized by the affine invariant Riemannian metric. Additionally, a computationally efficient algorithm is presented for optimizing the model.\n\nExperimental results on various computer vision datasets showcase the superior classification and retrieval performance of our approach compared to sparse coding using alternative non-Riemannian formulations."}, "1603.09742": {"original_text": "  Semantic segmentation is critical to image content understanding and object\nlocalization. Recent development in fully-convolutional neural network (FCN)\nhas enabled accurate pixel-level labeling. One issue in previous works is that\nthe FCN based method does not exploit the object boundary information to\ndelineate segmentation details since the object boundary label is ignored in\nthe network training. To tackle this problem, we introduce a double branch\nfully convolutional neural network, which separates the learning of the\ndesirable semantic class labeling with mask-level object proposals guided by\nrelabeled boundaries. This network, called object boundary guided FCN\n(OBG-FCN), is able to integrate the distinct properties of object shape and\nclass features elegantly in a fully convolutional way with a designed masking\narchitecture. We conduct experiments on the PASCAL VOC segmentation benchmark,\nand show that the end-to-end trainable OBG-FCN system offers great improvement\nin optimizing the target semantic segmentation quality.\n", "rewritten_text": "Semantic segmentation plays a crucial role in understanding image content and localizing objects. Recent advancements in fully-convolutional neural networks (FCNs) have made it possible to achieve accurate pixel-level labeling. However, a limitation of previous approaches is that FCN-based methods often overlook object boundary information, which is essential for detailed segmentation. To address this issue, we propose a novel approach called the Object Boundary Guided FCN (OBG-FCN). This approach utilizes a double-branch fully convolutional neural network that separates the learning of semantic class labeling from mask-level object proposals guided by relabeled boundaries. By integrating object shape and class features effectively through a specially designed masking architecture, the OBG-FCN system elegantly combines these distinct properties in a fully convolutional manner. Experimental results on the PASCAL VOC segmentation benchmark demonstrate that the end-to-end trainable OBG-FCN system significantly enhances the quality of semantic segmentation optimization."}, "2004.03677": {"original_text": "  Image manipulation can be considered a special case of image generation where\nthe image to be produced is a modification of an existing image. Image\ngeneration and manipulation have been, for the most part, tasks that operate on\nraw pixels. However, the remarkable progress in learning rich image and object\nrepresentations has opened the way for tasks such as text-to-image or\nlayout-to-image generation that are mainly driven by semantics. In our work, we\naddress the novel problem of image manipulation from scene graphs, in which a\nuser can edit images by merely applying changes in the nodes or edges of a\nsemantic graph that is generated from the image. Our goal is to encode image\ninformation in a given constellation and from there on generate new\nconstellations, such as replacing objects or even changing relationships\nbetween objects, while respecting the semantics and style from the original\nimage. We introduce a spatio-semantic scene graph network that does not require\ndirect supervision for constellation changes or image edits. This makes it\npossible to train the system from existing real-world datasets with no\nadditional annotation effort.\n", "rewritten_text": "Image manipulation can be seen as a specialized form of image generation, where the resulting image is a modification of an existing one. Traditionally, image generation and manipulation tasks have primarily involved working with raw pixels. However, recent advancements in learning sophisticated image and object representations have paved the way for new tasks like text-to-image or layout-to-image generation, which are predominantly driven by semantics.\n\nIn our research, we tackle the innovative challenge of image manipulation using scene graphs. This approach allows users to edit images by making changes to the nodes or edges of a semantic graph derived from the original image. Our objective is to encode image details within a specific configuration and then create new configurations, such as replacing objects or altering object relationships, while preserving the semantics and style of the original image.\n\nWe propose a spatio-semantic scene graph network that does not necessitate direct supervision for making changes to configurations or editing images. This enables the system to be trained using existing real-world datasets without the need for additional annotations."}, "2208.09669": {"original_text": "  Contextualized word embeddings in language models have given much advance to\nNLP. Intuitively, sentential information is integrated into the representation\nof words, which can help model polysemy. However, context sensitivity also\nleads to the variance of representations, which may break the semantic\nconsistency for synonyms. We quantify how much the contextualized embeddings of\neach word sense vary across contexts in typical pre-trained models. Results\nshow that contextualized embeddings can be highly consistent across contexts.\nIn addition, part-of-speech, number of word senses, and sentence length have an\ninfluence on the variance of sense representations. Interestingly, we find that\nword representations are position-biased, where the first words in different\ncontexts tend to be more similar. We analyze such a phenomenon and also propose\na simple way to alleviate such bias in distance-based word sense disambiguation\nsettings.\n", "rewritten_text": "Contextualized word embeddings in language models have significantly advanced NLP by integrating sentential information into word representations, aiding in modeling polysemy. However, this context sensitivity can also introduce variance in representations, potentially compromising semantic consistency for synonyms. We conducted a study to quantify the extent of variation in contextualized embeddings for each word sense across different contexts in typical pre-trained models. Our results demonstrate that contextualized embeddings can exhibit high consistency across contexts. Furthermore, factors such as part-of-speech, number of word senses, and sentence length influence the variance of sense representations. Notably, we observed a position bias in word representations, where the initial words in various contexts tend to be more similar. We delve into this phenomenon and propose a straightforward method to mitigate such bias in distance-based word sense disambiguation scenarios."}, "1706.00842": {"original_text": "  We present a fully automatic method employing convolutional neural networks\nbased on the 2D U-net architecture and random forest classifier to solve the\nautomatic liver lesion segmentation problem of the ISBI 2017 Liver Tumor\nSegmentation Challenge (LiTS). In order to constrain the ROI in which the\ntumors could be located, a liver segmentation is performed first. For the organ\nsegmentation, an ensemble of convolutional networks is trained to segment a\nliver using a set of 179 liver CT datasets from liver surgery planning. Inside\nof the liver ROI a neural network, trained using 127 challenge training\ndatasets, identifies tumor candidates, which are subsequently filtered with a\nrandom forest classifier yielding the final tumor segmentation. The evaluation\non the 70 challenge test cases resulted in a mean Dice coefficient of 0.65,\nranking our method in the second place.\n", "rewritten_text": "We introduce a fully automatic method that utilizes convolutional neural networks based on the 2D U-net architecture and a random forest classifier to address the automatic liver lesion segmentation challenge presented in the ISBI 2017 Liver Tumor Segmentation Challenge (LiTS). Initially, a liver segmentation is conducted to confine the region of interest where tumors may be present. To achieve organ segmentation, an ensemble of convolutional networks is trained on a dataset comprising 179 liver CT scans from liver surgery planning. Within the liver region of interest, a neural network trained on 127 challenge training datasets identifies potential tumor locations, which are then refined using a random forest classifier to produce the final tumor segmentation. Our method was evaluated on 70 challenge test cases, resulting in a mean Dice coefficient of 0.65, positioning our approach in second place in the rankings."}, "2108.07848": {"original_text": "  Identifying players in sports videos by recognizing their jersey numbers is a\nchallenging task in computer vision. We have designed and implemented a\nmulti-task learning network for jersey number recognition. In order to train a\nnetwork to recognize jersey numbers, two output label representations are used\n(1) Holistic - considers the entire jersey number as one class, and (2)\nDigit-wise - considers the two digits in a jersey number as two separate\nclasses. The proposed network learns both holistic and digit-wise\nrepresentations through a multi-task loss function. We determine the optimal\nweights to be assigned to holistic and digit-wise losses through an ablation\nstudy. Experimental results demonstrate that the proposed multi-task learning\nnetwork performs better than the constituent holistic and digit-wise\nsingle-task learning networks.\n", "rewritten_text": "Identifying players in sports videos by recognizing their jersey numbers presents a challenging task in computer vision. A multi-task learning network has been designed and implemented specifically for jersey number recognition. To train the network effectively, two output label representations are utilized: (1) Holistic, which views the entire jersey number as one class, and (2) Digit-wise, which treats the two digits in a jersey number as separate classes. The proposed network is capable of learning both holistic and digit-wise representations simultaneously through a multi-task loss function. Optimal weights for the holistic and digit-wise losses are determined through an ablation study. Experimental results clearly indicate that the proposed multi-task learning network outperforms the individual holistic and digit-wise single-task learning networks."}, "2104.01136": {"original_text": "  We design a family of image classification architectures that optimize the\ntrade-off between accuracy and efficiency in a high-speed regime. Our work\nexploits recent findings in attention-based architectures, which are\ncompetitive on highly parallel processing hardware. We revisit principles from\nthe extensive literature on convolutional neural networks to apply them to\ntransformers, in particular activation maps with decreasing resolutions. We\nalso introduce the attention bias, a new way to integrate positional\ninformation in vision transformers. As a result, we propose LeVIT: a hybrid\nneural network for fast inference image classification. We consider different\nmeasures of efficiency on different hardware platforms, so as to best reflect a\nwide range of application scenarios. Our extensive experiments empirically\nvalidate our technical choices and show they are suitable to most\narchitectures. Overall, LeViT significantly outperforms existing convnets and\nvision transformers with respect to the speed/accuracy tradeoff. For example,\nat 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on\nCPU. We release the code at https://github.com/facebookresearch/LeViT\n", "rewritten_text": "We have developed a family of image classification architectures that aim to optimize the balance between accuracy and efficiency in high-speed scenarios. Our approach leverages recent advancements in attention-based architectures, which have shown competitiveness when deployed on highly parallel processing hardware. By drawing upon established principles from the extensive literature on convolutional neural networks and adapting them for transformers, particularly focusing on activation maps with decreasing resolutions, we have introduced the concept of attention bias. This novel approach allows for the integration of positional information in vision transformers.\n\nOur resulting creation, LeViT, represents a hybrid neural network tailored for rapid inference in image classification tasks. We have taken into account various efficiency metrics across different hardware platforms to ensure applicability in a wide array of scenarios. Through a series of comprehensive experiments, we have empirically validated our technical decisions, demonstrating their compatibility with most architectures.\n\nOverall, LeViT surpasses existing convolutional networks and vision transformers in terms of the trade-off between speed and accuracy. For instance, at 80% ImageNet top-1 accuracy, LeViT outperforms EfficientNet on CPU by a factor of five in terms of speed. The code for LeViT is publicly available at https://github.com/facebookresearch/LeViT."}, "2103.02984": {"original_text": "  Abrupt motion of camera or objects in a scene result in a blurry video, and\ntherefore recovering high quality video requires two types of enhancements:\nvisual enhancement and temporal upsampling. A broad range of research attempted\nto recover clean frames from blurred image sequences or temporally upsample\nframes by interpolation, yet there are very limited studies handling both\nproblems jointly. In this work, we present a novel framework for deblurring,\ninterpolating and extrapolating sharp frames from a motion-blurred video in an\nend-to-end manner. We design our framework by first learning the pixel-level\nmotion that caused the blur from the given inputs via optical flow estimation\nand then predict multiple clean frames by warping the decoded features with the\nestimated flows. To ensure temporal coherence across predicted frames and\naddress potential temporal ambiguity, we propose a simple, yet effective\nflow-based rule. The effectiveness and favorability of our approach are\nhighlighted through extensive qualitative and quantitative evaluations on\nmotion-blurred datasets from high speed videos.\n", "rewritten_text": "Abrupt camera or object movements in a scene can result in a blurry video. Therefore, achieving high-quality video recovery involves two key enhancements: visual enhancement and temporal upsampling. While previous research has focused on recovering clean frames from blurred image sequences or temporally upsampling frames through interpolation, there is a lack of studies that address both issues simultaneously. \n\nIn this study, we introduce a novel framework for deblurring, interpolating, and extrapolating sharp frames from motion-blurred videos in an end-to-end manner. Our framework begins by learning the pixel-level motion responsible for the blur from the input data using optical flow estimation. Subsequently, we predict multiple clean frames by warping the decoded features with the estimated flows. To ensure temporal coherence among the predicted frames and resolve potential temporal ambiguities, we propose a simple yet effective flow-based rule. \n\nThe effectiveness and superiority of our approach are demonstrated through comprehensive qualitative and quantitative evaluations on motion-blurred datasets sourced from high-speed videos."}, "1611.09932": {"original_text": "  Compared to earlier multistage frameworks using CNN features, recent\nend-to-end deep approaches for fine-grained recognition essentially enhance the\nmid-level learning capability of CNNs. Previous approaches achieve this by\nintroducing an auxiliary network to infuse localization information into the\nmain classification network, or a sophisticated feature encoding method to\ncapture higher order feature statistics. We show that mid-level representation\nlearning can be enhanced within the CNN framework, by learning a bank of\nconvolutional filters that capture class-specific discriminative patches\nwithout extra part or bounding box annotations. Such a filter bank is well\nstructured, properly initialized and discriminatively learned through a novel\nasymmetric multi-stream architecture with convolutional filter supervision and\na non-random layer initialization. Experimental results show that our approach\nachieves state-of-the-art on three publicly available fine-grained recognition\ndatasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and\nvisualizations are provided to understand our approach.\n", "rewritten_text": "Recent end-to-end deep approaches for fine-grained recognition have significantly improved the mid-level learning capability of Convolutional Neural Networks (CNNs) compared to earlier multistage frameworks utilizing CNN features. In the past, this enhancement was achieved by incorporating an auxiliary network to integrate localization information into the primary classification network, or by employing a sophisticated feature encoding method to capture higher-order feature statistics. However, we demonstrate that mid-level representation learning can be effectively enhanced within the CNN framework itself. This is accomplished by training a bank of convolutional filters that specifically capture discriminative patches unique to each class, without the need for additional part or bounding box annotations.\n\nThe proposed filter bank is meticulously structured, appropriately initialized, and discriminatively learned through an innovative asymmetric multi-stream architecture that includes convolutional filter supervision and non-random layer initialization. Experimental results showcase the effectiveness of our approach, as it outperforms existing methods on three widely used fine-grained recognition datasets (CUB-200-2011, Stanford Cars, and FGVC-Aircraft). Additionally, we provide ablation studies and visualizations to offer insights into the workings of our approach."}, "1504.01013": {"original_text": "  Recent advances in semantic image segmentation have mostly been achieved by\ntraining deep convolutional neural networks (CNNs). We show how to improve\nsemantic segmentation through the use of contextual information; specifically,\nwe explore `patch-patch' context between image regions, and `patch-background'\ncontext. For learning from the patch-patch context, we formulate Conditional\nRandom Fields (CRFs) with CNN-based pairwise potential functions to capture\nsemantic correlations between neighboring patches. Efficient piecewise training\nof the proposed deep structured model is then applied to avoid repeated\nexpensive CRF inference for back propagation. For capturing the\npatch-background context, we show that a network design with traditional\nmulti-scale image input and sliding pyramid pooling is effective for improving\nperformance. Our experimental results set new state-of-the-art performance on a\nnumber of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC\n2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an\nintersection-over-union score of 78.0 on the challenging PASCAL VOC 2012\ndataset.\n", "rewritten_text": "Recent advancements in semantic image segmentation have predominantly been made by training deep convolutional neural networks (CNNs). This study demonstrates a method to enhance semantic segmentation by leveraging contextual information. Specifically, we investigate the contextual relationships between image regions, known as 'patch-patch' context, as well as the context between patches and the background.\n\nTo utilize the patch-patch context, we introduce Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations among neighboring patches. To streamline the training process of the deep structured model, we implement efficient piecewise training to avoid the need for repetitive and computationally expensive CRF inference during backpropagation.\n\nIn capturing the patch-background context, we propose a network architecture that incorporates traditional multi-scale image input and sliding pyramid pooling, which proves to be effective in enhancing performance. Our experimental findings showcase a new state-of-the-art level of performance across various popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. Notably, we achieve an intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012 dataset."}, "2106.1262": {"original_text": "  The self-attention-based model, transformer, is recently becoming the leading\nbackbone in the field of computer vision. In spite of the impressive success\nmade by transformers in a variety of vision tasks, it still suffers from heavy\ncomputation and intensive memory costs. To address this limitation, this paper\npresents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).\nWe start by observing a large amount of redundant computation, mainly spent on\nuncorrelated input patches, and then introduce an interpretable module to\ndynamically and gracefully drop these redundant patches. This novel framework\nis then extended to a hierarchical structure, where uncorrelated tokens at\ndifferent stages are gradually removed, resulting in a considerable shrinkage\nof computational cost. We include extensive experiments on both image and video\ntasks, where our method could deliver up to 1.4x speed-up for state-of-the-art\nmodels like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy.\nMore importantly, contrary to other acceleration approaches, our method is\ninherently interpretable with substantial visual evidence, making vision\ntransformer closer to a more human-understandable architecture while being\nlighter. We demonstrate that the interpretability that naturally emerged in our\nframework can outperform the raw attention learned by the original visual\ntransformer, as well as those generated by off-the-shelf interpretation\nmethods, with both qualitative and quantitative results. Project Page:\nhttp://people.csail.mit.edu/bpan/ia-red/.\n", "rewritten_text": "The transformer, a self-attention-based model, has recently emerged as the primary backbone in the field of computer vision. Despite the remarkable success achieved by transformers across various vision tasks, they still face challenges related to high computation and memory requirements. To overcome this limitation, this paper introduces the Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).\n\nThe framework identifies a significant amount of redundant computation, particularly in uncorrelated input patches, and introduces an interpretable module to selectively drop these redundant patches dynamically and gracefully. This innovative approach is further developed into a hierarchical structure, gradually eliminating uncorrelated tokens at different stages, resulting in a substantial reduction in computational costs.\n\nExtensive experiments are conducted on both image and video tasks, demonstrating that our method can achieve up to a 1.4x speed-up for state-of-the-art models such as DeiT and TimeSformer, with a minimal sacrifice of less than 0.7% in accuracy. Importantly, unlike other acceleration techniques, our method is inherently interpretable, providing significant visual evidence and bringing vision transformers closer to a more human-understandable architecture while maintaining efficiency.\n\nThe study shows that the interpretability inherent in our framework can outperform the raw attention learned by the original visual transformer and interpretations generated by standard methods, supported by both qualitative and quantitative results. For more information, visit the project page at http://people.csail.mit.edu/bpan/ia-red/."}, "1803.03852": {"original_text": "  Tracking the pose of instruments is a central problem in image-guided\nsurgery. For microscopic scenarios, optical coherence tomography (OCT) is\nincreasingly used as an imaging modality. OCT is suitable for accurate pose\nestimation due to its micrometer range resolution and volumetric field of view.\nHowever, OCT image processing is challenging due to speckle noise and\nreflection artifacts in addition to the images' 3D nature. We address pose\nestimation from OCT volume data with a new deep learning-based tracking\nframework. For this purpose, we design a new 3D convolutional neural network\n(CNN) architecture to directly predict the 6D pose of a small marker geometry\nfrom OCT volumes. We use a hexapod robot to automatically acquire labeled data\npoints which we use to train 3D CNN architectures for multi-output regression.\nWe use this setup to provide an in-depth analysis on deep learning-based pose\nestimation from volumes. Specifically, we demonstrate that exploiting volume\ninformation for pose estimation yields higher accuracy than relying on 2D\nrepresentations with depth information. Supporting this observation, we provide\nquantitative and qualitative results that 3D CNNs effectively exploit the depth\nstructure of marker objects. Regarding the deep learning aspect, we present\nefficient design principles for 3D CNNs, making use of insights from the 2D\ndeep learning community. In particular, we present Inception3D as a new\narchitecture which performs best for our application. We show that our deep\nlearning approach reaches errors at our ground-truth label's resolution. We\nachieve a mean average error of $\\SI{14.89 \\pm 9.3}{\\micro\\metre}$ and\n$\\SI{0.096 \\pm 0.072}{\\degree}$ for position and orientation learning,\nrespectively.\n", "rewritten_text": "Tracking the position of instruments is a key challenge in image-guided surgery, particularly in microscopic settings where optical coherence tomography (OCT) is increasingly utilized as an imaging technique. OCT stands out for its high-resolution in the micrometer range and its wide field of view, making it well-suited for precise position estimation. However, processing OCT images poses difficulties due to speckle noise, reflection artifacts, and the inherently three-dimensional nature of the images.\n\nTo tackle the issue of position estimation from OCT volume data, we introduce a novel tracking framework based on deep learning. Our approach involves the development of a new 3D convolutional neural network (CNN) architecture specifically designed to predict the six-dimensional pose of small marker geometries directly from OCT volumes. We leverage a hexapod robot to automatically generate labeled data points, which are then used to train the 3D CNN architectures for multi-output regression.\n\nThrough our methodology, we conduct a comprehensive analysis of deep learning-based pose estimation from volumes. Our findings reveal that utilizing volume information leads to higher accuracy compared to relying solely on 2D representations with depth information. We support this conclusion with quantitative and qualitative results demonstrating the effectiveness of 3D CNNs in capturing the depth structure of marker objects.\n\nIn terms of the deep learning aspect, we introduce efficient design principles for 3D CNNs, drawing on insights from the 2D deep learning community. Notably, we propose Inception3D as a new architecture that outperforms others in our application. Our deep learning approach achieves errors at the resolution of our ground-truth labels, with a mean average error of $14.89 \u00b1 9.3 \u03bcm$ for position learning and $0.096 \u00b1 0.072\u00b0$ for orientation learning."}, "2104.04986": {"original_text": "  Aspect-based Sentiment Analysis (ABSA), aiming at predicting the polarities\nfor aspects, is a fine-grained task in the field of sentiment analysis.\nPrevious work showed syntactic information, e.g. dependency trees, can\neffectively improve the ABSA performance. Recently, pre-trained models (PTMs)\nalso have shown their effectiveness on ABSA. Therefore, the question naturally\narises whether PTMs contain sufficient syntactic information for ABSA so that\nwe can obtain a good ABSA model only based on PTMs. In this paper, we firstly\ncompare the induced trees from PTMs and the dependency parsing trees on several\npopular models for the ABSA task, showing that the induced tree from fine-tuned\nRoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis\nexperiments reveal that the FT-RoBERTa Induced Tree is more\nsentiment-word-oriented and could benefit the ABSA task. The experiments also\nshow that the pure RoBERTa-based model can outperform or approximate to the\nprevious SOTA performances on six datasets across four languages since it\nimplicitly incorporates the task-oriented syntactic information.\n", "rewritten_text": "Aspect-based Sentiment Analysis (ABSA) is a fine-grained task within the field of sentiment analysis that aims to predict polarities for aspects. Previous research has demonstrated that incorporating syntactic information, such as dependency trees, can significantly enhance ABSA performance. More recently, pre-trained models (PTMs) have also proven to be effective in ABSA tasks.\n\nThis raises the question of whether PTMs contain enough syntactic information for ABSA to develop a high-performing model solely based on PTMs. In this study, we first compare the trees induced by PTMs with dependency parsing trees from various popular models for ABSA. Our findings show that the induced tree from fine-tuned RoBERTa (FT-RoBERTa) outperforms the parser-provided tree.\n\nFurther analysis experiments indicate that the FT-RoBERTa Induced Tree is more sentiment-word-oriented and can be advantageous for the ABSA task. The experiments also demonstrate that a pure RoBERTa-based model can either outperform or closely approximate the state-of-the-art performances on six datasets across four languages, as it implicitly integrates task-specific syntactic information."}, "2012.11581": {"original_text": "  Humans live within a 3D space and constantly interact with it to perform\ntasks. Such interactions involve physical contact between surfaces that is\nsemantically meaningful. Our goal is to learn how humans interact with scenes\nand leverage this to enable virtual characters to do the same. To that end, we\nintroduce a novel Human-Scene Interaction (HSI) model that encodes proximal\nrelationships, called POSA for \"Pose with prOximitieS and contActs\". The\nrepresentation of interaction is body-centric, which enables it to generalize\nto new scenes. Specifically, POSA augments the SMPL-X parametric human body\nmodel such that, for every mesh vertex, it encodes (a) the contact probability\nwith the scene surface and (b) the corresponding semantic scene label. We learn\nPOSA with a VAE conditioned on the SMPL-X vertices, and train on the PROX\ndataset, which contains SMPL-X meshes of people interacting with 3D scenes, and\nthe corresponding scene semantics from the PROX-E dataset. We demonstrate the\nvalue of POSA with two applications. First, we automatically place 3D scans of\npeople in scenes. We use a SMPL-X model fit to the scan as a proxy and then\nfind its most likely placement in 3D. POSA provides an effective representation\nto search for \"affordances\" in the scene that match the likely contact\nrelationships for that pose. We perform a perceptual study that shows\nsignificant improvement over the state of the art on this task. Second, we show\nthat POSA's learned representation of body-scene interaction supports monocular\nhuman pose estimation that is consistent with a 3D scene, improving on the\nstate of the art. Our model and code are available for research purposes at\nhttps://posa.is.tue.mpg.de.\n", "rewritten_text": "Humans exist within a three-dimensional space and engage with it continuously to carry out tasks. These interactions involve meaningful physical contact between surfaces. Our objective is to understand how humans interact with their surroundings and utilize this knowledge to enable virtual characters to mimic these interactions. To achieve this, we introduce a new model called Human-Scene Interaction (HSI), referred to as POSA (Pose with prOximitieS and contActs). The POSA model focuses on encoding close relationships and interactions, with a body-centric representation that allows for generalization to different scenes.\n\nSpecifically, POSA enhances the SMPL-X parametric human body model by encoding, for each mesh vertex, (a) the probability of contact with the scene surface and (b) the corresponding semantic label of the scene. We train the POSA model using a Variational Autoencoder (VAE) conditioned on the SMPL-X vertices, utilizing the PROX dataset, which includes SMPL-X meshes of individuals interacting with three-dimensional scenes, along with scene semantics from the PROX-E dataset.\n\nWe showcase the effectiveness of POSA through two applications. Firstly, we automate the placement of 3D scans of individuals within scenes by fitting a SMPL-X model to the scan and determining its optimal placement in 3D space. POSA facilitates the identification of \"affordances\" in the scene that align with the probable contact relationships for a given pose. A perceptual study demonstrates a significant improvement over existing methods in this task. Secondly, we illustrate that POSA's learned representation of body-scene interaction supports monocular human pose estimation that aligns with a three-dimensional scene, surpassing current standards.\n\nOur model and code are accessible for research purposes at https://posa.is.tue.mpg.de."}, "1609.04079": {"original_text": "  We present a single-shot system to recover surface geometry of objects with\nspatially-varying albedos, from images captured under a calibrated RGB\nphotometric stereo setup---with three light directions multiplexed across\ndifferent color channels in the observed RGB image. Since the problem is\nill-posed point-wise, we assume that the albedo map can be modeled as\npiece-wise constant with a restricted number of distinct albedo values. We show\nthat under ideal conditions, the shape of a non-degenerate local constant\nalbedo surface patch can theoretically be recovered exactly. Moreover, we\npresent a practical and efficient algorithm that uses this model to robustly\nrecover shape from real images. Our method first reasons about shape locally in\na dense set of patches in the observed image, producing shape distributions for\nevery patch. These local distributions are then combined to produce a single\nconsistent surface normal map. We demonstrate the efficacy of the approach\nthrough experiments on both synthetic renderings as well as real captured\nimages.\n", "rewritten_text": "We introduce a single-shot system designed to reconstruct the surface geometry of objects with spatially-varying albedos. This is achieved using images obtained from a calibrated RGB photometric stereo setup, where three light directions are multiplexed across different color channels in the observed RGB image. Given the inherently ill-posed nature of the problem at a point-wise level, we make the assumption that the albedo map can be represented as piece-wise constant with a limited number of distinct albedo values.\n\nOur study demonstrates that, under ideal conditions, the shape of a non-degenerate local constant albedo surface patch can be theoretically reconstructed with precision. Additionally, we present an efficient and practical algorithm that leverages this model to reliably recover shape information from real-world images. The proposed method initially analyzes shape characteristics locally within a dense array of patches in the observed image, generating shape distributions for each patch. These local distributions are then amalgamated to create a unified surface normal map that is consistent.\n\nThe effectiveness of our approach is validated through experiments conducted on both synthetic renderings and actual captured images."}, "2305.15699": {"original_text": "  Understanding action recognition in egocentric videos has emerged as a vital\nresearch topic with numerous practical applications. With the limitation in the\nscale of egocentric data collection, learning robust deep learning-based action\nrecognition models remains difficult. Transferring knowledge learned from the\nlarge-scale exocentric data to the egocentric data is challenging due to the\ndifference in videos across views. Our work introduces a novel cross-view\nlearning approach to action recognition (CVAR) that effectively transfers\nknowledge from the exocentric to the selfish view. First, we present a novel\ngeometric-based constraint into the self-attention mechanism in Transformer\nbased on analyzing the camera positions between two views. Then, we propose a\nnew cross-view self-attention loss learned on unpaired cross-view data to\nenforce the self-attention mechanism learning to transfer knowledge across\nviews. Finally, to further improve the performance of our cross-view learning\napproach, we present the metrics to measure the correlations in videos and\nattention maps effectively. Experimental results on standard egocentric action\nrecognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, and\nEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-art\nperformance.\n", "rewritten_text": "Understanding action recognition in egocentric videos has become a crucial research topic with numerous practical applications. However, due to the limited scale of egocentric data collection, developing robust deep learning-based action recognition models remains challenging. The transfer of knowledge from large-scale exocentric data to egocentric data presents difficulties due to variations in videos across different perspectives.\n\nOur work introduces a novel approach to action recognition, termed Cross-View Learning for Action Recognition (CVAR), which effectively transfers knowledge from exocentric to egocentric views. Initially, we incorporate a novel geometric-based constraint into the self-attention mechanism of the Transformer model by analyzing the differences in camera positions between views. Subsequently, we propose a new cross-view self-attention loss, trained on unpaired cross-view data, to facilitate the transfer of knowledge across perspectives.\n\nTo enhance the performance of our cross-view learning approach, we introduce metrics to effectively measure correlations in videos and attention maps. Experimental results on standard egocentric action recognition benchmarks, including Charades-Ego, EPIC-Kitchens-55, and EPIC-Kitchens-100, demonstrate the effectiveness and state-of-the-art performance of our approach."}, "2407.10753": {"original_text": "  Accurate depth information is crucial for enhancing the performance of\nmulti-view 3D object detection. Despite the success of some existing multi-view\n3D detectors utilizing pixel-wise depth supervision, they overlook two\nsignificant phenomena: 1) the depth supervision obtained from LiDAR points is\nusually distributed on the surface of the object, which is not so friendly to\nexisting DETR-based 3D detectors due to the lack of the depth of 3D object\ncenter; 2) for distant objects, fine-grained depth estimation of the whole\nobject is more challenging. Therefore, we argue that the object-wise depth (or\n3D center of the object) is essential for accurate detection. In this paper, we\npropose a new multi-view 3D object detector named OPEN, whose main idea is to\neffectively inject object-wise depth information into the network through our\nproposed object-wise position embedding. Specifically, we first employ an\nobject-wise depth encoder, which takes the pixel-wise depth map as a prior, to\naccurately estimate the object-wise depth. Then, we utilize the proposed\nobject-wise position embedding to encode the object-wise depth information into\nthe transformer decoder, thereby producing 3D object-aware features for final\ndetection. Extensive experiments verify the effectiveness of our proposed\nmethod. Furthermore, OPEN achieves a new state-of-the-art performance with\n64.4% NDS and 56.7% mAP on the nuScenes test benchmark.\n", "rewritten_text": "Accurate depth information plays a crucial role in enhancing the performance of multi-view 3D object detection. While some existing multi-view 3D detectors have achieved success using pixel-wise depth supervision, they often overlook two significant phenomena. Firstly, the depth supervision derived from LiDAR points tends to be distributed on the object's surface, which poses challenges for existing DETR-based 3D detectors as they lack the depth information of the 3D object center. Secondly, fine-grained depth estimation for distant objects presents additional difficulties. Therefore, we contend that object-wise depth, or the 3D center of the object, is essential for precise detection.\n\nIn this study, we introduce a novel multi-view 3D object detector called OPEN. The primary concept behind OPEN is to effectively integrate object-wise depth information into the network using our proposed object-wise position embedding. Initially, we employ an object-wise depth encoder that utilizes the pixel-wise depth map as a reference to accurately estimate the object-wise depth. Subsequently, we leverage the object-wise position embedding to encode the object-wise depth information into the transformer decoder, generating 3D object-aware features for the final detection. Through extensive experiments, we validate the effectiveness of our proposed approach.\n\nMoreover, OPEN achieves a new state-of-the-art performance, boasting 64.4% NDS and 56.7% mAP on the nuScenes test benchmark."}, "2308.10445": {"original_text": "  Incremental learning aims to overcome catastrophic forgetting when learning\ndeep networks from sequential tasks. With impressive learning efficiency and\nperformance, prompt-based methods adopt a fixed backbone to sequential tasks by\nlearning task-specific prompts. However, existing prompt-based methods heavily\nrely on strong pretraining (typically trained on ImageNet-21k), and we find\nthat their models could be trapped if the potential gap between the pretraining\ntask and unknown future tasks is large. In this work, we develop a learnable\nAdaptive Prompt Generator (APG). The key is to unify the prompt retrieval and\nprompt learning processes into a learnable prompt generator. Hence, the whole\nprompting process can be optimized to reduce the negative effects of the gap\nbetween tasks effectively. To make our APG avoid learning ineffective\nknowledge, we maintain a knowledge pool to regularize APG with the feature\ndistribution of each class. Extensive experiments show that our method\nsignificantly outperforms advanced methods in exemplar-free incremental\nlearning without (strong) pretraining. Besides, under strong retraining, our\nmethod also has comparable performance to existing prompt-based models, showing\nthat our method can still benefit from pretraining. Codes can be found at\nhttps://github.com/TOM-tym/APG\n", "rewritten_text": "Incremental learning aims to address catastrophic forgetting that occurs when training deep networks on sequential tasks. Prompt-based methods have shown impressive learning efficiency and performance by utilizing a fixed backbone for sequential tasks and learning task-specific prompts. However, existing prompt-based methods heavily depend on strong pretraining, typically conducted on ImageNet-21k. We have observed that these models may struggle when faced with a significant gap between the pretraining task and future unknown tasks.\n\nIn this study, we introduce a learnable Adaptive Prompt Generator (APG) to mitigate this issue. The key innovation is the integration of prompt retrieval and prompt learning processes into a single learnable prompt generator. This approach allows for the optimization of the prompting process to effectively reduce the negative impact of task gaps. To prevent the acquisition of ineffective knowledge, we implement a knowledge pool to regularize the APG based on the feature distribution of each class.\n\nExtensive experiments demonstrate that our method outperforms state-of-the-art techniques in exemplar-free incremental learning without the need for strong pretraining. Furthermore, when subjected to strong retraining, our method achieves comparable performance to existing prompt-based models, indicating its ability to leverage pretraining benefits. The source code is available at https://github.com/TOM-tym/APG."}, "2409.18355": {"original_text": "  Cone Beam Computed Tomography (CBCT) finds diverse applications in medicine.\nEnsuring high image quality in CBCT scans is essential for accurate diagnosis\nand treatment delivery. Yet, the susceptibility of CBCT images to noise and\nartifacts undermines both their usefulness and reliability. Existing methods\ntypically address CBCT artifacts through image-to-image translation approaches.\nThese methods, however, are limited by the artifact types present in the\ntraining data, which may not cover the complete spectrum of CBCT degradations\nstemming from variations in imaging protocols. Gathering additional data to\nencompass all possible scenarios can often pose a challenge. To address this,\nwe present SinoSynth, a physics-based degradation model that simulates various\nCBCT-specific artifacts to generate a diverse set of synthetic CBCT images from\nhigh-quality CT images without requiring pre-aligned data. Through extensive\nexperiments, we demonstrate that several different generative networks trained\non our synthesized data achieve remarkable results on heterogeneous\nmulti-institutional datasets, outperforming even the same networks trained on\nactual data. We further show that our degradation model conveniently provides\nan avenue to enforce anatomical constraints in conditional generative models,\nyielding high-quality and structure-preserving synthetic CT images.\n", "rewritten_text": "Cone Beam Computed Tomography (CBCT) has a wide range of applications in medicine. It is crucial to ensure high image quality in CBCT scans for accurate diagnosis and treatment delivery. However, CBCT images are prone to noise and artifacts, which can compromise their utility and reliability. Current methods typically address CBCT artifacts using image-to-image translation approaches. These methods are constrained by the types of artifacts present in the training data, which may not cover the full spectrum of CBCT degradations resulting from variations in imaging protocols. Acquiring additional data to encompass all possible scenarios can be challenging.\n\nTo tackle this issue, we introduce SinoSynth, a physics-based degradation model that simulates various CBCT-specific artifacts to generate a diverse set of synthetic CBCT images from high-quality CT images without the need for pre-aligned data. Through extensive experiments, we demonstrate that several generative networks trained on our synthesized data achieve outstanding results on diverse multi-institutional datasets, surpassing networks trained on actual data. Additionally, we show that our degradation model offers a convenient way to enforce anatomical constraints in conditional generative models, producing high-quality synthetic CT images that preserve anatomical structures effectively."}, "1904.07904": {"original_text": "  Spoken question answering (SQA) is challenging due to complex reasoning on\ntop of the spoken documents. The recent studies have also shown the\ncatastrophic impact of automatic speech recognition (ASR) errors on SQA.\nTherefore, this work proposes to mitigate the ASR errors by aligning the\nmismatch between ASR hypotheses and their corresponding reference\ntranscriptions. An adversarial model is applied to this domain adaptation task,\nwhich forces the model to learn domain-invariant features the QA model can\neffectively utilize in order to improve the SQA results. The experiments\nsuccessfully demonstrate the effectiveness of our proposed model, and the\nresults are better than the previous best model by 2% EM score.\n", "rewritten_text": "Answering spoken questions (SQA) presents a challenge due to the need for complex reasoning based on spoken documents. Recent studies have highlighted the significant negative impact of errors in automatic speech recognition (ASR) on SQA outcomes. To address this issue, this study suggests a method to reduce ASR errors by aligning the discrepancies between ASR hypotheses and their corresponding reference transcriptions. An adversarial model is employed for this domain adaptation task, compelling the model to acquire domain-invariant features that the QA model can effectively leverage to enhance SQA performance. The experiments conducted successfully demonstrate the efficacy of our proposed model, surpassing the previous best model by a 2% increase in EM score."}, "2310.00936": {"original_text": "  Recent studies on StyleGAN variants show promising performances for various\ngeneration tasks. In these models, latent codes have traditionally been\nmanipulated and searched for the desired images. However, this approach\nsometimes suffers from a lack of photorealism in generated images due to a lack\nof knowledge about the geometry of the trained latent space. In this paper, we\nshow a simple unsupervised method that provides well-trained local latent\nsubspace, enabling latent code navigation while preserving the photorealism of\nthe generated images. Specifically, the method identifies densely mapped latent\nspaces and restricts latent manipulations within the local latent subspace.\nExperimental results demonstrate that images generated within the local latent\nsubspace maintain photorealism even when the latent codes are significantly and\nrepeatedly manipulated. Moreover, experiments show that the method can be\napplied to latent code optimization for various types of style-based models.\nOur empirical evidence of the method will benefit applications in style-based\nmodels.\n", "rewritten_text": "Recent studies on StyleGAN variants have shown promising performances for various generation tasks. In these models, latent codes have traditionally been manipulated and searched to produce desired images. However, this approach can sometimes result in a lack of photorealism in the generated images due to a limited understanding of the geometry of the trained latent space. \n\nThis paper introduces a simple unsupervised method that establishes a well-trained local latent subspace, facilitating latent code navigation while maintaining the photorealism of the generated images. The method specifically identifies densely mapped latent spaces and confines latent manipulations within the local latent subspace. \n\nExperimental results illustrate that images generated within the local latent subspace retain photorealism even when the latent codes are significantly and repeatedly manipulated. Furthermore, the experiments demonstrate the method's applicability to latent code optimization across various types of style-based models. \n\nThe empirical evidence presented in this paper suggests that this method can enhance applications in style-based models."}, "2310.00796": {"original_text": "  Strong inductive biases enable learning from little data and help\ngeneralization outside of the training distribution. Popular neural\narchitectures such as Transformers lack strong structural inductive biases for\nseq2seq NLP tasks on their own. Consequently, they struggle with systematic\ngeneralization beyond the training distribution, e.g. with extrapolating to\nlonger inputs, even when pre-trained on large amounts of text. We show how a\nstructural inductive bias can be efficiently injected into a seq2seq model by\npre-training it to simulate structural transformations on synthetic data.\nSpecifically, we inject an inductive bias towards Finite State Transducers\n(FSTs) into a Transformer by pre-training it to simulate FSTs given their\ndescriptions. Our experiments show that our method imparts the desired\ninductive bias, resulting in improved systematic generalization and better\nfew-shot learning for FST-like tasks. Our analysis shows that fine-tuned models\naccurately capture the state dynamics of the unseen underlying FSTs, suggesting\nthat the simulation process is internalized by the fine-tuned model.\n", "rewritten_text": "Robust inductive biases are crucial for effective learning from limited data and facilitating generalization beyond the training dataset. While popular neural architectures like Transformers lack strong structural inductive biases for seq2seq NLP tasks on their own, they encounter challenges in systematic generalization beyond the training distribution, such as extrapolating to longer inputs, even after extensive pre-training on vast amounts of text. To address this limitation, we propose a method to efficiently introduce a structural inductive bias into a seq2seq model by pre-training it to replicate structural transformations using synthetic data. Specifically, we incorporate an inductive bias towards Finite State Transducers (FSTs) into a Transformer by pre-training it to mimic FSTs based on their descriptions. Our experiments demonstrate that this approach successfully imparts the desired inductive bias, leading to enhanced systematic generalization and improved few-shot learning performance on FST-like tasks. Furthermore, our analysis reveals that fine-tuned models accurately capture the state dynamics of unseen underlying FSTs, indicating that the simulation process becomes internalized by the fine-tuned model."}, "2309.11566": {"original_text": "  We introduce SignBank+, a clean version of the SignBank dataset, optimized\nfor machine translation between spoken language text and SignWriting, a\nphonetic sign language writing system. In addition to previous work that\nemploys complex factorization techniques to enable translation between text and\nSignWriting, we show that a traditional text-to-text translation approach\nperforms equally effectively on the cleaned SignBank+ dataset. Our evaluation\nresults indicate that models trained on SignBank+ surpass those on the original\ndataset, establishing a new benchmark for SignWriting-based sign language\ntranslation and providing an open resource for future research.\n", "rewritten_text": "We present SignBank+, a refined version of the SignBank dataset designed for enhancing machine translation between spoken language text and SignWriting, a phonetic sign language writing system. Unlike previous methods that utilize intricate factorization techniques to facilitate translation between text and SignWriting, we demonstrate that a conventional text-to-text translation approach is equally effective when applied to the optimized SignBank+ dataset. Our evaluation findings reveal that models trained on SignBank+ outperform those trained on the original dataset, setting a new benchmark for SignWriting-based sign language translation and offering an accessible resource for future research endeavors."}, "2212.00244": {"original_text": "  Domain adaptation for Cross-LiDAR 3D detection is challenging due to the\nlarge gap on the raw data representation with disparate point densities and\npoint arrangements. By exploring domain-invariant 3D geometric characteristics\nand motion patterns, we present an unsupervised domain adaptation method that\novercomes above difficulties. First, we propose the Spatial Geometry Alignment\nmodule to extract similar 3D shape geometric features of the same object class\nto align two domains, while eliminating the effect of distinct point\ndistributions. Second, we present Temporal Motion Alignment module to utilize\nmotion features in sequential frames of data to match two domains. Prototypes\ngenerated from two modules are incorporated into the pseudo-label reweighting\nprocedure and contribute to our effective self-training framework for the\ntarget domain. Extensive experiments show that our method achieves\nstate-of-the-art performance on cross-device datasets, especially for the\ndatasets with large gaps captured by mechanical scanning LiDARs and solid-state\nLiDARs in various scenes. Project homepage is at\nhttps://github.com/4DVLab/CL3D.git\n", "rewritten_text": "Domain adaptation for Cross-LiDAR 3D detection poses a significant challenge due to the substantial gap in raw data representation, characterized by varying point densities and arrangements. To address these challenges, we introduce an unsupervised domain adaptation method that leverages domain-invariant 3D geometric characteristics and motion patterns. Our approach consists of two key modules: the Spatial Geometry Alignment module, which extracts similar 3D shape geometric features to align object classes across domains while mitigating the impact of differing point distributions, and the Temporal Motion Alignment module, which utilizes motion features from sequential data frames to facilitate domain matching. The prototypes generated by these modules are integrated into a pseudo-label reweighting procedure, forming the basis of our effective self-training framework for the target domain. Extensive experiments demonstrate that our method achieves state-of-the-art performance on cross-device datasets, particularly excelling in scenarios with significant disparities between mechanical scanning LiDARs and solid-state LiDARs across diverse scenes. For more information, please visit our project homepage at https://github.com/4DVLab/CL3D.git."}, "2210.10239": {"original_text": "  This paper aims to investigate representation learning for large scale visual\nplace recognition, which consists of determining the location depicted in a\nquery image by referring to a database of reference images. This is a\nchallenging task due to the large-scale environmental changes that can occur\nover time (i.e., weather, illumination, season, traffic, occlusion). Progress\nis currently challenged by the lack of large databases with accurate ground\ntruth. To address this challenge, we introduce GSV-Cities, a new image dataset\nproviding the widest geographic coverage to date with highly accurate ground\ntruth, covering more than 40 cities across all continents over a 14-year\nperiod. We subsequently explore the full potential of recent advances in deep\nmetric learning to train networks specifically for place recognition, and\nevaluate how different loss functions influence performance. In addition, we\nshow that performance of existing methods substantially improves when trained\non GSV-Cities. Finally, we introduce a new fully convolutional aggregation\nlayer that outperforms existing techniques, including GeM, NetVLAD and\nCosPlace, and establish a new state-of-the-art on large-scale benchmarks, such\nas Pittsburgh, Mapillary-SLS, SPED and Nordland. The dataset and code are\navailable for research purposes at https://github.com/amaralibey/gsv-cities.\n", "rewritten_text": "This paper investigates representation learning for large-scale visual place recognition, which involves determining the location depicted in a query image by referencing a database of reference images. The task is challenging due to significant environmental changes that can occur over time, such as weather, illumination, season, traffic, and occlusion. Progress in this area is hindered by the limited availability of large databases with accurate ground truth.\n\nTo address this challenge, we introduce GSV-Cities, a new image dataset that offers the broadest geographic coverage to date with highly precise ground truth. Spanning over 40 cities across all continents over a 14-year period, GSV-Cities enables us to explore the full potential of recent advancements in deep metric learning for training networks tailored for place recognition. We also evaluate how different loss functions impact performance.\n\nOur results demonstrate that training on GSV-Cities significantly enhances the performance of existing methods. Additionally, we introduce a novel fully convolutional aggregation layer that surpasses current techniques, including GeM, NetVLAD, and CosPlace. This advancement establishes a new state-of-the-art on large-scale benchmarks such as Pittsburgh, Mapillary-SLS, SPED, and Nordland.\n\nFor research purposes, the dataset and code can be accessed at https://github.com/amaralibey/gsv-cities."}, "2202.04101": {"original_text": "  Photoplethysmography (PPG) signals have become a key technology in many\nfields, such as medicine, well-being, or sports. Our work proposes a set of\npipelines to extract remote PPG signals (rPPG) from the face robustly,\nreliably, and configurable. We identify and evaluate the possible choices in\nthe critical steps of unsupervised rPPG methodologies. We assess a\nstate-of-the-art processing pipeline in six different datasets, incorporating\nimportant corrections in the methodology that ensure reproducible and fair\ncomparisons. In addition, we extend the pipeline by proposing three novel\nideas; 1) a new method to stabilize the detected face based on a rigid mesh\nnormalization; 2) a new method to dynamically select the different regions in\nthe face that provide the best raw signals, and 3) a new RGB to rPPG\ntransformation method, called Orthogonal Matrix Image Transformation (OMIT)\nbased on QR decomposition, that increases robustness against compression\nartifacts. We show that all three changes introduce noticeable improvements in\nretrieving rPPG signals from faces, obtaining state-of-the-art results compared\nwith unsupervised, non-learning-based methodologies and, in some databases,\nvery close to supervised, learning-based methods. We perform a comparative\nstudy to quantify the contribution of each proposed idea. In addition, we\ndepict a series of observations that could help in future implementations.\n", "rewritten_text": "Photoplethysmography (PPG) signals have emerged as a crucial technology in various fields, including medicine, well-being, and sports. Our study introduces a series of pipelines designed to robustly, reliably, and flexibly extract remote PPG signals (rPPG) from facial data. We meticulously analyze and assess the available options in the key stages of unsupervised rPPG methodologies. Through an evaluation across six distinct datasets, we enhance a cutting-edge processing pipeline by implementing essential adjustments to ensure reproducibility and equitable comparisons.\n\nFurthermore, we enhance the pipeline by introducing three innovative concepts: 1) a novel technique for stabilizing the detected face through rigid mesh normalization; 2) a method for dynamically selecting optimal facial regions for raw signal extraction; and 3) a new RGB to rPPG transformation approach, termed Orthogonal Matrix Image Transformation (OMIT), which leverages QR decomposition to enhance resilience against compression artifacts. Our findings demonstrate that these modifications significantly enhance the retrieval of rPPG signals from facial data, achieving state-of-the-art outcomes compared to unsupervised, non-learning-based methodologies and, in certain datasets, approaching the performance of supervised, learning-based methods.\n\nWe conduct a comprehensive comparative analysis to quantify the impact of each proposed innovation. Additionally, we present a series of insights that could guide future implementations."}, "1803.05449": {"original_text": "  We introduce SentEval, a toolkit for evaluating the quality of universal\nsentence representations. SentEval encompasses a variety of tasks, including\nbinary and multi-class classification, natural language inference and sentence\nsimilarity. The set of tasks was selected based on what appears to be the\ncommunity consensus regarding the appropriate evaluations for universal\nsentence representations. The toolkit comes with scripts to download and\npreprocess datasets, and an easy interface to evaluate sentence encoders. The\naim is to provide a fairer, less cumbersome and more centralized way for\nevaluating sentence representations.\n", "rewritten_text": "Introducing SentEval, a toolkit designed for assessing the quality of universal sentence representations. SentEval covers a range of tasks, such as binary and multi-class classification, natural language inference, and sentence similarity. The selection of tasks was based on the community's consensus regarding the most suitable evaluations for universal sentence representations. The toolkit includes scripts for downloading and preprocessing datasets, as well as a user-friendly interface for evaluating sentence encoders. The goal is to offer a more equitable, streamlined, and centralized approach to evaluating sentence representations."}, "1506.05929": {"original_text": "  Previous work has shown that the artist of an artwork can be identified by\nuse of computational methods that analyse digital images. However, the\ndigitised artworks are often investigated at a coarse scale discarding many of\nthe important details that may define an artist's style. In recent years high\nresolution images of artworks have become available, which, combined with\nincreased processing power and new computational techniques, allow us to\nanalyse digital images of artworks at a very fine scale. In this work we train\nand evaluate a Convolutional Neural Network (CNN) on the task of artist\nattribution using artwork images of varying resolutions. To this end, we\ncombine two existing methods to enable the application of high resolution\nimages to CNNs. By comparing the attribution performances obtained at different\nscales, we find that in most cases finer scales are beneficial to the\nattribution performance, whereas for a minority of the artists, coarser scales\nappear to be preferable. We conclude that artist attribution would benefit from\na multi-scale CNN approach which vastly expands the possibilities for\ncomputational art forensics.\n", "rewritten_text": "Previous research has demonstrated that computational methods analyzing digital images can identify the artist of an artwork. However, when examining digitized artworks, many important details that define an artist's style are often overlooked due to the coarse scale of investigation. In recent years, the availability of high-resolution images of artworks, coupled with advancements in processing power and computational techniques, has enabled the analysis of digital images at a much finer scale.\n\nIn this study, we have trained and evaluated a Convolutional Neural Network (CNN) for artist attribution using artwork images of varying resolutions. To achieve this, we have integrated two existing methods to facilitate the utilization of high-resolution images in CNNs. Through comparing attribution performances across different scales, we have observed that finer scales generally enhance attribution performance, although for a minority of artists, coarser scales may be more suitable.\n\nOur findings suggest that a multi-scale CNN approach would enhance artist attribution by significantly expanding the capabilities of computational art forensics."}, "2112.03803": {"original_text": "  Despite the great progress in video understanding made by deep convolutional\nneural networks, feature representation learned by existing methods may be\nbiased to static visual cues. To address this issue, we propose a novel method\nto suppress static visual cues (SSVC) based on probabilistic analysis for\nself-supervised video representation learning. In our method, video frames are\nfirst encoded to obtain latent variables under standard normal distribution via\nnormalizing flows. By modelling static factors in a video as a random variable,\nthe conditional distribution of each latent variable becomes shifted and scaled\nnormal. Then, the less-varying latent variables along time are selected as\nstatic cues and suppressed to generate motion-preserved videos. Finally,\npositive pairs are constructed by motion-preserved videos for contrastive\nlearning to alleviate the problem of representation bias to static cues. The\nless-biased video representation can be better generalized to various\ndownstream tasks. Extensive experiments on publicly available benchmarks\ndemonstrate that the proposed method outperforms the state of the art when only\nsingle RGB modality is used for pre-training.\n", "rewritten_text": "Despite the significant advancements in video understanding achieved by deep convolutional neural networks, the feature representation learned by current methods may exhibit bias towards static visual cues. To tackle this issue, we introduce a novel approach for suppressing static visual cues (SSVC) through probabilistic analysis in self-supervised video representation learning. Our method involves encoding video frames to derive latent variables following a standard normal distribution using normalizing flows. By treating static factors in a video as a random variable, the conditional distribution of each latent variable is transformed into a shifted and scaled normal distribution. Subsequently, we identify and suppress the less variable latent variables over time as static cues to generate motion-preserved videos. These motion-preserved videos are then used to create positive pairs for contrastive learning, aiming to mitigate the bias towards static cues in representation. The resulting video representation, less biased towards static cues, exhibits improved generalization capabilities across various downstream tasks. Extensive experiments conducted on publicly available benchmarks demonstrate that our proposed method surpasses the current state of the art when utilizing only a single RGB modality for pre-training."}, "2103.16889": {"original_text": "  Current state-of-the-art visual recognition systems usually rely on the\nfollowing pipeline: (a) pretraining a neural network on a large-scale dataset\n(e.g., ImageNet) and (b) finetuning the network weights on a smaller,\ntask-specific dataset. Such a pipeline assumes the sole weight adaptation is\nable to transfer the network capability from one domain to another domain,\nbased on a strong assumption that a fixed architecture is appropriate for all\ndomains. However, each domain with a distinct recognition target may need\ndifferent levels/paths of feature hierarchy, where some neurons may become\nredundant, and some others are re-activated to form new network structures. In\nthis work, we prove that dynamically adapting network architectures tailored\nfor each domain task along with weight finetuning benefits in both efficiency\nand effectiveness, compared to the existing image recognition pipeline that\nonly tunes the weights regardless of the architecture. Our method can be easily\ngeneralized to an unsupervised paradigm by replacing supernet training with\nself-supervised learning in the source domain tasks and performing linear\nevaluation in the downstream tasks. This further improves the search efficiency\nof our method. Moreover, we also provide principled and empirical analysis to\nexplain why our approach works by investigating the ineffectiveness of existing\nneural architecture search. We find that preserving the joint distribution of\nthe network architecture and weights is of importance. This analysis not only\nbenefits image recognition but also provides insights for crafting neural\nnetworks. Experiments on five representative image recognition tasks such as\nperson re-identification, age estimation, gender recognition, image\nclassification, and unsupervised domain adaptation demonstrate the\neffectiveness of our method.\n", "rewritten_text": "State-of-the-art visual recognition systems typically follow a specific pipeline: first, a neural network is pretrained on a large-scale dataset like ImageNet, and then the network weights are fine-tuned on a smaller, task-specific dataset. This pipeline operates under the assumption that adjusting the weights alone can transfer the network's capabilities from one domain to another, based on the belief that a fixed architecture suits all domains. However, different domains with unique recognition targets may require varying levels or paths of feature hierarchy. In some cases, certain neurons may become redundant while others are reactivated to create new network structures.\n\nIn our research, we demonstrate that dynamically adapting network architectures tailored to each domain task, in addition to weight fine-tuning, yields benefits in terms of both efficiency and effectiveness compared to the conventional image recognition pipeline that focuses solely on weight tuning without considering architecture. Our approach can be extended to an unsupervised paradigm by replacing supernet training with self-supervised learning in the source domain tasks and conducting linear evaluation in downstream tasks, thereby enhancing the search efficiency of our method.\n\nFurthermore, we offer both principled and empirical analyses to elucidate why our approach is successful, highlighting the shortcomings of existing neural architecture search methods. We emphasize the significance of preserving the joint distribution of network architecture and weights. This analysis not only enhances image recognition but also provides valuable insights for designing neural networks.\n\nOur method's efficacy is validated through experiments on five diverse image recognition tasks, including person re-identification, age estimation, gender recognition, image classification, and unsupervised domain adaptation."}, "2003.13239": {"original_text": "  Cross view feature fusion is the key to address the occlusion problem in\nhuman pose estimation. The current fusion methods need to train a separate\nmodel for every pair of cameras making them difficult to scale. In this work,\nwe introduce MetaFuse, a pre-trained fusion model learned from a large number\nof cameras in the Panoptic dataset. The model can be efficiently adapted or\nfinetuned for a new pair of cameras using a small number of labeled images. The\nstrong adaptation power of MetaFuse is due in large part to the proposed\nfactorization of the original fusion model into two parts (1) a generic fusion\nmodel shared by all cameras, and (2) lightweight camera-dependent\ntransformations. Furthermore, the generic model is learned from many cameras by\na meta-learning style algorithm to maximize its adaptation capability to\nvarious camera poses. We observe in experiments that MetaFuse finetuned on the\npublic datasets outperforms the state-of-the-arts by a large margin which\nvalidates its value in practice.\n", "rewritten_text": "The key to addressing the occlusion problem in human pose estimation lies in cross-view feature fusion. Current fusion methods require training a separate model for each pair of cameras, making scalability challenging. This study introduces MetaFuse, a pre-trained fusion model derived from a diverse set of cameras in the Panoptic dataset. MetaFuse can be efficiently adapted or fine-tuned for a new camera pair using a small number of labeled images. Its robust adaptation capability is primarily attributed to the proposed factorization of the original fusion model into two components: a universal fusion model shared across all cameras, and lightweight camera-specific transformations. Moreover, the generic model is trained using a meta-learning algorithm on multiple cameras to enhance its adaptability to different camera perspectives. Experimental results demonstrate that MetaFuse, when fine-tuned on public datasets, significantly outperforms existing methods, underscoring its practical value."}, "2011.12528": {"original_text": "  We propose a novel reference-based video colorization framework with\nspatiotemporal correspondence. Reference-based methods colorize grayscale\nframes referencing a user input color frame. Existing methods suffer from the\ncolor leakage between objects and the emergence of average colors, derived from\nnon-local semantic correspondence in space. To address this issue, we warp\ncolors only from the regions on the reference frame restricted by\ncorrespondence in time. We propagate masks as temporal correspondences, using\ntwo complementary tracking approaches: off-the-shelf instance tracking for high\nperformance segmentation, and newly proposed dense tracking to track various\ntypes of objects. By restricting temporally-related regions for referencing\ncolors, our approach propagates faithful colors throughout the video.\nExperiments demonstrate that our method outperforms state-of-the-art methods\nquantitatively and qualitatively.\n", "rewritten_text": "We present a new reference-based video colorization framework that incorporates spatiotemporal correspondence. In this framework, grayscale frames are colorized by referencing a user input color frame. Existing methods often encounter issues such as color leakage between objects and the generation of average colors based on non-local semantic correspondence in space. To tackle these challenges, we selectively extract colors only from regions in the reference frame that are constrained by temporal correspondence. This is achieved by utilizing masks as temporal correspondences, employing two complementary tracking methods: off-the-shelf instance tracking for precise segmentation and a novel dense tracking approach for tracking various object types. By confining color referencing to temporally-related regions, our approach ensures accurate color propagation throughout the video. Experimental results demonstrate that our method surpasses state-of-the-art techniques both quantitatively and qualitatively."}}