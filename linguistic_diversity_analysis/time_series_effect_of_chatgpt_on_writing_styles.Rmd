---
title: "Time Series Analysis; Effect of ChatGPT on Writing Styles"
output: html_notebook
---
```{r}

# Load required libraries
library(tidyverse)
library(nlme)        # For GLS models
library(broom)       # For model summaries
library(ggplot2)     # Visualization
library(tseries)


# =====================
# Step 1: Load the Data
# =====================

## go over all the data sources and for each, read the file "data_sources/data_sources_prefixes_pca.csv"

# Function to fit GLS for a given column
fit_gls_papers <- function(column_name) {
  formula <- as.formula(
    paste(
      column_name,
      "~ time + ONSET_chatgpt + POST_chatgpt"
    )
  )
  
  # Fit GLS model with AR(1) correlation structure
  model <- gls(formula,
               data = data,
               correlation = corAR1(form = ~ as.numeric(time)))
  
  return(model)
}

fit_gls_reddit <- function(column_name) {
  formula <- as.formula(
    paste(
      column_name,
      "~ time + ONSET_chatgpt + POST_chatgpt"
    )
  )
  
  # Fit GLS model with AR(1) correlation structure
  model <- gls(formula,
               data = data,
               correlation = corAR1(form = ~ as.numeric(time)))
  
  return(model)
}

fit_gls_news <- function(column_name) {
  formula <- as.formula(
    paste(
      column_name,
      "~ time + ONSET_chatgpt + POST_chatgpt"
    )
  )
  
  # Fit GLS model with AR(1) correlation structure
  model <- gls(formula,
               data = data,
               correlation = corAR1(form = ~ as.numeric(time)))
  
  return(model)
}


extract_gls_results <- function(model, column_name, conf_level = 0.95) {
  summary_model <- summary(model)
  coefficients <- summary_model$tTable
  confidence_intervals <- confint(model, level = conf_level)  # Get confidence intervals
  
  # Convert coefficients to a data frame
  results_df <- as.data.frame(coefficients) %>%
    rownames_to_column(var = "term") %>%
    mutate(
      PC = column_name,
      estimate = Value,
      std.error = Std.Error,
      statistic = `t-value`,
      p.value = `p-value`,
      conf.low = confidence_intervals[, 1],  # Lower bound of CI
      conf.high = confidence_intervals[, 2]  # Upper bound of CI
    ) %>%
    select(PC, term, estimate, std.error, statistic, p.value, conf.low, conf.high)
  
  return(results_df)
}


# ==========================
# Step 5: Assumption Checks
# ==========================
# Function to check residual autocorrelation
check_residuals <- function(model, column_name) {
  residuals <- residuals(model, type = "normalized")
  
  # Perform Durbin-Watson Test
  dw_test <- nlme::corAR1(form = ~ as.numeric(time))
  
  cat("### Assumption Checks for:", column_name, "###\n")
  print(summary(dw_test))
}

visualize_gls <- function(column_name, model) {
  ggplot(data, aes(x = time, y = .data[[column_name]])) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", color = "blue") +
    labs(
      title = paste("GLS Model for", column_name),
      x = "Time",
      y = column_name
    ) +
    theme_minimal()
}

data_sources = c("papers", "reddit", "news")
for (data_source_index in 1:length(data_sources)) {
  data_source = data_sources[data_source_index]
  
  print(data_source)
  
  data = read_csv(paste(
    "data",
    "/",
    data_source,
    "/",
    data_source,
    "_final_features.csv",
    sep = ""
  ))
  
  if (data_source == "news") {
    data <- data %>%
      mutate(
        time = as.Date(date),
        ONSET_chatgpt = ifelse(time >= as.Date("2022-11-01"), 1, 0),
        POST_chatgpt = ifelse(ONSET_chatgpt == 1, as.numeric(time - as.Date("2022-11-01")), 0),
        ONSET_gpt4 = ifelse(time >= as.Date("2023-03-14"), 1, 0),
        POST_gpt4 = ifelse(ONSET_gpt4 == 1, as.numeric(time - as.Date("2023-03-14")), 0),
        ONSET_claude = ifelse(time >= as.Date("2023-07-11"), 1, 0),
        POST_claude = ifelse(ONSET_claude == 1, as.numeric(time - as.Date("2023-07-11")), 0),
      )
  } else {
    data <- data %>%
    mutate(
      time = as.Date(date),
      ONSET_chatgpt = ifelse(time >= as.Date("2022-11-01"), 1, 0),
      POST_chatgpt = ifelse(ONSET_chatgpt == 1, as.numeric(time - as.Date("2022-11-01")), 0),
      ONSET_gpt4 = ifelse(time >= as.Date("2023-03-14"), 1, 0),
      POST_gpt4 = ifelse(ONSET_gpt4 == 1, as.numeric(time - as.Date("2023-03-14")), 0),
      ONSET_gpt4o = ifelse(time >= as.Date("2024-05-13"), 1, 0),
      POST_gpt4o = ifelse(ONSET_gpt4o == 1, as.numeric(time - as.Date("2024-05-13")), 0),
      ONSET_claude = ifelse(time >= as.Date("2023-07-11"), 1, 0),
      POST_claude = ifelse(ONSET_claude == 1, as.numeric(time - as.Date("2023-07-11")), 0),
      ONSET_gemini = ifelse(time >= as.Date("2023-12-06"), 1, 0),
      POST_gemini = ifelse(ONSET_gemini == 1, as.numeric(time - as.Date("2023-12-06")), 0)
    )  
  }
  
  pacf(data$complexity)
  
  

  # filter the data to contain only the data after 2018
  # Select only PC columns
  pc_columns <- array(c("complexity"), dim = 1)
  
  if (data_source == "papers") {
    gls_models <- map(pc_columns, fit_gls_papers)
  } else if (data_source == "reddit") {
    gls_models <- map(pc_columns, fit_gls_reddit)
  } else {
    gls_models <- map(pc_columns, fit_gls_news)
  }
  
  gls_results <- map_df(seq_along(pc_columns),
                        ~ extract_gls_results(gls_models[[.x]], pc_columns[.x]))
  
  
  # Filter for significant results
  significant_gls_results <- gls_results

  
  
  # Run assumption checks for significant models
  significant_pcs <- unique(significant_gls_results$PC)
  significant_gls_models <- gls_models[pc_columns %in% significant_pcs]
  
  map2(significant_gls_models,
       significant_pcs,
       ~ check_residuals(.x, .y))
  
 
  
  # ===========================
  # Step 7: Export Results
  # ===========================
  write_csv(
    significant_gls_results,
    paste(
      "data",
      "/",
      data_source,
      "/",
      data_source,
      "_",
      "significant_results_final_features.csv",
      sep = ""
    )
  )
  
}

rm(list = ls())

```
